{"layout": 0, "type": "text", "text": "MM Detection Release 2.18.0 ", "text_level": 1, "page_idx": 0, "bbox": [384, 106, 544, 162], "page_size": [612.0, 792.0]}
{"layout": 1, "type": "text", "text": "MM Detection Authors ", "page_idx": 0, "bbox": [362.3710021972656, 294.251708984375, 539.9995727539062, 318.9041442871094], "page_size": [612.0, 792.0]}
{"layout": 2, "type": "text", "text": "1 Prerequisites ", "text_level": 1, "page_idx": 2, "bbox": [72, 242, 143, 253], "page_size": [612.0, 792.0]}
{"layout": 3, "type": "text", "text": "2 Installation 3 ", "text_level": 1, "page_idx": 2, "bbox": [71, 264, 542, 274], "page_size": [612.0, 792.0]}
{"layout": 4, "type": "text", "text": "2.1 Prepare environment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n 2.2 Install MM Detection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n 2.3 Install without GPU support  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n 2.4 Another option: Docker Image  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n 2.5 A from-scratch setup script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n 2.6 Developing with multiple MM Detection versions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n\n ", "page_idx": 2, "bbox": [86, 274.6904602050781, 540, 347.7754211425781], "page_size": [612.0, 792.0]}
{"layout": 5, "type": "text", "text": "3 Verification ", "text_level": 1, "page_idx": 2, "bbox": [71, 357, 136, 369], "page_size": [612.0, 792.0]}
{"layout": 6, "type": "text", "text": "4 Benchmark and Model Zoo 9 ", "text_level": 1, "page_idx": 2, "bbox": [70, 378, 541, 390], "page_size": [612.0, 792.0]}
{"layout": 7, "type": "text", "text": "4.1 Mirror sites  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n 4.2 Common settings  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n 4.3 ImageNet Pretrained Models  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n 4.4 Baselines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n 4.5 Speed benchmark  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n 4.6 Comparison with Detectron2  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n ", "page_idx": 2, "bbox": [86, 390.2563781738281, 540, 463.34234619140625], "page_size": [612.0, 792.0]}
{"layout": 8, "type": "text", "text": "5 1: Inference and train with existing models and standard datasets 17 ", "text_level": 1, "page_idx": 2, "bbox": [69, 473, 541, 484], "page_size": [612.0, 792.0]}
{"layout": 9, "type": "text", "text": "5.1 Inference with existing models  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n 5.2 Test existing models on standard datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n 5.3 Train predefined models on standard datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n ", "page_idx": 2, "bbox": [86, 483.9053039550781, 540, 521.1253051757812], "page_size": [612.0, 792.0]}
{"layout": 10, "type": "text", "text": "6 2: Train with customized datasets 29 ", "text_level": 1, "page_idx": 2, "bbox": [69, 530, 544, 542], "page_size": [612.0, 792.0]}
{"layout": 11, "type": "text", "text": "6.1 Prepare the customized dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n 6.2 Prepare a config . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n 6.3 Train a new model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\n 6.4 Test and inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\n ", "page_idx": 2, "bbox": [86, 541.688232421875, 540, 590.8643188476562], "page_size": [612.0, 792.0]}
{"layout": 12, "type": "text", "text": "7 3: Train with customized models and standard datasets 35 ", "text_level": 1, "page_idx": 2, "bbox": [70, 600, 546, 611], "page_size": [612.0, 792.0]}
{"layout": 13, "type": "text", "text": "7.1 Prepare the standard dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n 7.2 Prepare your own customized model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n 7.3 Prepare a config . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n 7.4 Train a new model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\n 7.5 Test and inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\n ", "page_idx": 2, "bbox": [86, 611.42724609375, 540, 672.5582885742188], "page_size": [612.0, 792.0]}
{"layout": 14, "type": "text", "text": "8 Tutorial 1: Learn about Configs 41 ", "text_level": 1, "page_idx": 2, "bbox": [72, 682, 540, 700.75], "page_size": [612.0, 792.0]}
{"layout": 15, "type": "text", "text": "8.2 Config File Structure  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 8.3 Config Name Style  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n 8.4 Deprecated train_cfg/test_cfg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n 8.5 An Example of Mask R-CNN  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\n 8.6 FAQ  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n\n ", "page_idx": 2, "bbox": [86, 705.0762329101562, 540, 718.3862915039062], "page_size": [612.0, 792.0]}
{"layout": 16, "type": "text", "text": "", "page_idx": 3, "bbox": [86, 71.45246887207031, 540, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 17, "type": "text", "text": "9 Tutorial 2: Customize Datasets 55 ", "text_level": 1, "page_idx": 3, "bbox": [69, 130, 542, 142], "page_size": [612.0, 792.0]}
{"layout": 18, "type": "text", "text": "9.1 Support new data format  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n\n 9.2 Customize datasets by dataset wrappers  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n\n 9.3 Modify Dataset Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n\n 9.4 COCO Panoptic Dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n\n ", "page_idx": 3, "bbox": [86, 141.19056701660156, 540, 190.3666229248047], "page_size": [612.0, 792.0]}
{"layout": 19, "type": "text", "text": "10 Tutorial 3: Customize Data Pipelines 67 ", "text_level": 1, "page_idx": 3, "bbox": [70, 200, 540, 211], "page_size": [612.0, 792.0]}
{"layout": 20, "type": "text", "text": "10.1 Design of Data pipelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\n 10.2 Extend and use custom pipelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\n ", "page_idx": 3, "bbox": [86, 210.92958068847656, 540, 236.1946258544922], "page_size": [612.0, 792.0]}
{"layout": 21, "type": "text", "text": "11.1 Develop new components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n\n ", "page_idx": 3, "bbox": [86, 256.7575988769531, 540, 270.067626953125], "page_size": [612.0, 792.0]}
{"layout": 22, "type": "text", "text": "12 Tutorial 5: Customize Runtime Settings 79 ", "text_level": 1, "page_idx": 3, "bbox": [70, 279, 540, 291], "page_size": [612.0, 792.0]}
{"layout": 23, "type": "text", "text": "12.1 Customize optimization settings  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n 12.2 Customize training schedules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n\n 12.3 Customize workflow  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\n 12.4 Customize hooks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\n ", "page_idx": 3, "bbox": [86, 290.6305847167969, 540, 339.8055725097656], "page_size": [612.0, 792.0]}
{"layout": 24, "type": "text", "text": "13 Tutorial 6: Customize Losses 87 ", "text_level": 1, "page_idx": 3, "bbox": [70, 349, 542, 361], "page_size": [612.0, 792.0]}
{"layout": 25, "type": "text", "text": "13.1 Computation pipeline of a loss  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n\n 13.2 Set sampling method (step 1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n\n 13.3 Tweaking loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n\n 13.4 Weighting loss (step 3)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n\n ", "page_idx": 3, "bbox": [86, 360.3685302734375, 540, 409.5445251464844], "page_size": [612.0, 792.0]}
{"layout": 26, "type": "text", "text": "14 Tutorial 7: Finetuning Models 91 ", "text_level": 1, "page_idx": 3, "bbox": [70, 419, 540, 430], "page_size": [612.0, 792.0]}
{"layout": 27, "type": "text", "text": "14.1 Inherit base configs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n\n 14.2 Modify head  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n\n 14.3 Modify dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\n 14.4 Modify training schedule  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\n 14.5 Use pre-trained model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n\n ", "page_idx": 3, "bbox": [86, 430.10748291015625, 540, 491.23846435546875], "page_size": [612.0, 792.0]}
{"layout": 28, "type": "text", "text": "15 Tutorial 8: Pytorch to ONNX (Experimental) ", "text_level": 1, "page_idx": 3, "bbox": [70, 501, 537, 512], "page_size": [612.0, 792.0]}
{"layout": 29, "type": "text", "text": "15.1 How to convert models from Pytorch to ONNX  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n\n 15.2 How to evaluate the exported models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n\n 15.3 List of supported models exportable to ONNX  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n\n 15.4 The Parameters of Non-Maximum Suppression in ONNX Export . . . . . . . . . . . . . . . . . . . 99\n\n 15.5 Reminders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n 15.6 FAQs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n ", "page_idx": 3, "bbox": [86, 511.8003845214844, 540, 584.8864135742188], "page_size": [612.0, 792.0]}
{"layout": 30, "type": "text", "text": "16 Tutorial 9: ONNX to TensorRT (Experimental) 101 ", "text_level": 1, "page_idx": 3, "bbox": [70, 594, 538, 606], "page_size": [612.0, 792.0]}
{"layout": 31, "type": "text", "text": "16.1 How to convert models from ONNX to TensorRT  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n\n 16.2 How to evaluate the exported models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n\n 16.3 List of supported models convertible to TensorRT . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n\n 16.4 Reminders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n\n 16.5 FAQs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n\n ", "page_idx": 3, "bbox": [86, 605.4494018554688, 540, 666.5804443359375], "page_size": [612.0, 792.0]}
{"layout": 32, "type": "text", "text": "17 Tutorial 10: Weight initialization 105 ", "text_level": 1, "page_idx": 3, "bbox": [70, 676, 540, 687], "page_size": [612.0, 792.0]}
{"layout": 33, "type": "text", "text": "17.1 Description  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n\n 17.2 Initialize parameters  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 17.3 Usage of init_cfg  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 ", "page_idx": 3, "bbox": [86, 687.1433715820312, 540, 712.408447265625], "page_size": [612.0, 792.0]}
{"layout": 34, "type": "text", "text": "", "page_idx": 4, "bbox": [86, 71.45246887207031, 540, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 35, "type": "text", "text": "18 Log Analysis 109 ", "text_level": 1, "page_idx": 4, "bbox": [72, 94, 542, 106], "page_size": [612.0, 792.0]}
{"layout": 36, "type": "text", "text": "19 Result Analysis 111 ", "text_level": 1, "page_idx": 4, "bbox": [73, 116, 540, 128], "page_size": [612.0, 792.0]}
{"layout": 37, "type": "text", "text": "20 Visualization 113 ", "text_level": 1, "page_idx": 4, "bbox": [71, 138, 541, 149], "page_size": [612.0, 792.0]}
{"layout": 38, "type": "text", "text": "20.1 Visualize Datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 20.2 Visualize Models  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 20.3 Visualize Predictions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 ", "page_idx": 4, "bbox": [86, 149.16041564941406, 540, 186.38145446777344], "page_size": [612.0, 792.0]}
{"layout": 39, "type": "text", "text": "21 Error Analysis 115 ", "text_level": 1, "page_idx": 4, "bbox": [73, 195, 542, 208], "page_size": [612.0, 792.0]}
{"layout": 40, "type": "text", "text": "22 Model Serving 117 ", "text_level": 1, "page_idx": 4, "bbox": [71, 217, 541, 226.75], "page_size": [612.0, 792.0]}
{"layout": 41, "type": "text", "text": "22.1 1. Convert model from MM Detection to TorchServe  . . . . . . . . . . . . . . . . . . . . . . . . . . 117 22.2 2. Build  mmdet-serve  docker image  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 22.3 3. Run  mmdet-serve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 22.4 4. Test deployment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 ", "page_idx": 4, "bbox": [86, 228.86146545410156, 540, 278.03753662109375], "page_size": [612.0, 792.0]}
{"layout": 42, "type": "text", "text": "23 Model Complexity 121 ", "text_level": 1, "page_idx": 4, "bbox": [72, 287, 539, 299], "page_size": [612.0, 792.0]}
{"layout": 43, "type": "text", "text": "24 Model conversion 123 ", "text_level": 1, "page_idx": 4, "bbox": [70, 308, 542, 317.75], "page_size": [612.0, 792.0]}
{"layout": 44, "type": "text", "text": "24.1 MM Detection model to ONNX (experimental)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 24.2 MM Detection 1.x model to MM Detection 2.x  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 24.3 RegNet model to MM Detection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 24.4 Detectron ResNet to Pytorch  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 24.5 Prepare a model for publishing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 ", "page_idx": 4, "bbox": [86, 320.51849365234375, 540, 381.6484680175781], "page_size": [612.0, 792.0]}
{"layout": 45, "type": "text", "text": "25 Dataset Conversion 125 ", "text_level": 1, "page_idx": 4, "bbox": [73, 390, 541, 403], "page_size": [612.0, 792.0]}
{"layout": 46, "type": "text", "text": "26 Benchmark 127 26.1 Robust Detection Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 26.2 FPS Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 ", "page_idx": 4, "bbox": [72, 411.5467834472656, 540, 449.3944396972656], "page_size": [612.0, 792.0]}
{"layout": 47, "type": "text", "text": "27 Miscellaneous 129 27.1 Evaluating a metric  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 27.2 Print the entire config . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 ", "page_idx": 4, "bbox": [72, 457.374755859375, 540, 495.222412109375], "page_size": [612.0, 792.0]}
{"layout": 48, "type": "text", "text": "28 Hyper-parameter Optimization 131 28.1 YOLO Anchor Optimization  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 ", "page_idx": 4, "bbox": [72, 503.2027587890625, 540, 529.0953979492188], "page_size": [612.0, 792.0]}
{"layout": 49, "type": "text", "text": "29 Conventions 133 ", "text_level": 1, "page_idx": 4, "bbox": [71, 539, 542, 547.75], "page_size": [612.0, 792.0]}
{"layout": 50, "type": "text", "text": "29.1 Loss  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 29.2 Empty Proposals  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 29.3 Coco Panoptic Dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 ", "page_idx": 4, "bbox": [86, 549.6583251953125, 540, 586.87939453125], "page_size": [612.0, 792.0]}
{"layout": 51, "type": "text", "text": "30 Compatibility of MM Detection 2.x 135 ", "text_level": 1, "page_idx": 4, "bbox": [69, 595, 541, 607], "page_size": [612.0, 792.0]}
{"layout": 52, "type": "text", "text": "30.1 MM Detection 2.18.0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 30.2 MM Detection 2.14.0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 30.3 MM Detection 2.12.0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 30.4 Compatibility with MM Detection 1.x  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 30.5 py coco tools compatibility  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 ", "page_idx": 4, "bbox": [86, 607.4423217773438, 540, 668.5723876953125], "page_size": [612.0, 792.0]}
{"layout": 53, "type": "text", "text": "31 Projects based on MM Detection 139 ", "text_level": 1, "page_idx": 4, "bbox": [69, 677, 540, 689], "page_size": [612.0, 792.0]}
{"layout": 54, "type": "text", "text": "31.1 Projects as an extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 31.2 Projects of papers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 ", "page_idx": 4, "bbox": [86, 689.1353149414062, 540, 714.4014282226562], "page_size": [612.0, 792.0]}
{"layout": 55, "type": "text", "text": "32 Changelog ", "text_level": 1, "page_idx": 5, "bbox": [71, 73, 133, 81.75], "page_size": [612.0, 792.0]}
{"layout": 56, "type": "text", "text": "32.1 v2.18.0 (27/10/2021)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n\n 32.2 v2.17.0 (28/9/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n\n 32.3 v2.16.0 (30/8/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n\n 32.4 v2.15.1 (11/8/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n\n 32.5 v2.15.0 (02/8/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n\n 32.6 v2.14.0 (29/6/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n\n 32.7 v2.13.0 (01/6/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n\n 32.8 v2.12.0 (01/5/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n\n 32.9 v2.11.0 (01/4/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n\n 32.10 v2.10.0 (01/03/2021)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 32.11 v2.9.0 (01/02/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n\n 32.12 v2.8.0 (04/01/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n\n 32.13 v2.7.0 (30/11/2020) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\n 32.14 v2.6.0 (1/11/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n\n 32.15 v2.5.0 (5/10/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n\n 32.16 v2.4.0 (5/9/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n\n 32.17 v2.3.0 (5/8/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n\n 32.18 v2.2.0 (1/7/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n\n 32.19 v2.1.0 (8/6/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n\n 32.20 v2.0.0 (6/5/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n\n 32.21 v1.1.0 (24/2/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n\n 32.22 v1.0.0 (30/1/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n\n 32.23 v1.0rc1 (13/12/2019)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n\n 32.24 v1.0rc0 (27/07/2019)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.25 v0.6.0 (14/04/2019) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.26 v0.6rc0(06/02/2019)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.27 v0.5.7 (06/02/2019) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.28 v0.5.6 (17/01/2019) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.29 v0.5.5 (22/12/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.30 v0.5.4 (27/11/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n 32.31 v0.5.3 (26/11/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n 32.32 v0.5.2 (21/10/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n 32.33 v0.5.1 (20/10/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n ", "page_idx": 5, "bbox": [86, 83.40748596191406, 540, 479.28253173828125], "page_size": [612.0, 792.0]}
{"layout": 57, "type": "text", "text": "33.1 MMCV Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n\n 33.2 PyTorch/CUDA Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n\n 33.3 Training  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n\n 33.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n\n ", "page_idx": 5, "bbox": [86, 499.8454895019531, 540, 549.021484375], "page_size": [612.0, 792.0]}
{"layout": 58, "type": "text", "text": "34 English ", "text_level": 1, "page_idx": 5, "bbox": [72, 559, 119, 570], "page_size": [612.0, 792.0]}
{"layout": 59, "type": "text", "text": "36 mmdet.apis ", "text_level": 1, "page_idx": 5, "bbox": [71, 603, 136, 613], "page_size": [612.0, 792.0]}
{"layout": 60, "type": "text", "text": "37 mmdet.core 183 ", "text_level": 1, "page_idx": 5, "bbox": [71, 625, 540, 636], "page_size": [612.0, 792.0]}
{"layout": 61, "type": "text", "text": "37.1 anchor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n\n 37.2 bbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n\n 37.3 export  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n\n 37.4 mask  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n\n 37.5 evaluation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n\n 37.6 post processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n\n 37.7 utils  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 38.1 datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 38.2 pipelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 38.3 samplers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 38.4 api wrappers  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 ", "page_idx": 5, "bbox": [86, 635.3374633789062, 540, 720.3785400390625], "page_size": [612.0, 792.0]}
{"layout": 62, "type": "text", "text": "", "page_idx": 6, "bbox": [86, 83.40748596191406, 540, 132.5825653076172], "page_size": [612.0, 792.0]}
{"layout": 63, "type": "text", "text": "39 mmdet.models 259 39.1 detectors  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259 39.2 backbones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273 39.3 necks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 39.4 dense heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301 39.5 roi_heads  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 39.6 losses  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412 39.7 utils  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425 ", "page_idx": 6, "bbox": [72.00003051757812, 140.5628662109375, 540, 238.18663024902344], "page_size": [612.0, 792.0]}
{"layout": 64, "type": "text", "text": "439 ", "page_idx": 6, "bbox": [525, 268.0849609375, 540, 282.6203918457031], "page_size": [612.0, 792.0]}
{"layout": 65, "type": "text", "text": "441 ", "page_idx": 6, "bbox": [525, 290.0029602050781, 540, 304.53839111328125], "page_size": [612.0, 792.0]}
{"layout": 66, "type": "text", "text": "443 ", "page_idx": 6, "bbox": [525, 311.92095947265625, 540, 326.4563903808594], "page_size": [612.0, 792.0]}
{"layout": 67, "type": "text", "text": "PREREQUISITES ", "page_idx": 8, "bbox": [423.6090087890625, 162.4604949951172, 540, 183.00425720214844], "page_size": [612.0, 792.0]}
{"layout": 68, "type": "text", "text": "", "page_idx": 8, "bbox": [87, 227, 318, 234.75], "page_size": [612.0, 792.0]}
{"layout": 69, "type": "text", "text": "• Python  $^{3.6+}$  • PyTorch   $1.3+$  • CUDA  $9.2+$   (If you build PyTorch from source, CUDA 9.0 is also compatible) • GCC  $^{5+}$  •  MMCV ", "page_idx": 8, "bbox": [88, 243.80543518066406, 408.8658447265625, 328.84649658203125], "page_size": [612.0, 792.0]}
{"layout": 70, "type": "text", "text": "Compatible MM Detection and MMCV versions are shown as below. Please install the correct version of MMCV to avoid installation issues. ", "page_idx": 8, "bbox": [72, 333.469482421875, 540, 358.7344970703125], "page_size": [612.0, 792.0]}
{"layout": 71, "type": "text", "text": "Note:  You need to run  pip uninstall mmcv  first if you have mmcv installed. If mmcv and mmcv-full are both installed, there will be  Module Not Found Error . ", "page_idx": 8, "bbox": [72, 362.7298278808594, 540, 388.62249755859375], "page_size": [612.0, 792.0]}
{"layout": 72, "type": "text", "text": "INSTALLATION ", "text_level": 1, "page_idx": 10, "bbox": [436, 164, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 73, "type": "text", "text": "2.1 Prepare environment ", "text_level": 1, "page_idx": 10, "bbox": [70, 229, 245, 245], "page_size": [612.0, 792.0]}
{"layout": 74, "type": "text", "text": "1. Create a conda virtual environment and activate it. ", "page_idx": 10, "bbox": [84.4530029296875, 260.8024597167969, 298.03118896484375, 274.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 75, "type": "table", "page_idx": 10, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_0.jpg", "bbox": [89, 273.25, 546, 500], "page_size": [612.0, 792.0], "ocr_text": "conda create -n openmmlab python=3.7 -y\nconda activate openmmlab\n\n. Install PyTorch and torchvision following the official instructions, e.g.,\n\nconda install pytorch torchvision -c pytorch\n\nNote: Make sure that your compilation CUDA version and runtime CUDA version match. You can check the\nsupported CUDA version for precompiled packages on the PyTorch website.\n\nE.g.1 If you have CUDA 10.1 installed under /usr/local/cuda and would like to install PyTorch 1.5, you\nneed to install the prebuilt PyTorch with CUDA 10.1.\n\nconda install pytorch cudatoolkit=10.1 torchvision -c pytorch\n\nE.g. 2 If you have CUDA 9.2 installed under /usr/local/cuda and would like to install PyTorch 1.3.1., you\nneed to install the prebuilt PyTorch with CUDA 9.2.\n\nconda install pytorch=1.3.1 cudatoolkit=9.2 torchvision=0.4.2 -c pytorch\n\n", "vlm_text": "The image provides a step-by-step guide for setting up a Python environment using conda, specifically for using PyTorch and torchvision with CUDA support. Here's a breakdown of the steps shown:\n\n1. **Create and activate a conda environment:**\n   - Use the command `conda create -n openmmlab python=3.7 -y` to create a new environment named `openmmlab` with Python version 3.7.\n   - Activate the environment using `conda activate openmmlab`.\n\n2. **Install PyTorch and torchvision:**\n   - Follow the official instructions, which are not entirely detailed in the image, but generally involve using the command:\n     ```\n     conda install pytorch torchvision -c pytorch\n     ```\n   - It is important to ensure that your compilation CUDA version and runtime CUDA version match. Check compatibility on the PyTorch website.\n\n3. **Examples of CUDA installation:**\n   - **Example 1:** For a setup with CUDA 10.1:\n     ```\n     conda install pytorch cudatoolkit=10.1 torchvision -c pytorch\n     ```\n   - **Example 2:** For a setup with CUDA 9.2 and PyTorch 1.3.1:\n     ```\n     conda install pytorch=1.3.1 cudatoolkit=9.2 torchvision=0.4.2 -c pytorch\n     ```\n\nThese instructions highlight the importance of matching the version of PyTorch and CUDA toolkit to ensure compatibility when setting up the environment for machine learning purposes."}
{"layout": 76, "type": "text", "text": "If you build PyTorch from source instead of installing the prebuilt package, you can use more CUDA versions such as 9.0. ", "page_idx": 10, "bbox": [96, 505.8835144042969, 540, 531.1484985351562], "page_size": [612.0, 792.0]}
{"layout": 77, "type": "text", "text": "2.2 Install MM Detection ", "text_level": 1, "page_idx": 10, "bbox": [70, 553, 236, 569], "page_size": [612.0, 792.0]}
{"layout": 78, "type": "text", "text": "It is recommended to install MM Detection with  MIM , which automatically handle the dependencies of OpenMMLab projects, including mmcv and other python packages. ", "page_idx": 10, "bbox": [72, 585.0244750976562, 540, 610.2894897460938], "page_size": [612.0, 792.0]}
{"layout": 79, "type": "table", "page_idx": 10, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_1.jpg", "bbox": [68, 616, 543, 645], "page_size": [612.0, 792.0], "ocr_text": "pip install openmim\nmim install mmdet\n", "vlm_text": "The table contains two command lines:\n\n1. `pip install openmim` \n2. `mim install mmdet`\n\nThese commands are typically used in a software development context to install Python packages. The first command uses pip (a package manager for Python) to install a package named `openmim`, and the second command uses the `mim` tool to install a package called `mmdet`. The exact purpose of these packages would be determined by their respective functionalities in the context they are used."}
{"layout": 80, "type": "text", "text": "Or you can still install MM Detection manually: ", "page_idx": 10, "bbox": [72, 653.7664184570312, 260.641845703125, 667.0764770507812], "page_size": [612.0, 792.0]}
{"layout": 81, "type": "text", "text": "(continued from previous page) ", "page_idx": 11, "bbox": [439.68798828125, 72.96453094482422, 540, 83.61258697509766], "page_size": [612.0, 792.0]}
{"layout": 82, "type": "text", "text": "Please replace  {cu_version}  and  {torch version}  in the url to your desired one. For example, to install the latest  mmcv-full  with  CUDA 11.0  and  PyTorch 1.7.0 , use the following command: ", "page_idx": 11, "bbox": [96, 104.53645324707031, 540, 129.80149841308594], "page_size": [612.0, 792.0]}
{"layout": 83, "type": "text", "text": "pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/  $\\hookrightarrow$  index.html → ", "page_idx": 11, "bbox": [96, 139.5347442626953, 531.0277099609375, 164.38136291503906], "page_size": [612.0, 792.0]}
{"layout": 84, "type": "text", "text": "See  here  for different versions of MMCV compatible to different PyTorch and CUDA versions. ", "page_idx": 11, "bbox": [96, 174.07545471191406, 474.62896728515625, 187.38548278808594], "page_size": [612.0, 792.0]}
{"layout": 85, "type": "text", "text": "Optionally you can compile mmcv from source if you need to develop both mmcv and mmdet. Refer to the  guide for details. ", "page_idx": 11, "bbox": [96, 192.00843811035156, 540, 217.2734832763672], "page_size": [612.0, 792.0]}
{"layout": 86, "type": "text", "text": "2. Install MM Detection. ", "page_idx": 11, "bbox": [84, 221.8954620361328, 183, 235.2054901123047], "page_size": [612.0, 792.0]}
{"layout": 87, "type": "text", "text": "You can simply install mm detection with the following command: ", "page_idx": 11, "bbox": [96, 239.8284454345703, 359.8299255371094, 253.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 88, "type": "table", "page_idx": 11, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_2.jpg", "bbox": [93, 255, 544, 360], "page_size": [612.0, 792.0], "ocr_text": "pip install mmdet\n\nor clone the repository and then install it:\n\ngit clone https://github.com/open-mmlab/mmdetection.git\n\ncd mmdetection\n\npip install -r requirements/build.txt\n\npip install -v -e .\n\n# or \"python setup.py develop\"\n\n", "vlm_text": "The table provides two methods for installing the `mmdet` library:\n\n1. Install `mmdet` using pip:\n   ```\n   pip install mmdet\n   ```\n\n2. Clone the repository and install it manually. The steps are:\n   - Clone the repository from GitHub:\n     ```\n     git clone https://github.com/open-mmlab/mmdetection.git\n     ```\n   - Change the directory to the cloned repository:\n     ```\n     cd mmdetection\n     ```\n   - Install the requirements from `requirements/build.txt`:\n     ```\n     pip install -r requirements/build.txt\n     ```\n   - Install the package in editable mode:\n     ```\n     pip install -v -e .  # or \"python setup.py develop\"\n     ```"}
{"layout": 89, "type": "text", "text": "3. Install extra dependencies for Instaboost, Panoptic Segmentation, LVIS dataset, or Albumen tat ions. ", "page_idx": 11, "bbox": [84, 366.95147705078125, 492, 380.2615051269531], "page_size": [612.0, 792.0]}
{"layout": 90, "type": "text", "text": "# for instaboost pip install insta boost fast # for panoptic segmentation pip install git+https://github.com/coco data set/pan optic api.git # for LVIS dataset pip install git+https://github.com/lvis-dataset/lvis-api.git # for albumen tat ions pip install albumen tat ions>  $\\scriptstyle\\cdot=\\!0$  .3.2 --no-binary imgaug,albumen tat ions ", "page_idx": 11, "bbox": [96, 389.9957275390625, 447.341796875, 483.6940612792969], "page_size": [612.0, 792.0]}
{"layout": 91, "type": "text", "text": "Note: ", "text_level": 1, "page_idx": 11, "bbox": [71, 497, 95, 509], "page_size": [612.0, 792.0]}
{"layout": 92, "type": "text", "text": "a. When specifying  -e  or  develop , MM Detection is installed on dev mode , any local modifications made to the code will take effect without re installation. ", "page_idx": 11, "bbox": [72, 514.199462890625, 540, 539.4645385742188], "page_size": [612.0, 792.0]}
{"layout": 93, "type": "text", "text": "b. If you would like to use  opencv-python-headless  instead of  opencv-python , you can install it before installing MMCV. ", "page_idx": 11, "bbox": [72, 544.0874633789062, 540, 569.3525390625], "page_size": [612.0, 792.0]}
{"layout": 94, "type": "text", "text": "c. Some dependencies are optional. Simply running  pip install -v -e .  will only install the minimum runtime requirements. To use optional dependencies like  albumen tat ions  and  image corruptions  either install them man- ually with  pip install -r requirements/optional.txt  or specify desired extras when calling  pip  (e.g.  pip install -v -e .[optional] ). Valid keys for the extras field are:  all ,  tests ,  build , and  optional . ", "page_idx": 11, "bbox": [72, 573.9754638671875, 540, 623.1515502929688], "page_size": [612.0, 792.0]}
{"layout": 95, "type": "text", "text": "d. If you would like to use  albumen tat ions , we suggest using  pip install albumen tat ions>  $\\scriptstyle{\\sum=0}$  .3.2 --no-binary imgaug,albumen tat ions . If you simply use  pip install albumen tat ions  $\\scriptstyle:>=\\varnothing\\,.\\,3\\,.\\,2$  , it will in- stall  opencv-python-headless  simultaneously (even though you have already installed  opencv-python ). We should not allow  opencv-python  and  opencv-python-headless  installed at the same time, because it might cause unexpected issues. Please refer to  official documentation  for more details. ", "page_idx": 11, "bbox": [72, 627.7734985351562, 540, 688.904541015625], "page_size": [612.0, 792.0]}
{"layout": 96, "type": "text", "text": "2.3 Install without GPU support ", "text_level": 1, "page_idx": 12, "bbox": [70, 71, 290, 89], "page_size": [612.0, 792.0]}
{"layout": 97, "type": "text", "text": "MM Detection can be built for CPU only environment (where CUDA isn’t available). ", "page_idx": 12, "bbox": [72, 102.99446105957031, 408.7955627441406, 116.30449676513672], "page_size": [612.0, 792.0]}
{"layout": 98, "type": "text", "text": "In CPU mode you can run the demo/webcam demo.py for example. However some functionality is gone in this mode: ", "page_idx": 12, "bbox": [72, 120.92646789550781, 540, 134.2364959716797], "page_size": [612.0, 792.0]}
{"layout": 99, "type": "text", "text": "• Deformable Convolution • Modulated Deformable Convolution • ROI pooling • Deformable ROI pooling • CARAFE: Content-Aware ReAssembly of FEatures • Sync Batch Norm • CrissCross Attention: Criss-Cross Attention • Masked Con v 2 d • Temporal Interlace Shift • nms_cuda • s igm oid focal loss cuda • b box overlaps ", "page_idx": 12, "bbox": [88, 138.8594512939453, 303, 349.4294738769531], "page_size": [612.0, 792.0]}
{"layout": 100, "type": "text", "text": "If you try to run inference with a model containing above ops, an error will be raised. The following table lists affected algorithms. ", "page_idx": 12, "bbox": [72, 354.0524597167969, 540, 379.3174743652344], "page_size": [612.0, 792.0]}
{"layout": 101, "type": "text", "text": "Notice:  MM Detection does not support training with CPU for now. ", "page_idx": 12, "bbox": [72, 383.31280517578125, 340.9002380371094, 397.8482360839844], "page_size": [612.0, 792.0]}
{"layout": 102, "type": "text", "text": "2.4 Another option: Docker Image ", "text_level": 1, "page_idx": 12, "bbox": [70, 419, 307, 436], "page_size": [612.0, 792.0]}
{"layout": 103, "type": "text", "text": "We provide a  Dockerfile  to build an image. Ensure that you are using  docker version  $>=19.03$  . ", "page_idx": 12, "bbox": [72, 451.12646484375, 446.6534423828125, 464.4364929199219], "page_size": [612.0, 792.0]}
{"layout": 104, "type": "text", "text": "# build an image with PyTorch 1.6, CUDA 10.1 docker build -t mm detection docker/ ", "page_idx": 12, "bbox": [72, 473.37274169921875, 303, 495.34014892578125], "page_size": [612.0, 792.0]}
{"layout": 105, "type": "text", "text": "Run it with ", "page_idx": 12, "bbox": [72, 507.9134826660156, 117, 521.2235107421875], "page_size": [612.0, 792.0]}
{"layout": 106, "type": "text", "text": "docker run --gpus all --shm-size = 8g -it -v  { DATA_DIR } :/mm detection/data mm detection ", "page_idx": 12, "bbox": [72, 530.1597290039062, 506.1207580566406, 540.1721801757812], "page_size": [612.0, 792.0]}
{"layout": 107, "type": "text", "text": "2.5 A from-scratch setup script ", "text_level": 1, "page_idx": 12, "bbox": [71, 570, 288, 587], "page_size": [612.0, 792.0]}
{"layout": 108, "type": "text", "text": "Assuming that you already have CUDA 10.1 installed, here is a full script for setting up MM Detection with conda. ", "page_idx": 12, "bbox": [72, 601.9974365234375, 527.9083862304688, 615.3074951171875], "page_size": [612.0, 792.0]}
{"layout": 109, "type": "text", "text": "conda create -n openmmlab  python = 3 .7 -y conda activate openmmlab conda install  pytorch  $\\scriptstyle{\\mathcal{S}}=1.6.0$   torch vision  $\\scriptstyle{1\\equiv=0}$  .7.0  cuda toolkit = 10 .1 -c pytorch -y # install the latest mmcv pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index. ˓ → html (continues on next page) ", "page_idx": 12, "bbox": [72, 624.2447509765625, 540, 720.8223266601562], "page_size": [612.0, 792.0]}
{"layout": 110, "type": "text", "text": "2.6 Developing with multiple MM Detection versions ", "text_level": 1, "page_idx": 13, "bbox": [71, 188, 424, 204], "page_size": [612.0, 792.0]}
{"layout": 111, "type": "text", "text": "The train and test scripts already modify the  PYTHONPATH  to ensure the script use the MM Detection in the current directory. ", "page_idx": 13, "bbox": [72, 219.9734649658203, 540, 245.23851013183594], "page_size": [612.0, 792.0]}
{"layout": 112, "type": "text", "text": "To use the default MM Detection installed in the environment rather than that you are working with, you can remove the following line in those scripts ", "page_idx": 13, "bbox": [72, 249.86146545410156, 540, 275.12652587890625], "page_size": [612.0, 792.0]}
{"layout": 113, "type": "text", "text": "VERIFICATION ", "page_idx": 14, "bbox": [440.35101318359375, 162.4604949951172, 540, 183.00425720214844], "page_size": [612.0, 792.0]}
{"layout": 114, "type": "text", "text": "To verify whether MM Detection is installed correctly, we can run the following sample code to initialize a detector and inference a demo image. ", "page_idx": 14, "bbox": [70, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 115, "type": "text", "text": "from  mmdet.apis  import  in it detector, inference detector ", "page_idx": 14, "bbox": [70, 260.2739562988281, 365.1102600097656, 271.1830139160156], "page_size": [612.0, 792.0]}
{"layout": 116, "type": "text", "text": "config file  $=$   ' configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py '\n\n # download the checkpoint from model zoo and put it in  \\` checkpoints/ \\`\n\n # url: https://download.openmmlab.com/mm detection/v2.0/faster r cnn/faster r cnn r 50 fp n\n\n  $\\hookrightarrow$  1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n\n → checkpoint file  $=$   ' checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth ' device  $=$   ' cuda:0 ' # init a detector model  $=$   in it detector(config file, checkpoint file, device  $=$  device)  $\\#$   inference the demo image inference detector(model,  ' demo/demo.jpg ' ) ", "page_idx": 14, "bbox": [70, 284.7817077636719, 527.0421752929688, 402.4176330566406], "page_size": [612.0, 792.0]}
{"layout": 117, "type": "text", "text": "The above code is supposed to run successfully upon you finish the installation. ", "page_idx": 14, "bbox": [70, 414.9634704589844, 388.1032409667969, 428.27349853515625], "page_size": [612.0, 792.0]}
{"layout": 118, "type": "text", "text": "BENCHMARK AND MODEL ZOO ", "text_level": 1, "page_idx": 16, "bbox": [319, 164, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 119, "type": "text", "text": "4.1 Mirror sites ", "text_level": 1, "page_idx": 16, "bbox": [71, 228, 179, 245], "page_size": [612.0, 792.0]}
{"layout": 120, "type": "text", "text": "We only use aliyun to maintain the model zoo since MM Detection V2.0. The model zoo of V1.x has been deprecated. ", "page_idx": 16, "bbox": [71, 260.8024597167969, 540, 274.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 121, "type": "text", "text": "4.2 Common settings ", "text_level": 1, "page_idx": 16, "bbox": [70, 297, 222, 314], "page_size": [612.0, 792.0]}
{"layout": 122, "type": "text", "text": "• All models were trained on  coco 2017 train , and tested on the  coco 2017 val . ", "page_idx": 16, "bbox": [88, 328.6744384765625, 431.27264404296875, 341.9844665527344], "page_size": [612.0, 792.0]}
{"layout": 123, "type": "text", "text": "• We use distributed training. • All pytorch-style pretrained backbones on ImageNet are from PyTorch model zoo, caffe-style pretrained back- bones are converted from the newly released model from detectron2. • For fair comparison with other codebases, we report the GPU memory as the maximum value of  torch.cuda. max memory allocated()  for all 8 GPUs. Note that this value is usually less than what  nvidia-smi  shows. • We report the inference time as the total time of network forwarding and post-processing, excluding the data loading time. Results are obtained with the script  benchmark.py  which computes the average time on 2000 images. ", "page_idx": 16, "bbox": [88, 346.6064453125, 540, 461.53546142578125], "page_size": [612.0, 792.0]}
{"layout": 124, "type": "text", "text": "4.3 ImageNet Pretrained Models ", "text_level": 1, "page_idx": 16, "bbox": [71, 483, 293, 500], "page_size": [612.0, 792.0]}
{"layout": 125, "type": "text", "text": "It is common to initialize from backbone models pre-trained on ImageNet classification task. All pre-trained model links can be found at  open_mmlab . According to  img norm cf g  and source of weight, we can divide all the ImageNet pre-trained model weights into some cases: ", "page_idx": 16, "bbox": [71, 516.0973510742188, 540, 553.3174438476562], "page_size": [612.0, 792.0]}
{"layout": 126, "type": "text", "text": "• Torch Vision: Corresponding to torch vision weight, including ResNet50, ResNet101. The  img norm cf g  is dict(mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], std  $=$  [58.395, 57.12, 57.375], to_rgb  $=$  True) . • Pycls: Corresponding to  pycls  weight, including RegNetX. The  img norm cf g  is  dict( mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [103.530, 116.280, 123.675], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [57.375, 57.12, 58.395], to_rgb  $=$  False) . • MSRA styles: Corresponding to  MSRA  weights, including Res Net 50 Caff e and Res Net 101 Caff e. The img norm cf g is dict( mean  $\\risingdotseq$  [103.530, 116.280, 123.675], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [1.0, 1.0, 1.0], to_rgb  $\\leftrightharpoons$  False) . • Caffe2 styles: Currently only contains Res Next 101 32 x 8 d. The  img norm cf g  is  dict(mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [103.530, 116.280, 123.675], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [57.375, 57.120, 58.395], to_rgb  $\\mathrm{=}$  False) . ", "page_idx": 16, "bbox": [88, 557.9403686523438, 540, 684.824462890625], "page_size": [612.0, 792.0]}
{"layout": 127, "type": "text", "text": "• Other styles: E.g SSD which corresponds to  img norm cf g  is  dict(mean  $\\risingdotseq$  [123.675, 116.28, 103.53], std  $\\scriptstyle=\\left[1\\right]$  , 1, 1], to_rgb  $\\leftrightharpoons$  True)  and YOLOv3 which corresponds to  img norm cf g  is  dict(mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [0, 0, 0], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [255., 255., 255.], to_rgb  $\\risingdotseq$  True) . ", "page_idx": 17, "bbox": [88.43800354003906, 71.45246887207031, 540, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 128, "type": "text", "text": "The detailed table of the commonly used backbone models in MM Detection is listed below : ", "page_idx": 17, "bbox": [72, 113.29548645019531, 440.187744140625, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 129, "type": "text", "text": "4.4 Baselines ", "text_level": 1, "page_idx": 17, "bbox": [71, 150, 168, 164], "page_size": [612.0, 792.0]}
{"layout": 130, "type": "text", "text": "4.4.1 RPN ", "page_idx": 17, "bbox": [72, 181.2563934326172, 129.80343627929688, 198.37623596191406], "page_size": [612.0, 792.0]}
{"layout": 131, "type": "text", "text": "Please refer to  RPN  for details. ", "page_idx": 17, "bbox": [72, 208.56044006347656, 195, 221.87046813964844], "page_size": [612.0, 792.0]}
{"layout": 132, "type": "text", "text": "4.4.2 Faster R-CNN ", "text_level": 1, "page_idx": 17, "bbox": [71, 241, 185, 255], "page_size": [612.0, 792.0]}
{"layout": 133, "type": "text", "text": "Please refer to  Faster R-CNN  for details. ", "page_idx": 17, "bbox": [72, 266.7884216308594, 232, 280.09844970703125], "page_size": [612.0, 792.0]}
{"layout": 134, "type": "text", "text": "4.4.3 Mask R-CNN ", "text_level": 1, "page_idx": 17, "bbox": [71, 299, 179, 313], "page_size": [612.0, 792.0]}
{"layout": 135, "type": "text", "text": "Please refer to  Mask R-CNN  for details. ", "page_idx": 17, "bbox": [72, 325.01641845703125, 232, 338.3264465332031], "page_size": [612.0, 792.0]}
{"layout": 136, "type": "text", "text": "4.4.4 Fast R-CNN (with pre-computed proposals) ", "text_level": 1, "page_idx": 17, "bbox": [71, 357, 352, 372], "page_size": [612.0, 792.0]}
{"layout": 137, "type": "text", "text": "Please refer to  Fast R-CNN  for details. ", "page_idx": 17, "bbox": [72, 383.2444152832031, 224.92591857910156, 396.554443359375], "page_size": [612.0, 792.0]}
{"layout": 138, "type": "text", "text": "4.4.5 RetinaNet ", "text_level": 1, "page_idx": 17, "bbox": [71, 416, 162, 429], "page_size": [612.0, 792.0]}
{"layout": 139, "type": "text", "text": "Please refer to  RetinaNet  for details. ", "page_idx": 17, "bbox": [72, 441.4734191894531, 215.830078125, 454.783447265625], "page_size": [612.0, 792.0]}
{"layout": 140, "type": "text", "text": "4.4.6 Cascade R-CNN and Cascade Mask R-CNN ", "page_idx": 17, "bbox": [72, 472.3963623046875, 348.0575866699219, 489.5162048339844], "page_size": [612.0, 792.0]}
{"layout": 141, "type": "text", "text": "Please refer to  Cascade R-CNN  for details. ", "page_idx": 17, "bbox": [72, 499.701416015625, 242.0117950439453, 513.0114135742188], "page_size": [612.0, 792.0]}
{"layout": 142, "type": "text", "text": "4.4.7 Hybrid Task Cascade (HTC) ", "text_level": 1, "page_idx": 17, "bbox": [70, 532, 262, 546], "page_size": [612.0, 792.0]}
{"layout": 143, "type": "text", "text": "Please refer to  HTC  for details. ", "page_idx": 17, "bbox": [72, 557.9293823242188, 195, 571.2394409179688], "page_size": [612.0, 792.0]}
{"layout": 144, "type": "text", "text": "4.4.8 SSD ", "text_level": 1, "page_idx": 17, "bbox": [71, 591, 132, 604], "page_size": [612.0, 792.0]}
{"layout": 145, "type": "text", "text": "Please refer to  SSD  for details. ", "page_idx": 17, "bbox": [72, 616.1574096679688, 195, 629.4674682617188], "page_size": [612.0, 792.0]}
{"layout": 146, "type": "text", "text": "4.4.9 Group Normalization (GN) ", "page_idx": 18, "bbox": [72, 68.51341247558594, 252, 85.63326263427734], "page_size": [612.0, 792.0]}
{"layout": 147, "type": "text", "text": "Please refer to  Group Normalization  for details. ", "page_idx": 18, "bbox": [72, 95.81745910644531, 261.3292236328125, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 148, "type": "text", "text": "4.4.10 Weight Standardization Please refer to  Weight Standardization  for details. ", "page_idx": 18, "bbox": [72, 126.74137878417969, 269.2893371582031, 167.35545349121094], "page_size": [612.0, 792.0]}
{"layout": 149, "type": "text", "text": "4.4.11 Deformable Convolution v2 ", "page_idx": 18, "bbox": [72, 184.96934509277344, 266.05682373046875, 202.0891876220703], "page_size": [612.0, 792.0]}
{"layout": 150, "type": "text", "text": "Please refer to  Deformable Convolutional Networks  for details. ", "page_idx": 18, "bbox": [72, 212.2733917236328, 322.79840087890625, 225.5834197998047], "page_size": [612.0, 792.0]}
{"layout": 151, "type": "text", "text": "4.4.12 CARAFE: Content-Aware ReAssembly of FEatures Please refer to  CARAFE  for details. ", "page_idx": 18, "bbox": [72, 243.1973114013672, 396.8467712402344, 283.8114013671875], "page_size": [612.0, 792.0]}
{"layout": 152, "type": "text", "text": "4.4.13 Instaboost Please refer to  Instaboost  for details. ", "page_idx": 18, "bbox": [72, 301.42529296875, 216, 342.0393981933594], "page_size": [612.0, 792.0]}
{"layout": 153, "type": "text", "text": "4.4.14 Libra R-CNN ", "page_idx": 18, "bbox": [72, 359.6533203125, 182.5138702392578, 376.7731628417969], "page_size": [612.0, 792.0]}
{"layout": 154, "type": "text", "text": "Please refer to  Libra R-CNN  for details. ", "page_idx": 18, "bbox": [72, 386.9573669433594, 230.39535522460938, 400.26739501953125], "page_size": [612.0, 792.0]}
{"layout": 155, "type": "text", "text": "4.4.15 Guided Anchoring ", "page_idx": 18, "bbox": [72, 417.88128662109375, 216, 435.00115966796875], "page_size": [612.0, 792.0]}
{"layout": 156, "type": "text", "text": "Please refer to  Guided Anchoring  for details. ", "page_idx": 18, "bbox": [72, 445.18536376953125, 252, 458.4953918457031], "page_size": [612.0, 792.0]}
{"layout": 157, "type": "text", "text": "4.4.16 FCOS Please refer to  FCOS  for details. ", "page_idx": 18, "bbox": [72, 476.10931396484375, 200.6470184326172, 516.723388671875], "page_size": [612.0, 792.0]}
{"layout": 158, "type": "text", "text": "4.4.17 FoveaBox Please refer to  FoveaBox  for details. ", "page_idx": 18, "bbox": [72, 534.3372802734375, 216, 574.951416015625], "page_size": [612.0, 792.0]}
{"layout": 159, "type": "text", "text": "4.4.18 RepPoints ", "page_idx": 18, "bbox": [72, 592.5653076171875, 171, 609.6851196289062], "page_size": [612.0, 792.0]}
{"layout": 160, "type": "text", "text": "Please refer to  RepPoints  for details. ", "page_idx": 18, "bbox": [72, 619.8693237304688, 216, 633.1793823242188], "page_size": [612.0, 792.0]}
{"layout": 161, "type": "text", "text": "4.4.19 FreeAnchor Please refer to  FreeAnchor  for details. ", "page_idx": 19, "bbox": [72, 68.51341247558594, 223.2621307373047, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 162, "type": "text", "text": "4.4.20 Grid R-CNN (plus) Please refer to  Grid R-CNN  for details. ", "page_idx": 19, "bbox": [72, 126.74137878417969, 227.2770538330078, 167.35545349121094], "page_size": [612.0, 792.0]}
{"layout": 163, "type": "text", "text": "4.4.21 GHM ", "text_level": 1, "page_idx": 19, "bbox": [71, 187, 141, 200], "page_size": [612.0, 792.0]}
{"layout": 164, "type": "text", "text": "Please refer to  GHM  for details. ", "page_idx": 19, "bbox": [72, 212.2733917236328, 199, 225.5834197998047], "page_size": [612.0, 792.0]}
{"layout": 165, "type": "text", "text": "4.4.22 GCNet Please refer to  GCNet  for details. ", "page_idx": 19, "bbox": [72, 243.1973114013672, 203, 283.8114013671875], "page_size": [612.0, 792.0]}
{"layout": 166, "type": "text", "text": "4.4.23 HRNet Please refer to  HRNet  for details. ", "page_idx": 19, "bbox": [72, 301.42529296875, 203, 342.0393981933594], "page_size": [612.0, 792.0]}
{"layout": 167, "type": "text", "text": "4.4.24 Mask Scoring R-CNN Please refer to  Mask Scoring R-CNN  for details. ", "page_idx": 19, "bbox": [72, 359.6533203125, 264.6368103027344, 400.26739501953125], "page_size": [612.0, 792.0]}
{"layout": 168, "type": "text", "text": "4.4.25 Train from Scratch ", "text_level": 1, "page_idx": 19, "bbox": [71, 420, 219, 433], "page_size": [612.0, 792.0]}
{"layout": 169, "type": "text", "text": "Please refer to  Rethinking ImageNet Pre-training  for details. ", "page_idx": 19, "bbox": [72, 445.18536376953125, 311.3215026855469, 458.4953918457031], "page_size": [612.0, 792.0]}
{"layout": 170, "type": "text", "text": "4.4.26 NAS-FPN Please refer to  NAS-FPN  for details. ", "page_idx": 19, "bbox": [72, 476.10931396484375, 217.0454559326172, 516.723388671875], "page_size": [612.0, 792.0]}
{"layout": 171, "type": "text", "text": "4.4.27 ATSS Please refer to  ATSS  for details. ", "page_idx": 19, "bbox": [72, 534.3372802734375, 199, 574.951416015625], "page_size": [612.0, 792.0]}
{"layout": 172, "type": "text", "text": "4.4.28 FSAF ", "text_level": 1, "page_idx": 19, "bbox": [71, 594, 144, 608], "page_size": [612.0, 792.0]}
{"layout": 173, "type": "text", "text": "Please refer to  FSAF  for details. ", "page_idx": 19, "bbox": [72, 619.8693237304688, 199, 633.1793823242188], "page_size": [612.0, 792.0]}
{"layout": 174, "type": "text", "text": "4.4.29 RegNetX Please refer to  RegNet  for details. ", "page_idx": 20, "bbox": [72, 68.51341247558594, 205, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 175, "type": "text", "text": "4.4.30 Res2Net ", "page_idx": 20, "bbox": [72, 126.74137878417969, 161, 143.86122131347656], "page_size": [612.0, 792.0]}
{"layout": 176, "type": "text", "text": "Please refer to  Res2Net  for details. ", "page_idx": 20, "bbox": [72, 154.04542541503906, 209.74288940429688, 167.35545349121094], "page_size": [612.0, 792.0]}
{"layout": 177, "type": "text", "text": "4.4.31 GRoIE ", "text_level": 1, "page_idx": 20, "bbox": [71, 187, 149, 200], "page_size": [612.0, 792.0]}
{"layout": 178, "type": "text", "text": "Please refer to  GRoIE  for details. ", "page_idx": 20, "bbox": [72, 212.2733917236328, 205, 225.5834197998047], "page_size": [612.0, 792.0]}
{"layout": 179, "type": "text", "text": "4.4.32 Dynamic R-CNN Please refer to  Dynamic R-CNN  for details. ", "page_idx": 20, "bbox": [72, 243.1973114013672, 245, 283.8114013671875], "page_size": [612.0, 792.0]}
{"layout": 180, "type": "text", "text": "4.4.33 PointRend Please refer to  PointRend  for details. ", "page_idx": 20, "bbox": [72, 301.42529296875, 218, 342.0393981933594], "page_size": [612.0, 792.0]}
{"layout": 181, "type": "text", "text": "4.4.34 DetectoRS ", "page_idx": 20, "bbox": [72, 359.6533203125, 171, 376.7731628417969], "page_size": [612.0, 792.0]}
{"layout": 182, "type": "text", "text": "Please refer to  DetectoRS  for details. ", "page_idx": 20, "bbox": [72, 386.9573669433594, 218, 400.26739501953125], "page_size": [612.0, 792.0]}
{"layout": 183, "type": "text", "text": "4.4.35 Generalized Focal Loss ", "page_idx": 20, "bbox": [72, 417.88128662109375, 245, 435.00115966796875], "page_size": [612.0, 792.0]}
{"layout": 184, "type": "text", "text": "Please refer to  Generalized Focal Loss  for details. ", "page_idx": 20, "bbox": [72, 445.18536376953125, 269.69781494140625, 458.4953918457031], "page_size": [612.0, 792.0]}
{"layout": 185, "type": "text", "text": "4.4.36 CornerNetPlease refer to  CornerNet  for details. ", "page_idx": 20, "bbox": [72, 476.10931396484375, 218, 516.723388671875], "page_size": [612.0, 792.0]}
{"layout": 186, "type": "text", "text": "4.4.37 YOLOv3 Please refer to  YOLOv3  for details. ", "page_idx": 20, "bbox": [72, 534.3372802734375, 212.7615203857422, 574.951416015625], "page_size": [612.0, 792.0]}
{"layout": 187, "type": "text", "text": "4.4.38 PAA ", "page_idx": 20, "bbox": [72, 592.5653076171875, 135.6734161376953, 609.6851196289062], "page_size": [612.0, 792.0]}
{"layout": 188, "type": "text", "text": "Please refer to  PAA  for details. ", "page_idx": 20, "bbox": [72, 619.8693237304688, 194.85874938964844, 633.1793823242188], "page_size": [612.0, 792.0]}
{"layout": 189, "type": "text", "text": "4.4.39 SABL ", "text_level": 1, "page_idx": 21, "bbox": [71, 71, 144, 84], "page_size": [612.0, 792.0]}
{"layout": 190, "type": "text", "text": "Please refer to  SABL  for details. ", "page_idx": 21, "bbox": [72, 95.81745910644531, 201, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 191, "type": "text", "text": "4.4.40 Centripetal Net Please refer to  Centripetal Net  for details. ", "page_idx": 21, "bbox": [72, 126.74137878417969, 234.53982543945312, 167.35545349121094], "page_size": [612.0, 792.0]}
{"layout": 192, "type": "text", "text": "4.4.41 ResNeSt ", "text_level": 1, "page_idx": 21, "bbox": [71, 187, 163, 200], "page_size": [612.0, 792.0]}
{"layout": 193, "type": "text", "text": "Please refer to  ResNeSt  for details. ", "page_idx": 21, "bbox": [72, 212.2733917236328, 212, 225.5834197998047], "page_size": [612.0, 792.0]}
{"layout": 194, "type": "text", "text": "4.4.42 DETR ", "page_idx": 21, "bbox": [72, 243.1973114013672, 143.755126953125, 260.3171691894531], "page_size": [612.0, 792.0]}
{"layout": 195, "type": "text", "text": "Please refer to  DETR  for details. ", "page_idx": 21, "bbox": [72, 270.5013732910156, 201, 283.8114013671875], "page_size": [612.0, 792.0]}
{"layout": 196, "type": "text", "text": "4.4.43 Deformable DETR Please refer to  Deformable DETR  for details. ", "page_idx": 21, "bbox": [72, 301.42529296875, 251.71531677246094, 342.0393981933594], "page_size": [612.0, 792.0]}
{"layout": 197, "type": "text", "text": "4.4.44 AutoAssign Please refer to  AutoAssign  for details. ", "page_idx": 21, "bbox": [72, 359.6533203125, 223.1326141357422, 400.26739501953125], "page_size": [612.0, 792.0]}
{"layout": 198, "type": "text", "text": "4.4.45 YOLOF Please refer to  YOLOF  for details. ", "page_idx": 21, "bbox": [72, 417.88128662109375, 212, 458.4953918457031], "page_size": [612.0, 792.0]}
{"layout": 199, "type": "text", "text": "4.4.46 Seesaw Loss Please refer to  Seesaw Loss  for details. ", "page_idx": 21, "bbox": [72, 476.10931396484375, 226.71917724609375, 516.723388671875], "page_size": [612.0, 792.0]}
{"layout": 200, "type": "text", "text": "4.4.47 CenterNet Please refer to  CenterNet  for details. ", "page_idx": 21, "bbox": [72, 534.3372802734375, 216.62705993652344, 574.951416015625], "page_size": [612.0, 792.0]}
{"layout": 201, "type": "text", "text": "4.4.48 YOLOX ", "text_level": 1, "page_idx": 21, "bbox": [71, 594, 153, 607], "page_size": [612.0, 792.0]}
{"layout": 202, "type": "text", "text": "Please refer to  YOLOX  for details. ", "page_idx": 21, "bbox": [72, 619.8693237304688, 212, 633.1793823242188], "page_size": [612.0, 792.0]}
{"layout": 203, "type": "text", "text": "4.4.49 PVT ", "text_level": 1, "page_idx": 22, "bbox": [71, 70, 136, 84], "page_size": [612.0, 792.0]}
{"layout": 204, "type": "text", "text": "Please refer to  PVT  for details. ", "page_idx": 22, "bbox": [71, 95.81745910644531, 194.6495361328125, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 205, "type": "text", "text": "4.4.50 SOLO ", "text_level": 1, "page_idx": 22, "bbox": [71, 128, 147, 142], "page_size": [612.0, 792.0]}
{"layout": 206, "type": "text", "text": "Please refer to  SOLO  for details. ", "page_idx": 22, "bbox": [71, 154.04542541503906, 201.842529296875, 167.35545349121094], "page_size": [612.0, 792.0]}
{"layout": 207, "type": "text", "text": "4.4.51 QueryInst ", "text_level": 1, "page_idx": 22, "bbox": [71, 187, 170, 201], "page_size": [612.0, 792.0]}
{"layout": 208, "type": "text", "text": "Please refer to  QueryInst  for details. ", "page_idx": 22, "bbox": [71, 212.2733917236328, 215.72044372558594, 225.5834197998047], "page_size": [612.0, 792.0]}
{"layout": 209, "type": "text", "text": "4.4.52 Other datasets ", "text_level": 1, "page_idx": 22, "bbox": [71, 245, 197, 259], "page_size": [612.0, 792.0]}
{"layout": 210, "type": "text", "text": "We also benchmark some methods on  PASCAL VOC ,  Cityscapes  and  WIDER FACE . ", "page_idx": 22, "bbox": [71, 270.5013732910156, 414.03594970703125, 283.8114013671875], "page_size": [612.0, 792.0]}
{"layout": 211, "type": "text", "text": "4.4.53 Pre-trained Models ", "text_level": 1, "page_idx": 22, "bbox": [71, 303, 221, 317], "page_size": [612.0, 792.0]}
{"layout": 212, "type": "text", "text": "We also train  Faster R-CNN  and  Mask R-CNN  using ResNet-50 and  RegNetX-3.2G  with multi-scale training and longer schedules. These models serve as strong pre-trained models for downstream tasks for convenience. ", "page_idx": 22, "bbox": [71, 328.7293701171875, 540, 353.994384765625], "page_size": [612.0, 792.0]}
{"layout": 213, "type": "text", "text": "4.5 Speed benchmark ", "text_level": 1, "page_idx": 22, "bbox": [71, 376, 224, 393], "page_size": [612.0, 792.0]}
{"layout": 214, "type": "text", "text": "4.5.1 Training Speed benchmark ", "text_level": 1, "page_idx": 22, "bbox": [71, 410, 258, 424], "page_size": [612.0, 792.0]}
{"layout": 215, "type": "text", "text": "We provide  analyze logs.py  to get average time of iteration in training. You can find examples in  Log Analysis . ", "page_idx": 22, "bbox": [71, 435.9503173828125, 515.7938842773438, 449.2603454589844], "page_size": [612.0, 792.0]}
{"layout": 216, "type": "text", "text": "We compare the training speed of Mask R-CNN with some other popular frameworks (The data is copied from  detec- tron2 ). For mm detection, we benchmark with  mask r cnn r 50 caff e fp n poly 1 x coco v 1.py , which should have the same setting with  mask r cnn R 50 FP N no aug 1 x.yaml  of detectron2. We also provide the  checkpoint  and  training log  for reference. The throughput is computed as the average throughput in iterations 100-500 to skip GPU warmup time. ", "page_idx": 22, "bbox": [71, 453.8833312988281, 540, 515.0133056640625], "page_size": [612.0, 792.0]}
{"layout": 217, "type": "text", "text": "4.5.2 Inference Speed Benchmark ", "text_level": 1, "page_idx": 22, "bbox": [71, 534, 266, 548], "page_size": [612.0, 792.0]}
{"layout": 218, "type": "text", "text": "We provide  benchmark.py  to benchmark the inference latency. The script benchmark es the model with 2000 images and calculates the average time ignoring first 5 times. You can change the output log interval (defaults: 50) by setting LOG-INTERVAL . ", "page_idx": 22, "bbox": [71, 559.9312744140625, 540, 597.1522827148438], "page_size": [612.0, 792.0]}
{"layout": 219, "type": "text", "text": "python toools/benchmark.py  \\${ CONFIG } \\${ CHECKPOINT }  [ --log-interval \\$ [ LOG-INTERVAL ]] [ --\n\n  $\\hookrightarrow$  fuse-conv-bn ]\n\n → ", "page_idx": 22, "bbox": [71, 606.8857421875, 527.739501953125, 631.7323608398438], "page_size": [612.0, 792.0]}
{"layout": 220, "type": "text", "text": "The latency of all models in our model zoo is benchmarked without setting  fuse-conv-bn , you can get a lower latency by setting it. ", "page_idx": 22, "bbox": [71, 641.4264526367188, 540, 666.6914672851562], "page_size": [612.0, 792.0]}
{"layout": 221, "type": "text", "text": "4.6 Comparison with Detectron2 ", "text_level": 1, "page_idx": 23, "bbox": [70, 71, 296, 88], "page_size": [612.0, 792.0]}
{"layout": 222, "type": "text", "text": "We compare mm detection with  Detectron2  in terms of speed and performance. We use the commit id 185c27e (30/4/2020) of detectron. For fair comparison, we install and run both frameworks on the same machine. ", "page_idx": 23, "bbox": [72, 102.99446105957031, 540, 128.25950622558594], "page_size": [612.0, 792.0]}
{"layout": 223, "type": "text", "text": "4.6.1 Hardware ", "text_level": 1, "page_idx": 23, "bbox": [71, 148, 161, 162], "page_size": [612.0, 792.0]}
{"layout": 224, "type": "text", "text": "• 8 NVIDIA Tesla V100 (32G) GPUs • Intel(R) Xeon(R) Gold 6148 CPU   $@$   2.40GHz ", "page_idx": 23, "bbox": [88, 173.1774444580078, 282.59979248046875, 204.4204559326172], "page_size": [612.0, 792.0]}
{"layout": 225, "type": "text", "text": "4.6.2 Software environment ", "text_level": 1, "page_idx": 23, "bbox": [71, 224, 232, 238], "page_size": [612.0, 792.0]}
{"layout": 226, "type": "text", "text": "• Python 3.7 • PyTorch 1.4 • CUDA 10.1 • CUDNN 7.6.03 • NCCL 2.4.08 ", "page_idx": 23, "bbox": [88, 249.3384552001953, 159.72117614746094, 334.3795166015625], "page_size": [612.0, 792.0]}
{"layout": 227, "type": "text", "text": "4.6.3 Performance ", "text_level": 1, "page_idx": 23, "bbox": [71, 353, 179, 367], "page_size": [612.0, 792.0]}
{"layout": 228, "type": "text", "text": "4.6.4 Training Speed ", "text_level": 1, "page_idx": 23, "bbox": [71, 379, 192, 394], "page_size": [612.0, 792.0]}
{"layout": 229, "type": "text", "text": "The training speed is measure with s/iter. The lower, the better. ", "page_idx": 23, "bbox": [72, 405.6554870605469, 322.84832763671875, 418.96551513671875], "page_size": [612.0, 792.0]}
{"layout": 230, "type": "text", "text": "4.6.5 Inference Speed ", "text_level": 1, "page_idx": 23, "bbox": [71, 438, 198, 452], "page_size": [612.0, 792.0]}
{"layout": 231, "type": "text", "text": "The inference speed is measured with fps   $\\mathrm{(img/s)}$   on a single GPU, the higher, the better. To be consistent with Detec- tron2, we report the pure inference speed (without the time of data loading). For Mask R-CNN, we exclude the time of RLE encoding in post-processing. We also include the officially reported speed in the parentheses, which is slightly higher than the results tested on our server due to differences of hardwares. ", "page_idx": 23, "bbox": [72, 463.88348388671875, 540, 513.0584716796875], "page_size": [612.0, 792.0]}
{"layout": 232, "type": "text", "text": "4.6.6 Training memory ", "text_level": 1, "page_idx": 23, "bbox": [71, 532, 202, 546], "page_size": [612.0, 792.0]}
{"layout": 233, "type": "text", "text": "1: INFERENCE AND TRAIN WITH EXISTING MODELS AND STANDARD DATASETS ", "text_level": 1, "page_idx": 24, "bbox": [143, 161, 544, 201], "page_size": [612.0, 792.0]}
{"layout": 234, "type": "text", "text": "MM Detection provides hundreds of existing and existing detection models in  Model Zoo ), and supports multiple stan- dard datasets, including Pascal VOC, COCO, CityScapes, LVIS, etc. This note will show how to perform common tasks on these existing models and standard datasets, including: ", "page_idx": 24, "bbox": [71, 243.80543518066406, 540, 281.0264892578125], "page_size": [612.0, 792.0]}
{"layout": 235, "type": "text", "text": "• Use existing models to inference on given images. • Test existing models on standard datasets. • Train predefined models on standard datasets. ", "page_idx": 24, "bbox": [88, 285.6484680175781, 297, 334.82452392578125], "page_size": [612.0, 792.0]}
{"layout": 236, "type": "text", "text": "5.1 Inference with existing models ", "text_level": 1, "page_idx": 24, "bbox": [70, 357, 309, 373], "page_size": [612.0, 792.0]}
{"layout": 237, "type": "text", "text": "By inference, we mean using trained models to detect objects on images. In MM Detection, a model is defined by a configuration file and existing model parameters are save in a checkpoint file. ", "page_idx": 24, "bbox": [71, 389.386474609375, 540, 414.6514892578125], "page_size": [612.0, 792.0]}
{"layout": 238, "type": "text", "text": "To start with, we recommend  Faster RCNN  with this  configuration file  and this  checkpoint file . It is recommended to download the checkpoint file to  checkpoints  directory. ", "page_idx": 24, "bbox": [71, 419.2734680175781, 540, 444.53948974609375], "page_size": [612.0, 792.0]}
{"layout": 239, "type": "text", "text": "5.1.1 High-level APIs for inference ", "page_idx": 24, "bbox": [71, 462.15240478515625, 266.798095703125, 479.2722473144531], "page_size": [612.0, 792.0]}
{"layout": 240, "type": "text", "text": "MM Detection provide high-level Python APIs for inference on images. Here is an example of building the model and inference on given images or videos. ", "page_idx": 24, "bbox": [71, 489.45745849609375, 540, 514.7224731445312], "page_size": [612.0, 792.0]}
{"layout": 241, "type": "text", "text": "from  mmdet.apis  import  in it detector, inference detector import  mmcv # Specify the path to model config and checkpoint file config file  $=$   ' configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py ' checkpoint file  $=$   ' checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth ' # build the model from a config file and a checkpoint file model  $=$   in it detector(config file, checkpoint file, device  $\\equiv^{\\dagger}$  cuda:0 ' ) # test a single image and show the results img  $=$  ' test.jpg ' # or img  $=$   mmcv.imread(img), which will only load it once result  $=$   inference detector(model, img) # visualize the results in a new window model . show result(img, result) ", "page_idx": 24, "bbox": [71, 523.8580322265625, 495.6603698730469, 701.8411865234375], "page_size": [612.0, 792.0]}
{"layout": 242, "type": "text", "text": "# or save the visualization results to image files model . show result(img, result, out_file  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}^{\\dagger}$  ' result.jpg ' ) # test a video and show the results video  $=$   mmcv . Video Reader( ' video.mp4 ' ) for  frame  in  video: result  $=$   inference detector(model, frame) model . show result(frame, result, wait_time  $\\mathbf{\\omega}=\\!1.$  ) ", "page_idx": 25, "bbox": [72, 87.67274475097656, 349.20941162109375, 181.37216186523438], "page_size": [612.0, 792.0]}
{"layout": 243, "type": "text", "text": "A notebook demo can be found in  demo/inference demo.ipynb . Note:  inference detector  only supports single-image inference for now. ", "page_idx": 25, "bbox": [72, 193.94447326660156, 375.5709228515625, 225.18748474121094], "page_size": [612.0, 792.0]}
{"layout": 244, "type": "text", "text": "5.1.2 Asynchronous interface - supported for Python  $\\mathbf{3.7+}$  ", "text_level": 1, "page_idx": 25, "bbox": [71, 246, 400, 259], "page_size": [612.0, 792.0]}
{"layout": 245, "type": "text", "text": "For Python   $3.7+$  , MM Detection also supports async interfaces. By utilizing CUDA streams, it allows not to block CPU on GPU bound inference code and enables better CPU/GPU utilization for single-threaded application. Inference can be done concurrently either between different input data samples or between different models of some inference pipeline. ", "page_idx": 25, "bbox": [72, 270.1054382324219, 540, 319.28143310546875], "page_size": [612.0, 792.0]}
{"layout": 246, "type": "text", "text": "See  tests/a sync benchmark.py  to compare the speed of synchronous and asynchronous interfaces. ", "page_idx": 25, "bbox": [72, 323.9034118652344, 482.5091552734375, 337.21343994140625], "page_size": [612.0, 792.0]}
{"layout": 247, "type": "image", "page_idx": 25, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_3.jpg", "bbox": [67, 341, 545, 707.75], "page_size": [612.0, 792.0], "ocr_text": "import asyncio\n\nimport torch\n\nfrom mmdet.apis import init_detector, async_inference_detector\nfrom mmdet.utils.contextmanagers import concurrent\n\nasync def main():\n\nconfig_file = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\ndevice = 'cuda:0'\n\nmodel = init_detector(config_file, checkpoint=checkpoint_file, device=device)\n\n# queue is used for concurrent inference of multiple images\nstreamqueue = asyncio.Queue()\n\n# queue size defines concurrency level\n\nstreamqueue_size = 3\n\nfor _ in range(streamqueue_size):\nstreamqueue.put_nowait(torch.cuda.Stream(device=device))\n\n# test a single image and show the results\nimg = 'test.jpg' # or img = mmcv.imread(img), which will only load it once\n\nasync with concurrent (streamqueue) :\nresult = await async_inference_detector(model, img)\n\n# visualize the results in a new window\nmodel.show_result(img, result)\n\n# or save the visualization results to image files\nmodel.show_result(img, result, out_file='result.jpg')\n\n", "vlm_text": "This image depicts a Python script that uses libraries like `asyncio`, `torch`, and MMDetection to perform asynchronous inference with a Faster R-CNN model. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `asyncio` for handling asynchronous operations.\n   - `torch` presumably for PyTorch functionalities.\n   - Functions from `mmdet` for initializing the detector and performing async inference.\n\n2. **Function `main`**:\n   - Defines config and checkpoint files for the Faster R-CNN model.\n   - Initializes the model using `init_detector`.\n\n3. **Asynchronous Operations**:\n   - Uses `asyncio.Queue` to manage concurrency.\n   - The queue size is set to 3 for concurrent processing.\n\n4. **Loading and Processing**:\n   - Adds streams to the queue for GPU processing.\n   - Loads an image `test.jpg` for testing.\n\n5. **Inference and Result Display**:\n   - Asynchronously runs inference using `async_inference_detector`.\n   - Visualizes the results using `model.show_result`, either in a window or saved to `result.jpg`."}
{"layout": 248, "type": "text", "text": "(continues on next page) ", "page_idx": 25, "bbox": [461.9010009765625, 705.78955078125, 540, 716.4376220703125], "page_size": [612.0, 792.0]}
{"layout": 249, "type": "table", "page_idx": 26, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_4.jpg", "bbox": [67, 82, 542, 113], "page_size": [612.0, 792.0], "ocr_text": "asyncio.run(main())\n", "vlm_text": "The table contains the line of Python code: `asyncio.run(main())`."}
{"layout": 250, "type": "text", "text": "5.1.3 Demos ", "text_level": 1, "page_idx": 26, "bbox": [71, 137, 146, 151], "page_size": [612.0, 792.0]}
{"layout": 251, "type": "text", "text": "We also provide three demo scripts, implemented with high-level APIs and supporting functionality codes. Source codes are available  here . ", "page_idx": 26, "bbox": [72, 162.5094757080078, 540.0029296875, 187.77452087402344], "page_size": [612.0, 792.0]}
{"layout": 252, "type": "text", "text": "Image demo ", "text_level": 1, "page_idx": 26, "bbox": [71, 208, 131, 219], "page_size": [612.0, 792.0]}
{"layout": 253, "type": "text", "text": "This script performs inference on a single image. ", "page_idx": 26, "bbox": [72, 230.69947814941406, 266.938232421875, 244.00950622558594], "page_size": [612.0, 792.0]}
{"layout": 254, "type": "table", "page_idx": 26, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_5.jpg", "bbox": [67, 250, 543, 330], "page_size": [612.0, 792.0], "ocr_text": "python demo/image_demo.py \\\n${IMAGE_FILE} \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n[--device ${GPU_ID}] \\\n[--score-thr ${SCORE_THR}]\n\n", "vlm_text": "The table contains a command line script for running a Python demo. Here’s the breakdown:\n\n```\npython demo/image_demo.py \\\n    ${IMAGE_FILE} \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--device ${GPU_ID}] \\\n    [--score-thr ${SCORE_THR}]\n```\n\n- `${IMAGE_FILE}`: Placeholder for the path to the image file.\n- `${CONFIG_FILE}`: Placeholder for the path to the configuration file.\n- `${CHECKPOINT_FILE}`: Placeholder for the checkpoint file.\n- `[--device ${GPU_ID}]`: Optional argument to specify the GPU ID.\n- `[--score-thr ${SCORE_THR}]`: Optional argument for setting the score threshold."}
{"layout": 255, "type": "text", "text": "Examples: ", "page_idx": 26, "bbox": [72, 336.1044616699219, 113, 349.41448974609375], "page_size": [612.0, 792.0]}
{"layout": 256, "type": "text", "text": "python demo/image_demo.py demo/demo.jpg  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --device cpu ", "page_idx": 26, "bbox": [72, 358.54998779296875, 422.4343566894531, 405.026123046875], "page_size": [612.0, 792.0]}
{"layout": 257, "type": "text", "text": "Webcam demo ", "text_level": 1, "page_idx": 26, "bbox": [71, 432, 143, 444], "page_size": [612.0, 792.0]}
{"layout": 258, "type": "text", "text": "This is a live demo from a webcam. ", "page_idx": 26, "bbox": [72, 455.9014587402344, 214.1264190673828, 469.21148681640625], "page_size": [612.0, 792.0]}
{"layout": 259, "type": "table", "page_idx": 26, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_6.jpg", "bbox": [67, 474, 544, 555], "page_size": [612.0, 792.0], "ocr_text": "python demo/webcam_demo.py \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n[--device ${GPU_ID}] \\\n[--camera-id ${CAMERA-ID}] \\\n[--score-thr ${SCORE_THR}]\n\n", "vlm_text": "The image shows a code snippet for running a Python script called `webcam_demo.py`, which appears to be part of a demonstration or application that uses a webcam. The script is run with the following command-line arguments:\n\n1. `${CONFIG_FILE}` - This placeholder is meant to be replaced with the path to the configuration file needed by the script.\n\n2. `${CHECKPOINT_FILE}` - This placeholder is meant to be replaced with the path to the checkpoint file, which is likely a model file needed for the script to execute.\n\nOptional arguments include:\n\n- `--device ${GPU_ID}` - This specifies the device to be used, with `${GPU_ID}` being an optional placeholder for the GPU ID if the script should run on a specific GPU.\n\n- `--camera-id ${CAMERA-ID}` - This specifies the camera ID, which likely selects which webcam to use if more than one is available.\n\n- `--score-thr ${SCORE_THR}` - This specifies a score threshold for the operation, with `${SCORE_THR}` being a placeholder for the value.\n\nThe placeholders (denoted by `${...}`) need to be replaced with actual values when executing the command."}
{"layout": 260, "type": "text", "text": "Examples: ", "page_idx": 26, "bbox": [72, 561.3064575195312, 113, 574.6165161132812], "page_size": [612.0, 792.0]}
{"layout": 261, "type": "text", "text": "python demo/webcam demo.py  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth ", "page_idx": 26, "bbox": [72, 583.7520141601562, 411.9735107421875, 618.2721557617188], "page_size": [612.0, 792.0]}
{"layout": 262, "type": "text", "text": "Video demo ", "text_level": 1, "page_idx": 27, "bbox": [72, 73, 129, 84], "page_size": [612.0, 792.0]}
{"layout": 263, "type": "text", "text": "This script performs inference on a video. ", "page_idx": 27, "bbox": [72, 95.81745910644531, 239, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 264, "type": "text", "text": "python demo/video_demo.py  \\ \\${ VIDEO_FILE }  \\ \\${ CONFIG FILE }  \\ \\${ CHECKPOINT FILE }  \\ [ --device  \\${ GPU_ID } ]  \\ [ --score-thr  \\${ SCORE_THR } ]  \\ [ --out  \\${ OUT_FILE } ]  \\ [ --show ]  \\ [ --wait-time  \\${ WAIT_TIME } ] ", "page_idx": 27, "bbox": [72, 117.5979995727539, 239, 223.85025024414062], "page_size": [612.0, 792.0]}
{"layout": 265, "type": "text", "text": "Examples: ", "page_idx": 27, "bbox": [72, 236.4224395751953, 113.96246337890625, 249.7324676513672], "page_size": [612.0, 792.0]}
{"layout": 266, "type": "text", "text": "python demo/video_demo.py demo/demo.mp4  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --out result.mp4 ", "page_idx": 27, "bbox": [72, 258.2040100097656, 422.4343566894531, 304.6791687011719], "page_size": [612.0, 792.0]}
{"layout": 267, "type": "text", "text": "5.2 Test existing models on standard datasets ", "text_level": 1, "page_idx": 27, "bbox": [71, 335, 389, 351], "page_size": [612.0, 792.0]}
{"layout": 268, "type": "text", "text": "To evaluate a model’s accuracy, one usually tests the model on some standard datasets. MM Detection supports multiple public datasets including COCO, Pascal VOC, CityScapes, and  more . This section will show how to test existing models on supported datasets. ", "page_idx": 27, "bbox": [72, 366.61846923828125, 540, 403.8394775390625], "page_size": [612.0, 792.0]}
{"layout": 269, "type": "text", "text": "5.2.1 Prepare datasets ", "text_level": 1, "page_idx": 27, "bbox": [72, 423, 202, 436], "page_size": [612.0, 792.0]}
{"layout": 270, "type": "text", "text": "Public datasets like  Pascal VOC  or mirror and  COCO  are available from official websites or mirrors. Note: In the detection task, Pascal VOC 2012 is an extension of Pascal VOC 2007 without overlap, and we usually use them together. It is recommended to download and extract the dataset somewhere outside the project directory and symlink the dataset root to  \\$MM DETECTION/data  as below. If your folder structure is different, you may need to change the corresponding paths in config files. ", "page_idx": 27, "bbox": [72, 448.1844482421875, 540, 509.3154296875], "page_size": [612.0, 792.0]}
{"layout": 271, "type": "table", "page_idx": 27, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_7.jpg", "bbox": [67, 514, 543, 712.75], "page_size": [612.0, 792.0], "ocr_text": "mmdetection\n\n— mmdet\nL— tools\nL— configs\nL— data\nIK— coco\n— annotations\nK— train2017\n+ — val2017\n-— test2017\n— cityscapes\nt— annotations\nt— leftImg8bit\nL— train\nt— val\nt- gtFine\n\n", "vlm_text": "The image displays a directory structure, likely related to a machine learning or computer vision project using the MMDetection framework. Here’s a breakdown of the directory structure:\n\n- **mmdetection**: The root directory of the project.\n  - **mmdet**: Presumably a directory for core MMDetection functionalities or library code.\n  - **tools**: Likely contains scripts or tools related to running experiments or processing data.\n  - **configs**: Probably holds configuration files for various models and experiments.\n  - **data**: A directory designated for datasets.\n    - **coco**: A sub-directory for the COCO dataset.\n      - **annotations**: Contains annotation files for COCO data.\n      - **train2017**: Training data from the 2017 COCO dataset.\n      - **val2017**: Validation data from the 2017 COCO dataset.\n      - **test2017**: Test data from the 2017 COCO dataset.\n    - **cityscapes**: A sub-directory for the Cityscapes dataset.\n      - **annotations**: Contains annotation files for Cityscapes data.\n      - **leftImg8bit**: A folder likely containing Cityscapes images processed in 8-bit format.\n        - **train**: Training images for Cityscapes.\n        - **val**: Validation images for Cityscapes.\n      - **gtFine**: Likely contains the finely annotated ground truth data for Cityscapes.\n\nThis hierarchical structure organizes the project into folders for core modules, tools, configurations, and datasets, specifically COCO and Cityscapes."}
{"layout": 272, "type": "text", "text": "(continues on next page) ", "page_idx": 27, "bbox": [461.9010009765625, 709.8545532226562, 540, 720.5026245117188], "page_size": [612.0, 792.0]}
{"layout": 273, "type": "image", "page_idx": 28, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_8.jpg", "bbox": [69, 83, 542, 150], "page_size": [612.0, 792.0], "ocr_text": "t— train\nt— val\nvoCdevkit\n\nt— VOC2007\nt— VOC2012\n\n", "vlm_text": "The image shows a directory structure, likely related to a dataset. There is a main directory named `VOCdevkit`, which contains two subdirectories named `VOC2007` and `VOC2012`. Additionally, there are two folders `train` and `val` which are possibly subdirectories under each of `VOC2007` and `VOC2012`. This structure suggests it might be related to a dataset used for training and validation purposes, possibly the PASCAL VOC dataset."}
{"layout": 274, "type": "text", "text": "Some models require additional  COCO-stuff  datasets, such as HTC, DetectoRS and SCNet, you can download and unzip then move to the coco folder. The directory should be like this. ", "page_idx": 28, "bbox": [72, 158.07948303222656, 540, 183.3445281982422], "page_size": [612.0, 792.0]}
{"layout": 275, "type": "image", "page_idx": 28, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_9.jpg", "bbox": [67, 188, 543, 292], "page_size": [612.0, 792.0], "ocr_text": "mmdetection\ndata\n\ncoco\nt— annotations\nt— train2017\nt— val2017\nt— test2017\nL— stuffthingmaps\n\n", "vlm_text": "The image shows a directory structure related to a machine learning project using the COCO dataset. It looks like this:\n\n```\nmmdetection\n│\n└─── data\n     └─── coco\n          ├─── annotations\n          ├─── train2017\n          ├─── val2017\n          ├─── test2017\n          └─── stuffthingmaps\n```\n\nThis structure is typically used in computer vision tasks for object detection or segmentation, containing annotations and splits for training, validation, and testing."}
{"layout": 276, "type": "text", "text": "Panoptic segmentation models like Pan optic FP N require additional  COCO Panoptic  datasets, you can download and unzip then move to the coco annotation folder. The directory should be like this. ", "page_idx": 28, "bbox": [72, 299.3494567871094, 540, 324.6144714355469], "page_size": [612.0, 792.0]}
{"layout": 277, "type": "image", "page_idx": 28, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_10.jpg", "bbox": [67, 330, 543, 469], "page_size": [612.0, 792.0], "ocr_text": "mmdetection\n\nL— data\n\nL— coco\n\nt— annotations\n\nt— panoptic_train2017.json\nt— panoptic_train2017\nt— panoptic_val2017.json\nt— panoptic_val2017\n\n— train2017\n\nt— val2017\n\nt— test2017\n\n", "vlm_text": "The image shows a directory structure for a dataset setup in MMDetection. It includes the following folders and files:\n\n- `mmdetection`\n  - `data`\n    - `coco`\n      - `annotations`\n        - `panoptic_train2017.json`\n        - `panoptic_train2017`\n        - `panoptic_val2017.json`\n        - `panoptic_val2017`\n      - `train2017`\n      - `val2017`\n      - `test2017`"}
{"layout": 278, "type": "text", "text": "The  cityscapes  annotations need to be converted into the coco format using  tools/data set converters/ cityscapes.py : ", "page_idx": 28, "bbox": [72, 476.4854736328125, 540, 501.75048828125], "page_size": [612.0, 792.0]}
{"layout": 279, "type": "text", "text": "pip install cityscape s scripts python tools/data set converters/cityscapes.py  \\ ./data/cityscapes  \\ --nproc  8  \\ --out-dir ./data/cityscapes/annotations ", "page_idx": 28, "bbox": [72, 511.48370361328125, 317.8273620605469, 581.2721557617188], "page_size": [612.0, 792.0]}
{"layout": 280, "type": "text", "text": "TODO: CHANGE TO THE NEW PATH ", "page_idx": 28, "bbox": [72, 593.845458984375, 236.0939178466797, 607.155517578125], "page_size": [612.0, 792.0]}
{"layout": 281, "type": "text", "text": "5.2.2 Test existing models ", "text_level": 1, "page_idx": 29, "bbox": [71, 71, 223, 84], "page_size": [612.0, 792.0]}
{"layout": 282, "type": "text", "text": "We provide testing scripts for evaluating an existing model on the whole dataset (COCO, PASCAL VOC, Cityscapes, etc.). The following testing environments are supported: ", "page_idx": 29, "bbox": [71, 95.81745910644531, 540, 121.08251190185547], "page_size": [612.0, 792.0]}
{"layout": 283, "type": "text", "text": "• single GPU • single node multiple GPUs • multiple nodes ", "page_idx": 29, "bbox": [88, 125.70545959472656, 204.56283569335938, 174.88047790527344], "page_size": [612.0, 792.0]}
{"layout": 284, "type": "text", "text": "Choose the proper script to perform testing depending on the testing environment. ", "page_idx": 29, "bbox": [71, 179.50343322753906, 398.6735534667969, 192.81346130371094], "page_size": [612.0, 792.0]}
{"layout": 285, "type": "image", "page_idx": 29, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_11.jpg", "bbox": [68, 198, 543, 385], "page_size": [612.0, 792.0], "ocr_text": "# single-gpu testing\n\npython tools/test.py \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n[--out ${RESULT_FILE}] \\\n[--eval ${EVAL_METRICS}] \\\n[--show]\n\n# multi-gpu testing\n\nbash tools/dist_test.sh \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n${GPU_NUM} \\\n[--out ${RESULT_FILE}] \\\n[--eval ${EVAL_METRICS}]\n\n", "vlm_text": "The image contains text that appears to be a script or command line instructions for testing some software or model on a computer system. It includes commands for both single-GPU and multi-GPU testing.\n\n1. **Single-GPU Testing**:\n    - The command used is a Python script: `python tools/test.py`\n    - It takes several arguments in the form of placeholders:\n        - `${CONFIG_FILE}`: The configuration file required for testing.\n        - `${CHECKPOINT_FILE}`: The checkpoint file of the model.\n        - `[--out ${RESULT_FILE}]`: An optional argument to specify the output result file.\n        - `[--eval ${EVAL_METRICS}]`: An optional argument for evaluation metrics.\n        - `[--show]`: An optional flag to display additional information.\n\n2. **Multi-GPU Testing**:\n    - The command used is a bash script: `bash tools/dist_test.sh`\n    - It similarly takes several arguments:\n        - `${CONFIG_FILE}`: The configuration file required for testing.\n        - `${CHECKPOINT_FILE}`: The checkpoint file of the model.\n        - `${GPU_NUM}`: The number of GPUs to use.\n        - `[--out ${RESULT_FILE}]`: An optional argument to specify the output result file.\n        - `[--eval ${EVAL_METRICS}]`: An optional argument for evaluation metrics.\n\nThese commands are likely used in the context of machine learning or deep learning experiments for testing models."}
{"layout": 286, "type": "text", "text": "tools/dist_test.sh  also supports multi-node testing, but relies on PyTorch’s  launch utility . ", "page_idx": 29, "bbox": [71, 392.5044860839844, 450.349365234375, 405.81451416015625], "page_size": [612.0, 792.0]}
{"layout": 287, "type": "text", "text": "Optional arguments: ", "page_idx": 29, "bbox": [71, 410.4375, 155, 423.7475280761719], "page_size": [612.0, 792.0]}
{"layout": 288, "type": "text", "text": "•  RESULT FILE : Filename of the output results in pickle format. If not specified, the results will not be saved to a file. ", "page_idx": 29, "bbox": [88, 428.3705139160156, 540, 453.6355285644531], "page_size": [612.0, 792.0]}
{"layout": 289, "type": "text", "text": "•  E VAL METRICS : Items to be evaluated on the results. Allowed values depend on the dataset, e.g., proposal fast ,  proposal ,  bbox ,  segm  are available for COCO,  mAP ,  recall  for PASCAL VOC. Cityscapes could be evaluated by  cityscapes  as well as all COCO metrics. •  --show : If specified, detection results will be plotted on the images and shown in a new window. It is only applicable to single GPU testing and used for debugging and visualization. Please make sure that GUI is available in your environment. Otherwise, you may encounter an error like  cannot connect to X server . •  --show-dir : If specified, detection results will be plotted on the images and saved to the specified directory. It is only applicable to single GPU testing and used for debugging and visualization. You do NOT need a GUI available in your environment for using this option. •  --show-score-thr : If specified, detections with scores below this threshold will be removed. •  --cfg-options : if specified, the key-value pair optional cfg will be merged into config file ", "page_idx": 29, "bbox": [88, 458.2585144042969, 540, 615.030517578125], "page_size": [612.0, 792.0]}
{"layout": 290, "type": "text", "text": "•  --eval-options : if specified, the key-value pair optional eval cfg will be kwargs for dataset.evaluate() function, it’s only for evaluation ", "page_idx": 29, "bbox": [88, 619.6534423828125, 540, 644.9185180664062], "page_size": [612.0, 792.0]}
{"layout": 291, "type": "text", "text": "5.2.3 Examples ", "text_level": 1, "page_idx": 30, "bbox": [71, 71, 161, 85], "page_size": [612.0, 792.0]}
{"layout": 292, "type": "text", "text": "Assuming that you have already downloaded the checkpoints to the directory  checkpoints/ . ", "page_idx": 30, "bbox": [72.0, 95.81745910644531, 447, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 293, "type": "text", "text": "1. Test Faster R-CNN and visualize the results. Press any key for the next image. Config and checkpoint files are available  here . ", "page_idx": 30, "bbox": [84, 113.75044250488281, 540, 139.01548767089844], "page_size": [612.0, 792.0]}
{"layout": 294, "type": "text", "text": "python tools/test.py  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --show ", "page_idx": 30, "bbox": [96, 147.82296752929688, 447, 194.29818725585938], "page_size": [612.0, 792.0]}
{"layout": 295, "type": "text", "text": "2. Test Faster R-CNN and save the painted images for future visualization. Config and checkpoint files are available here . ", "page_idx": 30, "bbox": [84, 206.87147521972656, 540, 232.1365203857422], "page_size": [612.0, 792.0]}
{"layout": 296, "type": "table", "page_idx": 30, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_12.jpg", "bbox": [93, 238, 541, 291], "page_size": [612.0, 792.0], "ocr_text": "python tools/test.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n--show-dir faster_rcnn_r50_fpn_1x_results\n", "vlm_text": "The image displays a command being executed in a terminal. This command is used to run a Python script from a set of machine learning tools, likely related to object detection using a model such as Faster R-CNN. Here's a breakdown of the command:\n\n1. `python tools/test.py \\`: This is the command to run a Python script named `test.py`, which is located in the `tools` directory.\n\n2. `configs/faster_rcnn/faster_rcnn_r50_fpn_1x.py \\`: This refers to a configuration file likely used to set up the Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN) with a 1x schedule.\n\n3. `checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\`: This specifies the path to a pre-trained model checkpoint file. The file is named in such a way that it indicates a Faster R-CNN model with ResNet-50 and FPN, pre-trained on the COCO dataset, with a date stamp of 2020-01-30.\n\n4. `--show-dir faster_rcnn_r50_fpn_1x_results`: This specifies the directory where the results of this test or evaluation will be saved or displayed."}
{"layout": 297, "type": "text", "text": "3. Test Faster R-CNN on PASCAL VOC (without saving the test results) and evaluate the mAP. Config and check- point files are available  here . ", "page_idx": 30, "bbox": [84, 299.9924621582031, 540, 325.2574768066406], "page_size": [612.0, 792.0]}
{"layout": 298, "type": "text", "text": "python tools/test.py  \\ configs/pascal_voc/faster r cnn r 50 fp n 1 x voc.py  \\ checkpoints/faster r cnn r 50 fp n 1 x voc 0712 20200624-c9895d40.pth  \\ --eval mAP ", "page_idx": 30, "bbox": [96, 334.0649719238281, 463.0323486328125, 380.54010009765625], "page_size": [612.0, 792.0]}
{"layout": 299, "type": "text", "text": "4. Test Mask R-CNN with 8 GPUs, and evaluate the bbox and mask AP. Config and checkpoint files are available here . ", "page_idx": 30, "bbox": [84, 393.11346435546875, 540, 418.37847900390625], "page_size": [612.0, 792.0]}
{"layout": 300, "type": "image", "page_idx": 30, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_13.jpg", "bbox": [93, 424, 541, 502], "page_size": [612.0, 792.0], "ocr_text": "./tools/dist_test.sh \\\nconfigs/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n8 \\\n--out results.pkl \\\n--eval bbox segm\n", "vlm_text": "The image shows a command line input for testing a model using a script. The command is structured as follows:\n\n- `./tools/dist_test.sh`: This is the test script being executed.\n- `configs/mask_rcnn_r50_fpn_1x_coco.py`: This is the configuration file for testing, specifying the Mask R-CNN model with ResNet-50 and FPN on the COCO dataset.\n- `checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth`: This is the file path to the pre-trained model checkpoint.\n- `8`: Indicates the number of GPUs to be used for testing.\n- `--out results.pkl`: This flag specifies the output file where the results will be saved, in this case, `results.pkl`.\n- `--eval bbox segm`: This flag specifies the evaluation metrics, which in this case are bounding box and segmentation."}
{"layout": 301, "type": "text", "text": "5. Test Mask R-CNN with 8 GPUs, and evaluate the  classwise  bbox and mask AP. Config and checkpoint files are available  here . ", "page_idx": 30, "bbox": [84, 509.516845703125, 540, 535.4094848632812], "page_size": [612.0, 792.0]}
{"layout": 302, "type": "image", "page_idx": 30, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_14.jpg", "bbox": [93, 541, 541, 631], "page_size": [612.0, 792.0], "ocr_text": "./tools/dist_test.sh \\\nconfigs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n8 \\\n\n--out results.pkl \\\n--eval bbox segm \\\n--options \"classwise=True\"\n", "vlm_text": "The image displays a command-line script used to run a distributed test for object detection and segmentation using Mask R-CNN. The command is executed using the `dist_test.sh` script located in the `tools` directory. The configuration file used is `mask_rcnn_r50_fpn_1x_coco.py` from the `configs/mask_rcnn` directory, and the checkpoint file is `mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth` from the `checkpoints` directory. It specifies 8 GPUs for distributed testing (`8 \\`). The output will be saved to `results.pkl`. The evaluation metrics are bounding box (bbox) and segmentation (segm). An additional option to evaluate results class-wise is set as `\"classwise=True\"`."}
{"layout": 303, "type": "text", "text": "6. Test Mask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files for submitting to the official evalu- ation server. Config and checkpoint files are available  here . ", "page_idx": 30, "bbox": [84, 639.1314086914062, 540, 664.396484375], "page_size": [612.0, 792.0]}
{"layout": 304, "type": "image", "page_idx": 30, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_15.jpg", "bbox": [94, 670, 541, 715], "page_size": [612.0, 792.0], "ocr_text": "./tools/dist_test.sh \\\nconfigs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n\n", "vlm_text": "The image shows a command-line script being executed. It runs a shell script `dist_test.sh` from the `tools` directory with the following arguments:\n\n1. `configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py`: A configuration file for Mask R-CNN using a ResNet-50 backbone with FPN on the COCO dataset.\n2. `checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth`: A checkpoint file for the Mask R-CNN model, which likely contains pre-trained model weights."}
{"layout": 305, "type": "image", "page_idx": 31, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_16.jpg", "bbox": [95, 83, 542, 126], "page_size": [612.0, 792.0], "ocr_text": "8 \\,\n--format-only \\\n--options \"jsonfile_prefix=./mask_rcnn_test-dev_results\"\n", "vlm_text": "The image shows a snippet of a command or script involving some options and parameters. It includes:\n\n- A line number: `8 \\`\n- Command line arguments:\n  - `--format-only \\`\n  - `--options \"jsonfile_prefix=./mask_rcnn_test-dev_results\"`\n\nThis command appears to be related to processing or configuring results for a task, possibly in data processing or machine learning, indicated by the mention of `mask_rcnn`."}
{"layout": 306, "type": "text", "text": "This command generates two JSON files mask r cnn test-dev results.bbox.json and mask r cnn test-dev results.segm.json . ", "page_idx": 31, "bbox": [96, 134.16847229003906, 540, 159.4344940185547], "page_size": [612.0, 792.0]}
{"layout": 307, "type": "text", "text": "7. Test Mask R-CNN on Cityscapes test with 8 GPUs, and generate txt and png files for submitting to the official evaluation server. Config and checkpoint files are available  here . ", "page_idx": 31, "bbox": [84.45301818847656, 164.0564727783203, 540, 189.32249450683594], "page_size": [612.0, 792.0]}
{"layout": 308, "type": "table", "page_idx": 31, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_17.jpg", "bbox": [93, 194, 542, 273], "page_size": [612.0, 792.0], "ocr_text": "./tools/dist_test.sh \\\nconfigs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227-afe5id5a.pth \\\n8 \\\n\n--format-only \\\n--options \"txtfile_prefix=./mask_rcnn_cityscapes_test_results\"\n\n", "vlm_text": "This image contains a command-line script that is used for testing a model using a distributed testing script. The command provides the paths to both the model configuration file and the checkpoint file. Here’s a breakdown of the command:\n\n- `./tools/dist_test.sh`: This script is used for executing distributed testing.\n\n- `configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py`: This is the path to the configuration file for the Mask R-CNN model, which is configured for use on the Cityscapes dataset with a ResNet-50 backbone and a Feature Pyramid Network (FPN).\n\n- `checkpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227-afe51d5a.pth`: This is the path to the checkpoint file containing the pre-trained weights for the model.\n\n- `8`: This number likely indicates the number of GPUs to be used for the distributed test.\n\n- `--format-only`: A flag indicating that the script should run in a format-only mode, typically used to evaluate detection results without computing evaluation metrics.\n\n- `--options \"txtfile_prefix=./mask_rcnn_cityscapes_test_results\"`: This option specifies output options, such as the prefix for the text files generated as results. In this case, the results will be prefixed with `./mask_rcnn_cityscapes_test_results`.\n\nThe command is structured and formatted for ease of reading when extended over multiple lines using backslashes (`\\`)."}
{"layout": 309, "type": "text", "text": "The generated png and txt would be under  ./mask r cnn cityscape s test results  directory. ", "page_idx": 31, "bbox": [96, 280.6194763183594, 491.0576171875, 293.92950439453125], "page_size": [612.0, 792.0]}
{"layout": 310, "type": "text", "text": "5.2.4 Test without Ground Truth Annotations ", "text_level": 1, "page_idx": 31, "bbox": [71, 313, 327, 327], "page_size": [612.0, 792.0]}
{"layout": 311, "type": "text", "text": "MM Detection supports to test models without ground-truth annotations using  Coco Data set . If your dataset format is not in COCO format, please convert them to COCO format. For example, if your dataset format is VOC, you can directly convert it to COCO format by the  script in tools.  If your dataset format is Cityscapes, you can directly convert it to COCO format by the  script in tools.  The rest of the formats can be converted using  this script . ", "page_idx": 31, "bbox": [71, 338.1614685058594, 540, 387.33746337890625], "page_size": [612.0, 792.0]}
{"layout": 312, "type": "table", "page_idx": 31, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_18.jpg", "bbox": [68, 391, 542, 459], "page_size": [612.0, 792.0], "ocr_text": "python tools/dataset_converters/images2coco.py \\\n${IMG_PATH} \\\n${CLASSES} \\\n${OUT} \\\n[--exclude-extensions]\n\n", "vlm_text": "The table contains a command for running a Python script, likely used for converting images to COCO dataset format. Here's the command:\n\n```bash\npython tools/dataset_converters/images2coco.py \\\n    ${IMG_PATH} \\\n    ${CLASSES} \\\n    ${OUT} \\\n    [--exclude-extensions]\n```\n\n- `${IMG_PATH}`: Placeholder for the path to the images.\n- `${CLASSES}`: Placeholder for the classes or labels.\n- `${OUT}`: Placeholder for the output directory or file.\n- `[--exclude-extensions]`: An optional argument to exclude certain file extensions."}
{"layout": 313, "type": "text", "text": "arguments ", "page_idx": 31, "bbox": [71, 466.6794738769531, 113.35474395751953, 479.989501953125], "page_size": [612.0, 792.0]}
{"layout": 314, "type": "text", "text": "•  IMG_PATH : The root path of images. •  CLASSES : The text file with a list of categories. •  OUT : The output annotation json file name. The save dir is in the same directory as  IMG_PATH . •  exclude-extensions : The suffix of images to be excluded, such as ‘png’ and ‘bmp’. ", "page_idx": 31, "bbox": [88, 484.61248779296875, 471.9296569824219, 551.7205200195312], "page_size": [612.0, 792.0]}
{"layout": 315, "type": "text", "text": "After the conversion is complete, you can use the following command to test ", "page_idx": 31, "bbox": [71, 556.3434448242188, 376.3673095703125, 569.6535034179688], "page_size": [612.0, 792.0]}
{"layout": 316, "type": "image", "page_idx": 31, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_19.jpg", "bbox": [68, 573, 543, 717], "page_size": [612.0, 792.0], "ocr_text": "# single-gpu testing\n\npython tools/test.py \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n--format-only \\\n--options ${JSONFILE_PREFIX} \\\n[--show]\n\n# multi-gpu testing\nbash tools/dist_test.sh \\\n${CONFIG_FILE} \\\n\nTLAaneoneG cfr ret sae)\n", "vlm_text": "The image displays a snippet of code that provides commands for testing a machine learning model using Python and shell scripts. It covers two scenarios: single-GPU and multi-GPU testing.\n\n1. **Single-GPU Testing**:\n   - The command begins with `python tools/test.py`, indicating the usage of a Python script located in the `tools` directory.\n   - It takes in several arguments:\n     - `${CONFIG_FILE}`: A placeholder for the configuration file.\n     - `${CHECKPOINT_FILE}`: A placeholder for the model's checkpoint file.\n     - `--format-only`: A flag indicating that the script should run in a mode where no evaluations are conducted, possibly just formatting.\n     - `--options ${JSONFILE_PREFIX}`: An option to specify JSON file-related settings.\n     - `[--show]`: An optional flag for displaying results.\n\n2. **Multi-GPU Testing**:\n   - The command begins with `bash tools/dist_test.sh`, indicating the usage of a shell script for distributed testing.\n   - It appears to take in one argument, `${CONFIG_FILE}`, which is a placeholder for the configuration file.\n\nThe code snippets are accompanied by comments that describe their purpose. The image's content is visually separated, with blue text denoting the actual command elements and purple text for placeholders or variable parts of the commands."}
{"layout": 317, "type": "image", "page_idx": 32, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_20.jpg", "bbox": [68, 82, 543, 149], "page_size": [612.0, 792.0], "ocr_text": "${CHECKPOINT_FILE} \\\n${GPU_NUM} \\\n\n--format-only \\\n\n--options ${JSONFILE_PREFIX} \\\n[--show]\n", "vlm_text": "The image is a screenshot of a command-line snippet or script. It seems to be a part of a larger command related to some computational task, possibly involving machine learning or data processing. The placeholders or variables include `${CHECKPOINT_FILE}`, `${GPU_NUM}`, and `${JSONFILE_PREFIX}`, suggesting these values need to be substituted with actual file paths or numbers before running the command. The options `--format-only`, `--options`, and optionally `--show` indicate selectable flags or parameters to control the behavior of whatever process this command runs. The `\\` at the end of each line signifies line continuation, likely in a shell or script that spans multiple lines for better readability."}
{"layout": 318, "type": "text", "text": "Assuming that the checkpoints in the  model zoo  have been downloaded to the directory  checkpoints/ , we can test Mask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files using the following command. ", "page_idx": 32, "bbox": [72, 158.07948303222656, 540, 183.3445281982422], "page_size": [612.0, 792.0]}
{"layout": 319, "type": "text", "text": "./tools/dist_test.sh  \\ configs/mask_rcnn/mask r cnn r 50 fp n 1 x coco.py  \\ checkpoints/mask r cnn r 50 fp n 1 x coco 20200205-d4b0c5d6.pth  \\ 8  \\ -format-only \\--options  \"json file prefix  $=$  ./mask r cnn test-dev results\" ", "page_idx": 32, "bbox": [72, 192.06497192382812, 411.974365234375, 262.4512023925781], "page_size": [612.0, 792.0]}
{"layout": 320, "type": "text", "text": "This command generates two JSON files mask r cnn test-dev results.bbox.json and mask r cnn test-dev results.segm.json . ", "page_idx": 32, "bbox": [72, 275.02447509765625, 540, 300.28948974609375], "page_size": [612.0, 792.0]}
{"layout": 321, "type": "text", "text": "5.2.5 Batch Inference ", "text_level": 1, "page_idx": 32, "bbox": [71, 319, 195, 333], "page_size": [612.0, 792.0]}
{"layout": 322, "type": "text", "text": "MM Detection supports inference with a single image or batched images in test mode. By default, we use single-image inference and you can use batch inference by modifying  samples per gpu  in the config of test data. You can do that either by modifying the config as below. ", "page_idx": 32, "bbox": [72, 344.8504638671875, 540, 382.0704650878906], "page_size": [612.0, 792.0]}
{"layout": 323, "type": "image", "page_idx": 32, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_21.jpg", "bbox": [68, 389, 527, 405], "page_size": [612.0, 792.0], "ocr_text": "data = dict(train=dict(...), val=dict(...), test=dict(samples_per_gpu=2, ...))\n", "vlm_text": "The image contains a snippet of Python code. It defines a dictionary called `data` with three keys: `train`, `val`, and `test`. Each key is associated with another dictionary. The `test` dictionary includes a parameter `samples_per_gpu` set to 2. The ellipses (`...`) indicate that additional content may be included in the actual code."}
{"layout": 324, "type": "text", "text": "Or you can set it through  --cfg-options  as  --cfg-options data.test.samples per gpu  $^{=2}$  ", "page_idx": 32, "bbox": [72, 413.9744567871094, 469, 427.28448486328125], "page_size": [612.0, 792.0]}
{"layout": 325, "type": "text", "text": "5.2.6 Deprecated Image To Tensor ", "text_level": 1, "page_idx": 32, "bbox": [71, 446, 259, 461], "page_size": [612.0, 792.0]}
{"layout": 326, "type": "text", "text": "In test mode,  Image To Tensor  pipeline is deprecated, it’s replaced by  Default Format Bundle  that recommended to manually replace it in the test data pipeline in your config file. examples: ", "page_idx": 32, "bbox": [72, 471.845458984375, 540, 497.1104736328125], "page_size": [612.0, 792.0]}
{"layout": 327, "type": "table", "page_idx": 32, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_22.jpg", "bbox": [68, 500, 543, 712], "page_size": [612.0, 792.0], "ocr_text": "# use ImageToTensor (deprecated)\npipelines = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntrans forms=[\ndict(type='Resize', keep_ratio=True),\ndict (type='RandomFlip'),\ndict(type='Normalize', mean=[0, 9, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n))\n", "vlm_text": "The table contains Python code for image processing using pipelines. Here's a breakdown of the code:\n\n1. **LoadImageFromFile**: Loads an image from a file.\n\n2. **MultiScaleFlipAug**: Handles multi-scale and flip augmentation.\n   - `img_scale=(1333, 800)`: The target image scale.\n   - `flip=False`: Disables image flipping.\n   - `transforms`: A list of transformations:\n     - **Resize**: Resizes the image while keeping the aspect ratio.\n     - **RandomFlip**: Randomly flips the image.\n     - **Normalize**: Normalizes the image using the given mean and std deviation (`mean=[0, 0, 0]`, `std=[1, 1, 1]`).\n     - **Pad**: Pads the image to a specified size divisor (`size_divisor=32`).\n     - **ImageToTensor**: Converts the image to a tensor.\n     - **Collect**: Collects the image into a batch.\n\nThe pipeline is setup to load, augment, and preprocess images for model training or inference."}
{"layout": 328, "type": "image", "page_idx": 33, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_23.jpg", "bbox": [68, 82, 543, 283], "page_size": [612.0, 792.0], "ocr_text": "# manually replace ImageToTensor to DefaultFormatBundle (recommended)\npipelines = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntrans forms=[\ndict(type='Resize', keep_ratio=True),\ndict (type='RandomFlip'),\ndict(type='Normalize', mean=[0, 9, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict (type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img']),\n1)\n\n", "vlm_text": "The image shows a snippet of Python code configuring a data processing pipeline, likely for deep learning purposes. Here's a breakdown of the code:\n\n- **pipelines** is a list of dictionaries, each representing a different stage in the data processing pipeline.\n\n1. **LoadImageFromFile**: Loads an image from a file.\n\n2. **MultiScaleFlipAug**: Applies multiple augmentations:\n   - `img_scale`: Sets the image scale to (1333, 800).\n   - `flip`: Sets the flip operation to False.\n   - `transforms`: Contains a list of transformations to apply:\n     - **Resize**: Resizes the image, keeping the aspect ratio.\n     - **RandomFlip**: Randomly flips the image.\n     - **Normalize**: Normalizes the image using mean [0, 0, 0] and std [1, 1, 1].\n     - **Pad**: Pads the image to a size divisible by 32.\n     - **DefaultFormatBundle**: Formats data for training.\n     - **Collect**: Collects the keys, with 'img' specified here.\n\nThe comment suggests replacing `ImageToTensor` with `DefaultFormatBundle`."}
{"layout": 329, "type": "text", "text": "5.3 Train predefined models on standard datasets ", "text_level": 1, "page_idx": 33, "bbox": [71, 308, 414, 324], "page_size": [612.0, 792.0]}
{"layout": 330, "type": "text", "text": "MM Detection also provides out-of-the-box tools for training detection models. This section will show how to train predefined  models (under  configs ) on standard datasets i.e. COCO. ", "page_idx": 33, "bbox": [71, 339.5254821777344, 540, 364.7904968261719], "page_size": [612.0, 792.0]}
{"layout": 331, "type": "text", "text": "Important : The default learning rate in config files is for 8 GPUs and  $2{\\mathrm{~img/gmu}}$   (batch   ${\\mathrm{size}}=8^{*}2=16$  ). According to the  linear scaling rule , you need to set the learning rate proportional to the batch size if you use different GPUs or images per GPU, e.g.,  $\\scriptstyle{1\\!\\mathrm{r}=\\!\\varnothing.\\varnothing1}$   for 4 GPUs   $^{*}\\,2$   imgs/gpu and  $\\scriptstyle1r=0.08$   for 16 GPUs  $^{\\ast}\\,4$   imgs/gpu. ", "page_idx": 33, "bbox": [71, 368.7848205566406, 540, 406.63348388671875], "page_size": [612.0, 792.0]}
{"layout": 332, "type": "text", "text": "5.3.1 Prepare datasets ", "text_level": 1, "page_idx": 33, "bbox": [72, 426, 202, 440], "page_size": [612.0, 792.0]}
{"layout": 333, "type": "text", "text": "Training requires preparing datasets too. See section  Prepare datasets  above for details. ", "page_idx": 33, "bbox": [71, 451.4020080566406, 421.2054443359375, 464.8614807128906], "page_size": [612.0, 792.0]}
{"layout": 334, "type": "text", "text": "Note : Currently, the config files under  configs/cityscapes  use COCO pretrained weights to initialize. You could download the existing models in advance if the network connection is unavailable or slow. Otherwise, it would cause errors at the beginning of training. ", "page_idx": 33, "bbox": [71, 468.8568115234375, 540, 506.7044677734375], "page_size": [612.0, 792.0]}
{"layout": 335, "type": "text", "text": "5.3.2 Training on a single GPU ", "text_level": 1, "page_idx": 33, "bbox": [71, 526, 247, 540], "page_size": [612.0, 792.0]}
{"layout": 336, "type": "text", "text": "We provide  tools/train.py  to launch training jobs on a single GPU. The basic usage is as follows. ", "page_idx": 33, "bbox": [71, 551.6224365234375, 475.1172180175781, 564.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 337, "type": "image", "page_idx": 33, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_24.jpg", "bbox": [67, 570, 542, 613], "page_size": [612.0, 792.0], "ocr_text": "python tools/train.py \\\n${CONFIG_FILE} \\\n{optional arguments |\n", "vlm_text": "The image contains a command line snippet for running a Python script. The command is:\n\n```\npython tools/train.py \\\n    ${CONFIG_FILE} \\\n    [optional arguments]\n```\n\nThis command is likely used to execute a training script named `train.py` located in the `tools` directory. It requires a configuration file specified by the placeholder `${CONFIG_FILE}` and can accept optional arguments indicated by `[optional arguments]`. The use of backslashes (`\\`) at the end of the lines suggests that the command can be written across multiple lines for readability."}
{"layout": 338, "type": "text", "text": "During training, log files and checkpoints will be saved to the working directory, which is specified by  work_dir  in the config file or via CLI argument  --work-dir . ", "page_idx": 33, "bbox": [71, 621.1614379882812, 540, 646.426513671875], "page_size": [612.0, 792.0]}
{"layout": 339, "type": "text", "text": "By default, the model is evaluated on the validation set every epoch, the evaluation interval can be specified in the config file as shown below. ", "page_idx": 33, "bbox": [71, 651.0494384765625, 540, 676.3145141601562], "page_size": [612.0, 792.0]}
{"layout": 340, "type": "text", "text": "This tool accepts several optional arguments, including: ", "page_idx": 34, "bbox": [71, 71.45246887207031, 294.0065002441406, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 341, "type": "text", "text": "•  --no-validate  ( not suggested ): Disable evaluation during training. •  --work-dir \\${WORK_DIR} : Override the working directory. •  --resume-from \\${CHECKPOINT FILE} : Resume from a previous checkpoint file. •  --options  ' Key  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  value ' : Overrides other settings in the used config. ", "page_idx": 34, "bbox": [88, 88.7568359375, 431.1127014160156, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 342, "type": "text", "text": "Note : ", "page_idx": 34, "bbox": [71, 160.48779296875, 94.53558349609375, 175.02322387695312], "page_size": [612.0, 792.0]}
{"layout": 343, "type": "text", "text": "Difference between  resume-from  and  load-from : ", "page_idx": 34, "bbox": [71, 179.0484161376953, 278.99359130859375, 192.3584442138672], "page_size": [612.0, 792.0]}
{"layout": 344, "type": "text", "text": "resume-from  loads both the model weights and optimizer status, and the epoch is also inherited from the specified checkpoint. It is usually used for resuming the training process that is interrupted accidentally.  load-from  only loads the model weights and the training epoch starts from 0. It is usually used for finetuning. ", "page_idx": 34, "bbox": [71, 196.9813995361328, 540, 234.2014617919922], "page_size": [612.0, 792.0]}
{"layout": 345, "type": "text", "text": "5.3.3 Training on multiple GPUs ", "text_level": 1, "page_idx": 34, "bbox": [70, 253, 255, 268], "page_size": [612.0, 792.0]}
{"layout": 346, "type": "text", "text": "We provide  tools/dist_train.sh  to launch training on multiple GPUs. The basic usage is as follows. ", "page_idx": 34, "bbox": [71, 279.1194152832031, 489.7225341796875, 292.429443359375], "page_size": [612.0, 792.0]}
{"layout": 347, "type": "table", "page_idx": 34, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_25.jpg", "bbox": [67, 297, 543, 354], "page_size": [612.0, 792.0], "ocr_text": "bash ./tools/dist_train.sh \\\n${CONFIG_FILE} \\\n${GPU_NUM} \\\n{optional arguments |\n\n", "vlm_text": "The table contains a shell command for running a distributed training script. Here's the command:\n\n```bash\nbash ./tools/dist_train.sh \\\n${CONFIG_FILE} \\\n${GPU_NUM} \\\n[optional arguments]\n```\n\n- `CONFIG_FILE`: Placeholder for the configuration file.\n- `GPU_NUM`: Placeholder for the number of GPUs to use.\n- `[optional arguments]`: Placeholder for any optional arguments."}
{"layout": 348, "type": "text", "text": "Optional arguments remain the same as stated  above . ", "page_idx": 34, "bbox": [71, 360.46502685546875, 283.1486511230469, 373.92449951171875], "page_size": [612.0, 792.0]}
{"layout": 349, "type": "text", "text": "Launch multiple jobs simultaneously ", "page_idx": 34, "bbox": [71, 391.7870788574219, 246.99302673339844, 406.05352783203125], "page_size": [612.0, 792.0]}
{"layout": 350, "type": "text", "text": "If you would like to launch multiple jobs on a single machine, e.g., 2 jobs of 4-GPU training on a machine with 8 GPUs, you need to specify different ports (29500 by default) for each job to avoid communication conflict. ", "page_idx": 34, "bbox": [71, 416.8494567871094, 540, 442.1144714355469], "page_size": [612.0, 792.0]}
{"layout": 351, "type": "text", "text": "If you use  dist_train.sh  to launch training jobs, you can set the port in commands. ", "page_idx": 34, "bbox": [71, 446.7374572753906, 414.9827575683594, 460.0474853515625], "page_size": [612.0, 792.0]}
{"layout": 352, "type": "table", "page_idx": 34, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_26.jpg", "bbox": [69, 466, 542, 496], "page_size": [612.0, 792.0], "ocr_text": ",2,3 PORT=29500 ./tools/dist_train.sh ${CONFIG_FILE} 4\n\nCUDA_VISIBLE_DEVICES=0,1,2,3\nS=4,5,6,7 PORT=29501 ./tools/dist_train.sh ${CONFIG_FILE} 4\n\nCUDA_VISIBLE_DEVICE\n", "vlm_text": "The table contains two command-line entries, each for executing a distributed training script (`dist_train.sh`). The configurations specified in each line are:\n\n1. The `CUDA_VISIBLE_DEVICES` environment variable is set to specific GPU device IDs:\n   - In the first line, devices 0, 1, 2, and 3 are used.\n   - In the second line, devices 4, 5, 6, and 7 are used.\n\n2. The `PORT` environment variable is set with unique port numbers for each line:\n   - In the first line, the port is set to 29500.\n   - In the second line, the port is set to 29501.\n\n3. Both lines execute the script `./tools/dist_train.sh` with the same command-line arguments:\n   - They both use the `${CONFIG_FILE}` placeholder, likely representing a configuration file for the training.\n   - '4' is likely the number of GPUs or some other parameter related to training.\n\nThis setup seems to be for running parallel distributed training jobs with different subsets of GPUs and port configurations."}
{"layout": 353, "type": "text", "text": "5.3.4 Training on multiple nodes ", "text_level": 1, "page_idx": 34, "bbox": [70, 518, 259, 534], "page_size": [612.0, 792.0]}
{"layout": 354, "type": "text", "text": "MM Detection relies on  torch.distributed  package for distributed training. Thus, as a basic usage, one can launch distributed training via PyTorch’s  launch utility . ", "page_idx": 34, "bbox": [71, 544.616455078125, 540, 569.8825073242188], "page_size": [612.0, 792.0]}
{"layout": 355, "type": "text", "text": "5.3.5 Manage jobs with Slurm ", "text_level": 1, "page_idx": 34, "bbox": [70, 588, 243, 604], "page_size": [612.0, 792.0]}
{"layout": 356, "type": "text", "text": "Slurm  is a good job scheduling system for computing clusters. On a cluster managed by Slurm, you can use slur m train.sh  to spawn training jobs. It supports both single-node and multi-node training. ", "page_idx": 34, "bbox": [71, 614.8004150390625, 540, 640.0654907226562], "page_size": [612.0, 792.0]}
{"layout": 357, "type": "text", "text": "The basic usage is as follows. ", "page_idx": 34, "bbox": [71, 644.6884155273438, 189.1800994873047, 657.9984741210938], "page_size": [612.0, 792.0]}
{"layout": 358, "type": "image", "page_idx": 34, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_27.jpg", "bbox": [69, 665, 543, 681], "page_size": [612.0, 792.0], "ocr_text": "[GPUS=${GPUS}] ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR}\n", "vlm_text": "The image shows a command line script for executing a shell script using SLURM, a job scheduler for Linux. The command is:\n\n```\nGPUS=${GPUS} ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR}\n```\n\nVariables such as `${GPUS}`, `${PARTITION}`, `${JOB_NAME}`, `${CONFIG_FILE}`, and `${WORK_DIR}` are placeholders likely meant to be replaced with actual values when running the script."}
{"layout": 359, "type": "text", "text": "Below is an example of using 16 GPUs to train Mask R-CNN on a Slurm partition named  dev , and set the work-dir to some shared file systems. ", "page_idx": 34, "bbox": [71, 690.1670532226562, 540, 715.58251953125], "page_size": [612.0, 792.0]}
{"layout": 360, "type": "text", "text": "GPUS = 16  ./tools/slur m train.sh dev mask r 50 1 x configs/mask r cnn r 50 fp n 1 x coco.py /nfs/ xxxx/mask r cnn r 50 fp n 1 x ˓ → ", "page_idx": 35, "bbox": [72, 79.55177307128906, 537, 104.39839172363281], "page_size": [612.0, 792.0]}
{"layout": 361, "type": "text", "text": "You can check  the source code  to review full arguments and environment variables. When using Slurm, the port option need to be set in one of the following ways: 1. Set the port through  --options . This is more recommended since it does not change the original configs. ", "page_idx": 35, "bbox": [72, 114.09248352050781, 404.05340576171875, 127.40251922607422], "page_size": [612.0, 792.0]}
{"layout": 362, "type": "text", "text": "", "page_idx": 35, "bbox": [72, 132.0244903564453, 385.81182861328125, 145.3345184326172], "page_size": [612.0, 792.0]}
{"layout": 363, "type": "text", "text": "", "page_idx": 35, "bbox": [82, 149.9574737548828, 520.2275390625, 163.2675018310547], "page_size": [612.0, 792.0]}
{"layout": 364, "type": "image", "page_idx": 35, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_28.jpg", "bbox": [95, 170, 542, 223], "page_size": [612.0, 792.0], "ocr_text": "CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME}.\nsconfigl.py ${WORK_DIR} --options 'dist_params.port=29500'\nCUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME}.\nsconfig2.py ${WORK_DIR} --options 'dist_params.port=29501'\n", "vlm_text": "The image shows two command lines for running scripts with specified GPU devices using SLURM and CUDA:\n\n1. **First Command:**\n   - **CUDA_VISIBLE_DEVICES=0,1,2,3**: Specifies which GPUs to use.\n   - **GPUS=4**: Number of GPUs allocated.\n   - **Script**: `./tools/slurm_train.sh`\n   - **Parameters**: `${PARTITION}` and `${JOB_NAME}`\n   - **Configuration**: `config1.py`\n   - **Options**: `--options 'dist_params.port=29500'`\n\n2. **Second Command:**\n   - **CUDA_VISIBLE_DEVICES=4,5,6,7**: Specifies a different set of GPUs.\n   - **GPUS=4**: Number of GPUs allocated.\n   - **Script**: `./tools/slurm_train.sh`\n   - **Parameters**: `${PARTITION}` and `${JOB_NAME}`\n   - **Configuration**: `config2.py`\n   - **Options**: `--options 'dist_params.port=29501'`"}
{"layout": 365, "type": "text", "text": "2. Modify the config files to set different communication ports. ", "page_idx": 35, "bbox": [82, 231.4524688720703, 337, 244.7624969482422], "page_size": [612.0, 792.0]}
{"layout": 366, "type": "text", "text": "In  config1.py , set ", "page_idx": 35, "bbox": [96, 249.3844757080078, 176, 262.69451904296875], "page_size": [612.0, 792.0]}
{"layout": 367, "type": "table", "page_idx": 35, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_29.jpg", "bbox": [95, 269, 541, 288], "page_size": [612.0, 792.0], "ocr_text": "dist_params = dict(backend='nccl', port=29500)\n\n", "vlm_text": "The image shows a piece of Python code that initializes a dictionary named `dist_params` with two key-value pairs. The `backend` key is assigned the value `'nccl'`, and the `port` key is assigned the value `29500`."}
{"layout": 368, "type": "text", "text": "In  config2.py , set ", "page_idx": 35, "bbox": [96, 295.0134582519531, 176, 308.323486328125], "page_size": [612.0, 792.0]}
{"layout": 369, "type": "text", "text": "dist params  $=$   dict (backend = ' nccl ' , port = 29501 ) ", "page_idx": 35, "bbox": [96, 318, 337, 328.09576416015625], "page_size": [612.0, 792.0]}
{"layout": 370, "type": "text", "text": "Then you can launch two jobs with  config1.py  and  config2.py ", "page_idx": 35, "bbox": [96, 340.6424865722656, 362.8287658691406, 353.9525146484375], "page_size": [612.0, 792.0]}
{"layout": 371, "type": "text", "text": "CUDA VISIBLE DEVICES  $\\scriptstyle=\\0$  ,1,2,3  GPUS  $_{:=4}$   ./tools/slur m train.sh    $\\mathcal{S}$  { PARTITION } \\${ JOB_NAME } ␣  $\\hookrightarrow$  config1.py  \\${ WORK_DIR } → CUDA VISIBLE DEVICES  ${=}4$  ,5,6,7  GPUS  $_{:=4}$   ./tools/slur m train.sh  \\${ PARTITION } \\${ JOB_NAME } ␣  $\\hookrightarrow$  config2.py  \\${ WORK_DIR } → ", "page_idx": 35, "bbox": [96, 363, 537, 412.4433288574219], "page_size": [612.0, 792.0]}
{"layout": 372, "type": "text", "text": "2: TRAIN WITH CUSTOMIZED DATASETS ", "text_level": 1, "page_idx": 36, "bbox": [259, 162, 542, 182], "page_size": [612.0, 792.0]}
{"layout": 373, "type": "text", "text": "In this note, you will know how to inference, test, and train predefined models with customized datasets. We use the balloon dataset  as an example to describe the whole process. ", "page_idx": 36, "bbox": [72, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 374, "type": "text", "text": "The basic steps are as below: ", "page_idx": 36, "bbox": [72, 255.7604522705078, 187, 269.07049560546875], "page_size": [612.0, 792.0]}
{"layout": 375, "type": "text", "text": "1. Prepare the customized dataset 2. Prepare a config 3. Train, test, inference models on the customized dataset. ", "page_idx": 36, "bbox": [84, 273.6934814453125, 317.109619140625, 322.8695373535156], "page_size": [612.0, 792.0]}
{"layout": 376, "type": "text", "text": "6.1 Prepare the customized dataset ", "text_level": 1, "page_idx": 36, "bbox": [71, 346, 317, 361], "page_size": [612.0, 792.0]}
{"layout": 377, "type": "text", "text": "There are three ways to support a new dataset in MM Detection: ", "page_idx": 36, "bbox": [72, 377.4014892578125, 325.1396484375, 390.7115173339844], "page_size": [612.0, 792.0]}
{"layout": 378, "type": "text", "text": "1. reorganize the dataset into COCO format. 2. reorganize the dataset into a middle format. 3. implement a new dataset. ", "page_idx": 36, "bbox": [84, 395.33349609375, 269.83709716796875, 444.5095520019531], "page_size": [612.0, 792.0]}
{"layout": 379, "type": "text", "text": "Usually we recommend to use the first two methods which are usually easier than the third. ", "page_idx": 36, "bbox": [72, 449.1325378417969, 434.1404113769531, 462.44256591796875], "page_size": [612.0, 792.0]}
{"layout": 380, "type": "text", "text": "In this note, we give an example for converting the data into COCO format. ", "page_idx": 36, "bbox": [72, 467.0645446777344, 371.58526611328125, 480.37457275390625], "page_size": [612.0, 792.0]}
{"layout": 381, "type": "text", "text": "Note : MM Detection only supports evaluating mask AP of dataset in COCO format for now. So for instance segmen- tation task users should convert the data into coco format. ", "page_idx": 36, "bbox": [72, 484.3699035644531, 540, 510.2625732421875], "page_size": [612.0, 792.0]}
{"layout": 382, "type": "text", "text": "6.1.1 COCO annotation format ", "text_level": 1, "page_idx": 36, "bbox": [71, 530, 246, 543], "page_size": [612.0, 792.0]}
{"layout": 383, "type": "text", "text": "The necessary keys of COCO format for instance segmentation is as below, for the complete details, please refer  here . ", "page_idx": 36, "bbox": [72, 555.1515502929688, 540, 568.4616088867188], "page_size": [612.0, 792.0]}
{"layout": 384, "type": "text", "text": "\"images\": [image], \"annotations\": [annotation], \"categories\": [category] ", "page_idx": 36, "bbox": [92, 590.11474609375, 239.3711395263672, 624.0381469726562], "page_size": [612.0, 792.0]}
{"layout": 385, "type": "text", "text": "image = { \"id\": int, \"width\": int, \"height\": int, ", "page_idx": 36, "bbox": [72, 661.8457641601562, 166.1460723876953, 707.7241821289062], "page_size": [612.0, 792.0]}
{"layout": 386, "type": "table", "page_idx": 37, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_30.jpg", "bbox": [68, 82, 544, 306], "page_size": [612.0, 792.0], "ocr_text": "“file_name\": str,\n\nannotation = {\n\"id\": int,\n“image_id\": int,\n\"“category_id\": int,\n\"segmentation\": RLE or [polygon],\n\"area\": float,\n“bbox\": [x,y,width, height],\n“iscrowd\": 0 or 1,\n\ncategories = [{\n\"id\": int,\n\"name\": str,\n“supercategory\": str,\n\ni]\n\n", "vlm_text": "The table contains a data structure typically used in image annotations within datasets.\n\n1. **Image Information**:\n   - `\"file_name\": str`: Represents the file name as a string.\n\n2. **Annotation**:\n   - `\"id\": int`: An integer identifier for the annotation.\n   - `\"image_id\": int`: An integer identifier linking the annotation to a specific image.\n   - `\"category_id\": int`: An integer identifier linking to a specific category.\n   - `\"segmentation\": RLE or [polygon]`: Defines segmentation, either as Run-Length Encoding (RLE) or polygon coordinates.\n   - `\"area\": float`: A floating-point number representing the area of the annotation.\n   - `\"bbox\": [x, y, width, height]`: A list defining the bounding box with x and y coordinates, along with width and height.\n   - `\"iscrowd\": 0 or 1`: A binary value indicating if the annotation is for a crowd.\n\n3. **Categories**:\n   - `categories`: A list containing category information.\n   - `\"id\": int`: An integer identifier for the category.\n   - `\"name\": str`: The name of the category as a string.\n   - `\"supercategory\": str`: The supercategory name as a string."}
{"layout": 387, "type": "text", "text": "Assume we use the balloon dataset. After downloading the data, we need to implement a function to convert the annotation format into the COCO format. Then we can use implemented COCO Data set to load the data and perform training and evaluation. ", "page_idx": 37, "bbox": [72, 313.4964599609375, 540, 350.7164611816406], "page_size": [612.0, 792.0]}
{"layout": 388, "type": "text", "text": "If you take a look at the dataset, you will find the dataset format is as below: ", "page_idx": 37, "bbox": [72, 355.3394470214844, 374.3050842285156, 368.64947509765625], "page_size": [612.0, 792.0]}
{"layout": 389, "type": "table", "page_idx": 37, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_31.jpg", "bbox": [67, 372, 546, 714], "page_size": [612.0, 792.0], "ocr_text": "{'base64_img_data': ;\n'file_attributes': {},\n\"filename': '34020010494_e5cb88e1c4_k.jpg',\n\"fileref': '',\n\"regions': {'O': {'region_attributes': {},\n\"shape_attributes': {'all_points_x': [1020,\n000,\n994,\n003,\n023,\n050,\n089,\n134,\n190,\n265,\n321,\n361,\n403,\n428,\n442,\n445,\n441,\n427,\n400,\n361,\n316,\n269,\n228,\n\n", "vlm_text": "This table appears to contain a snippet of JSON data, likely used for image annotation or analysis purposes. Here's a breakdown of the contents:\n\n- `'base64_img_data'`: An empty string, which might be intended to hold base64 encoded image data.\n- `'file_attributes'`: An empty object, probably meant for storing additional metadata about the file.\n- `'filename'`: The name of the image file, `'34020010494_e5cb88e1c4_k.jpg'`.\n- `'fileref'`: An empty string, possibly meant to reference the file location.\n- `'regions'`: An object that contains regions within the image. In this case, there is one region (`'0'`), which includes:\n  - `'region_attributes'`: An empty object that could store specific attributes of the region.\n  - `'shape_attributes'`: Contains the key `'all_points_x'`, an array of x-coordinates that likely define a shape or path within the image.\n\nThe table essentially represents a structure that could be used in image annotation software for defining shapes or regions of interest in an image file."}
{"layout": 390, "type": "text", "text": "1207, 1210, 1190, 1177, 1172, 1174, 1170, 1153, 1127, 1104, 1061, 1032, 1020],\n\n ' all points y ' : [963, 899, 841, 787, 738, 700, 663, 638, 621, 619, 643, 672, 720, 765, 800, 860, 896, 942, 990, 1035, 1079, 1112, 1129, 1134, 1144, 1153, 1166, 1166, 1150, 1136, 1129, 1122, 1112, 1084, 1037, 989, 963],\n\n ' name ' :  ' polygon ' }}}, ' size ' : 1115004} ", "page_idx": 38, "bbox": [92, 99.62873840332031, 202, 707.4257202148438], "page_size": [612.0, 792.0]}
{"layout": 391, "type": "text", "text": "", "page_idx": 39, "bbox": [77.2300033569336, 87.67274475097656, 162, 97.7117691040039], "page_size": [612.0, 792.0]}
{"layout": 392, "type": "text", "text": "The annotation is a JSON file where each key indicates an image’s all annotations. The code to convert the balloon dataset into coco format is as below. ", "page_idx": 39, "bbox": [72, 110.25843811035156, 540.0030517578125, 135.5234832763672], "page_size": [612.0, 792.0]}
{"layout": 393, "type": "text", "text": "import  os.path  as  osp def  convert balloon to coco (ann_file, out_file, image prefix): data_infos  $=$   mmcv . load(ann_file) annotations  $=$   [] images    $=$   [] obj_count  $\\mathbf{\\varepsilon}=\\mathbf{\\varepsilon}\\,\\Updownarrow$  for  idx, v  in  enumerate (mmcv . track it er progress(data_infos . values())): filename  $=$   v[ ' filename ' ] img_path  $=$   osp . join(image prefix, filename) height, width  $=$   mmcv . imread(img_path) . shape[: 2 ] images . append( dict ( id = idx, file_name  $\\scriptstyle{\\varepsilon}$  filename, height  $=$  height, width = width)) bboxes    $\\begin{array}{l l}{\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}[\\mathbf{\\Sigma}]}\\end{array}$  labels  $\\begin{array}{l l}{\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}[\\mathbf{\\Sigma}]}\\end{array}$  masks  $\\begin{array}{l l}{\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}}\\\\ {\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}=\\mathbf{\\Sigma}}\\end{array}\\left[\\begin{array}{l l l}{\\mathbf{\\Sigma}}\\end{array}\\right]$  for  _, obj  in  v[ ' regions ' ] . items(): assert not  obj[ ' region attributes ' ] obj  $=$   obj[ ' shape attributes ' ] p  $\\tt{\\textbf{x}=}$   obj[ ' all points x ' ] py  $=$   obj[ ' all points y ' ]  $\\mathtt{p o l y}\\ =\\ \\left[\\left(\\mathbf{x}\\ +\\ \\mathbb{0}\\,,5\\,,\\textbf{y}+\\,\\mathbb{0}\\,,5\\right)\\right.$   for  x, y  in  zip (px, py)] poly  $=$   [p  for  x  in  poly  for    $\\mathtt{p}$   in  x] x_min, y_min, x_max, y_max  $=$   ( min (px),  min (py),  max (px),  max (py)) data_anno  $=$   dict ( image_  $\\scriptstyle{\\mathrm{i}}{\\mathsf{d}}={\\mathrm{i}}{\\mathsf{d}}\\mathbf{x}$  , id = obj_count, category_  $\\scriptstyle{\\dot{\\mathbf{\\tau}}}{\\mathsf{d}}=\\mathbb{0}$  , bbox  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  [x_min, y_min, x_max  -  x_min, y_max  -  y_min], area  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  (x_max  -  x_min)  \\*  (y_max  -  y_min), segmentation  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  [poly], iscrowd  $\\scriptstyle\\cdot=\\!0$  ) annotations . append(data_anno) obj_count    $\\mathbf{\\varepsilon}+=\\mathbf{\\varepsilon}^{1}$  coco format json  $=$   dict ( images  $=$  images, ", "page_idx": 39, "bbox": [72, 144.66000366210938, 464.3182373046875, 705.2070922851562], "page_size": [612.0, 792.0]}
{"layout": 394, "type": "table", "page_idx": 40, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_32.jpg", "bbox": [68, 81, 541, 125], "page_size": [612.0, 792.0], "ocr_text": "annotations=annotations,\ncategories=[{'id':0, 'name': 'balloon'}])\nmmcv.dump(coco_format_json, out_file)\n", "vlm_text": "The image shows a snippet of Python code formatted to appear as if it is within a table. The code processes data into the COCO (Common Objects in Context) format, which is commonly used for object detection tasks in computer vision. Here's a breakdown of the contents:\n\n1. `annotations=annotations,` - A parameter setting where the variable `annotations` is assigned to the key `annotations`.\n\n2. `categories=[{'id':0, 'name': 'balloon'}]` - This line defines a list with a dictionary describing a category for the dataset. The category has an ID of 0 and a name 'balloon'.\n\n3. `mmcv.dump(coco_format_json, out_file)` - This method likely writes or saves the JSON content formatted in COCO structure to an output file.\n\nThis snippet seems to be part of a larger script that converts or manipulates data related to object detection, specifically involving a category for 'balloon'."}
{"layout": 395, "type": "text", "text": "Using the function above, users can successfully convert the annotation file into json format, then we can use Coco Data set  to train and evaluate the model. ", "page_idx": 40, "bbox": [71, 134.16847229003906, 540, 159.4344940185547], "page_size": [612.0, 792.0]}
{"layout": 396, "type": "text", "text": "6.2 Prepare a config ", "text_level": 1, "page_idx": 40, "bbox": [70, 183, 214, 199], "page_size": [612.0, 792.0]}
{"layout": 397, "type": "text", "text": "The second step is to prepare a config thus the dataset could be successfully loaded. Assume that we want to use Mask R-CNN with FPN, the config to train the detector on balloon dataset is as below. Assume the config is under directory configs/balloon/  and named as  mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py , the config is as below.\n\n ", "page_idx": 40, "bbox": [71, 213.99549865722656, 540, 263.17156982421875], "page_size": [612.0, 792.0]}
{"layout": 398, "type": "text", "text": "# The new config inherits a base config to highlight the necessary modification\n\n _base_  $=$   ' mask_rcnn/mask r cnn r 50 caff e fp n m strain-poly 1 x coco.py '\n\n # We also need to change the num classes in head to match the dataset ' s annotation model  $=$   dict ( roi_head  $\\overbar{\\;-\\;}$  dict ( bbox_head = dict (num classes  $_{:=1}$  ), mask_head = dict (num classes  $^{=1}$  ))) # Modify dataset related settings data set type  $=$   ' COCO Data set ' classes $=$  ('balloon',)data  $=$   dict ( train  $\\overbar{\\ }$  dict ( img_prefix  $\\c=\"$  ' balloon/train/ ' , classes  $,=$  classes, ann_file  $\\equiv^{1}$  ' balloon/train/annotation coco.json ' ), val  $\\mathbf{\\beta}\\!=$  dict ( img_prefix  $\\c=\"$  ' balloon/val/ ' , classes  $,=$  classes, ann_file  $\\equiv^{\\dagger}$  balloon/val/annotation coco.json ' ), test  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  dict ( img_prefix  $\\c=\"$  ' balloon/val/ ' , classes  $,=$  classes, ann_file  $\\equiv$  ' balloon/val/annotation coco.json ' )) # We can use the pre-trained Mask RCNN model to obtain higher performance ", "page_idx": 40, "bbox": [71, 272.90472412109375, 497.4535217285156, 593.7520141601562], "page_size": [612.0, 792.0]}
{"layout": 399, "type": "text", "text": "load_from  $=$   ' checkpoints/mask r cnn r 50 caff e fp n m strain-poly 3 x coco b box mAP-0.408__\n\n  $\\hookrightarrow$  segm_mAP-0.37_20200504_163245-42aa3d00.pth '\n\n → ", "page_idx": 40, "bbox": [71, 595, 521.8120727539062, 620.5411376953125], "page_size": [612.0, 792.0]}
{"layout": 400, "type": "text", "text": "6.3 Train a new model ", "text_level": 1, "page_idx": 41, "bbox": [70, 72, 225, 87], "page_size": [612.0, 792.0]}
{"layout": 401, "type": "text", "text": "To train a model with the new config, you can simply run ", "page_idx": 41, "bbox": [72, 102.99446105957031, 299.4759521484375, 116.30449676513672], "page_size": [612.0, 792.0]}
{"layout": 402, "type": "text", "text": "python tools/train.py configs/balloon/mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py ", "page_idx": 41, "bbox": [72, 126.03773498535156, 532, 136.05014038085938], "page_size": [612.0, 792.0]}
{"layout": 403, "type": "text", "text": "For more detailed usages, please refer to the  Case 1 . ", "page_idx": 41, "bbox": [72, 148.47398376464844, 279, 161.93345642089844], "page_size": [612.0, 792.0]}
{"layout": 404, "type": "text", "text": "6.4 Test and inference ", "text_level": 1, "page_idx": 41, "bbox": [70, 186, 227, 200], "page_size": [612.0, 792.0]}
{"layout": 405, "type": "text", "text": "To test the trained model, you can simply run ", "page_idx": 41, "bbox": [72, 216.4944610595703, 251.75509643554688, 229.8044891357422], "page_size": [612.0, 792.0]}
{"layout": 406, "type": "text", "text": "python tools/test.py configs/balloon/mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py ␣\n\n  $\\hookrightarrow$  work_dirs/mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py/latest.pth --eval bbox ␣\n\n →\n\n  $\\hookrightarrow$  segm\n\n → ", "page_idx": 41, "bbox": [72, 239.53773498535156, 532, 276.3403625488281], "page_size": [612.0, 792.0]}
{"layout": 407, "type": "text", "text": "For more detailed usages, please refer to the  Case 1 . ", "page_idx": 41, "bbox": [72, 285.884033203125, 279, 299.343505859375], "page_size": [612.0, 792.0]}
{"layout": 408, "type": "text", "text": "3: TRAIN WITH CUSTOMIZED MODELS AND STANDARD DATASETS ", "text_level": 1, "page_idx": 42, "bbox": [75, 162, 541, 182], "page_size": [612.0, 792.0]}
{"layout": 409, "type": "text", "text": "In this note, you will know how to train, test and inference your own customized models under standard datasets. We use the cityscapes dataset to train a customized Cascade Mask R-CNN R50 model as an example to demonstrate the whole process, which using  AugFPN  to replace the default  FPN  as neck, and add  Rotate  or  Translate  as training-time auto augmentation. ", "page_idx": 42, "bbox": [72, 225.87342834472656, 540, 275.04852294921875], "page_size": [612.0, 792.0]}
{"layout": 410, "type": "text", "text": "The basic steps are as below: ", "page_idx": 42, "bbox": [72, 279.6715087890625, 187.17764282226562, 292.9815368652344], "page_size": [612.0, 792.0]}
{"layout": 411, "type": "text", "text": "1. Prepare the standard dataset 2. Prepare your own customized model 3. Prepare a config 4. Train, test, and inference models on the standard dataset. ", "page_idx": 42, "bbox": [84, 297.6045227050781, 322.3598937988281, 364.71258544921875], "page_size": [612.0, 792.0]}
{"layout": 412, "type": "text", "text": "7.1 Prepare the standard dataset ", "text_level": 1, "page_idx": 42, "bbox": [71, 388, 298, 403], "page_size": [612.0, 792.0]}
{"layout": 413, "type": "text", "text": "In this note, as we use the standard cityscapes dataset as an example. ", "page_idx": 42, "bbox": [72, 419.2745361328125, 344.76611328125, 432.5845642089844], "page_size": [612.0, 792.0]}
{"layout": 414, "type": "text", "text": "It is recommended to symlink the dataset root to    $\\S$  MM DETECTION/data . If your folder structure is different, you may need to change the corresponding paths in config files. ", "page_idx": 42, "bbox": [72, 437.20654296875, 540, 462.4715576171875], "page_size": [612.0, 792.0]}
{"layout": 415, "type": "table", "page_idx": 42, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_33.jpg", "bbox": [68, 467, 545, 705], "page_size": [612.0, 792.0], "ocr_text": "mmdetection\n\nL— mmdet\nL— tools\nt— configs\nL— data\n— coco\nt— annotations\nt— train2017\nt— val2017\nr— test2017\n+— cityscapes\n— annotations\nt— leftImg8bit\nH— train\nt— val\nt— gtFine\nL— train\nt— val\nL— VOCdevkit\n\n", "vlm_text": "The table represents a directory structure related to a project, likely involving deep learning and computer vision, particularly for object detection tasks. The hierarchy is displayed as follows:\n\n- `mmdetection`: The root directory, likely referring to the MMDetection open-source toolbox for object detection.\n\n  - `mmdet`: A directory that may contain MMDetection-related modules or code.\n  \n  - `tools`: A directory likely containing scripts or tools for facilitating various tasks such as training, testing, or data processing.\n  \n  - `configs`: A directory likely containing configuration files for different experiments or model configurations.\n  \n  - `data`: A directory designated for storing datasets.\n  \n    - `coco`: A subdirectory for the COCO dataset. Within this, there are further subdirectories for:\n    \n      - `annotations`: This likely contains annotation files for the COCO dataset.\n      \n      - `train2017`: This likely contains the training images for the 2017 version of the COCO dataset.\n      \n      - `val2017`: This likely contains the validation images for the COCO dataset, 2017 version.\n      \n      - `test2017`: This likely contains test images for the 2017 version of the COCO dataset.\n\n    - `cityscapes`: A subdirectory for the Cityscapes dataset. It contains:\n    \n      - `annotations`: Likely contains annotation files for the Cityscapes dataset.\n      \n      - `leftImg8bit`: Subdirectory containing 8-bit left view images, with its own subdirectories for:\n      \n        - `train`: Likely contains training images.\n        \n        - `val`: Likely contains validation images.\n        \n      - `gtFine`: A subdirectory likely containing fine-grained annotations, with:\n      \n        - `train`: Likely contains training annotations.\n        \n        - `val`: Likely contains validation annotations.\n\n    - `VOCdevkit`: A directory that likely contains data related to the PASCAL VOC dataset, commonly used for object detection tasks.\n\nThis structure is useful for organizing data and configurations needed for training and evaluating object detection models."}
{"layout": 416, "type": "table", "page_idx": 43, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_34.jpg", "bbox": [69, 84, 541, 113], "page_size": [612.0, 792.0], "ocr_text": "VOC2007\nVOC2012\n", "vlm_text": "The table includes two entries: VOC2007 and VOC2012, arranged in a hierarchical or tree-like structure."}
{"layout": 417, "type": "text", "text": "The cityscapes annotations have to be converted into the coco format using  tools/data set converters/ cityscapes.py : ", "page_idx": 43, "bbox": [72, 122.21345520019531, 540, 147.47850036621094], "page_size": [612.0, 792.0]}
{"layout": 418, "type": "table", "page_idx": 43, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_35.jpg", "bbox": [68, 154, 542, 195], "page_size": [612.0, 792.0], "ocr_text": "pip install cityscapesscripts\npython tools/dataset_converters/cityscapes.py ./data/cityscapes --nproc 8 --out-dir ./\n—data/cityscapes/annotations\n", "vlm_text": "The image is not a table, but rather shows code snippets.\n\n1. The first line is a command to install a Python package:\n   ```\n   pip install cityscapescripts\n   ```\n\n2. The second line is a Python command to run a script that converts dataset formats:\n   ```\n   python tools/dataset_converters/cityscapes.py ./data/cityscapes --nproc 8 --out-dir ./data/cityscapes/annotations\n   ```\n\n   - `python tools/dataset_converters/cityscapes.py` is the script being executed.\n   - `./data/cityscapes` specifies the dataset directory.\n   - `--nproc 8` sets the number of processing threads to 8.\n   - `--out-dir ./data/cityscapes/annotations` specifies the output directory for annotations."}
{"layout": 419, "type": "text", "text": "Currently the config files in  cityscapes  use COCO pre-trained weights to initialize. You could download the pre- trained models in advance if network is unavailable or slow, otherwise it would cause errors at the beginning of training. ", "page_idx": 43, "bbox": [72, 203.7084503173828, 540, 228.97349548339844], "page_size": [612.0, 792.0]}
{"layout": 420, "type": "text", "text": "7.2 Prepare your own customized model ", "text_level": 1, "page_idx": 43, "bbox": [71, 252, 350, 267], "page_size": [612.0, 792.0]}
{"layout": 421, "type": "text", "text": "The second step is to use your own module or training setting. Assume that we want to implement a new neck called AugFPN  to replace with the default  FPN  under the existing detector Cascade Mask R-CNN R50. The following imple- ments AugFPN  under MM Detection. ", "page_idx": 43, "bbox": [72, 283.5344543457031, 540, 320.7554626464844], "page_size": [612.0, 792.0]}
{"layout": 422, "type": "text", "text": "7.2.1 1. Define a new neck (e.g. AugFPN) ", "text_level": 1, "page_idx": 43, "bbox": [71, 341, 307, 354], "page_size": [612.0, 792.0]}
{"layout": 423, "type": "text", "text": "Firstly create a new file  mmdet/models/necks/augfpn.py ", "page_idx": 43, "bbox": [72, 365.6734313964844, 313.6933288574219, 378.98345947265625], "page_size": [612.0, 792.0]}
{"layout": 424, "type": "table", "page_idx": 43, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_36.jpg", "bbox": [68, 384, 543, 595], "page_size": [612.0, 792.0], "ocr_text": "from ..builder import NECKS\n\n@NECKS .register_module()\nclass AugFPN(nn.Module):\n\ndef __init__(self,\nin_channels,\nout_channels,\nnum_outs,\nstart_level=0,\nend_level=-1,\nadd_extra_convs=False):\n\npass\n\ndef forward(self, inputs):\n# implementation is ignored\npass\n\n", "vlm_text": "The image you provided is not a table; it is a snippet of Python code. Here's a summary of the code:\n\n1. The code snippet imports `NECKS` from a module named `builder` that is located in the parent directory.\n\n2. It defines a new class named `AugFPN`, which inherits from `nn.Module`. This suggests that `AugFPN` is part of a neural network implementation, possibly related to feature pyramid networks (FPN).\n\n3. The `AugFPN` class is decorated with `@NECKS.register_module()`, which likely means that it is being registered as a module in the NECKS registry for some framework.\n\n4. The `__init__` method initializes the class with the following parameters:\n   - `in_channels`\n   - `out_channels`\n   - `num_outs`\n   - `start_level` with a default value of 0\n   - `end_level` with a default value of -1\n   - `add_extra_convs` with a default value of `False`\n   This method currently contains a `pass` statement, indicating that the body of the method is not implemented.\n\n5. The `forward` method defines the forward pass of the network, accepting `inputs` as an argument. It also contains a `pass` statement and a comment saying \"# implementation is ignored\", suggesting that the actual implementation of the forward pass is missing.\n\nOverall, the code defines the structure for a class used in neural network modeling, but it does not implement any functionality yet."}
{"layout": 425, "type": "text", "text": "7.2.2 2. Import the module ", "text_level": 1, "page_idx": 44, "bbox": [71, 71, 224, 85], "page_size": [612.0, 792.0]}
{"layout": 426, "type": "text", "text": "You can either add the following line to  mmdet/models/necks/__init__.py , ", "page_idx": 44, "bbox": [72, 95.81745910644531, 391, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 427, "type": "table", "page_idx": 44, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_37.jpg", "bbox": [67, 112, 545, 203], "page_size": [612.0, 792.0], "ocr_text": "from .augfpn import AugFPN\n\nor alternatively add\n\ncustom_imports = dict(\nimports=['mmdet.models.necks.augfpn.py'],\nallow_failed_imports=False)\n\n", "vlm_text": "The table contains Python code snippets related to importing a module named `AugFPN`. The first snippet shows a direct import statement:\n\n```python\nfrom .augfpn import AugFPN\n```\n\nThe second snippet provides an alternative method for importing by adding entries to a `custom_imports` dictionary. This dictionary specifies a path to import from and an option to not allow failed imports:\n\n```python\ncustom_imports = dict(\n    imports=['mmdet.models.necks.augfpn.py'],\n    allow_failed_imports=False\n)\n```"}
{"layout": 428, "type": "text", "text": "to the config file and avoid modifying the original code. ", "page_idx": 44, "bbox": [72, 209.3914337158203, 296, 222.7014617919922], "page_size": [612.0, 792.0]}
{"layout": 429, "type": "text", "text": "7.2.3 3. Modify the config file ", "text_level": 1, "page_idx": 44, "bbox": [71, 242, 239, 256], "page_size": [612.0, 792.0]}
{"layout": 430, "type": "table", "page_idx": 44, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_38.jpg", "bbox": [68, 267, 543, 334], "page_size": [612.0, 792.0], "ocr_text": "neck=dict(\ntype='AugFPN',\nin_channels=[256, 512, 1024, 2048],\nout_channels=256,\nnum_outs=5)\n\n", "vlm_text": "The table contains a Python dictionary setup for a neural network component named `neck`. Here's the breakdown:\n\n- `type`: 'AugFPN', indicating the type of feature pyramid network.\n- `in_channels`: [256, 512, 1024, 2048], specifying input channel sizes.\n- `out_channels`: 256, defining the output channel size.\n- `num_outs`: 5, indicating the number of output feature maps."}
{"layout": 431, "type": "text", "text": "For more detailed usages about customize your own models (e.g. implement a new backbone, head, loss, etc) and runtime training settings (e.g. define a new optimizer, use gradient clip, customize training schedules and hooks, etc), please refer to the guideline  Customize Models  and  Customize Runtime Settings  respectively. ", "page_idx": 44, "bbox": [72, 341.6534729003906, 540, 378.87347412109375], "page_size": [612.0, 792.0]}
{"layout": 432, "type": "text", "text": "7.3 Prepare a config ", "text_level": 1, "page_idx": 44, "bbox": [71, 400, 213, 418], "page_size": [612.0, 792.0]}
{"layout": 433, "type": "text", "text": "The third step is to prepare a config for your own training setting. Assume that we want to add AugFPN  and  Rotate  or  Translate  augmentation to existing Cascade Mask R-CNN R50 to train the cityscapes dataset, and assume the config is under directory configs/cityscapes/ and named as cascade mask r cnn r 50 aug fp n auto aug 10 e cityscape s.py , the config is as below.\n\n ", "page_idx": 44, "bbox": [72, 432.74945068359375, 540, 481.9244384765625], "page_size": [612.0, 792.0]}
{"layout": 434, "type": "text", "text": "# The new config inherits the base configs to highlight the necessary modification\n\n _base_  $\\begin{array}{r l}{\\mathbf{\\eta}=}&{{}\\mathsf{[}\\mathbf{\\eta}}\\end{array}$  ' ../_base_/models/cascade mask r cnn r 50 fp n.py ' , ' ../_base_/datasets/cityscape s instance.py ' ,  ' ../_base_/default runtime.py '\n\n ] model  $=$   dict ( # set None to avoid loading ImageNet pretrained backbone, # instead here we set  \\` load_from \\`  to load from COCO pretrained detectors. backbone  $=$  dict (init_cfg  $=$  None ), # replace neck from defaultly  \\` FPN \\`  to our new implemented module  \\` AugFPN \\` neck  $\\fallingdotseq$  dict ( type $\\equiv^{\\dagger}$ AugFPN',in channels  $=$  [ 256 ,  512 ,  1024 ,  2048 ], out channel  $\\mathsf{s}\\!\\!=\\!\\!256$  , num_outs  ${=}5$  ), # We also need to change the num classes in head from 80 to 8, to match the # cityscapes dataset ' s annotation. This modification involves  \\` bbox_head \\`  and  \\` mask_ head \\` . ˓ → ", "page_idx": 44, "bbox": [72, 490.8607177734375, 523.0968017578125, 718.9462890625], "page_size": [612.0, 792.0]}
{"layout": 435, "type": "table", "page_idx": 45, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_39.jpg", "bbox": [70, 81, 545, 712.75], "page_size": [612.0, 792.0], "ocr_text": "roi_head=dict(\nbbox_head=[\ndict(\ntype='Shared2FCBBoxHead',\nin_channels=256,\nfc_out_channels=1024,\nroi_feat_size=7,\n# change the number of classes from defaultly COCO to cityscapes\nnum_classes=8,\nbbox_coder=dict (\ntype='DeltaXYWHBBoxCoder',\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.1, 0.1, 0.2, 0.2]),\nreg_class_agnostic=True,\nloss_cls=dict(\ntype='CrossEntropyLoss',\nuse_sigmoid=False,\nloss_weight=1.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0,\nloss_weight=1.0)),\ndict(\ntype='Shared2FCBBoxHead',\nin_channels=256,\nfc_out_channels=1024,\nroi_feat_size=7,\n# change the number of classes from defaultly COCO to cityscapes\nnum_classes=8,\nbbox_coder=dict (\ntype='DeltaXYWHBBoxCoder',\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.05, 0.05, 0.1, 0.1]),\nreg_class_agnostic=True,\nloss_cls=dict(\ntype='CrossEntropyLoss',\nuse_sigmoid=False,\nloss_weight=1.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0,\nloss_weight=1.9)),\ndict(\ntype='Shared2FCBBoxHead',\nin_channels=256,\nfc_out_channels=1024,\nroi_feat_size=7,\n# change the number of classes from defaultly COCO to cityscapes\nnum_classes=8,\nbbox_coder=dict (\ntype='DeltaXYWHBBoxCoder',\ntarget_means=[0., 9., 0., 0.],\ntarget_stds=[0.033, 0.033, 0.067, 0.067]),\nreg_class_agnostic=True,\nloss_cls=dict(\ntype='CrossEntropyLoss',\n\n", "vlm_text": "The table contains code snippets for setting up configuration dictionaries, likely for a machine learning model. Here are the main components:\n\n1. **Model Settings**:\n   - **type**: 'Shared2FCBBoxHead'\n   - **in_channels**: 256\n   - **fc_out_channels**: 1024\n   - **roi_feat_size**: 7\n   - **num_classes**: 8 (indicating a change from default COCO to Cityscapes)\n\n2. **Bounding Box Coder Settings**:\n   - **type**: 'DeltaXYWHBBoxCoder'\n   - **target_means**: [0.0, 0.0, 0.0, 0.0]\n   - **target_stds**: Different sets for different configurations:\n     - [0.1, 0.1, 0.2, 0.2]\n     - [0.05, 0.05, 0.1, 0.1]\n     - [0.033, 0.033, 0.067, 0.067]\n\n3. **Classification and Bounding Box Loss**:\n   - **reg_class_agnostic**: True\n   - **loss_cls**: CrossEntropyLoss\n     - **use_sigmoid**: False\n     - **loss_weight**: 1.0\n   - **loss_bbox**: SmoothL1Loss\n     - **beta**: 1.0\n     - **loss_weight**: 1.0\n\nThese configurations are used in object detection models to define the architecture and loss functions for training."}
{"layout": 436, "type": "text", "text": "(continues on next page) ", "page_idx": 45, "bbox": [461.9010009765625, 709.8545532226562, 540.0, 720.5026245117188], "page_size": [612.0, 792.0]}
{"layout": 437, "type": "text", "text": "use s igm oid = False , loss weight  $\\scriptstyle=1.\\,\\mathbb{O}.$  ), loss_bbox  $\\fallingdotseq$  dict ( type  $\\equiv^{\\dagger}$  Smooth L 1 Loss ' , beta  $\\scriptstyle=1.\\,\\mathbb{0}$  , loss weight  $\\,=\\!1\\,.\\,\\mathbb{O}.$  )) ], mask_head = dict ( type  $\\equiv^{\\dagger}$  FC N Mask Head ' , num_convs  ${=}4$  , in channels  $\\bullet{=}256$  , con v out channel  $s{=}256$  , # change the number of classes from defaultly COCO to cityscapes num classes  $^{=8}$  , loss_mask  $\\fallingdotseq$  dict ( type  $\\equiv^{\\dagger}$  Cross Entropy Loss ' , use_mask  $\\risingdotseq$  True , loss weight  $\\scriptstyle=1.\\,\\mathbb{O}$  )))) # over-write  \\` train pipeline \\`  for new added  \\` Auto Augment \\`  training setting img norm cf g  $=$   dict ( mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ 123.675 ,  116.28 ,  103.53 ], std  $\\overbar{\\;-\\;}$  [ 58.395 ,  57.12 ,  57.375 ], to_rgb  $\\risingdotseq$  True ) train pipeline  $=$   [ dict ( type  $=\"$  Load Image From File ' ), dict ( type  $\\risingdotseq$  Load Annotations ' , with_bbox  $\\fallingdotseq$  True , with_mask  $\\risingdotseq$  True ), dict ( type  $\\equiv^{1}$  ' Auto Augment ' , policies  $=$  [ [ dict ( type  $\\risingdotseq$  Rotate ' , level  ${\\tt=}5$  , img fill val  $\\mathbf{\\beta}\\!=$  ( 124 ,  116 ,  104 ), prob  $\\scriptstyle{\\mathfrak{s}}=\\mathbb{0}\\,.\\,5$  , scal  ${\\mathsf{e}}{\\mathsf{=}}{\\mathsf{1}}.$  ) ], [ dict ( type  $\\equiv^{\\dagger}$  Rotate ' , level  ${}^{=7}$  , img fill val  $\\mathbf{\\beta}\\!=$  ( 124 ,  116 ,  104 )), dict ( type  $\\risingdotseq$  Translate ' , level  ${\\tt=}5$  , prob  $\\scriptstyle{\\mathfrak{s}}=\\mathbb{0}\\,.\\,5$  , img fill val  $\\mathbf{\\beta}\\!=$  ( 124 ,  116 ,  104 )) ], ]), dict ( type  $\\equiv^{\\dagger}$  Resize ' , img_scale  $=$  [( 2048 ,  800 ), ( 2048 ,  1024 )], keep_ratio  $\\risingdotseq$  True ), dict ( type  $\\risingdotseq$  RandomFlip ' , flip_ratio  $\\scriptstyle{\\mathfrak{s}}=\\emptyset$  .5 ), dict ( type  $\\risingdotseq$  Normalize ' ,  \\*\\* img norm cf g), dict ( type  $\\scriptstyle{\\varepsilon}$  ' Pad ' , size divisor  $=\\!32$  ), dict ( type  $\\risingdotseq$  Default Format Bundle ' ), dict ( type  $\\circeq$  ' Collect ' , keys  $,=$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ,  ' gt_masks ' ]),\n\n ]\n\n # set batch_size per gpu, and set new training pipeline data  $=$   dict ( samples per gpu  $_{=1}$  , workers per gpu  $^{=3}$  , # over-write  \\` pipeline \\`  with new training pipeline setting ", "page_idx": 46, "bbox": [71, 87.07498931884766, 485, 708.3753662109375], "page_size": [612.0, 792.0]}
{"layout": 438, "type": "image", "page_idx": 47, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_40.jpg", "bbox": [69, 81, 543, 306], "page_size": [612.0, 792.0], "ocr_text": "train=dict (dataset=dict (pipeline=train_pipeline)))\n\n# Set optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# Set customized learning policy\nlr_config = dict(\npolicy='step',\nwarmup='linear',\nwarmup_iters=500,\nwarmup_ratio=0.001,\nstep=[8])\nrunner = dict(type='EpochBasedRunner', max_epochs=10)\n\n# We can use the COCO pretrained Cascade Mask R-CNN R50 model for more stable,\nperformance initialization\n\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_\norcnn_r50_fpn_1x_coco/cascade_mask_rcnn_r50_fpn_1x_coco_20200203-9d4dcb24.pth'\n\n", "vlm_text": "The image contains a snippet of Python code, which appears to be a configuration setup for training a machine learning model, likely in a deep learning framework such as PyTorch or TensorFlow.\n\nKey elements of the code:\n\n1. **Optimizer Configuration:**\n   - The optimizer used is SGD (Stochastic Gradient Descent) with a learning rate (`lr`) of 0.01, a momentum of 0.9, and a weight decay of 0.0001.\n   - `optimizer_config` sets `grad_clip` to `None`.\n\n2. **Learning Rate Policy:**\n   - A step policy is employed for learning rate adjustment.\n   - Specifies a linear warmup strategy with 500 iterations (`warmup_iters`) and a warmup ratio of 0.001.\n   - The learning rate decays at step [8].\n\n3. **Runner Configuration:**\n   - Uses an `EpochBasedRunner` with a maximum of 10 epochs.\n\n4. **Loading Pretrained Models:**\n   - A pretrained Cascade Mask R-CNN R50 model is loaded from a COCO dataset URL for stable performance initialization. The model file seems to be stored at the specified URL.\n\n5. **Training Pipeline:**\n   - A dictionary setup for training data input, referred to as `train_pipeline`.\n\nThe code seems to be part of a larger script or configuration for training an object detection model."}
{"layout": 439, "type": "text", "text": "7.4 Train a new model ", "text_level": 1, "page_idx": 47, "bbox": [71, 332, 224, 348], "page_size": [612.0, 792.0]}
{"layout": 440, "type": "text", "text": "To train a model with the new config, you can simply run ", "page_idx": 47, "bbox": [72, 363.4354553222656, 299.4759521484375, 376.7454833984375], "page_size": [612.0, 792.0]}
{"layout": 441, "type": "text", "text": "python tools/train.py configs/cityscapes/cascade mask r cnn r 50 aug fp n auto aug 10 e\n\n  $\\hookrightarrow$  cityscapes.py\n\n → ", "page_idx": 47, "bbox": [72, 386.4787292480469, 500.890380859375, 411.3253479003906], "page_size": [612.0, 792.0]}
{"layout": 442, "type": "text", "text": "For more detailed usages, please refer to the  Case 1 . ", "page_idx": 47, "bbox": [72, 420.8700256347656, 279.5416564941406, 434.3294982910156], "page_size": [612.0, 792.0]}
{"layout": 443, "type": "text", "text": "7.5 Test and inference ", "text_level": 1, "page_idx": 47, "bbox": [71, 457, 226, 473], "page_size": [612.0, 792.0]}
{"layout": 444, "type": "text", "text": "To test the trained model, you can simply run ", "page_idx": 47, "bbox": [72, 488.8914794921875, 251.75509643554688, 502.2015075683594], "page_size": [612.0, 792.0]}
{"layout": 445, "type": "text", "text": "python tools/test.py configs/cityscapes/cascade mask r cnn r 50 aug fp n auto aug 10 e\n\n  $\\hookrightarrow$  cityscapes.py work_dirs/cascade mask r cnn r 50 aug fp n auto aug 10 e cityscape s.py/latest.\n\n →\n\n  $\\hookrightarrow$  pth --eval bbox segm\n\n → ", "page_idx": 47, "bbox": [72, 511.93475341796875, 532.2728271484375, 548.736328125], "page_size": [612.0, 792.0]}
{"layout": 446, "type": "text", "text": "TUTORIAL 1: LEARN ABOUT CONFIGS ", "text_level": 1, "page_idx": 48, "bbox": [272, 163, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 447, "type": "text", "text": "We incorporate modular and inheritance design into our config system, which is convenient to conduct various exper- iments. If you wish to inspect the config file, you may run  python tools/misc/print config.py /PATH/TO/ CONFIG  to see the complete config. ", "page_idx": 48, "bbox": [71, 225.87342834472656, 541, 263.093505859375], "page_size": [612.0, 792.0]}
{"layout": 448, "type": "text", "text": "8.1 Modify config through script arguments ", "text_level": 1, "page_idx": 48, "bbox": [71, 285, 371, 302], "page_size": [612.0, 792.0]}
{"layout": 449, "type": "text", "text": "When submitting jobs using “tools/train.py” or “tools/test.py”, you may specify  --cfg-options  to in-place modify the config. ", "page_idx": 48, "bbox": [71, 317.65545654296875, 541, 342.92047119140625], "page_size": [612.0, 792.0]}
{"layout": 450, "type": "text", "text": "• Update config keys of dict chains. ", "page_idx": 48, "bbox": [88, 347.54345703125, 231.29248046875, 360.8534851074219], "page_size": [612.0, 792.0]}
{"layout": 451, "type": "text", "text": "The config options can be specified following the order of the dict keys in the original config. For example, --cfg-options model.backbone.norm_eval  $\\b=$  False  changes the all BN modules in model backbones to train  mode. ", "page_idx": 48, "bbox": [96, 365.4754638671875, 541, 402.69647216796875], "page_size": [612.0, 792.0]}
{"layout": 452, "type": "text", "text": "• Update keys inside a list of configs. ", "page_idx": 48, "bbox": [88, 407.3184509277344, 237.6685791015625, 420.62847900390625], "page_size": [612.0, 792.0]}
{"layout": 453, "type": "text", "text": "Some config dicts are composed as a list in your config. For example, the training pipeline  data.train. pipeline  is normally a list e.g. [dict(type  $:=$  ' Load Image From File ' ), ...] . If you want to change ' Load Image From File '  to  ' Load Image From Webcam '  in the pipeline, you may specify  --cfg-options data.train.pipeline.0.type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  Load Image From Webcam . ", "page_idx": 48, "bbox": [96, 425.25146484375, 541, 474.4274597167969], "page_size": [612.0, 792.0]}
{"layout": 454, "type": "text", "text": "• Update values of list/tuples. ", "page_idx": 48, "bbox": [88, 479.0494384765625, 207.45205688476562, 492.3594665527344], "page_size": [612.0, 792.0]}
{"layout": 455, "type": "text", "text": "If the value to be updated is a list or a tuple. For example, the config file normally sets  workflow  $\\leftleftarrows$  [( ' train ' , 1)] . If you want to change this key, you may specify  --cfg-options workflow=\"[(train,1),(val,1)]\" . Note that the quotation mark “ is necessary to support list/tuple data types, and that  NO  white space is allowed inside the quotation marks in the specified value. ", "page_idx": 48, "bbox": [96, 496.9824523925781, 541, 546.158447265625], "page_size": [612.0, 792.0]}
{"layout": 456, "type": "text", "text": "8.2 Config File Structure ", "text_level": 1, "page_idx": 48, "bbox": [70, 568, 242, 585], "page_size": [612.0, 792.0]}
{"layout": 457, "type": "text", "text": "There are 4 basic component types under  config/_base_ , dataset, model, schedule, default runtime. Many methods could be easily constructed with one of each like Faster R-CNN, Mask R-CNN, Cascade R-CNN, RPN, SSD. The configs that are composed by components from  _base_  are called  primitive . ", "page_idx": 48, "bbox": [71, 600.7194213867188, 541, 637.9404296875], "page_size": [612.0, 792.0]}
{"layout": 458, "type": "text", "text": "For all configs under the same folder, it is recommended to have only  one  primitive  config. All other configs should inherit from the  primitive  config. In this way, the maximum of inheritance level is 3. ", "page_idx": 48, "bbox": [71, 641.9347534179688, 541, 667.8284301757812], "page_size": [612.0, 792.0]}
{"layout": 459, "type": "text", "text": "For easy understanding, we recommend contributors to inherit from existing methods. For example, if some modifica- tion is made base on Faster R-CNN, user may first inherit the basic Faster R-CNN structure by specifying  _base_  $=$  ../faster r cnn/faster r cnn r 50 fp n 1 x coco.py , then modify the necessary fields in the config files. ", "page_idx": 48, "bbox": [71, 672.4503784179688, 541, 709.6714477539062], "page_size": [612.0, 792.0]}
{"layout": 460, "type": "text", "text": "If you are building an entirely new method that does not share the structure with any of the existing methods, you may create a folder  xxx_rcnn  under  configs , ", "page_idx": 49, "bbox": [71, 71.45246887207031, 540, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 461, "type": "text", "text": "Please refer to  mmcv  for detailed documentation. ", "page_idx": 49, "bbox": [71, 101.34046936035156, 267.9742736816406, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 462, "type": "text", "text": "8.3 Config Name Style ", "text_level": 1, "page_idx": 49, "bbox": [70, 138, 228, 154], "page_size": [612.0, 792.0]}
{"layout": 463, "type": "text", "text": "We follow the below style to name config files. Contributors are advised to follow the same style.\n\n ", "page_idx": 49, "bbox": [71, 169.2115020751953, 458.64837646484375, 182.5215301513672], "page_size": [612.0, 792.0]}
{"layout": 464, "type": "text", "text": "{model}_[model setting]_{backbone}_{neck}_[norm setting]_[misc]_[gpu x batch per gpu]_\n\n  $\\hookrightarrow$  {schedule}_{dataset}\n\n →\n\n ", "page_idx": 49, "bbox": [71, 192.2547149658203, 521.8118286132812, 217.10231018066406], "page_size": [612.0, 792.0]}
{"layout": 465, "type": "text", "text": " $\\{{\\bf x x x}\\}$   is required field and  [yyy]  is optional. ", "page_idx": 49, "bbox": [71, 226.7954864501953, 255.56175231933594, 240.1055145263672], "page_size": [612.0, 792.0]}
{"layout": 466, "type": "text", "text": "•  {model} : model type like  faster r cnn ,  mask_rcnn , etc. •  [model setting] : specific setting for some model, like  without semantic  for  htc ,  moment  for  reppoints , etc. •  {backbone} : backbone type like  r50  (ResNet-50),  x101  (ResNeXt-101). •  {neck} : neck type like  fpn ,  pafpn ,  nasfpn ,  c4 . •  [norm setting] :  bn  (Batch Normalization) is used unless specified, other norm layer type could be  gn  (Group Normalization),  syncbn  (Synchronized Batch Normalization).  gn-head / gn-neck  indicates GN is applied in head/neck only, while  gn-all  means GN is applied in the entire model, e.g. backbone, neck, head. •  [misc] : miscellaneous setting/plugins of model, e.g.  dconv ,  gcb ,  attention ,  albu ,  mstrain . •  [gpu x batch per gpu] : GPUs and samples per GPU,  8x2  is used by default. •  {schedule} : training schedule, options are  1x ,  2x ,  20e , etc.  1x  and    $2\\mathbf{x}$   means 12 epochs and 24 epochs respectively.    $2\\mathbb{o}$   is adopted in cascade models, which denotes 20 epochs. For    $1\\mathbf{x}/2\\mathbf{x}$  , initial learning rate decays by a factor of 10 at the 8/16th and 11/22th epochs. For  20e , initial learning rate decays by a factor of 10 at the 16th and 19th epochs. •  {dataset} : dataset like  coco ,  cityscapes ,  voc_0712 ,  wider_face . ", "page_idx": 49, "bbox": [88, 244.7284698486328, 540, 473.23150634765625], "page_size": [612.0, 792.0]}
{"layout": 467, "type": "text", "text": "8.4 Deprecated train_cfg/test_cfg ", "text_level": 1, "page_idx": 49, "bbox": [70, 496, 301, 513], "page_size": [612.0, 792.0]}
{"layout": 468, "type": "text", "text": "The  train_cfg  and  test_cfg  are deprecated in config file, please specify them in the model config. The original config structure is as below. ", "page_idx": 49, "bbox": [71, 527.7933959960938, 540, 553.0584716796875], "page_size": [612.0, 792.0]}
{"layout": 469, "type": "table", "page_idx": 49, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_41.jpg", "bbox": [67, 557, 543, 649], "page_size": [612.0, 792.0], "ocr_text": "# deprecated\nmodel = dict(\ntype=...,\n\n)\ntrain_cfg=dict(...)\ntest_cfg=dict(...)\n", "vlm_text": "The image appears to contain a code snippet, not a table. The snippet shows the setup of configuration dictionaries for a model, where:\n\n- `model` is assigned a dictionary with a `type` key. The contents are marked as deprecated.\n- `train_cfg` is assigned a dictionary for training configuration.\n- `test_cfg` is assigned a dictionary for testing configuration.\n\nSpecific keys and values in these configurations are not fully visible."}
{"layout": 470, "type": "text", "text": "The migration example is as below. ", "page_idx": 49, "bbox": [71, 657.1084594726562, 212.83128356933594, 670.4185180664062], "page_size": [612.0, 792.0]}
{"layout": 471, "type": "text", "text": "# recommended model  $=$   dict ( type =... , ... train_cfg  $|=$  dict ( ... ), test_cfg  $\\leftrightharpoons$  dict ( ... ), ) ", "page_idx": 50, "bbox": [69, 79.55177307128906, 192.29824829101562, 161.29525756835938], "page_size": [612.0, 792.0]}
{"layout": 472, "type": "text", "text": "8.5 An Example of Mask R-CNN ", "text_level": 1, "page_idx": 50, "bbox": [70, 191, 290, 207], "page_size": [612.0, 792.0]}
{"layout": 473, "type": "text", "text": "To help the users have a basic idea of a complete config and the modules in a modern detection system, we make brief comments on the config of Mask R-CNN using ResNet50 and FPN as the following. For more detailed usage and the corresponding alternative for each modules, please refer to the API documentation.\n\n ", "page_idx": 50, "bbox": [69, 223.1204376220703, 540, 260.34149169921875], "page_size": [612.0, 792.0]}
{"layout": 474, "type": "text", "text": " $\\mathtt{m o d e l\\;=\\;d i c t(}$  type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' MaskRCNN ' , # The name of detector backbone  $=$  dict( # The config of backbone type  $=$  ' ResNet ' , # The type of the backbone, refer to https://github.com/open-\n\n  $\\hookrightarrow$  mmlab/mm detection/blob/master/mmdet/models/backbones/resnet.py#L308 for more details.\n\n → depth  $\\scriptstyle{=50}$  , # The depth of backbone, usually it is 50 or 101 for ResNet and ␣\n\n  $\\hookrightarrow$  ResNext backbones.\n\n → num_stages  ${=}4$  , # Number of stages of the backbone. out indices  $=$  (0, 1, 2, 3), # The index of output feature maps produced in each ␣\n\n  $\\hookrightarrow$  stages\n\n → frozen stages  $_{=1}$  , # The weights in the first 1 stage are fronzen norm_cfg  $=$  dict( # The config of normalization layers. type  $=$  ' BN ' , # Type of norm layer, usually it is BN or GN requires grad  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  True), # Whether to train the gamma and beta in BN norm_eval  $=$  True, # Whether to freeze the statistics in BN style  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' pytorch '  # The style of backbone,  ' pytorch '  means that stride 2 layers ␣\n\n  $\\hookrightarrow$  are in 3x3 conv,  ' caffe '  means stride 2 layers are in 1x1 convs.\n\n → init_cfg  $=$  dict(type  $=$  ' Pretrained ' , checkpoint  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' torch vision://resnet50 ' )), # ␣\n\n  $\\hookrightarrow$  The ImageNet pretrained backbone to be loaded\n\n → neck  $\\fallingdotseq$  dict( type  $=$  ' FPN ' , # The neck of detector is FPN. We also support  ' NASFPN ' ,  ' PAFPN ' , ␣\n\n  $\\hookrightarrow$  etc. Refer to https://github.com/open-mmlab/mm detection/blob/master/mmdet/models/necks/\n\n →\n\n  $\\hookrightarrow$  fpn.py#L10 for more details.\n\n → in channels  $=$  [256, 512, 1024, 2048], # The input channels, this is consistent ␣\n\n  $\\hookrightarrow$  with the output channels of backbone\n\n → out channel  $\\mathfrak{s}{=}256$  , # The output channels of each level of the pyramid feature map num_outs  $_{:=5}$  ), # The number of output scales rpn_head  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( type  $=$  ' RPNHead ' , # The type of RPN head is  ' RPNHead ' , we also support  ' GARPNHead\n\n  $\\hookrightarrow{}^{\\bullet}$  ' , etc. Refer to https://github.com/open-mmlab/mm detection/blob/master/mmdet/models/\n\n  $\\hookrightarrow$  dense heads/rpn_head.py#L12 for more details.\n\n → in_channel  $s{=}256$  , # The input channels of each input feature map, this is ␣\n\n  $\\hookrightarrow$  consistent with the output channels of neck\n\n → feat channel  $s{=}256$  , # Feature channels of convolutional layers in the head. anchor generator  $=$  dict( # The config of anchor generator type  $=$  ' Anchor Generator ' , # Most of methods use Anchor Generator, SSD ␣\n\n  $\\hookrightarrow$  Detectors uses  \\` SSD Anchor Generator \\` . Refer to https://github.com/open-mmlab/\n\n → ", "page_idx": 50, "bbox": [69, 267, 540, 712.5550537109375], "page_size": [612.0, 792.0]}
{"layout": 475, "type": "text", "text": "mm detection/blob/master/mmdet/core/anchor/anchor generator.py#L10 for more details (continues on next page) ", "page_idx": 50, "bbox": [82.46097564697266, 709.8545532226562, 540, 721.6309204101562], "page_size": [612.0, 792.0]}
{"layout": 476, "type": "text", "text": " $\\mathtt{s c a l e s}\\!\\!=\\!\\![8]$  , # Basic scale of the anchor, the area of the anchor in one ␣ position of a feature map will be scale \\* base_sizes ", "page_idx": 51, "bbox": [82, 87, 511, 109.64115142822266], "page_size": [612.0, 792.0]}
{"layout": 477, "type": "text", "text": "ratios  $\\overleftarrow{}$  [0.5, 1.0, 2.0], # The ratio between height and width. ", "page_idx": 51, "bbox": [133, 111.58375549316406, 459, 121.5961685180664], "page_size": [612.0, 792.0]}
{"layout": 478, "type": "text", "text": "strides  $=$  [4, 8, 16, 32, 64]), # The strides of the anchor generator. This is ␣ consistent with the FPN feature strides. The strides will be taken as base_sizes if ␣ base_sizes is not set. ", "page_idx": 51, "bbox": [82, 123.53877258300781, 538, 157.46121215820312], "page_size": [612.0, 792.0]}
{"layout": 479, "type": "text", "text": "bbox_coder  $\\leftleftarrows$  dict( # Config of box coder to encode and decode the boxes during ␣ training and testing ", "page_idx": 51, "bbox": [82, 159.40382385253906, 521, 181.37222290039062], "page_size": [612.0, 792.0]}
{"layout": 480, "type": "text", "text": "type  $=$  ' Delta XY WH B Box Code r ' , # Type of box coder.  ' Delta XY WH B Box Code r '  is ␣ applied for most of methods. Refer to https://github.com/open-mmlab/mm detection/blob/ master/mmdet/core/bbox/coder/delta xy wh b box code r.py#L9 for more details. ", "page_idx": 51, "bbox": [82, 183.31483459472656, 527, 217.23727416992188], "page_size": [612.0, 792.0]}
{"layout": 481, "type": "text", "text": "target means  $=$  [0.0, 0.0, 0.0, 0.0], # The target means used to encode and ␣ decode boxes → target stds  $=$  [1.0, 1.0, 1.0, 1.0]), # The standard variance used to encode ␣ and decode boxes ", "page_idx": 51, "bbox": [75.6810073852539, 219.1798858642578, 527, 265.0583190917969], "page_size": [612.0, 792.0]}
{"layout": 482, "type": "text", "text": "loss_cls  $=$  dict( # Config of loss function for the classification branch ", "page_idx": 51, "bbox": [113, 267.00091552734375, 485, 277.0133361816406], "page_size": [612.0, 792.0]}
{"layout": 483, "type": "text", "text": "type  $={}^{1}$  ' Cross Entropy Loss ' , # Type of loss for classification branch, we also ␣ support FocalLoss etc. ", "page_idx": 51, "bbox": [82, 278.9559020996094, 532, 300.9233093261719], "page_size": [612.0, 792.0]}
{"layout": 484, "type": "text", "text": "use s igm oid  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  True, # RPN usually perform two-class classification, so it ␣ usually uses sigmoid function. ", "page_idx": 51, "bbox": [82, 302.86688232421875, 516, 324.83428955078125], "page_size": [612.0, 792.0]}
{"layout": 485, "type": "text", "text": "loss weight  $_{=1}$  .0), # Loss weight of the classification branch. loss_bbox  $=$  dict( # Config of loss function for the regression branch. ", "page_idx": 51, "bbox": [113, 326, 474.7386169433594, 348.7442626953125], "page_size": [612.0, 792.0]}
{"layout": 486, "type": "text", "text": "type  $=$  ' L1Loss ' , # Type of loss, we also support many IoU Losses and smooth ␣ L1-loss, etc. Refer to https://github.com/open-mmlab/mm detection/blob/master/mmdet/ models/losses/smooth l 1 loss.py#L56 for implementation. ", "page_idx": 51, "bbox": [82, 350.68682861328125, 527, 384.6102294921875], "page_size": [612.0, 792.0]}
{"layout": 487, "type": "text", "text": "loss weight  $\\scriptstyle=1.\\,\\emptyset)$  ), # Loss weight of the regression branch. ", "page_idx": 51, "bbox": [133, 386, 443.3569030761719, 397], "page_size": [612.0, 792.0]}
{"layout": 488, "type": "text", "text": "roi_head  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # RoIHead encapsulates the second stage of two-stage/cascade ␣ detectors. ", "page_idx": 51, "bbox": [82, 398.5077819824219, 495, 420.4751892089844], "page_size": [612.0, 792.0]}
{"layout": 489, "type": "text", "text": "type  $=$  ' Standard RoI Head ' , # Type of the RoI head. Refer to https://github.com/ open-mmlab/mm detection/blob/master/mmdet/models/roi_heads/standard roi head.py#L10 for ␣ implementation. ", "page_idx": 51, "bbox": [82, 422.4177551269531, 538, 456.3411560058594], "page_size": [612.0, 792.0]}
{"layout": 490, "type": "text", "text": "b box roi extractor  $=$  dict( # RoI feature extractor for bbox regression. ", "page_idx": 51, "bbox": [113, 458.2837219238281, 479.9689636230469, 468.296142578125], "page_size": [612.0, 792.0]}
{"layout": 491, "type": "text", "text": "type  $=$  ' Single RoI Extractor ' , # Type of the RoI feature extractor, most of ␣ methods uses Single RoI Extractor. Refer to https://github.com/open-mmlab/mm detection/ blob/master/mmdet/models/roi_heads/roi extractors/single level.py#L10 for details. ", "page_idx": 51, "bbox": [82, 470.23870849609375, 521, 504.1611022949219], "page_size": [612.0, 792.0]}
{"layout": 492, "type": "text", "text": "roi_layer  $\\leftleftarrows$  dict( # Config of RoI Layer type  $=$  ' RoIAlign ' , # Type of RoI Layer, Deform RoI Pooling Pack and ", "page_idx": 51, "bbox": [133, 506.10467529296875, 333.51898193359375, 516.1171264648438], "page_size": [612.0, 792.0]}
{"layout": 493, "type": "text", "text": "", "page_idx": 51, "bbox": [155, 518.0596313476562, 485, 528.0986938476562], "page_size": [612.0, 792.0]}
{"layout": 494, "type": "text", "text": "Modulated Deform RoI Pooling Pack are also supported. Refer to https://github.com/open- mmlab/mm detection/blob/master/mmdet/ops/roi_align/roi_align.py#L79 for details. ", "page_idx": 51, "bbox": [82, 530.0146484375, 516, 551.9821166992188], "page_size": [612.0, 792.0]}
{"layout": 495, "type": "text", "text": "sampling ratio  $\\scriptstyle{\\mathfrak{\\varphi}}=\\varnothing$  ), # Sampling ratio when extracting the RoI features.   $\\smash{\\mathfrak{O}_{\\perp}}$  means adaptive ratio. ", "page_idx": 51, "bbox": [82, 565, 538, 587.8480834960938], "page_size": [612.0, 792.0]}
{"layout": 496, "type": "text", "text": "out channel  $s{=}256$  , # output channels of the extracted feature. ", "page_idx": 51, "bbox": [133, 589, 459, 599.8031005859375], "page_size": [612.0, 792.0]}
{"layout": 497, "type": "text", "text": "feat map strides  $=$  [4, 8, 16, 32]), # Strides of multi-scale feature maps. It ␣ should be consistent to the architecture of the backbone. ", "page_idx": 51, "bbox": [82, 601.7456665039062, 532, 623.713134765625], "page_size": [612.0, 792.0]}
{"layout": 498, "type": "text", "text": "bbox_head=dict( # Config of box head in the RoIHead. ", "page_idx": 51, "bbox": [113, 625.6557006835938, 391.05255126953125, 635.6681518554688], "page_size": [612.0, 792.0]}
{"layout": 499, "type": "text", "text": "type  $=$  ' Shared 2 FCB Box Head ' , # Type of the bbox head, Refer to https://github. com/open-mmlab/mm detection/blob/master/mmdet/models/roi_heads/bbox_heads/con v fc b box head.py#L177 for implementation details. ", "page_idx": 51, "bbox": [82, 637.6106567382812, 532, 671.5341186523438], "page_size": [612.0, 792.0]}
{"layout": 500, "type": "text", "text": "in_channel  $s{=}256$  , # Input channels for bbox head. This is consistent with ␣ the out channels in roi extractor ", "page_idx": 51, "bbox": [82, 673, 521, 695.4441528320312], "page_size": [612.0, 792.0]}
{"layout": 501, "type": "text", "text": "fc out channel  $\\scriptstyle{.s=1024}$  , # Output feature channels of FC layers. num classes  $\\scriptstyle=80$  , # Number of classes for classification bbox_coder  $=$  dict( # Box coder used in the second stage. ", "page_idx": 51, "bbox": [133, 697, 459, 707.3991088867188], "page_size": [612.0, 792.0]}
{"layout": 502, "type": "text", "text": "", "page_idx": 52, "bbox": [134, 99, 422, 121.5961685180664], "page_size": [612.0, 792.0]}
{"layout": 503, "type": "text", "text": "type  $=$  ' Delta XY WH B Box Code r ' , # Type of box coder.  ' Delta XY WH B Box Code r '  is ␣ applied for most of methods. ", "page_idx": 52, "bbox": [82, 123.53877258300781, 537, 145.50619506835938], "page_size": [612.0, 792.0]}
{"layout": 504, "type": "text", "text": "use s igm oid  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  False, # Whether to use sigmoid. loss weight  $\\scriptstyle=1.\\,\\emptyset.$  ), # Loss weight of the classification branch. loss_bbox  $=$  dict( # Config of loss function for the regression branch. ", "page_idx": 52, "bbox": [134, 243.0908966064453, 495, 277.0133361816406], "page_size": [612.0, 792.0]}
{"layout": 505, "type": "text", "text": "type  $=\"$  L1Loss ' , # Type of loss, we also support many IoU Losses and ␣ smooth L1-loss, etc. ", "page_idx": 52, "bbox": [82, 278.9559020996094, 511, 300.9233093261719], "page_size": [612.0, 792.0]}
{"layout": 506, "type": "text", "text": "loss weight  $\\scriptstyle=1\\,.\\,\\emptyset)$  ), # Loss weight of the regression branch. ", "page_idx": 52, "bbox": [155, 302, 464, 313], "page_size": [612.0, 792.0]}
{"layout": 507, "type": "text", "text": "", "page_idx": 52, "bbox": [111, 320.25, 499, 331.75], "page_size": [612.0, 792.0]}
{"layout": 508, "type": "text", "text": "methods uses Single RoI Extractor. ", "page_idx": 52, "bbox": [82, 338.7318420410156, 249.8325958251953, 348.7442626953125], "page_size": [612.0, 792.0]}
{"layout": 509, "type": "text", "text": "roi_layer  $\\leftleftarrows$  dict( # Config of RoI Layer that extracts features for instance ␣ segmentation ", "page_idx": 52, "bbox": [82, 350.68682861328125, 527, 372.65423583984375], "page_size": [612.0, 792.0]}
{"layout": 510, "type": "text", "text": "type  $:=$  ' RoIAlign ' , # Type of RoI Layer, Deform RoI Pooling Pack and ␣ Modulated Deform RoI Pooling Pack are also supported ", "page_idx": 52, "bbox": [155, 374.5978088378906, 490.430419921875, 384.6368408203125], "page_size": [612.0, 792.0]}
{"layout": 511, "type": "text", "text": "", "page_idx": 52, "bbox": [82, 386.55279541015625, 333.51861572265625, 396.5652160644531], "page_size": [612.0, 792.0]}
{"layout": 512, "type": "text", "text": "output size  $\\scriptstyle=14$  , # The output size of feature maps. sampling ratio  $\\scriptstyle{\\mathfrak{\\varphi}}=\\varnothing$  ), # Sampling ratio when extracting the RoI features. out channel  $s{=}256$  , # Output channels of the extracted feature. ", "page_idx": 52, "bbox": [134, 398, 521, 432.43017578125], "page_size": [612.0, 792.0]}
{"layout": 513, "type": "text", "text": "feat map strides  $=$  [4, 8, 16, 32]), # Strides of multi-scale feature maps. mask_head  $=$  dict( # Mask prediction head ", "page_idx": 52, "bbox": [113, 434.37274169921875, 511, 456.3411560058594], "page_size": [612.0, 792.0]}
{"layout": 514, "type": "text", "text": "type  $=$  ' FC N Mask Head ' , # Type of mask head, refer to https://github.com/open- mmlab/mm detection/blob/master/mmdet/models/roi_heads/mask_heads/fc n mask head.py#L21 ␣ for implementation details. ", "page_idx": 52, "bbox": [82, 458.2837219238281, 527, 492.20611572265625], "page_size": [612.0, 792.0]}
{"layout": 515, "type": "text", "text": "num_convs  ${\\it\\Delta\\phi}_{\\mathrm{7}}=\\!4$  , # Number of convolutional layers in mask head. ", "page_idx": 52, "bbox": [134, 494, 448.5872497558594, 504.1611022949219], "page_size": [612.0, 792.0]}
{"layout": 516, "type": "text", "text": "in_channel  $s{=}256$  , # Input channels, should be consistent with the output ␣ channels of mask roi extractor. ", "page_idx": 52, "bbox": [82, 506, 516.5821533203125, 528.0720825195312], "page_size": [612.0, 792.0]}
{"layout": 517, "type": "text", "text": "con v out channel  $\\mathfrak{s}{=}256$  , # Output channels of the convolutional layer. ", "page_idx": 52, "bbox": [134, 529, 495, 540.027099609375], "page_size": [612.0, 792.0]}
{"layout": 518, "type": "text", "text": "num classes  $\\scriptstyle=80$  , # Number of class to be segmented. loss_mask  $\\fallingdotseq$  dict( # Config of loss function for the mask branch. type  $={}^{1}$  ' Cross Entropy Loss ' , # Type of loss used for segmentation use_mask  $\\fallingdotseq$  True, # Whether to only train the mask in the correct class. loss weight  $\\scriptstyle=1\\,.\\,\\emptyset)$  ))) # Loss weight of mask branch. ", "page_idx": 52, "bbox": [134, 541.9696655273438, 521, 600], "page_size": [612.0, 792.0]}
{"layout": 519, "type": "text", "text": "train_cfg  $=$   dict( # Config of training hyper parameters for rpn and rcnn ", "page_idx": 52, "bbox": [92.92194366455078, 601.7456665039062, 469.5086364746094, 611.7581176757812], "page_size": [612.0, 792.0]}
{"layout": 520, "type": "text", "text": "rpn  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # Training config of rpn assigner  $=$  dict( # Config of assigner ", "page_idx": 52, "bbox": [113, 613.70068359375, 323.05816650390625, 635.6681518554688], "page_size": [612.0, 792.0]}
{"layout": 521, "type": "text", "text": "type  $=$  ' MaxI oU As signer ' , # Type of assigner, MaxI oU As signer is used for ␣ many common detectors. Refer to https://github.com/open-mmlab/mm detection/blob/master/ → mmdet/core/bbox/assigners/max i ou as signer.py#L10 for more details. ", "page_idx": 52, "bbox": [75.68094635009766, 637.6106567382812, 532.2727661132812, 671.5341186523438], "page_size": [612.0, 792.0]}
{"layout": 522, "type": "text", "text": "pos i ou thr  $\\scriptstyle{\\ =0.7}$  , # IoU  $>=$   threshold 0.7 will be taken as positive ␣ ", "page_idx": 52, "bbox": [155, 673, 506.12091064453125, 683.4891357421875], "page_size": [612.0, 792.0]}
{"layout": 523, "type": "text", "text": "samples ", "text_level": 1, "page_idx": 52, "bbox": [80, 686, 119, 696], "page_size": [612.0, 792.0]}
{"layout": 524, "type": "text", "text": "ne g i ou thr  $\\scriptstyle{\\;=0}$  .3, # IoU   $<$   threshold 0.3 will be taken as negative samples ", "page_idx": 52, "bbox": [155, 697, 537, 707.3991088867188], "page_size": [612.0, 792.0]}
{"layout": 525, "type": "table", "page_idx": 53, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_42.jpg", "bbox": [71, 79, 542, 713], "page_size": [612.0, 792.0], "ocr_text": "a ee ee ee es\n\nmin_pos_iou=0.3, # The minimal IoU threshold to take boxes as positive.\nsamples\nmatch_low_quality=True, # Whether to match the boxes under low quality.\n«(see API doc for more details).\nignore_iof_thr=-1), # IoF threshold for ignoring bboxes\nsampler=dict( # Config of positive/negative sampler\ntype='RandomSampler', # Type of sampler, PseudoSampler and other,\nsamplers are also supported. Refer to https://github.com/open-mmlab/mmdetection/blob/\n—master/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details.\nnum=256, # Number of samples\npos_fraction=0.5, # The ratio of positive samples in the total samples.\nneg_pos_ub=-1, # The upper bound of negative samples based on the.\nnumber of positive samples.\nadd_gt_as_proposals=False), # Whether add GT as proposals after,\nsampling.\nallowed_border=-1, # The border allowed after padding for valid anchors.\npos_weight=-1, # The weight of positive samples during training.\ndebug=False), # Whether to set the debug mode\nrpn_proposal=dict( # The config to generate proposals during training\nnms_across_levels=False, # Whether to do NMS for boxes across levels. Only.\nwork in ~GARPNHead’, naive rpn does not support do nms cross levels.\nnms_pre=2000, # The number of boxes before NMS\nnms_post=1000, # The number of boxes to be kept by NMS, Only work in,\n..°GARPNHead*.\nmax_per_img=1000, # The number of boxes to be kept after NMS.\nnms=dict( # Config of NMS\ntype='nms', # Type of NMS\niou_threshold=0.7 # NMS threshold\n),\nmin_bbox_size=0), # The allowed minimal box size\nrenn=dict( # The config for the roi heads.\nassigner=dict( # Config of assigner for second stage, this is different for.\nthat in rpn\ntype='MaxIoUAssigner', # Type of assigner, MaxIoUAssigner is used for...\nall roi_heads for now. Refer to https://github.com/open-mmlab/mmdetection/blob/master/\n—mmdet/core/bbox/assigners/max_iou_assigner.py#L10 for more details.\npos_iou_thr=0.5, # IoU >= threshold 0.5 will be taken as positive.\nsamples\nneg_iou_thr=0.5, # IoU < threshold 0.5 will be taken as negative samples\nmin_pos_iou=0.5, # The minimal IoU threshold to take boxes as positive.\nsamples\nmatch_low_quality=False, # Whether to match the boxes under low quality.\n«.(see API doc for more details).\nignore_iof_thr=-1), # IoF threshold for ignoring bboxes\nsampler=dict(\ntype='RandomSampler', # Type of sampler, PseudoSampler and other,\nsamplers are also supported. Refer to https://github.com/open-mmlab/mmdetection/blob/\n—master/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details.\nnum=512, # Number of samples\npos_fraction=0.25, # The ratio of positive samples in the total samples.\nneg_pos_ub=-1, # The upper bound of negative samples based on the.\nnumber of positive samples.\n\n", "vlm_text": "The table provides a configuration for a machine learning model, specifically detailing settings related to region proposal networks (RPN) and region of interest (ROI) heads, often used in object detection tasks. Here's a summary of the table contents:\n\n1. **IoU and Sampling Configuration:**\n   - `min_pos_iou=0.3`: Minimum Intersection over Union (IoU) threshold for considering a bounding box as positive.\n   - `match_low_quality=True`: Option to match boxes under low quality conditions.\n   - `ignore_iof_thr=-1`: Ignore IoU threshold for ignoring bounding boxes.\n\n2. **Sampler Configuration:**\n   - `sampler=dict(...)`: Configuration for positive/negative sampler with a `type` of `RandomSampler`.\n   - `num=256` or `num=512`: Number of samples.\n   - `pos_fraction=0.5` or `pos_fraction=0.25`: Ratio of positive samples in total samples.\n   - `neg_pos_ub=-1`: Upper bound of negative samples.\n\n3. **Proposal Configuration:**\n   - `add_gt_as_proposals=False`: Whether to add ground truth as proposals after sampling.\n   - `allowed_border=-1`: Border allowed after padding for valid anchors.\n   - `pos_weight=-1`: Weight of positive samples during training.\n   - `debug=False`: Debug mode flag.\n   - `rpn_proposal=dict(...)`: Configuration for generating proposals during training.\n   - `nms_across_levels=False`: Non-maximum suppression (NMS) across levels flag.\n   - `nms_pre=2000`: Number of boxes before NMS.\n   - `nms_post=1000`: Number of boxes kept by NMS.\n   - `max_per_img=1000`: Maximum number of boxes to keep after NMS.\n   - `nms=dict(...)`: Configuration for NMS with type 'nms' and `iou_threshold=0.7`.\n\n4. **Box Size Configuration:**\n   - `min_bbox_size=0`: Minimum allowed box size.\n\n5. **ROI Head Configuration:**\n   - `rcnn=dict(...)`: Configuration for the ROI heads with `assigner=dict(...)`.\n   - `type='MaxIoUAssigner'`: Type of assigner, using `MaxIoUAssigner`.\n   - `pos_iou_thr=0.5`: IoU threshold considered positive.\n   - `neg_iou_thr=0.5`: IoU threshold considered negative.\n\nOverall, the table outlines settings for how samples are selected and processed in a model, detailing thresholds for bounding box selection, NMS, and the sampling strategy for training machine learning models, particularly in object detection frameworks like MMDetection."}
{"layout": 526, "type": "text", "text": "add gt as proposal  $\\mathtt{s}{=}$  True ), # Whether add GT as proposals after sampling. mask_size  $_{:=28}$  , # Size of mask pos_weight  $_{\\cdot}{=}{-}1$  , # The weight of positive samples during training. debug  $=$  False)) # Whether to set the debug mode test_cfg  $=$   dict( # Config for testing hyper parameters for rpn and rcnn rpn  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # The config to generate proposals during testing nm s across levels  $\\overleftarrow{}$  False, # Whether to do NMS for boxes across levels. Only ␣\n\n  $\\hookrightarrow$  work in  \\` GARPNHead \\` , naive rpn does not support do nms cross levels.\n\n → nms_pre  $\\scriptstyle{\\underline{{\\mathbf{\\a}}}}=1000$  , # The number of boxes before NMS nms_post  $\\scriptstyle=1000$  , # The number of boxes to be kept by NMS, Only work in ␣\n\n  $\\hookrightarrow$  \\` GARPNHead \\` .\n\n → max per img  $\\scriptstyle=1000$  , # The number of boxes to be kept after NMS. nms  $=$  dict( # Config of NMS type  $:=$  ' nms ' , #Type of NMS i ou threshold=0.7 # NMS threshold ), min b box size  $\\scriptstyle=\\!0$  ), # The allowed minimal box size rcnn  $\\risingdotseq$  dict( # The config for the roi heads. score_thr  $\\scriptstyle{\\overline{{\\mathbf{\\xi}}}}=\\varnothing.\\,\\varnothing5$  , # Threshold to filter out boxes nms  $=$  dict( # Config of NMS in the second stage  $\\scriptstyle{\\mathrm{type}}={\\mathrm{!nms}}\\,^{\\prime}$  , # Type of NMS iou_thr $\\mathrm{\\Lambda}{=}\\mathbb{0}\\cdot5$ ),# NMS thresholdmax per img  $\\scriptstyle=100$  , # Max number of detections of each image mask thr bin ar  $\\scriptstyle{\\mathfrak{v}}=\\varnothing\\,.\\,5)$  ) # Threshold of mask prediction data set type  $=$   ' Coco Data set ' # Dataset type, this will be used to define the dataset data_root  $=$   ' data/coco/ ' # Root path of data img norm cf g  $=$   dict( # Image normalization config to normalize the input images mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], # Mean values used to pre-training the pre-trained ␣\n\n  $\\hookrightarrow$  backbone models\n\n → std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], # Standard variance used to pre-training the pre-\n\n  $\\hookrightarrow$  trained backbone models\n\n → to_rgb  $=$  True\n\n ) # The channel orders of image used to pre-training the pre-trained backbone models train pipeline  $=$   [ # Training pipeline dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Load Image From File ' ), # First pipeline to load images from file path dict( type  $=$  ' Load Annotations ' , # Second pipeline to load annotations for current image with_bbox  $\\circeq$  True, # Whether to use bounding box, True for detection with_mask  $\\fallingdotseq$  True, # Whether to use instance mask, True for instance segmentation poly2mask  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  False), # Whether to convert the polygon mask to instance mask, set ␣\n\n  $\\hookrightarrow$  False for acceleration and to save memory\n\n → dict( type  $=\"$  Resize ' , # Augmentation pipeline that resize the images and their ␣\n\n  $\\hookrightarrow$  annotations\n\n → img_scale  $=$  (1333, 800), # The largest scale of image keep_ratio  $=$  True ), # whether to keep the ratio between height and width. dict( type  $={}^{1}$  ' RandomFlip ' , # Augmentation pipeline that flip the images and their ␣\n\n  $\\hookrightarrow$  annotations\n\n → flip_ratio  $\\scriptstyle{\\mathfrak{o}}=\\varnothing.5$  ), # The ratio or probability to flip ", "page_idx": 54, "bbox": [71, 87.67274475097656, 532, 707.3991088867188], "page_size": [612.0, 792.0]}
{"layout": 527, "type": "text", "text": "dict( type  $=$  ' Normalize ' , # Augmentation pipeline that normalize the input images mean  $\\risingdotseq$  [123.675, 116.28, 103.53], # These keys are the same of img norm cf g since ␣\n\n  $\\hookrightarrow$  the\n\n → std=[58.395, 57.12, 57.375], # keys of img norm cf g are used here as arguments to_rgb  $\\leftrightharpoons$  True), dict( type  $=$  ' Pad ' , # Padding config size divisor  $=\\!32$  ), # The number the padded images should be divisible dict(type  $\\circeq$  ' Default Format Bundle ' ), # Default format bundle to gather data in the ␣\n\n  $\\hookrightarrow$  pipeline\n\n → dict( type  $=$  ' Collect ' , # Pipeline that decides which keys in the data should be passed ␣\n\n  $\\hookrightarrow$  to the detector\n\n → keys  $\\overleftarrow{}$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ,  ' gt_masks ' ])\n\n ] test pipeline  $=$   [ dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Load Image From File ' ), # First pipeline to load images from file path dict( type  $=$  ' Multi Scale Flip Aug ' , # An encapsulation that encapsulates the testing ␣\n\n  $\\hookrightarrow$  augmentations\n\n → img_scale  $=$  (1333, 800), # Decides the largest scale for testing, used for the ␣\n\n  $\\hookrightarrow$  Resize pipeline\n\n → flip  $\\leftrightharpoons$  False, # Whether to flip images during testing transforms  $=$  [ dict(type  $\\circeq$  ' Resize ' , # Use resize augmentation keep_ratio  $\\mathrm{=}$  True), # Whether to keep the ratio between height and width,\n\n  $\\hookrightarrow$  the img_scale set here will be suppressed by the img_scale set above.\n\n → dict(type  $=$  ' RandomFlip ' ), # Thought RandomFlip is added in pipeline, it is ␣\n\n  $\\hookrightarrow$  not used because flip  $\\mathrm{\\textmu}$  False\n\n → dict( type  $\\risingdotseq$  ' Normalize ' , # Normalization config, the values are from img_norm_\n\n  $\\hookrightarrow$  cfg\n\n → mean  $\\risingdotseq$  [123.675, 116.28, 103.53], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], to_rgb  $\\leftrightharpoons$  True), dict( type  $=$  ' Pad ' , # Padding config to pad images divisible by 32. size divisor  $\\scriptstyle{=32}$  ), dict( type  $=$  ' Image To Tensor ' , # convert image to tensor keys  $\\overleftarrow{}$  [ ' img ' ]), dict( type  $=$  ' Collect ' , # Collect pipeline that collect necessary keys for ␣\n\n  $\\hookrightarrow$  testing.\n\n → keys $:=$ ['img'])])\n\n ] data  $=$   dict( ", "page_idx": 55, "bbox": [71, 87.67274475097656, 540, 671.5341186523438], "page_size": [612.0, 792.0]}
{"layout": 528, "type": "text", "text": "samples per gpu  $^{=2}$  , # Batch size of a single GPU workers per gpu  $^{=2}$  , # Worker to pre-fetch data for each single GPU train  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # Train dataset config ", "page_idx": 55, "bbox": [92, 673, 438.1263122558594, 707.3991088867188], "page_size": [612.0, 792.0]}
{"layout": 529, "type": "text", "text": "(continues on next page) ", "page_idx": 55, "bbox": [461.9010009765625, 709.8545532226562, 540, 720.5026245117188], "page_size": [612.0, 792.0]}
{"layout": 530, "type": "text", "text": "type  $=$  ' Coco Data set ' , # Type of dataset, refer to https://github.com/open-mmlab/\n\n  $\\hookrightarrow$  mm detection/blob/master/mmdet/datasets/coco.py#L19 for details.\n\n → ann_file  $=$  ' data/coco/annotations/instances train 2017.json ' , # Path of annotation ␣\n\n  $\\hookrightarrow$  file\n\n → img_prefix  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' data/coco/train2017/ ' , # Prefix of image path pipeline=[ # pipeline, this is passed by the train pipeline created before. dict(type  $=$  ' Load Image From File ' ), dict( type  $:=$  ' Load Annotations ' , with_bbox  $=$  True, with_mask  $\\fallingdotseq$  True, poly2mask  $\\fallingdotseq$  False), dict(type  $=$  ' Resize ' , img_scale  $=$  (1333, 800), keep_ratio  $\\leftrightharpoons$  True), dict(type  $=$  ' RandomFlip ' , flip_ratio  $\\scriptstyle=\\!0$  .5), dict( type  $=$  ' Normalize ' , mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], to_rgb  $\\leftrightharpoons$  True), dict(type  $=$  ' Pad ' , size divisor  $=\\!32$  ), dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Default Format Bundle ' ), dict( type  $=$  ' Collect ' , keys  $=$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ,  ' gt_masks ' ]) ]), val  $=$  dict( # Validation dataset config type  $=$  ' Coco Data set ' , ann_file  $=$  ' data/coco/annotations/instances val 2017.json ' , img_prefix  $=$  ' data/coco/val2017/ ' , pipeline=[ # Pipeline is passed by test pipeline created before dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Load Image From File ' ), dict( type  $={}^{1}$  ' Multi Scale Flip Aug ' , img_scale  $=$  (1333, 800), flip  $\\leftrightharpoons$  False, transforms=[ dict(type  $=$  ' Resize ' , keep_ratio  $=$  True), dict(type  $=$  ' RandomFlip ' ), dict( type  $=$  ' Normalize ' , mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], to_rgb  $\\mathrm{\\textmu}$  True), dict(type  $=$  ' Pad ' , size divisor  $\\scriptstyle{=32}$  ), dict(type  $=$  ' Image To Tensor ' , keys  $=$  [ ' img ' ]), dict(type  $=$  ' Collect ' , keys  $=$  [ ' img ' ]) ]) ]), test  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # Test dataset config, modify the ann_file for test-dev/test submission type  $=$  ' Coco Data set ' , ann_file  $=$  ' data/coco/annotations/instances val 2017.json ' , img_prefix  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' data/coco/val2017/ ' , ", "page_idx": 56, "bbox": [73, 87.67274475097656, 537.5028686523438, 707.4257202148438], "page_size": [612.0, 792.0]}
{"layout": 531, "type": "image", "page_idx": 57, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_43.jpg", "bbox": [69, 85, 540, 721], "page_size": [612.0, 792.0], "ocr_text": "pipeline=[ # Pipeline is passed by test_pipeline created before\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True),\ndict(type='RandomFlip'),\ndict(\ntype='Normalize',\nmean=[123.675, 116.28, 103.53],\nstd=[58.395, 57.12, 57.375],\nto_rgb=True) ,\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img'])\n\n1)\n],\nsamples_per_gpu=2 # Batch size of a single GPU used in testing\n))\n\nevaluation = dict( # The config to build the evaluation hook, refer to https://github.\n.com/open-mmlab/mmdetection/blob/master/mmdet/core/evaluation/eval_hooks.py#L7 for more,\ndetails.\ninterval=1, # Evaluation interval\nmetric=['bbox', 'segm']) # Metrics used during evaluation\noptimizer = dict( # Config used to build optimizer, support all the optimizers in,\nPyTorch whose arguments are also the same as those in PyTorch\ntype='SGD', # Type of optimizers, refer to https://github.com/open-mmlab/\n—mmdetection/blob/master/mmdet/core/optimizer/default_constructor.py#L13 for more.\ndetails\nlr=0.02, # Learning rate of optimizers, see detail usages of the parameters in the.\ndocumentation of PyTorch\nmomentum=0.9, # Momentum\nweight_decay=0.0001) # Weight decay of SGD\noptimizer_config = dict( # Config used to build the optimizer hook, refer to https://\ngithub. com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/optimizer.py#L8 for.\nimplementation details.\ngrad_clip=None) # Most of the methods do not use gradient clip\nlr_config = dict( # Learning rate scheduler config used to register LrUpdater hook\npolicy='step', # The policy of scheduler, also support CosineAnnealing, Cyclic, etc.\n«= Refer to details of supported LrUpdater from https://github.com/open-mmlab/mmcv/blob/\n—master/mmcv/runner/hooks/1lr_updater.py#L9.\nwarmup='linear', # The warmup policy, also support ‘exp’ and ‘constant°.\nwarmup_iters=500, # The number of iterations for warmup\nwarmup_ratio=\n0.001, # The ratio of the starting learning rate used for warmup\nstep=[8, 11]) # Steps to decay the learning rate\nrunner = dict(\ntype='EpochBasedRunner', # Type of runner to use (i.e. IterBasedRunner or,,\n..EpochBasedRunner)\nmax_epochs=12) # Runner that runs the workflow in total max_epochs. For.\n_IterBasedRunner use “max_iters (continues on next page)\n\n", "vlm_text": "The image contains a configuration script for a machine learning model, likely related to the MMDetection library. It includes settings for data processing pipelines, evaluation metrics, optimizer configuration, and learning rate schedules. Specific parameters, such as image scaling, normalization values, batch size, optimizer type, and training schedule, are specified. It also references specific parts of documentation for further details."}
{"layout": 532, "type": "image", "page_idx": 58, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_44.jpg", "bbox": [69, 84, 542, 341], "page_size": [612.0, 792.0], "ocr_text": "checkpoint_config = dict( # Config to set the checkpoint hook, Refer to https://github.\n—.com/open-mmlab/mncv/blob/master/mmcv/runner/hooks/checkpoint.py for implementation.\n\ninterval=1) # The save interval is 1\nlog_config = dict( # config to register logger hook\n\ninterval=50, # Interval to print the log\n\nhooks=[\n\n# dict(type='TensorboardLoggerHook') # The Tensorboard logger is also supported\ndict (type='TextLoggerHook')\n\n]) # The logger used to record the training process.\ndist_params = dict(backend='nccl') # Parameters to setup distributed training, the port.\ncan also be set.\nlog_level = 'INFO' # The level of logging.\nload_from = None # load models as a pre-trained model from a given path. This will not.\nresume training.\nresume_from = None # Resume checkpoints from a given path, the training will be resumed.\nfrom the epoch when the checkpoint's is saved.\nworkflow = [('train', 1)] # Workflow for runner. [('train', 1)] means there is only one,\nworkflow and the workflow named 'train' is executed once. The workflow trains the.\n«model by 12 epochs according to the total_epochs.\nwork_dir = 'work_dir' # Directory to save the model checkpoints and logs for the.\ncurrent experiments.\n", "vlm_text": "The image contains a configuration script for a machine learning setup, likely related to setting up training routines. Here's a breakdown of the key components:\n\n1. **checkpoint_config**: Configures checkpoint settings, specifying save intervals and providing a reference URL for implementation.\n\n2. **log_config**: Sets up logging configurations, with intervals for logs and hooks for Tensorboard and text logging.\n\n3. **dist_params**: Contains parameters for distributed training using the 'nccl' backend.\n\n4. **log_level**: Specifies the log level as 'INFO'.\n\n5. **load_from**: Indicates path settings for loading pre-trained models (initially set to `None`).\n\n6. **resume_from**: Specifies how to resume training from checkpoints (initially set to `None`).\n\n7. **workflow**: Defines the training workflow with a tuple `('train', 1)` indicating a single training phase per iteration.\n\n8. **work_dir**: Designates a directory for saving model checkpoints and logs."}
{"layout": 533, "type": "text", "text": "8.6 FAQ ", "text_level": 1, "page_idx": 58, "bbox": [71, 367, 130, 383], "page_size": [612.0, 792.0]}
{"layout": 534, "type": "text", "text": "8.6.1 Ignore some fields in the base configs ", "text_level": 1, "page_idx": 58, "bbox": [71, 402, 322, 415], "page_size": [612.0, 792.0]}
{"layout": 535, "type": "text", "text": "Sometimes, you may set  _delete_  $\\bar{\\cdot}$  True  to ignore some of fields in base configs. You may refer to  mmcv  for simple illustration. ", "page_idx": 58, "bbox": [71, 426.6944580078125, 539.9988403320312, 451.9604797363281], "page_size": [612.0, 792.0]}
{"layout": 536, "type": "text", "text": "In MM Detection, for example, to change the backbone of Mask R-CNN with the following config. ", "page_idx": 58, "bbox": [71, 456.58245849609375, 463.05181884765625, 469.8924865722656], "page_size": [612.0, 792.0]}
{"layout": 537, "type": "table", "page_idx": 58, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_45.jpg", "bbox": [68, 474, 544, 661], "page_size": [612.0, 792.0], "ocr_text": "model = dict(\ntype='MaskRCNN',\npretrained='torchvision://resnet50',\nbackbone=dict(\ntype='ResNet',\ndepth=50,\nnum_stages=4,\nout_indices=(0, 1, 2, 3),\nfrozen_stages=1,\nnorm_cfg=dict(type='BN', requires_grad=True) ,\nnorm_eval=True,\nstyle='pytorch'),\nneck=dict(...),\nrpn_head=dict(...),\nroi_head=dict(...))\n\n", "vlm_text": "The image is a code block showing a Python dictionary configuration for setting up a model, specifically a Mask R-CNN (Mask Region-based Convolutional Neural Network). Here's a breakdown of this configuration:\n\n- `model`: This is a dictionary that defines the model configuration.\n  \n- `type`: The type of model is specified as `'MaskRCNN'`.\n  \n- `pretrained`: Indicates a pretrained model is being used, in this case, the ResNet-50 model from torchvision.\n  \n- `backbone`: This is a nested dictionary that specifies the backbone network for the model.\n  - `type`: The backbone type is `'ResNet'`.\n  - `depth`: The depth of the ResNet is 50.\n  - `num_stages`: The number of stages is 4.\n  - `out_indices`: A tuple specifying which stages to output, here, (0, 1, 2, 3).\n  - `frozen_stages`: Specifies that stage 1 is frozen and will not be updated during training.\n  - `norm_cfg`: Another nested dictionary for normalization configuration.\n    - `type`: Batch Normalization ('BN') is used.\n    - `requires_grad`: This is set to `True`, meaning gradients are computed for the normalization layers, allowing them to be updated during training.\n  - `norm_eval`: Evaluates the model with running statistics on current batch if `True`.\n  - `style`: Specifies the style, in this case, it's `'pytorch'`.\n\n- `neck`, `rpn_head`, and `roi_head`: These are placeholders for further configurations specific to the FPN neck, RPN head, and ROI head components of the Mask R-CNN model, but the specific details are not shown in the image."}
{"layout": 538, "type": "text", "text": "ResNet  and  HRNet  use different keywords to construct. ", "page_idx": 58, "bbox": [71, 669.5834350585938, 293.16943359375, 682.8934936523438], "page_size": [612.0, 792.0]}
{"layout": 539, "type": "image", "page_idx": 59, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_46.jpg", "bbox": [68, 72, 545, 467], "page_size": [612.0, 792.0], "ocr_text": "_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\npretrained='open-mmlab://msra/hrnetv2_w32',\nbackbone=dict(\n_delete_=True,\ntype='HRNet',\nextra=dict(\nstagel=dict(\nnum_modules=1,\nnum_branches=1,\nblock='BOTTLENECK' ,\nnum_blocks=(4, ),\nnum_channels=(64, )),\nstage2=dict(\nnum_modules=1,\nnum_branches=2,\nblock='BASIC',\nnum_blocks=(4, 4),\nnum_channels=(32, 64)),\nstage3=dict(\nnum_modules=4,\nnum_branches=3,\nblock='BASIC',\nnum_blocks=(4, 4, 4),\nnum_channels=(32, 64, 128)),\nstage4=dict(\nnum_modules=3,\nnum_branches=4,\nblock='BASIC',\nnum_blocks=(4, 4, 4, 4),\nnum_channels=(32, 64, 128, 256)))),\nneck=dict(...))\n\n", "vlm_text": "The image shows a configuration script written in Python for a model, likely related to computer vision or deep learning. It specifies the use of a pretrained model, `open-mmlab://msra/hrnetv2_w32`, and details the `backbone` structure with parameters such as `type`, `num_modules`, `num_branches`, `block`, `num_blocks`, and `num_channels` for different stages (`stage1`, `stage2`, `stage3`, and `stage4`). The `neck` section is also defined but not fully shown. This configuration appears to be for a High-Resolution Network (HRNet) used in Mask RCNN setups for tasks such as image segmentation."}
{"layout": 540, "type": "text", "text": "The  _delete_  $\\overrightharpoon{\\cdot}$  True  would replace all old keys in  backbone  field with new keys. ", "page_idx": 59, "bbox": [71, 472.7474670410156, 400.4376525878906, 486.0574951171875], "page_size": [612.0, 792.0]}
{"layout": 541, "type": "text", "text": "8.6.2 Use intermediate variables in configs ", "text_level": 1, "page_idx": 59, "bbox": [71, 505, 316, 519], "page_size": [612.0, 792.0]}
{"layout": 542, "type": "text", "text": "Some intermediate variables are used in the configs files, like  train pipeline / test pipeline  in datasets. It’s worth noting that when modifying intermediate variables in the children configs, user need to pass the intermediate variables into corresponding fields again. For example, we would like to use multi scale strategy to train a Mask R-CNN. train pipeline / test pipeline  are intermediate variable we would like modify. ", "page_idx": 59, "bbox": [71, 530.9754028320312, 540, 580.1505126953125], "page_size": [612.0, 792.0]}
{"layout": 543, "type": "text", "text": "_base_  $=$   ' ./mask r cnn r 50 fp n 1 x coco.py ' img norm cf g  $=$   dict ( mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ 123.675 ,  116.28 ,  103.53 ], std  $\\overbar{\\;-\\;}$  [ 58.395 ,  57.12 ,  57.375 ], to_rgb  $\\risingdotseq$  True ) train pipeline  $=$   [ dict ( type  $=\"$  Load Image From File ' ), dict ( type  $=\"$  Load Annotations ' , with_bbox  $\\fallingdotseq$  True , with_mask  $\\risingdotseq$  True ), dict ( type  $\\equiv^{\\dagger}$  Resize ' , img_scale  $=$  [( 1333 ,  640 ), ( 1333 ,  672 ), ( 1333 ,  704 ), ( 1333 ,  736 ), ( 1333 ,  768 ), ( 1333 ,  800 )], ", "page_idx": 59, "bbox": [71, 589.8847045898438, 474.73834228515625, 707.4932250976562], "page_size": [612.0, 792.0]}
{"layout": 544, "type": "image", "page_idx": 60, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_47.jpg", "bbox": [69, 83, 544, 413], "page_size": [612.0, 792.0], "ocr_text": "multiscale_mode=\"value\",\nkeep_ratio=True) ,\ndict(type='RandomFlip', flip_ratio=0.5),\ndict(type='Normalize', mg_norm_cfg),\ndict(type='Pad', size_divisor=32),\ndict(type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n\n]\ntest_pipeline = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True),\ndict(type='RandomFlip'),\ndict(type='Normalize', **img_norm_cfg),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n))\n]\ndata = dict(\ntrain=dict(pipeline=train_pipeline),\nval=dict (pipeline=test_pipeline) ,\ntest=dict (pipeline=test_pipeline))\n\n", "vlm_text": "This image shows a segment of code written in Python, likely related to data preprocessing pipelines for a machine learning or deep learning framework such as MMDetection, which is used for object detection tasks. \n\nThe code snippet contains:\n\n1. Definitions for `train_pipeline` and `test_pipeline` configurations using a list of transformation dictionaries. These transformations include data augmentations and preprocessing steps such as:\n   - `RandomFlip`: Randomly flips images with a specified probability.\n   - `Normalize`: Normalizes the image using given parameters.\n   - `Pad`: Pads the image to ensure the dimensions are divisible by a specific number, in this case, 32.\n   - `DefaultFormatBundle` and `Collect`: Formats and collects necessary data from the dictionary.\n   - For testing, additional transformations like `ImageToTensor` are present to prepare the image tensor.\n\n2. The `test_pipeline` involves loading an image from a file and further image transformations including:\n   - `MultiScaleFlipAug`: Multi-scale augmentation to resize the image to a fixed scale and apply additional transformations.\n   - A similar list of transformations for resizing, normalizing, and collecting data as in the training pipeline.\n\n3. A `data` dictionary which organizes the pipelines for training, validation, and testing phases, specifying which pipeline to use for their respective datasets. \n\nThis code is commonly used in the configuration files of machine learning models to standardize the input data processing."}
{"layout": 545, "type": "text", "text": "We first define the new  train pipeline / test pipeline  and pass them into  data ", "page_idx": 60, "bbox": [71, 421.09246826171875, 410.46051025390625, 434.4024963378906], "page_size": [612.0, 792.0]}
{"layout": 546, "type": "text", "text": "Similarly, if we would like to switch from  SyncBN  to  BN  or  MMSyncBN , we need to substitute every  norm_cfg  in the config. ", "page_idx": 60, "bbox": [71, 439.0254821777344, 540.00439453125, 464.2904968261719], "page_size": [612.0, 792.0]}
{"layout": 547, "type": "text", "text": "_base_  $=$   ' ./mask r cnn r 50 fp n 1 x coco.py ' norm_cfg  $=$   dict ( type  $\\equiv^{\\dagger}$  BN ' , requires grad = True ) model  $=$   dict ( backbone  $=$  dict (norm_cfg  $=$  norm_cfg), neck  $\\fallingdotseq$  dict (norm_cfg  $|=$  norm_cfg), ... ) ", "page_idx": 60, "bbox": [71, 474, 312.59735107421875, 543.8131103515625], "page_size": [612.0, 792.0]}
{"layout": 548, "type": "text", "text": "TUTORIAL 2: CUSTOMIZE DATASETS ", "text_level": 1, "page_idx": 62, "bbox": [283, 164, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 549, "type": "text", "text": "9.1 Support new data format ", "text_level": 1, "page_idx": 62, "bbox": [71, 229, 269, 245], "page_size": [612.0, 792.0]}
{"layout": 550, "type": "text", "text": "To support a new data format, you can either convert them to existing formats (COCO format or PASCAL format) or directly convert them to the middle format. You could also choose to convert them offline (before training by a script) or online (implement a new dataset and do the conversion at training). In MM Detection, we recommend to convert the data into COCO formats and do the conversion offline, thus you only need to modify the config’s data annotation paths and classes after the conversion of your data. ", "page_idx": 62, "bbox": [72, 260.8024597167969, 540, 321.9334411621094], "page_size": [612.0, 792.0]}
{"layout": 551, "type": "text", "text": "9.1.1 Reorganize new data formats to existing format ", "page_idx": 62, "bbox": [72, 339.54632568359375, 374, 356.66619873046875], "page_size": [612.0, 792.0]}
{"layout": 552, "type": "text", "text": "The simplest way is to convert your dataset to existing dataset formats (COCO or PASCAL VOC). ", "page_idx": 62, "bbox": [72, 366.8514099121094, 462.414306640625, 380.16143798828125], "page_size": [612.0, 792.0]}
{"layout": 553, "type": "text", "text": "The annotation json files in COCO format has the following necessary keys: ", "page_idx": 62, "bbox": [72, 384.7834167480469, 374, 398.09344482421875], "page_size": [612.0, 792.0]}
{"layout": 554, "type": "image", "page_idx": 62, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_48.jpg", "bbox": [69, 400, 545, 708], "page_size": [612.0, 792.0], "ocr_text": "‘images': [\n\n4\n'file_name': 'COCO_val2014_000000001268.jpg',\n\"height': 427,\n\"'width': 640,\n\"id': 1268\n3,\n],\n‘annotations': [\nct\n\"segmentation': [[192.81,\n247.09,\n219.03,\n\n249.06]], # if you have mask labels\n\"area': 1035.749,\n\"iscrowd': 0,\n‘image_id': 1268,\n\"pbox': [192.81, 224.8, 74.73, 33.43],\n\"category_id': 16,\n\"id': 42986\n\n", "vlm_text": "The image contains a portion of a data structure, possibly a JSON or similar format, used to describe image metadata and annotations. It includes the following information:\n\n1. The `images` section provides details about an image file:\n   - `file_name`: 'COCO_val2014_000000001268.jpg'\n   - `height`: 427 pixels\n   - `width`: 640 pixels\n   - `id`: 1268\n\n2. The `annotations` section describes annotations associated with the image:\n   - `segmentation`: A list of coordinates representing the segmentation mask if mask labels are present.\n   - `area`: 1035.749, which could represent the area of the segmented region.\n   - `iscrowd`: 0, indicating whether the annotation represents a crowd of objects.\n   - `image_id`: 1268, linking the annotation to the corresponding image.\n   - `bbox`: A bounding box around the object in the format [x, y, width, height].\n   - `category_id`: 16, which might correspond to a certain category or class in the dataset.\n   - `id`: 42986, an identifier for the annotation."}
{"layout": 555, "type": "image", "page_idx": 63, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_49.jpg", "bbox": [69, 82, 543, 151], "page_size": [612.0, 792.0], "ocr_text": "1,\n\n\"categories': [\n{'id': 0, 'name': 'car'},\n\n]\n\n", "vlm_text": "The image shows a snippet of JSON-like data, which is structured as a Python dictionary or a similar format. This snippet contains a single key called 'categories'. The 'categories' key is associated with a list that contains a single dictionary with two key-value pairs: 'id' with a value of 0, and 'name' with a value of 'car'. This suggests that the data represents or categorizes something as a \"car\" with an associated ID of 0."}
{"layout": 556, "type": "text", "text": "There are three necessary keys in the json file: ", "page_idx": 63, "bbox": [71, 158.07948303222656, 255.71034240722656, 171.38951110839844], "page_size": [612.0, 792.0]}
{"layout": 557, "type": "text", "text": "•  images : contains a list of images with their information like  file_name ,  height ,  width , and  id . •  annotations : contains the list of instance annotations. •  categories : contains the list of categories names and their ID. ", "page_idx": 63, "bbox": [88, 176.01246643066406, 489.9426574707031, 225.18748474121094], "page_size": [612.0, 792.0]}
{"layout": 558, "type": "text", "text": "After the data pre-processing, there are two steps for users to train the customized new dataset with existing format (e.g. COCO format): ", "page_idx": 63, "bbox": [71, 229.81044006347656, 540, 255.0754852294922], "page_size": [612.0, 792.0]}
{"layout": 559, "type": "text", "text": "1. Modify the config file for using the customized dataset. 2. Check the annotations of the customized dataset. ", "page_idx": 63, "bbox": [84, 259.6984558105469, 319, 290.94049072265625], "page_size": [612.0, 792.0]}
{"layout": 560, "type": "text", "text": "Here we give an example to show the above two steps, which uses a customized dataset of 5 classes with COCO format to train an existing Cascade Mask R-CNN R50-FPN detector. ", "page_idx": 63, "bbox": [71, 295.5634765625, 540, 320.8284912109375], "page_size": [612.0, 792.0]}
{"layout": 561, "type": "text", "text": "1. Modify the config file for using the customized dataset ", "page_idx": 63, "bbox": [71, 338.6910705566406, 341.7672424316406, 352.95751953125], "page_size": [612.0, 792.0]}
{"layout": 562, "type": "text", "text": "There are two aspects involved in the modification of config file: ", "page_idx": 63, "bbox": [71, 363.75445556640625, 328.2479553222656, 377.0644836425781], "page_size": [612.0, 792.0]}
{"layout": 563, "type": "text", "text": "1. The  data  field. Specifically, you need to explicitly add the  classes  fields in  data.train ,  data.val  and data.test . 2. The  num classes  field in the  model  part. Explicitly over-write all the  num classes  from default value (e.g. 80 in COCO) to your classes number. ", "page_idx": 63, "bbox": [84, 381.6874694824219, 540, 436.8404846191406], "page_size": [612.0, 792.0]}
{"layout": 564, "type": "text", "text": "In  configs/my custom config.py : ", "page_idx": 63, "bbox": [71, 441.4634704589844, 226.9086151123047, 454.77349853515625], "page_size": [612.0, 792.0]}
{"layout": 565, "type": "image", "page_idx": 63, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_50.jpg", "bbox": [68, 459, 544, 705.75], "page_size": [612.0, 792.0], "ocr_text": "# the new config inherits the base configs to highlight the necessary modification\n_base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py'\n\n#1. dataset settings\ndataset_type = 'CocoDataset'\nclasses = ('a\", 'b'’, ‘“e', \"d’, “e')\ndata = dict(\nsamples_per_gpu=2,\nworkers_per_gpu=2,\ntrain=dict(\ntype=dataset_type,\n# explicitly add your class names to the field ‘classes*\nclasses=classes,\nann_file='path/to/your/train/annotation_data',\nimg_prefix='path/to/your/train/image_data'),\nval=dict(\ntype=dataset_type,\n# explicitly add your class names to the field ‘classes*\nclasses=classes,\n\n", "vlm_text": "This image shows a Python configuration script for setting up a dataset. It includes the following components:\n\n1. **Base Configuration**: Inherits from `./cascade_mask_rcnn_r50_fpn_1x_coco.py`.\n\n2. **Dataset Settings**:\n   - `dataset_type`: `'CocoDataset'`\n   - `classes`: A tuple with elements `'a'`, `'b'`, `'c'`, `'d'`, `'e'`\n\n3. **Data Dictionary**:\n   - `samples_per_gpu`: `2`\n   - `workers_per_gpu`: `2`\n\n4. **Train and Validation Configs**:\n   - Both have a `type` set to `dataset_type` and `classes` assigned the value of `classes`.\n   - `train` configuration specifies file paths for annotations and image data:\n     - `ann_file`: `'path/to/your/train/annotation_data'`\n     - `img_prefix`: `'path/to/your/train/image_data'`"}
{"layout": 566, "type": "text", "text": "(continues on next page) ", "page_idx": 63, "bbox": [461.9010009765625, 703.7975463867188, 540, 714.4456176757812], "page_size": [612.0, 792.0]}
{"layout": 567, "type": "image", "page_idx": 64, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_51.jpg", "bbox": [69, 83, 544, 438], "page_size": [612.0, 792.0], "ocr_text": "ann_file='path/to/your/val/annotation_data',\nimg_prefix='path/to/your/val/image_data'),\n\ntest=dict(\ntype=dataset_type,\n# explicitly add your class names to the field ‘classes*\nclasses=classes,\nann_file='path/to/your/test/annotation_data',\nimg_prefix='path/to/your/test/image_data'))\n\n#2. model settings\n\n# explicitly over-write all the ‘num_classes field from default 80 to 5.\nmodel = dict(\nroi_head=dict (\nbbox_head=[\ndict(\ntype='Shared2FCBBoxHead',\n# explicitly over-write all the ‘num_classes field from default 80 to 5.\nnum_classes=5),\ndict(\ntype='Shared2FCBBoxHead',\n# explicitly over-write all the ‘num_classes field from default 80 to 5.\nnum_classes=5),\ndict(\ntype='Shared2FCBBoxHead' ,\n# explicitly over-write all the ‘num_classes field from default 80 to 5.\nnum_classes=5)],\n# explicitly over-write all the ‘num_classes field from default 80 to 5.\nmask_head=dict (num_classes=5)))\n\n", "vlm_text": "The image shows a snippet of code written in Python. It appears to be a configuration script for setting up a machine learning model, likely for an object detection task. The code specifies paths for validation and test data, including annotation data and image data. It defines model settings, particularly for a region of interest (ROI) head and a mask head within the model. The script explicitly changes the number of classes from the default of 80 to 5, as indicated in multiple comments and settings throughout the code. This is done by setting the `num_classes` parameter to 5 for each component in the model."}
{"layout": 568, "type": "text", "text": "2. Check the annotations of the customized dataset ", "text_level": 1, "page_idx": 64, "bbox": [71, 460, 317, 471], "page_size": [612.0, 792.0]}
{"layout": 569, "type": "text", "text": "Assuming your customized dataset is COCO format, make sure you have the correct annotations in the customized dataset: ", "page_idx": 64, "bbox": [71, 483.30645751953125, 540, 508.57147216796875], "page_size": [612.0, 792.0]}
{"layout": 570, "type": "text", "text": "1. The length for  categories  field in annotations should exactly equal the tuple length of  classes  fields in your config, meaning the number of classes (e.g. 5 in this example). 2. The  classes  fields in your config file should have exactly the same elements and the same order with the  name in  categories  of annotations. MM Detection automatically maps the un continuous  id  in  categories  to the continuous label indices, so the string order of  name  in  categories  field affects the order of label indices. Meanwhile, the string order of  classes  in config affects the label text during visualization of predicted bounding boxes. ", "page_idx": 64, "bbox": [84, 513.1934204101562, 540, 604.2124633789062], "page_size": [612.0, 792.0]}
{"layout": 571, "type": "text", "text": "in  categories . ", "page_idx": 64, "bbox": [96, 620.7904052734375, 162.07260131835938, 634.1004638671875], "page_size": [612.0, 792.0]}
{"layout": 572, "type": "text", "text": "Here is a valid example of annotations: ", "page_idx": 64, "bbox": [71, 638.7234497070312, 227.35670471191406, 652.0335083007812], "page_size": [612.0, 792.0]}
{"layout": 573, "type": "image", "page_idx": 64, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_52.jpg", "bbox": [68, 656, 543, 711.75], "page_size": [612.0, 792.0], "ocr_text": "\"annotations': [\n\n{\n\"segmentation': [[192.81,\n\n", "vlm_text": "The image is a snippet of code or a data structure in JSON-like format. It shows an entry with the key `'annotations'`, which contains a list. Inside the list, there's an object with a key `'segmentation'` that includes a nested list with a number `192.81`. It appears to be part of data used for image annotation or segmentation tasks."}
{"layout": 574, "type": "text", "text": "(continues on next page) ", "page_idx": 64, "bbox": [461.9010009765625, 709.7745361328125, 540, 720.422607421875], "page_size": [612.0, 792.0]}
{"layout": 575, "type": "image", "page_idx": 65, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_53.jpg", "bbox": [68, 82, 542, 318], "page_size": [612.0, 792.0], "ocr_text": "1,\n\n247.09,\n\n219.03,\n\n249.06]], # if you have mask labels\n‘area’: 1035.749,\n‘iscrowd': 0,\n\"image_id': 1268,\n\"pbox': [192.81, 224.8, 74.73, 33.43],\n\"category_id': 16,\n\"id': 42986\n\n# MMDetection automatically maps the uncontinuous ‘id* to the continuous label indices.\n\"categories': [\n\n{ids LT, \"name\": “a, {\"dd\": 3, \"“name’: \"b\"}, {\"did\":s 4,\n\n«= 'name': 'd'}, {'id': 17, 'name': ‘e'},\n\n]\n\n‘name': 'c'}, {'id': 16,\n\n", "vlm_text": "This image contains a snippet of code that appears to be part of a dataset or annotation format often used in object detection tasks. Here’s a breakdown:\n\n- **Mask Labels**: There are placeholder values like `247.09`, `219.03`, and `249.06` which might represent points if masks are involved.\n- **Attributes**:\n  - `'area'`: The area of the bounding box is `1035.749`.\n  - `'iscrowd'`: A flag (`0`) indicating whether the annotation refers to a single object or a group.\n  - `'image_id'`: The ID of the image is `1268`.\n  - `'bbox'`: The bounding box coordinates are `[192.81, 224.8, 74.73, 33.43]`.\n  - `'category_id'`: The category ID is `16`.\n  - `'id'`: Unique ID `42986` for the annotation.\n\n- **Categories**: Listed with ID and name pairs:\n  - `{ 'id': 1, 'name': 'a' }`\n  - `{ 'id': 3, 'name': 'b' }`\n  - `{ 'id': 4, 'name': 'c' }`\n  - `{ 'id': 16, 'name': 'd' }`\n  - `{ 'id': 17, 'name': 'e' }`\n\n- **Note**: There is a comment indicating that MMDetection, an object detection framework, automatically maps these IDs to continuous label indices."}
{"layout": 576, "type": "text", "text": "We use this way to support CityScapes dataset. The script is in  cityscapes.py  and we also provide the finetuning  configs . ", "page_idx": 65, "bbox": [71, 325.45147705078125, 540, 338.7615051269531], "page_size": [612.0, 792.0]}
{"layout": 577, "type": "text", "text": "Note ", "text_level": 1, "page_idx": 65, "bbox": [71, 345, 92, 356], "page_size": [612.0, 792.0]}
{"layout": 578, "type": "text", "text": "1. For instance segmentation datasets,  MM Detection only supports evaluating mask AP of dataset in COCO format for now . 2. It is recommended to convert the data offline before training, thus you can still use  Coco Data set  and only need to modify the path of annotations and the training classes. ", "page_idx": 65, "bbox": [84, 360.6898498535156, 540, 416.47052001953125], "page_size": [612.0, 792.0]}
{"layout": 579, "type": "text", "text": "9.1.2 Reorganize new data format to middle format ", "text_level": 1, "page_idx": 65, "bbox": [70, 435, 361, 449], "page_size": [612.0, 792.0]}
{"layout": 580, "type": "text", "text": "It is also fine if you do not want to convert the annotation format to COCO or PASCAL format. Actually, we define a simple annotation format and all existing datasets are processed to be compatible with it, either online or offline. ", "page_idx": 65, "bbox": [71, 460.7024841308594, 540, 485.9674987792969], "page_size": [612.0, 792.0]}
{"layout": 581, "type": "text", "text": "The annotation of a dataset is a list of dict, each dict corresponds to an image. There are 3 field  filename  (relative path), width ,  height  for testing, and an additional field  ann  for training.  ann  is also a dict containing at least 2 fields:  bboxes and  labels , both of which are numpy arrays. Some datasets may provide annotations like crowd/difficult/ignored bboxes, we use  b boxes ignore  and  labels ignore  to cover them. ", "page_idx": 65, "bbox": [71, 490.5904846191406, 540, 539.7655029296875], "page_size": [612.0, 792.0]}
{"layout": 582, "type": "text", "text": "Here is an example. ", "page_idx": 65, "bbox": [71, 544.388427734375, 150.6746368408203, 557.698486328125], "page_size": [612.0, 792.0]}
{"layout": 583, "type": "text", "text": "[ { ' filename ' :  ' a.jpg ' , ' width ' :  1280 , ' height ' :  720 , ' ann ' : { ' bboxes ' :  < np . ndarray, float  $32\\textgreater$   (n,  4 ), ' labels ' :  < np . ndarray, int  $64\\textgreater$   (n, ), ' b boxes ignore ' :  < np . ndarray, float  $32\\textgreater$   (k,  4 ), ' labels ignore ' :  < np . ndarray, int  $64\\textgreater$   (k, ) (optional field) } ", "page_idx": 65, "bbox": [71, 578.5897216796875, 443.3564758300781, 708.1541748046875], "page_size": [612.0, 792.0]}
{"layout": 584, "type": "text", "text": "(continues on next page) ", "page_idx": 65, "bbox": [461.9010009765625, 709.8545532226562, 540, 720.5026245117188], "page_size": [612.0, 792.0]}
{"layout": 585, "type": "table", "page_idx": 66, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_54.jpg", "bbox": [69, 83, 542, 126], "page_size": [612.0, 792.0], "ocr_text": "},\n", "vlm_text": "The provided image does not contain any visible data or recognizable table structure. It only shows a section of what might be code, with characters like \"},\" and \"...]\". Without additional context or a more complete image, it's not possible to determine the contents or purpose of this table."}
{"layout": 586, "type": "text", "text": "There are two ways to work with custom datasets. ", "page_idx": 66, "bbox": [72, 134.16847229003906, 269.84722900390625, 147.47850036621094], "page_size": [612.0, 792.0]}
{"layout": 587, "type": "text", "text": "• online conversion ", "page_idx": 66, "bbox": [88, 152.10145568847656, 167, 165.41148376464844], "page_size": [612.0, 792.0]}
{"layout": 588, "type": "text", "text": "You can write a new Dataset class inherited from  Custom Data set , and overwrite two methods load annotations(self, ann_file)  and  get ann info(self, idx) , like  Coco Data set  and  VOC- Dataset . ", "page_idx": 66, "bbox": [96, 170.03443908691406, 540, 207.25450134277344], "page_size": [612.0, 792.0]}
{"layout": 589, "type": "text", "text": "• offline conversion ", "page_idx": 66, "bbox": [88, 211.87745666503906, 167, 225.18748474121094], "page_size": [612.0, 792.0]}
{"layout": 590, "type": "text", "text": "You can convert the annotation format to the expected format above and save it to a pickle or json file, like pascal_voc.py . Then you can simply use  Custom Data set . ", "page_idx": 66, "bbox": [96, 229.81044006347656, 540, 255.0754852294922], "page_size": [612.0, 792.0]}
{"layout": 591, "type": "text", "text": "9.1.3 An example of customized dataset ", "text_level": 1, "page_idx": 66, "bbox": [70, 275, 300, 289], "page_size": [612.0, 792.0]}
{"layout": 592, "type": "text", "text": "Assume the annotation is in a new format in text files. The bounding boxes annotations are stored in text file annotation.txt  as the following ", "page_idx": 66, "bbox": [72, 299.99346923828125, 540, 325.25848388671875], "page_size": [612.0, 792.0]}
{"layout": 593, "type": "table", "page_idx": 66, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_55.jpg", "bbox": [71, 331, 542, 494], "page_size": [612.0, 792.0], "ocr_text": "#\n000001. jpg\n1280 720\n\n2\n\n10 20 40 60 1\n20 40 50 60 2\n#\n\n000002. jpg\n1280 720\n\n", "vlm_text": "This table appears to represent annotations or metadata information about a series of image files, possibly for an image dataset. Each section provides details for a different image. The structure of each section is as follows:\n\n1. A line beginning with `#`, which may denote the start of a new image entry.\n2. A filename with a `.jpg` extension, indicating the image file being referenced (e.g., `000001.jpg` and `000002.jpg`).\n3. Two numbers following the filename, which likely indicate the width and height of the image (e.g., `1280 720`).\n4. A single number that follows, indicating the number of annotations or regions of interest in the image (e.g., `2` and `3`).\n5. Subsequent lines contain a series of numbers, which are seemingly grouped in sequences of five. Each sequence could represent an annotation box in the image and may have the structure: `x y width height class`, where `x` and `y` are coordinates, `width` and `height` are the dimensions of the bounding box, and `class` is an identifier for the object class being annotated (in the image).\n\nEach image section details:\n- `000001.jpg` has dimensions `1280 x 720` and contains 2 annotated regions.\n- `000002.jpg` also has dimensions `1280 x 720` and contains 3 annotated regions.\n\nThe numbers on the lines after the count likely describe the position and dimensions of bounding boxes for object detection tasks within each specified image."}
{"layout": 594, "type": "text", "text": "We can create a new dataset in  mmdet/datasets/my_dataset.py  to load the data. ", "page_idx": 66, "bbox": [72, 501.0394592285156, 409.0250549316406, 514.3494873046875], "page_size": [612.0, 792.0]}
{"layout": 595, "type": "image", "page_idx": 66, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_56.jpg", "bbox": [67, 520, 543, 708], "page_size": [612.0, 792.0], "ocr_text": "import mmcv\nimport numpy as np\n\nfrom .builder import DATASETS\nfrom .custom import CustomDataset\n@DATASETS. register_module()\nclass MyDataset (CustomDataset):\nCLASSES = ('person', 'bicycle', 'car', 'motorcycle')\n\ndef load_annotations(self, ann_file):\nann_list = mmcv.list_from_file(ann_file)\n\n", "vlm_text": "The image contains a snippet of Python code that seems to define a custom dataset for a machine learning project, potentially using the MMDetection framework. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `mmcv` and `numpy` are imported, with `numpy` being aliased as `np`.\n   - `DATASETS` is imported from a module named `.builder`.\n   - `CustomDataset` is imported from a module named `.custom`.\n\n2. **Class Definition**:\n   - A dataset class named `MyDataset` is defined, which inherits from `CustomDataset`.\n   - The dataset class is registered with a decorator `@DATASETS.register_module()`, which integrates this dataset into a framework, most likely MMDetection's dataset registry.\n\n3. **Attributes and Methods**:\n   - A class attribute `CLASSES` is defined as a tuple containing the classes `('person', 'bicycle', 'car', 'motorcycle')`.\n   - A method `load_annotations` is defined, which takes `self` and `ann_file` as parameters. This method reads annotations from a file using `mmcv.list_from_file(ann_file)` and stores it in `ann_list`.\n\nThis code is likely a part of a larger object detection framework setup. The `MyDataset` class appears to be a custom implementation tailored to handle datasets containing annotations for specific classes like person, bicycle, car, and motorcycle."}
{"layout": 596, "type": "image", "page_idx": 67, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_57.jpg", "bbox": [69, 80, 545, 468], "page_size": [612.0, 792.0], "ocr_text": "data_infos = []\nfor i, ann_line in enumerate(ann_list):\n\nif ann_line |= '#':\ncontinue\n\nimg_shape = ann_list[i + 2].splitC' ')\nwidth = int(img_shape[0])\n\nheight = int(img_shape[1])\nbbox_number = int(ann_list[i + 3])\n\nanns = ann_line.split(' ')\n\nbboxes = []\n\nlabels = []\n\nfor anns in ann_list[i + 4:i + 4 + bbox_number]:\nbboxes.append([float(ann) for ann in anns[:4]])\nlabels. append(int (anns[4]))\n\ndata_infos.append(\ndict(\nfilename=ann_list[i + 1],\nwidth=width,\nheight=height,\nann=dict (\nbboxes=np.array(bboxes) .astype(np. float32),\nlabels=np.array(labels) .astype(np.int64))\n)\n\nreturn data_infos\n\ndef get_ann_info(self, idx):\nreturn self.data_infos [idx] ['ann']\n\n", "vlm_text": "The image contains a snippet of Python code. The code appears to process a list of annotations (`ann_list`) and extracts information about image files, including their dimensions and bounding boxes with associated labels. The processed data is stored in a list called `data_infos`, each entry of which is a dictionary containing:\n\n- `filename`: The name of the image file.\n- `width`: The width of the image.\n- `height`: The height of the image.\n- `ann`: A dictionary with:\n  - `bboxes`: An array of bounding boxes as `np.float32`.\n  - `labels`: An array of labels as `np.int64`.\n\nThe `get_ann_info` method takes an index `idx` and returns the annotation information for the corresponding image from `data_infos`.\n\nKey parts of the code:\n- It loops through `ann_list` and checks if each line starts with `'#'`. If not, it continues to the next line.\n- It extracts image shape and bounding box number from specific indices.\n- Bounding box and label data are extracted and appended to the respective lists.\n- The processed information is appended to the `data_infos` list as a dictionary.\n- The `get_ann_info` function uses the `self.data_infos` list to retrieve information based on the provided index."}
{"layout": 597, "type": "text", "text": "Then in the config, to use  MyDataset  you can modify the config as the following ", "page_idx": 67, "bbox": [71, 468.9134826660156, 395.55621337890625, 482.2235107421875], "page_size": [612.0, 792.0]}
{"layout": 598, "type": "text", "text": "data set A train  $=$   dict ( type = ' MyDataset ' , ann_file  $=$   ' image_list.txt ' , pipeline  $=$  train pipeline ) ", "page_idx": 67, "bbox": [71, 491.95672607421875, 239.371337890625, 549.7901611328125], "page_size": [612.0, 792.0]}
{"layout": 599, "type": "text", "text": "9.2 Customize datasets by dataset wrappers ", "text_level": 1, "page_idx": 67, "bbox": [70, 580, 377, 597], "page_size": [612.0, 792.0]}
{"layout": 600, "type": "text", "text": "MM Detection also supports many dataset wrappers to mix the dataset or modify the dataset distribution for training. Currently it supports to three dataset wrappers as below: ", "page_idx": 67, "bbox": [71, 612.3024291992188, 540.0029907226562, 637.5675048828125], "page_size": [612.0, 792.0]}
{"layout": 601, "type": "text", "text": "•  Repeat Data set : simply repeat the whole dataset. •  Class Balanced Data set : repeat dataset in a class balanced manner. •  Con cat Data set : concat datasets. ", "page_idx": 67, "bbox": [88, 642.1904296875, 373.36895751953125, 691.365478515625], "page_size": [612.0, 792.0]}
{"layout": 602, "type": "text", "text": "9.2.1 Repeat dataset ", "text_level": 1, "page_idx": 68, "bbox": [71, 71, 190, 85], "page_size": [612.0, 792.0]}
{"layout": 603, "type": "text", "text": "We use  Repeat Data set  as wrapper to repeat the dataset. For example, suppose the original dataset is  Dataset_A , to repeat it, the config looks like the following ", "page_idx": 68, "bbox": [71, 95.81745910644531, 540, 121.08251190185547], "page_size": [612.0, 792.0]}
{"layout": 604, "type": "text", "text": "data set A train  $=$   dict ( type  $\\equiv^{\\dagger}$  Repeat Data set ' , times  $\\tt_{\\tau,=N}$  , dataset  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict ( # This is the original config of Dataset_A type  $\\equiv^{\\dagger}$  Dataset_A ' , ... pipeline  $=$  train pipeline ) ) ", "page_idx": 68, "bbox": [71, 130.8157501220703, 411.9746398925781, 236.47024536132812], "page_size": [612.0, 792.0]}
{"layout": 605, "type": "text", "text": "9.2.2 Class balanced dataset ", "text_level": 1, "page_idx": 68, "bbox": [71, 264, 238, 278], "page_size": [612.0, 792.0]}
{"layout": 606, "type": "text", "text": "We use  Class Balanced Data set  as wrapper to repeat the dataset based on category frequency. The dataset to repeat needs to instantiate function  self.get cat ids(idx)  to support  Class Balanced Data set . For example, to repeat Dataset_A  with  over sample thr  $\\scriptstyle:=1e^{-3}$  , the config looks like the following ", "page_idx": 68, "bbox": [71, 289.33843994140625, 540, 326.5584411621094], "page_size": [612.0, 792.0]}
{"layout": 607, "type": "table", "page_idx": 68, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_58.jpg", "bbox": [68, 333, 542, 446], "page_size": [612.0, 792.0], "ocr_text": "dataset_A_train = dict(\ntype='ClassBalancedDataset',\noversample_thr=1le-3,\ndataset=dict( # This is the original config of Dataset_A\ntype='Dataset_A',\n\npipeline=train_pipeline\n", "vlm_text": "The image shows a code snippet written in Python. It defines a dictionary `dataset_A_train` with the following structure:\n\n- **type**: 'ClassBalancedDataset'\n- **oversample_thr**: 1e-3\n- **dataset**: Another dictionary containing:\n  - **type**: 'Dataset_A'\n  - A comment: \"This is the original config of Dataset_A\"\n  - An ellipsis (`...`) indicating omitted content.\n- **pipeline**: Assigned to `train_pipeline`\n\nThis seems to be a configuration for a dataset used in a machine learning context."}
{"layout": 608, "type": "text", "text": "You may refer to  source code  for details. ", "page_idx": 68, "bbox": [71, 454.5184631347656, 233.1350860595703, 467.8284912109375], "page_size": [612.0, 792.0]}
{"layout": 609, "type": "text", "text": "9.2.3 Concatenate dataset ", "text_level": 1, "page_idx": 68, "bbox": [71, 487, 223, 501], "page_size": [612.0, 792.0]}
{"layout": 610, "type": "text", "text": "There are three ways to concatenate the dataset. ", "page_idx": 68, "bbox": [71, 512.7463989257812, 261.52850341796875, 526.0564575195312], "page_size": [612.0, 792.0]}
{"layout": 611, "type": "text", "text": "1. If the datasets you want to concatenate are in the same type with different annotation files, you can concatenate the dataset configs like the following. ", "page_idx": 68, "bbox": [84.4530029296875, 530.679443359375, 540, 555.9445190429688], "page_size": [612.0, 792.0]}
{"layout": 612, "type": "image", "page_idx": 68, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_59.jpg", "bbox": [93, 560, 543, 628], "page_size": [612.0, 792.0], "ocr_text": "dataset_A_train = dict(\ntype='Dataset_A',\nann_file = ['anno_file_1', ‘anno_file_2'],\npipeline=train_pipeline\n", "vlm_text": "The image shows a snippet of Python code. It defines a dictionary called `dataset_A_train`. The dictionary contains three key-value pairs:\n\n1. `type`: The value is a string `'Dataset_A'`.\n2. `ann_file`: The value is a list containing two strings: `'anno_file_1'` and `'anno_file_2'`.\n3. `pipeline`: The value is a variable `train_pipeline`."}
{"layout": 613, "type": "text", "text": "If the concatenated dataset is used for test or evaluation, this manner supports to evaluate each dataset separately. To test the concatenated datasets as a whole, you can set  separate e val  $\\b=$  False  as below. ", "page_idx": 68, "bbox": [96, 636.0844116210938, 540, 661.3494873046875], "page_size": [612.0, 792.0]}
{"layout": 614, "type": "table", "page_idx": 68, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_60.jpg", "bbox": [93, 666, 542, 709.75], "page_size": [612.0, 792.0], "ocr_text": "dataset_A_train = dict(\ntype='Dataset_A',\nann_file = ['anno_file_1', 'anno_file_2'],\n\n", "vlm_text": "The image you provided is not of a table in the traditional sense but rather a snippet of Python code. The code is defining a dictionary named `dataset_A_train` with the following key-value pairs:\n\n- `\"type\"`: This key has a value of `'Dataset_A'`. This typically represents the type or name of the dataset being described.\n- `\"ann_file\"`: This key has a value which is a list containing two strings: `'anno_file_1'` and `'anno_file_2'`. These strings likely represent names of annotation files related to the dataset. \n\nThis dictionary is commonly used in programming for setting configuration parameters related to datasets."}
{"layout": 615, "type": "text", "text": "(continues on next page) ", "page_idx": 68, "bbox": [461.9010009765625, 707.1355590820312, 540, 717.7836303710938], "page_size": [612.0, 792.0]}
{"layout": 616, "type": "table", "page_idx": 69, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_61.jpg", "bbox": [92, 83, 542, 126], "page_size": [612.0, 792.0], "ocr_text": "separate_eval=False,\npipeline=train_pipeline\n", "vlm_text": "This image shows a piece of code, not a table. It indicates two parameters being set in a configuration:\n\n- `separate_eval=False`: This suggests that the evaluation is not separate.\n- `pipeline=train_pipeline`: This assigns a training pipeline to a parameter named `pipeline`.\n\nThe code appears to be part of a larger configuration or function call related to machine learning or data processing."}
{"layout": 617, "type": "text", "text": "2. In case the dataset you want to concatenate is different, you can concatenate the dataset configs like the following. ", "page_idx": 69, "bbox": [84, 134.16847229003906, 540, 147.47850036621094], "page_size": [612.0, 792.0]}
{"layout": 618, "type": "image", "page_idx": 69, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_62.jpg", "bbox": [92, 152, 543, 316], "page_size": [612.0, 792.0], "ocr_text": "dataset_A_train\ndataset_B_train\n\ndictQO\ndictQO\n\ndata = dict(\nimgs_per_gpu=2,\nworkers_per_gpu=2,\ntrain = [\ndataset_A_train,\ndataset_B_train\n],\nval = dataset_A_val,\ntest = dataset_A_test\n)\n\n", "vlm_text": "The image shows a Python code snippet that appears to be configuring or defining some data parameters for a machine learning or deep learning task. Here's a breakdown of the code:\n\n1. Two empty dictionaries are initialized:\n   - `dataset_A_train = dict()`\n   - `dataset_B_train = dict()`\n\n2. Another dictionary named `data` is defined with various parameters:\n   - `imgs_per_gpu=2`: This suggests that 2 images will be processed per GPU.\n   - `workers_per_gpu=2`: This indicates there will be 2 worker processes per GPU.\n   - `train = [...]`: This is a list containing `dataset_A_train` and `dataset_B_train`, suggesting that these datasets will be used for training.\n   - `val = dataset_A_val`: This indicates that `dataset_A_val` is used for validation purposes.\n   - `test = dataset_A_test`: This suggests that `dataset_A_test` is used for testing.\n\nThe code snippet seems to set up data configurations, possibly for use in a deep learning model training setup, with specific training, validation, and test datasets."}
{"layout": 619, "type": "text", "text": "If the concatenated dataset is used for test or evaluation, this manner also supports to evaluate each dataset separately. ", "page_idx": 69, "bbox": [96, 323.25946044921875, 540, 348.52447509765625], "page_size": [612.0, 792.0]}
{"layout": 620, "type": "text", "text": "3. We also support to define  Con cat Data set  explicitly as the following. ", "page_idx": 69, "bbox": [84, 353.1474609375, 376.8159484863281, 366.4574890136719], "page_size": [612.0, 792.0]}
{"layout": 621, "type": "image", "page_idx": 69, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_63.jpg", "bbox": [92, 373, 542, 511], "page_size": [612.0, 792.0], "ocr_text": "dataset_A_val\ndataset_B_val\n\ndictO\ndictO\n\ndata = dict(\n\nimgs_per_gpu=2,\n\nworkers_per_gpu=2,\n\ntrain=dataset_A_train,\n\nval=dict(\ntype='ConcatDataset',\ndatasets=[dataset_A_val, dataset_B_val],\nseparate_eval=False))\n\n", "vlm_text": "This image shows a snippet of Python code related to setting up datasets for training and validation in a machine learning context. Here's a breakdown of the code:\n\n1. `dataset_A_val` and `dataset_B_val` are initialized as empty dictionaries.\n2. `data` is a dictionary that includes:\n   - `imgs_per_gpu`: Set to 2, indicating the number of images processed per GPU.\n   - `workers_per_gpu`: Set to 2, indicating the number of workers per GPU.\n   - `train`: Assigned to `dataset_A_train`, likely referring to the training dataset.\n   - `val`: A dictionary for validation settings with:\n     - `type`: Set to `'ConcatDataset'`, suggesting that the validation datasets will be concatenated.\n     - `datasets`: A list containing `dataset_A_val` and `dataset_B_val`.\n     - `separate_eval`: Set to `False`, indicating whether evaluation of datasets should be separate.\n\nThis setup is typical in machine learning frameworks for managing data input pipelines."}
{"layout": 622, "type": "text", "text": "This manner allows users to evaluate all the datasets as a single one by setting  separate e val  $\\cdot^{-}$  False . ", "page_idx": 69, "bbox": [96, 518.3284301757812, 510.8836669921875, 531.6384887695312], "page_size": [612.0, 792.0]}
{"layout": 623, "type": "text", "text": "Note: ", "text_level": 1, "page_idx": 69, "bbox": [71, 538, 95, 549], "page_size": [612.0, 792.0]}
{"layout": 624, "type": "text", "text": "1. The option  separate e val  $\\b=$  False  assumes the datasets use  self.data_infos  during evaluation. There- fore, COCO datasets do not support this behavior since COCO datasets do not fully rely on  self.data_infos for evaluation. Combining different types of datasets and evaluating them as a whole is not tested thus is not suggested. 2. Evaluating  Class Balanced Data set  and  Repeat Data set  is not supported thus evaluating concatenated ", "page_idx": 69, "bbox": [84, 554.1934204101562, 540, 621.302490234375], "page_size": [612.0, 792.0]}
{"layout": 625, "type": "text", "text": "datasets of these types is also not supported. ", "page_idx": 69, "bbox": [96, 619.9474487304688, 273.1453552246094, 633.2575073242188], "page_size": [612.0, 792.0]}
{"layout": 626, "type": "text", "text": "A more complex example that repeats  Dataset_A  and  Dataset_B  by N and M times, respectively, and then concate- nates the repeated datasets is as the following. ", "page_idx": 69, "bbox": [72, 637.8804321289062, 540, 663.1454467773438], "page_size": [612.0, 792.0]}
{"layout": 627, "type": "table", "page_idx": 69, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_64.jpg", "bbox": [68, 669, 541, 713], "page_size": [612.0, 792.0], "ocr_text": "dataset_A_train = dict(\ntype='RepeatDataset',\ntimes=N,\n\n", "vlm_text": "The image shows a snippet of code within a table or box, which defines a dictionary named `dataset_A_train`. The dictionary appears to be part of a configuration for a dataset in a machine learning or data processing task. Specifically, the dictionary includes:\n\n- A key `type` with the value `'RepeatDataset'`, which suggests that the dataset is intended to be repeated a certain number of times.\n- A key `times` with a value `N`, which likely specifies how many times the dataset should be repeated.\n\nThis kind of configuration is common in machine learning workflows where datasets need to be processed multiple times or augmented to increase the variety of training data."}
{"layout": 628, "type": "image", "page_idx": 70, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_65.jpg", "bbox": [68, 81, 545, 485], "page_size": [612.0, 792.0], "ocr_text": "dataset=dict(\ntype='Dataset_A',\n\npipeline=train_pipeline\n)\n\n)\ndataset_A_val = dict(\n\npipeline=test_pipeline\n)\ndataset_A_test = dict(\n\npipeline=test_pipeline\n)\ndataset_B_train = dict(\ntype='RepeatDataset',\ntimes=M,\ndataset=dict(\ntype='Dataset_B',\n\npipeline=train_pipeline\n)\n)\ndata = dict(\nimgs_per_gpu=2,\nworkers_per_gpu=2,\ntrain = [\ndataset_A_train,\ndataset_B_train\n1,\nval = dataset_A_val,\ntest = dataset_A_test\n\n", "vlm_text": "The image shows a configuration script in Python that sets up data pipelines for different datasets. Here's a breakdown:\n\n- `dataset`: Configures `Dataset_A` with a training pipeline (`train_pipeline`).\n- `dataset_A_val`: Defines a validation dataset for `Dataset_A` with a test pipeline (`test_pipeline`).\n- `dataset_A_test`: Defines a test dataset for `Dataset_A` with a test pipeline.\n- `dataset_B_train`: Configures `Dataset_B` as a repeatable training dataset (`RepeatDataset`) repeated `M` times, also using `train_pipeline`.\n- `data`: Overall data configuration specifying:\n  - Images per GPU (`imgs_per_gpu=2`).\n  - Workers per GPU (`workers_per_gpu=2`).\n  - Training list includes `dataset_A_train` and `dataset_B_train`.\n  - Validation set as `dataset_A_val`.\n  - Test set as `dataset_A_test`. \n\nThis setup is commonly used in machine learning and data processing pipelines."}
{"layout": 629, "type": "text", "text": "9.3 Modify Dataset Classes ", "text_level": 1, "page_idx": 70, "bbox": [70, 510, 261, 527], "page_size": [612.0, 792.0]}
{"layout": 630, "type": "text", "text": "With existing dataset types, we can modify the class names of them to train subset of the annotations. For example, if you want to train only three classes of the current dataset, you can modify the classes of dataset. The dataset will filter out the ground truth boxes of other classes automatically. ", "page_idx": 70, "bbox": [71, 542.762451171875, 540, 579.9834594726562], "page_size": [612.0, 792.0]}
{"layout": 631, "type": "text", "text": "classes  $=$   ( ' person ' ,  ' bicycle ' ,  ' car ' ) data  $=$   dict ( train  $\\overbar{\\ }$  dict (classes  $,=$  classes), val  $=$  dict (classes  $,=$  classes), test  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  dict (classes  $,=$  classes)) ", "page_idx": 70, "bbox": [71, 589.7167358398438, 270.75433349609375, 647.5501708984375], "page_size": [612.0, 792.0]}
{"layout": 632, "type": "text", "text": "MM Detection V2.0 also supports to read the classes from a file, which is common in real applications. For example, assume the  classes.txt  contains the name of classes as the following. ", "page_idx": 70, "bbox": [71, 660.1224365234375, 540, 685.3875122070312], "page_size": [612.0, 792.0]}
{"layout": 633, "type": "table", "page_idx": 71, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_66.jpg", "bbox": [66, 75, 543, 118], "page_size": [612.0, 792.0], "ocr_text": "person\nbicycle\ncar\n", "vlm_text": "The table contains a list with three items: \"person,\" \"bicycle,\" and \"car.\" There is no additional context or data provided in the table."}
{"layout": 634, "type": "text", "text": "Users can set the classes as a file path, the dataset will load it and convert it to a list automatically. ", "page_idx": 71, "bbox": [72, 126.04743957519531, 461.7169189453125, 139.3574676513672], "page_size": [612.0, 792.0]}
{"layout": 635, "type": "table", "page_idx": 71, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_67.jpg", "bbox": [68, 143, 544, 212], "page_size": [612.0, 792.0], "ocr_text": "classes = 'path/to/classes.txt'\n\ndata = dict(\ntrain=dict(classes=classes) ,\nval=dict(classes=classes),\ntest=dict(classes=classes))\n\n", "vlm_text": "The image depicts a code snippet rather than a traditional table with rows and columns. The code is written in Python and is used to create a dictionary. Here is an explanation of the code:\n\n1. `classes = 'path/to/classes.txt'`: This line assigns a file path (as a string) to the variable `classes`. The file path points to a text file presumably containing class labels.\n\n2. `data = dict(...)`: This line creates a dictionary called `data`.\n\n3. Within the `dict(...)` constructor:\n   - `train=dict(classes=classes)`: This creates a dictionary for the training dataset with a key `classes` and assigns it the value of the `classes` variable.\n   - `val=dict(classes=classes)`: This creates a dictionary for the validation dataset with a key `classes` and assigns it the value of the `classes` variable.\n   - `test=dict(classes=classes)`: This creates a dictionary for the test dataset with a key `classes` and assigns it the value of the `classes` variable.\n\nIn summary, this code snippet is defining a dictionary that contains sub-dictionaries for training, validation, and testing datasets, each linking to the specified classes within the `classes.txt` file.\n"}
{"layout": 636, "type": "text", "text": "Note : ", "text_level": 1, "page_idx": 71, "bbox": [71, 221, 94, 232], "page_size": [612.0, 792.0]}
{"layout": 637, "type": "text", "text": "• Before MM Detection v2.5.0, the dataset will filter out the empty GT images automatically if the classes are set and there is no way to disable that through config. This is an undesirable behavior and introduces confusion be- cause if the classes are not set, the dataset only filter the empty GT images when  filter empty gt  $=$  True  and test_mode  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  False . After MM Detection v2.5.0, we decouple the image filtering process and the classes modifi- cation, i.e., the dataset will only filter empty GT images when  filter empty gt  $\\cdot^{-}$  True  and  test_mode  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  False , no matter whether the classes are set. Thus, setting the classes only influences the annotations of classes used for training and users could decide whether to filter empty GT images by themselves. ", "page_idx": 71, "bbox": [88, 237.42945861816406, 540, 322.470458984375], "page_size": [612.0, 792.0]}
{"layout": 638, "type": "text", "text": "• Since the middle format only has box labels and does not contain the class names, when using  Custom Data set , users cannot filter out the empty GT images through configs but only do this offline. ", "page_idx": 71, "bbox": [88, 327.09344482421875, 540, 352.35845947265625], "page_size": [612.0, 792.0]}
{"layout": 639, "type": "text", "text": "• Please remember to modify the  num classes  in the head when specifying  classes  in dataset. We implemented Num Class Check Hook  to check whether the numbers are consistent since v2.9.0(after PR#4508). ", "page_idx": 71, "bbox": [88, 356.9814453125, 540, 382.2464599609375], "page_size": [612.0, 792.0]}
{"layout": 640, "type": "text", "text": "• The features for setting dataset classes and dataset filtering will be refactored to be more user-friendly in the future (depends on the progress). ", "page_idx": 71, "bbox": [88, 386.86944580078125, 540, 412.13446044921875], "page_size": [612.0, 792.0]}
{"layout": 641, "type": "text", "text": "9.4 COCO Panoptic Dataset ", "text_level": 1, "page_idx": 71, "bbox": [70, 434, 263, 451], "page_size": [612.0, 792.0]}
{"layout": 642, "type": "text", "text": "Now we support COCO Panoptic Dataset, the format of panoptic annotations is different from COCO format. Both the foreground and the background will exist in the annotation file. The annotation json files in COCO Panoptic format has the following necessary keys: ", "page_idx": 71, "bbox": [72, 466.6964111328125, 540, 503.9164123535156], "page_size": [612.0, 792.0]}
{"layout": 643, "type": "image", "page_idx": 71, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_68.jpg", "bbox": [69, 507, 544, 709], "page_size": [612.0, 792.0], "ocr_text": "‘images': [\n\n{\n\"file_name': '000000001268.jpg',\n\"height': 427,\n\"width': 640,\n‘id': 1268\n3,\n\n]\n\n‘annotations': [\n{\n\"filename': '000000001268.jpg',\n\"image_id': 1268,\n\"segments_info': [\n\n{\n\n", "vlm_text": "The image contains a snippet of JSON data. It appears to represent metadata for an image. \n\nKey information includes:\n- The file name is `000000001268.jpg`.\n- The image dimensions are 640x427 pixels.\n- The image has an ID of 1268.\n- There is an `annotations` section that references the same image and includes a field `segments_info`, which might contain additional data (not fully visible here)."}
{"layout": 644, "type": "image", "page_idx": 72, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_69.jpg", "bbox": [68, 83, 542, 306], "page_size": [612.0, 792.0], "ocr_text": "\"id':8345037, # One-to-one correspondence with the id in the annotation,\nmap.\n\n\"category_id': 51,\n\n‘iscrowd': 0,\n\n\"bbox': (x1, yl, w, h), # The bbox of the background is the outer,\norectangle of its mask.\n\n\"area': 24315\n\nby\n\n]\n\n\"categories': [ # including both foreground categories and background categories\n{'id': 0, 'name': 'person'},\n\n", "vlm_text": "The image shows a code snippet that appears to be part of a data annotation or object detection process, likely in JSON format. It includes information about:\n\n- An \"id\" with the value `8345037`.\n- A \"category_id\" with the value `51`.\n- An \"iscrowd\" flag set to `0`.\n- A \"bbox\" (bounding box) defined by the coordinates `(x1, y1, w, h)`.\n- An \"area\" with the value `24315`.\n- A \"categories\" list containing an object with `id: 0` and `name: 'person'`.\n\nThe snippet also contains comments explaining the structure."}
{"layout": 645, "type": "text", "text": "Moreover, the  seg_prefix  must be set to the path of the panoptic annotation images. ", "page_idx": 72, "bbox": [71, 313.4964599609375, 413.707275390625, 326.8064880371094], "page_size": [612.0, 792.0]}
{"layout": 646, "type": "text", "text": "data  =  dict ( type = ' Coco Pan optic Data set ' , train  $\\overbar{\\ }$  dict ( seg_prefix  $=$   ' path/to/your/train/panoptic/image annotation data ' ), val  $\\mathbf{\\beta}\\!=$  dict ( seg_prefix  $=$   ' path/to/your/train/panoptic/image annotation data ' ) ) ", "page_idx": 72, "bbox": [71, 336.53973388671875, 448, 442.1940612792969], "page_size": [612.0, 792.0]}
{"layout": 647, "type": "text", "text": "TUTORIAL 3: CUSTOMIZE DATA PIPELINES ", "text_level": 1, "page_idx": 74, "bbox": [242, 164, 540, 181], "page_size": [612.0, 792.0]}
{"layout": 648, "type": "text", "text": "10.1 Design of Data pipelines ", "text_level": 1, "page_idx": 74, "bbox": [71, 229, 274, 245], "page_size": [612.0, 792.0]}
{"layout": 649, "type": "text", "text": "Following typical conventions, we use  Dataset  and  DataLoader  for data loading with multiple workers.  Dataset returns a dict of data items corresponding the arguments of models’ forward method. Since the data in object detection may not be the same size (image size, gt bbox size, etc.), we introduce a new  Data Container  type in MMCV to help collect and distribute data of different size. See  here  for more details. ", "page_idx": 74, "bbox": [72, 260.8024597167969, 540, 309.9774475097656], "page_size": [612.0, 792.0]}
{"layout": 650, "type": "text", "text": "The data preparation pipeline and the dataset is decomposed. Usually a dataset defines how to process the annotations and a data pipeline defines all the steps to prepare a data dict. A pipeline consists of a sequence of operations. Each operation takes a dict as input and also output a dict for the next transform. ", "page_idx": 74, "bbox": [72, 314.6004333496094, 540, 351.8214416503906], "page_size": [612.0, 792.0]}
{"layout": 651, "type": "text", "text": "We present a classical pipeline in the following figure. The blue blocks are pipeline operations. With the pipeline going on, each operator can add new keys (marked as green) to the result dict or update the existing keys (marked as orange). ", "page_idx": 74, "bbox": [72, 356.44342041015625, 540, 381.70843505859375], "page_size": [612.0, 792.0]}
{"layout": 652, "type": "image", "page_idx": 74, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_70.jpg", "bbox": [71, 381.25, 542, 531], "page_size": [612.0, 792.0], "ocr_text": "“gt labels”\n\n} “bbox_t fields\":\n\n“bbox_fields”:\n“scale\n*scale_idx’:\n\n“scale_factor’:\n\n*keep_ratio”:\n“flip”:\n\n“keep_ratio”:\n“tip”:\n“img_norm_cfg”:\n\n}\n\nis\n\n“bbox_fields”:\n“scale”:\n“scale_idx”:\n“scale_factor’:\n*keep_rati\n“ip”:\n*img_norm_cfg\":\n*pad_fixed_size’\n\n“pad_size_divisor”:\n\n“pad_size_divisor’:\n\n“gt_labels”:\n“bbox_fields”:\n\n“img”:\n“img_meta”: {\n“ori_shape\n*img_shape\":\n“pad_shape\"\n“scale_factor”:\n“tip”:\n*img_norm_cfg”:\n\n+\n“gt_bboxes”:\n“gt labels”:\n\n", "vlm_text": "The image is a flowchart depicting a data processing pipeline for image preprocessing tasks typically performed in computer vision, specifically in the field of object detection. The pipeline includes the following steps:\n\n1. **LoadImageFromFile**: Load the image and capture its shape and original shape.\n2. **LoadAnnotations**: Add annotations such as ground truth bounding boxes and labels.\n3. **Resize**: Adjust the image size, updating attributes like padding shape, scale, index, factor, and ratio.\n4. **RandomFlip**: Optionally flip the image, recording the flip status.\n5. **Normalize**: Apply normalization to the image using a specific config.\n6. **Pad**: Add padding if necessary, updating padding attributes.\n7. **DefaultFormatBundle**: Prepare data with a fixed size and divisor for consistent formatting.\n8. **Collect**: Gather all processed image data and metadata for further use.\n\nEach step adds or modifies data attributes, preparing the image for model input."}
{"layout": 653, "type": "text", "text": "figure The operations are categorized into data loading, pre-processing, formatting and test-time augmentation. ", "page_idx": 74, "bbox": [72, 532.29345703125, 487.9883117675781, 563.5364990234375], "page_size": [612.0, 792.0]}
{"layout": 654, "type": "text", "text": "Here is a pipeline example for Faster R-CNN. ", "page_idx": 74, "bbox": [72, 568.159423828125, 253.12005615234375, 581.469482421875], "page_size": [612.0, 792.0]}
{"layout": 655, "type": "table", "page_idx": 74, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_71.jpg", "bbox": [68, 586, 543, 715], "page_size": [612.0, 792.0], "ocr_text": "img_norm_cfg = dict(\n\nmean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n\ndict(type='LoadImageFromFile'),\n\ndict(type='LoadAnnotations', with_bbox=True) ,\n\ndict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n\ndict(type='RandomFlip', flip_ratio=0.5),\n\ndict(type='Normalize', **img_norm_cfg),\n\ndict(type='Pad', size_divisor=32),\n\ndict(type='DefaultFormatBundle'),\n\n", "vlm_text": "The image appears to show a configuration snippet for a deep learning framework, likely related to the preprocessing pipeline for training images for a computer vision model. Here's a breakdown of what each part of the table does:\n\n1. **img_norm_cfg**: This is a dictionary containing normalization parameters.\n   - **mean**: The mean values for each channel (R, G, B) used to normalize the images.\n   - **std**: The standard deviation values for each channel (R, G, B) used for normalization.\n   - **to_rgb**: A Boolean set to `True`, indicating whether to convert images to RGB format.\n\n2. **train_pipeline**: This is a list that outlines the sequence of operations applied to training images.\n   - **LoadImageFromFile**: Loads an image from a file.\n   - **LoadAnnotations**: Loads the annotations, with bounding box information included (indicated by `with_bbox=True`).\n   - **Resize**: Resizes the image to the specified scale (1333, 800) while maintaining the aspect ratio (`keep_ratio=True`).\n   - **RandomFlip**: May randomly flip the image horizontally with a probability defined by `flip_ratio=0.5`.\n   - **Normalize**: Applies normalization to the image using the settings from `img_norm_cfg`.\n   - **Pad**: Pads the image to make its dimensions a multiple of the specified `size_divisor`, which is 32 in this case.\n   - **DefaultFormatBundle**: Transforms and packages the data into a default format that the model can process.\n\nThis pipeline is typically used to ensure that images are preprocessed consistently in a manner that helps improve model training stability and accuracy."}
{"layout": 656, "type": "image", "page_idx": 75, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_72.jpg", "bbox": [70, 81, 543, 295], "page_size": [612.0, 792.0], "ocr_text": "dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntrans forms=[\ndict(type='Resize', keep_ratio=True),\ndict (type='RandomFlip')\ndict(type='Normalize', **img_norm_cfg) ,\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n\n1)\n\n", "vlm_text": "The image contains a snippet of Python code that appears to configure a data processing pipeline, presumably for a machine learning application, possibly one involving computer vision. Here's a breakdown of what the code is doing:\n\n1. **LoadImageFromFile**: This seems to load images from a file.\n\n2. **MultiScaleFlipAug**: This might be a transformation to handle multi-scale or flip augmentation:\n   - `img_scale=(1333, 800)`: This likely sets the image scale to a specific size.\n   - `flip=False`: This indicates that the image should not be flipped.\n\n3. **Transforms**: A series of transformations to be applied to the images:\n   - `Resize`: Resizes the image, maintaining the aspect ratio (`keep_ratio=True`).\n   - `RandomFlip`: Randomly flips the image.\n   - `Normalize`: Normalizes the image data using some configuration (`**img_norm_cfg`).\n   - `Pad`: Pads the image to match the size divisor of 32.\n   - `ImageToTensor`: Converts the image to a tensor, which is a common format for input data in machine learning.\n   - `Collect`: Collects the `img` key.\n\nOverall, this code likely defines a series of pre-processing steps for image data before feeding it into a neural network for tasks like image recognition or object detection. This type of setup is common in deep learning frameworks like PyTorch or TensorFlow when working with custom datasets."}
{"layout": 657, "type": "text", "text": "For each operation, we list the related dict fields that are added/updated/removed. ", "page_idx": 75, "bbox": [72, 301.5414733886719, 395.0572204589844, 314.85150146484375], "page_size": [612.0, 792.0]}
{"layout": 658, "type": "text", "text": "10.1.1 Data loading ", "text_level": 1, "page_idx": 75, "bbox": [71, 334, 184, 349], "page_size": [612.0, 792.0]}
{"layout": 659, "type": "text", "text": "Load Image From File ", "page_idx": 75, "bbox": [72, 362.0907287597656, 160.91615295410156, 372.1031494140625], "page_size": [612.0, 792.0]}
{"layout": 660, "type": "text", "text": "", "page_idx": 75, "bbox": [87, 379, 226, 386.75], "page_size": [612.0, 792.0]}
{"layout": 661, "type": "text", "text": "Load Annotations • add: gt_bboxes, gt b boxes ignore, gt_labels, gt_masks, gt semantic seg, b box fields, mask fields Load Proposals • add: proposals ", "page_idx": 75, "bbox": [72, 397.95574951171875, 492.0934143066406, 462.7435607910156], "page_size": [612.0, 792.0]}
{"layout": 662, "type": "text", "text": "10.1.2 Pre-processing ", "text_level": 1, "page_idx": 75, "bbox": [71, 483, 199, 496], "page_size": [612.0, 792.0]}
{"layout": 663, "type": "text", "text": "Resize ", "page_idx": 75, "bbox": [72, 509.9827880859375, 103.3821792602539, 519.9952392578125], "page_size": [612.0, 792.0]}
{"layout": 664, "type": "text", "text": "• add: scale, scale_idx, pad_shape, scale factor, keep_ratio • update: img, img_shape, \\*b box fields, \\*mask fields, \\*seg_fields ", "page_idx": 75, "bbox": [88, 525.593505859375, 356.69171142578125, 556.8365478515625], "page_size": [612.0, 792.0]}
{"layout": 665, "type": "text", "text": "RandomFlip ", "page_idx": 75, "bbox": [72, 563.7808227539062, 124.30362701416016, 573.7932739257812], "page_size": [612.0, 792.0]}
{"layout": 666, "type": "text", "text": "• add: flip • update: img, \\*b box fields, \\*mask fields, \\*seg_fields ", "page_idx": 75, "bbox": [88, 579.3925170898438, 308.5424499511719, 610.6345825195312], "page_size": [612.0, 792.0]}
{"layout": 667, "type": "text", "text": "Pad ", "page_idx": 75, "bbox": [72, 617.5787963867188, 87.69109344482422, 627.5912475585938], "page_size": [612.0, 792.0]}
{"layout": 668, "type": "text", "text": "• add: pad fixed size, pad size divisor • update: img, pad_shape, \\*mask fields, \\*seg_fields ", "page_idx": 75, "bbox": [88, 633.1904907226562, 299.466552734375, 664.4335327148438], "page_size": [612.0, 792.0]}
{"layout": 669, "type": "text", "text": "RandomCrop ", "text_level": 1, "page_idx": 75, "bbox": [71, 671, 125, 682], "page_size": [612.0, 792.0]}
{"layout": 670, "type": "text", "text": "• update: img, pad_shape, gt_bboxes, gt_labels, gt_masks, \\*b box fields ", "page_idx": 75, "bbox": [88, 686.9884643554688, 377.0652160644531, 700.2985229492188], "page_size": [612.0, 792.0]}
{"layout": 671, "type": "text", "text": "Normalize ", "page_idx": 75, "bbox": [72, 707.2427978515625, 119.0732650756836, 717.2552490234375], "page_size": [612.0, 792.0]}
{"layout": 672, "type": "text", "text": "• add: img norm cf g • update: img SegRescale • update: gt semantic seg PhotoMetric Distortion • update: img Expand • update: img, gt_bboxes MinI oU Random Crop • update: img, gt_bboxes, gt_labels Corrupt• update: img ", "page_idx": 76, "bbox": [71, 71.45246887207031, 231.3024444580078, 282.02239990234375], "page_size": [612.0, 792.0]}
{"layout": 673, "type": "text", "text": "10.1.3 Formatting ", "text_level": 1, "page_idx": 76, "bbox": [71, 301, 175, 316], "page_size": [612.0, 792.0]}
{"layout": 674, "type": "text", "text": "ToTensor ", "page_idx": 76, "bbox": [71, 329.2616271972656, 113.84290313720703, 339.2740478515625], "page_size": [612.0, 792.0]}
{"layout": 675, "type": "text", "text": "• update: specified by  keys . ", "page_idx": 76, "bbox": [88, 416.60443115234375, 203.0586395263672, 429.9144592285156], "page_size": [612.0, 792.0]}
{"layout": 676, "type": "text", "text": "• update: specified by  fields . ", "page_idx": 76, "bbox": [88, 452.4694519042969, 213.51963806152344, 465.77947998046875], "page_size": [612.0, 792.0]}
{"layout": 677, "type": "text", "text": "Default Format Bundle ", "page_idx": 76, "bbox": [71, 472.7237243652344, 171.37686157226562, 482.73614501953125], "page_size": [612.0, 792.0]}
{"layout": 678, "type": "text", "text": "• update: img, proposals, gt_bboxes, gt b boxes ignore, gt_labels, gt_masks, gt semantic seg ", "page_idx": 76, "bbox": [88, 488.3354797363281, 464.2080078125, 501.6455078125], "page_size": [612.0, 792.0]}
{"layout": 679, "type": "text", "text": "• add: img_meta (the keys of img_meta is specified by  meta_keys ) • remove: all other keys except for those specified by  keys ", "page_idx": 76, "bbox": [88, 524.2005004882812, 360.4585266113281, 555.4435424804688], "page_size": [612.0, 792.0]}
{"layout": 680, "type": "text", "text": "10.1.4 Test time augmentation ", "text_level": 1, "page_idx": 76, "bbox": [71, 575, 245, 589], "page_size": [612.0, 792.0]}
{"layout": 681, "type": "text", "text": "Multi Scale Flip Aug ", "page_idx": 76, "bbox": [71, 602.6827392578125, 160.91615295410156, 612.6951904296875], "page_size": [612.0, 792.0]}
{"layout": 682, "type": "text", "text": "10.2 Extend and use custom pipelines ", "text_level": 1, "page_idx": 77, "bbox": [72, 72, 335, 87], "page_size": [612.0, 792.0]}
{"layout": 683, "type": "text", "text": "1. Write a new pipeline in a file, e.g., in  my pipeline.py . It takes a dict as input and returns a dict.\n\n ", "page_idx": 77, "bbox": [84, 102.99446105957031, 486.6542053222656, 116.30449676513672], "page_size": [612.0, 792.0]}
{"layout": 684, "type": "table", "page_idx": 77, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_73.jpg", "bbox": [93, 121, 543, 357], "page_size": [612.0, 792.0], "ocr_text": "import random\nfrom mmdet.datasets import PIPELINES\n\n@PIPELINES. register_module()\nclass MyTransform:\n\"\"\"Add your transform\n\nArgs:\np (float): Probability of shifts. Default 0.5.\n\nnoe\n\ndef __init__(self, p=0.5):\nself.p =p\n\ndef _call__(self, results):\nif random.random() > self.p:\nresults['dummy'] = True\nreturn results\n\n", "vlm_text": "The image shows a Python code snippet. It defines a custom transform class for a machine learning pipeline using a decorator from `mmdet.datasets`. Here’s a breakdown:\n\n- Imports:\n  - `random`\n  - `Pipelines` from `mmdet.datasets`\n\n- `@PIPELINES.register_module()` is used as a decorator to register the class as a module.\n\n- Class: `MyTransform`\n  - It has a docstring for documentation.\n  - **Args:**\n    - `p (float)`: Probability of shifts, with a default value of 0.5.\n\n- Method: `__init__`\n  - Initializes with a parameter `p`, defaulting to 0.5.\n\n- Method: `__call__`\n  - Takes `results` as a parameter.\n  - Uses `random.random()` to decide if a shift should occur based on the probability `p`.\n  - If the condition is met, it adds `results['dummy'] = True`.\n  - Returns `results`."}
{"layout": 685, "type": "text", "text": "2. Import and use the pipeline in your config file. Make sure the import is relative to where your train script is located. ", "page_idx": 77, "bbox": [84, 363.81646728515625, 540, 389.08148193359375], "page_size": [612.0, 792.0]}
{"layout": 686, "type": "text", "text": "custom imports  $=$   dict (imports  $=$  [ ' path.to.my pipeline ' ], allow failed imports  $\\risingdotseq$  False ) img norm cf g  $=$   dict ( mean  $\\risingdotseq$  [ 123.675 ,  116.28 ,  103.53 ], std  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  [ 58.395 ,  57.12 ,  57.375 ], to_rgb  $\\leftrightharpoons$  True ) train pipeline    $=$   [ dict ( type  $=\"$  Load Image From File ' ), dict ( type  $\\risingdotseq$  Load Annotations ' , with_bbox  $\\risingdotseq$  True ), dict ( type  $=\"$  Resize ' , img_scale  $=$  ( 1333 ,  800 ), keep_ratio  $\\risingdotseq$  True ), dict ( type  $\\risingdotseq$  RandomFlip ' , flip_ratio  $\\scriptstyle=\\0$  .5 ), dict ( type  $\\equiv^{1}$  Normalize ' ,  \\*\\* img norm cf g), dict ( type  $\\scriptstyle{\\varepsilon}$  ' Pad ' , size divisor  $=\\!32$  ), dict ( type  $=\"$  My Transform ' ,   $\\scriptstyle{\\mathfrak{p}}=0\\,.\\,2)$  , dict ( type  $\\circeq$  Default Format Bundle ' ), dict ( type  $\\risingdotseq$  Collect ' , keys  $,=$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ]), ]\n\n ", "page_idx": 77, "bbox": [96, 398.21697998046875, 525.79736328125, 576.1990966796875], "page_size": [612.0, 792.0]}
{"layout": 687, "type": "text", "text": "3. Visualize the output of your augmentation pipeline ", "page_idx": 77, "bbox": [84, 588.7724609375, 299.41595458984375, 602.08251953125], "page_size": [612.0, 792.0]}
{"layout": 688, "type": "text", "text": "To visualize the output of your ag ment ation pipeline,  tools/misc/browse data set.py  can help the user to browse a detection dataset (both images and bounding box annotations) visually, or save the image to a designated directory. More detials can refer to  useful tools ", "page_idx": 77, "bbox": [96, 606.7054443359375, 540, 643.9254760742188], "page_size": [612.0, 792.0]}
{"layout": 689, "type": "text", "text": "TUTORIAL 4: CUSTOMIZE MODELS ", "text_level": 1, "page_idx": 78, "bbox": [296, 163, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 690, "type": "text", "text": "We basically categorize model components into 5 types. ", "page_idx": 78, "bbox": [72, 225.87342834472656, 295.9293518066406, 239.18345642089844], "page_size": [612.0, 792.0]}
{"layout": 691, "type": "text", "text": "• backbone: usually an FCN network to extract feature maps, e.g., ResNet, MobileNet. • neck: the component between backbones and heads, e.g., FPN, PAFPN. • head: the component for specific tasks, e.g., bbox prediction and mask prediction. • roi extractor: the part for extracting RoI features from feature maps, e.g., RoI Align. • loss: the component in head for calculating losses, e.g., FocalLoss, L1Loss, and GHMLoss. ", "page_idx": 78, "bbox": [88, 243.80543518066406, 461.5682067871094, 328.84649658203125], "page_size": [612.0, 792.0]}
{"layout": 692, "type": "text", "text": "11.1 Develop new components ", "text_level": 1, "page_idx": 78, "bbox": [71, 352, 285, 368], "page_size": [612.0, 792.0]}
{"layout": 693, "type": "text", "text": "11.1.1 Add a new backbone ", "page_idx": 78, "bbox": [72, 383.49835205078125, 229.9521484375, 400.61822509765625], "page_size": [612.0, 792.0]}
{"layout": 694, "type": "text", "text": "Here we show how to develop new components with an example of MobileNet. ", "page_idx": 78, "bbox": [72, 410.80242919921875, 387.4257507324219, 424.1124572753906], "page_size": [612.0, 792.0]}
{"layout": 695, "type": "text", "text": "1. Define a new backbone (e.g. MobileNet) ", "page_idx": 78, "bbox": [72, 441.97503662109375, 272.1983947753906, 456.2414855957031], "page_size": [612.0, 792.0]}
{"layout": 696, "type": "text", "text": "Create a new file  mmdet/models/backbones/mobilenet.py . import  torch.nn  as  nn from  ..builder  import  BACKBONES @BACKBONES.register module()class  MobileNet (nn . Module): def  __init__ ( self , arg1, arg2): pass def  forward ( self , x): # should return a tuple pass ", "page_idx": 78, "bbox": [72, 467.03741455078125, 333.8375549316406, 643.8550415039062], "page_size": [612.0, 792.0]}
{"layout": 697, "type": "text", "text": "2. Import the module ", "page_idx": 79, "bbox": [72, 70.75508880615234, 171.15773010253906, 85.02153015136719], "page_size": [612.0, 792.0]}
{"layout": 698, "type": "text", "text": "You can either add the following line to  mmdet/models/backbones/__init__.py ", "page_idx": 79, "bbox": [72, 95.81745910644531, 409.6337585449219, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 699, "type": "table", "page_idx": 79, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_74.jpg", "bbox": [68, 112, 545, 205], "page_size": [612.0, 792.0], "ocr_text": "from .mobilenet import MobileNet\n\nor alternatively add\n\ncustom_imports = dict(\nimports=['mmdet.models.backbones.mobilenet'],\nallow_failed_imports=False)\n\n", "vlm_text": "The image depicts a code snippet, not a traditional table. It consists of code for importing the `MobileNet` module and an alternative method to handle custom imports using the `mmdet` library. Here's a breakdown of the code:\n\n1. The first line imports the `MobileNet` module from a local module named `mobilenet`.\n   ```python\n   from .mobilenet import MobileNet\n   ```\n\n2. The code provides an alternative method to import by using a dictionary `custom_imports` which specifies:\n   - `imports`: This is a list containing the import path `'mmdet.models.backbones.mobilenet'`.\n   - `allow_failed_imports`: This is a boolean set to `False`.\n   ```python\n   custom_imports = dict(\n       imports=['mmdet.models.backbones.mobilenet'],\n       allow_failed_imports=False)\n   ```\n\nOverall, the code relates to importing a `MobileNet` model, either directly or by specifying a custom import path using the `mmdet` library."}
{"layout": 700, "type": "text", "text": "to the config file to avoid modifying the original code.\n\n ", "page_idx": 79, "bbox": [72, 210.9854278564453, 287, 224.2954559326172], "page_size": [612.0, 792.0]}
{"layout": 701, "type": "text", "text": "3. Use the backbone in your config file\n\n ", "page_idx": 79, "bbox": [72, 242.1580352783203, 254.92323303222656, 256.42449951171875], "page_size": [612.0, 792.0]}
{"layout": 702, "type": "table", "page_idx": 79, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_75.jpg", "bbox": [68, 268, 542, 360], "page_size": [612.0, 792.0], "ocr_text": "model = dict(\n\nbackbone=dict(\ntype='MobileNet',\nargl=xxx,\narg2=xxx),\n\n", "vlm_text": "The image shows a snippet of a configuration in Python dictionary format, typically used for setting up machine learning models or similar tasks. This particular configuration appears to be defining a model with a backbone, using 'MobileNet' as the type. The specific arguments (`arg1` and `arg2`) are placeholders with values set to `xxx`. The ellipses (`...`) suggest that there are additional configurations or parameters in the actual code that are not shown in this snippet. This is not a table in a traditional sense with rows and columns of data; instead, it is a code snippet within a bordered box that may be presented alongside a table or in a document containing tables."}
{"layout": 703, "type": "text", "text": "11.1.2 Add new necks ", "text_level": 1, "page_idx": 79, "bbox": [72, 382, 199, 395], "page_size": [612.0, 792.0]}
{"layout": 704, "type": "text", "text": "1. Define a neck (e.g. PAFPN) ", "page_idx": 79, "bbox": [72, 407.8400573730469, 211.66566467285156, 422.10650634765625], "page_size": [612.0, 792.0]}
{"layout": 705, "type": "text", "text": "Create a new file  mmdet/models/necks/pafpn.py . ", "page_idx": 79, "bbox": [72, 432.9024353027344, 287, 446.21246337890625], "page_size": [612.0, 792.0]}
{"layout": 706, "type": "table", "page_idx": 79, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_76.jpg", "bbox": [68, 452, 544, 663], "page_size": [612.0, 792.0], "ocr_text": "from ..builder import NECKS\n\n@NECKS . register_module()\nclass PAFPN(nn.Module):\n\ndef __init__(self,\nin_channels,\nout_channels,\nnum_outs,\nstart_level=0,\nend_level=-1,\nadd_extra_convs=False):\n\npass\n\ndef forward(self, inputs):\n# implementation is ignored\npass\n\n", "vlm_text": "The image shows a snippet of Python code, not a table. Here's a brief explanation of what's in the code:\n\n- It imports `NECKS` from a module called `..builder`.\n- A decorator `@NECKS.register_module()` is used on the `PAFPN` class, indicating it is part of the `NECKS` module.\n- `PAFPN` is a class that inherits from `nn.Module`.\n- The `__init__` method initializes the class with parameters: `in_channels`, `out_channels`, `num_outs`, `start_level`, `end_level`, and `add_extra_convs`, with default values for the last three.\n- The `forward` method is defined but not implemented (indicated by \"pass\" and the comment \"# implementation is ignored\")."}
{"layout": 707, "type": "text", "text": "2. Import the module ", "text_level": 1, "page_idx": 80, "bbox": [71, 73, 172, 84], "page_size": [612.0, 792.0]}
{"layout": 708, "type": "text", "text": "You can either add the following line to  mmdet/models/necks/__init__.py , ", "page_idx": 80, "bbox": [72, 95.81745910644531, 391.20263671875, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 709, "type": "table", "page_idx": 80, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_77.jpg", "bbox": [67, 112, 545, 204], "page_size": [612.0, 792.0], "ocr_text": "from .pafpn import PAFPN\n\nor alternatively add\n\ncustom_imports = dict(\nimports=['mmdet.models.necks.pafpn.py'],\nallow_failed_imports=False)\n\n", "vlm_text": "The image shows a code snippet, not a traditional table with rows and columns. The code is related to Python imports, specifically for the `PAFPN` module. The snippet offers two methods for importing:\n\n1. Direct Import:\n   ```python\n   from .pafpn import PAFPN\n   ```\n   This line of code imports the `PAFPN` class or function from a module named `pafpn` located in the current package.\n\n2. Alternative Import via Custom Imports:\n   ```python\n   custom_imports = dict(\n       imports=['mmdet.models.necks.pafpn.py'],\n       allow_failed_imports=False\n   )\n   ```\n   This section defines a dictionary `custom_imports` that specifies a list for `imports` with one entry: `'mmdet.models.necks.pafpn.py'`. It also includes a setting, `allow_failed_imports`, which is set to `False`, meaning that failed imports are not allowed and would raise an error. The dictionary seems to configure a custom import mechanism, presumably in a larger context where such mechanisms are used."}
{"layout": 710, "type": "text", "text": "to the config file and avoid modifying the original code. ", "page_idx": 80, "bbox": [72, 210.5424346923828, 294.0364074707031, 223.8524627685547], "page_size": [612.0, 792.0]}
{"layout": 711, "type": "text", "text": "3. Modify the config file ", "text_level": 1, "page_idx": 80, "bbox": [71, 243, 184, 255], "page_size": [612.0, 792.0]}
{"layout": 712, "type": "table", "page_idx": 80, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_78.jpg", "bbox": [68, 268, 544, 334], "page_size": [612.0, 792.0], "ocr_text": "neck=dict(\ntype='PAFPN',\nin_channels=[256, 512, 1024, 2048],\nout_channels=256,\nnum_outs=5)\n", "vlm_text": "This is not a table, but rather a code snippet indicating the configuration of a neural network component (likely a feature pyramid network). The code is a dictionary defining parameters:\n\n- `type`: Set to `'PAFPN'`, which suggests it might be a specific type of Feature Pyramid Network.\n- `in_channels`: A list `[256, 512, 1024, 2048]` indicating input channels for each level of the pyramid.\n- `out_channels`: Set to `256`, specifying the number of output channels.\n- `num_outs`: Set to `5`, indicating the number of output feature maps."}
{"layout": 713, "type": "text", "text": "11.1.3 Add new heads ", "text_level": 1, "page_idx": 80, "bbox": [72, 357, 200, 370], "page_size": [612.0, 792.0]}
{"layout": 714, "type": "text", "text": "Here we show how to develop a new head with the example of  Double Head R-CNN  as the following. ", "page_idx": 80, "bbox": [72, 381.9874572753906, 476.5212707519531, 395.2974853515625], "page_size": [612.0, 792.0]}
{"layout": 715, "type": "text", "text": "First, add a new bbox head in  mmdet/models/roi_heads/bbox_heads/double b box head.py . Double Head R- CNN implements a new bbox head for object detection. To implement a bbox head, basically we need to implement three functions of the new module as the following. ", "page_idx": 80, "bbox": [72, 399.92047119140625, 540, 437.1404724121094], "page_size": [612.0, 792.0]}
{"layout": 716, "type": "table", "page_idx": 80, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_79.jpg", "bbox": [67, 440, 545, 714], "page_size": [612.0, 792.0], "ocr_text": "from mmdet.models.builder import HEADS\nfrom .bbox_head import BBoxHead\n\nGHEADS. register_module()\nclass DoubleConvFCBBoxHead (BBoxHead) :\nr\"\"\"Bbox head used in Double-Head R-CNN\n\n/-> cls\n/-> Shared convs ->\n\\-> reg\nroi features\n/-> cls\n\\-> shared fc ->\n\\-> reg\n\"\"\"\" # noqa: W605\n\ndef __init__(self,\nnum_convs=0,\nnum_fcs=0,\nconv_out_channels=1024,\nfc_out_channels=1024,\nconv_cfg=None,\n\n", "vlm_text": "The image you provided is not a table; it's a screenshot of a Python code snippet from a deep learning library, likely related to object detection, specifically a bounding box (bbox) head used in a model such as Double-Head R-CNN.\n\nHere is an explanation of the code:\n\n1. **Imports**:\n   - `from mmdet.models.builder import HEADS`: Importing the `HEADS` module from `mmdet.models.builder`.\n   - `from .bbox_head import BBoxHead`: Importing the `BBoxHead` class from a local module `bbox_head`.\n\n2. **Class Definition**:\n   - `@HEADS.register_module()`: A decorator indicating that the `DoubleConvFCBBoxHead` class should be registered within the `HEADS` module registry, a design pattern often used in machine learning libraries to keep track of various components.\n   - `class DoubleConvFCBBoxHead(BBoxHead)`: A new class `DoubleConvFCBBoxHead` which inherits from `BBoxHead`.\n\n3. **Docstring**:\n   - A raw string (indicated by `r\"\"\"...\"\"\"`) is used as a class docstring to describe the purpose and function of the class. It provides a high-level description that this class implements a bbox head used in a Double-Head R-CNN with schematic comments about its architecture.\n\n4. **Constructor (`__init__` method)**:\n   - The `__init__` method initializes the class with given parameters:\n     - `num_convs=0`: The number of convolutional layers.\n     - `num_fcs=0`: The number of fully connected layers.\n     - `conv_out_channels=1024`: The number of output channels for each convolutional layer.\n     - `fc_out_channels=1024`: The number of output channels for each fully connected layer.\n     - `conv_cfg=None`: Configuration for the convolutional layers, if any.\n\nThe code primarily sets up a structure for a bounding box head, which is typically part of an object detection model, allowing for classification (`cls`) and regression (`reg`) of regions of interest (roi features)."}
{"layout": 717, "type": "text", "text": "Second, implement a new RoI Head if it is necessary. We plan to inherit the new  Double Head RoI Head  from ", "page_idx": 81, "bbox": [71, 181.9894561767578, 540, 195.2994842529297], "page_size": [612.0, 792.0]}
{"layout": 718, "type": "text", "text": "Standard RoI Head  Standard RoI Head import  torch from  mmdet.core  import  b box 2 result, bbox2roi, build as signer, build sampler from  ..builder  import  HEADS, build_head, build roi extractor from  .base roi head  import  Base RoI Head from  .test mix in s  import  B Box Test Mix in, Mask Test Mix in @HEADS . register module() class  Standard RoI Head (Base RoI Head, B Box Test Mix in, Mask Test Mix in): \"\"\"Simplest base roi head including one bbox head and one mask head. \"\"\" def  in it as signer sampler ( self ): def  in it b box head ( self , b box roi extractor, bbox_head): def  in it mask head ( self , mask roi extractor, mask_head): def  forward dummy ( self , x, proposals): def  forward train ( self , x, img_metas, proposal list, gt_bboxes, gt_labels, gt b boxes ignore  $\\risingdotseq$  None , gt_masks  $=$  None ): def  b box forward ( self , x, rois): def  b box forward train ( self , x, sampling results, gt_bboxes, gt_labels, img_metas): def  mask forward train ( self , x, sampling results, bbox_feats, gt_masks, img_metas): def  mask forward ( self , x, rois  $\\risingdotseq$  None , pos_inds  $\\risingdotseq$  None , bbox_feats = None ): ", "page_idx": 81, "bbox": [71, 196.26576232910156, 469, 705.5059204101562], "page_size": [612.0, 792.0]}
{"layout": 719, "type": "text", "text": "(continues on next page) ", "page_idx": 81, "bbox": [461.9010009765625, 707.3375244140625, 540, 717.985595703125], "page_size": [612.0, 792.0]}
{"layout": 720, "type": "image", "page_idx": 82, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_80.jpg", "bbox": [69, 83, 544, 198], "page_size": [612.0, 792.0], "ocr_text": "def simple_test(self,\nx,\nproposal_list,\nimg_metas,\nproposals=None,\nrescale=False):\n\"\"\"Test without augmentation.\n\nmoe\n\n", "vlm_text": "This image shows a snippet of Python code, defining a function called `simple_test`. The function takes several parameters: `self`, `x`, `proposal_list`, `img_metas`, `proposals` (with a default value of `None`), and `rescale` (with a default value of `False`). There's also a docstring that states: \"Test without augmentation.\""}
{"layout": 721, "type": "text", "text": "Double Head’s modification is mainly in the b box forward logic, and it inherits other logics from the Standard RoI Head . In the  mmdet/models/roi_heads/double roi head.py , we implement the new RoI Head as the following: ", "page_idx": 82, "bbox": [72, 205.89942932128906, 540, 243.12046813964844], "page_size": [612.0, 792.0]}
{"layout": 722, "type": "image", "page_idx": 82, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_81.jpg", "bbox": [69, 247, 546, 641], "page_size": [612.0, 792.0], "ocr_text": "from ..builder import HEADS\nfrom .standard_roi_head import StandardRoIHead\n\nGHEADS . register_module()\nclass DoubleHeadRoIHead(StandardRoIHead):\n\"\"\"RoI head for Double Head RCNN\n\nhttps: //arxiv.org/abs/1904. 06493\n\nwon\n\ndef __init__(self, reg_roi_scale_factor, **kwargs):\nsuper (DoubleHeadRolIHead, self).__init__(**kwargs)\nself.reg_roi_scale_factor = reg_roi_scale_factor\n\ndef _bbox_forward(self, x, rois):\nbbox_cls_feats = self.bbox_roi_extractor(\nx[:self.bbox_roi_extractor.num_inputs], rois)\nbbox_reg_feats = self.bbox_roi_extractor(\nx[:self.bbox_roi_extractor.num_inputs],\nrois,\nroi_scale_factor=self.reg_roi_scale_factor)\nif self.with_shared_head:\nbbox_cls_feats = self.shared_head(bbox_cls_feats)\nbbox_reg_feats = self.shared_head(bbox_reg_feats)\ncls_score, bbox_pred = self.bbox_head(bbox_cls_feats, bbox_reg_feats)\n\nbbox_results = dict(\ncls_score=cls_score,\nbbox_pred=bbox_pred,\nbbox_feats=bbox_cls_feats)\nreturn bbox_results\n\n", "vlm_text": "The image shows a code snippet in Python for a class called `DoubleHeadRoIHead`, which inherits from `StandardRoIHead`. This class is part of an object detection framework and is registered as a module using `@HEADS.register_module`. The code appears to be designed for use with a Double Head RCNN, a specific type of Region-based Convolutional Neural Network (RCNN).\n\nHere's a summary of the key parts:\n\n- **Imports**: It imports `HEADS` from `.builder` and `StandardRoIHead` from `.standard_roi_head`.\n- **Class Definition**: The class `DoubleHeadRoIHead` extends `StandardRoIHead`.\n- **Initialization**: The `__init__` method initializes the class with `reg_roi_scale_factor` and any additional keyword arguments.\n- **Method `bbox_forward`**: This method takes `self`, `x`, and `rois` as arguments and performs operations using the `bbox_roi_extractor` and `shared_head`. It returns a dictionary with classification scores, bounding box predictions, and extracted features.\n\nThere's also a comment linking to a relevant paper on arXiv."}
{"layout": 723, "type": "text", "text": "Alternatively, the users can add ", "page_idx": 82, "bbox": [72, 675.9374389648438, 196.84132385253906, 689.2474975585938], "page_size": [612.0, 792.0]}
{"layout": 724, "type": "text", "text": "custom imports  $=$  dict ( imports  $,=$  [ ' mmdet.models.roi_heads.double roi head ' ,  ' mmdet.models.bbox_heads.double_ bbox_head ' ]) ˓ → ", "page_idx": 83, "bbox": [71, 79.55177307128906, 527.0426025390625, 116.35340881347656], "page_size": [612.0, 792.0]}
{"layout": 725, "type": "text", "text": "to the config file and achieve the same goal. ", "page_idx": 83, "bbox": [71, 126.04743957519531, 245.84735107421875, 139.3574676513672], "page_size": [612.0, 792.0]}
{"layout": 726, "type": "text", "text": "The config file of Double Head R-CNN is as the following ", "page_idx": 83, "bbox": [71, 143.9804229736328, 304.2978820800781, 157.2904510498047], "page_size": [612.0, 792.0]}
{"layout": 727, "type": "table", "page_idx": 83, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_82.jpg", "bbox": [69, 160, 544, 446], "page_size": [612.0, 792.0], "ocr_text": "_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\nroi_head=dict(\ntype='DoubleHeadRolIHead',\nreg_roi_scale_factor=1.3,\nbbox_head=dict(\n_delete_=True,\ntype='DoubleConvFCBBoxHead' ,\nnum_convs=4,\nnum_fcs=2,\nin_channels=256,\nconv_out_channels=1024,\nfc_out_channels=1024,\nroi_feat_size=7,\nnum_classes=80,\nbbox_coder=dict(\ntype='DeltaXYWHBBoxCoder',\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.1, 0.1, 0.2, 0.2]),\nreg_class_agnostic=False,\nloss_cls=dict(\ntype='CrossEntropyLoss', use_sigmoid=False, loss_weight=2.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0, loss_weight=2.0))))\n\n", "vlm_text": "The image contains a configuration script for a machine learning model, likely using the MMDetection framework. Here’s a breakdown of its contents:\n\n- **Base Configuration**: The model builds upon a base configuration file located at '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'.\n\n- **Model Configuration**:\n  - **roi_head**:\n    - **type**: 'DoubleHeadRoHead'\n    - **reg_roi_scale_factor**: 1.3\n    - **bbox_head**:\n      - A dictionary containing various settings and parameters:\n        - **_delete_**: True\n        - **type**: 'DoubleConvFCBBoxHead'\n        - **num_convs**: 4\n        - **num_fcs**: 2\n        - **in_channels**: 256\n        - **conv_out_channels**: 1024\n        - **fc_out_channels**: 1024\n        - **roi_feat_size**: 7\n        - **num_classes**: 80\n        - **bbox_coder**:\n          - **type**: 'DeltaXYWHBBoxCoder'\n          - **target_means**: [0., 0., 0., 0.]\n          - **target_stds**: [0.1, 0.1, 0.2, 0.2]\n    - **reg_class_agnostic**: False\n\n- **Loss Function Configuration**:\n  - **loss_cls**:\n    - **type**: 'CrossEntropyLoss'\n    - **use_sigmoid**: False\n    - **loss_weight**: 2.0\n  - **loss_bbox**:\n    - **type**: 'SmoothL1Loss'\n    - **beta**: 1.0\n    - **loss_weight**: 2.0\n\nThis configuration file is likely meant for customizing a Faster R-CNN model."}
{"layout": 728, "type": "text", "text": "Since MM Detection 2.0, the config system supports to inherit configs such that the users can focus on the modification. The Double Head R-CNN mainly uses a new Double Head RoI Head and a new  Double Con v FCB Box Head , the arguments are set according to the  __init__  function of each module. ", "page_idx": 83, "bbox": [71, 451.8254699707031, 540, 489.0464782714844], "page_size": [612.0, 792.0]}
{"layout": 729, "type": "text", "text": "11.1.4 Add new loss ", "text_level": 1, "page_idx": 83, "bbox": [71, 509, 190, 521], "page_size": [612.0, 792.0]}
{"layout": 730, "type": "text", "text": "Assume you want to add a new loss as  MyLoss , for bounding box regression. To add a new loss function, the users need implement it in  mmdet/models/losses/my_loss.py . The decorator  weighted loss  enable the loss to be weighted for each element. ", "page_idx": 83, "bbox": [71, 533.2774047851562, 540, 570.4984741210938], "page_size": [612.0, 792.0]}
{"layout": 731, "type": "table", "page_idx": 83, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_83.jpg", "bbox": [68, 573, 542, 714], "page_size": [612.0, 792.0], "ocr_text": "import torch\nimport torch.nn as nn\n\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n@weighted_loss\n\ndef my_loss(pred, target):\nassert pred.size() == target.size() and target.numel() > 0\nloss = torch.abs(pred - target)\nreturn loss\n\n", "vlm_text": "The table contains a Python code snippet for defining a custom loss function using PyTorch. Here’s a summary of the code:\n\n1. **Imports:**\n   - `import torch`\n   - `import torch.nn as nn`\n   - `from ..builder import LOSSES`\n   - `from .utils import weighted_loss`\n   \n2. **Function Definition:**\n   - The function `my_loss` is defined, which takes `pred` (prediction) and `target` (ground truth) as inputs.\n   - It uses the `@weighted_loss` decorator.\n   - An assertion checks that `pred` and `target` have the same size and that `target` is not empty.\n   - The loss is calculated as the absolute difference between `pred` and `target` using `torch.abs`.\n   - The calculated loss is returned. \n\nThis code snippet is likely part of a larger framework for handling losses in a machine learning context using PyTorch."}
{"layout": 732, "type": "image", "page_idx": 84, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_84.jpg", "bbox": [69, 84, 545, 346], "page_size": [612.0, 792.0], "ocr_text": "@LOSSES .\n\nregister_module()\n\nclass MyLoss(nn.Module):\n\ndef\n\ndef\n\n__init__(self, reduction='mean', loss_weight=1.0):\nsuper(MyLoss, self).__init__Q\n\nself.reduction = reduction\n\nself.loss_weight = loss_weight\n\nforward(self,\npred,\ntarget,\nweight=None,\navg_factor=None,\nreduction_override=None) :\nassert reduction_override in (None, 'none', 'mean', 'sum')\nreduction = (\nreduction_override if reduction_override else self.reduction)\nloss_bbox = self.loss_weight * my_loss(\npred, target, weight, reduction=reduction, avg_factor=avg_factor)\nreturn loss_bbox\n\n", "vlm_text": "This image depicts a Python class definition for a custom loss function, `MyLoss`, which is a subclass of `nn.Module`. The class includes:\n\n- An `__init__` method that initializes the loss with parameters `reduction` (default is 'mean') and `loss_weight` (default is 1.0). These parameters are stored in the instance variables `self.reduction` and `self.loss_weight`.\n\n- A `forward` method that takes several arguments: `pred`, `target`, `weight`, `avg_factor`, and `reduction_override`. It asserts that `reduction_override` is one of the specified options, determines the reduction method, calculates the loss weighted by `self.loss_weight`, and returns it.\n\nThe class is registered with a decorator `@LOSSES.register_module()`, suggesting it might be part of a larger framework or library for handling different types of losses."}
{"layout": 733, "type": "text", "text": "Then the users need to add it in the  mmdet/models/losses/__init__.py . ", "page_idx": 84, "bbox": [71, 349.3614807128906, 378.65966796875, 362.6715087890625], "page_size": [612.0, 792.0]}
{"layout": 734, "type": "text", "text": "from  .my_loss  import  MyLoss, my_loss ", "page_idx": 84, "bbox": [71, 371.8079833984375, 260.50244140625, 382.717041015625], "page_size": [612.0, 792.0]}
{"layout": 735, "type": "text", "text": "Alternatively, you can add ", "page_idx": 84, "bbox": [71, 394.990478515625, 176.24859619140625, 408.3005065917969], "page_size": [612.0, 792.0]}
{"layout": 736, "type": "text", "text": "to the config file and achieve the same goal. ", "page_idx": 84, "bbox": [71, 452.574462890625, 245.84735107421875, 465.8844909667969], "page_size": [612.0, 792.0]}
{"layout": 737, "type": "text", "text": "To use it, modify the  loss_xxx  field. Since MyLoss is for regression, you need to modify the  loss_bbox  field in the head. ", "page_idx": 84, "bbox": [71, 470.5074768066406, 539.9971313476562, 495.7724914550781], "page_size": [612.0, 792.0]}
{"layout": 738, "type": "text", "text": "loss_bbox  $\\because$  dict ( type = ' MyLoss ' , loss weight = 1.0 )) ", "page_idx": 84, "bbox": [71, 505.50677490234375, 317.8270568847656, 515.5458374023438], "page_size": [612.0, 792.0]}
{"layout": 739, "type": "text", "text": "TUTORIAL 5: CUSTOMIZE RUNTIME SETTINGS ", "text_level": 1, "page_idx": 86, "bbox": [216, 163, 542, 182], "page_size": [612.0, 792.0]}
{"layout": 740, "type": "text", "text": "12.1 Customize optimization settings\n\n ", "page_idx": 86, "bbox": [72, 226.2214813232422, 329, 246.76524353027344], "page_size": [612.0, 792.0]}
{"layout": 741, "type": "text", "text": "12.1.1 Customize optimizer supported by Pytorch ", "page_idx": 86, "bbox": [72, 260.89239501953125, 354.6448974609375, 278.0122375488281], "page_size": [612.0, 792.0]}
{"layout": 742, "type": "text", "text": "We already support to use all the optimizers implemented by PyTorch, and the only modification is to change the optimizer  field of config files. For example, if you want to use  ADAM  (note that the performance could drop a lot), the modification could be as the following. ", "page_idx": 86, "bbox": [72, 288.1964416503906, 540, 325.41644287109375], "page_size": [612.0, 792.0]}
{"layout": 743, "type": "text", "text": "optimizer  =  dict ( type = ' Adam ' , lr = 0.0003 , weight decay = 0.0001 ) ", "page_idx": 86, "bbox": [72, 335.1507263183594, 391.0523376464844, 345.18975830078125], "page_size": [612.0, 792.0]}
{"layout": 744, "type": "text", "text": "To modify the learning rate of the model, the users only need to modify the  lr  in the config of optimizer. The users can directly set arguments following the  API doc  of PyTorch.\n\n ", "page_idx": 86, "bbox": [72, 357.7354736328125, 540, 383.00048828125], "page_size": [612.0, 792.0]}
{"layout": 745, "type": "text", "text": "12.1.2 Customize self-implemented optimizer\n\n ", "page_idx": 86, "bbox": [72, 400.6143798828125, 329, 417.7342529296875], "page_size": [612.0, 792.0]}
{"layout": 746, "type": "text", "text": "1. Define a new optimizer ", "page_idx": 86, "bbox": [72, 428.8150939941406, 191.86001586914062, 443.08154296875], "page_size": [612.0, 792.0]}
{"layout": 747, "type": "text", "text": "A customized optimizer could be defined as following. ", "page_idx": 86, "bbox": [72, 453.87847900390625, 289.7026672363281, 467.1885070800781], "page_size": [612.0, 792.0]}
{"layout": 748, "type": "text", "text": "Assume you want to add a optimizer named  My Optimizer , which has arguments  a ,  b , and  c . You need to create a new directory named  mmdet/core/optimizer . And then implement the new optimizer in a file, e.g., in  mmdet/core/ optimizer/my optimizer.py : ", "page_idx": 86, "bbox": [72, 471.8114929199219, 540, 509.031494140625], "page_size": [612.0, 792.0]}
{"layout": 749, "type": "text", "text": "def  __init__ ( self , a, b, c) ", "page_idx": 86, "bbox": [92.9209976196289, 601.85302734375, 234.4603271484375, 612.7620239257812], "page_size": [612.0, 792.0]}
{"layout": 750, "type": "text", "text": "2. Add the optimizer to registry ", "text_level": 1, "page_idx": 87, "bbox": [71, 72, 221, 85], "page_size": [612.0, 792.0]}
{"layout": 751, "type": "text", "text": "To find the above module defined above, this module should be imported into the main namespace at first. There are two options to achieve it. ", "page_idx": 87, "bbox": [71, 95.81745910644531, 540, 121.08251190185547], "page_size": [612.0, 792.0]}
{"layout": 752, "type": "text", "text": "• Modify  mmdet/core/optimizer/__init__.py  to import it. The newly defined module should be imported in  mmdet/core/optimizer/__init__.py  so that the registry will find the new module and add it: ", "page_idx": 87, "bbox": [88, 125.70545959472656, 540, 168.9034881591797], "page_size": [612.0, 792.0]}
{"layout": 753, "type": "text", "text": "from  .my optimizer  import  My Optimizer ", "page_idx": 87, "bbox": [71, 178.03897094726562, 265.7330017089844, 188.94802856445312], "page_size": [612.0, 792.0]}
{"layout": 754, "type": "text", "text": "• Use  custom imports  in the config to manually import it ", "page_idx": 87, "bbox": [88, 201.22242736816406, 327.40167236328125, 214.53245544433594], "page_size": [612.0, 792.0]}
{"layout": 755, "type": "text", "text": "custom imports  $=$   dict (imports  $,=$  [ ' mmdet.core.optimizer.my optimizer ' ], allow failed  $\\hookrightarrow$  imports  $=$  False ) → ", "page_idx": 87, "bbox": [71, 224.26576232910156, 500.8900451660156, 249.1123809814453], "page_size": [612.0, 792.0]}
{"layout": 756, "type": "text", "text": "The module  mmdet.core.optimizer.my optimizer  will be imported at the beginning of the program and the class My Optimizer  is then automatically registered. Note that only the package containing the class  My Optimizer  should be imported.  mmdet.core.optimizer.my optimizer.My Optimizer  cannot  be imported directly. ", "page_idx": 87, "bbox": [71, 258.8064880371094, 540, 296.6242370605469], "page_size": [612.0, 792.0]}
{"layout": 757, "type": "text", "text": "Actually users can use a totally different file directory structure using this importing method, as long as the module root can be located in  PYTHONPATH . ", "page_idx": 87, "bbox": [71, 300.64947509765625, 540, 325.91448974609375], "page_size": [612.0, 792.0]}
{"layout": 758, "type": "text", "text": "3. Specify the optimizer in the config file ", "text_level": 1, "page_idx": 87, "bbox": [70, 345, 265, 357], "page_size": [612.0, 792.0]}
{"layout": 759, "type": "text", "text": "Then you can use  My Optimizer  in  optimizer  field of config files. In the configs, the optimizers are defined by the field  optimizer  like the following: ", "page_idx": 87, "bbox": [71, 368.8404541015625, 540, 394.10546875], "page_size": [612.0, 792.0]}
{"layout": 760, "type": "table", "page_idx": 87, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_85.jpg", "bbox": [68, 398, 544, 464], "page_size": [612.0, 792.0], "ocr_text": "optimizer\n\n= dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n\nTo use your own optimizer, the field can be changed to\n\noptimizer\n\n= dict(type='MyOptimizer', a=a_value, b=b_value, c=c_value)\n\n", "vlm_text": "The table contains two code snippets for configuring an optimizer in a dictionary format:\n\n1. The first snippet sets up a Stochastic Gradient Descent (SGD) optimizer with the following parameters:\n   - `type`: `'SGD'`\n   - `lr`: `0.02` (learning rate)\n   - `momentum`: `0.9`\n   - `weight_decay`: `0.0001`\n\n2. The second snippet provides a template for using a custom optimizer by replacing the type and parameters:\n   - `type`: `'MyOptimizer'`\n   - Custom parameters `a`, `b`, `c` with placeholders `a_value`, `b_value`, `c_value`."}
{"layout": 761, "type": "text", "text": "12.1.3 Customize optimizer constructor ", "text_level": 1, "page_idx": 87, "bbox": [71, 487, 298, 501], "page_size": [612.0, 792.0]}
{"layout": 762, "type": "text", "text": "Some models may have some parameter-specific settings for optimization, e.g. weight decay for BatchNorm layers. The users can do those fine-grained parameter tuning through customizing optimizer constructor. ", "page_idx": 87, "bbox": [71, 512.3484497070312, 540, 537.6134643554688], "page_size": [612.0, 792.0]}
{"layout": 763, "type": "image", "page_idx": 87, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_86.jpg", "bbox": [68, 542, 543, 709], "page_size": [612.0, 792.0], "ocr_text": "from mncv.utils import build_from_cfg\nfrom mmcv.runner.optimizer import OPTIMIZER_BUILDERS, OPTIMIZERS\n\nfrom mmdet.utils import get_root_logger\nfrom .my_optimizer import MyOptimizer\n\n@OPTIMIZER_BUILDERS . register_module()\nclass MyOptimizerConstructor (object):\ndef __init__(self, optimizer_cfg, paramwise_cfg=None) :\n\ndef __call__(self, model):\n\n——_——\n\nUS eT SS rTSreT ars a\n", "vlm_text": "The image shows a snippet of Python code related to optimizer configuration in a machine learning context, possibly using the MMDetection framework. It includes:\n\n1. Imports from various modules such as `mmcv` and `mmdet`, and a custom module `.my_optimizer`.\n2. A decorator `@OPTIMIZER_BUILDERS.register_module()` is used to register a class.\n3. Definition of a class `MyOptimizerConstructor` with an `__init__` method, which takes `optimizer_cfg` and an optional `paramwise_cfg`.\n4. A `__call__` method that takes a `model` as a parameter. \n\nThe code appears to be setting up a custom optimizer."}
{"layout": 764, "type": "table", "page_idx": 88, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_87.jpg", "bbox": [68, 79, 544, 114], "page_size": [612.0, 792.0], "ocr_text": "eee RE ee eee ee\n\nreturn my_optimizer\n", "vlm_text": "The table contains a single line of code: `return my_optimizer`. This looks like it could be part of a function in a programming language such as Python, where the function is likely designed to return an object or value named `my_optimizer`. The exact context or purpose of this code is not provided in the image."}
{"layout": 765, "type": "text", "text": "The default optimizer constructor is implemented  here , which could also serve as a template for new optimizer con- structor. ", "page_idx": 88, "bbox": [72, 122.21345520019531, 540, 147.47850036621094], "page_size": [612.0, 792.0]}
{"layout": 766, "type": "text", "text": "12.1.4 Additional settings ", "text_level": 1, "page_idx": 88, "bbox": [71, 167, 220, 181], "page_size": [612.0, 792.0]}
{"layout": 767, "type": "text", "text": "Tricks not implemented by the optimizer should be implemented through optimizer constructor (e.g., set parameter- wise learning rates) or hooks. We list some common settings that could stabilize the training or accelerate the training. Feel free to create PR, issue for more settings. ", "page_idx": 88, "bbox": [72, 192.3964385986328, 540, 229.6174774169922], "page_size": [612.0, 792.0]}
{"layout": 768, "type": "text", "text": "•  Use gradient clip to stabilize training : Some models need gradient clip to clip the gradients to stabilize the training process. An example is as below: ", "page_idx": 88, "bbox": [88, 233.61279296875, 540, 259.5054931640625], "page_size": [612.0, 792.0]}
{"layout": 769, "type": "image", "page_idx": 88, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_88.jpg", "bbox": [94, 265, 543, 295], "page_size": [612.0, 792.0], "ocr_text": "optimizer_config = dict(\n_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n", "vlm_text": "The image shows a snippet of Python code that defines a dictionary named `optimizer_config`. The dictionary contains:\n\n- A key `_delete_` set to `True`.\n- A nested dictionary under the key `grad_clip`, which includes:\n  - `max_norm` set to `35`\n  - `norm_type` set to `2`"}
{"layout": 770, "type": "text", "text": "If your config inherits the base config which already sets the  optimizer config , you might need _delete_  $\\bar{\\cdot}$  True  to override the unnecessary settings. See the  config documentation  for more details. ", "page_idx": 88, "bbox": [96, 303.77947998046875, 540, 329.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 771, "type": "text", "text": "•  Use momentum schedule to accelerate model convergence : We support momentum scheduler to modify model’s momentum according to learning rate, which could make the model converge in a faster way. Mo- mentum scheduler is usually used with LR scheduler, for example, the following config is used in 3D detection to accelerate convergence. For more details, please refer to the implementation of  Cyclic Lr Updater  and  Cyclic- Momentum Updater . ", "page_idx": 88, "bbox": [88, 333.0398254394531, 540, 394.7974548339844], "page_size": [612.0, 792.0]}
{"layout": 772, "type": "table", "page_idx": 88, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_89.jpg", "bbox": [92, 400, 543, 550], "page_size": [612.0, 792.0], "ocr_text": "lr_config = dict(\npolicy='cyclic',\ntarget_ratio=(10, le-4),\ncyclic_times=1,\nstep_ratio_up=0.4,\n\n)\n\nmomentum_config = dict(\npolicy='cyclic',\ntarget_ratio=(0.85 / 0.95, 1),\ncyclic_times=1,\nstep_ratio_up=0.4,\n", "vlm_text": "This is not a table, but a snippet of Python code. It defines two dictionaries: `lr_config` and `momentum_config`. Both dictionaries use a 'cyclic' policy and contain several parameters:\n\n- `lr_config` settings:\n  - `policy='cyclic'`\n  - `target_ratio=(10, 1e-4)`\n  - `cyclic_times=1`\n  - `step_ratio_up=0.4`\n\n- `momentum_config` settings:\n  - `policy='cyclic'`\n  - `target_ratio=(0.85 / 0.95, 1)`\n  - `cyclic_times=1`\n  - `step_ratio_up=0.4`\n\nThese configurations might be used for setting learning rate and momentum schedules, often within machine learning or optimization contexts."}
{"layout": 773, "type": "text", "text": "12.2 Customize training schedules ", "text_level": 1, "page_idx": 88, "bbox": [71, 576, 312, 593], "page_size": [612.0, 792.0]}
{"layout": 774, "type": "text", "text": "By default we use step learning rate with 1x schedule, this calls  StepLRHook  in MMCV. We support many other learning rate schedule  here , such as  Cosine Annealing  and  Poly  schedule. Here are some examples ", "page_idx": 88, "bbox": [72, 608.5624389648438, 540, 633.8275146484375], "page_size": [612.0, 792.0]}
{"layout": 775, "type": "text", "text": "• Poly schedule: ", "page_idx": 88, "bbox": [88, 638.450439453125, 154.65020751953125, 651.760498046875], "page_size": [612.0, 792.0]}
{"layout": 776, "type": "image", "page_idx": 88, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_90.jpg", "bbox": [93, 657, 543, 677], "page_size": [612.0, 792.0], "ocr_text": "lr_config = dict(policy='poly', power=0.9, min_lr=le-4, by_epoch=False)\n\n", "vlm_text": "The image shows a snippet of Python code, specifically configuring a learning rate schedule. The code is:\n\n```python\nlr_config = dict(policy='poly', power=0.9, min_lr=1e-4, by_epoch=False)\n```\n\nThis dictionary sets the learning rate policy to 'poly', with a power of 0.9, a minimum learning rate of 0.0001, and specifies that adjustments are not made by epoch."}
{"layout": 777, "type": "text", "text": "• Con sine Annealing schedule: ", "page_idx": 88, "bbox": [88, 684.0794067382812, 211.03854370117188, 697.3894653320312], "page_size": [612.0, 792.0]}
{"layout": 778, "type": "table", "page_idx": 89, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_91.jpg", "bbox": [92, 74, 544, 155], "page_size": [612.0, 792.0], "ocr_text": "lr_config = dict(\npolicy='CosineAnnealing',\nwarmup='linear',\nwarmup_iters=1000,\nwarmup_ratio=1.0 / 10,\nmin_lr_ratio=le-5)\n\n", "vlm_text": "The table contains a Python dictionary configuration for learning rate scheduling in a machine learning context. Here are the key elements:\n\n- `policy`: 'CosineAnnealing' (type of learning rate schedule)\n- `warmup`: 'linear' (method of warming up the learning rate)\n- `warmup_iters`: 1000 (number of iterations for warming up)\n- `warmup_ratio`: 0.1 (starting ratio of the learning rate during warmup)\n- `min_lr_ratio`: 1e-5 (minimum learning rate ratio)"}
{"layout": 779, "type": "text", "text": "12.3 Customize workflow ", "text_level": 1, "page_idx": 89, "bbox": [72, 180, 246, 196], "page_size": [612.0, 792.0]}
{"layout": 780, "type": "text", "text": "Workflow is a list of (phase, epochs) to specify the running order and epochs. By default it is set to be ", "page_idx": 89, "bbox": [71, 211.71949768066406, 477.4279479980469, 225.02952575683594], "page_size": [612.0, 792.0]}
{"layout": 781, "type": "text", "text": "workflow  =  [( ' train ' ,  1 )] ", "page_idx": 89, "bbox": [71, 234.60975646972656, 202.7587890625, 244.64877319335938], "page_size": [612.0, 792.0]}
{"layout": 782, "type": "text", "text": "which means running 1 epoch for training. Sometimes user may want to check some metrics (e.g. loss, accuracy) about the model on the validate set. In such case, we can set the workflow as ", "page_idx": 89, "bbox": [71, 257.1944885253906, 540, 282.46051025390625], "page_size": [612.0, 792.0]}
{"layout": 783, "type": "text", "text": "[( ' train ' ,  1 ), ( ' val ' ,  1 )] ", "page_idx": 89, "bbox": [71, 292.03973388671875, 207.98980712890625, 302.0787658691406], "page_size": [612.0, 792.0]}
{"layout": 784, "type": "text", "text": "so that 1 epoch for training and 1 epoch for validation will be run iterative ly. ", "page_idx": 89, "bbox": [71, 314.6254577636719, 375.2613830566406, 327.93548583984375], "page_size": [612.0, 792.0]}
{"layout": 785, "type": "text", "text": "Note : ", "text_level": 1, "page_idx": 89, "bbox": [71, 334, 94, 345], "page_size": [612.0, 792.0]}
{"layout": 786, "type": "text", "text": "1. The parameters of model will not be updated during val epoch. 2. Keyword  total epochs  in the config only controls the number of training epochs and will not affect the vali- dation workflow. ", "page_idx": 89, "bbox": [84, 350.490478515625, 540, 393.6885070800781], "page_size": [612.0, 792.0]}
{"layout": 787, "type": "text", "text": "3. Workflows  [( ' train ' , 1), ( ' val ' , 1)]  and  [( ' train ' , 1)]  will not change the behavior of  EvalHook because  EvalHook  is called by  after train epoch  and validation workflow only affect hooks that are called through  after val epoch . Therefore, the only difference between  [( ' train ' , 1), ( ' val ' , 1)]  and [( ' train ' , 1)]  is that the runner will calculate losses on validation set after each training epoch. ", "page_idx": 89, "bbox": [84, 398.3114929199219, 540, 447.48748779296875], "page_size": [612.0, 792.0]}
{"layout": 788, "type": "text", "text": "12.4 Customize hooks ", "text_level": 1, "page_idx": 89, "bbox": [72, 470, 227, 486], "page_size": [612.0, 792.0]}
{"layout": 789, "type": "text", "text": "12.4.1 Customize self-implemented hooks ", "text_level": 1, "page_idx": 89, "bbox": [72, 504, 312, 517], "page_size": [612.0, 792.0]}
{"layout": 790, "type": "text", "text": "1. Implement a new hook ", "page_idx": 89, "bbox": [71, 530.20703125, 190.85372924804688, 544.4735107421875], "page_size": [612.0, 792.0]}
{"layout": 791, "type": "text", "text": "There are some occasions when the users might need to implement a new hook. MM Detection supports customized hooks in training (#3395) since v2.3.0. Thus the users could implement a hook directly in mmdet or their mmdet-based codebases and use the hook by only modifying the config in training. Before v2.3.0, the users need to modify the code to get the hook registered before training starts. Here we give an example of creating a new hook in mmdet and using it in training. ", "page_idx": 89, "bbox": [71, 555.2694091796875, 540, 616.4004516601562], "page_size": [612.0, 792.0]}
{"layout": 792, "type": "table", "page_idx": 89, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_92.jpg", "bbox": [67, 620, 542, 713], "page_size": [612.0, 792.0], "ocr_text": "from mmcv.runner import HOOKS, Hook\n\nGHOOKS . register_module()\nclass MyHook (Hook):\n\ndef __init__(self, a, b):\n\n", "vlm_text": "The image contains code written in Python, which is not a table. The code defines a custom hook class that can be used in a machine learning model training framework that uses the MMCV library. Here’s a breakdown of the code:\n\n1. **Imports**:\n   ```python\n   from mmcv.runner import HOOKS, Hook\n   ```\n   - This line imports `HOOKS` and `Hook` from the `mmcv.runner` module.\n\n2. **Decorator**:\n   ```python\n   @HOOKS.register_module()\n   ```\n   - This decorator is used to register the custom hook class with the MMCV framework, making it available for use.\n\n3. **Class Definition**:\n   ```python\n   class MyHook(Hook):\n   ```\n   - This line defines a new class `MyHook` that inherits from `Hook`.\n\n4. **Constructor Method**:\n   ```python\n   def __init__(self, a, b):\n   ```\n   - This is the initialization method for the `MyHook` class. It takes two parameters, `a` and `b`.\n\nThis setup is typically used to extend the functionality of a model training process, allowing the execution of custom code at specific points during the training."}
{"layout": 793, "type": "image", "page_idx": 90, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_93.jpg", "bbox": [70, 82, 543, 318], "page_size": [612.0, 792.0], "ocr_text": "pass\n\ndef before_run(self, runner):\npass\n\ndef after_run(self, runner):\npass\n\ndef before_epoch(self, runner):\npass\n\ndef after_epoch(self, runner):\npass\n\ndef before_iter(self, runner):\npass\n\ndef after_iter(self, runner):\npass\n\n", "vlm_text": "The image displays a snippet of Python code which consists of several method definitions within a class. These methods appear to be placeholders because each method body contains only the `pass` statement, which indicates that they are yet to be implemented. The methods are:\n\n1. `before_run(self, runner)`\n2. `after_run(self, runner)`\n3. `before_epoch(self, runner)`\n4. `after_epoch(self, runner)`\n5. `before_iter(self, runner)`\n6. `after_iter(self, runner)`\n\nEach method takes a `runner` parameter, and the method names suggest that they are intended as hook methods to be executed at specific points in a process, likely related to machine learning or training iterations (before and after a run, epoch, or iteration)."}
{"layout": 794, "type": "text", "text": "Depending on the functionality of the hook, the users need to specify what the hook will do at each stage of the training in  before_run ,  after_run ,  before epoch ,  after epoch ,  before it er , and  after_iter . ", "page_idx": 90, "bbox": [72, 325.45147705078125, 540, 350.71649169921875], "page_size": [612.0, 792.0]}
{"layout": 795, "type": "text", "text": "2. Register the new hook ", "page_idx": 90, "bbox": [72, 368.5790710449219, 190.1962432861328, 382.84552001953125], "page_size": [612.0, 792.0]}
{"layout": 796, "type": "text", "text": "Then we need to make  MyHook  imported. Assuming the file is in  mmdet/core/utils/my_hook.py  there are two ways to do that: ", "page_idx": 90, "bbox": [72, 393.6424560546875, 540, 418.907470703125], "page_size": [612.0, 792.0]}
{"layout": 797, "type": "text", "text": "• Modify  mmdet/core/utils/__init__.py  to import it. The newly defined module should be imported in  mmdet/core/utils/__init__.py  so that the registry will find the new module and add it: ", "page_idx": 90, "bbox": [88.43800354003906, 423.53045654296875, 540, 466.7284851074219], "page_size": [612.0, 792.0]}
{"layout": 798, "type": "text", "text": "3. Modify the config ", "text_level": 1, "page_idx": 90, "bbox": [71, 559, 167, 571], "page_size": [612.0, 792.0]}
{"layout": 799, "type": "image", "page_idx": 90, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_94.jpg", "bbox": [69, 585, 542, 626], "page_size": [612.0, 792.0], "ocr_text": "custom_hooks = [\ndict(type='MyHook', a=a_value, b=b_value)\n]\n", "vlm_text": "The image shows a code snippet in Python. It creates a list called `custom_hooks` containing a single dictionary. The dictionary has three key-value pairs:\n\n- `type` with a value of `'MyHook'`\n- `a` with a value of `a_value`\n- `b` with a value of `b_value`"}
{"layout": 800, "type": "text", "text": "You can also set the priority of the hook by adding key  priority  to  ' NORMAL '  or  ' HIGHEST '  as below ", "page_idx": 90, "bbox": [72, 634.58544921875, 486.8533630371094, 647.8955078125], "page_size": [612.0, 792.0]}
{"layout": 801, "type": "image", "page_idx": 90, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_95.jpg", "bbox": [68, 653, 539, 695], "page_size": [612.0, 792.0], "ocr_text": "custom_hooks = [\ndict(type='MyHook', a=a_value, b=b_value, priority='NORMAL')\n", "vlm_text": "The image shows a snippet of Python code that defines a list called `custom_hooks`. The list contains a single dictionary with the following key-value pairs: \n\n- `type` set to `'MyHook'`\n- `a` set to `a_value`\n- `b` set to `b_value`\n- `priority` set to `'NORMAL'`"}
{"layout": 802, "type": "text", "text": "By default the hook’s priority is set as  NORMAL  during registration. ", "page_idx": 90, "bbox": [72, 704.1244506835938, 336.4079895019531, 717.4345092773438], "page_size": [612.0, 792.0]}
{"layout": 803, "type": "text", "text": "12.4.2 Use hooks implemented in MMCV ", "text_level": 1, "page_idx": 91, "bbox": [71, 70, 304, 85], "page_size": [612.0, 792.0]}
{"layout": 804, "type": "text", "text": "If the hook is already implemented in MMCV, you can directly modify the config to use the hook as below ", "page_idx": 91, "bbox": [71, 95.81745910644531, 496.64581298828125, 109.12749481201172], "page_size": [612.0, 792.0]}
{"layout": 805, "type": "text", "text": "4. Example:  Num Class Check Hook ", "text_level": 1, "page_idx": 91, "bbox": [71, 128, 222, 140], "page_size": [612.0, 792.0]}
{"layout": 806, "type": "text", "text": "We implement a customized hook named  Num Class Check Hook  to check whether the  num classes  in head matches the length of  CLASSSES  in  dataset . ", "page_idx": 91, "bbox": [71, 152.05348205566406, 540, 177.3185272216797], "page_size": [612.0, 792.0]}
{"layout": 807, "type": "text", "text": "We set it in  default runtime.py . ", "page_idx": 91, "bbox": [71, 181.9405059814453, 197.68809509277344, 195.2505340576172], "page_size": [612.0, 792.0]}
{"layout": 808, "type": "text", "text": "custom hooks  $=$   [ dict ( type  $\\circeq$  ' Num Class Check Hook ' )] ", "page_idx": 91, "bbox": [71, 204.98475646972656, 317.8277282714844, 215.02377319335938], "page_size": [612.0, 792.0]}
{"layout": 809, "type": "text", "text": "12.4.3 Modify default runtime hooks ", "text_level": 1, "page_idx": 91, "bbox": [72, 242, 279, 257], "page_size": [612.0, 792.0]}
{"layout": 810, "type": "text", "text": "There are some common hooks that are not registered through  custom hooks , they are ", "page_idx": 91, "bbox": [71, 267.8655090332031, 421.3800354003906, 281.175537109375], "page_size": [612.0, 792.0]}
{"layout": 811, "type": "text", "text": "• log_config • checkpoint config • evaluation • lr_config • optimizer config • momentum config ", "page_idx": 91, "bbox": [88, 285.7975158691406, 172, 388.7716064453125], "page_size": [612.0, 792.0]}
{"layout": 812, "type": "text", "text": "In those hooks, only the logger hook has the  VERY_LOW  priority, others’ priority are  NORMAL . The above-mentioned tutorials already covers how to modify  optimizer config ,  momentum config , and  lr_config . Here we reveals how what we can do with  log_config ,  checkpoint config , and  evaluation . ", "page_idx": 91, "bbox": [71, 393.39459228515625, 540, 430.6145935058594], "page_size": [612.0, 792.0]}
{"layout": 813, "type": "text", "text": "Checkpoint config ", "text_level": 1, "page_idx": 91, "bbox": [71, 450, 160, 462], "page_size": [612.0, 792.0]}
{"layout": 814, "type": "text", "text": "The MMCV runner will use  checkpoint config  to initialize  Checkpoint Hook . ", "page_idx": 91, "bbox": [71, 473.5405578613281, 401, 486.8505859375], "page_size": [612.0, 792.0]}
{"layout": 815, "type": "text", "text": "checkpoint config  =  dict (interval = 1 ) ", "page_idx": 91, "bbox": [71, 496.583740234375, 260, 506.5961608886719], "page_size": [612.0, 792.0]}
{"layout": 816, "type": "text", "text": "The users could set  max keep ck pts  to only save only small number of checkpoints or decide whether to store state dict of optimizer by  save optimizer . More details of the arguments are  here ", "page_idx": 91, "bbox": [71, 519.16943359375, 540, 544.4345092773438], "page_size": [612.0, 792.0]}
{"layout": 817, "type": "text", "text": "Log config ", "text_level": 1, "page_idx": 91, "bbox": [71, 565, 124, 576], "page_size": [612.0, 792.0]}
{"layout": 818, "type": "text", "text": "The  log_config  wraps multiple logger hooks and enables to set intervals. Now MMCV supports  Wand bLogger Hook , Ml flow Logger Hook , and  Tensor board Logger Hook . The detail usages can be found in the  doc . ", "page_idx": 91, "bbox": [71, 587.3594360351562, 540, 612.62548828125], "page_size": [612.0, 792.0]}
{"layout": 819, "type": "text", "text": "log_config  $=$   dict ( interval  $\\scriptstyle{\\varepsilon=50}$  , hooks  $=$  [ dict ( type  $=\"$  Text Logger Hook ' ), dict ( type  $=\"$  Tensor board Logger Hook ' ) ]) ", "page_idx": 91, "bbox": [71, 622.3587036132812, 291.67535400390625, 692.147216796875], "page_size": [612.0, 792.0]}
{"layout": 820, "type": "text", "text": "Evaluation config ", "text_level": 1, "page_idx": 92, "bbox": [71, 73, 156, 85], "page_size": [612.0, 792.0]}
{"layout": 821, "type": "text", "text": "The config of  evaluation  will be used to initialize the  EvalHook . Except the key  interval , other arguments such as  metric  will be passed to the  dataset.evaluate() ", "page_idx": 92, "bbox": [71, 95.81745910644531, 539.99853515625, 121.08251190185547], "page_size": [612.0, 792.0]}
{"layout": 822, "type": "text", "text": "TUTORIAL 6: CUSTOMIZE LOSSES ", "text_level": 1, "page_idx": 94, "bbox": [299, 163, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 823, "type": "text", "text": "MM Detection provides users with different loss functions. But the default configuration may be not applicable for different datasets or models, so users may want to modify a specific loss to adapt the new situation. ", "page_idx": 94, "bbox": [72, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 824, "type": "text", "text": "This tutorial first elaborate the computation pipeline of losses, then give some instructions about how to modify each step. The modification can be categorized as tweaking and weighting. ", "page_idx": 94, "bbox": [72, 255.7604522705078, 540, 281.0264892578125], "page_size": [612.0, 792.0]}
{"layout": 825, "type": "text", "text": "13.1 Computation pipeline of a loss ", "page_idx": 94, "bbox": [72, 301.00653076171875, 316.0863952636719, 321.55029296875], "page_size": [612.0, 792.0]}
{"layout": 826, "type": "text", "text": "Given the input prediction and target, as well as the weights, a loss function maps the input tensor to the final loss scalar. The mapping can be divided into four steps: ", "page_idx": 94, "bbox": [72, 335.58746337890625, 540, 360.85247802734375], "page_size": [612.0, 792.0]}
{"layout": 827, "type": "text", "text": "1. Set the sampling method to sample positive and negative samples. 2. Get  element-wise  or  sample-wise  loss by the loss kernel function. 3. Weighting the loss with a weight tensor  element-wisely . 4. Reduce the loss tensor to a  scalar . 5. Weighting the loss with a  scalar . ", "page_idx": 94, "bbox": [84, 365.4754638671875, 362, 451.1142883300781], "page_size": [612.0, 792.0]}
{"layout": 828, "type": "text", "text": "13.2 Set sampling method (step 1) ", "text_level": 1, "page_idx": 94, "bbox": [71, 473, 308, 490], "page_size": [612.0, 792.0]}
{"layout": 829, "type": "text", "text": "For some loss functions, sampling strategies are needed to avoid imbalance between positive and negative samples. For example, when using  Cross Entropy Loss  in RPN head, we need to set  Random Sampler  in  train_cfg ", "page_idx": 94, "bbox": [72, 505.0784606933594, 529.9608154296875, 536.321533203125], "page_size": [612.0, 792.0]}
{"layout": 830, "type": "text", "text": "train_cfg = dict ( rpn  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict ( sampler = dict ( type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}^{\\dagger}$  ' Random Sampler ' , num = 256 , pos fraction = 0.5 , neg_pos_ub  $\\mathrm{=}\\mathrm{-}\\mathrm{1}$  , add gt as proposals  $,=$  False )) ", "page_idx": 94, "bbox": [72, 546.0547485351562, 275.9847717285156, 640.052001953125], "page_size": [612.0, 792.0]}
{"layout": 831, "type": "text", "text": "For some other losses which have positive and negative sample balance mechanism such as Focal Loss, GHMC, and Quality Focal Loss, the sampler is no more necessary. ", "page_idx": 94, "bbox": [72, 652.3264770507812, 540, 677.5914916992188], "page_size": [612.0, 792.0]}
{"layout": 832, "type": "text", "text": "13.3 Tweaking loss ", "text_level": 1, "page_idx": 95, "bbox": [72, 71, 205, 87], "page_size": [612.0, 792.0]}
{"layout": 833, "type": "text", "text": "Tweaking a loss is more related with step 2, 4, 5, and most modifications can be specified in the config. Here we take Focal Loss (FL)  as an example. The following code sniper are the construction method and config of FL respectively, they are actually one to one correspondence. ", "page_idx": 95, "bbox": [72, 102.99446105957031, 540, 140.2145233154297], "page_size": [612.0, 792.0]}
{"layout": 834, "type": "image", "page_idx": 95, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_96.jpg", "bbox": [68, 142, 544, 349], "page_size": [612.0, 792.0], "ocr_text": "GLOSSES .register_module()\nclass FocalLoss(nn.Module):\n\ndef __init__(self,\nuse_sigmoid=True,\ngamma=2.0,\nalpha=0.25,\nreduction='mean',\nloss_weight=1.0):\n\nloss_cls=dict(\ntype='FocalLoss',\nuse_sigmoid=True,\ngamma=2.0,\nalpha=0.25,\nloss_weight=1.0)\n\n", "vlm_text": "The image contains Python code related to the implementation of a custom loss function called `FocalLoss`, which is a subclass of `nn.Module`. The `__init__` method of `FocalLoss` includes parameters `use_sigmoid`, `gamma`, `alpha`, `reduction`, and `loss_weight`.\n\nBelow this, there is a dictionary named `loss_cls` that holds configuration settings for `FocalLoss`, with keys and values corresponding to the parameters provided in the `__init__` method, such as `type`, `use_sigmoid`, `gamma`, `alpha`, and `loss_weight`."}
{"layout": 835, "type": "text", "text": "13.3.1 Tweaking hyper-parameters (step 2) ", "text_level": 1, "page_idx": 95, "bbox": [71, 370, 315, 385], "page_size": [612.0, 792.0]}
{"layout": 836, "type": "text", "text": "gamma  and  beta  are two hyper-parameters in the Focal Loss. Say if we want to change the value of  gamma  to be 1.5 and  alpha  to be 0.5, then we can specify them in the config as follows: ", "page_idx": 95, "bbox": [72, 395.9424743652344, 540, 421.2074890136719], "page_size": [612.0, 792.0]}
{"layout": 837, "type": "image", "page_idx": 95, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_97.jpg", "bbox": [69, 425, 542, 506], "page_size": [612.0, 792.0], "ocr_text": "loss_cls=dict(\ntype='FocalLoss',\nuse_sigmoid=True,\ngamma=1.5,\nalpha=0.5,\nloss_weight=1.0)\n\n", "vlm_text": "The image contains code that configures a dictionary for a classification loss function. It specifies parameters for \"FocalLoss,\" including:\n\n- `type`: 'FocalLoss'\n- `use_sigmoid`: True\n- `gamma`: 1.5\n- `alpha`: 0.5\n- `loss_weight`: 1.0\n\nThese parameters are commonly used in machine learning models for adjusting the loss function behavior."}
{"layout": 838, "type": "text", "text": "13.3.2 Tweaking the way of reduction (step 3) ", "text_level": 1, "page_idx": 95, "bbox": [71, 528, 331, 542], "page_size": [612.0, 792.0]}
{"layout": 839, "type": "text", "text": "The default way of reduction is  mean  for FL. Say if we want to change the reduction from  mean  to  sum , we can specify it in the config as follows: ", "page_idx": 95, "bbox": [72, 553.5974731445312, 540, 578.8624877929688], "page_size": [612.0, 792.0]}
{"layout": 840, "type": "image", "page_idx": 95, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_98.jpg", "bbox": [70, 583, 542, 675], "page_size": [612.0, 792.0], "ocr_text": "loss_cls=dict(\ntype='FocalLoss',\nuse_sigmoid=True,\ngamma=2.0,\nalpha=0.25,\nloss_weight=1.0,\nreduction='sum')\n", "vlm_text": "The image shows a snippet of Python code that defines a dictionary named `loss_cls`. This dictionary is used to configure the parameters for a loss function, specifically the Focal Loss, which is a type of loss function often used in training deep learning models for classification tasks. The parameters set in the dictionary include:\n\n- `type`: Specifies the type of loss function, which is set to `'FocalLoss'`.\n- `use_sigmoid`: A boolean set to `True`, indicating that a sigmoid function should be used.\n- `gamma`: A float set to `2.0`, which is a hyperparameter of the Focal Loss that adjusts the rate at which easy examples are down-weighted.\n- `alpha`: A float set to `0.25`, another hyperparameter that balances the importance of positive/negative examples.\n- `loss_weight`: A float set to `1.0`, indicating the weight of this loss during optimization.\n- `reduction`: A string set to `'sum'`, indicating that the loss values should be summed."}
{"layout": 841, "type": "text", "text": "13.3.3 Tweaking loss weight (step 5) ", "text_level": 1, "page_idx": 96, "bbox": [72, 70, 279, 85], "page_size": [612.0, 792.0]}
{"layout": 842, "type": "text", "text": "The loss weight here is a scalar which controls the weight of different losses in multi-task learning, e.g. classification loss and regression loss. Say if we want to change to loss weight of classification loss to be 0.5, we can specify it in the config as follows: ", "page_idx": 96, "bbox": [72, 95.81745910644531, 540, 133.0375213623047], "page_size": [612.0, 792.0]}
{"layout": 843, "type": "text", "text": "loss_cls = dict ( type = ' FocalLoss ' , use s igm oid = True , gamma  $\\scriptstyle-2\\,.\\,\\mathbb{0}$  , alpha  ${\\it\\Delta\\phi}=\\ 0\\ .\\ 25$  , loss weight = 0.5 ) ", "page_idx": 96, "bbox": [72, 142.77174377441406, 181, 212.55923461914062], "page_size": [612.0, 792.0]}
{"layout": 844, "type": "text", "text": "13.4 Weighting loss (step 3) ", "text_level": 1, "page_idx": 96, "bbox": [71, 242, 265, 260], "page_size": [612.0, 792.0]}
{"layout": 845, "type": "text", "text": "Weighting loss means we re-weight the loss element-wisely. To be more specific, we multiply the loss tensor with a weight tensor which has the same shape. As a result, different entries of the loss can be scaled differently, and so called element-wisely. The loss weight varies across different models and highly context related, but overall there are two kinds of loss weights,  label weights  for classification loss and  b box weights  for bbox regression loss. You can find them in the  get_target  method of the corresponding head. Here we take  ATSSHead  as an example, which inherit AnchorHead  but overwrite its  get targets  method which yields different  label weights  and  b box weights . ", "page_idx": 96, "bbox": [72, 275.07147216796875, 540, 348.1574401855469], "page_size": [612.0, 792.0]}
{"layout": 846, "type": "table", "page_idx": 96, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_99.jpg", "bbox": [69, 352, 542, 516], "page_size": [612.0, 792.0], "ocr_text": "class ATSSHead(AnchorHead):\n\ndef get_targets(self,\nanchor_list,\nvalid_flag_list,\ngt_bboxes_list,\nimg_metas,\ngt_bboxes_ignore_list=None,\ngt_labels_list=None,\nlabel_channels=1,\nunmap_outputs=True) :\n", "vlm_text": "This image shows a snippet of Python code defining a class and a method within that class. It is not a table. Here's a breakdown of the code:\n\n- **Class Definition:**\n  - `ATSSHead` is a class that inherits from `AnchorHead`.\n\n- **Method Definition:**\n  - `get_targets(self, ...)`: This is a method within the `ATSSHead` class.\n  - It takes several parameters:\n    - `self`: Refers to the instance of the class.\n    - `anchor_list`\n    - `valid_flag_list`\n    - `gt_bboxes_list`\n    - `img_metas`\n    - `gt_bboxes_ignore_list` (default=None)\n    - `gt_labels_list` (default=None)\n    - `label_channels` (default=1)\n    - `unmap_outputs` (default=True)\n\nThis method likely deals with processing or handling targets for the object detection task."}
{"layout": 847, "type": "text", "text": "TUTORIAL 7: FINETUNING MODELS ", "text_level": 1, "page_idx": 98, "bbox": [291, 163, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 848, "type": "text", "text": "Detectors pre-trained on the COCO dataset can serve as a good pre-trained model for other datasets, e.g., CityScapes and KITTI Dataset. This tutorial provides instruction for users to use the models provided in the  Model Zoo  for other datasets to obtain better performance. ", "page_idx": 98, "bbox": [72, 225.87342834472656, 540, 263.093505859375], "page_size": [612.0, 792.0]}
{"layout": 849, "type": "text", "text": "There are two steps to finetune a model on a new dataset. ", "page_idx": 98, "bbox": [72, 267.7164611816406, 299.18707275390625, 281.0264892578125], "page_size": [612.0, 792.0]}
{"layout": 850, "type": "text", "text": "• Add support for the new dataset following  Tutorial 2: Customize Datasets . • Modify the configs as will be discussed in this tutorial. ", "page_idx": 98, "bbox": [88, 285.6484680175781, 392.8061828613281, 316.8915100097656], "page_size": [612.0, 792.0]}
{"layout": 851, "type": "text", "text": "Take the finetuning process on Cityscapes Dataset as an example, the users need to modify five parts in the config. ", "page_idx": 98, "bbox": [72, 321.5144958496094, 526.5337524414062, 334.82452392578125], "page_size": [612.0, 792.0]}
{"layout": 852, "type": "text", "text": "14.1 Inherit base configs ", "text_level": 1, "page_idx": 98, "bbox": [72, 357, 243, 373], "page_size": [612.0, 792.0]}
{"layout": 853, "type": "text", "text": "To release the burden and reduce bugs in writing the whole configs, MM Detection V2.0 support inheriting configs from multiple existing configs. To finetune a Mask RCNN model, the new config needs to inherit  _base_/models/ mask_rcnn_  $\\mathtt{r50}$  _fpn.py  to build the basic structure of the model. To use the Cityscapes Dataset, the new config can also simply inherit  _base_/datasets/cityscape s instance.py . For runtime settings such as training schedules, the new config needs to inherit  _base_/default runtime.py . This configs are in the  configs  directory and the users can also choose to write the whole contents rather than use inheritance. ", "page_idx": 98, "bbox": [72, 389.02349853515625, 540, 462.1094665527344], "page_size": [612.0, 792.0]}
{"layout": 854, "type": "text", "text": "14.2 Modify head ", "text_level": 1, "page_idx": 98, "bbox": [72, 548, 192, 563], "page_size": [612.0, 792.0]}
{"layout": 855, "type": "text", "text": "Then the new config needs to modify the head according to the class numbers of the new datasets. By only changing num classes  in the roi_head, the weights of the pre-trained models are mostly reused except the final prediction head. ", "page_idx": 98, "bbox": [72, 579.4484252929688, 540, 604.7135009765625], "page_size": [612.0, 792.0]}
{"layout": 856, "type": "text", "text": "num classes  $^{=8}$  , bbox_coder  $=$  dict ( type  $\\equiv^{\\dagger}$  Delta XY WH B Box Code r ' , target means  $=$  [ 0. ,  0. ,  0. ,  0. ], target stds  $,=$  [ 0.1 ,  0.1 ,  0.2 ,  0.2 ]), reg class agnostic  $=$  False , loss_cls  $=$  dict ( type  $\\equiv^{\\dagger}$  Cross Entropy Loss ' , use s igm oid = False , loss weight  $\\scriptstyle=1.\\,\\mathbb{O}.$  ), loss_bbox  $\\fallingdotseq$  dict ( type  $\\equiv^{1}$  ' Smooth L 1 Loss ' , beta  $\\scriptstyle=1.\\,\\mathbb{0}$  , loss weight  $\\scriptstyle=1\\,.\\,\\mathbb{0})$  ), mask_head = dict ( type  $\\equiv^{\\dagger}$  FC N Mask Head ' , num_convs  ${=}4$  , in channels  $\\iota{=}256$  , con v out channel  $s{=}256$  , num classes  $^{=8}$  , loss_mask  $\\fallingdotseq$  dict ( type $\\L=\\Gamma$ Cross Entropy Loss', use_mask $\\risingdotseq$ True, loss weight=1.0))))", "page_idx": 99, "bbox": [113.843017578125, 87, 474.7381896972656, 289.2672119140625], "page_size": [612.0, 792.0]}
{"layout": 857, "type": "text", "text": "14.3 Modify dataset ", "text_level": 1, "page_idx": 99, "bbox": [71, 319, 209, 336], "page_size": [612.0, 792.0]}
{"layout": 858, "type": "text", "text": "The users may also need to prepare the dataset and write the configs about dataset. MM Detection V2.0 already support VOC, WIDER FACE, COCO and Cityscapes Dataset. ", "page_idx": 99, "bbox": [71, 351.48046875, 540, 376.7454833984375], "page_size": [612.0, 792.0]}
{"layout": 859, "type": "text", "text": "14.4 Modify training schedule ", "text_level": 1, "page_idx": 99, "bbox": [71, 399, 276, 416], "page_size": [612.0, 792.0]}
{"layout": 860, "type": "text", "text": "The finetuning hyper parameters vary from the default schedule. It usually requires smaller learning rate and less training epochs\n\n ", "page_idx": 99, "bbox": [71, 431.30743408203125, 540, 456.57244873046875], "page_size": [612.0, 792.0]}
{"layout": 861, "type": "text", "text": "# optimizer\n\n # lr is set for a batch size of 8 optimizer  $=$   dict ( type  $\\equiv^{1}$  SGD ' ,   $\\scriptstyle1r=0\\,.\\,0\\,1$  , momentum  $\\scriptstyle1=0\\,.\\,9$  , weight decay = 0.0001 ) optimizer config  $=$   dict (grad_clip  $\\risingdotseq$  None ) # learning policy lr_config  $=$   dict ( policy  $\\leftleftarrows$  ' step ' , warmup  $\\acute{=}$  linear ' , warm up it ers  $\\scriptstyle{\\bullet=500}$  , warm up rat i $\\mathfrak{o}{=}\\mathbb{0}\\cdot\\mathbb{0}\\mathbb{0}1$ ,step  $\\leftrightharpoons$  [ 7 ]) # the max_epochs and step in lr_config need specifically tuned for the customized dataset runner  $=$   dict (max_epochs  $\\mathbf{\\varepsilon}\\mathbf{=}\\mathbf{8}.$  ) log_config  $=$   dict (interval  $\\scriptstyle\\cdot=100$  ) ", "page_idx": 99, "bbox": [71, 466.30572509765625, 540, 631.735107421875], "page_size": [612.0, 792.0]}
{"layout": 862, "type": "text", "text": "14.5 Use pre-trained model ", "text_level": 1, "page_idx": 100, "bbox": [72, 72, 260, 87], "page_size": [612.0, 792.0]}
{"layout": 863, "type": "text", "text": "To use the pre-trained model, the new config add the link of pre-trained models in the  load_from . The users might need to download the model weights before training to avoid the download time during training. ", "page_idx": 100, "bbox": [72, 102.99446105957031, 539, 128.25950622558594], "page_size": [612.0, 792.0]}
{"layout": 864, "type": "text", "text": "load_from    $=$   ' https://download.openmmlab.com/mm detection/v2.0/mask_rcnn/mask r cnn r 50\n\n  $\\hookrightarrow$  caff e fp n m strain-poly 3 x coco/mask r cnn r 50 caff e fp n m strain-poly 3 x coco b box mAP-0.\n\n →\n\n  $\\hookrightarrow408,$  408__segm_mAP-0.37_20200504_163245-42aa3d00.pth ' # noqa\n\n → ", "page_idx": 100, "bbox": [72, 137.9927520751953, 539, 174.7943878173828], "page_size": [612.0, 792.0]}
{"layout": 865, "type": "text", "text": "TUTORIAL 8: PYTORCH TO ONNX (EXPERIMENTAL) ", "text_level": 1, "page_idx": 102, "bbox": [182, 162, 542, 182], "page_size": [612.0, 792.0]}
{"layout": 866, "type": "text", "text": "•  Tutorial 8: Pytorch to ONNX (Experimental) –  How to convert models from Pytorch to ONNX ∗ Prerequisite ∗ Usage ∗ Description of all arguments –  How to evaluate the exported models ∗ Prerequisite ∗ Usage ∗ Description of all arguments ∗ Results and Models –  List of supported models exportable to ONNX –  The Parameters of Non-Maximum Suppression in ONNX Export –  Reminders –  FAQs ", "page_idx": 102, "bbox": [88.43799591064453, 225.72398376464844, 373.4580383300781, 472.9063415527344], "page_size": [612.0, 792.0]}
{"layout": 867, "type": "text", "text": "15.1 How to convert models from Pytorch to ONNX ", "text_level": 1, "page_idx": 102, "bbox": [72, 495, 422, 511], "page_size": [612.0, 792.0]}
{"layout": 868, "type": "text", "text": "15.1.1 Prerequisite ", "page_idx": 102, "bbox": [72.0, 526.9605102539062, 180.43368530273438, 544.080322265625], "page_size": [612.0, 792.0]}
{"layout": 869, "type": "text", "text": "1. Install the prerequisites following  get started.md/Prepare environment . 2. Build custom operators for ONNX Runtime and install MMCV manually following  How to build custom oper- ators for ONNX Runtime 3. Install MM detection manually following steps 2-3 in  get started.md/Install MM detection . ", "page_idx": 102, "bbox": [84, 554.1151123046875, 540.002685546875, 615.3955688476562], "page_size": [612.0, 792.0]}
{"layout": 870, "type": "text", "text": "15.1.2 Usage ", "text_level": 1, "page_idx": 103, "bbox": [72, 71, 148, 85], "page_size": [612.0, 792.0]}
{"layout": 871, "type": "text", "text": "python tools/deployment/py torch 2 on nx.py  \\ \\${ CONFIG FILE }  \\ \\${ CHECKPOINT FILE }  \\ --output-file  $\\mathcal{S}$ {OUTPUT FILE} \\--input-img \\${INPUT IMAGE PATH} \\--shape    $\\mathcal{S}$  { IMAGE SHAPE }  \\ --test-img \\${TEST IMAGE PATH} \\--opset-version  \\${ OP SET VERSION }  \\ --cfg-options  \\${ CF G OPTIONS } --dynamic-export  \\ --show  \\ --verify  \\ --simplify  \\ ", "page_idx": 103, "bbox": [72, 100.33097076416016, 286.44537353515625, 254.70217895507812], "page_size": [612.0, 792.0]}
{"layout": 872, "type": "text", "text": "15.1.3 Description of all arguments ", "text_level": 1, "page_idx": 103, "bbox": [72, 282, 274, 296], "page_size": [612.0, 792.0]}
{"layout": 873, "type": "text", "text": "•  config  $:$   The path of a model config file. •  checkpoint  $:$   The path of a model checkpoint file. •  --output-file : The path of output ONNX model. If not specified, it will be set to  tmp.onnx . •  --input-img : The path of an input image for tracing and conversion. By default, it will be set to  tests/data/ color.jpg . •  --shape : The height and width of input tensor to the model. If not specified, it will be set to  800 1216 . •  --test-img  $:$   The path of an image to verify the exported ONNX model. By default, it will be set to  None , meaning it will use  --input-img  for verification. •  --opset-version  $:$   The opset version of ONNX. If not specified, it will be set to  11 . •  --dynamic-export : Determines whether to export ONNX model with dynamic input and output shapes. If not specified, it will be set to  False . •  --show : Determines whether to print the architecture of the exported model and whether to show detection outputs when  --verify  is set to  True . If not specified, it will be set to  False . •  --verify : Determines whether to verify the correctness of an exported model. If not specified, it will be set to False . •  --simplify : Determines whether to simplify the exported ONNX model. If not specified, it will be set to False . •  --cfg-options : Override some settings in the used config file, the key-value pair in  xxx=yyy  format will be merged into config file. •  --skip-post process : Determines whether export model without post process. If not specified, it will be set to  False . Notice: This is an experimental option. Only work for some single stage models. Users need to implement the post-process by themselves. We do not guarantee the correctness of the exported model. ", "page_idx": 103, "bbox": [88, 307.27044677734375, 540, 643.3705444335938], "page_size": [612.0, 792.0]}
{"layout": 874, "type": "text", "text": "Example: ", "page_idx": 103, "bbox": [72, 647.9934692382812, 110.08708953857422, 661.3035278320312], "page_size": [612.0, 792.0]}
{"layout": 875, "type": "table", "page_idx": 103, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_100.jpg", "bbox": [69, 666, 543, 711], "page_size": [612.0, 792.0], "ocr_text": "python tools/deployment/pytorch2onnx.py \\\nconfigs/yolo/yolov3_d53_mstrain-608_273e_coco.py \\\ncheckpoints/yolo/yolov3_d53_mstrain-608_273e_coco.pth \\\n\n", "vlm_text": "The image shows a command likely used for converting a PyTorch model to the ONNX format, using a Python script. Here's a breakdown of the command:\n\n```bash\npython tools/deployment/pytorch2onnx.py \\\n    configs/yolo/yolov3_d53_mstrain-608_273e_coco.py \\\n    checkpoints/yolo/yolov3_d53_mstrain-608_273e_coco.pth \\\n```\n\n- `python tools/deployment/pytorch2onnx.py`: This is the script responsible for the conversion.\n- `configs/yolo/yolov3_d53_mstrain-608_273e_coco.py`: This refers to the configuration file for the YOLOv3 model.\n- `checkpoints/yolo/yolov3_d53_mstrain-608_273e_coco.pth`: This is the file containing the trained weights of the model. \n\nThe backslashes (`\\`) indicate that the command continues on the next line."}
{"layout": 876, "type": "text", "text": "--output-file checkpoints/yolo/yo lov 3 d 53 m strain-608 273 e coco.onnx  \\ --input-img demo/demo.jpg  \\ --test-img tests/data/color.jpg  \\ --shape  608 608  \\ --show  \\ --verify  \\ --dynamic-export  \\ --cfg-options  \\ model.test_cfg.deploy nm s pre  $_{=-1}$   \\ ", "page_idx": 104, "bbox": [92, 87.07498931884766, 459.04736328125, 193.62612915039062], "page_size": [612.0, 792.0]}
{"layout": 877, "type": "text", "text": "15.2 How to evaluate the exported models ", "text_level": 1, "page_idx": 104, "bbox": [72, 224, 361, 240], "page_size": [612.0, 792.0]}
{"layout": 878, "type": "text", "text": "We prepare a tool  tools/de plop y ment/test.py  to evaluate ONNX models with ON NX Runtime and TensorRT. ", "page_idx": 104, "bbox": [72.0, 255.8384552001953, 529.4827880859375, 269.14849853515625], "page_size": [612.0, 792.0]}
{"layout": 879, "type": "text", "text": "15.2.1 Prerequisite ", "text_level": 1, "page_idx": 104, "bbox": [71, 288, 181, 303], "page_size": [612.0, 792.0]}
{"layout": 880, "type": "text", "text": "• Install onnx and on nx runtime (CPU version) ", "page_idx": 104, "bbox": [88, 314.06646728515625, 274.5400390625, 327.3764953613281], "page_size": [612.0, 792.0]}
{"layout": 881, "type": "table", "page_idx": 104, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_101.jpg", "bbox": [88, 327.25, 544, 411], "page_size": [612.0, 792.0], "ocr_text": "pip install onnx onnxruntime==1.5.1\n\nIf you want to run the model on GPU, please remove the CPU version before using the GPU version.\n\npip uninstall onnxruntime\npip install onnxruntime-gpu\n\n", "vlm_text": "The table contains instructions for installing ONNX and ONNX Runtime Python packages:\n\n1. To install ONNX with a specific version of ONNX Runtime:\n   ```\n   pip install onnx onnxruntime==1.5.1\n   ```\n\n2. Note on running the model on GPU:\n   - You must uninstall the CPU version of ONNX Runtime before installing the GPU version.\n\n3. Commands to switch from CPU to GPU version:\n   ```\n   pip uninstall onnxruntime\n   pip install onnxruntime-gpu\n   ```"}
{"layout": 882, "type": "text", "text": "Note: on nx runtime-gpu is version-dependent on CUDA and CUDNN, please ensure that your environment meets the requirements. ", "page_idx": 104, "bbox": [96, 417.27947998046875, 540.0034790039062, 442.54449462890625], "page_size": [612.0, 792.0]}
{"layout": 883, "type": "text", "text": "• Build custom operators for ONNX Runtime following  How to build custom operators for ONNX Runtime • Install TensorRT by referring to  How to build TensorRT plugins in MMCV  (optional) ", "page_idx": 104, "bbox": [88, 447.16748046875, 518.703369140625, 460.4775085449219], "page_size": [612.0, 792.0]}
{"layout": 884, "type": "text", "text": "", "page_idx": 104, "bbox": [88, 465.1004943847656, 437.5081787109375, 478.4105224609375], "page_size": [612.0, 792.0]}
{"layout": 885, "type": "text", "text": "15.2.2 Usage ", "text_level": 1, "page_idx": 104, "bbox": [72, 498, 149, 512], "page_size": [612.0, 792.0]}
{"layout": 886, "type": "table", "page_idx": 104, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_102.jpg", "bbox": [69, 524, 544, 663], "page_size": [612.0, 792.0], "ocr_text": "python tools/deployment/test.py \\\n${CONFIG_FILE} \\\n${MODEL_FILE} \\\n--out ${OUTPUT_FILE} \\\n--backend ${BACKEND} \\\n--format-only ${FORMAT_ONLY} \\\n--eval ${EVALUATION_METRICS} \\\n--show-dir ${SHOW_DIRECTORY} \\\n----show-score-thr ${SHOW_SCORE_THRESHOLD} \\\n----cfg-options §{CFG_OPTIONS} \\\n----eval-options ${EVALUATION_OPTIONS} \\\n\n", "vlm_text": "The image shows a command line input for running a Python script, specifically for testing deployments with a set of configurable parameters. The command is structured with placeholders that are meant to be replaced with actual values:\n\n- `python tools/deployment/test.py \\`\n- `${CONFIG_FILE}`: configuration file\n- `${MODEL_FILE}`: model file\n- `--out ${OUTPUT_FILE}`: output file\n- `--backend ${BACKEND}`: backend option\n- `--format-only ${FORMAT_ONLY}`: format-only option\n- `--eval ${EVALUATION_METRICS}`: evaluation metrics\n- `--show-dir ${SHOW_DIRECTORY}`: show directory\n- `----show-score-thr ${SHOW_SCORE_THRESHOLD}`: show score threshold\n- `----cfg-options ${CFG_OPTIONS}`: configuration options\n- `----eval-options ${EVALUATION_OPTIONS}`: evaluation options\n\nEach of these placeholders is intended to be filled with specific information related to the deployment and testing process."}
{"layout": 887, "type": "text", "text": "15.2.3 Description of all arguments ", "text_level": 1, "page_idx": 105, "bbox": [72, 70, 274, 85], "page_size": [612.0, 792.0]}
{"layout": 888, "type": "text", "text": "•  config : The path of a model config file. •  model : The path of an input model file. •  --out : The path of output result file in pickle format. •  --backend : Backend for input model to run and should be  on nx runtime  or  tensorrt . •  --format-only  $:$   Format the output results without perform evaluation. It is useful when you want to format the result to a specific format and submit it to the test server. If not specified, it will be set to  False . •  --eval : Evaluation metrics, which depends on the dataset, e.g., “bbox”, “segm”, “proposal” for COCO, and “mAP”, “recall” for PASCAL VOC. •  --show-dir : Directory where painted images will be saved •  --show-score-thr : Score threshold. Default is set to  0.3 . •  --cfg-options : Override some settings in the used config file, the key-value pair in  xxx=yyy  format will be merged into config file. •  --eval-options : Custom options for evaluation, the key-value pair in  xxx  $=$  yyy  format will be kwargs for dataset.evaluate()  function ", "page_idx": 105, "bbox": [88, 95.81745910644531, 540, 318.3424377441406], "page_size": [612.0, 792.0]}
{"layout": 889, "type": "text", "text": "", "text_level": 1, "page_idx": 105, "bbox": [71, 324, 97, 329.75], "page_size": [612.0, 792.0]}
{"layout": 890, "type": "text", "text": "• If the deployed backend platform is TensorRT, please add environment variables before running the file: ", "page_idx": 105, "bbox": [88, 340.8984375, 510.0359802246094, 354.2084655761719], "page_size": [612.0, 792.0]}
{"layout": 891, "type": "text", "text": "export  ON NX BACKEND  $^{1=}$  MM CV Tensor RT ", "page_idx": 105, "bbox": [96, 363.9417419433594, 264.2785949707031, 373.95416259765625], "page_size": [612.0, 792.0]}
{"layout": 892, "type": "text", "text": "• If you want to use the  --dynamic-export  parameter in the TensorRT backend to export ONNX, please remove the  --simplify  parameter, and vice versa. ", "page_idx": 105, "bbox": [88, 386.5274658203125, 540, 411.79248046875], "page_size": [612.0, 792.0]}
{"layout": 893, "type": "text", "text": "15.2.4 Results and Models ", "text_level": 1, "page_idx": 105, "bbox": [72, 430, 225, 445], "page_size": [612.0, 792.0]}
{"layout": 894, "type": "text", "text": "Notes: ", "text_level": 1, "page_idx": 105, "bbox": [71, 458, 97, 469], "page_size": [612.0, 792.0]}
{"layout": 895, "type": "text", "text": "• All ONNX models are evaluated with dynamic shape on coco dataset and images are pre processed according to the original config file. Note that CornerNet is evaluated without test-time flip, since currently only single-scale evaluation is supported with ONNX Runtime. • Mask AP of Mask R-CNN drops by  $1\\%$   for ON NX Runtime. The main reason is that the predicted masks are directly interpolated to original image in PyTorch, while they are at first interpolated to the pre processed input image of the model and then to original image in other backend. ", "page_idx": 105, "bbox": [88, 474.6434631347656, 540, 553.7064819335938], "page_size": [612.0, 792.0]}
{"layout": 896, "type": "text", "text": "15.3 List of supported models exportable to ONNX ", "text_level": 1, "page_idx": 105, "bbox": [71, 576, 419, 592], "page_size": [612.0, 792.0]}
{"layout": 897, "type": "text", "text": "The table below lists the models that are guaranteed to be exportable to ONNX and runnable in ONNX Runtime. Notes: ", "page_idx": 105, "bbox": [71, 608.2684326171875, 521.5323486328125, 639.511474609375], "page_size": [612.0, 792.0]}
{"layout": 898, "type": "text", "text": "• Minimum required version of MMCV is  1.3.5 •  All models above are tested with Pytorch  $==l.6.0$   and onnxruntim  $\\scriptstyle{\\==I.5.I}$  , except for CornerNet. For more details about the torch version when exporting CornerNet to ONNX, which involves  mmcv::cummax , please refer to the  Known Issues  in mmcv. ", "page_idx": 105, "bbox": [88, 644.1334228515625, 540, 699.2864990234375], "page_size": [612.0, 792.0]}
{"layout": 899, "type": "text", "text": "• Though supported, it is  not recommended  to use batch inference in on nx runtime for  DETR , because there is huge performance gap between ONNX and torch model (e.g. 33.5 vs  $39.9\\;\\mathrm{mAP}$   on COCO for on nx runtime and torch respectively, with a batch size 2). The main reason for the gap is that these is non-negligible effect on the predicted regressions during batch inference for ONNX, since the predicted coordinates is normalized by img_shape  (without padding) and should be converted to absolute format, but  img_shape  is not dynamically traceable thus the padded  img shape for on nx  is used. • Currently only single-scale evaluation is supported with ONNX Runtime, also mmcv::Soft Non Max Suppression  is only supported for single image by now. ", "page_idx": 106, "bbox": [88, 71.30303192138672, 540, 174.4255828857422], "page_size": [612.0, 792.0]}
{"layout": 900, "type": "text", "text": "15.4 The Parameters of Non-Maximum Suppression in ONNX Export ", "text_level": 1, "page_idx": 106, "bbox": [73, 197, 540, 213], "page_size": [612.0, 792.0]}
{"layout": 901, "type": "text", "text": "In the process of exporting the ONNX model, we set some parameters for the NMS op to control the number of output bounding boxes. The following will introduce the parameter setting of the NMS op in the supported models. You can set these parameters through  --cfg-options . ", "page_idx": 106, "bbox": [72, 228.98756408691406, 540, 266.2076416015625], "page_size": [612.0, 792.0]}
{"layout": 902, "type": "text", "text": "•  nms_pre : The number of boxes before NMS. The default setting is  1000 . •  deploy nm s pre : The number of boxes before NMS when exporting to ONNX model. The default setting is 0 . •  max per img : The number of boxes to be kept after NMS. The default setting is  100 . •  max output boxes per class : Maximum number of output boxes per class of NMS. The default setting is 200 . ", "page_idx": 106, "bbox": [88, 270.83062744140625, 540, 361.84967041015625], "page_size": [612.0, 792.0]}
{"layout": 903, "type": "text", "text": "15.5 Reminders ", "text_level": 1, "page_idx": 106, "bbox": [71, 383, 184, 401], "page_size": [612.0, 792.0]}
{"layout": 904, "type": "text", "text": "• When the input model has custom op such as  RoIAlign  and if you want to verify the exported ONNX model, you may have to build  mmcv  with  ON NX Runtime  from source. •  mmcv.onnx.simplify  feature is based on  onnx-simplifier . If you want to try it, please refer to  onnx in  mmcv and  on nx runtime op in  mmcv  for more information. • If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, please try to dig a little deeper and debug a little bit more and hopefully solve them by yourself. • Because this feature is experimental and may change fast, please always try with the latest  mmcv  and  mmdetecion . ", "page_idx": 106, "bbox": [88, 416.4106140136719, 540, 531.3396606445312], "page_size": [612.0, 792.0]}
{"layout": 905, "type": "text", "text": "15.6 FAQs ", "text_level": 1, "page_idx": 106, "bbox": [72, 554, 144, 569], "page_size": [612.0, 792.0]}
{"layout": 906, "type": "text", "text": "• None ", "page_idx": 106, "bbox": [88, 585.9015502929688, 118.28668212890625, 599.2116088867188], "page_size": [612.0, 792.0]}
{"layout": 907, "type": "text", "text": "TUTORIAL 9: ONNX TO TENSORRT (EXPERIMENTAL) ", "text_level": 1, "page_idx": 108, "bbox": [173, 162, 541, 182], "page_size": [612.0, 792.0]}
{"layout": 908, "type": "text", "text": "•  Tutorial 9: ONNX to TensorRT (Experimental) –  How to convert models from ONNX to TensorRT ∗ Prerequisite ∗ Usage –  How to evaluate the exported models –  List of supported models convertible to TensorRT –  Reminders –  FAQs ", "page_idx": 108, "bbox": [88.43799591064453, 225.72398376464844, 315.0772705078125, 365.3102722167969], "page_size": [612.0, 792.0]}
{"layout": 909, "type": "text", "text": "16.1 How to convert models from ONNX to TensorRT ", "text_level": 1, "page_idx": 108, "bbox": [72, 387, 433, 403], "page_size": [612.0, 792.0]}
{"layout": 910, "type": "text", "text": "16.1.1 Prerequisite ", "text_level": 1, "page_idx": 108, "bbox": [72, 421, 182, 435], "page_size": [612.0, 792.0]}
{"layout": 911, "type": "text", "text": "1. Please refer to  get started.md  for installation of MMCV and MM Detection from source. 2. Please refer to  ON NX Runtime in mmcv  and  TensorRT plugin in mmcv  to install  mmcv-full  with ONNXRun- time custom ops and TensorRT plugins. 3. Use our tool  py torch 2 on nx  to convert the model from PyTorch to ONNX. ", "page_idx": 108, "bbox": [84, 446.66748046875, 540.0029296875, 507.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 912, "type": "text", "text": "16.1.2 Usage ", "text_level": 1, "page_idx": 108, "bbox": [71, 527, 148, 541], "page_size": [612.0, 792.0]}
{"layout": 913, "type": "table", "page_idx": 108, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_103.jpg", "bbox": [67, 552, 544, 692], "page_size": [612.0, 792.0], "ocr_text": "python tools/deployment/onnx2tensorrt.py \\\n${CONFIG} \\\n${MODEL} \\\n--trt-file ${TRT_FILE} \\\n--input-img ${INPUT_IMAGE_PATH} \\\n--shape ${INPUT_IMAGE_SHAPE} \\\n--min-shape ${MIN_IMAGE_SHAPE} \\\n--max-shape ${MAX_IMAGE_SHAPE} \\\n--workspace-size {WORKSPACE_SIZE} \\\n--show \\\n--verify \\\n\n", "vlm_text": "The provided image shows a block of code rather than a traditional table. The code is a command-line script used to run a Python script, which likely facilitates the conversion of an ONNX model to a TensorRT model using various parameters or flags. Here's a breakdown of the command:\n\n- `python tools/deployment/onnx2tensorrt.py`: This initial part indicates that a Python script located at `tools/deployment/onnx2tensorrt.py` is being executed.\n- `${CONFIG}`: Represents a placeholder for a configuration file or argument to be replaced by the user.\n- `${MODEL}`: Placeholder for the model file or argument.\n- `--trt-file ${TRT_FILE}`: Specifies the output filename for the TensorRT model.\n- `--input-img ${INPUT_IMAGE_PATH}`: Specifies the path to an input image.\n- `--shape ${INPUT_IMAGE_SHAPE}`: Indicates the shape of the input image.\n- `--min-shape ${MIN_IMAGE_SHAPE}`: Minimum shape of the input image.\n- `--max-shape ${MAX_IMAGE_SHAPE}`: Maximum shape of the input image.\n- `--workspace-size {WORKSPACE_SIZE}`: Specifies the workspace size for TensorRT engine building.\n- `--show`: An option that, when invoked, may display additional information or outcomes.\n- `--verify`: An option probably used to verify the conversion or output.\n\nEach line ends with a backslash (`\\`), which in many programming and scripting languages indicates that the command continues onto the next line. This format helps in making complex command-line inputs more readable."}
{"layout": 914, "type": "text", "text": "•  config  $:$   The path of a model config file. •  model  $:$   The path of an ONNX model file. •  --trt-file : The Path of output TensorRT engine file. If not specified, it will be set to  tmp.trt . •  --input-img  $:$   The path of an input image for tracing and conversion. By default, it will be set to  demo/demo. jpg . •  --shape : The height and width of model input. If not specified, it will be set to  400 600 . •  --min-shape : The minimum height and width of model input. If not specified, it will be set to the same as --shape . •  --max-shape : The maximum height and width of model input. If not specified, it will be set to the same as --shape . •  --workspace-size  : The required GPU workspace size in GiB to build TensorRT engine. If not specified, it will be set to  1  GiB. •  --show : Determines whether to show the outputs of the model. If not specified, it will be set to  False . •  --verify : Determines whether to verify the correctness of models between ON NX Runtime and TensorRT. If not specified, it will be set to  False . •  --verbose : Determines whether to print logging messages. It’s useful for debugging. If not specified, it will be set to  False . ", "page_idx": 109, "bbox": [88, 71.45246887207031, 540, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 915, "type": "text", "text": "Example: ", "text_level": 1, "page_idx": 109, "bbox": [70, 342, 111, 354], "page_size": [612.0, 792.0]}
{"layout": 916, "type": "text", "text": "python tools/deployment/on nx 2 tensor rt.py  \\ configs/retinanet/retina net r 50 fp n 1 x coco.py  \\ checkpoints/retina net r 50 fp n 1 x coco.onnx  \\ --trt-file checkpoints/retina net r 50 fp n 1 x coco.trt  \\ --input-img demo/demo.jpg  \\ --shape  400 600  \\ --show  \\ --verify  \\ ", "page_idx": 109, "bbox": [72, 362.88897705078125, 375.36138916015625, 457.48394775390625], "page_size": [612.0, 792.0]}
{"layout": 917, "type": "text", "text": "16.2 How to evaluate the exported models ", "text_level": 1, "page_idx": 109, "bbox": [71, 488, 361, 504], "page_size": [612.0, 792.0]}
{"layout": 918, "type": "text", "text": "We prepare a tool  tools/de plop y ment/test.py  to evaluate TensorRT models. ", "page_idx": 109, "bbox": [72, 519.6974487304688, 396.6015625, 533.0075073242188], "page_size": [612.0, 792.0]}
{"layout": 919, "type": "text", "text": "Please refer to following links for more information. ", "page_idx": 109, "bbox": [72, 537.6304321289062, 279.4909973144531, 550.9404907226562], "page_size": [612.0, 792.0]}
{"layout": 920, "type": "text", "text": "•  how-to-evaluate-the-exported-models •  results-and-models ", "page_idx": 109, "bbox": [88, 555.5624389648438, 246.2165069580078, 586.8054809570312], "page_size": [612.0, 792.0]}
{"layout": 921, "type": "text", "text": "16.3 List of supported models convertible to TensorRT ", "text_level": 1, "page_idx": 110, "bbox": [72, 71, 447, 87], "page_size": [612.0, 792.0]}
{"layout": 922, "type": "text", "text": "The table below lists the models that are guaranteed to be convertible to TensorRT. ", "page_idx": 110, "bbox": [72, 102.99446105957031, 401.1443176269531, 116.30449676513672], "page_size": [612.0, 792.0]}
{"layout": 923, "type": "text", "text": "Notes: ", "page_idx": 110, "bbox": [72, 120.92646789550781, 97.81309509277344, 134.2364959716797], "page_size": [612.0, 792.0]}
{"layout": 924, "type": "text", "text": "•  All models above are tested with Pytorch  $==l.6.0,$  ,   $o n n x{=}{l.7.0}$   and TensorRT-7.2.1.6.Ubuntu-16.04.x86_64- gnu.cuda-10.2.cudnn8.0 ", "page_idx": 110, "bbox": [88, 138.7100067138672, 540, 164.01490783691406], "page_size": [612.0, 792.0]}
{"layout": 925, "type": "text", "text": "16.4 Reminders ", "text_level": 1, "page_idx": 110, "bbox": [71, 187, 183, 203], "page_size": [612.0, 792.0]}
{"layout": 926, "type": "text", "text": "• If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, we may not provide much help here due to the limited resources. Please try to dig a little deeper and debug by yourself. • Because this feature is experimental and may change fast, please always try with the latest  mmcv  and  mmdetecion . ", "page_idx": 110, "bbox": [88, 218.6864776611328, 540, 273.83953857421875], "page_size": [612.0, 792.0]}
{"layout": 927, "type": "text", "text": "16.5 FAQs ", "text_level": 1, "page_idx": 110, "bbox": [72, 297, 144, 312], "page_size": [612.0, 792.0]}
{"layout": 928, "type": "text", "text": "• None ", "page_idx": 110, "bbox": [88, 328.4014892578125, 118.2867202758789, 341.7115173339844], "page_size": [612.0, 792.0]}
{"layout": 929, "type": "text", "text": "TUTORIAL 10: WEIGHT INITIALIZATION", "text_level": 1, "page_idx": 112, "bbox": [269, 163, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 930, "type": "text", "text": "During training, a proper initialization strategy is beneficial to speeding up the training or obtaining a higher perfor- mance.  MMCV  provide some commonly used methods for initializing modules like  nn.Conv2d . Model initialization in MM detection mainly uses  init_cfg . Users can initialize models with following two steps: ", "page_idx": 112, "bbox": [71, 225.87342834472656, 540, 263.093505859375], "page_size": [612.0, 792.0]}
{"layout": 931, "type": "text", "text": "1. Define  init_cfg  for a model or its components in  model_cfg , but  init_cfg  of children components have higher priority and will override  init_cfg  of parents modules. 2. Build model as usual, but call  model.in it weights()  method explicitly, and model parameters will be ini- tialized as configuration. ", "page_idx": 112, "bbox": [84, 267.7164611816406, 540, 322.8694763183594], "page_size": [612.0, 792.0]}
{"layout": 932, "type": "text", "text": "The high-level workflow of initialization in MM detection is : ", "page_idx": 112, "bbox": [71, 327.4924621582031, 314.7784729003906, 340.802490234375], "page_size": [612.0, 792.0]}
{"layout": 933, "type": "text", "text": "model_cfg(init_cfg)   $->$   build from cf g   $->$   model   $->$   in it weight()  $->$   initialize(self, self.init_cfg)  $->$   children’s in it weight()", "page_idx": 112, "bbox": [71, 345.4244689941406, 540, 370.6894836425781], "page_size": [612.0, 792.0]}
{"layout": 934, "type": "text", "text": "17.1 Description ", "text_level": 1, "page_idx": 112, "bbox": [72, 392, 187, 409], "page_size": [612.0, 792.0]}
{"layout": 935, "type": "text", "text": "It is dict or list[dict], and contains the following keys and values: ", "page_idx": 112, "bbox": [71, 424.5654602050781, 329.1147155761719, 437.87548828125], "page_size": [612.0, 792.0]}
{"layout": 936, "type": "text", "text": "•  type  (str), containing the initialize r name in  INTI ALIZ ERS , and followed by arguments of the initialize r. •  layer  (str or list[str]), containing the names of basic layers in Pytorch or MMCV with learnable parameters that will be initialized, e.g.  ' Conv2d ' , ' Deform Con v 2 d ' . •  override  (dict or list[dict]), containing the sub-modules that not inherit from BaseModule and whose initializa- tion configuration is different from other layers’ which are in  ' layer '  key. Initialize r defined in  type  will work for all layers defined in  layer , so if sub-modules are not derived Classes of  BaseModule  but can be initialized as same ways of layers in  layer , it does not need to use  override .  override  contains: –  type  followed by arguments of initialize r; –  name  to indicate sub-module which will be initialized. ", "page_idx": 112, "bbox": [88, 442.49847412109375, 540, 575.9572143554688], "page_size": [612.0, 792.0]}
{"layout": 937, "type": "text", "text": "17.2 Initialize parameters ", "text_level": 1, "page_idx": 112, "bbox": [72, 599, 246, 614], "page_size": [612.0, 792.0]}
{"layout": 938, "type": "text", "text": "Inherit a new model from  mmcv.runner.BaseModule  or  mmdet.models  Here we show an example of FooModel. ", "page_idx": 112, "bbox": [71, 629.2354125976562, 531.4761962890625, 642.5454711914062], "page_size": [612.0, 792.0]}
{"layout": 939, "type": "table", "page_idx": 113, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_104.jpg", "bbox": [69, 82, 543, 151], "page_size": [612.0, 792.0], "ocr_text": "argl,\narg2,\ninit_cfg=None):\nsuper(FooModel, self).__init__(Cinit_cfg)\n\n", "vlm_text": "The image you provided does not contain a table. Instead, it shows a snippet of Python code. The code appears to be part of a class definition where the `__init__` method is being defined. The method takes arguments `arg1`, `arg2`, and `init_cfg` (with a default value of `None`). Within the `__init__` method, the `super()` function is used to call the `__init__` method of the superclass, passing `init_cfg` to it. The class name appears to be `FooModel`."}
{"layout": 940, "type": "text", "text": "• Initialize model by using  init_cfg  directly in code ", "page_idx": 113, "bbox": [88, 158.07948303222656, 304.5072937011719, 171.38951110839844], "page_size": [612.0, 792.0]}
{"layout": 941, "type": "image", "page_idx": 113, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_105.jpg", "bbox": [93, 178, 543, 316], "page_size": [612.0, 792.0], "ocr_text": "import torch.nn as nn\nfrom mmcv.runner import BaseModule\n# or directly inherit mmdet models\n\nclass FooModel (BaseModule)\ndef __init__(self,\nargl,\narg2,\ninit_cfg=XXX):\nsuper(FooModel, self).__init__(init_cfg)\n\n", "vlm_text": "The image shows a snippet of Python code. Here's a breakdown:\n\n1. **Imports**:\n   - `torch.nn as nn`: Imports the neural network module from PyTorch.\n   - `from mmcv.runner import BaseModule`: Imports `BaseModule` from the `mmcv.runner` package.\n   \n2. **Comment**:\n   - `# or directly inherit mmdet models`: This comment suggests an alternative to inheriting the `BaseModule`.\n\n3. **Class Definition**:\n   - `class FooModel(BaseModule)`: Defines a class named `FooModel` that inherits from `BaseModule`.\n\n4. **Initialization Method**:\n   - `def __init__(self, arg1, arg2, init_cfg=XXX):`: Constructor method for initializing the `FooModel` class.\n   - `super(FooModel, self).__init__(init_cfg)`: Calls the constructor of the parent `BaseModule` class with the `init_cfg` parameter.\n\nThe ellipsis (`...`) at the end indicates that more code would follow."}
{"layout": 942, "type": "text", "text": "• Initialize model by using  init_cfg  directly in  mmcv.Sequential  or  mmcv.ModuleList  code ", "page_idx": 113, "bbox": [88, 323.25946044921875, 477.618408203125, 336.5694885253906], "page_size": [612.0, 792.0]}
{"layout": 943, "type": "text", "text": "from  mmcv.runner  import  BaseModule, ModuleList class FooModel(BaseModule)def  __init__ ( self , arg1, arg2, init_cfg = None ): super (FooModel,  self ) . __init__ (init_cfg) ... self . conv1  $=$   ModuleList(init_cfg  $=$  XXX) ", "page_idx": 113, "bbox": [96, 345.70599365234375, 389.807861328125, 463.9120788574219], "page_size": [612.0, 792.0]}
{"layout": 944, "type": "text", "text": "• Initialize model by using  init_cfg  in config file ", "page_idx": 113, "bbox": [88, 476.4854736328125, 293.1499328613281, 489.7955017089844], "page_size": [612.0, 792.0]}
{"layout": 945, "type": "table", "page_idx": 113, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_106.jpg", "bbox": [94, 495, 543, 598], "page_size": [612.0, 792.0], "ocr_text": "model = dict(\n\nmodel = dict(\ntype='FooModel',\narg1=XXXx,\narg2=XXX,\ninit_cfg=XXX),\n", "vlm_text": "The image shows a snippet of code, likely in Python, that defines a nested dictionary configuration for a model. Here are the key components:\n\n- The outer dictionary is assigned to the variable `model`.\n- Inside the outer dictionary, there is another dictionary also assigned to `model` with fields:\n  - `type='FooModel'`: Specifies the model type as \"FooModel\".\n  - `arg1=XXX`: A placeholder for the first argument.\n  - `arg2=XXX`: A placeholder for the second argument.\n  - `init_cfg=XXX`: A placeholder for the model's initialization configuration. \n\nThe `XXX` placeholders imply that specific values need to be filled in."}
{"layout": 946, "type": "text", "text": "17.3 Usage of init_cfg ", "text_level": 1, "page_idx": 114, "bbox": [72, 71, 224, 89], "page_size": [612.0, 792.0]}
{"layout": 947, "type": "text", "text": "1. Initialize model by  layer  key ", "page_idx": 114, "bbox": [84, 102.99446105957031, 216.92686462402344, 116.30449676513672], "page_size": [612.0, 792.0]}
{"layout": 948, "type": "text", "text": "If we only define  layer , it just initialize the layer in  layer  key. ", "page_idx": 114, "bbox": [96, 120.92646789550781, 351.0433349609375, 134.2364959716797], "page_size": [612.0, 792.0]}
{"layout": 949, "type": "text", "text": "NOTE: Value of  layer  key is the class name with attributes weights and bias of Pytorch, (so such as Multi head Attention layer  is not supported).\n\n ", "page_idx": 114, "bbox": [96, 138.8594512939453, 540, 164.12449645996094], "page_size": [612.0, 792.0]}
{"layout": 950, "type": "text", "text": "• Define  layer  key for initializing module with same configuration.\n\n ", "page_idx": 114, "bbox": [88, 168.74745178222656, 361.6031799316406, 182.05747985839844], "page_size": [612.0, 792.0]}
{"layout": 951, "type": "table", "page_idx": 114, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_107.jpg", "bbox": [94, 188, 541, 218], "page_size": [612.0, 792.0], "ocr_text": "init_cfg = dict(type='Constant', layer=['Convid', 'Conv2d', 'Linear'], val=1)\n# initialize whole module with same configuration\n", "vlm_text": "This isn't a table; it's a code snippet. It initializes a configuration dictionary `init_cfg` with a constant type and specifies layers such as 'Conv1d', 'Conv2d', and 'Linear', all initialized to the value 1. Additionally, there's a comment explaining this configuration applies to the whole module."}
{"layout": 952, "type": "text", "text": "• Define  layer  key for initializing layer with different configurations.\n\n ", "page_idx": 114, "bbox": [88, 226.33143615722656, 368.029052734375, 239.64146423339844], "page_size": [612.0, 792.0]}
{"layout": 953, "type": "table", "page_idx": 114, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_108.jpg", "bbox": [69, 246, 429, 323], "page_size": [612.0, 792.0], "ocr_text": "init_cfg = [dict(type='Constant', layer='Convid', val=1),\ndict(type='Constant', layer='Conv2d', val=2),\ndict(type='Constant', layer='Linear', val=3)]\n\n# nn.Convid will be initialized with dict(type='Constant', val=1)\n\n# nn.Conv2d will be initialized with dict(type='Constant', val=2)\n\n# nn.Linear will be initialized with dict(type='Constant', val=3)\n", "vlm_text": "The image is not of a traditional table, but rather a code snippet. This code snippet appears to define the initialization configuration for neural network layers using a list of dictionaries. \n\nEach dictionary specifies a layer type and its corresponding initialization constant value:\n1. The `Conv1d` layer will be initialized with a constant value of 1.\n2. The `Conv2d` layer will be initialized with a constant value of 2.\n3. The `Linear` layer will be initialized with a constant value of 3.\n\nThe snippet indicates that each of these layers (Convolutional layers and Linear layers in a neural network) will use the `Constant` initialization method with specified values when being initialized."}
{"layout": 954, "type": "text", "text": "1. Initialize model by  override  key\n\n ", "page_idx": 114, "bbox": [84, 331.7364807128906, 232.61785888671875, 345.0465087890625], "page_size": [612.0, 792.0]}
{"layout": 955, "type": "text", "text": "• When initializing some specific part with its attribute name, we can use  override  key, and the value in  override will ignore the value in init_cfg. ", "page_idx": 114, "bbox": [88, 349.66949462890625, 540, 374.93450927734375], "page_size": [612.0, 792.0]}
{"layout": 956, "type": "text", "text": " $\\#$   layers # self.feat  $=$   nn.Conv1d(3, 1, 3) # self.reg  $=$   nn.Conv2d(3, 3, 3) # self.cls  $=$   nn.Linear(1,2) init_cfg  $=$   dict ( type  $=\"$  Constant ' , layer  $=$  [ ' Conv1d ' , ' Conv2d ' ], val  $^{=1}$  , bias  ${}=\\!2$  , override  $\\scriptstyle{\\varepsilon}$  dict ( type = ' Constant ' , name  $\\circeq$  ' reg ' , val  $^{=3}$  , bias  ${=}4$  )) # self.feat and self.cls will be initialized with dict(type  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}^{\\dagger}$  Constant ' ,   $V a L\\!=\\!\\!1$  ,  $\\hookrightarrow$  bias=2) → # The module called  ' reg '  will be initialized with dict(type  $\\circeq$  Constant ' ,   $V a l\\!=\\!3$  , bias=4)\n\n ", "page_idx": 114, "bbox": [96, 384, 540, 515.2084350585938], "page_size": [612.0, 792.0]}
{"layout": 957, "type": "text", "text": "• If  layer  is None in init_cfg, only sub-module with the name in override will be initialized, and type and other args in override can be omitted. ", "page_idx": 114, "bbox": [88, 526.804443359375, 540, 552.0704956054688], "page_size": [612.0, 792.0]}
{"layout": 958, "type": "text", "text": "# layers # self.feat  $=$  nn.Conv1d(3, 1, 3) # self.reg  $=$   nn.Conv2d(3, 3, 3) # self.cls  $=$   nn.Linear(1,2) init_cfg  $=$   dict ( type  $=\"$  Constant ' , val  $^{=1}$  , bias  ${}=\\!2$  , override  $=$  dict (name  $\\equiv^{\\dagger}$  reg ' )) # self.feat and self.cls will be initialized by Pytorch # The module called  ' reg '  will be initialized with dict(type= ' Constant ' ,   $V a L\\!=\\!\\!1$  , bias=2)\n\n ", "page_idx": 114, "bbox": [96, 561.8037109375, 540, 668.4335327148438], "page_size": [612.0, 792.0]}
{"layout": 959, "type": "text", "text": "• If we don’t define  layer  key or  override  key, it will not initialize anything.\n\n ", "page_idx": 114, "bbox": [88, 680.0304565429688, 403.7652282714844, 693.3405151367188], "page_size": [612.0, 792.0]}
{"layout": 960, "type": "text", "text": "• Invalid usage ", "page_idx": 114, "bbox": [88, 697.9634399414062, 149, 711.2734985351562], "page_size": [612.0, 792.0]}
{"layout": 961, "type": "table", "page_idx": 115, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_109.jpg", "bbox": [91, 74, 545, 227], "page_size": [612.0, 792.0], "ocr_text": "# It is invalid that override don't have name key\ninit_cfg = dict(type='Constant', layer ['Convid','Conv2d'], val=1, bias=2,\noverride=dict(type='Constant', val=3, bias=4))\n\n# It is also invalid that override has name and other args except type\ninit_cfg = dict(type='Constant', layer ['Convid','Conv2d'], val=1, bias=2,\noverride=dict(name='reg', val=3, bias=4))\n\nInitialize model with the pretrained model\n\ninit_cfg = dict(type='Pretrained',\ncheckpoint='torchvision://resnet50')\n\n", "vlm_text": "The table contains Python code snippets related to model initialization configurations.\n\n1. **Invalid Override Configurations:**\n   - First snippet shows an invalid configuration where `override` is missing a `name` key.\n   ```python\n   init_cfg = dict(type='Constant', layer=['Conv1d', 'Conv2d'], val=1, bias=2,\n                   override=dict(type='Constant', val=3, bias=4))\n   ```\n   - Second snippet shows an invalid configuration where `override` has a `name` key but is missing the `type` key.\n   ```python\n   init_cfg = dict(type='Constant', layer=['Conv1d', 'Conv2d'], val=1, bias=2,\n                   override=dict(name='reg', val=3, bias=4))\n   ```\n\n2. **Pretrained Model Initialization:**\n   - The last snippet demonstrates initializing a model with a pretrained model from `torchvision`.\n   ```python\n   init_cfg = dict(type='Pretrained', checkpoint='torchvision://resnet50')\n   ```"}
{"layout": 962, "type": "text", "text": "More details can refer to the documentation in  MMCV  and MMCV  PR #780 ", "page_idx": 115, "bbox": [72, 231.4524688720703, 378.25030517578125, 244.7624969482422], "page_size": [612.0, 792.0]}
{"layout": 963, "type": "text", "text": "Apart from training/testing scripts, We provide lots of useful tools under the  tools/  directory. ", "page_idx": 115, "bbox": [72, 249.3844757080078, 449.43463134765625, 262.69451904296875], "page_size": [612.0, 792.0]}
{"layout": 964, "type": "text", "text": "LOG ANALYSIS ", "text_level": 1, "page_idx": 116, "bbox": [434, 164, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 965, "type": "text", "text": "tools/analysis tools/analyze logs.py  plots loss/mAP curves given a training log file. Run  pip install seaborn  first to install the dependency. ", "page_idx": 116, "bbox": [72, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 966, "type": "text", "text": "python tools/analysis tools/analyze logs.py plot_curve  [ --keys  \\${ KEYS } ] [ --title  \\${ TITLE } ] [ --legend  \\${ LEGEND } ] [ --backend  \\${ BACKEND } ] [ --style  \\${ STYLE } ] [ --out  \\${ OUT_FILE } ] ˓ → ", "page_idx": 116, "bbox": [72, 260.8717041015625, 540, 285.71832275390625], "page_size": [612.0, 792.0]}
{"layout": 967, "type": "image", "page_idx": 116, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_110.jpg", "bbox": [74, 298, 478, 607], "page_size": [612.0, 792.0], "ocr_text": "1.0\n\n10000\n\n20000\niter\n\n— loss cls\n— loss_bbox\n\n", "vlm_text": "The image is a line graph depicting two types of loss values over iterations named \"loss_cls\" and \"loss_bbox.\" \n\n- The x-axis represents the iterations, ranging from 0 to over 40,000.\n- The y-axis represents the loss value, ranging from 0.0 to 1.0.\n- Two lines are plotted: \n  - \"loss_cls\" (classification loss) is shown in blue.\n  - \"loss_bbox\" (bounding box loss) is shown in orange.\n\nBoth lines show a decreasing trend, indicating a reduction in loss over time."}
{"layout": 968, "type": "text", "text": "age ", "page_idx": 116, "bbox": [72, 612.9074096679688, 85.57902526855469, 626.2174682617188], "page_size": [612.0, 792.0]}
{"layout": 969, "type": "text", "text": "• Plot the classification loss of some run. ", "page_idx": 116, "bbox": [88, 648.7734375, 252.91136169433594, 662.08349609375], "page_size": [612.0, 792.0]}
{"layout": 970, "type": "text", "text": "python tools/analysis tools/analyze logs.py plot_curve log.json --keys loss_cls -- legend loss_cls ˓ → ", "page_idx": 116, "bbox": [96, 671.8167114257812, 525, 696.663330078125], "page_size": [612.0, 792.0]}
{"layout": 971, "type": "text", "text": "• Plot the classification and regression loss of some run, and save the figure to a pdf. ", "page_idx": 116, "bbox": [88, 706.357421875, 425.56329345703125, 719.66748046875], "page_size": [612.0, 792.0]}
{"layout": 972, "type": "table", "page_idx": 117, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_111.jpg", "bbox": [94, 77, 539, 104], "page_size": [612.0, 792.0], "ocr_text": "python tools/analysis_tools/analyze_logs.py plot_curve log.json --keys loss_cls..\n~loss_bbox --out losses.pdf\n", "vlm_text": "The image shows a command being entered into a console or terminal. This command appears to be for analyzing logs and plotting metrics from a JSON log file using a Python script. Here is a breakdown of the command:\n\n- `python tools/analysis_tools/analyze_logs.py plot_curve log.json`:\n  - This part of the command is calling a Python script named `analyze_logs.py`, which is located in the `tools/analysis_tools/` directory.\n  - The script is being executed to perform a function named `plot_curve`.\n  - The input to the script is a JSON file named `log.json`, which likely contains the log data.\n\n- `--keys loss_cls loss_bbox`:\n  - These are additional arguments passed to the script, specifying the keys `loss_cls` and `loss_bbox`, which could refer to class loss and bounding box loss metrics, respectively.\n\n- `--out losses.pdf`:\n  - This part of the command specifies an output file named `losses.pdf`, where the plotted curves are likely to be saved.\n\nThis command is used for analyzing and plotting specified loss metrics from log data during a model training process or similar task."}
{"layout": 973, "type": "text", "text": "• Compare the bbox mAP of two runs in the same figure.\n\n ", "page_idx": 117, "bbox": [88, 114.09248352050781, 316.950927734375, 127.40251922607422], "page_size": [612.0, 792.0]}
{"layout": 974, "type": "table", "page_idx": 117, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_112.jpg", "bbox": [94, 134, 539, 163], "page_size": [612.0, 792.0], "ocr_text": "python tools/analysis_tools/analyze_logs.py plot_curve log1l.json log2.json --keys.\n~bbox_mAP --legend runl run2\n", "vlm_text": "This is not a table; it's a command line input for a Python script used to analyze logs. The command appears to be for generating a plot of some performance metrics. Here's a breakdown:\n\n- `python tools/analysis_tools/analyze_logs.py`: Runs a Python script located at this path.\n- `plot_curve`: Likely an argument to specify the operation to perform (e.g., plotting a curve).\n- `log1.json log2.json`: These are JSON files containing logs to be analyzed.\n- `--keys bbox_mAP`: Specifies which metrics or keys to plot, in this case, \"bbox_mAP,\" which likely refers to bounding box mean Average Precision.\n- `--legend run1 run2`: Adds a legend to the plot with labels \"run1\" and \"run2\"."}
{"layout": 975, "type": "text", "text": "• Compute the average training speed. ", "page_idx": 117, "bbox": [88, 171.6764678955078, 241.822998046875, 184.9864959716797], "page_size": [612.0, 792.0]}
{"layout": 976, "type": "image", "page_idx": 117, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_113.jpg", "bbox": [94, 192, 538, 221], "page_size": [612.0, 792.0], "ocr_text": "python tools/analysis_tools/analyze_logs.py cal_train_time log.json [--include-\noutliers]\n", "vlm_text": "The image shows a command line instruction:\n\n```\npython tools/analysis_tools/analyze_logs.py cal_train_time log.json [--include-outliers]\n```\n\nThis command appears to run a Python script named `analyze_logs.py` located in the `tools/analysis_tools` directory. The script is likely used to calculate training time based on a log file named `log.json`. The optional parameter `--include-outliers` may be used to include outlier data in the analysis."}
{"layout": 977, "type": "text", "text": "The output is expected to be like the following. ", "page_idx": 117, "bbox": [96, 229.2604522705078, 284.0544128417969, 242.5704803466797], "page_size": [612.0, 792.0]}
{"layout": 978, "type": "text", "text": "-----Analyze train time of work_dirs/some_exp/20190611 192040.log.json----- slowest epoch 11, average time is 1.2024 fastest epoch 1, average time is 1.1909 time std over epochs is 0.0028 average iter time: 1.1959 s/iter ", "page_idx": 117, "bbox": [96, 252.30372619628906, 489.184814453125, 310.1371154785156], "page_size": [612.0, 792.0]}
{"layout": 979, "type": "text", "text": "RESULT ANALYSIS ", "text_level": 1, "page_idx": 118, "bbox": [408, 163, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 980, "type": "text", "text": "tools/analysis tools/analyze results.py  calculates single image mAP and saves or shows the topk images with the highest and lowest scores based on prediction results. ", "page_idx": 118, "bbox": [72, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 981, "type": "text", "text": "Usage ", "text_level": 1, "page_idx": 118, "bbox": [71, 258, 98, 269], "page_size": [612.0, 792.0]}
{"layout": 982, "type": "text", "text": "python tools/analysis tools/analyze results.py  \\ \\${ CONFIG }  \\ \\${ PREDICTION PATH }  \\ \\${ SHOW_DIR }  \\ [ --show ]  \\ [ --wait-time  \\${ WAIT_TIME } ]  \\ [ --topk  \\${ TOPK } ]  \\ [ --show-score-thr  \\${ SHOW SCORE THR } ]  \\ [ --cfg-options  \\${ CF G OPTIONS } ] ", "page_idx": 118, "bbox": [72, 278.20697021484375, 323.057373046875, 384.4580383300781], "page_size": [612.0, 792.0]}
{"layout": 983, "type": "text", "text": "Description of all arguments: ", "page_idx": 118, "bbox": [72, 397.0314636230469, 188.54249572753906, 410.34149169921875], "page_size": [612.0, 792.0]}
{"layout": 984, "type": "text", "text": "•  config  $:$   The path of a model config file. •  prediction path : Output result file in pickle format from  tools/test.py •  show_dir : Directory where painted GT and detection images will be saved •  --show Determines whether to show painted images, If not specified, it will be set to  False •  --wait-time : The interval of show (s), 0 is block •  --topk : The number of saved images that have the highest and lowest  topk  scores after sorting. If not specified, it will be set to  20 . •  --show-score-thr : Show score threshold. If not specified, it will be set to  0 . •  --cfg-options : If specified, the key-value pair optional cfg will be merged into config file ", "page_idx": 118, "bbox": [88, 414.9634704589844, 540, 565.758544921875], "page_size": [612.0, 792.0]}
{"layout": 985, "type": "text", "text": "Examples : ", "text_level": 1, "page_idx": 118, "bbox": [71, 572, 116, 584], "page_size": [612.0, 792.0]}
{"layout": 986, "type": "text", "text": "Assume that you have got result file in pickle format from  tools/test.py  in the path ‘./result.pkl’. 1. Test Faster R-CNN and visualize the results, save images to the directory  results/ ", "page_idx": 118, "bbox": [72, 588.3134765625, 470.6144714355469, 619.5565185546875], "page_size": [612.0, 792.0]}
{"layout": 987, "type": "table", "page_idx": 118, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_114.jpg", "bbox": [69, 623, 543, 692], "page_size": [612.0, 792.0], "ocr_text": "python tools/analysis_tools/analyze_results.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\nresult.pkl \\\nresults \\\n--show\n", "vlm_text": "The table contains a command-line script for running a Python analysis tool. The script is broken down with backslashes for continuation:\n\n- `python tools/analysis_tools/analyze_results.py`: Executes a Python script for analyzing results.\n- `configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py`: Specifies the configuration file for the Faster R-CNN model.\n- `result.pkl`: Refers to a pickle file containing the results to analyze.\n- `results`: Specifies the directory to output or save results.\n- `--show`: Indicates that results will be visually displayed.\n\nThis appears to be related to analyzing results from a machine learning model."}
{"layout": 988, "type": "text", "text": "1. Test Faster R-CNN and specified topk to 50, save images to the directory  results/ ", "page_idx": 118, "bbox": [84, 699.6964721679688, 430, 713.0065307617188], "page_size": [612.0, 792.0]}
{"layout": 989, "type": "image", "page_idx": 119, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_115.jpg", "bbox": [68, 76, 543, 143], "page_size": [612.0, 792.0], "ocr_text": "python tools/analysis_tools/analyze_results.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\nresult.pkl \\\nresults \\\n--topk 50\n\n", "vlm_text": "The image displays a command used for analyzing results, likely in a machine learning or computer vision context. \n\nHere's a breakdown of the command:\n\n- `python`: This indicates that a Python script is being run.\n- `tools/analysis_tools/analyze_results.py`: This is the script being executed.\n- `\\`: These backslashes allow the command to be split across multiple lines.\n- `configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py`: This specifies a configuration file, possibly for a Faster R-CNN model.\n- `result.pkl`: This is likely the name of a file containing results to be analyzed.\n- `results`: This might be an output directory or a file where analyzed results will be stored.\n- `--topk 50`: This is an option that specifies the top 50 results to consider for the analysis."}
{"layout": 990, "type": "text", "text": "1. If you want to filter the low score prediction results, you can specify the  show-score-thr  parameter ", "page_idx": 119, "bbox": [84.4530029296875, 149.9574737548828, 500.2834777832031, 163.2675018310547], "page_size": [612.0, 792.0]}
{"layout": 991, "type": "table", "page_idx": 119, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_116.jpg", "bbox": [67, 169, 544, 235], "page_size": [612.0, 792.0], "ocr_text": "python tools/analysis_tools/analyze_results.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\nresult.pkl \\\nresults \\\n--show-score-thr 0.3\n", "vlm_text": "The table contains a command for running a Python script. Here's a breakdown:\n\n- `python tools/analysis_tools/analyze_results.py`: This is the command to execute a Python script for analyzing results.\n- `configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py`: This specifies the configuration file for the analysis, using a Faster R-CNN model.\n- `result.pkl`: This is likely the results file in a pickle format that the script will analyze.\n- `results`: This might specify the output directory or file.\n- `--show-score-thr 0.3`: This is an option to set the score threshold to 0.3, filtering the results.\n\nThis appears to be related to machine learning, possibly for object detection or similar tasks."}
{"layout": 992, "type": "text", "text": "VISUALIZATION ", "text_level": 1, "page_idx": 120, "bbox": [431, 164, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 993, "type": "text", "text": "20.1 Visualize Datasets ", "text_level": 1, "page_idx": 120, "bbox": [71, 229, 236, 245], "page_size": [612.0, 792.0]}
{"layout": 994, "type": "text", "text": "tools/misc/browse data set.py  helps the user to browse a detection dataset (both images and bounding box an- notations) visually, or save the image to a designated directory. ", "page_idx": 120, "bbox": [71, 260.8024597167969, 540, 286.0674743652344], "page_size": [612.0, 792.0]}
{"layout": 995, "type": "text", "text": "python tools/misc/browse data set.py  \\${ CONFIG }  [ -h ] [ --skip-type  \\${ SKIP_TYPE [SKIP_TYPE...  $\\scriptscriptstyle\\hookrightarrow]\\mathcal{Y}]$  ] [ --output-dir  \\${ OUTPUT_DIR } ] [ --not-show ] [ --show-interval  \\${ SHOW INTERVAL } ] ", "page_idx": 120, "bbox": [71, 295.80072021484375, 532.8451538085938, 318], "page_size": [612.0, 792.0]}
{"layout": 996, "type": "text", "text": "20.2 Visualize Models ", "text_level": 1, "page_idx": 120, "bbox": [70, 348, 226, 365], "page_size": [612.0, 792.0]}
{"layout": 997, "type": "text", "text": "First, convert the model to ONNX as described  here . Note that currently only RetinaNet is supported, support for other models will be coming in later versions. The converted model could be visualized by tools like  Netron . ", "page_idx": 120, "bbox": [71, 380.1310119628906, 540, 405.54547119140625], "page_size": [612.0, 792.0]}
{"layout": 998, "type": "text", "text": "20.3 Visualize Predictions ", "text_level": 1, "page_idx": 120, "bbox": [71, 428, 252, 444], "page_size": [612.0, 792.0]}
{"layout": 999, "type": "text", "text": "If you need a lightweight GUI for visualizing the detection results, you can refer  DetVisGUI project . ", "page_idx": 120, "bbox": [71, 460.107421875, 471.42047119140625, 473.4174499511719], "page_size": [612.0, 792.0]}
{"layout": 1000, "type": "text", "text": "ERROR ANALYSIS ", "text_level": 1, "page_idx": 122, "bbox": [413, 164, 541, 180], "page_size": [612.0, 792.0]}
{"layout": 1001, "type": "text", "text": "tools/analysis tools/coco error analysis.py  analyzes COCO results per category and by different crite- rion. It can also make a plot to provide useful information. ", "page_idx": 122, "bbox": [72, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 1002, "type": "text", "text": "python tools/analysis tools/coco error analysis.py  \\${ RESULT } \\${ OUT_DIR }  [ -h ] [ --ann  \\$ { ANN } ] [ --types  \\${ TYPES [TYPES...] } ] ˓ → ", "page_idx": 122, "bbox": [72, 260.8717041015625, 519, 285.71832275390625], "page_size": [612.0, 792.0]}
{"layout": 1003, "type": "text", "text": "Example: ", "page_idx": 122, "bbox": [72, 295.4124755859375, 110.0870132446289, 308.7225036621094], "page_size": [612.0, 792.0]}
{"layout": 1004, "type": "text", "text": "Assume that you have got  Mask R-CNN checkpoint file  in the path ‘checkpoint’. For other checkpoints, please refer to our  model zoo . You can use the following command to get the results bbox and segmentation json file. ", "page_idx": 122, "bbox": [72, 313.3454895019531, 540, 338.6105041503906], "page_size": [612.0, 792.0]}
{"layout": 1005, "type": "text", "text": "# out: results.bbox.json and results.segm.json python tools/test.py  \\ configs/mask_rcnn/mask r cnn r 50 fp n 1 x coco.py  \\ checkpoint/mask r cnn r 50 fp n 1 x coco 20200205-d4b0c5d6.pth  \\ --format-only  \\ --options  \"json file prefix  $\\leftleftarrows$  ./results\" ", "page_idx": 122, "bbox": [72, 348.3437194824219, 422.4353942871094, 418.132080078125], "page_size": [612.0, 792.0]}
{"layout": 1006, "type": "text", "text": "1. Get COCO bbox error results per category , save analyze result images to the directory  results/ ", "page_idx": 122, "bbox": [82, 430.7044677734375, 486.34600830078125, 444.0144958496094], "page_size": [612.0, 792.0]}
{"layout": 1007, "type": "text", "text": "python tools/analysis tools/coco error analysis.py  \\ results.bbox.json  \\ results  \\ --ann = data/coco/annotations/instances val 2017.json  \\ ", "page_idx": 122, "bbox": [72, 453.1509704589844, 380, 499.92498779296875], "page_size": [612.0, 792.0]}
{"layout": 1008, "type": "text", "text": "1. Get COCO segmentation error results per category , save analyze result images to the directory  results/ ", "page_idx": 122, "bbox": [82, 512.1994018554688, 519, 525.5094604492188], "page_size": [612.0, 792.0]}
{"layout": 1009, "type": "text", "text": "python tools/analysis tools/coco error analysis.py  \\ results.segm.json  \\ results  \\ --ann  $\\risingdotseq$  data/coco/annotations/instances val 2017.json  \\ --types  $\\equiv^{\\dagger}$  segm ' ", "page_idx": 122, "bbox": [72, 534.64501953125, 380, 593.102783203125], "page_size": [612.0, 792.0]}
{"layout": 1010, "type": "text", "text": "MODEL SERVING ", "text_level": 1, "page_idx": 124, "bbox": [418, 164, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 1011, "type": "text", "text": "In order to serve an  MM Detection  model with  TorchServe , you can follow the steps: ", "page_idx": 124, "bbox": [71, 225.87342834472656, 415.16217041015625, 239.18345642089844], "page_size": [612.0, 792.0]}
{"layout": 1012, "type": "text", "text": "22.1 1. Convert model from MM Detection to TorchServe ", "text_level": 1, "page_idx": 124, "bbox": [70, 261, 455, 278], "page_size": [612.0, 792.0]}
{"layout": 1013, "type": "text", "text": "python tools/deployment/mm det 2 torch serve.py  \\${ CONFIG FILE }   $\\mathcal{S}$  { CHECKPOINT FILE }  \\\n\n --output-folder  \\${ MODEL STORE }  \\\n\n --model-name \\${MODEL_NAME}", "page_idx": 124, "bbox": [71, 297.10198974609375, 485.8963317871094, 331.62213134765625], "page_size": [612.0, 792.0]}
{"layout": 1014, "type": "text", "text": "Note : \\${MODEL STORE} needs to be an absolute path to a folder. ", "page_idx": 124, "bbox": [71, 343.56781005859375, 343, 358.1032409667969], "page_size": [612.0, 792.0]}
{"layout": 1015, "type": "text", "text": "22.2 2. Build  mmdet-serve  docker image ", "page_idx": 124, "bbox": [71, 377.48651123046875, 343, 398.0302734375], "page_size": [612.0, 792.0]}
{"layout": 1016, "type": "text", "text": "docker build -t mmdet-serve:latest docker/serve/ ", "page_idx": 124, "bbox": [71, 416.021728515625, 323.05755615234375, 426.0341491699219], "page_size": [612.0, 792.0]}
{"layout": 1017, "type": "text", "text": "22.3 3. Run  mmdet-serve ", "text_level": 1, "page_idx": 124, "bbox": [71, 456, 241, 473], "page_size": [612.0, 792.0]}
{"layout": 1018, "type": "text", "text": "Check the official docs for  running TorchServe with docker . ", "page_idx": 124, "bbox": [71, 488.5464782714844, 309.8271179199219, 501.85650634765625], "page_size": [612.0, 792.0]}
{"layout": 1019, "type": "text", "text": "In order to run in GPU, you need to install  nvidia-docker . You can omit the  --gpus  argument in order to run in CPU. ", "page_idx": 124, "bbox": [71, 506.4794616699219, 539.0775756835938, 519.7894897460938], "page_size": [612.0, 792.0]}
{"layout": 1020, "type": "text", "text": "Example: ", "page_idx": 124, "bbox": [71, 524.4114379882812, 110.08704376220703, 537.7214965820312], "page_size": [612.0, 792.0]}
{"layout": 1021, "type": "text", "text": "docker run --rm  \\\n\n --cpus  8  \\\n\n --gpus  device = 0  \\\n\n -p8080:8080 -p8081:8081 -p8082:8082  \\\n\n --mount  type = bind,source  $=$  \\$MODEL STORE ,target  $\\overline{{\\overline{{\\mathbf{\\alpha}}}}}.$  /home/model-server/model-store  \\ mmdet-serve:latest ", "page_idx": 124, "bbox": [71, 546.8580322265625, 474.7383728027344, 617.2442016601562], "page_size": [612.0, 792.0]}
{"layout": 1022, "type": "text", "text": "Read the docs  about the Inference (8080), Management (8081) and Metrics (8082) APis ", "page_idx": 124, "bbox": [71, 629.8164672851562, 422.4543151855469, 643.1265258789062], "page_size": [612.0, 792.0]}
{"layout": 1023, "type": "text", "text": "22.4 4. Test deployment ", "text_level": 1, "page_idx": 125, "bbox": [71, 72, 238, 88], "page_size": [612.0, 792.0]}
{"layout": 1024, "type": "text", "text": "curl -O curl -O https://raw.g it hub user content.com/pytorch/serve/master/docs/images/3dogs.\n\n  $\\hookrightarrow$  jpg\n\n → curl http://127.0.0.1:8080/predictions/ \\${ MODEL_NAME }  -T 3dogs.jpg ", "page_idx": 125, "bbox": [71, 106.84376525878906, 540, 140.76718139648438], "page_size": [612.0, 792.0]}
{"layout": 1025, "type": "text", "text": "You should obtain a response similar to:\n\n ", "page_idx": 125, "bbox": [71, 153.3394317626953, 234, 166.6494598388672], "page_size": [612.0, 792.0]}
{"layout": 1026, "type": "text", "text": "[ { \"class_name\" :  \"dog\" , \"bbox\" : [ 294.63409423828125 , 203.99111938476562 , 417.048583984375 , 281.62744140625 ], \"score\" :  0.9987992644309998 }, { \"class_name\" :  \"dog\" , \"bbox\" : [ 404.26019287109375 , 126.0080795288086 , 574.5091552734375 , 293.6662292480469 ], \"score\" :  0.9979367256164551 }, { \"class_name\" :  \"dog\" , \"bbox\" : [ 197.2144775390625 , 93.3067855834961 , 307.8505554199219 , 276.7560119628906 ], \"score\" :  0.993338406085968 }\n\n ] ", "page_idx": 125, "bbox": [71, 176.27772521972656, 234, 556.9009399414062], "page_size": [612.0, 792.0]}
{"layout": 1027, "type": "text", "text": "And you can use  test torch server.py  to compare result of torch server and pytorch, and visualize them. ", "page_idx": 125, "bbox": [71, 569.4734497070312, 501.219482421875, 582.7835083007812], "page_size": [612.0, 792.0]}
{"layout": 1028, "type": "text", "text": "python tools/deployment/test torch server.py  \\${ IMAGE_FILE } \\${ CONFIG FILE } \\${ CHECKPOINT\n\n  $\\hookrightarrow$  FILE } \\${ MODEL_NAME }\n\n →\n\n [ --inference-addr  \\${ INFERENCE A DDR } ] [ --device  \\${ DEVICE } ] [ --score-thr  \\${ SCORE_THR } ] ", "page_idx": 125, "bbox": [71, 592.4117431640625, 522.7330322265625, 626.3351440429688], "page_size": [612.0, 792.0]}
{"layout": 1029, "type": "text", "text": "Example: ", "page_idx": 125, "bbox": [71, 638.9074096679688, 110.0870132446289, 652.2174682617188], "page_size": [612.0, 792.0]}
{"layout": 1030, "type": "text", "text": "python tools/deployment/test torch server.py  \\ demo/demo.jpg  \\ configs/yolo/yo lov 3 d 53 320 273 e coco.py  \\ checkpoint/yo lov 3 d 53 320 273 e coco-421362b6.pth  \\ ", "page_idx": 125, "bbox": [71, 661.248046875, 333.5183410644531, 708.0230102539062], "page_size": [612.0, 792.0]}
{"layout": 1031, "type": "text", "text": "(continues on next page) ", "page_idx": 125, "bbox": [461.9010009765625, 709.8545532226562, 540, 720.5026245117188], "page_size": [612.0, 792.0]}
{"layout": 1032, "type": "text", "text": "MODEL COMPLEXITY ", "text_level": 1, "page_idx": 128, "bbox": [390, 163, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 1033, "type": "text", "text": "tools/analysis tools/get_flops.py  is a script adapted from  flops-counter.pytorch  to compute the FLOPs and params of a given model. ", "page_idx": 128, "bbox": [72, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 1034, "type": "text", "text": "python tools/analysis tools/get_flops.py  \\${ CONFIG FILE }  [ --shape  \\${ INPUT SHAPE } ] ", "page_idx": 128, "bbox": [72, 260.8717041015625, 491.12738037109375, 270.8841247558594], "page_size": [612.0, 792.0]}
{"layout": 1035, "type": "text", "text": "You will get the results like this. ", "page_idx": 128, "bbox": [72, 283.45745849609375, 200.2983856201172, 296.7674865722656], "page_size": [612.0, 792.0]}
{"layout": 1036, "type": "text", "text": "============================== Input shape: (3, 1280, 800) Flops: 239.32 GFLOPs Params: 37.74 M ============================== ", "page_idx": 128, "bbox": [72, 306.500732421875, 228, 364.3341064453125], "page_size": [612.0, 792.0]}
{"layout": 1037, "type": "text", "text": "Note : This tool is still experimental and we do not guarantee that the number is absolutely correct. You may well use the result for simple comparisons, but double check it before you adopt it in technical reports or papers. ", "page_idx": 128, "bbox": [72, 376.27880859375, 540, 402.1714782714844], "page_size": [612.0, 792.0]}
{"layout": 1038, "type": "text", "text": "1. FLOPs are related to the input shape while parameters are not. The default input shape is (1, 3, 1280, 800). ", "page_idx": 128, "bbox": [84, 406.7944641113281, 522.7377319335938, 420.1044921875], "page_size": [612.0, 792.0]}
{"layout": 1039, "type": "text", "text": "2. Some operators are not counted into FLOPs like GN and custom operators. Refer to  mmcv.cnn. get model complexity info()  for details. ", "page_idx": 128, "bbox": [84, 424.72747802734375, 540, 449.99249267578125], "page_size": [612.0, 792.0]}
{"layout": 1040, "type": "text", "text": "3. The FLOPs of two-stage detectors is dependent on the number of proposals. ", "page_idx": 128, "bbox": [84, 454.615478515625, 399.42059326171875, 467.9255065917969], "page_size": [612.0, 792.0]}
{"layout": 1041, "type": "text", "text": "MODEL CONVERSION ", "text_level": 1, "page_idx": 130, "bbox": [386, 164, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 1042, "type": "text", "text": "24.1 MM Detection model to ONNX (experimental) ", "text_level": 1, "page_idx": 130, "bbox": [71, 227, 409, 246], "page_size": [612.0, 792.0]}
{"layout": 1043, "type": "text", "text": "We provide a script to convert model to  ONNX  format. We also support comparing the output results between Pytorch and ONNX model for verification. ", "page_idx": 130, "bbox": [72, 260.8024597167969, 540, 286.0674743652344], "page_size": [612.0, 792.0]}
{"layout": 1044, "type": "text", "text": "python tools/deployment/py torch 2 on nx.py  \\${ CONFIG FILE } \\${ CHECKPOINT FILE }  --output file    $\\mathcal{S}\n\n$   $\\hookrightarrow$  { ONNX_FILE }  [ --shape  \\${ INPUT SHAPE }  --verify ]\n\n → ", "page_idx": 130, "bbox": [72, 295.80072021484375, 540, 320.6483459472656], "page_size": [612.0, 792.0]}
{"layout": 1045, "type": "text", "text": "Note : This tool is still experimental. Some customized operators are not supported for now. For a detailed description of the usage and the list of supported models, please refer to  py torch 2 on nx . ", "page_idx": 130, "bbox": [72, 329.71380615234375, 540, 355.6064758300781], "page_size": [612.0, 792.0]}
{"layout": 1046, "type": "text", "text": "24.2 MM Detection 1.x model to MM Detection 2.x ", "text_level": 1, "page_idx": 130, "bbox": [70, 377, 402, 394], "page_size": [612.0, 792.0]}
{"layout": 1047, "type": "text", "text": "tools/model converters/upgrade model version.py  upgrades a previous MM Detection checkpoint to the new version. Note that this script is not guaranteed to work as some breaking changes are introduced in the new version. It is recommended to directly use the new checkpoints. ", "page_idx": 130, "bbox": [72, 410.16845703125, 540, 447.3884582519531], "page_size": [612.0, 792.0]}
{"layout": 1048, "type": "text", "text": "python tools/model converters/upgrade model version.py  \\${ IN_FILE } \\${ OUT_FILE }  [ -h ] [ --\n\n  $\\hookrightarrow$  num-classes NUM CLASSES ]\n\n → ", "page_idx": 130, "bbox": [72, 457.12274169921875, 522.5089111328125, 481.9693603515625], "page_size": [612.0, 792.0]}
{"layout": 1049, "type": "text", "text": "24.3 RegNet model to MM Detection ", "text_level": 1, "page_idx": 130, "bbox": [70, 509, 316, 526], "page_size": [612.0, 792.0]}
{"layout": 1050, "type": "text", "text": "tools/model converters/reg net 2 mm det.py  convert keys in pycls pretrained RegNet models to MM Detection style. ", "page_idx": 130, "bbox": [72, 541.6014404296875, 540, 566.8665161132812], "page_size": [612.0, 792.0]}
{"layout": 1051, "type": "text", "text": "python tools/model converters/reg net 2 mm det.py \\${SRC} \\${DST} [-h]", "page_idx": 130, "bbox": [72, 576.6007080078125, 407.4414978027344, 586.6131591796875], "page_size": [612.0, 792.0]}
{"layout": 1052, "type": "text", "text": "24.4 Detectron ResNet to Pytorch ", "text_level": 1, "page_idx": 131, "bbox": [70, 71, 304, 88], "page_size": [612.0, 792.0]}
{"layout": 1053, "type": "text", "text": "tools/model converters/detect ron 2 py torch.py  converts keys in the original detectron pretrained ResNet models to PyTorch style. ", "page_idx": 131, "bbox": [72, 102.99446105957031, 539.9984741210938, 128.25950622558594], "page_size": [612.0, 792.0]}
{"layout": 1054, "type": "text", "text": "python tools/model converters/detect ron 2 py torch.py  \\${ SRC } \\${ DST } \\${ DEPTH }  [ -h ] ", "page_idx": 131, "bbox": [72, 137.9927520751953, 481.0144958496094, 148.00515747070312], "page_size": [612.0, 792.0]}
{"layout": 1055, "type": "text", "text": "24.5 Prepare a model for publishing ", "text_level": 1, "page_idx": 131, "bbox": [70, 179, 319, 196], "page_size": [612.0, 792.0]}
{"layout": 1056, "type": "text", "text": "tools/model converters/publish model.py  helps users to prepare their model for publishing. ", "page_idx": 131, "bbox": [72, 210.51747131347656, 473.9809875488281, 223.82749938964844], "page_size": [612.0, 792.0]}
{"layout": 1057, "type": "text", "text": "Before you upload a model to AWS, you may want to ", "page_idx": 131, "bbox": [72, 228.44947814941406, 283.7151184082031, 241.75950622558594], "page_size": [612.0, 792.0]}
{"layout": 1058, "type": "text", "text": "1. convert model weights to CPU tensors 2. delete the optimizer states and 3. compute the hash of the checkpoint file and append the hash id to the filename. ", "page_idx": 131, "bbox": [82, 246.38246154785156, 411.16650390625, 295.5585021972656], "page_size": [612.0, 792.0]}
{"layout": 1059, "type": "text", "text": "python tools/model converters/publish model.py  \\${ INPUT FILENAME } \\${ OUTPUT FILENAME } ", "page_idx": 131, "bbox": [72, 305.2917175292969, 506.6933898925781, 315.30413818359375], "page_size": [612.0, 792.0]}
{"layout": 1060, "type": "text", "text": "python tools/model converters/publish model.py work_dirs/faster r cnn/latest.pth faster_ rcnn_r50_fpn_1x_20190801.pth ˓ → ", "page_idx": 131, "bbox": [72, 350.92071533203125, 527.0421752929688, 375.767333984375], "page_size": [612.0, 792.0]}
{"layout": 1061, "type": "text", "text": "The final output filename will be  faster r cnn r 50 fp n 1 x 20190801-{hash id}.pth . ", "page_idx": 131, "bbox": [72, 385.4614562988281, 442.7686767578125, 398.771484375], "page_size": [612.0, 792.0]}
{"layout": 1062, "type": "text", "text": "DATASET CONVERSION ", "text_level": 1, "page_idx": 132, "bbox": [374, 163, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 1063, "type": "text", "text": "tools/data converters/  contains tools to convert the Cityscapes dataset and Pascal VOC dataset to the COCO format. ", "page_idx": 132, "bbox": [71, 225.87342834472656, 539.9982299804688, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 1064, "type": "text", "text": "python tools/data set converters/cityscapes.py  \\${ CITYSCAPE S PATH }  [ -h ] [ --img-dir  \\${ IMG_ DIR } ] [ --gt-dir  \\${ GT_DIR } ] [ -o  \\${ OUT_DIR } ] [ --nproc  \\${ NPROC } ] ˓ → python tools/data set converters/pascal_voc.py  \\${ DEV KIT PATH }  [ -h ] [ -o  \\${ OUT_DIR } ] ", "page_idx": 132, "bbox": [71, 260.8717041015625, 527.6154174804688, 294.7940979003906], "page_size": [612.0, 792.0]}
{"layout": 1065, "type": "text", "text": "BENCHMARK ", "text_level": 1, "page_idx": 134, "bbox": [445, 164, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 1066, "type": "text", "text": "26.1 Robust Detection Benchmark ", "text_level": 1, "page_idx": 134, "bbox": [70, 228, 310, 245], "page_size": [612.0, 792.0]}
{"layout": 1067, "type": "text", "text": "tools/analysis tools/test robustness.py and tools/analysis tools/robustness e val.py helps users to evaluate model robustness. The core idea comes from  Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming . For more information how to evaluate models on corrupted images and results for a set of standard models please refer to  robustness benchmarking.md . ", "page_idx": 134, "bbox": [72, 260.8024597167969, 540, 309.9774475097656], "page_size": [612.0, 792.0]}
{"layout": 1068, "type": "text", "text": "26.2 FPS Benchmark ", "text_level": 1, "page_idx": 134, "bbox": [70, 331, 220, 349], "page_size": [612.0, 792.0]}
{"layout": 1069, "type": "text", "text": "tools/analysis tools/benchmark.py  helps users to calculate FPS. The FPS value includes model forward and post-processing. In order to get a more accurate value, currently only supports single GPU distributed startup mode. ", "page_idx": 134, "bbox": [72, 364.5394287109375, 540, 389.804443359375], "page_size": [612.0, 792.0]}
{"layout": 1070, "type": "text", "text": "python -m torch.distributed.launch --n proc per node  $^{=1}$   --master port  $\\cdot^{-}$  \\${ PORT }  tools/\n\n  $\\hookrightarrow$  analysis tools/benchmark.py  \\\n\n → \\${ CONFIG }  \\ \\${ CHECKPOINT }  \\ [ --repeat-num  \\${ REPEAT_NUM } ]  \\ [ --max-iter    $\\mathcal{S}$  { MAX_ITER } ]  \\ [ --log-interval  \\${ LOG INTERVAL } ]  \\ --launcher pytorch ", "page_idx": 134, "bbox": [72, 399, 501.2392578125, 493.237060546875], "page_size": [612.0, 792.0]}
{"layout": 1071, "type": "text", "text": "Examples: Assuming that you have already downloaded the  Faster R-CNN  model checkpoint to the directory checkpoints/ . ", "page_idx": 134, "bbox": [72, 505.8094787597656, 540, 531.0755004882812], "page_size": [612.0, 792.0]}
{"layout": 1072, "type": "text", "text": "python -m torch.distributed.launch --n proc per node  $^{=1}$   --master port = 29500  tools/analysis_\n\n  $\\hookrightarrow$  tools/benchmark.py  \\\n\n → configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --launcher pytorch ", "page_idx": 134, "bbox": [72, 540.8087158203125, 540, 598.6411743164062], "page_size": [612.0, 792.0]}
{"layout": 1073, "type": "text", "text": "MISCELLANEOUS ", "text_level": 1, "page_idx": 136, "bbox": [415, 164, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 1074, "type": "text", "text": "27.1 Evaluating a metric ", "text_level": 1, "page_idx": 136, "bbox": [71, 229, 241, 246], "page_size": [612.0, 792.0]}
{"layout": 1075, "type": "text", "text": "tools/analysis tools/e val metric.py  evaluates certain metrics of a pkl result file according to a config file. ", "page_idx": 136, "bbox": [72, 260.8024597167969, 532.8107299804688, 274.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 1076, "type": "text", "text": "python tools/analysis tools/e val metric.py  \\${ CONFIG } \\${ P KL RESULTS }  [ -h ] [ --format-only ] ␣\n\n  $\\hookrightarrow$  [ --eval  \\${ EVAL [EVAL ...] } ]\n\n → [ --cfg-options  \\${ CF G OPTIONS  [CF G OPTIONS ...] } ] [ --eval-options  \\${ E VAL OPTIONS  [E VAL OPTIONS ...] } ] ", "page_idx": 136, "bbox": [72, 283.8457336425781, 538.2007446289062, 329.72412109375], "page_size": [612.0, 792.0]}
{"layout": 1077, "type": "text", "text": "27.2 Print the entire config ", "text_level": 1, "page_idx": 136, "bbox": [70, 360, 258, 377], "page_size": [612.0, 792.0]}
{"layout": 1078, "type": "text", "text": "tools/misc/print config.py  prints the whole config verbatim, expanding all its imports. ", "page_idx": 136, "bbox": [72, 392.2354736328125, 446.0057678222656, 405.5455017089844], "page_size": [612.0, 792.0]}
{"layout": 1079, "type": "text", "text": "python tools/misc/print config.py  \\${ CONFIG }  [ -h ] [ --options  \\${ OPTIONS  [OPTIONS...] } ] ", "page_idx": 136, "bbox": [72, 415.27972412109375, 512.04833984375, 425.2921447753906], "page_size": [612.0, 792.0]}
{"layout": 1080, "type": "text", "text": "HYPER-PARAMETER OPTIMIZATION ", "text_level": 1, "page_idx": 138, "bbox": [289, 164, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 1081, "type": "text", "text": "28.1 YOLO Anchor Optimization ", "text_level": 1, "page_idx": 138, "bbox": [70, 228, 292, 245], "page_size": [612.0, 792.0]}
{"layout": 1082, "type": "text", "text": "tools/analysis tools/optimize anchors.py  provides two method to optimize YOLO anchors. ", "page_idx": 138, "bbox": [71, 260.8024597167969, 480.00799560546875, 274.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 1083, "type": "text", "text": "One is k-means anchor cluster which refers from  darknet . ", "page_idx": 138, "bbox": [71, 278.7354736328125, 299.9044494628906, 292.0455017089844], "page_size": [612.0, 792.0]}
{"layout": 1084, "type": "text", "text": "python tools/analysis tools/optimize anchors.py  \\${ CONFIG }  --algorithm k-means --input-  $\\hookrightarrow$  shape  \\${ INPUT SHAPE  [WIDTH HEIGHT] }  --output-dir  \\${ OUTPUT_DIR } → ", "page_idx": 138, "bbox": [71, 300.9817199707031, 522, 325.8283386230469], "page_size": [612.0, 792.0]}
{"layout": 1085, "type": "text", "text": "Another is using differential evolution to optimize anchors. ", "page_idx": 138, "bbox": [71, 335.5224609375, 306.9081115722656, 348.8324890136719], "page_size": [612.0, 792.0]}
{"layout": 1086, "type": "text", "text": "python tools/analysis tools/optimize anchors.py  \\${ CONFIG }  --algorithm differential  $\\hookrightarrow$  evolution --input-shape  \\${ INPUT SHAPE  [WIDTH HEIGHT] }  --output-dir  \\${ OUTPUT_DIR } → ", "page_idx": 138, "bbox": [71, 357.76873779296875, 506.4693908691406, 382.6153564453125], "page_size": [612.0, 792.0]}
{"layout": 1087, "type": "text", "text": "E.g., ", "text_level": 1, "page_idx": 138, "bbox": [71, 395, 90, 405], "page_size": [612.0, 792.0]}
{"layout": 1088, "type": "text", "text": "python tools/analysis tools/optimize anchors.py configs/yolo/yo lov 3 d 53 320 273 e coco.py ␣  $\\hookrightarrow$  --algorithm differential evolution --input-shape  608 608  --device cuda --output-dir ␣ →  $\\hookrightarrow$  work_dirs → ", "page_idx": 138, "bbox": [71, 414.55572509765625, 537.5028686523438, 451.3573303222656], "page_size": [612.0, 792.0]}
{"layout": 1089, "type": "table", "page_idx": 138, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_117.jpg", "bbox": [69, 462, 545, 722], "page_size": [612.0, 792.0], "ocr_text": "You will get:\n\nloading annotations into memory...\n\nDone (t=9.70s)\n\ncreating index...\n\nindex created!\n\n2021-07-19 19:37:20,951 - mmdet - INFO - Collecting bboxes from annotation...\n[>>>>>>>>>>>>>> >>> >> >> > >> >>> >> >>> >>> D> >>> >>>>>>>>>>] 117266/117266, 15874.5 task/s,.\nelapsed: 7s, ETA: Os\n\n2021-07-19 19:37:28,753 - mmdet - INFO - Collected 849902 bboxes.\ndifferential_evolution step 1: f(x)= 0.506055\ndifferential_evolution step 2: f(x)= 0.506055\n\ndifferential_evolution step 489: f(x)= 0.386625\n\n2021-07-19 19:46:40,775 - mmdet - INFO Anchor evolution finish. Average IOU: 0.\n-+6133754253387451\n\n2021-07-19 19:46:40,776 - mmdet - INFO Anchor differential evolution result:[[10, 12],.\n+[15, 30], [32, 22], [29, 59], [61, 46], [57, 116], [112, 89], [154, 198], [349, 336]]\n2021-07-19 19:46:40,798 - mmdet - INFO Result saved in work_dirs/anchor_optimize_result.\n\njson (continues on next page)\n\n", "vlm_text": "The table contains log information from a machine learning process, specifically about object detection using `mmdet` (likely MMDetection). Here's a summary of the content:\n\n- **Loading Data:** Annotations are loaded into memory.\n- **Index Creation:** An index is successfully created.\n- **Bounding Boxes Collection:** \n  - Process starts with collecting bounding boxes from annotations (117266 total).\n  - 849902 bounding boxes are collected.\n- **Differential Evolution Steps:** \n  - Step 1 f(x) value: 0.506055\n  - Continues through step 489 with f(x) value: 0.386625\n- **Anchor Evolution:** \n  - Finished with an average IOU of 0.\n  - Results of anchor differential evolution: coordinates for anchor boxes are listed.\n- **Results Saved:** Output is saved in a specified directory.\n\nThis log is part of optimization during object detection model training."}
{"layout": 1090, "type": "text", "text": "CONVENTIONS ", "text_level": 1, "page_idx": 140, "bbox": [434, 164, 541, 180], "page_size": [612.0, 792.0]}
{"layout": 1091, "type": "text", "text": "Please check the following conventions if you would like to modify MM Detection as your own project. ", "page_idx": 140, "bbox": [71, 225.87342834472656, 481.41290283203125, 239.18345642089844], "page_size": [612.0, 792.0]}
{"layout": 1092, "type": "text", "text": "29.1 Loss ", "text_level": 1, "page_idx": 140, "bbox": [71, 262, 142, 278], "page_size": [612.0, 792.0]}
{"layout": 1093, "type": "text", "text": "In MM Detection, a  dict  containing losses and metrics will be returned by  model(\\*\\*data) ", "page_idx": 140, "bbox": [71, 293.74444580078125, 440.4978942871094, 307.0544738769531], "page_size": [612.0, 792.0]}
{"layout": 1094, "type": "text", "text": "For example, in bbox head, ", "page_idx": 140, "bbox": [71, 311.6774597167969, 182, 324.98748779296875], "page_size": [612.0, 792.0]}
{"layout": 1095, "type": "text", "text": "class  BBoxHead (nn . Module): ... def  loss ( self ,  ... ): losses  $=$   dict () # classification loss losses[ ' loss_cls ' ]  $=$   self . loss_cls( ... ) # classification accuracy losses[ ' acc ' ]  $=$   accuracy( ... ) # bbox regression loss losses[ ' loss_bbox ' ]  $=$   self . loss_bbox( ... ) return  losses ", "page_idx": 140, "bbox": [71, 334.12298583984375, 328.28851318359375, 464.58392333984375], "page_size": [612.0, 792.0]}
{"layout": 1096, "type": "text", "text": "bbox_head.loss()  will be called during model forward. The returned dict contains  ' loss_bbox ' ,  ' loss_cls ' , ' acc '  . Only  ' loss_bbox ' ,  ' loss_cls '  will be used during back propagation,  ' acc '  will only be used as a metric to monitor training process. ", "page_idx": 140, "bbox": [71, 476.85845947265625, 540, 514.0784301757812], "page_size": [612.0, 792.0]}
{"layout": 1097, "type": "text", "text": "By default, only values whose keys contain  ' loss '  will be back propagated. This behavior could be changed by modifying  Base Detector.train_step() . ", "page_idx": 140, "bbox": [71, 518.701416015625, 540, 543.9664916992188], "page_size": [612.0, 792.0]}
{"layout": 1098, "type": "text", "text": "29.2 Empty Proposals ", "text_level": 1, "page_idx": 140, "bbox": [71, 567, 226, 583], "page_size": [612.0, 792.0]}
{"layout": 1099, "type": "text", "text": "In MM Detection, We have added special handling and unit test for empty proposals of two-stage. We need to deal with the empty proposals of the entire batch and single image at the same time. For example, in Cascade RoI Head,\n\n ", "page_idx": 140, "bbox": [71, 598.5283813476562, 540, 623.79345703125], "page_size": [612.0, 792.0]}
{"layout": 1100, "type": "text", "text": "...\n\n # There is no proposal in the whole batch if  rois . shape  $[\\mathbb{O}]\\;==\\;\\mathbb{O}$  : b box results  $=$   [[ np . zeros(( 0 ,  5 ), dtype = np . float32) ", "page_idx": 140, "bbox": [71, 645.4817504882812, 291.6754150390625, 703.315185546875], "page_size": [612.0, 792.0]}
{"layout": 1101, "type": "image", "page_idx": 141, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_118.jpg", "bbox": [69, 81, 544, 366], "page_size": [612.0, 792.0], "ocr_text": "for _ in range(self.bbox_head[-1].num_classes)\nJj] * num_imgs\nif self.with_mask:\nmask_classes = self.mask_head[-1].num_classes\nsegm_results = [[[] for _ in range(mask_classes)]\nfor _ in range(num_imgs) ]\nresults = list(zip(bbox_results, segm_results))\nelse:\nresults = bbox_results\nreturn results\n\n# There is no proposal in the single image\nfor i in range(self.num_stages):\n\nif i < self.num_stages - 1:\nfor j in range(num_imgs):\n\n# Handle empty proposal\n\nif rois[j].shape[0] > 0:\nbbox_label = cls_score[j][:, :-1].argmax(dim=1)\nrefine_roi = self.bbox_head[i].regress_by_class(\n\nrois[j], bbox_label, bbox_pred[j], img_metas[j])\n\nrefine_roi_list.append(refine_roi)\n\n", "vlm_text": "The image shows a snippet of Python code dealing with object detection or image processing, likely using deep learning frameworks. It involves bounding box and mask classification, handling proposals in a single image across multiple stages, and refining regions of interest (ROIs). Key components include loops, conditional statements, and function calls related to bounding box and mask operations."}
{"layout": 1102, "type": "text", "text": "If you have customized  RoIHead , you can refer to the above method to deal with empty proposals. ", "page_idx": 141, "bbox": [72, 373.2724609375, 463.0918273925781, 386.5824890136719], "page_size": [612.0, 792.0]}
{"layout": 1103, "type": "text", "text": "29.3 Coco Panoptic Dataset ", "text_level": 1, "page_idx": 141, "bbox": [70, 409, 264, 426], "page_size": [612.0, 792.0]}
{"layout": 1104, "type": "text", "text": "In MM Detection, we have supported COCO Panoptic dataset. We clarify a few conventions about the implementation of  Coco Pan optic Data set  here. ", "page_idx": 141, "bbox": [72, 441.1434631347656, 540, 466.40948486328125], "page_size": [612.0, 792.0]}
{"layout": 1105, "type": "text", "text": "1. For mmdet  $<=2.16.0$  , the range of foreground and background labels in semantic segmentation are different from the default setting of MM Detection. The label  $\\Updownarrow$   stands for  VOID  label and the category labels start from  1 . Since mmdet  $=\\!2.17.0$  , the category labels of semantic segmentation start from    $\\Updownarrow$   and label  255  stands for  VOID for consistency with labels of bounding boxes. To achieve that, the  Pad  pipeline supports setting the padding value for  seg . 2. In the evaluation, the panoptic result is a map with the same shape as the original image. Each value in the result map has the format of  instance id \\* INSTANCE OFFSET   $^+$   category id . ", "page_idx": 141, "bbox": [84, 471.0314636230469, 540, 562.0504760742188], "page_size": [612.0, 792.0]}
{"layout": 1106, "type": "text", "text": "COMPATIBILITY OF MM DETECTION 2.X ", "text_level": 1, "page_idx": 142, "bbox": [269, 163, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 1107, "type": "text", "text": "30.1 MMDetection 2.18.0\n\n ", "page_idx": 142, "bbox": [72, 226.2214813232422, 240.08010864257812, 246.76524353027344], "page_size": [612.0, 792.0]}
{"layout": 1108, "type": "text", "text": "30.1.1 DIIHead compatibility ", "page_idx": 142, "bbox": [72, 260.89239501953125, 232, 278.0122375488281], "page_size": [612.0, 792.0]}
{"layout": 1109, "type": "text", "text": "In order to support QueryInst, attn_feats is added into the returned tuple of DIIHead. ", "page_idx": 142, "bbox": [72, 288.1964416503906, 409.4232177734375, 301.5064697265625], "page_size": [612.0, 792.0]}
{"layout": 1110, "type": "text", "text": "30.2 MMDetection 2.14.0 ", "text_level": 1, "page_idx": 142, "bbox": [71, 324, 242, 340], "page_size": [612.0, 792.0]}
{"layout": 1111, "type": "text", "text": "30.2.1 MMCV Version ", "page_idx": 142, "bbox": [72, 356.1573486328125, 194.4332275390625, 373.2772216796875], "page_size": [612.0, 792.0]}
{"layout": 1112, "type": "text", "text": "In order to fix the problem that the priority of EvalHook is too low, all hook priorities have been re-adjusted in 1.3.8, so MM Detection 2.14.0 needs to rely on the latest MMCV 1.3.8 version. For related information, please refer to  #1120 , for related issues, please refer to  #5343 . ", "page_idx": 142, "bbox": [72, 383.4624328613281, 540, 420.68243408203125], "page_size": [612.0, 792.0]}
{"layout": 1113, "type": "text", "text": "30.2.2 SSD compatibility ", "page_idx": 142, "bbox": [72, 438.29632568359375, 212.9159698486328, 455.41619873046875], "page_size": [612.0, 792.0]}
{"layout": 1114, "type": "text", "text": "In v2.14.0, to make SSD more flexible to use,  PR5291  refactored its backbone, neck and head. The users can use the script  tools/model converters/upgrade ssd version.py  to convert their models. ", "page_idx": 142, "bbox": [72, 465.60040283203125, 540, 490.86541748046875], "page_size": [612.0, 792.0]}
{"layout": 1115, "type": "text", "text": "python tools/model converters/upgrade ssd version.py  \\${ OLD MODEL PATH } \\${ NEW MODEL PATH } ", "page_idx": 142, "bbox": [72, 500.5987243652344, 532.8453369140625, 510.61114501953125], "page_size": [612.0, 792.0]}
{"layout": 1116, "type": "text", "text": "• OLD MODEL PATH: the path to load the old version SSD model. • NEW MODEL PATH: the path to save the converted model weights. ", "page_idx": 142, "bbox": [88, 523.1844482421875, 375.5209655761719, 554.427490234375], "page_size": [612.0, 792.0]}
{"layout": 1117, "type": "text", "text": "MM Detection is going through big refactoring for more general and convenient usages during the releases from v2.12.0 to v2.18.0 (maybe longer). In v2.12.0 MM Detection inevitably brings some BC-breakings, including the MMCV dependency, model initialization, model registry, and mask AP evaluation. ", "page_idx": 142, "bbox": [72, 608.9884643554688, 540, 646.2095336914062], "page_size": [612.0, 792.0]}
{"layout": 1118, "type": "text", "text": "30.3.1 MMCV Version ", "text_level": 1, "page_idx": 143, "bbox": [71, 70, 196, 85], "page_size": [612.0, 792.0]}
{"layout": 1119, "type": "text", "text": "MM Detection v2.12.0 relies on the newest features in MMCV 1.3.3, including  BaseModule  for unified parameter initialization, model registry, and the CUDA operator  Multi Scale De formable At tn  for  Deformable DETR . Note that MMCV 1.3.2 already contains all the features used by MMDet but has known issues. Therefore, we recommend users to skip MMCV v1.3.2 and use v1.3.2, though v1.3.2 might work for most of the cases. ", "page_idx": 143, "bbox": [72, 95.81745910644531, 540, 144.99351501464844], "page_size": [612.0, 792.0]}
{"layout": 1120, "type": "text", "text": "30.3.2 Unified model initialization ", "text_level": 1, "page_idx": 143, "bbox": [71, 164, 264, 178], "page_size": [612.0, 792.0]}
{"layout": 1121, "type": "text", "text": "To unify the parameter initialization in OpenMMLab projects, MMCV supports  BaseModule  that accepts  init_cfg  to allow the modules’ parameters initialized in a flexible and unified manner. Now the users need to explicitly call  model. in it weights()  in the training script to initialize the model (as in  here , previously this was handled by the detector. The downstream projects must update their model initialization accordingly to use MM Detection v2.12.0 . Please refer to PR #4750 for details. ", "page_idx": 143, "bbox": [72, 189.91151428222656, 540, 251.04161071777344], "page_size": [612.0, 792.0]}
{"layout": 1122, "type": "text", "text": "30.3.3 Unified model registry ", "text_level": 1, "page_idx": 143, "bbox": [71, 270, 239, 285], "page_size": [612.0, 792.0]}
{"layout": 1123, "type": "text", "text": "To easily use backbones implemented in other OpenMMLab projects, MM Detection v2.12.0 inherits the model registry created in MMCV (#760). In this way, as long as the backbone is supported in an OpenMMLab project and that project also uses the registry in MMCV, users can use that backbone in MM Detection by simply modifying the config without copying the code of that backbone into MM Detection. Please refer to PR #5059 for more details. ", "page_idx": 143, "bbox": [72, 295.9595947265625, 540, 345.1355895996094], "page_size": [612.0, 792.0]}
{"layout": 1124, "type": "text", "text": "30.3.4 Mask AP evaluation ", "text_level": 1, "page_idx": 143, "bbox": [71, 364, 225, 378], "page_size": [612.0, 792.0]}
{"layout": 1125, "type": "text", "text": "Before  PR 4898  and V2.12.0, the mask AP of small, medium, and large instances is calculated based on the bounding box area rather than the real mask area. This leads to higher  APs  and  APm  but lower  APl  but will not affect the overall mask AP.  PR 4898  change it to use mask areas by deleting  bbox  in mask AP calculation. The new calculation does not affect the overall mask AP evaluation and is consistent with  Detectron2 . ", "page_idx": 143, "bbox": [72, 390.0535583496094, 540, 439.2285461425781], "page_size": [612.0, 792.0]}
{"layout": 1126, "type": "text", "text": "30.4 Compatibility with MM Detection 1.x ", "text_level": 1, "page_idx": 143, "bbox": [71, 461, 349, 478], "page_size": [612.0, 792.0]}
{"layout": 1127, "type": "text", "text": "MM Detection 2.0 goes through a big refactoring and addresses many legacy issues. It is not compatible with the 1.x version, i.e., running inference with the same model weights in these two versions will produce different results. Thus, MM Detection 2.0 re-benchmarks all the models and provides their links and logs in the model zoo. ", "page_idx": 143, "bbox": [72, 493.79052734375, 540, 531.0105590820312], "page_size": [612.0, 792.0]}
{"layout": 1128, "type": "text", "text": "The major differences are in four folds: coordinate system, codebase conventions, training hyper parameters, and mod- ular design. ", "page_idx": 143, "bbox": [72, 535.6334838867188, 540, 560.8984985351562], "page_size": [612.0, 792.0]}
{"layout": 1129, "type": "text", "text": "30.4.1 Coordinate System ", "text_level": 1, "page_idx": 143, "bbox": [71, 580, 222, 594], "page_size": [612.0, 792.0]}
{"layout": 1130, "type": "text", "text": "The new coordinate system is consistent with  Detectron2  and treats the center of the most left-top pixel as   $(0,0)$   rather than the left-top corner of that pixel. Accordingly, the system interprets the coordinates in COCO bounding box and segmentation annotations as coordinates in range  [0, width]  or  [0, height] . This modification affects all the computation related to the bbox and pixel selection, which is more natural and accurate. ", "page_idx": 143, "bbox": [72, 605.8164672851562, 540, 654.9925537109375], "page_size": [612.0, 792.0]}
{"layout": 1131, "type": "text", "text": "• The height and width of a box with corners (x1, y1) and   $(\\mathrm{x}2,\\mathrm{y}2)$   in the new coordinate system is computed as width  $\\mathbf{\\xi}=\\mathbf{\\xi}\\mathbf{x}2\\ \\mathbf{\\xi}-\\ \\mathbf{x}1$   and  height  $\\mathbf{\\xi}=\\ \\mathbf{y}2\\ \\mathbf{\\xi}-\\ \\mathbf{y}1$  . In MM Detection 1.x and previous version, a  $^{**}+1^{**}$   was added both height and width. This modification are in three folds: ", "page_idx": 143, "bbox": [88.43803405761719, 659.615478515625, 540, 696.8355102539062], "page_size": [612.0, 792.0]}
{"layout": 1132, "type": "text", "text": "1. Box transformation and encoding/decoding in regression. ", "page_idx": 143, "bbox": [106.37104034423828, 701.45849609375, 347.48583984375, 714.7685546875], "page_size": [612.0, 792.0]}
{"layout": 1133, "type": "text", "text": "2. IoU calculation. This affects the matching process between ground truth and bounding box and the NMS process. The effect to compatibility is very negligible, though. ", "page_idx": 144, "bbox": [106, 71.45246887207031, 541, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1134, "type": "text", "text": "3. The corners of bounding box is in float type and no longer quantized. This should provide more accurate bounding box results. This also makes the bounding box and RoIs not required to have minimum size of 1, whose effect is small, though.\n\n ", "page_idx": 144, "bbox": [106, 101.34046936035156, 541, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 1135, "type": "text", "text": "• The anchors are center-aligned to feature grid points and in float type. In MM Detection 1.x and previous version, the anchors are in  int  type and not center-aligned. This affects the anchor generation in RPN and all the anchor- based methods.\n\n ", "page_idx": 144, "bbox": [88, 143.18348693847656, 541, 180.40354919433594], "page_size": [612.0, 792.0]}
{"layout": 1136, "type": "text", "text": "• ROIAlign is better aligned with the image coordinate system. The new implementation is adopted from  Detec- tron2 . The RoIs are shifted by half a pixel by default when they are used to cropping RoI features, compared to MM Detection 1.x. The old behavior is still available by setting  aligned=False  instead of  aligned  $\\risingdotseq$  True .\n\n ", "page_idx": 144, "bbox": [88, 185.02650451660156, 541, 222.24656677246094], "page_size": [612.0, 792.0]}
{"layout": 1137, "type": "text", "text": "• Mask cropping and pasting are more accurate. ", "page_idx": 144, "bbox": [88, 226.86952209472656, 282, 240.17955017089844], "page_size": [612.0, 792.0]}
{"layout": 1138, "type": "text", "text": "1. We use the new RoIAlign to crop mask targets. In MM Detection 1.x, the bounding box is quantized before it is used to crop mask target, and the crop process is implemented by numpy. In new implementation, the bounding box for crop is not quantized and sent to RoIAlign. This implementation accelerates the training speed by a large margin   $\\mathord{\\sim}0.1\\mathrm{s}$   per iter,   ${\\sim}2$   hour when training Mask R50 for 1x schedule) and should be more accurate. ", "page_idx": 144, "bbox": [106, 244.80250549316406, 541, 305.93255615234375], "page_size": [612.0, 792.0]}
{"layout": 1139, "type": "text", "text": "2. In MM Detection 2.0, the “ paste_mask() ” function is different and should be more accurate than those in previous versions. This change follows the modification in  Detectron2  and can improve mask AP on COCO by  ${\\sim}0.5\\%$   absolute.\n\n ", "page_idx": 144, "bbox": [106, 310.5555419921875, 541, 347.7755432128906], "page_size": [612.0, 792.0]}
{"layout": 1140, "type": "text", "text": "30.4.2 Codebase Conventions ", "text_level": 1, "page_idx": 144, "bbox": [70, 367, 245, 381], "page_size": [612.0, 792.0]}
{"layout": 1141, "type": "text", "text": "• MM Detection 2.0 changes the order of class labels to reduce unused parameters in regression and mask branch more naturally (without  $+1$   and  $^{-1}$  ). This effect all the classification layers of the model to have a different ordering of class labels. The final layers of regression branch and mask head no longer keep  $\\mathbf{K}\\!+\\!1$   channels for K categories, and their class orders are consistent with the classification branch. ", "page_idx": 144, "bbox": [88, 392.6935119628906, 541, 441.8695068359375], "page_size": [612.0, 792.0]}
{"layout": 1142, "type": "text", "text": "–  In MM Detection 2.0, label “K” means background, and labels [0, K-1] correspond to the   $\\textsc{K}=$  num categories object categories. –  In MM Detection 1.x and previous version, label “0” means background, and labels [1, K] correspond to the K categories. – Note : The class order of softmax RPN is still the same as that in 1.x in versions  $<=\\!2.4.0$   while sigmoid RPN is not affected. The class orders in all heads are unified since MM Detection   $\\mathrm{v}2.5.0$  .\n\n ", "page_idx": 144, "bbox": [106, 445.8648376464844, 541, 531.5335083007812], "page_size": [612.0, 792.0]}
{"layout": 1143, "type": "text", "text": "• Low quality matching in R-CNN is not used. In MM Detection 1.x and previous versions, the max i ou as signer  will match low quality boxes for each ground truth box in both RPN and R-CNN training. We observe this sometimes does not assign the most perfect GT box to some bounding boxes, thus MM Detection 2.0 do not allow low quality matching by default in R-CNN training in the new system. This sometimes may slightly improve the box AP (  ${\\sim}0.1\\%$   absolute).\n\n ", "page_idx": 144, "bbox": [88, 536.1564331054688, 541, 597.2864990234375], "page_size": [612.0, 792.0]}
{"layout": 1144, "type": "text", "text": "• Separate scale factors for width and height. In MM Detection 1.x and previous versions, the scale factor is a single float in mode  keep_ratio  $\\mathrm{=}$  True . This is slightly inaccurate because the scale factors for width and height have slight difference. MM Detection 2.0 adopts separate scale factors for width and height, the improvement on AP  ${\\sim}0.1\\%$   absolute.\n\n ", "page_idx": 144, "bbox": [88, 601.9094848632812, 541, 651.0855102539062], "page_size": [612.0, 792.0]}
{"layout": 1145, "type": "text", "text": "• Configs name conventions are changed. MM Detection V2.0 adopts the new name convention to maintain the gradually growing model zoo as the following: ", "page_idx": 144, "bbox": [88, 655.7074584960938, 541, 680.9725341796875], "page_size": [612.0, 792.0]}
{"layout": 1146, "type": "table", "page_idx": 144, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_119.jpg", "bbox": [92, 687, 541, 716], "page_size": [612.0, 792.0], "ocr_text": "{model]_(model setting)_[backbone]_[neck]_(norm setting)_(misc)_(gpu x batch)_\n«[schedule]_[dataset].py,\n", "vlm_text": "The table contains a file naming convention typically used in machine learning or deep learning projects. The format is structured as:\n\n- `[model]` - Name of the model used.\n- `(model setting)` - Specific settings for the model.\n- `[backbone]` - The backbone architecture of the model.\n- `[neck]` - Intermediate layers used to connect parts of the model.\n- `(norm setting)` - Normalization settings.\n- `(misc)` - Miscellaneous settings or information.\n- `(gpu x batch)` - Configuration indicating the number of GPUs and batch size.\n- `[schedule]` - The training schedule or plan.\n- `[dataset]` - The dataset used for training/testing.\n\nThe file extension is `.py`, indicating it is a Python script."}
{"layout": 1147, "type": "text", "text": "where the ( misc ) includes DCN and GCBlock, etc. More details are illustrated in the  documentation for config ", "page_idx": 145, "bbox": [96, 71.30303192138672, 540, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 1148, "type": "text", "text": "• MM Detection V2.0 uses new ResNet Caffe backbones to reduce warnings when loading pre-trained models. Most of the new backbones’ weights are the same as the former ones but do not have  conv.bias , except that they use a different  img norm cf g . Thus, the new backbone will not cause warning of unexpected keys. ", "page_idx": 145, "bbox": [88, 89.38447570800781, 540, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 1149, "type": "text", "text": "30.4.3 Training Hyper parameters ", "text_level": 1, "page_idx": 145, "bbox": [71, 146, 260, 160], "page_size": [612.0, 792.0]}
{"layout": 1150, "type": "text", "text": "The change in training hyper parameters does not affect model-level compatibility but slightly improves the perfor- mance. The major ones are: ", "page_idx": 145, "bbox": [72, 171.5235137939453, 540, 196.78855895996094], "page_size": [612.0, 792.0]}
{"layout": 1151, "type": "text", "text": "• The number of proposals after nms is changed from 2000 to 1000 by setting  nms_post  $\\scriptstyle=1000$   and max_num  $\\scriptstyle1\\equiv1000$  . This slightly improves both mask AP and bbox AP by  ${\\sim}0.2\\%$   absolute. • The default box regression losses for Mask R-CNN, Faster R-CNN and RetinaNet are changed from smooth L1 Loss to L1 loss. This leads to an overall improvement in box AP (  ${\\sim}0.6\\%$   absolute). However, using L1-loss for other methods such as Cascade R-CNN and HTC does not improve the performance, so we keep the original settings for these methods. • The sample num of RoIAlign layer is set to be 0 for simplicity. This leads to slightly improvement on mask AP (  ${\\sim}0.2\\%$   absolute). • The default setting does not use gradient clipping anymore during training for faster training speed. This does not degrade performance of the most of models. For some models such as RepPoints we keep using gradient clipping to stabilize the training process and to obtain better performance. • The default warmup ratio is changed from 1/3 to 0.001 for a more smooth warming up process since the gradient clipping is usually not used. The effect is found negligible during our re-benchmarking, though. ", "page_idx": 145, "bbox": [88, 201.41151428222656, 540, 382.0935974121094], "page_size": [612.0, 792.0]}
{"layout": 1152, "type": "text", "text": "30.4.4 Upgrade Models from 1.x to 2.0 ", "text_level": 1, "page_idx": 145, "bbox": [71, 401, 290, 416], "page_size": [612.0, 792.0]}
{"layout": 1153, "type": "text", "text": "To convert the models trained by MM Detection V1.x to MM Detection V2.0, the users can use the script  tools/ model converters/upgrade model version.py  to convert their models. The converted models can be run in MM Detection V2.0 with slightly dropped performance (less than   $1\\%$   AP absolute). Details can be found in  configs/ legacy . ", "page_idx": 145, "bbox": [72, 427.0115661621094, 540, 476.18756103515625], "page_size": [612.0, 792.0]}
{"layout": 1154, "type": "text", "text": "30.5 py coco tools compatibility ", "text_level": 1, "page_idx": 145, "bbox": [71, 498, 285, 515], "page_size": [612.0, 792.0]}
{"layout": 1155, "type": "text", "text": "mm py coco tools  is the OpenMMlab’s folk of official  py coco tools , which works for both MM Detection and De- tectron2. Before  PR 4939 , since  py coco tools  and  mm py coco tool  have the same package name, if users already installed  py c coco tools  (installed Detectron2 first under the same environment), then the setup of MM Detection will skip installing  mm py coco tool . Thus MM Detection fails due to the missing  mm py coco tools . If MM Detection is in- stalled before Detectron2, they could work under the same environment.  PR 4939  deprecates mm py coco tools in favor of official py coco tools. Users may install MM Detection and Detectron2 under the same environment after  PR 4939 , no matter what the installation order is. ", "page_idx": 145, "bbox": [72, 530.7484741210938, 540, 615.78955078125], "page_size": [612.0, 792.0]}
{"layout": 1156, "type": "text", "text": "PROJECTS BASED ON MM DETECTION ", "text_level": 1, "page_idx": 146, "bbox": [271, 162, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 1157, "type": "text", "text": "There are many projects built upon MM Detection. We list some of them as examples of how to extend MM Detection for your own projects. As the page might not be completed, please feel free to create a PR to update this page. ", "page_idx": 146, "bbox": [72, 225.87342834472656, 540, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 1158, "type": "text", "text": "31.1 Projects as an extension ", "text_level": 1, "page_idx": 146, "bbox": [71, 274, 276, 290], "page_size": [612.0, 792.0]}
{"layout": 1159, "type": "text", "text": "Some projects extend the boundary of MM Detection for deployment or other research fields. They reveal the potential of what MM Detection can do. We list several of them as below. ", "page_idx": 146, "bbox": [72, 305.6994323730469, 540, 330.9654541015625], "page_size": [612.0, 792.0]}
{"layout": 1160, "type": "text", "text": "•  OTE Detection : OpenVINO training extensions for object detection. •  MM Detection 3 d : OpenMMLab’s next-generation platform for general 3D object detection. ", "page_idx": 146, "bbox": [88, 335.5874328613281, 460.23297119140625, 366.8304748535156], "page_size": [612.0, 792.0]}
{"layout": 1161, "type": "text", "text": "31.2 Projects of papers ", "text_level": 1, "page_idx": 146, "bbox": [71, 389, 234, 406], "page_size": [612.0, 792.0]}
{"layout": 1162, "type": "text", "text": "There are also projects released with papers. Some of the papers are published in top-tier conferences (CVPR, ICCV, and ECCV), the others are also highly influential. To make this list also a reference for the community to develop and compare new object detection algorithms, we list them following the time order of top-tier conferences. Methods already supported and maintained by MM Detection are not listed. ", "page_idx": 146, "bbox": [72, 421.3924560546875, 540, 470.56744384765625], "page_size": [612.0, 792.0]}
{"layout": 1163, "type": "text", "text": "• Involution: Inverting the Inherence of Convolution for Visual Recognition, CVPR21.  [paper][github] • Multiple Instance Active Learning for Object Detection, CVPR 2021.  [paper][github] • Adaptive Class Suppression Loss for Long-Tail Object Detection, CVPR 2021.  [paper][github] • General iz able Pedestrian Detection: The Elephant In The Room, CVPR2021.  [paper][github] • Group Fisher Pruning for Practical Network Compression, ICML2021.  [paper][github] • Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax, CVPR2020. [paper][github] • Coherent Reconstruction of Multiple Humans from a Single Image, CVPR2020.  [paper][github] • Look-into-Object: Self-supervised Structure Modeling for Object Recognition, CVPR 2020.  [paper][github] • Video Panoptic Segmentation, CVPR2020.  [paper][github] • D2Det: Towards High Quality Object Detection and Instance Segmentation, CVPR2020.  [paper][github] • Centripetal Net: Pursuing High-quality Keypoint Pairs for Object Detection, CVPR2020.  [paper][github] • Learning a Unified Sample Weighting Network for Object Detection, CVPR 2020.  [paper][github] • Scale-equalizing Pyramid Convolution for Object Detection, CVPR2020.  [paper] [github] ", "page_idx": 146, "bbox": [88, 475.1904296875, 540, 715.6484985351562], "page_size": [612.0, 792.0]}
{"layout": 1164, "type": "text", "text": "• Revisiting the Sibling Head in Object Detector, CVPR2020.  [paper][github]\n\n • PolarMask: Single Shot Instance Segmentation with Polar Representation, CVPR2020.  [paper][github]\n\n • Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection, CVPR2020.  [paper][github]\n\n • ZeroQ: A Novel Zero Shot Quantization Framework, CVPR2020.  [paper][github]\n\n • CBNet: A Novel Composite Backbone Network Architecture for Object Detection, AAAI2020.  [paper][github]\n\n • RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation, AAAI2020. [paper][github]\n\n • Training-Time-Friendly Network for Real-Time Object Detection, AAAI2020.  [paper][github]\n\n • Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution, NeurIPS 2019. [paper][github]\n\n • Reasoning R-CNN: Unifying Adaptive Global Reasoning into Large-scale Object Detection, CVPR2019.  [pa- per][github]\n\n • Learning RoI Transformer for Oriented Object Detection in Aerial Images, CVPR2019.  [paper][github]\n\n • SOLO: Segmenting Objects by Locations.  [paper][github]\n\n • SOLOv2: Dynamic, Faster and Stronger.  [paper][github]\n\n • Dense Peppoints: Representing Visual Objects with Dense Point Sets.  [paper][github]\n\n • IterDet: Iterative Scheme for Object Detection in Crowded Environments.  [paper][github]\n\n • Cross-Iteration Batch Normalization.  [paper][github]\n\n • A Ranking-based, Balanced Loss Function Unifying Classification and Local is ation in Object Detection, Neu rIPS 2020  [paper][github]\n\n • Relation Net  $^{++}$  : Bridging Visual Representations for Object Detection via Transformer Decoder, Neu rIPS 2020 [paper][github]\n\n • Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection, CVPR2021 [paper][github]\n\n • Instances as Queries, ICCV2021 [paper][github]\n\n • Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV2021 [paper][github]\n\n • Focal Transformer: Focal Self-attention for Local-Global Interactions in Vision Transformers, Neu rIPS 2021 [paper][github]\n\n • End-to-End Semi-Supervised Object Detection with Soft Teacher, ICCV2021 [paper][github]\n\n • CBNetV2: A Novel Composite Backbone Network Architecture for Object Detection  [paper][github]\n\n • Instances as Queries, ICCV2021  [paper][github] CHANGELOG\n\n ", "page_idx": 147, "bbox": [88, 71.45246887207031, 540, 580.9015502929688], "page_size": [612.0, 792.0]}
{"layout": 1165, "type": "text", "text": "", "page_idx": 148, "bbox": [446.9649963378906, 162.4604949951172, 540, 183.00425720214844], "page_size": [612.0, 792.0]}
{"layout": 1166, "type": "text", "text": "32.1 v2.18.0 (27/10/2021) ", "text_level": 1, "page_idx": 148, "bbox": [71, 228, 243, 245], "page_size": [612.0, 792.0]}
{"layout": 1167, "type": "text", "text": "32.1.1 Highlights ", "text_level": 1, "page_idx": 148, "bbox": [71, 262, 172, 278], "page_size": [612.0, 792.0]}
{"layout": 1168, "type": "text", "text": "• Support  QueryInst  (#6050)\n\n • Refactor dense heads to decouple onnx export logics from  get_bboxes  and speed up inference (#5317, #6003, #6369, #6268, #6315)\n\n ", "page_idx": 148, "bbox": [88, 288.1964416503906, 540, 331.39447021484375], "page_size": [612.0, 792.0]}
{"layout": 1169, "type": "text", "text": "32.1.2 New Features ", "text_level": 1, "page_idx": 148, "bbox": [71, 350, 191, 365], "page_size": [612.0, 792.0]}
{"layout": 1170, "type": "text", "text": "• Support  QueryInst  (#6050)\n\n • Support infinite sampler (#5996)\n\n ", "page_idx": 148, "bbox": [88, 376.31243896484375, 227.49673461914062, 407.55548095703125], "page_size": [612.0, 792.0]}
{"layout": 1171, "type": "text", "text": "32.1.3 Bug Fixes ", "text_level": 1, "page_idx": 148, "bbox": [71, 426, 169, 441], "page_size": [612.0, 792.0]}
{"layout": 1172, "type": "text", "text": "• Fix in it weight in fc n mask head (#6378)\n\n • Fix type error in im show b boxes of RPN (#6386)\n\n • Fix broken colab link in MM Detection Tutorial (#6382)\n\n • Make sure the device and dtype of scale factor are the same as bboxes (#6374)\n\n • Remove sampling hardcode (#6317)\n\n • Fix Random Affine bbox coordinate re correction (#6293)\n\n • Fix init bug of final cls/reg layer in convfc head (#6279)\n\n • Fix img_shape broken in auto augment (#6259)\n\n • Fix kwargs parameter missing error in two_stage (#6256) ", "page_idx": 148, "bbox": [88, 452.47344970703125, 411.2270202636719, 609.2455444335938], "page_size": [612.0, 792.0]}
{"layout": 1173, "type": "text", "text": "32.1.4 Improvements ", "text_level": 1, "page_idx": 149, "bbox": [70, 70, 194, 85], "page_size": [612.0, 792.0]}
{"layout": 1174, "type": "text", "text": "• Unify the interface of stuff head and panoptic head (#6308) • Polish readme (#6243) • Add code-spell pre-commit hook and fix a typo (#6306) • Fix typo (#6245, #6190) • Fix sampler unit test (#6284) • Fix  forward dummy  of YOLACT to enable  get_flops  (#6079) • Fix link error in the config documentation (#6252) • Adjust the order to beautify the document (#6195) ", "page_idx": 149, "bbox": [88, 95.81745910644531, 357, 234.6564178466797], "page_size": [612.0, 792.0]}
{"layout": 1175, "type": "text", "text": "32.1.5 Refactors ", "text_level": 1, "page_idx": 149, "bbox": [71, 254, 167, 268], "page_size": [612.0, 792.0]}
{"layout": 1176, "type": "text", "text": "• Refactor one-stage get_bboxes logic (#5317) • Refactor ONNX export of One-Stage models (#6003, #6369) • Refactor dense_head and speedup (#6268) • Migrate to use prior generator in training of dense heads (#6315) ", "page_idx": 149, "bbox": [88, 279.5743713378906, 357, 346.6834411621094], "page_size": [612.0, 792.0]}
{"layout": 1177, "type": "text", "text": "32.1.6 Contributors ", "text_level": 1, "page_idx": 149, "bbox": [70, 366, 185, 380], "page_size": [612.0, 792.0]}
{"layout": 1178, "type": "text", "text": "A total of 18 developers contributed to this release. Thanks   $@$  Boyden,   $@$  onnkeat,   $@\\mathrm{st}9007\\mathrm{a}$  ,   $@$  vealocia,   $@$  yhcao6,  $@$  Da pang pang X,   $@$  yellow dolphin,   $@$  cclauss,   $@$  kenny m ck or mick,   $@$  ping guo killer,   $@$  collinzrj,   $@$  AndreaPi,   $@$  Aron- Lin,   $@$  BIG Wang Yu Dong,   $@$  hhaAndroid,   $@$  jshilong,   $@$  RangiLyu,   $@$  ZwwWayne ", "page_idx": 149, "bbox": [71, 391.6014099121094, 540, 428.8214111328125], "page_size": [612.0, 792.0]}
{"layout": 1179, "type": "text", "text": "32.2 v2.17.0 (28/9/2021) ", "text_level": 1, "page_idx": 149, "bbox": [71, 451, 233, 467], "page_size": [612.0, 792.0]}
{"layout": 1180, "type": "text", "text": "32.2.1 Highlights ", "text_level": 1, "page_idx": 149, "bbox": [71, 485, 171, 499], "page_size": [612.0, 792.0]}
{"layout": 1181, "type": "text", "text": "• Support  PVT  and  PVTv2 • Support  SOLO • Support large scale jittering and New Mask R-CNN baselines • Speed up  YOLOv3  inference ", "page_idx": 149, "bbox": [88, 510.7773742675781, 341, 577.8854370117188], "page_size": [612.0, 792.0]}
{"layout": 1182, "type": "text", "text": "32.2.2 New Features ", "text_level": 1, "page_idx": 149, "bbox": [71, 597, 190, 611], "page_size": [612.0, 792.0]}
{"layout": 1183, "type": "text", "text": "• Support  PVT  and  PVTv2  (#5780) • Support  SOLO  (#5832) • Support large scale jittering and New Mask R-CNN baselines (#6132) • Add a general data structrue for the results of models (#5508) • Added a base class for one-stage instance segmentation (#5904) ", "page_idx": 149, "bbox": [88, 622.8033447265625, 375.75018310546875, 707.8444213867188], "page_size": [612.0, 792.0]}
{"layout": 1184, "type": "text", "text": "• Speed up  YOLOv3  inference (#5991)\n\n • Release Swin Transformer pre-trained models (#6100)\n\n • Support mixed precision training in  YOLOX  (#5983)\n\n • Support  val  workflow in  YOLACT  (#5986)\n\n • Add script to test  torchserve  (#5936)\n\n • Support  onnxsim  with dynamic input shape (#6117)\n\n ", "page_idx": 150, "bbox": [88, 71.45246887207031, 314, 174.4254608154297], "page_size": [612.0, 792.0]}
{"layout": 1185, "type": "text", "text": "32.2.3 Bug Fixes ", "text_level": 1, "page_idx": 150, "bbox": [70, 193, 169, 208], "page_size": [612.0, 792.0]}
{"layout": 1186, "type": "text", "text": "• Fix the function naming errors in  model wrappers  (#5975)\n\n • Fix regression loss bug when the input is an empty tensor (#5976)\n\n • Fix scores not contiguous error in  center net head  (#6016)\n\n • Fix missing parameters bug in  im show b boxes  (#6034)\n\n • Fix bug in  aug_test  of  HTC  when the length of  det_bboxes  is 0 (#6088)\n\n • Fix empty proposal errors in the training of some two-stage models (#5941)\n\n • Fix  dynamic axes  parameter error in  ONNX  dynamic shape export (#6104)\n\n • Fix  dynamic shape  bug of  Sync Random Size Hook  (#6144)\n\n • Fix the Swin Transformer config link error in the configuration (#6172)\n\n ", "page_idx": 150, "bbox": [88, 219.0233917236328, 398, 375.79547119140625], "page_size": [612.0, 792.0]}
{"layout": 1187, "type": "text", "text": "32.2.4 Improvements ", "text_level": 1, "page_idx": 150, "bbox": [71, 394, 193, 409], "page_size": [612.0, 792.0]}
{"layout": 1188, "type": "text", "text": "• Add filter rules in  Mosaic  transform (#5897)\n\n • Add size divisor in get flops to avoid some potential bugs (#6076)\n\n • Add Chinese translation of  docs_zh-CN/tutorials/customize data set.md  (#5915)\n\n • Add Chinese translation of  conventions.md  (#5825)\n\n • Add description of the output of data pipeline (#5886)\n\n • Add dataset information in the README file for  Pan optic FP N  (#5996)\n\n • Add  extra_repr  for  DropBlock  layer to get details in the model printing (#6140)\n\n • Fix CI out of memory and add PyTorch1.9 Python3.9 unit tests (#5862)\n\n • Fix download links error of some model (#6069)\n\n • Improve the generalization of XML dataset (#5943)\n\n • Polish assertion error messages (#6017)\n\n • Remove  opencv-python-headless  dependency by  albumen tat ions  (#5868)\n\n • Check dtype in transform unit tests (#5969)\n\n • Replace the default theme of documentation with PyTorch Sphinx Theme (#6146)\n\n • Update the paper and code fields in the metafile (#6043)\n\n • Support to customize padding value of segmentation map (#6152)\n\n • Support to resize multiple segmentation maps (#5747) ", "page_idx": 150, "bbox": [88, 420.3934326171875, 455.44158935546875, 720.6275024414062], "page_size": [612.0, 792.0]}
{"layout": 1189, "type": "text", "text": "32.2.5 Contributors ", "text_level": 1, "page_idx": 151, "bbox": [71, 71, 184, 84], "page_size": [612.0, 792.0]}
{"layout": 1190, "type": "text", "text": "A total of 24 developers contributed to this release. Thanks   $@$  mork o vka 1337,   $@$  HarborYuan,   $@$  guillaume f rd,\n\n  $@$  guigarfr,   $@$  www516717402,   $@$  gao tong xiao,   $@$  ypwhs,   $@$  MartaYang,  $@$  shinya7y,   $@$  justiceeem,   $@$  zhao j in jian 0000,\n\n  $@$  VVsssssk,   $@$  aravind-anantha,   $@$  wangbo-zhao,   $@$  czczup,   $@$  whai362,   $@$  czczup,   $@$  marijnl,   $@$  AronLin,   $@$  BIG- WangYuDong,   $@$  hhaAndroid,   $@.$  jshilong,   $@$  RangiLyu,   $@$  ZwwWayne ", "page_idx": 151, "bbox": [72, 95.81745910644531, 540, 144.99351501464844], "page_size": [612.0, 792.0]}
{"layout": 1191, "type": "text", "text": "32.3 v2.16.0 (30/8/2021) ", "text_level": 1, "page_idx": 151, "bbox": [71, 168, 233, 183], "page_size": [612.0, 792.0]}
{"layout": 1192, "type": "text", "text": "32.3.1 Highlights ", "page_idx": 151, "bbox": [72, 199.6444549560547, 169.66204833984375, 216.76429748535156], "page_size": [612.0, 792.0]}
{"layout": 1193, "type": "text", "text": "• Support  Panoptic FPN  and  Swin Transformer ", "page_idx": 151, "bbox": [88, 226.94850158691406, 280, 240.25852966308594], "page_size": [612.0, 792.0]}
{"layout": 1194, "type": "text", "text": "32.3.2 New Features ", "text_level": 1, "page_idx": 151, "bbox": [71, 259, 190, 274], "page_size": [612.0, 792.0]}
{"layout": 1195, "type": "text", "text": "• Support  Panoptic FPN  and release models (#5577, #5902) • Support Swin Transformer backbone (#5748) • Release RetinaNet models pre-trained with multi-scale 3x schedule (#5636) • Add script to convert unlabeled image list to coco format (#5643) • Add hook to check whether the loss value is valid (#5674) • Add YOLO anchor optimizing tool (#5644) • Support export onnx models without post process. (#5851) • Support classwise evaluation in Coco Pan optic Data set (#5896) • Adapt browse data set for concatenated datasets. (#5935) • Add  PatchEmbed  and  Patch Merging  with  Adaptive Padding  (#5952) ", "page_idx": 151, "bbox": [88, 285.1764831542969, 398.25567626953125, 459.8816223144531], "page_size": [612.0, 792.0]}
{"layout": 1196, "type": "text", "text": "32.3.3 Bug Fixes ", "text_level": 1, "page_idx": 151, "bbox": [71, 479, 169, 494], "page_size": [612.0, 792.0]}
{"layout": 1197, "type": "text", "text": "• Fix unit tests of YOLOX (#5859) • Fix lose randomness in  im show det b boxes  (#5845) • Make output result of  Image To Tensor  contiguous (#5756) • Fix inference bug when calling  regress by class  in RoIHead in some cases (#5884) • Fix bug in CIoU loss where alpha should not have gradient. (#5835) • Fix the bug that  multi scale output  is defined but not used in HRNet (#5887) • Set the priority of EvalHook to LOW. (#5882) • Fix a YOLOX bug when applying bbox rescaling in test mode (#5899) • Fix mosaic coordinate error (#5947) • Fix dtype of bbox in Random Affine. (#5930) ", "page_idx": 151, "bbox": [88, 504.7995910644531, 446.1960144042969, 679.5045776367188], "page_size": [612.0, 792.0]}
{"layout": 1198, "type": "text", "text": "32.3.4 Improvements ", "text_level": 1, "page_idx": 152, "bbox": [70, 70, 195, 85], "page_size": [612.0, 792.0]}
{"layout": 1199, "type": "text", "text": "• Add Chinese version of  data pipeline  and (#5662) • Support to remove state dicts of EMA when publishing models. (#5858) • Refactor the loss function in HTC and SCNet (#5881) • Use warnings instead of logger.warning (#5540) • Use legacy coordinate in metric of VOC (#5627) • Add Chinese version of customize losses (#5826) • Add Chinese version of model_zoo (#5827) ", "page_idx": 152, "bbox": [88, 95.81745910644531, 385.20458984375, 216.7244110107422], "page_size": [612.0, 792.0]}
{"layout": 1200, "type": "text", "text": "32.3.5 Contributors ", "text_level": 1, "page_idx": 152, "bbox": [70, 235, 185, 250], "page_size": [612.0, 792.0]}
{"layout": 1201, "type": "text", "text": "A total of 19 developers contributed to this release. Thanks   $@$  ypwhs,   $@$  zywvvd,   $@$  collinzrj,   $@$  OceanPang,   $@$  ddo- natien,   $@\\,\\@$  haotian-liu,   $@$  viibridges,   $@$  Muyun99,   $@$  guigarfr,   $@$  zhao j in jian 0000,   $@$  jbwang1997,  $@$  wangbo-zhao,  $@$  xvjiarui,   $@$  RangiLyu,   $@.$  jshilong,   $@$  AronLin,   $@$  BIG Wang Yu Dong,   $@$  hhaAndroid, @ZwwWayne ", "page_idx": 152, "bbox": [71, 261.6424255371094, 540, 298.8624267578125], "page_size": [612.0, 792.0]}
{"layout": 1202, "type": "text", "text": "32.4 v2.15.1 (11/8/2021) ", "text_level": 1, "page_idx": 152, "bbox": [70, 320, 235, 338], "page_size": [612.0, 792.0]}
{"layout": 1203, "type": "text", "text": "32.4.1 Highlights ", "text_level": 1, "page_idx": 152, "bbox": [70, 354, 172, 370], "page_size": [612.0, 792.0]}
{"layout": 1204, "type": "text", "text": "• Support  YOLOX ", "page_idx": 152, "bbox": [88, 380.8183898925781, 164.81204223632812, 394.12841796875], "page_size": [612.0, 792.0]}
{"layout": 1205, "type": "text", "text": "32.4.2 New Features ", "text_level": 1, "page_idx": 152, "bbox": [70, 413, 191, 427], "page_size": [612.0, 792.0]}
{"layout": 1206, "type": "text", "text": "• Support  YOLOX (#5756, #5758, #5760, #5767, #5770, #5774, #5777, #5808, #5828, #5848) ", "page_idx": 152, "bbox": [88, 439.04638671875, 465, 452.3564147949219], "page_size": [612.0, 792.0]}
{"layout": 1207, "type": "text", "text": "32.4.3 Bug Fixes ", "text_level": 1, "page_idx": 152, "bbox": [71, 472, 169, 486], "page_size": [612.0, 792.0]}
{"layout": 1208, "type": "text", "text": "• Update correct SSD models. (#5789) • Fix casting error in mask structure (#5820) • Fix MMCV deployment documentation links. (#5790) ", "page_idx": 152, "bbox": [88, 497.2743835449219, 314, 546.4494018554688], "page_size": [612.0, 792.0]}
{"layout": 1209, "type": "text", "text": "32.4.4 Improvements ", "text_level": 1, "page_idx": 152, "bbox": [70, 565, 194, 580], "page_size": [612.0, 792.0]}
{"layout": 1210, "type": "text", "text": "• Use dynamic MMCV download link in TorchServe dockerfile (#5779) • Rename the function  up sample like  to  interpolate as  for more general usage (#5788) ", "page_idx": 152, "bbox": [88, 591.3673706054688, 465, 622.6104125976562], "page_size": [612.0, 792.0]}
{"layout": 1211, "type": "text", "text": "32.4.5 Contributors ", "text_level": 1, "page_idx": 153, "bbox": [71, 71, 185, 85], "page_size": [612.0, 792.0]}
{"layout": 1212, "type": "text", "text": "A total of 14 developers contributed to this release. Thanks   $@$  HAOCHENYE,   $@$  xiaohu2015,   $@$  HsLOL,   $@$  zhiqwang,\n\n  $@$  Adamdad,  $@$  shinya7y,   $@$  Johnson-Wang,   $@$  RangiLyu,   $@$  jshilong,   $@$  mmeendez8,   $@$  AronLin,   $@$  BIG Wang Yu Dong,\n\n  $@$  hhaAndroid,   $@$  ZwwWayne ", "page_idx": 153, "bbox": [72, 95.81745910644531, 540, 133.0375213623047], "page_size": [612.0, 792.0]}
{"layout": 1213, "type": "text", "text": "32.5 v2.15.0 (02/8/2021) ", "text_level": 1, "page_idx": 153, "bbox": [71, 156, 233, 171], "page_size": [612.0, 792.0]}
{"layout": 1214, "type": "text", "text": "32.5.1 Highlights ", "text_level": 1, "page_idx": 153, "bbox": [71, 190, 171, 204], "page_size": [612.0, 792.0]}
{"layout": 1215, "type": "text", "text": "• Support adding  MIM  dependencies during pip installation • Support Mobile Ne tV 2 for SSD-Lite and YOLOv3 • Support Chinese Documentation ", "page_idx": 153, "bbox": [88, 214.9934844970703, 328, 264.16949462890625], "page_size": [612.0, 792.0]}
{"layout": 1216, "type": "text", "text": "32.5.2 New Features ", "text_level": 1, "page_idx": 153, "bbox": [70, 283, 190, 298], "page_size": [612.0, 792.0]}
{"layout": 1217, "type": "text", "text": "• Add function up sample like (#5732)• Support to output pdf and epub format documentation (#5738) • Support and release Cascade Mask R-CNN  $3\\mathbf{X}$   pre-trained models (#5645) • Add  ignore index  to Cross Entropy Loss (#5646) • Support adding  MIM  dependencies during pip installation (#5676) • Add Mobile Ne tV 2 config and models for YOLOv3 (#5510) • Support COCO Panoptic Dataset (#5231) • Support ONNX export of cascade models (#5486) • Support DropBlock with RetinaNet (#5544) • Support Mobile Ne tV 2 SSD-Lite (#5526) ", "page_idx": 153, "bbox": [88, 309.08746337890625, 392.9854736328125, 483.7915954589844], "page_size": [612.0, 792.0]}
{"layout": 1218, "type": "text", "text": "32.5.3 Bug Fixes ", "text_level": 1, "page_idx": 153, "bbox": [70, 503, 169, 518], "page_size": [612.0, 792.0]}
{"layout": 1219, "type": "text", "text": "• Fix the device of label in multi class nm s (#5673) • Fix error of backbone initialization from pre-trained checkpoint in config file (#5603, #5550) • Fix download links of RegNet pretrained weights (#5655) • Fix two-stage runtime error given empty proposal (#5559) • Fix flops count error in DETR (#5654) • Fix unittest for  Num Class Check Hook  when it is not used. (#5626) • Fix description bug of using custom dataset (#5546) • Fix bug of  multi class nm s  that returns the global indices (#5592) • Fix  valid_mask  logic error in RPNHead (#5562) • Fix unit test error of pretrained configs (#5561) • Fix typo error in anchor head.py (#5555) • Fix bug when using dataset wrappers (#5552) • Fix a typo error in demo/MM Det Tutorial.ipynb (#5511) • Fixing crash in  get root logger  when  cfg.log_level  is not None (#5521) • Fix docker version (#5502) • Fix optimizer parameter error when using  It er Based Runner  (#5490) ", "page_idx": 153, "bbox": [88, 528.7095336914062, 466.95770263671875, 721.3475952148438], "page_size": [612.0, 792.0]}
{"layout": 1220, "type": "text", "text": "", "page_idx": 154, "bbox": [88, 71.45246887207031, 414.5544738769531, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 1221, "type": "text", "text": "32.5.4 Improvements ", "text_level": 1, "page_idx": 154, "bbox": [70, 176, 195, 190], "page_size": [612.0, 792.0]}
{"layout": 1222, "type": "text", "text": "• Add unit tests for MMTracking (#5620) • Add Chinese translation of documentation (#5718, #5618, #5558, #5423, #5593, #5421, #5408. #5369, #5419, #5530, #5531) • Update resource limit (#5697) • Update docstring for InstaBoost (#5640) • Support key  reduction override  in all loss functions (#5515) • Use repeat data set to accelerate CenterNet training (#5509) • Remove unnecessary code in autoassign (#5519) • Add documentation about  init_cfg  (#5273) ", "page_idx": 154, "bbox": [88, 201.4114532470703, 540, 352.2055358886719], "page_size": [612.0, 792.0]}
{"layout": 1223, "type": "text", "text": "32.5.5 Contributors ", "text_level": 1, "page_idx": 154, "bbox": [70, 371, 185, 385], "page_size": [612.0, 792.0]}
{"layout": 1224, "type": "text", "text": "A total of 18 developers contributed to this release. Thanks   $@$  OceanPang,   $@$  AronLin,   $@$  hellock,   $@$  Outsider 565,\n\n  $@$  RangiLyu,   $@$  Electronic Elephant,   $@$  likyoo,   $@$  BIG Wang Yu Dong,   $@$  hhaAndroid,   $@$  noobying,   $@$  yyz561,   $@$  likyoo,\n\n  $@$  zeakey,   $@$  ZwwWayne,   $@$  Chen yang Liu,   $@$  johnson-magic,   $@$  qingswu,   $@$  BuxianChen ", "page_idx": 154, "bbox": [71, 397.1235046386719, 540, 434.343505859375], "page_size": [612.0, 792.0]}
{"layout": 1225, "type": "text", "text": "32.6 v2.14.0 (29/6/2021) ", "text_level": 1, "page_idx": 154, "bbox": [71, 456, 233, 473], "page_size": [612.0, 792.0]}
{"layout": 1226, "type": "text", "text": "32.6.1 Highlights ", "text_level": 1, "page_idx": 154, "bbox": [71, 490, 171, 505], "page_size": [612.0, 792.0]}
{"layout": 1227, "type": "text", "text": "• Add  simple test  to dense heads to improve the consistency of single-stage and two-stage detectors • Revert the  test mix in s  to single image test to improve efficiency and readability • Add Faster R-CNN and Mask R-CNN config using multi-scale training with  $3\\mathbf{X}$   schedule ", "page_idx": 154, "bbox": [88, 516.2993774414062, 499.5849304199219, 565.4754638671875], "page_size": [612.0, 792.0]}
{"layout": 1228, "type": "text", "text": "32.6.2 New Features ", "text_level": 1, "page_idx": 154, "bbox": [71, 585, 190, 599], "page_size": [612.0, 792.0]}
{"layout": 1229, "type": "text", "text": "• Support pretrained models from MoCo v2 and SwAV (#5286) • Add Faster R-CNN and Mask R-CNN config using multi-scale training with   $3\\mathbf{X}$   schedule (#5179, #5233) • Add  reduction override  in MSELoss (#5437) • Stable support of exporting DETR to ONNX with dynamic shapes and batch inference (#5168) • Stable support of exporting PointRend to ONNX with dynamic shapes and batch inference (#5440) ", "page_idx": 154, "bbox": [88, 610.3933715820312, 513.0845336914062, 695.4344482421875], "page_size": [612.0, 792.0]}
{"layout": 1230, "type": "text", "text": "32.6.3 Bug Fixes ", "text_level": 1, "page_idx": 155, "bbox": [70, 70, 170, 85], "page_size": [612.0, 792.0]}
{"layout": 1231, "type": "text", "text": "• Fix size mismatch bug in  multi class nm s  (#4980)\n\n • Fix the import path of  Multi Scale De formable Attention  (#5338)\n\n • Fix errors in config of GCNet ResNext101 models (#5360)\n\n • Fix Grid-RCNN error when there is no bbox result (#5357)\n\n • Fix errors in  on nx export  of bbox_head when setting reg class agnostic (#5468)\n\n • Fix type error of AutoAssign in the document (#5478)\n\n • Fix web links ending with  .md  (#5315)\n\n ", "page_idx": 155, "bbox": [88, 95.81745910644531, 427.8240051269531, 216.7244110107422], "page_size": [612.0, 792.0]}
{"layout": 1232, "type": "text", "text": "32.6.4 Improvements ", "text_level": 1, "page_idx": 155, "bbox": [70, 236, 193, 250], "page_size": [612.0, 792.0]}
{"layout": 1233, "type": "text", "text": "• Add  simple test  to dense heads to improve the consistency of single-stage and two-stage detectors (#5264)\n\n • Add support for mask diagonal flip in TTA (#5403)\n\n • Revert the  test mix in s  to single image test to improve efficiency and readability (#5249)\n\n • Make YOLOv3 Neck more flexible (#5218)\n\n • Refactor SSD to make it more general (#5291)\n\n • Refactor  anchor generator  and  point generator  (#5349)\n\n • Allow to configure out the  mask_head  of the HTC algorithm (#5389)\n\n • Delete deprecated warning in FPN (#5311)\n\n • Move  model.pretrained  to  model.backbone.init_cfg  (#5370)\n\n • Make deployment tools more friendly to use (#5280)\n\n • Clarify installation documentation (#5316)\n\n • Add ImageNet Pretrained Models docs (#5268)\n\n • Add FAQ about training loss  $\\scriptstyle,=$  nan solution and COCO AP or AR  $_{=-1}$   (# 5312, #5313)\n\n • Change all weight links of http to https (#5328)\n\n ", "page_idx": 155, "bbox": [88, 261.6424255371094, 533.6172485351562, 508.0776062011719], "page_size": [612.0, 792.0]}
{"layout": 1234, "type": "text", "text": "32.7 v2.13.0 (01/6/2021) ", "text_level": 1, "page_idx": 155, "bbox": [71, 530, 233, 547], "page_size": [612.0, 792.0]}
{"layout": 1235, "type": "text", "text": "32.7.1 Highlights ", "text_level": 1, "page_idx": 155, "bbox": [70, 564, 171, 579], "page_size": [612.0, 792.0]}
{"layout": 1236, "type": "text", "text": "• Support new methods:  CenterNet ,  Seesaw Loss ,  Mobile Ne tV 2 ", "page_idx": 155, "bbox": [88, 590.0335693359375, 349, 603.3436279296875], "page_size": [612.0, 792.0]}
{"layout": 1237, "type": "text", "text": "32.7.2 New Features ", "text_level": 1, "page_idx": 156, "bbox": [70, 70, 191, 85], "page_size": [612.0, 792.0]}
{"layout": 1238, "type": "text", "text": "• Support paper  Objects as Points  (#4602)\n\n • Support paper  Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021)  (#5128)\n\n • Support  Mobile Ne tV 2  backbone and inverted residual block (#5122)\n\n • Support  MIM  (#5143)\n\n • ONNX exportation with dynamic shapes of CornerNet (#5136)\n\n • Add  mask_soft  config option to allow non-binary masks (#4615)\n\n • Add PWC metafile (#5135)\n\n ", "page_idx": 156, "bbox": [88, 95.81745910644531, 456.3774108886719, 216.7244110107422], "page_size": [612.0, 792.0]}
{"layout": 1239, "type": "text", "text": "32.7.3 Bug Fixes ", "text_level": 1, "page_idx": 156, "bbox": [70, 236, 169, 250], "page_size": [612.0, 792.0]}
{"layout": 1240, "type": "text", "text": "", "page_idx": 156, "bbox": [86, 263, 265, 270.75], "page_size": [612.0, 792.0]}
{"layout": 1241, "type": "text", "text": "• Fix Cacscade R-CNN TTA test error when  det_bboxes  length is 0 (#5221)\n\n • Fix  iou_thr  variable naming errors in VOC recall calculation function (#5195)\n\n • Fix Faster R-CNN performance dropped in ONNX Runtime (#5197)\n\n • Fix DETR dict changed error when using python 3.8 during iteration (#5226)\n\n ", "page_idx": 156, "bbox": [88, 279.5744323730469, 416, 346.6835021972656], "page_size": [612.0, 792.0]}
{"layout": 1242, "type": "text", "text": "32.7.4 Improvements ", "text_level": 1, "page_idx": 156, "bbox": [70, 366, 193, 380], "page_size": [612.0, 792.0]}
{"layout": 1243, "type": "text", "text": "• Refactor ONNX export of two stage detector (#5205)\n\n • Replace MM Detection’s EvalHook with MMCV’s EvalHook for consistency (#4806)\n\n • Update RoI extractor for ONNX (#5194)\n\n • Use better parameter initialization in YOLOv3 head for higher performance (#5181)\n\n • Release new DCN models of Mask R-CNN by mixed-precision training (#5201)\n\n • Update YOLOv3 model weights (#5229)\n\n • Add DetectoRS ResNet-101 model weights (#4960)\n\n • Discard bboxes with sizes equals to  min b box size  (#5011)\n\n • Remove duplicated code in DETR head (#5129)\n\n • Remove unnecessary object in class definition (#5180)\n\n • Fix doc link (#5192) ", "page_idx": 156, "bbox": [88, 391.6014709472656, 436.8707275390625, 584.2385864257812], "page_size": [612.0, 792.0]}
{"layout": 1244, "type": "text", "text": "32.8 v2.12.0 (01/5/2021) ", "text_level": 1, "page_idx": 157, "bbox": [70, 71, 234, 88], "page_size": [612.0, 792.0]}
{"layout": 1245, "type": "text", "text": "32.8.1 Highlights ", "text_level": 1, "page_idx": 157, "bbox": [70, 105, 171, 120], "page_size": [612.0, 792.0]}
{"layout": 1246, "type": "text", "text": "• Support new methods:  AutoAssign ,  YOLOF , and  Deformable DETR • Stable support of exporting models to ONNX with batched images and dynamic shape (#5039) ", "page_idx": 157, "bbox": [88, 130.38844299316406, 475.26654052734375, 161.63047790527344], "page_size": [612.0, 792.0]}
{"layout": 1247, "type": "text", "text": "32.8.2 Backwards Incompatible Changes ", "text_level": 1, "page_idx": 157, "bbox": [70, 181, 306, 195], "page_size": [612.0, 792.0]}
{"layout": 1248, "type": "text", "text": "MM Detection is going through big refactoring for more general and convenient usages during the releases from v2.12.0 to v2.15.0 (maybe longer). In v2.12.0 MM Detection inevitably brings some BC-breakings, including the MMCV dependency, model initialization, model registry, and mask AP evaluation. ", "page_idx": 157, "bbox": [72, 206.5484161376953, 540, 243.7694549560547], "page_size": [612.0, 792.0]}
{"layout": 1249, "type": "text", "text": "• MMCV version. MM Detection v2.12.0 relies on the newest features in MMCV 1.3.3, including  BaseModule for unified parameter initialization, model registry, and the CUDA operator  Multi Scale De formable At tn  for Deformable DETR . Note that MMCV 1.3.2 already contains all the features used by MMDet but has known issues. Therefore, we recommend users skip MMCV v1.3.2 and use v1.3.3, though v1.3.2 might work for most cases. • Unified model initialization (#4750). To unify the parameter initialization in OpenMMLab projects, MMCV supports  BaseModule  that accepts  init_cfg  to allow the modules’ parameters initialized in a flexible and uni- fied manner. Now the users need to explicitly call  model.in it weights()  in the training script to initialize the model (as in  here , previously this was handled by the detector. The models in MM Detection have been re- benchmarked to ensure accuracy based on PR #4750.  The downstream projects should update their code accordingly to use MM Detection v2.12.0 . • Unified model registry (#5059). To easily use backbones implemented in other OpenMMLab projects, MMDe- tection migrates to inherit the model registry created in MMCV (#760). In this way, as long as the backbone is supported in an OpenMMLab project and that project also uses the registry in MMCV, users can use that backbone in MM Detection by simply modifying the config without copying the code of that backbone into MM Detection. • Mask AP evaluation (#4898). Previous versions calculate the areas of masks through the bounding boxes when calculating the mask AP of small, medium, and large instances. To indeed use the areas of masks, we pop the key  bbox  during mask AP calculation. This change does not affect the overall mask AP evaluation and aligns the mask AP of similar models in other projects like Detectron2. ", "page_idx": 157, "bbox": [88, 248.3924102783203, 540, 506.7833251953125], "page_size": [612.0, 792.0]}
{"layout": 1250, "type": "text", "text": "32.8.3 New Features ", "text_level": 1, "page_idx": 157, "bbox": [70, 525, 190, 540], "page_size": [612.0, 792.0]}
{"layout": 1251, "type": "text", "text": "• Support paper  AutoAssign: Differentiable Label Assignment for Dense Object Detection  (#4295) • Support paper  You Only Look One-level Feature  (#4295) • Support paper  Deformable DETR: Deformable Transformers for End-to-End Object Detection  (#4778) • Support calculating IoU with FP16 tensor in  b box overlaps  to save memory and keep speed (#4889) • Add  __repr__  in custom dataset to count the number of instances (#4756) • Add windows support by updating requirements.txt (#5052) • Stable support of exporting models to ONNX with batched images and dynamic shape, including SSD, FSAF,FCOS, YOLOv3, RetinaNet, Faster R-CNN, and Mask R-CNN (#5039) ", "page_idx": 157, "bbox": [88, 551.7012329101562, 540, 684.5623168945312], "page_size": [612.0, 792.0]}
{"layout": 1252, "type": "text", "text": "32.8.4 Improvements ", "text_level": 1, "page_idx": 158, "bbox": [70, 70, 194, 86], "page_size": [612.0, 792.0]}
{"layout": 1253, "type": "text", "text": "• Use MMCV  MODEL REGISTRY  (#5059)\n\n • Unified parameter initialization for more flexible usage (#4750)\n\n • Rename variable names and fix docstring in anchor head (#4883)\n\n • Support training with empty GT in Cascade RPN (#4928)\n\n • Add more details of usage of  test robustness  in documentation (#4917)\n\n • Changing to use  py coco tools  instead of  mm py coco tools  to fully support Detectron2 and MM Detection in one environment (#4939)\n\n • Update torch serve dockerfile to support dockers of more versions (#4954)\n\n • Add check for training with single class dataset (#4973)\n\n • Refactor transformer and DETR Head (#4763)\n\n • Update FPG model zoo (#5079)\n\n • More accurate mask AP of small/medium/large instances (#4898)\n\n ", "page_idx": 158, "bbox": [88, 95.81745910644531, 540, 300.4104309082031], "page_size": [612.0, 792.0]}
{"layout": 1254, "type": "text", "text": "32.8.5 Bug Fixes ", "text_level": 1, "page_idx": 158, "bbox": [70, 320, 169, 335], "page_size": [612.0, 792.0]}
{"layout": 1255, "type": "text", "text": "• Fix bug in mean_ap.py when calculating mAP by 11 points (#4875)\n\n • Fix error when key  meta  is not in old checkpoints (#4936)\n\n • Fix hanging bug when training with empty GT in VFNet, GFL, and FCOS by changing the place of  reduce mean (#4923, #4978, #5058)\n\n • Fix a sync roni zed inference error and provide related demo (#4941)\n\n • Fix IoU losses dimensionality unmatch error (#4982)\n\n • Fix torch.randperm whtn using PyTorch 1.8 (#5014)\n\n • Fix empty bbox error in  mask_head  when using CARAFE (#5062)\n\n • Fix  supplement mask  bug when there are zero-size RoIs (#5065)\n\n • Fix testing with empty rois in RoI Heads (#5081)\n\n ", "page_idx": 158, "bbox": [88, 345.3283996582031, 540, 514.0554809570312], "page_size": [612.0, 792.0]}
{"layout": 1256, "type": "text", "text": "32.9 v2.11.0 (01/4/2021) ", "text_level": 1, "page_idx": 158, "bbox": [71, 536, 233, 553], "page_size": [612.0, 792.0]}
{"layout": 1257, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 158, "bbox": [71, 570, 117, 582], "page_size": [612.0, 792.0]}
{"layout": 1258, "type": "text", "text": "• Support new method:  Localization Distillation for Object Detection\n\n • Support Py torch 2 ON NX with batch inference and dynamic shape\n\n ", "page_idx": 158, "bbox": [88, 586.6194458007812, 367, 617.8624877929688], "page_size": [612.0, 792.0]}
{"layout": 1259, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 158, "bbox": [71, 624, 130, 635], "page_size": [612.0, 792.0]}
{"layout": 1260, "type": "text", "text": "• Support  Localization Distillation for Object Detection  (#4758)\n\n • Support Py torch 2 ON NX with batch inference and dynamic shape for Faster-RCNN and mainstream one-stage detectors (#4796)\n\n ", "page_idx": 158, "bbox": [88, 640.4174194335938, 540, 683.6155395507812], "page_size": [612.0, 792.0]}
{"layout": 1261, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 158, "bbox": [71, 690, 134, 701], "page_size": [612.0, 792.0]}
{"layout": 1262, "type": "text", "text": "• Support batch inference in head of RetinaNet (#4699) ", "page_idx": 158, "bbox": [88, 706.1714477539062, 310, 719.4815063476562], "page_size": [612.0, 792.0]}
{"layout": 1263, "type": "text", "text": "• Add batch dimension in second stage of Faster-RCNN (#4785)\n\n • Support batch inference in bbox coder (#4721)\n\n • Add check for  ann_ids  in  COCO Data set  to ensure it is unique (#4789)\n\n • support for showing the FPN results (#4716)\n\n • support dynamic shape for grid anchor (#4684)\n\n • Move py coco tools version check to when it is used (#4880)\n\n ", "page_idx": 159, "bbox": [88, 71.45246887207031, 382.0868835449219, 174.4254608154297], "page_size": [612.0, 792.0]}
{"layout": 1264, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 159, "bbox": [71, 180, 114, 192], "page_size": [612.0, 792.0]}
{"layout": 1265, "type": "text", "text": "• Fix a bug of TridentNet when doing the batch inference (#4717)\n\n • Fix a bug of Py torch 2 ON NX in FASF (#4735)\n\n • Fix a bug when show the image with float type (#4732)\n\n ", "page_idx": 159, "bbox": [88, 196.9813995361328, 352.0191955566406, 246.1564178466797], "page_size": [612.0, 792.0]}
{"layout": 1266, "type": "text", "text": "32.10 v2.10.0 (01/03/2021) ", "text_level": 1, "page_idx": 159, "bbox": [71, 268, 250, 285], "page_size": [612.0, 792.0]}
{"layout": 1267, "type": "text", "text": "32.10.1 Highlights ", "text_level": 1, "page_idx": 159, "bbox": [70, 302, 178, 317], "page_size": [612.0, 792.0]}
{"layout": 1268, "type": "text", "text": "• Support new methods:  FPG\n\n • Support ON NX 2 Tensor RT for SSD, FSAF, FCOS, YOLOv3, and Faster R-CNN.\n\n ", "page_idx": 159, "bbox": [88, 328.11236572265625, 419.266845703125, 359.35540771484375], "page_size": [612.0, 792.0]}
{"layout": 1269, "type": "text", "text": "32.10.2 New Features ", "text_level": 1, "page_idx": 159, "bbox": [71, 378, 196, 392], "page_size": [612.0, 792.0]}
{"layout": 1270, "type": "text", "text": "• Support ON NX 2 Tensor RT for SSD, FSAF, FCOS, YOLOv3, and Faster R-CNN (#4569)\n\n • Support  Feature Pyramid Grids (FPG)  (#4645)\n\n • Support video demo (#4420)\n\n • Add seed option for sampler (#4665)\n\n • Support to customize type of runner (#4570, #4669)\n\n • Support synchronizing BN buffer in  EvalHook  (#4582)\n\n • Add script for GIF demo (#4573)\n\n ", "page_idx": 159, "bbox": [88, 404.27337646484375, 451.00762939453125, 525.1795043945312], "page_size": [612.0, 792.0]}
{"layout": 1271, "type": "text", "text": "32.10.3 Bug Fixes ", "text_level": 1, "page_idx": 159, "bbox": [71, 545, 176, 559], "page_size": [612.0, 792.0]}
{"layout": 1272, "type": "text", "text": "• Fix ConfigDict Attribute Error and add Colab link (#4643)\n\n • Avoid crash in empty gt training of GFL head (#4631)\n\n • Fix  iou_thrs  bug in RPN evaluation (#4581)\n\n • Fix syntax error of config when upgrading model version (#4584) ", "page_idx": 159, "bbox": [88, 570.097412109375, 358.7937316894531, 637.2064819335938], "page_size": [612.0, 792.0]}
{"layout": 1273, "type": "text", "text": "32.10.4 Improvements ", "text_level": 1, "page_idx": 160, "bbox": [70, 70, 201, 85], "page_size": [612.0, 792.0]}
{"layout": 1274, "type": "text", "text": "• Refactor unit test file structures (#4600)\n\n • Refactor nms config (#4636)\n\n • Get loading pipeline by checking the class directly rather than through config strings (#4619)\n\n • Add doctests for mask target generation and mask structures (#4614)\n\n • Use deep copy when copying pipeline arguments (#4621)\n\n • Update documentations (#4642, #4650, #4620, #4630)\n\n • Remove redundant code calling  import modules from strings  (#4601)\n\n • Clean deprecated FP16 API (#4571)\n\n • Check whether  CLASSES  is correctly initialized in the initialization of  XMLDataset  (#4555)\n\n • Support batch inference in the inference API (#4462, #4526)\n\n • Clean deprecated warning and fix ‘meta’ error (#4695)\n\n ", "page_idx": 160, "bbox": [88, 95.81745910644531, 467.1170349121094, 288.4554138183594], "page_size": [612.0, 792.0]}
{"layout": 1275, "type": "text", "text": "32.11 v2.9.0 (01/02/2021) ", "text_level": 1, "page_idx": 160, "bbox": [71, 310, 241, 327], "page_size": [612.0, 792.0]}
{"layout": 1276, "type": "text", "text": "32.11.1 Highlights ", "text_level": 1, "page_idx": 160, "bbox": [71, 344, 177, 359], "page_size": [612.0, 792.0]}
{"layout": 1277, "type": "text", "text": "• Support new methods:  SCNet ,  Sparse R-CNN\n\n ", "page_idx": 160, "bbox": [88, 370.4103698730469, 280.7866516113281, 383.72039794921875], "page_size": [612.0, 792.0]}
{"layout": 1278, "type": "text", "text": "• Move  train_cfg  and  test_cfg  into model in configs\n\n • Support to visualize results based on prediction quality\n\n ", "page_idx": 160, "bbox": [88, 388.3433837890625, 317, 419.58642578125], "page_size": [612.0, 792.0]}
{"layout": 1279, "type": "text", "text": "32.11.2 New Features ", "text_level": 1, "page_idx": 160, "bbox": [71, 438, 196, 453], "page_size": [612.0, 792.0]}
{"layout": 1280, "type": "text", "text": "", "page_idx": 160, "bbox": [87, 465, 193, 471.75], "page_size": [612.0, 792.0]}
{"layout": 1281, "type": "text", "text": "• Support  Sparse R-CNN  (#4219)\n\n • Support evaluate mAP by multiple IoUs (#4398)\n\n • Support concatenate dataset for testing (#4452)\n\n • Support to visualize results based on prediction quality (#4441)\n\n • Add ONNX simplify option to Py torch 2 ON NX script (#4468)\n\n • Add hook for checking compatibility of class numbers in heads and datasets (#4508) ", "page_idx": 160, "bbox": [88, 482.4364013671875, 433.4336242675781, 585.4104614257812], "page_size": [612.0, 792.0]}
{"layout": 1282, "type": "text", "text": "32.11.3 Bug Fixes ", "text_level": 1, "page_idx": 161, "bbox": [70, 70, 177, 85], "page_size": [612.0, 792.0]}
{"layout": 1283, "type": "text", "text": "• Fix CPU inference bug of Cascade RPN (#4410)\n\n • Fix NMS error of CornerNet when there is no prediction box (#4409)\n\n • Fix TypeError in CornerNet inference (#4411)\n\n • Fix bug of PAA when training with background images (#4391)\n\n • Fix the error that the window data is not destroyed when  out_file is not None  and  show  $==$  False  (#4442)\n\n • Fix order of NMS  score factor  that will decrease the performance of YOLOv3 (#4473)\n\n • Fix bug in HTC TTA when the number of detection boxes is 0 (#4516)\n\n • Fix resize error in mask data structures (#4520)\n\n ", "page_idx": 161, "bbox": [88, 95.81745910644531, 539.3466796875, 234.6564178466797], "page_size": [612.0, 792.0]}
{"layout": 1284, "type": "text", "text": "32.11.4 Improvements ", "text_level": 1, "page_idx": 161, "bbox": [70, 254, 200, 268], "page_size": [612.0, 792.0]}
{"layout": 1285, "type": "text", "text": "• Allow to customize classes in LVIS dataset (#4382)\n\n • Add tutorials for building new models with existing datasets (#4396)\n\n • Add CPU compatibility information in documentation (#4405)\n\n • Add documentation of deprecated  Image To Tensor  for batch inference (#4408)\n\n • Add more details in documentation for customizing dataset (#4430)\n\n • Switch  im show det b boxes  visualization backend from OpenCV to Matplotlib (#4389)\n\n • Deprecate  Image To Tensor  in  image_demo.py  (#4400)\n\n • Move train_cfg/test_cfg into model (#4347, #4489)\n\n • Update docstring for  reg decoded b box  option in bbox heads (#4467)\n\n • Update dataset information in documentation (#4525)\n\n • Release pre-trained R50 and R101 PAA detectors with multi-scale  $3\\mathbf{X}$   training schedules (#4495)\n\n • Add guidance for speed benchmark (#4537)\n\n ", "page_idx": 161, "bbox": [88, 279.5743713378906, 481.8815612792969, 490.1455383300781], "page_size": [612.0, 792.0]}
{"layout": 1286, "type": "text", "text": "32.12 v2.8.0 (04/01/2021) ", "text_level": 1, "page_idx": 161, "bbox": [71, 512, 241, 529], "page_size": [612.0, 792.0]}
{"layout": 1287, "type": "text", "text": "32.12.1 Highlights ", "text_level": 1, "page_idx": 161, "bbox": [71, 546, 178, 561], "page_size": [612.0, 792.0]}
{"layout": 1288, "type": "text", "text": "• Support new methods:  Cascade RPN ,  TridentNet ", "page_idx": 161, "bbox": [88, 572.1005249023438, 293, 585.4105834960938], "page_size": [612.0, 792.0]}
{"layout": 1289, "type": "text", "text": "32.12.2 New Features ", "text_level": 1, "page_idx": 162, "bbox": [70, 70, 198, 85], "page_size": [612.0, 792.0]}
{"layout": 1290, "type": "text", "text": "• Support  Cascade RPN  (#1900)\n\n • Support  TridentNet  (#3313)\n\n ", "page_idx": 162, "bbox": [88, 95.81745910644531, 220.2937774658203, 127.06047821044922], "page_size": [612.0, 792.0]}
{"layout": 1291, "type": "text", "text": "32.12.3 Bug Fixes ", "text_level": 1, "page_idx": 162, "bbox": [70, 146, 176, 161], "page_size": [612.0, 792.0]}
{"layout": 1292, "type": "text", "text": "• Fix bug of show result in a sync benchmark (#4367)\n\n • Fix scale factor in Mask Test Mix in (#4366)\n\n • Fix but when returning indices in  multi class nm s  (#4362)\n\n • Fix bug of empirical attention in resnext backbone error (#4300)\n\n • Fix bug of  img norm cf g  in FCOS-HRNet models with updated performance and models (#4250)\n\n • Fix invalid checkpoint and log in Mask R-CNN models on Cityscapes dataset (#4287)\n\n • Fix bug in distributed sampler when dataset is too small (#4257)\n\n • Fix bug of ‘PAFPN has no attribute extra con vs on inputs’ (#4235)\n\n ", "page_idx": 162, "bbox": [88, 171.9784698486328, 493.7666015625, 310.8174743652344], "page_size": [612.0, 792.0]}
{"layout": 1293, "type": "text", "text": "32.12.4 Improvements ", "text_level": 1, "page_idx": 162, "bbox": [70, 330, 200, 345], "page_size": [612.0, 792.0]}
{"layout": 1294, "type": "text", "text": "• Update model url from aws to aliyun (#4349)\n\n • Update ATSS for PyTorch   $1.6+$   (#4359)\n\n • Update script to install ruby in pre-commit installation (#4360)\n\n • Delete deprecated  mmdet.ops  (#4325)\n\n • Refactor hungarian assigner for more general usage in Sparse R-CNN (#4259)\n\n • Handle scipy import in DETR to reduce package dependencies (#4339)\n\n • Update documentation of usages for config options after MMCV (1.2.3) supports overriding list in config (#4326)\n\n • Update pre-train models of faster rcnn trained on COCO subsets (#4307)\n\n • Avoid zero or too small value for beta in Dynamic R-CNN (#4303)\n\n • Add doc cu ment ation for Py torch 2 ON NX (#4271)\n\n • Add deprecated warning FPN arguments (#4264)\n\n • Support returning indices of kept bboxes when using nms (#4251)\n\n • Update type and device requirements when creating tensors  GFLHead  (#4210)\n\n • Update device requirements when creating tensors in  Cross Entropy Loss  (#4224) ", "page_idx": 162, "bbox": [88, 355.7354431152344, 540.0034790039062, 602.1715698242188], "page_size": [612.0, 792.0]}
{"layout": 1295, "type": "text", "text": "32.13 v2.7.0 (30/11/2020) ", "text_level": 1, "page_idx": 163, "bbox": [70, 70, 242, 88], "page_size": [612.0, 792.0]}
{"layout": 1296, "type": "text", "text": "• Support new method:  DETR ,  ResNest , Faster R-CNN DC5.\n\n • Support YOLO, Mask R-CNN, and Cascade R-CNN models exportable to ONNX.\n\n ", "page_idx": 163, "bbox": [88, 102.99446105957031, 425.8719482421875, 134.2364959716797], "page_size": [612.0, 792.0]}
{"layout": 1297, "type": "text", "text": "32.13.1 New Features ", "text_level": 1, "page_idx": 163, "bbox": [71, 153, 197, 168], "page_size": [612.0, 792.0]}
{"layout": 1298, "type": "text", "text": "• Support  DETR  (#4201, #4206)\n\n • Support to link the best checkpoint in training (#3773)\n\n • Support to override config through options in inference.py (#4175)\n\n • Support YOLO, Mask R-CNN, and Cascade R-CNN models exportable to ONNX (#4087, #4083)\n\n • Support  ResNeSt  backbone (#2959)\n\n • Support unclip border bbox regression (#4076)\n\n • Add tpfp func in evaluating AP (#4069)\n\n • Support mixed precision training of SSD detector with other backbones (#4081)\n\n • Add Faster R-CNN DC5 models (#4043)\n\n ", "page_idx": 163, "bbox": [88, 179.03846740722656, 487.30126953125, 335.81048583984375], "page_size": [612.0, 792.0]}
{"layout": 1299, "type": "text", "text": "32.13.2 Bug Fixes ", "text_level": 1, "page_idx": 163, "bbox": [71, 355, 175, 369], "page_size": [612.0, 792.0]}
{"layout": 1300, "type": "text", "text": "• Fix bug of  gpu_id  in distributed training mode (#4163)\n\n • Support Albumen tat ions with version higher than 0.5 (#4032)\n\n • Fix num classes bug in faster rcnn config (#4088)\n\n • Update code in docs/2 new data model.md (#4041)\n\n ", "page_idx": 163, "bbox": [88, 380.6124572753906, 343, 447.7215270996094], "page_size": [612.0, 792.0]}
{"layout": 1301, "type": "text", "text": "32.13.3 Improvements ", "text_level": 1, "page_idx": 163, "bbox": [71, 467, 199, 481], "page_size": [612.0, 792.0]}
{"layout": 1302, "type": "text", "text": "• Ensure DCN offset to have similar type as features in VFNet (#4198)\n\n • Add config links in README files of models (#4190)\n\n • Add tutorials for loss conventions (#3818)\n\n • Add solution to installation issues in 30-series GPUs (#4176)\n\n • Update docker version in get started.md (#4145)\n\n • Add model statistics and polish some titles in configs README (#4140)\n\n • Clamp neg probability in FreeAnchor (#4082)\n\n • Speed up expanding large images (#4089)\n\n • Fix Pytorch 1.7 incompatibility issues (#4103)\n\n • Update trouble shooting page to resolve segmentation fault (#4055)\n\n • Update aLRP-Loss in project page (#4078)\n\n • Clean duplicated  reduce mean  function (#4056)\n\n • Refactor Q&A (#4045) ", "page_idx": 163, "bbox": [88, 492.52349853515625, 386.888427734375, 721.0264892578125], "page_size": [612.0, 792.0]}
{"layout": 1303, "type": "text", "text": "32.14 v2.6.0 (1/11/2020) ", "text_level": 1, "page_idx": 164, "bbox": [71, 71, 234, 88], "page_size": [612.0, 792.0]}
{"layout": 1304, "type": "text", "text": "• Support new method:  Var i focal Net .\n\n • Refactored documentation with more tutorials.\n\n ", "page_idx": 164, "bbox": [88, 102.99446105957031, 282.8290100097656, 134.2364959716797], "page_size": [612.0, 792.0]}
{"layout": 1305, "type": "text", "text": "32.14.1 New Features ", "text_level": 1, "page_idx": 164, "bbox": [70, 153, 197, 168], "page_size": [612.0, 792.0]}
{"layout": 1306, "type": "text", "text": "• Support GIoU calculation in  B box Overlaps 2 D , and re-implement  giou_loss  using  b box overlaps  (#3936)\n\n • Support random sampling in CPU mode (#3948)\n\n • Support VarifocalNet (#3666, #4024)\n\n ", "page_idx": 164, "bbox": [88, 179.1554718017578, 539, 228.3304901123047], "page_size": [612.0, 792.0]}
{"layout": 1307, "type": "text", "text": "32.14.2 Bug Fixes ", "text_level": 1, "page_idx": 164, "bbox": [70, 247, 176, 262], "page_size": [612.0, 792.0]}
{"layout": 1308, "type": "text", "text": "• Fix SABL validating bug in Cascade R-CNN (#3913)\n\n • Avoid division by zero in PAA head when num_pos  $\\mathrm{=}0$   (#3938)\n\n • Fix temporary directory bug of multi-node testing error (#4034, #4017)\n\n • Fix  --show-dir  option in test script (#4025)\n\n • Fix GA-RetinaNet r50 model url (#3983)\n\n • Update code in docs and fix broken urls (#3947)\n\n ", "page_idx": 164, "bbox": [88, 273.2484436035156, 381.7076721191406, 376.2225341796875], "page_size": [612.0, 792.0]}
{"layout": 1309, "type": "text", "text": "32.14.3 Improvements ", "text_level": 1, "page_idx": 164, "bbox": [70, 395, 201, 410], "page_size": [612.0, 792.0]}
{"layout": 1310, "type": "text", "text": "• Refactor py torch 2 on nx API into  mmdet.core.export  and use  generate inputs and wrap model  for py- torch2onnx (#3857, #3912)\n\n ", "page_idx": 164, "bbox": [88, 421.1405029296875, 539, 446.405517578125], "page_size": [612.0, 792.0]}
{"layout": 1311, "type": "text", "text": "• Update RPN upgrade scripts for v2.5.0 compatibility (#3986)\n\n ", "page_idx": 164, "bbox": [88, 451.02850341796875, 341.3990173339844, 464.3385314941406], "page_size": [612.0, 792.0]}
{"layout": 1312, "type": "text", "text": "• Use mmcv  tensor 2 img s  (#4010)\n\n ", "page_idx": 164, "bbox": [88, 468.9615173339844, 233.902587890625, 482.27154541015625], "page_size": [612.0, 792.0]}
{"layout": 1313, "type": "text", "text": "• Update test robustness (#4000)\n\n ", "page_idx": 164, "bbox": [88, 486.8935241699219, 219.9949188232422, 500.20355224609375], "page_size": [612.0, 792.0]}
{"layout": 1314, "type": "text", "text": "• Update trouble shooting page (#3994)\n\n ", "page_idx": 164, "bbox": [88, 504.8265075683594, 247, 518.1365356445312], "page_size": [612.0, 792.0]}
{"layout": 1315, "type": "text", "text": "• Accelerate PAA training speed (#3985)\n\n ", "page_idx": 164, "bbox": [88, 522.759521484375, 253.4094696044922, 536.069580078125], "page_size": [612.0, 792.0]}
{"layout": 1316, "type": "text", "text": "• Support batch_size  $>1$   in validation (#3966)\n\n ", "page_idx": 164, "bbox": [88, 540.6925048828125, 275.0482177734375, 554.0025634765625], "page_size": [612.0, 792.0]}
{"layout": 1317, "type": "text", "text": "• Use RoIAlign implemented in MMCV for inference in CPU mode (#3930) ", "page_idx": 164, "bbox": [88, 558.62451171875, 394.6591796875, 571.9345703125], "page_size": [612.0, 792.0]}
{"layout": 1318, "type": "text", "text": "32.15 v2.5.0 (5/10/2020) ", "text_level": 1, "page_idx": 165, "bbox": [71, 70, 234, 88], "page_size": [612.0, 792.0]}
{"layout": 1319, "type": "text", "text": "32.15.1 Highlights ", "page_idx": 165, "bbox": [71, 103.08335876464844, 176.30914306640625, 120.20320892333984], "page_size": [612.0, 792.0]}
{"layout": 1320, "type": "text", "text": "• Support new methods:  YOLACT ,  Centripetal Net . • Add more documentation s for easier and more clear usage. ", "page_idx": 165, "bbox": [88, 130.38844299316406, 330.848876953125, 161.63047790527344], "page_size": [612.0, 792.0]}
{"layout": 1321, "type": "text", "text": "32.15.2 Backwards Incompatible Changes ", "text_level": 1, "page_idx": 165, "bbox": [71, 181, 313, 195], "page_size": [612.0, 792.0]}
{"layout": 1322, "type": "text", "text": "FP16 related methods are imported from mmcv instead of mmdet. (#3766, #3822)  Mixed precision training utils in  mmdet.core.fp16  are moved to  mmcv.runner , including  force_fp32 ,  auto_fp16 ,  wrap fp 16 model , and Fp 16 Optimizer Hook . A deprecation warning will be raised if users attempt to import those methods from  mmdet. core.fp16 , and will be finally removed in V2.10.0. ", "page_idx": 165, "bbox": [71, 205.9207763671875, 540, 255.72447204589844], "page_size": [612.0, 792.0]}
{"layout": 1323, "type": "text", "text": "[0, N-1] represents foreground classes and N indicates background classes for all models. (#3221)  Before v2.5.0, the background label for RPN is 0, and N for other heads. Now the behavior is consistent for all models. Thus  self. background labels  in  dense heads  is removed and all heads use  self.num classes  to indicate the class index of background labels. This change has no effect on the pre-trained models in the  $\\mathrm{v}2.\\mathrm{x}$   model zoo, but will affect the training of all models with RPN heads. Two-stage detectors whose RPN head uses softmax will be affected because the order of categories is changed. ", "page_idx": 165, "bbox": [71, 259.71978759765625, 540, 333.43341064453125], "page_size": [612.0, 792.0]}
{"layout": 1324, "type": "text", "text": "Only call  get subset by classes  when  test_mode  $\\risingdotseq$  True  and  self.filter empty gt  $\\risingdotseq$  True  (#3695)  Func- tion  get subset by classes  in dataset is refactored and only filters out images when  test_mode  $=$  True  and  self. filter empty gt  $\\risingdotseq$  True . In the original implementation,  get subset by classes  is not related to the flag  self. filter empty gt  and will only be called when the classes is set during initialization no matter  test_mode  is  True  or False . This brings ambiguous behavior and potential bugs in many cases. After v2.5.0, if  filter empty gt  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  False , no matter whether the classes are specified in a dataset, the dataset will use all the images in the annotations. If filter empty gt  $\\risingdotseq$  True  and  test_mode  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  True , no matter whether the classes are specified, the dataset will call \\`\\`get subset by classes\\` to check the images and filter out images containing no GT boxes. Therefore, the users should be responsible for the data filtering/cleaning process for the test dataset. ", "page_idx": 165, "bbox": [71, 337.427734375, 540, 447.00732421875], "page_size": [612.0, 792.0]}
{"layout": 1325, "type": "text", "text": "32.15.3 New Features ", "text_level": 1, "page_idx": 165, "bbox": [71, 466, 197, 480], "page_size": [612.0, 792.0]}
{"layout": 1326, "type": "text", "text": "• Test time augmentation for single stage detectors (#3844, #3638) • Support to show the name of experiments during training (#3764) • Add Shear, Rotate, Translate Augmentation (#3656, #3619, #3687)• Add image-only transformations including  Constrast ,  Equalize ,  Color , and  Brightness . (#3643) • Support  YOLACT  (#3456) • Support  Centripetal Net  (#3390) • Support PyTorch 1.6 in docker (#3905) ", "page_idx": 165, "bbox": [88, 491.92529296875, 505.31463623046875, 612.8313598632812], "page_size": [612.0, 792.0]}
{"layout": 1327, "type": "text", "text": "32.15.4 Bug Fixes ", "text_level": 1, "page_idx": 166, "bbox": [70, 69, 177, 86], "page_size": [612.0, 792.0]}
{"layout": 1328, "type": "text", "text": "• Fix the bug of training ATSS when there is no ground truth boxes (#3702)\n\n • Fix the bug of using Focal Loss when there is  num_pos  is 0 (#3702)\n\n • Fix the label index mapping in dataset browser (#3708)\n\n • Fix Mask R-CNN training stuck problem when their is no positive rois (#3713)\n\n • Fix the bug of  self.rpn_head.test_cfg  in  RP N Test Mix in  by using  self.rpn_head  in rpn head (#3808)\n\n • Fix deprecated  Conv2d  from mmcv.ops (#3791)\n\n • Fix device bug in RepPoints (#3836)\n\n • Fix SABL validating bug (#3849)\n\n • Use  https://download.openmmlab.com/mmcv/dist/index.html  for installing MMCV (#3840)\n\n • Fix nonzero in NMS for PyTorch 1.6.0 (#3867)\n\n • Fix the API change bug of PAA (#3883)\n\n • Fix typo in bbox_flip (#3886)\n\n • Fix cv2 import error of ligGL.so.1 in Dockerfile (#3891)\n\n ", "page_idx": 166, "bbox": [88, 95.81745910644531, 535.2120971679688, 324.3204345703125], "page_size": [612.0, 792.0]}
{"layout": 1329, "type": "text", "text": "32.15.5 Improvements ", "text_level": 1, "page_idx": 166, "bbox": [70, 344, 200, 358], "page_size": [612.0, 792.0]}
{"layout": 1330, "type": "text", "text": "• Change to use  mmcv.utils.collect en v  for collecting environment information to avoid duplicate codes (#3779)\n\n • Update checkpoint file names to   $\\mathrm{v}2.0$   models in documentation (#3795)\n\n • Update tutorials for changing runtime settings (#3778), modifying loss (#3777)\n\n • Improve the function of  simple test b boxes  in SABL (#3853)\n\n • Convert mask to bool before using it as img’s index for robustness and speedup (#3870)\n\n • Improve documentation of modules and dataset customization (#3821)\n\n ", "page_idx": 166, "bbox": [88, 369.2384033203125, 540, 484.16748046875], "page_size": [612.0, 792.0]}
{"layout": 1331, "type": "text", "text": "32.16 v2.4.0 (5/9/2020) ", "text_level": 1, "page_idx": 166, "bbox": [71, 506, 225, 523], "page_size": [612.0, 792.0]}
{"layout": 1332, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 166, "bbox": [71, 540, 117, 552], "page_size": [612.0, 792.0]}
{"layout": 1333, "type": "text", "text": "• Fix lots of issues/bugs and reorganize the trouble shooting page\n\n • Support new methods  SABL ,  YOLOv3 , and  PAA Assign\n\n • Support Batch Inference\n\n • Start to publish  mmdet  package to PyPI since v2.3.0\n\n • Switch model zoo to download.openmmlab.com\n\n ", "page_idx": 166, "bbox": [88, 556.7313842773438, 349.9072265625, 641.7724609375], "page_size": [612.0, 792.0]}
{"layout": 1334, "type": "text", "text": "Backwards Incompatible Changes ", "text_level": 1, "page_idx": 166, "bbox": [71, 647, 218, 660], "page_size": [612.0, 792.0]}
{"layout": 1335, "type": "text", "text": "• Support Batch Inference (#3564, #3686, #3705): Since v2.4.0, MM Detection could inference model with mul- tiple images in a single GPU. This change influences all the test APIs in MM Detection and downstream code- bases. To help the users migrate their code, we use  replace Image To Tensor  (#3686) to convert legacy test data pipelines during dataset initialization. ", "page_idx": 166, "bbox": [88, 664.328369140625, 540, 713.5034790039062], "page_size": [612.0, 792.0]}
{"layout": 1336, "type": "text", "text": "• Support RandomFlip with horizontal/vertical/diagonal direction (#3608): Since v2.4.0, MM Detection supports horizontal/vertical/diagonal flip in the data augmentation. This influences bounding box, mask, and image trans- formations in data augmentation process and the process that will map those data back to the original format.\n\n • Migrate to use  mmlvis  and  mm py coco tools  for COCO and LVIS dataset (#3727). The APIs are fully compatible with the original  lvis  and  py coco tools . Users need to uninstall the existing py coco tools and lvis packages in their environment first and install  mmlvis  &  mm py coco tools .\n\n ", "page_idx": 167, "bbox": [88, 71.45246887207031, 540, 150.5155487060547], "page_size": [612.0, 792.0]}
{"layout": 1337, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 167, "bbox": [71, 156, 114, 168], "page_size": [612.0, 792.0]}
{"layout": 1338, "type": "text", "text": "• Fix default mean/std for onnx (#3491)\n\n • Fix coco evaluation and add metric items (#3497)\n\n • Fix typo for install.md (#3516)\n\n • Fix atss when sampler per gpu is 1 (#3528)\n\n • Fix import of fuse con v bn (#3529)\n\n • Fix bug of gaussian target, update unittest of heatmap (#3543)\n\n • Fixed VOC2012 evaluate (#3553)\n\n • Fix scale factor bug of rescale (#3566)\n\n • Fix with xxx attributes in base detector (#3567)\n\n • Fix boxes scaling when number is 0 (#3575)\n\n • Fix rfp check when neck config is a list (#3591)\n\n • Fix import of fuse conv bn in benchmark.py (#3606)\n\n • Fix webcam demo (#3634)\n\n • Fix typo and itemize issues in tutorial (#3658)\n\n • Fix error in distributed training when some levels of FPN are not assigned with bounding boxes (#3670)\n\n • Fix the width and height orders of stride in valid flag generation (#3685)\n\n • Fix weight initialization bug in Res2Net DCN (#3714)\n\n • Fix bug in OH EM Sampler (#3677)\n\n ", "page_idx": 167, "bbox": [88, 173.0714874267578, 511.4407043457031, 491.2376403808594], "page_size": [612.0, 792.0]}
{"layout": 1339, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 167, "bbox": [71, 497, 130, 509], "page_size": [612.0, 792.0]}
{"layout": 1340, "type": "text", "text": "• Support Cutout augmentation (#3521)\n\n • Support evaluation on multiple datasets through Con cat Data set (#3522)\n\n • Support  PAA assign  #(3547)\n\n • Support eval metric with pickle results (#3607)\n\n • Support  YOLOv3  (#3083)\n\n • Support  SABL  (#3603)\n\n • Support to publish to Pypi in github-action (#3510)\n\n • Support custom imports (#3641)\n\n ", "page_idx": 167, "bbox": [88, 513.7936401367188, 385, 652.6326293945312], "page_size": [612.0, 792.0]}
{"layout": 1341, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 167, "bbox": [71, 659, 133, 670], "page_size": [612.0, 792.0]}
{"layout": 1342, "type": "text", "text": "• Refactor common issues in documentation (#3530)\n\n • Add pytorch 1.6 to CI config (#3532) ", "page_idx": 167, "bbox": [88, 675.1885986328125, 301, 706.4306640625], "page_size": [612.0, 792.0]}
{"layout": 1343, "type": "text", "text": "• Add config to runner meta (#3534)\n\n • Add eval-option flag for testing (#3537)\n\n • Add init_eval to evaluation hook (#3550)\n\n • Add include b kg in Class Balanced Data set (#3577)\n\n • Using config’s loading in inference detector (#3611)\n\n • Add ATSS ResNet-101 models in model zoo (#3639)\n\n • Update urls to download.openmmlab.com (#3665)\n\n • Support non-mask training for Coco Data set (#3711)\n\n ", "page_idx": 168, "bbox": [88, 71.45246887207031, 309, 210.2914276123047], "page_size": [612.0, 792.0]}
{"layout": 1344, "type": "text", "text": "32.17 v2.3.0 (5/8/2020) ", "text_level": 1, "page_idx": 168, "bbox": [70, 233, 226, 249], "page_size": [612.0, 792.0]}
{"layout": 1345, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 168, "bbox": [71, 266, 117, 278], "page_size": [612.0, 792.0]}
{"layout": 1346, "type": "text", "text": "• The   $\\mathrm{ttCUDA/C++}$   operators have been moved to  mmcv.ops . For backward compatibility  mmdet.ops  is kept as warppers of  mmcv.ops .\n\n • Support new methods  CornerNet ,  DIOU / CIOU  loss, and new dataset:  LVIS V1\n\n • Provide more detailed colab training tutorials and more complete documentation.\n\n • Support to convert RetinaNet from Pytorch to ONNX.\n\n ", "page_idx": 168, "bbox": [88, 282.85540771484375, 540.0032348632812, 361.91845703125], "page_size": [612.0, 792.0]}
{"layout": 1347, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 168, "bbox": [71, 367, 115, 380], "page_size": [612.0, 792.0]}
{"layout": 1348, "type": "text", "text": "• Fix the model initialization bug of DetectoRS (#3187)\n\n • Fix the bug of module names in NAS FCO S Head (#3205)\n\n • Fix the filename bug in publish model.py (#3237)\n\n • Fix the dimensionality bug when  inside flags.any()  is  False  in dense heads (#3242)\n\n • Fix the bug of forgetting to pass flip directions in  Multi Scale Flip Aug  (#3262)\n\n • Fixed the bug caused by default value of  stem channels  (#3333)\n\n • Fix the bug of model checkpoint loading for CPU inference (#3318, #3316)\n\n • Fix topk bug when box number is smaller than the expected topk number in AT S S As signer (#3361)\n\n • Fix the gt priority bug in center region as signer.py (#3208)\n\n • Fix NaN issue of iou calculation in iou_loss.py (#3394)\n\n • Fix the bug that  iou_thrs  is not actually used during evaluation in coco.py (#3407)\n\n • Fix test-time augmentation of RepPoints (#3435)\n\n • Fix runtime Error caused by in contiguous tensor in Res2Net+DCN (#3412)\n\n ", "page_idx": 168, "bbox": [88, 384.4744567871094, 493.3586120605469, 612.9776000976562], "page_size": [612.0, 792.0]}
{"layout": 1349, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 168, "bbox": [71, 619, 131, 631], "page_size": [612.0, 792.0]}
{"layout": 1350, "type": "text", "text": "• Support  CornerNet  (#3036)\n\n • Support  DIOU / CIOU  loss (#3151)\n\n • Support  LVIS V1  dataset (#)\n\n • Support customized hooks in training (#3395)\n\n • Support fp16 training of generalized focal loss (#3410) ", "page_idx": 168, "bbox": [88, 635.5325317382812, 317, 720.5735473632812], "page_size": [612.0, 792.0]}
{"layout": 1351, "type": "text", "text": "• Support to convert RetinaNet from Pytorch to ONNX (#3075)\n\n ", "page_idx": 169, "bbox": [88, 71.45246887207031, 344.1387939453125, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 1352, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 169, "bbox": [71, 90, 135, 102], "page_size": [612.0, 792.0]}
{"layout": 1353, "type": "text", "text": "• Support to process ignore boxes in ATSS assigner (#3082)\n\n • Allow to crop images without ground truth in  RandomCrop  (#3153)\n\n • Enable the the  Accuracy  module to set threshold (#3155)\n\n • Refactoring unit tests (#3206)\n\n • Unify the training settings of  to_float32  and  norm_cfg  in RegNets configs (#3210)\n\n • Add colab training tutorials for beginners (#3213, #3273)\n\n • Move   $\\mathrm{ttCUDA/C++}$   operators into  mmcv.ops  and keep  mmdet.ops  as warppers for backward compatibility (#3232)(#3457)\n\n • Update installation scripts in documentation (#3290) and dockerfile (#3320)\n\n • Support to set image resize backend (#3392)\n\n • Remove git hash in version file (#3466)\n\n • Check mmcv version to force version compatibility (#3460)\n\n ", "page_idx": 169, "bbox": [88, 107.31745910644531, 540.0004272460938, 311.91046142578125], "page_size": [612.0, 792.0]}
{"layout": 1354, "type": "text", "text": "32.18 v2.2.0 (1/7/2020) ", "text_level": 1, "page_idx": 169, "bbox": [70, 334, 226, 351], "page_size": [612.0, 792.0]}
{"layout": 1355, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 169, "bbox": [71, 367, 117, 380], "page_size": [612.0, 792.0]}
{"layout": 1356, "type": "text", "text": "• Support new methods:  DetectoRS ,  PointRend ,  Generalized Focal Loss ,  Dynamic R-CNN\n\n ", "page_idx": 169, "bbox": [88, 384.47442626953125, 452.9006652832031, 397.7844543457031], "page_size": [612.0, 792.0]}
{"layout": 1357, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 169, "bbox": [71, 403, 114, 416], "page_size": [612.0, 792.0]}
{"layout": 1358, "type": "text", "text": "• Fix FreeAnchor when no gt in image (#3176)\n\n • Clean up deprecated usage of  register module()  (#3092, #3161)\n\n • Fix pretrain bug in NAS FCOS (#3145)\n\n • Fix  num classes  in SSD (#3142)\n\n • Fix FCOS warmup (#3119)\n\n • Fix  rstrip  in  tools/publish model.py\n\n • Fix  flip_ratio  default value in RandomFLip pipeline (#3106)\n\n • Fix cityscapes eval with ms_rcnn (#3112)\n\n • Fix RPN softmax (#3056)\n\n • Fix filename of LVIS@v0.5 (#2998)\n\n • Fix nan loss by filtering out-of-frame gt_bboxes in COCO (#2999)\n\n • Fix bug in FSAF (#3018)\n\n • Add FocalLoss  num classes  check (#2964)\n\n • Fix PISA Loss when there are no gts (#2992)\n\n • Avoid nan in  i ou calculator  (#2975)\n\n • Prevent possible bugs in loading and transforms caused by shallow copy (#2967) ", "page_idx": 169, "bbox": [88, 420.3394470214844, 418.08123779296875, 702.6415405273438], "page_size": [612.0, 792.0]}
{"layout": 1359, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 169, "bbox": [71, 709, 130, 720], "page_size": [612.0, 792.0]}
{"layout": 1360, "type": "text", "text": "• Add DetectoRS (#3064)\n\n• Support Generalize Focal Loss (#3097)\n\n • Support PointRend (#2752)\n\n • Support Dynamic R-CNN (#3040)\n\n • Add Deep Fashion dataset (#2968)\n\n • Implement FCOS training tricks (#2935)\n\n • Use Base Dense Head as base class for anchor-base heads (#2963)\n\n • Add  with_cp  for BasicBlock (#2891)\n\n • Add  stem channels  argument for ResNet (#2954)\n\n ", "page_idx": 170, "bbox": [88, 71.45246887207031, 355.9545593261719, 228.2244110107422], "page_size": [612.0, 792.0]}
{"layout": 1361, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 170, "bbox": [71, 234, 134, 247], "page_size": [612.0, 792.0]}
{"layout": 1362, "type": "text", "text": "• Add anchor free base head (#2867)\n\n • Migrate to github action (#3137)\n\n • Add docstring for datasets, pipelines, core modules and methods (#3130, #3125, #3120)\n\n • Add VOC benchmark (#3060)\n\n • Add  concat  mode in GRoI (#3098)\n\n • Remove cmd arg auto re scale-lr (#3080)\n\n• Use  len(data[ ' img_metas ' ])  to indicate  num samples  (#3073, #3053)\n\n • Switch to Epoch Based Runner (#2976)\n\n ", "page_idx": 170, "bbox": [88, 250.7793731689453, 447.4012145996094, 389.61846923828125], "page_size": [612.0, 792.0]}
{"layout": 1363, "type": "text", "text": "32.19 v2.1.0 (8/6/2020) ", "text_level": 1, "page_idx": 170, "bbox": [71, 412, 225, 428], "page_size": [612.0, 792.0]}
{"layout": 1364, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 170, "bbox": [71, 445, 117, 457], "page_size": [612.0, 792.0]}
{"layout": 1365, "type": "text", "text": "• Support new backbones:  RegNetX ,  Res2Net\n\n • Support new methods:  NASFCOS ,  PISA ,  GRoIE\n\n • Support new dataset:  LVIS\n\n ", "page_idx": 170, "bbox": [88, 462.1834411621094, 293.3594970703125, 511.3584899902344], "page_size": [612.0, 792.0]}
{"layout": 1366, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 170, "bbox": [71, 517, 114, 529], "page_size": [612.0, 792.0]}
{"layout": 1367, "type": "text", "text": "• Change the CLI argument  --validate  to  --no-validate  to enable validation after training epochs by default. (#2651)\n\n • Add missing cython to docker file (#2713)\n\n • Fix bug in nms cpu implementation (#2754)\n\n • Fix bug when showing mask results (#2763)\n\n • Fix gcc requirement (#2806)\n\n • Fix bug in async test (#2820)\n\n • Fix mask encoding-decoding bugs in test API (#2824)\n\n • Fix bug in test time augmentation (#2858, #2921, #2944)\n\n • Fix a typo in comment of apis/train (#2877) ", "page_idx": 170, "bbox": [88, 533.9144287109375, 539.9960327148438, 702.6414794921875], "page_size": [612.0, 792.0]}
{"layout": 1368, "type": "text", "text": "• Fix the bug of returning None when no gt bboxes are in the original image in  RandomCrop . Fix the bug that misses to handle  gt b boxes ignore ,  gt label ignore , and  gt masks ignore  in  RandomCrop , MinI oU Random Crop  and  Expand  modules. (#2810)\n\n • Fix bug of  base channels  of regnet (#2917)\n\n • Fix the bug of logger when loading pre-trained weights in base detector (#2936)\n\n ", "page_idx": 171, "bbox": [88, 71.45246887207031, 539, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 1369, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 171, "bbox": [71, 150, 131, 162], "page_size": [612.0, 792.0]}
{"layout": 1370, "type": "text", "text": "• Add IoU models (#2666)\n\n • Add colab demo for inference\n\n • Support class agnostic nms (#2553)\n\n • Add benchmark gathering scripts for development only (#2676)\n\n • Add mmdet-based project links (#2736, #2767, #2895)\n\n • Add config dump in training (#2779)\n\n • Add Class Balanced Data set (#2721)\n\n • Add res2net backbone (#2237)\n\n • Support RegNetX models (#2710)\n\n • Use  mmcv.FileClient  to support different storage backends (#2712)\n\n • Add Class Balanced Data set (#2721)\n\n • Code Release: Prime Sample Attention in Object Detection (CVPR 2020) (#2626)\n\n • Implement NASFCOS (#2682)\n\n • Add class weight in Cross Entropy Loss (#2797)\n\n • Support LVIS dataset (#2088)\n\n • Support GRoIE (#2584)\n\n ", "page_idx": 171, "bbox": [88, 167.0934600830078, 425.6727294921875, 449.3945617675781], "page_size": [612.0, 792.0]}
{"layout": 1371, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 171, "bbox": [71, 455, 134, 467], "page_size": [612.0, 792.0]}
{"layout": 1372, "type": "text", "text": "• Allow different x and y strides in anchor heads. (#2629)\n\n • Make FSAF loss more robust to no gt (#2680)\n\n • Compute pure inference time instead (#2657) and update inference speed (#2730)\n\n • Avoided the possibility that a patch with 0 area is cropped. (#2704)\n\n • Add warnings when deprecated  img s per gpu  is used. (#2700)\n\n • Add a mask rcnn example for config (#2645)\n\n • Update model zoo (#2762, #2866, #2876, #2879, #2831)\n\n • Add  ori filename  to img_metas and use it in test show-dir (#2612)\n\n • Use  img_fields  to handle multiple images during image transform (#2800)\n\n • Add up sample cf g support in FPN (#2787)\n\n • Add  [ ' img ' ]  as default  img_fields  for back compatibility (#2809)\n\n • Rename the pretrained model from open-mmlab://res net 50 caff e and open-mmlab:// res net 50 caff e b gr  to  open-mmlab://detectron/res net 50 caff e  and  open-mmlab://detectron2/ res net 50 caff e . (#2832) • Added sleep(2) in test.py to reduce hanging problem (#2847) • Support  c10::half  in CARAFE (#2890) • Improve documentation s (#2918, #2714) • Use optimizer constructor in mmcv and clean the original implementation in  mmdet.core.optimizer  (#2947) ", "page_idx": 171, "bbox": [88, 471.9505615234375, 539, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 1373, "type": "text", "text": "", "page_idx": 172, "bbox": [88, 71.45246887207031, 540, 138.5604705810547], "page_size": [612.0, 792.0]}
{"layout": 1374, "type": "text", "text": "32.20 v2.0.0 (6/5/2020) ", "text_level": 1, "page_idx": 172, "bbox": [70, 161, 226, 177], "page_size": [612.0, 792.0]}
{"layout": 1375, "type": "text", "text": "In this release, we made lots of major refactoring and modifications. ", "page_idx": 172, "bbox": [72, 193.12245178222656, 343.1122131347656, 206.43247985839844], "page_size": [612.0, 792.0]}
{"layout": 1376, "type": "text", "text": "1.  Faster speed . We optimize the training and inference speed for common models, achieving up to  $30\\%$   speedup for training and  $25\\%$   for inference. Please refer to  model zoo  for details. 2.  Higher performance . We change some default hyper parameters with no additional cost, which leads to a gain of performance for most models. Please refer to  compatibility  for details. 3.  More documentation and tutorials . We add a bunch of documentation and tutorials to help users get started more smoothly. Read it  here . 4.  Support PyTorch 1.5 . The support for 1.1 and 1.2 is dropped, and we switch to some new APIs. 5.  Better configuration system . Inheritance is supported to reduce the redundancy of configs. 6.  Better modular design . Towards the goal of simplicity and flexibility, we simplify some encapsulation while add more other configurable modules like BBoxCoder, I oU Calculator, Optimizer Constructor, RoIHead. Target computation is also included in heads and the call hierarchy is simpler. 7. Support new methods:  FSAF  and PAFPN (part of  PAFPN ). ", "page_idx": 172, "bbox": [84, 210.42681884765625, 540, 391.737548828125], "page_size": [612.0, 792.0]}
{"layout": 1377, "type": "text", "text": "Breaking Changes  Models training with MM Detection 1.x are not fully compatible with 2.0, please refer to the  com- patibility doc  for the details and how to migrate to the new version. ", "page_idx": 172, "bbox": [72, 395.73187255859375, 540, 421.62554931640625], "page_size": [612.0, 792.0]}
{"layout": 1378, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 172, "bbox": [71, 427, 134, 439], "page_size": [612.0, 792.0]}
{"layout": 1379, "type": "text", "text": "• Unify cuda and cpp API for custom ops. (#2277) • New config files with inheritance. (#2216) • Encapsulate the second stage into RoI heads. (#1999) • Refactor GCNet/Em per ical Attention into plugins. (#2345) • Set low quality match as an option in IoU-based bbox assigners. (#2375) • Change the codebase’s coordinate system. (#2380) • Refactor the category order in heads. 0 means the first positive class instead of background now. (#2374) • Add bbox sampler and assigner registry. (#2419) • Speed up the inference of RPN. (#2420) • Add  train_cfg  and  test_cfg  as class members in all anchor heads. (#2422) • Merge target computation methods into heads. (#2429) • Add bbox coder to support different bbox encoding and losses. (#2480) • Unify the API for regression loss. (#2156) • Refactor Anchor Generator. (#2474) • Make  lr  an optional argument for optimizers. (#2509) ", "page_idx": 172, "bbox": [88, 444.1805419921875, 514.7184448242188, 708.5496215820312], "page_size": [612.0, 792.0]}
{"layout": 1380, "type": "text", "text": "• Migrate to modules and methods in MMCV. (#2502, #2511, #2569, #2572)\n\n • Support PyTorch 1.5. (#2524)\n\n • Drop the support for Python 3.5 and use F-string in the codebase. (#2531)\n\n ", "page_idx": 173, "bbox": [88, 71.45246887207031, 397.5482482910156, 120.62749481201172], "page_size": [612.0, 792.0]}
{"layout": 1381, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 173, "bbox": [71, 125, 115, 138], "page_size": [612.0, 792.0]}
{"layout": 1382, "type": "text", "text": "• Fix the scale factors for resized images without keep the aspect ratio. (#2039)\n\n • Check if max_num  $>0$   before slicing in NMS. (#2486)\n\n • Fix Deformable RoIPool when there is no instance. (#2490)\n\n • Fix the default value of assigned labels. (#2536)\n\n • Fix the evaluation of Cityscapes. (#2578)\n\n ", "page_idx": 173, "bbox": [88, 143.1834259033203, 405.5383605957031, 228.2244110107422], "page_size": [612.0, 792.0]}
{"layout": 1383, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 173, "bbox": [70, 233, 130, 246], "page_size": [612.0, 792.0]}
{"layout": 1384, "type": "text", "text": "• Add deep_stem and avg_down option to ResNet, i.e., support ResNetV1d. (#2252)\n\n • Add L1 loss. (#2376)\n\n • Support both polygon and bitmap for instance masks. (#2353, #2540)\n\n • Support CPU mode for inference. (#2385)\n\n • Add optimizer constructor for complicated configuration of optimizers. (#2397, #2488)\n\n • Implement PAFPN. (#2392)\n\n • Support empty tensor input for some modules. (#2280)\n\n • Support for custom dataset classes without overriding it. (#2408, #2443)\n\n • Support to train subsets of coco dataset. (#2340)\n\n • Add i ou calculator to potentially support more IoU calculation methods. (2405)\n\n • Support class wise mean AP (was removed in the last version). (#2459)\n\n • Add option to save the testing result images. (#2414)\n\n • Support Momentum Updater Hook. (#2571)\n\n • Add a demo to inference a single image. (#2605)\n\n ", "page_idx": 173, "bbox": [88, 250.7793731689453, 444.9005126953125, 497.2155456542969], "page_size": [612.0, 792.0]}
{"layout": 1385, "type": "text", "text": "32.21 v1.1.0 (24/2/2020) ", "text_level": 1, "page_idx": 173, "bbox": [71, 519, 233, 536], "page_size": [612.0, 792.0]}
{"layout": 1386, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 173, "bbox": [71, 553, 117, 565], "page_size": [612.0, 792.0]}
{"layout": 1387, "type": "text", "text": "• Dataset evaluation is rewritten with a unified api, which is used by both evaluation hooks and test scripts.\n\n • Support new methods:  CARAFE .\n\n ", "page_idx": 173, "bbox": [88, 569.7794799804688, 515.1867065429688, 601.0225219726562], "page_size": [612.0, 792.0]}
{"layout": 1388, "type": "text", "text": "Breaking Changes ", "text_level": 1, "page_idx": 173, "bbox": [71, 607, 151, 619], "page_size": [612.0, 792.0]}
{"layout": 1389, "type": "text", "text": "• The new MMDDP inherits from the official DDP, thus the  __init__  api is changed to be the same as official DDP.\n\n • The  mask_head  field in HTC config files is modified.\n\n • The evaluation and testing script is updated.\n\n • In all transforms, instance masks are stored as a numpy array shaped (n, h, w) instead of a list of (h, w) arrays, where n is the number of instances. ", "page_idx": 173, "bbox": [88, 623.5774536132812, 540, 714.5965576171875], "page_size": [612.0, 792.0]}
{"layout": 1390, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 174, "bbox": [71, 73, 114, 84], "page_size": [612.0, 792.0]}
{"layout": 1391, "type": "text", "text": "• Fix IOU assigners when ignore i of thr  $>0$   and there is no pred boxes. (#2135) • Fix mAP evaluation when there are no ignored boxes. (#2116) • Fix the empty RoI input for Deformable RoI Pooling. (#2099) • Fix the dataset settings for multiple workflows. (#2103) • Fix the warning related to  torch.uint8  in PyTorch 1.4. (#2105) • Fix the inference demo on devices other than gpu:0. (#2098) • Fix Dockerfile. (#2097) • Fix the bug that  pad_val  is unused in Pad transform. (#2093) • Fix the album ent ation transform when there is no ground truth bbox. (#2032) ", "page_idx": 174, "bbox": [88, 89.38447570800781, 412.4523620605469, 246.1564178466797], "page_size": [612.0, 792.0]}
{"layout": 1392, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 174, "bbox": [71, 252, 133, 264], "page_size": [612.0, 792.0]}
{"layout": 1393, "type": "text", "text": "• Use torch instead of numpy for random sampling. (#2094) • Migrate to the new MMDDP implementation in MMCV v0.3. (#2090) • Add meta information in logs. (#2086) • Rewrite Soft NMS with pytorch extension and remove cython as a dependency. (#2056) • Rewrite dataset evaluation. (#2042, #2087, #2114, #2128) • Use numpy array for masks in transforms. (#2030) ", "page_idx": 174, "bbox": [88, 268.7123718261719, 446.8233337402344, 371.68646240234375], "page_size": [612.0, 792.0]}
{"layout": 1394, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 174, "bbox": [71, 377, 131, 389], "page_size": [612.0, 792.0]}
{"layout": 1395, "type": "text", "text": "• Implement “CARAFE: Content-Aware ReAssembly of FEatures”. (#1583) • Add  worker in it fn()  in data loader when seed is set. (#2066, #2111) • Add logging utils. (#2035) ", "page_idx": 174, "bbox": [88, 394.241455078125, 395, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 1396, "type": "text", "text": "32.22 v1.0.0 (30/1/2020) ", "text_level": 1, "page_idx": 174, "bbox": [71, 466, 233, 482], "page_size": [612.0, 792.0]}
{"layout": 1397, "type": "text", "text": "This release mainly improves the code quality and add more docstrings. ", "page_idx": 174, "bbox": [72.00003051757812, 497.9784851074219, 357, 511.28851318359375], "page_size": [612.0, 792.0]}
{"layout": 1398, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 174, "bbox": [71, 517, 117, 529], "page_size": [612.0, 792.0]}
{"layout": 1399, "type": "text", "text": "• Documentation is online now: https://mm detection.read the docs.io. • Support new models:  ATSS . • DCN is now available with the api  build con v layer  and  ConvModule  like the normal conv layer. • A tool to collect environment information is available for trouble shooting. ", "page_idx": 174, "bbox": [88, 533.844482421875, 500.0044250488281, 600.9525146484375], "page_size": [612.0, 792.0]}
{"layout": 1400, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 174, "bbox": [71, 607, 114, 618], "page_size": [612.0, 792.0]}
{"layout": 1401, "type": "text", "text": "• Fix the incompatibility of the latest numpy and py coco tools. (#2024) • Fix the case when distributed package is unavailable, e.g., on Windows. (#1985) • Fix the dimension issue for  refine b boxes() . (#1962) • Fix the typo when  seg_prefix  is a list. (#1906) • Add segmentation map cropping to RandomCrop. (#1880) ", "page_idx": 174, "bbox": [88, 623.5084838867188, 416.8558654785156, 708.5494995117188], "page_size": [612.0, 792.0]}
{"layout": 1402, "type": "text", "text": "• Fix the return value of  ga shape target single() . (#1853) • Fix the loaded shape of empty proposals. (#1819) • Fix the mask data type when using album ent ation. (#1818) ", "page_idx": 175, "bbox": [88, 71.45246887207031, 351.9497375488281, 120.62749481201172], "page_size": [612.0, 792.0]}
{"layout": 1403, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 175, "bbox": [71, 126, 134, 138], "page_size": [612.0, 792.0]}
{"layout": 1404, "type": "text", "text": "• Enhance Assign Result and Sampling Result. (#1995) • Add ability to overwrite existing module in Registry. (#1982) • Reorganize requirements and make albumen tat ions and image corruptions optional. (#1969) • Check NaN in  SSDHead . (#1935) • Encapsulate the DCN in ResNe(X)t into a ConvModule & Con v layers. (#1894) • Refactoring for mAP evaluation and support multiprocessing and logging. (#1889) • Init the root logger before constructing Runner to log more information. (#1865) • Split  Seg Resize Flip PadRe scale  into different existing transforms. (#1852) • Move  init_dist()  to MMCV. (#1851) • Documentation and docstring improvements. (#1971, #1938, #1869, #1838) • Fix the color of the same class for mask visualization. (#1834) • Remove the option  keep all stages  in HTC and Cascade R-CNN. (#1806) ", "page_idx": 175, "bbox": [88, 143.1834259033203, 461.9963073730469, 353.7534484863281], "page_size": [612.0, 792.0]}
{"layout": 1405, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 175, "bbox": [70, 360, 130, 371], "page_size": [612.0, 792.0]}
{"layout": 1406, "type": "text", "text": "• Add two test-time options  crop_mask  and  r le mask encode  for mask heads. (#2013) • Support loading grayscale images as single channel. (#1975) • Implement “Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection”. (#1872) • Add sphinx generated docs. (#1859, #1864) • Add GN support for flops computation. (#1850) • Collect env info for trouble shooting. (#1812) ", "page_idx": 175, "bbox": [88, 376.3084411621094, 540.0034790039062, 491.2375183105469], "page_size": [612.0, 792.0]}
{"layout": 1407, "type": "text", "text": "32.23 v1.0rc1 (13/12/2019) ", "text_level": 1, "page_idx": 175, "bbox": [71, 514, 250, 530], "page_size": [612.0, 792.0]}
{"layout": 1408, "type": "text", "text": "The RC1 release mainly focuses on improving the user experience, and fixing bugs. ", "page_idx": 175, "bbox": [72.0000228881836, 545.7994995117188, 404.153076171875, 559.1095581054688], "page_size": [612.0, 792.0]}
{"layout": 1409, "type": "text", "text": "Highlights ", "text_level": 1, "page_idx": 175, "bbox": [71, 565, 117, 577], "page_size": [612.0, 792.0]}
{"layout": 1410, "type": "text", "text": "• Support new models:  FoveaBox ,  RepPoints  and  FreeAnchor • Add a Dockerfile. • Add a jupyter notebook demo and a webcam demo. • Setup the code style and CI. • Add lots of docstrings and unit tests. • Fix lots of bugs. ", "page_idx": 175, "bbox": [88, 581.6644897460938, 341, 684.6384887695312], "page_size": [612.0, 792.0]}
{"layout": 1411, "type": "text", "text": "Breaking Changes ", "text_level": 1, "page_idx": 175, "bbox": [71, 690, 151, 703], "page_size": [612.0, 792.0]}
{"layout": 1412, "type": "text", "text": "• There was a bug for computing COCO-style mAP w.r.t different scales (AP_s, AP_m, AP_l), introduced by  $\\#621$  . (#1679)\n\n ", "page_idx": 176, "bbox": [88, 71.45246887207031, 540.003662109375, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1413, "type": "text", "text": "Bug Fixes ", "text_level": 1, "page_idx": 176, "bbox": [70, 101, 115, 114], "page_size": [612.0, 792.0]}
{"layout": 1414, "type": "text", "text": "• Fix a sampling interval bug in Libra R-CNN. (#1800)\n\n • Fix the learning rate in SSD300 WIDER FACE. (#1781)\n\n • Fix the scaling issue when  keep_ratio  $=$  False . (#1730)\n\n • Fix typos. (#1721, #1492, #1242, #1108, #1107)\n\n • Fix the shuffle argument in  build data loader . (#1693)\n\n • Clip the proposal when computing mask targets. (#1688)\n\n • Fix the “index out of range” bug for samplers in some corner cases. (#1610, #1404)\n\n • Fix the NMS issue on devices other than GPU:0. (#1603)\n\n • Fix SSD Head and GHM Loss on CPU. (#1578)\n\n • Fix the OOM error when there are too many gt bboxes. (#1575)\n\n • Fix the wrong keyword argument  nms_cfg  in HTC. (#1573)\n\n • Process masks and semantic segmentation in Expand and MinIoUCrop transforms. (#1550, #1361)\n\n • Fix a scale bug in the Non Local op. (#1528)\n\n • Fix a bug in transforms when  gt b boxes ignore  is None. (#1498)\n\n • Fix a bug when  img_prefix  is None. (#1497)\n\n • Pass the device argument to  grid anchors  and  valid flags . (#1478)\n\n • Fix the data pipeline for test robustness. (#1476)\n\n • Fix the argument type of deformable pooling. (#1390)\n\n • Fix the coco_eval when there are only two classes. (#1376)\n\n • Fix a bug in Modulated De formable Con v when de formable group>1. (#1359)\n\n • Fix the mask cropping in RandomCrop. (#1333)\n\n • Fix zero outputs in DeformConv when not running on cuda:0. (#1326)\n\n • Fix the type issue in Expand. (#1288)\n\n • Fix the inference API. (#1255)\n\n • Fix the inplace operation in Expand. (#1249)\n\n • Fix the from-scratch training config. (#1196)\n\n • Fix inplace add in RoI Extractor which cause an error in PyTorch 1.2. (#1160)\n\n • Fix FCOS when input images has no positive sample. (#1136)\n\n • Fix recursive imports. (#1099)\n\n ", "page_idx": 176, "bbox": [88, 119.27247619628906, 491.84423828125, 634.6995849609375], "page_size": [612.0, 792.0]}
{"layout": 1415, "type": "text", "text": "Improvements ", "text_level": 1, "page_idx": 176, "bbox": [70, 641, 134, 652], "page_size": [612.0, 792.0]}
{"layout": 1416, "type": "text", "text": "• Print the config file and mmdet version in the log. (#1721)\n\n • Lint the code before compiling in travis CI. (#1715)\n\n • Add a probability argument for the  Expand  transform. (#1651) ", "page_idx": 176, "bbox": [88, 657.2555541992188, 347, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 1417, "type": "text", "text": "• Update the PyTorch and CUDA version in the docker file. (#1615)\n\n • Raise a warning when specifying  --validate  in non-distributed training. (#1624, #1651)\n\n • Beautify the mAP printing. (#1614)\n\n • Add pre-commit hook. (#1536)\n\n • Add the argument  in channels  to backbones. (#1475)\n\n • Add lots of docstrings and unit tests, thanks to    $@$  Erotemic . (#1603, #1517, #1506, #1505, #1491, #1479, #1477, #1475, #1474)\n\n • Add support for multi-node distributed test when there is no shared storage. (#1399)\n\n • Optimize Dockerfile to reduce the image size. (#1306)\n\n • Update new results of HRNet. (#1284, #1182)\n\n • Add an argument  no norm on lateral  in FPN. (#1240)\n\n • Test the compiling in CI. (#1235)\n\n • Move docs to a separate folder. (#1233)\n\n • Add a jupyter notebook demo. (#1158)\n\n • Support different type of dataset for training. (#1133)\n\n • Use int64_t instead of long in cuda kernels. (#1131)\n\n • Support unsquare RoIs for bbox and mask heads. (#1128)\n\n • Manually add type promotion to make compatible to PyTorch 1.2. (#1114)\n\n • Allowing validation dataset for computing validation loss. (#1093)\n\n • Use  .scalar type()  instead of  .type()  to suppress some warnings. (#1070)\n\n ", "page_idx": 177, "bbox": [88, 71.45246887207031, 540.0036010742188, 419.50653076171875], "page_size": [612.0, 792.0]}
{"layout": 1418, "type": "text", "text": "New Features ", "text_level": 1, "page_idx": 177, "bbox": [71, 425, 129, 438], "page_size": [612.0, 792.0]}
{"layout": 1419, "type": "text", "text": "• Add an option  --with_ap  to compute the AP for each class. (#1549)\n\n • Implement “FreeAnchor: Learning to Match Anchors for Visual Object Detection”. (#1391)\n\n • Support  Albumen tat ions  for augmentations in the data pipeline. (#1354)\n\n • Implement “FoveaBox: Beyond Anchor-based Object Detector”. (#1339)\n\n • Support horizontal and vertical flipping. (#1273, #1115)\n\n • Implement “RepPoints: Point Set Representation for Object Detection”. (#1265)\n\n • Add test-time augmentation to HTC and Cascade R-CNN. (#1251)\n\n • Add a COCO result analysis tool. (#1228)\n\n • Add Dockerfile. (#1168)\n\n • Add a webcam demo. (#1155, #1150)\n\n • Add FLOPs counter. (#1127)\n\n • Allow arbitrary layer order for ConvModule. (#1078) ", "page_idx": 177, "bbox": [88, 442.0625305175781, 464.8655700683594, 652.632568359375], "page_size": [612.0, 792.0]}
{"layout": 1420, "type": "text", "text": "32.24 v1.0rc0 (27/07/2019) ", "text_level": 1, "page_idx": 178, "bbox": [70, 70, 252, 88], "page_size": [612.0, 792.0]}
{"layout": 1421, "type": "text", "text": "• Implement lots of new methods and components (Mixed Precision Training, HTC, Libra R-CNN, Guided An- choring, Empirical Attention, Mask Scoring R-CNN, Grid R-CNN (Plus), GHM, GCNet, FCOS, HRNet, Weight Standardization, etc.). Thank all collaborators!\n\n • Support two additional datasets: WIDER FACE and Cityscapes.\n\n • Refactoring for loss APIs and make it more flexible to adopt different losses and related hyper-parameters.\n\n • Speed up multi-gpu testing.\n\n • Integrate all compiling and installing in a single script.\n\n ", "page_idx": 178, "bbox": [88, 102.99446105957031, 540, 211.9454803466797], "page_size": [612.0, 792.0]}
{"layout": 1422, "type": "text", "text": "32.25 v0.6.0 (14/04/2019) ", "text_level": 1, "page_idx": 178, "bbox": [71, 234, 242, 250], "page_size": [612.0, 792.0]}
{"layout": 1423, "type": "text", "text": "• Up to  $30\\%$   speedup compared to the model zoo.\n\n • Support both PyTorch stable and nightly version.\n\n • Replace NMS and S igm oid Focal Loss with Pytorch CUDA extensions.\n\n ", "page_idx": 178, "bbox": [88, 266.5074768066406, 377.02532958984375, 315.6825256347656], "page_size": [612.0, 792.0]}
{"layout": 1424, "type": "text", "text": "32.26 v0.6rc0(06/02/2019) ", "text_level": 1, "page_idx": 178, "bbox": [70, 337, 248, 355], "page_size": [612.0, 792.0]}
{"layout": 1425, "type": "text", "text": "• Migrate to PyTorch 1.0.\n\n ", "page_idx": 178, "bbox": [88, 370.2445068359375, 191.9800262451172, 383.5545349121094], "page_size": [612.0, 792.0]}
{"layout": 1426, "type": "text", "text": "32.27 v0.5.7 (06/02/2019) ", "text_level": 1, "page_idx": 178, "bbox": [71, 405, 242, 422], "page_size": [612.0, 792.0]}
{"layout": 1427, "type": "text", "text": "• Add support for Deformable ConvNet v2. (Many thanks to the authors and    $@$  chengdazhi )\n\n • This is the last release based on PyTorch 0.4.1.\n\n ", "page_idx": 178, "bbox": [88, 438.11651611328125, 454.8232727050781, 469.3585510253906], "page_size": [612.0, 792.0]}
{"layout": 1428, "type": "text", "text": "32.28 v0.5.6 (17/01/2019) ", "text_level": 1, "page_idx": 178, "bbox": [71, 491, 242, 508], "page_size": [612.0, 792.0]}
{"layout": 1429, "type": "text", "text": "• Add support for Group Normalization.\n\n • Unify RPNHead and single stage heads (RetinaHead, SSDHead) with AnchorHead.\n\n ", "page_idx": 178, "bbox": [88, 523.9204711914062, 429.76739501953125, 555.16357421875], "page_size": [612.0, 792.0]}
{"layout": 1430, "type": "text", "text": "32.29 v0.5.5 (22/12/2018) ", "text_level": 1, "page_idx": 178, "bbox": [71, 577, 241, 594], "page_size": [612.0, 792.0]}
{"layout": 1431, "type": "text", "text": "• Add SSD for COCO and PASCAL VOC.\n\n • Add ResNeXt backbones and detection models.\n\n • Refactoring for Samplers/Assigners and add OHEM.\n\n • Add VOC dataset and evaluation scripts. ", "page_idx": 178, "bbox": [88, 609.7244873046875, 307.04815673828125, 676.8335571289062], "page_size": [612.0, 792.0]}
{"layout": 1432, "type": "text", "text": "32.30 v0.5.4 (27/11/2018) ", "text_level": 1, "page_idx": 179, "bbox": [71, 70, 243, 88], "page_size": [612.0, 792.0]}
{"layout": 1433, "type": "text", "text": "• Add Single Stage Detector and RetinaNet.\n\n ", "page_idx": 179, "bbox": [88, 102.99446105957031, 259.56640625, 116.30449676513672], "page_size": [612.0, 792.0]}
{"layout": 1434, "type": "text", "text": "32.31 v0.5.3 (26/11/2018) ", "text_level": 1, "page_idx": 179, "bbox": [71, 139, 242, 155], "page_size": [612.0, 792.0]}
{"layout": 1435, "type": "text", "text": "• Add Cascade R-CNN and Cascade Mask R-CNN.\n\n • Add support for Soft-NMS in config files.\n\n ", "page_idx": 179, "bbox": [88, 170.8654327392578, 294.9037170410156, 202.1084442138672], "page_size": [612.0, 792.0]}
{"layout": 1436, "type": "text", "text": "32.32 v0.5.2 (21/10/2018) ", "text_level": 1, "page_idx": 179, "bbox": [71, 225, 242, 241], "page_size": [612.0, 792.0]}
{"layout": 1437, "type": "text", "text": "• Add support for custom datasets.\n\n • Add a script to convert PASCAL VOC annotations to the expected format.\n\n ", "page_idx": 179, "bbox": [88, 256.6704406738281, 392.3577575683594, 287.9134826660156], "page_size": [612.0, 792.0]}
{"layout": 1438, "type": "text", "text": "32.33 v0.5.1 (20/10/2018) ", "text_level": 1, "page_idx": 179, "bbox": [71, 310, 241, 327], "page_size": [612.0, 792.0]}
{"layout": 1439, "type": "text", "text": "• Add B Box As signer and B Box Sampler, the  train_cfg  field in config files are restructured.\n\n •  Con v FC RoI Head  /  Shared FC RoI Head  are renamed to  Con v FCB Box Head  /  Shared FCB Box Head  for consistency. ", "page_idx": 179, "bbox": [88, 342.4744567871094, 539.9979248046875, 373.7174987792969], "page_size": [612.0, 792.0]}
{"layout": 1440, "type": "text", "text": "FREQUENTLY ASKED QUESTIONS ", "text_level": 1, "page_idx": 180, "bbox": [301, 163, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 1441, "type": "text", "text": "We list some common troubles faced by many users and their corresponding solutions here. Feel free to enrich the list if you find any frequent issues and have ways to help others to solve them. If the contents here do not cover your issue, please create an issue using the  provided templates  and make sure you fill in all required information in the template. ", "page_idx": 180, "bbox": [72, 225.87342834472656, 540, 263.093505859375], "page_size": [612.0, 792.0]}
{"layout": 1442, "type": "text", "text": "33.1 MMCV Installation ", "text_level": 1, "page_idx": 180, "bbox": [71, 284, 232, 302], "page_size": [612.0, 792.0]}
{"layout": 1443, "type": "text", "text": "• Compatibility issue between MMCV and MM Detection; “ConvWS is already registered in conv layer”; “Asser- tionError: MMCV  $\\scriptstyle{\\left(=-2\\right)}$  xxx is used but incompatible. Please install mmcv  $\\scriptstyle{>=\\mathbf{X}\\mathbf{X}\\mathbf{X}}$  ,  $<=\\tt X X X$  .” Please install the correct version of MMCV for the version of your MM Detection following the  installation instruction . ", "page_idx": 180, "bbox": [88, 317.65545654296875, 540, 372.8084716796875], "page_size": [612.0, 792.0]}
{"layout": 1444, "type": "text", "text": "• “No module named ‘mmcv.ops’”; “No module named ‘mmcv._ext’”. ", "page_idx": 180, "bbox": [88, 377.4304504394531, 370.6593322753906, 390.740478515625], "page_size": [612.0, 792.0]}
{"layout": 1445, "type": "text", "text": "1. Uninstall existing mmcv in the environment using  pip uninstall mmcv . 2. Install mmcv-full following the  installation instruction . ", "page_idx": 180, "bbox": [106, 395.36346435546875, 416.5176696777344, 426.60650634765625], "page_size": [612.0, 792.0]}
{"layout": 1446, "type": "text", "text": "33.2 PyTorch/CUDA Environment ", "text_level": 1, "page_idx": 180, "bbox": [71, 447, 301, 466], "page_size": [612.0, 792.0]}
{"layout": 1447, "type": "text", "text": "• “RTX 30 series card fails when building MMCV or MMDet” ", "page_idx": 180, "bbox": [88, 481.16845703125, 341.38922119140625, 494.4784851074219], "page_size": [612.0, 792.0]}
{"layout": 1448, "type": "text", "text": "1. Temporary work-around: do  MMCV_WITH_  ${\\tt O P S}{=}1$   MM CV CUDA ARG S  $\\overleftarrow{}$  ' -gencode  $=$  arch  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  compute_80, code  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  sm_80 '  pip install -e . . The common issue is  nvcc fatal : Unsupported gpu architecture  ' compute_86 ' . This means that the compiler should optimize for sm_86, i.e., nvidia 30 series card, but such optimization s have not been supported by CUDA toolkit 11.0. This work-around modifies the compile flag by adding  MM CV CUDA ARG S  $,=$  ' -gencode  $:=$  arch  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  compute_80,code  $=$  sm_80 ' , which tells  nvcc  to optimize for  $\\mathbf{sm}\\_{80}$  , i.e., Nvidia A100. Although A100 is different from the 30 series card, they use similar ampere architecture. This may hurt the performance but it works. ", "page_idx": 180, "bbox": [106, 499.1004638671875, 540, 584.1414794921875], "page_size": [612.0, 792.0]}
{"layout": 1449, "type": "text", "text": "2. PyTorch developers have updated that the default compiler flags should be fixed by  pytorch/pytorch#47585 . So using PyTorch-nightly may also be able to solve the problem, though we have not tested it yet. ", "page_idx": 180, "bbox": [106, 588.764404296875, 540, 614.0294799804688], "page_size": [612.0, 792.0]}
{"layout": 1450, "type": "text", "text": "• “invalid device function” or “no kernel image is available for execution”. ", "page_idx": 180, "bbox": [88, 618.6524047851562, 386.708984375, 631.9624633789062], "page_size": [612.0, 792.0]}
{"layout": 1451, "type": "text", "text": "1. Check if your cuda runtime version (under  /usr/local/ ),  nvcc --version  and  conda list cuda toolkit  version match. ", "page_idx": 180, "bbox": [106, 636.5853881835938, 540, 661.8504638671875], "page_size": [612.0, 792.0]}
{"layout": 1452, "type": "text", "text": "2. Run  python mmdet/utils/collect en v.py  to check whether PyTorch, torch vision, and MMCV are built for the correct GPU architecture. You may need to set  TORCH CUDA ARCH LIST  to reinstall MMCV. The GPU arch table could be found  here , i.e. run  TORCH CUDA ARCH LIS  $\\Gamma{=}7\\cdot\\Updownarrow$   pip install ", "page_idx": 180, "bbox": [106, 666.4734497070312, 540, 703.6934814453125], "page_size": [612.0, 792.0]}
{"layout": 1453, "type": "text", "text": "mmcv-full  to build MMCV for Volta GPUs. The compatibility issue could happen when using old GPUS, e.g., Tesla K80 (3.7) on colab. ", "page_idx": 181, "bbox": [118, 71.45246887207031, 540, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1454, "type": "text", "text": "3. Check whether the running environment is the same as that when mmcv/mmdet has compiled. For example, you may compile mmcv using CUDA 10.0 but run it on CUDA 9.0 environments.\n\n ", "page_idx": 181, "bbox": [106, 101.34046936035156, 540, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 1455, "type": "text", "text": "• “undefined symbol” or “cannot open xxx.so”. ", "page_idx": 181, "bbox": [88, 131.2284698486328, 278.69451904296875, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 1456, "type": "text", "text": "1. If those symbols are  $\\mathrm{ttCUDA/C++}$   symbols (e.g., libcudart.so or GLIBCXX), check whether the CUDA/GCC runtimes are the same as those used for compiling mmcv, i.e. run  python mmdet/utils/collect en v. py  to see if  \"MMCV Compiler\" / \"MMCV CUDA Compiler\"  is the same as  \"GCC\" / \"CUDA_HOME\" . 2. If those symbols are PyTorch symbols (e.g., symbols containing caffe, aten, and TH), check whether the PyTorch version is the same as that used for compiling mmcv. 3. Run  python mmdet/utils/collect en v.py  to check whether PyTorch, torch vision, and MMCV are built by and running on the same environment.\n\n ", "page_idx": 181, "bbox": [106, 149.1604766845703, 540, 246.1565399169922], "page_size": [612.0, 792.0]}
{"layout": 1457, "type": "text", "text": "• setuptools.sandbox.Unpick le able Exception: Dist utils Setup Error(“each element of ‘ext modules’ option must be an Extension instance or 2-tuple”) ", "page_idx": 181, "bbox": [88, 250.7794952392578, 540, 276.0445556640625], "page_size": [612.0, 792.0]}
{"layout": 1458, "type": "text", "text": "1. If you are using miniconda rather than anaconda, check whether Cython is installed as indicated in  #3379 . You need to manually install Cython first and then run command  pip install -r requirements.txt . ", "page_idx": 181, "bbox": [106, 280.66754150390625, 540, 305.93255615234375], "page_size": [612.0, 792.0]}
{"layout": 1459, "type": "text", "text": "2. You may also need to check the compatibility between the  setuptools ,  Cython , and  PyTorch  in your environment.\n\n ", "page_idx": 181, "bbox": [106, 310.5555419921875, 540, 335.820556640625], "page_size": [612.0, 792.0]}
{"layout": 1460, "type": "text", "text": "• “Segmentation fault”. ", "page_idx": 181, "bbox": [88, 340.44354248046875, 185, 353.7535705566406], "page_size": [612.0, 792.0]}
{"layout": 1461, "type": "text", "text": "1. Check you GCC version and use GCC 5.4. This usually caused by the incompatibility between PyTorch and the environment (e.g.,   $\\mathrm{GCC}<4.9$   for PyTorch). We also recommend the users to avoid using GCC 5.5 because many feedbacks report that GCC 5.5 will cause “segmentation fault” and simply changing it to GCC 5.4 could solve the problem. ", "page_idx": 181, "bbox": [106, 358.3765563964844, 540, 407.5515441894531], "page_size": [612.0, 792.0]}
{"layout": 1462, "type": "text", "text": "2. Check whether PyTorch is correctly installed and could use CUDA op, e.g. type the following command in your terminal. ", "page_idx": 181, "bbox": [106, 412.1745300292969, 540, 437.4395446777344], "page_size": [612.0, 792.0]}
{"layout": 1463, "type": "text", "text": "python -c  ' import torch; print(torch.cuda.is available()) And see whether they could correctly output results. 3. If Pytorch is correctly installed, check whether MMCV is correctly installed. python -c  ' import mmcv; import mmcv.ops If MMCV is correctly installed, then there will be no issue of the above two commands. ", "page_idx": 181, "bbox": [118, 447.1727294921875, 416.9551086425781, 457.2117614746094], "page_size": [612.0, 792.0]}
{"layout": 1464, "type": "text", "text": "", "page_idx": 181, "bbox": [118, 469.75848388671875, 326.5939636230469, 483.0685119628906], "page_size": [612.0, 792.0]}
{"layout": 1465, "type": "text", "text": "", "page_idx": 181, "bbox": [106, 487.6914978027344, 424, 501.00152587890625], "page_size": [612.0, 792.0]}
{"layout": 1466, "type": "text", "text": "", "page_idx": 181, "bbox": [118, 510.7347412109375, 322.8083190917969, 520.7738037109375], "page_size": [612.0, 792.0]}
{"layout": 1467, "type": "text", "text": "", "page_idx": 181, "bbox": [118, 533.3204345703125, 468.21240234375, 546.6304931640625], "page_size": [612.0, 792.0]}
{"layout": 1468, "type": "text", "text": "4. If MMCV and Pytorch is correctly installed, you man use  ipdb ,  pdb  to set breakpoints or directly add ‘print’ in mm detection code and see which part leads the segmentation fault.\n\n ", "page_idx": 181, "bbox": [106, 551.25244140625, 540, 576.5184936523438], "page_size": [612.0, 792.0]}
{"layout": 1469, "type": "text", "text": "33.3 Training ", "text_level": 1, "page_idx": 181, "bbox": [71, 599, 164, 616], "page_size": [612.0, 792.0]}
{"layout": 1470, "type": "text", "text": "• “Loss goes Nan” ", "page_idx": 181, "bbox": [88, 631.0794067382812, 164.16452026367188, 644.3894653320312], "page_size": [612.0, 792.0]}
{"layout": 1471, "type": "text", "text": "1. Check if the dataset annotations are valid: zero-size bounding boxes will cause the regression loss to be Nan due to the commonly used transformation for box regression. Some small size (width or height are smaller than 1) boxes will also cause this problem after data augmentation (e.g., instaboost). So check the data and try to filter out those zero-size boxes and skip some risky augmentations on the small-size boxes when you face the problem. ", "page_idx": 181, "bbox": [106, 649.012451171875, 540, 710.1434936523438], "page_size": [612.0, 792.0]}
{"layout": 1472, "type": "text", "text": "2. Reduce the learning rate: the learning rate might be too large due to some reasons, e.g., change of batch size. You can rescale them to the value that could stably train the model. 3. Extend the warmup iterations: some models are sensitive to the learning rate at the start of the training. You can extend the warmup iterations, e.g., change the  warm up it ers  from 500 to 1000 or 2000. 4. Add gradient clipping: some models requires gradient clipping to stabilize the training process. The default of  grad_clip  is  None , you can add gradient clippint to avoid gradients that are too large, i.e., set optimizer config=dict(_delete_  $\\bar{\\underline{{\\mathbf{\\alpha}}}}$  True, grad_clip  $\\leftrightharpoons$  dict(max_norm  $\\scriptstyle{|=35}$  , norm_type  $^{=2}$  )) in your config file. If your config does not inherits from any basic config that contains optimizer config  $\\vDash$  dict(grad_clip  $\\risingdotseq$  None) , you can simply add optimizer conf i  $\\mathfrak{g}=$  dict(grad_clip  $\\leftrightharpoons$  dict(max_norm  $\\scriptstyle{|=35}$  , norm_type  $^{=2}$  )) .\n\n ", "page_idx": 182, "bbox": [106, 71.45246887207031, 540, 204.31358337402344], "page_size": [612.0, 792.0]}
{"layout": 1473, "type": "text", "text": "• ’GPU out of memory” ", "page_idx": 182, "bbox": [88, 208.93653869628906, 186.48074340820312, 222.24656677246094], "page_size": [612.0, 792.0]}
{"layout": 1474, "type": "text", "text": "1. There are some scenarios when there are large amount of ground truth boxes, which may cause OOM during target assignment. You can set  gpu assign thr  $\\scriptstyle{\\mathrm{\\varepsilon}}=\\!\\mathtt{N}$   in the config of assigner thus the assigner will ", "page_idx": 182, "bbox": [106, 226.86952209472656, 540, 252.1345672607422], "page_size": [612.0, 792.0]}
{"layout": 1475, "type": "text", "text": "calculate box overlaps through CPU when there are more than N GT boxes. 2. Set  with_cp  $=$  True  in the backbone. This uses the sublinear strategy in PyTorch to reduce GPU memory cost in the backbone. 3. Try mixed precision training using following the examples in  config/fp16 . The  loss_scale  might need further tuning for different models.\n\n ", "page_idx": 182, "bbox": [106, 250.77955627441406, 540, 323.8655700683594], "page_size": [612.0, 792.0]}
{"layout": 1476, "type": "text", "text": "• “Runtime Error: Expected to have finished reduction in the prior iteration before starting a new one” ", "page_idx": 182, "bbox": [88, 328.4885559082031, 494.5640563964844, 341.798583984375], "page_size": [612.0, 792.0]}
{"layout": 1477, "type": "text", "text": "1. This error indicates that your module has parameters that were not used in producing loss. This phenomenon may be caused by running different branches in your code in DDP mode. 2. You can set  find unused parameters   $=$   True  in the config to solve the above problems or find those unused parameters manually.\n\n ", "page_idx": 182, "bbox": [106, 346.42156982421875, 540, 401.5745849609375], "page_size": [612.0, 792.0]}
{"layout": 1478, "type": "text", "text": "33.4 Evaluation ", "text_level": 1, "page_idx": 182, "bbox": [70, 423, 180, 441], "page_size": [612.0, 792.0]}
{"layout": 1479, "type": "text", "text": "• COCO Dataset, AP or  $\\mathrm{AR}={\\mathrm{-}}1$  ", "page_idx": 182, "bbox": [88, 456.1355285644531, 221, 469.445556640625], "page_size": [612.0, 792.0]}
{"layout": 1480, "type": "text", "text": "1. According to the definition of COCO dataset, the small and medium areas in an image are less than 1024  $(32^{*}32)$  , 9216   $(96^{*}96)$  , respectively. 2. If the corresponding area has no object, the result of AP and AR will set to -1. ", "page_idx": 182, "bbox": [106, 474.06854248046875, 540, 517.2665405273438], "page_size": [612.0, 792.0]}
{"layout": 1481, "type": "text", "text": "ENGLISH ", "page_idx": 184, "bbox": [476.0880126953125, 162.4604949951172, 540.0003051757812, 183.00425720214844], "page_size": [612.0, 792.0]}
{"layout": 1482, "type": "text", "text": "MMDET.APIS ", "page_idx": 188, "bbox": [450.86700439453125, 162.4604949951172, 539.9999389648438, 183.00425720214844], "page_size": [612.0, 792.0]}
{"layout": 1483, "type": "text", "text": "async  mmdet.apis. a sync inference detector ( model ,  imgs ) Async inference image(s) with the detector. ", "page_idx": 188, "bbox": [71, 225.72398376464844, 345.1103515625, 251.1384735107422], "page_size": [612.0, 792.0]}
{"layout": 1484, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 188, "bbox": [118, 257, 169, 269], "page_size": [612.0, 792.0]}
{"layout": 1485, "type": "text", "text": "•  model  ( nn.Module ) – The loaded detector. •  img  ( str | ndarray ) – Either image files or loaded images. ", "page_idx": 188, "bbox": [145, 273.6934814453125, 400.5169677734375, 304.9365234375], "page_size": [612.0, 792.0]}
{"layout": 1486, "type": "text", "text": "Returns  Awaitable detection results. ", "page_idx": 188, "bbox": [118, 308.9318542480469, 270, 323.46728515625], "page_size": [612.0, 792.0]}
{"layout": 1487, "type": "text", "text": "mmdet.apis. get root logger ( log_file  $=$  None ,  log_level=20 ) Get root logger. ", "page_idx": 188, "bbox": [71, 327.3420715332031, 333.9923400878906, 352.7575378417969], "page_size": [612.0, 792.0]}
{"layout": 1488, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 188, "bbox": [117, 358, 170, 371], "page_size": [612.0, 792.0]}
{"layout": 1489, "type": "text", "text": "•  log_file  ( str, optional ) – File path of log. Defaults to None. •  log_level  ( int, optional ) – The level of logger. Defaults to logging.INFO. Returns  The obtained logger Return type  logging.Logger ", "page_idx": 188, "bbox": [118, 375.3125305175781, 474.7338562011719, 443.0183410644531], "page_size": [612.0, 792.0]}
{"layout": 1490, "type": "text", "text": "Inference image(s) with the detector. ", "page_idx": 188, "bbox": [96, 458.9985656738281, 242, 472.30859375], "page_size": [612.0, 792.0]}
{"layout": 1491, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 188, "bbox": [117, 478, 170, 490], "page_size": [612.0, 792.0]}
{"layout": 1492, "type": "text", "text": "•  model  ( nn.Module ) – The loaded detector. •  imgs  ( str/ndarray or list[str/ndarray] or tuple[str/ndarray] ) – Either im- age files or loaded images.  If imgs is a list or tuple, the same length list type results will be returned, otherwise return ", "page_idx": 188, "bbox": [145, 494.8645935058594, 518, 555.99560546875], "page_size": [612.0, 792.0]}
{"layout": 1493, "type": "text", "text": "the detection results directly. ", "page_idx": 188, "bbox": [137.4549560546875, 554.6405639648438, 251.56661987304688, 567.9506225585938], "page_size": [612.0, 792.0]}
{"layout": 1494, "type": "text", "text": "mmdet.apis. in it detector ( config ,  checkpoin  $\\leftrightharpoons$  None ,  device  $\\mathbf{=}$  'cuda:0' ,  cf g options  $\\mathbf{\\hat{\\rho}}$  None ) Initialize a detector from config file. ", "page_idx": 188, "bbox": [71, 572.4231567382812, 455.3022766113281, 597.838623046875], "page_size": [612.0, 792.0]}
{"layout": 1495, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 188, "bbox": [117, 603, 170, 616], "page_size": [612.0, 792.0]}
{"layout": 1496, "type": "text", "text": "•  config  (str or  mmcv.Config ) – Config file path or the config object. •  checkpoint  ( str, optional ) – Checkpoint path. If left as None, the model will not load any weights. •  cf g options  ( dict ) – Options to override some settings in the used config. ", "page_idx": 188, "bbox": [145, 620.3935546875, 518, 681.5245971679688], "page_size": [612.0, 792.0]}
{"layout": 1497, "type": "text", "text": "Returns  The constructed detector. ", "page_idx": 188, "bbox": [118, 685.5199584960938, 259, 700.0553588867188], "page_size": [612.0, 792.0]}
{"layout": 1498, "type": "text", "text": "Return type  nn.Module ", "page_idx": 188, "bbox": [118, 703.4519653320312, 218.14175415039062, 717.9873657226562], "page_size": [612.0, 792.0]}
{"layout": 1499, "type": "text", "text": "mmdet.apis. multi gpu test ( model ,  data loader ,  tmpdir  $\\leftrightharpoons$  None ,  gpu collect=False ) Test model with multiple gpus. ", "page_idx": 189, "bbox": [71, 71.30303192138672, 429.8973693847656, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1500, "type": "text", "text": "This method tests model with multiple gpus and collects the results under two different modes: gpu and cpu modes. By setting ‘gpu collect  $\\risingdotseq$  True’ it encodes results to gpu tensors and use gpu communication for results collection. On cpu mode it saves the results on different gpus to ‘tmpdir’ and collects them by the rank 0 worker. ", "page_idx": 189, "bbox": [96, 101.34046936035156, 540, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 1501, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 189, "bbox": [117, 144, 169, 156], "page_size": [612.0, 792.0]}
{"layout": 1502, "type": "text", "text": "•  model  ( nn.Module ) – Model to be tested. •  data loader  ( nn.Dataloader ) – Pytorch data loader. •  tmpdir  ( str ) – Path of directory to save the temporary results from different gpus under cpu mode. •  gpu collect  ( bool ) – Option to use either gpu or cpu to collect results. ", "page_idx": 189, "bbox": [145, 161.11549377441406, 518, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 1503, "type": "text", "text": "Returns  The prediction results. ", "page_idx": 189, "bbox": [118.82402801513672, 244.1748046875, 247.19252014160156, 258.7102355957031], "page_size": [612.0, 792.0]}
{"layout": 1504, "type": "text", "text": "Return type  list ", "text_level": 1, "page_idx": 189, "bbox": [118, 264, 190, 276], "page_size": [612.0, 792.0]}
{"layout": 1505, "type": "text", "text": "mmdet.apis. set random seed ( seed ,  deterministic  $\\mathbf{\\dot{\\rho}}=\\mathbf{\\rho}$  False ) Set random seed. ", "page_idx": 189, "bbox": [71, 280.5180358886719, 323, 305.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 1506, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 189, "bbox": [117, 311, 170, 323], "page_size": [612.0, 792.0]}
{"layout": 1507, "type": "text", "text": "•  seed  ( int ) – Seed to be used. •  deterministic  ( bool ) – Whether to set the deterministic option for CUDNN backend, i.e., set  torch.backends.cudnn.deterministic  to True and  torch.backends.cudnn.benchmark  to False. Default: False. ", "page_idx": 189, "bbox": [145, 328.4884948730469, 518, 383.6415100097656], "page_size": [612.0, 792.0]}
{"layout": 1508, "type": "text", "text": "mmdet.apis. show result py plot ( model ,  img ,  result ,  score_th  $\\scriptstyle r=0.3$  ,  title  $=$  'result' ,  wait_time  $\\mathrm{\\Sigma=}0$  ) Visualize the detection results on the image. ", "page_idx": 189, "bbox": [71, 388.11505126953125, 477.603271484375, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 1509, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 189, "bbox": [117, 419, 169, 431], "page_size": [612.0, 792.0]}
{"layout": 1510, "type": "text", "text": "•  model  ( nn.Module ) – The loaded detector. •  img  ( str or np.ndarray ) – Image filename or loaded image. •  result  ( tuple[list] or list ) – The detection result, can be either (bbox, segm) or just bbox. •  score_thr  ( float ) – The threshold to visualize the bboxes and masks. •  title  ( str ) – Title of the pyplot figure. •  wait_time  ( float ) – Value of waitKey param. Default: 0. ", "page_idx": 189, "bbox": [145, 436.0845031738281, 518, 551.0136108398438], "page_size": [612.0, 792.0]}
{"layout": 1511, "type": "text", "text": "MMDET.CORE ", "page_idx": 190, "bbox": [442.97698974609375, 162.4604949951172, 540.0003662109375, 183.00425720214844], "page_size": [612.0, 792.0]}
{"layout": 1512, "type": "text", "text": "37.1 anchor ", "text_level": 1, "page_idx": 190, "bbox": [71, 229, 156, 245], "page_size": [612.0, 792.0]}
{"layout": 1513, "type": "text", "text": "class  mmdet.core.anchor. Anchor Generator ( strides ,  ratios ,  scales  $\\leftrightharpoons$  None ,  base_sizes  $\\leftrightharpoons$  None , scale major  $\\mathbf{\\dot{\\rho}}$  True ,  octave base scale  $\\mathbf{\\dot{\\rho}}$  None , scales per octave  $\\mathbf{=}$  None ,  centers  $\\mathbf{\\check{\\Sigma}}$  None ,  center offset  $\\mathord{:=}\\!\\!O.O$  ) Standard anchor generator for 2D anchor-based detectors. ", "page_idx": 190, "bbox": [72.0, 260.97698974609375, 523.561279296875, 310.3014221191406], "page_size": [612.0, 792.0]}
{"layout": 1514, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 190, "bbox": [118, 317, 168, 327], "page_size": [612.0, 792.0]}
{"layout": 1515, "type": "text", "text": "•  strides  ( list[int] | list[tuple[int, int]] ) – Strides of anchors in multiple fea- ture levels in order (w, h). •  ratios  ( list[float] ) – The list of ratios between the height and width of anchors in a single level. •  scales  ( list[int] | None ) – Anchor scales for anchors in a single level. It cannot be set at the same time if  octave base scale  and  scales per octave  are set. •  base_sizes  ( list[int] | None ) – The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. (If strides are non square, the shortest stride is taken.) •  scale major  ( bool ) – Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 •  octave base scale  ( int ) – The base scale of octave. •  scales per octave  ( int ) – Number of scales for each octave.  octave base scale  and scales per octave  are usually used in retinanet and the  scales  should be None when they are set. •  centers  ( list[tuple[float, float]] | None ) – The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. If a list of tuple of float is given, they will be used to shift the centers of anchors. •  center offset  ( float ) – The offset of center in proportion to anchors’ width and height. By default it is 0 in V2.0. ", "page_idx": 190, "bbox": [145, 332.857421875, 518, 621.1364135742188], "page_size": [612.0, 792.0]}
{"layout": 1516, "type": "text", "text": "Examples ", "text_level": 1, "page_idx": 191, "bbox": [96, 73, 144, 85], "page_size": [612.0, 792.0]}
{"layout": 1517, "type": "text", "text": " $>>$   from  mmdet.core  import  Anchor Generator\n\n  $>>$   self  $=$   Anchor Generator([ 16 ], [ 1. ], [ 1. ], [ 9 ])\n\n  $>>$   all anchors  $=$   self . grid priors([( 2 ,  2 )], device  $\\circeq$  ' cpu ' )\n\n  $>>$   print (all anchors)\n\n [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]])]\n\n >>>  self  $=$   Anchor Generator([ 16 ,  32 ], [ 1. ], [ 1. ], [ 9 ,  18 ])\n\n  $>>$   all anchors  $=$   self . grid priors([( 2 ,  2 ), ( 1 ,  1 )], device  $\\equiv^{1}$  ' cpu ' )\n\n  $>>$   print (all anchors)\n\n [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]]), tensor([[-9., -9., 9., 9.\n\n  $\\c_{\\rightarrow}]])]$  ", "page_idx": 191, "bbox": [95, 100.33097076416016, 521, 291], "page_size": [612.0, 792.0]}
{"layout": 1518, "type": "text", "text": "gen base anchors () ", "text_level": 1, "page_idx": 191, "bbox": [95, 304, 192, 316], "page_size": [612.0, 792.0]}
{"layout": 1519, "type": "text", "text": "Generate base anchors. ", "page_idx": 191, "bbox": [118, 314.79644775390625, 211, 328.1064758300781], "page_size": [612.0, 792.0]}
{"layout": 1520, "type": "text", "text": "Returns  Base anchors of a feature grid in multiple feature levels. ", "page_idx": 191, "bbox": [137, 332.101806640625, 400, 346.6372375488281], "page_size": [612.0, 792.0]}
{"layout": 1521, "type": "text", "text": "Return type  list(torch.Tensor) ", "page_idx": 191, "bbox": [137, 350.0338134765625, 261, 364.5692443847656], "page_size": [612.0, 792.0]}
{"layout": 1522, "type": "text", "text": "gen single level base anchors ( base_size ,  scales ,  ratios ,  center  $\\leftrightharpoons$  None ) Generate base anchors of a single level. ", "page_idx": 191, "bbox": [95, 368.4450378417969, 411.7853088378906, 393.8594970703125], "page_size": [612.0, 792.0]}
{"layout": 1523, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 191, "bbox": [136, 400, 187, 411], "page_size": [612.0, 792.0]}
{"layout": 1524, "type": "text", "text": "•  base_size  ( int | float ) – Basic size of an anchor. ", "page_idx": 191, "bbox": [152, 416.4154968261719, 382.6443176269531, 429.72552490234375], "page_size": [612.0, 792.0]}
{"layout": 1525, "type": "text", "text": "•  scales  ( torch.Tensor ) – Scales of the anchor. •  ratios  ( torch.Tensor ) – The ratio between between the height and width of anchors in a single level. •  center  ( tuple[float], optional ) – The center of the base anchor related to a single feature grid. Defaults to None. Returns  Anchors in a single-level feature maps. Return type  torch.Tensor ", "page_idx": 191, "bbox": [137, 434.3485107421875, 521, 543.8972778320312], "page_size": [612.0, 792.0]}
{"layout": 1526, "type": "text", "text": "grid anchors ( feat map sizes ,  device  $\\mathbf{=}$  'cuda' ) Generate grid anchors in multiple feature levels. ", "page_idx": 191, "bbox": [95, 547.7730712890625, 310, 573.1875610351562], "page_size": [612.0, 792.0]}
{"layout": 1527, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 191, "bbox": [136, 579, 187, 591], "page_size": [612.0, 792.0]}
{"layout": 1528, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – List of feature map sizes in multiple feature levels. •  device  ( str ) – Device where the anchors will be put on. Returns  Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N  $=$   width \\* height \\* num base anchors, width and height are the sizes of the corresponding feature level, num base anchors is the number of anchors for that level. Return type  list[torch.Tensor] ", "page_idx": 191, "bbox": [137, 595.7424926757812, 521, 687.3592529296875], "page_size": [612.0, 792.0]}
{"layout": 1529, "type": "text", "text": "grid priors ( feat map sizes ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $=$  'cuda' ) Generate grid anchors in multiple feature levels. ", "page_idx": 191, "bbox": [95, 691.2350463867188, 365.66827392578125, 716.6495361328125], "page_size": [612.0, 792.0]}
{"layout": 1530, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 192, "bbox": [136, 72, 188, 85], "page_size": [612.0, 792.0]}
{"layout": 1531, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – List of feature map sizes in multiple feature levels. ", "page_idx": 192, "bbox": [152, 89.38447570800781, 510.7232971191406, 102.69451141357422], "page_size": [612.0, 792.0]}
{"layout": 1532, "type": "text", "text": "•  dtype  ( torch.dtype ) – Dtype of priors. Default: torch.float32. •  device  ( str ) – The device where the anchors will be put on. ", "page_idx": 192, "bbox": [152, 107.31745910644531, 422.7039794921875, 138.5604705810547], "page_size": [612.0, 792.0]}
{"layout": 1533, "type": "text", "text": "Returns  Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N  $=$   width \\* height \\* num base anchors, width and height are the sizes of the corresponding feature level, num base anchors is the number of anchors for that level. ", "page_idx": 192, "bbox": [137, 142.5557861328125, 521, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 1534, "type": "text", "text": "Return type  list[torch.Tensor] ", "page_idx": 192, "bbox": [137, 184.3988037109375, 261.6285705566406, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 1535, "type": "text", "text": "property num base anchors total number of base anchors in a feature grid ", "page_idx": 192, "bbox": [96, 204.68295288085938, 299.5654296875, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 1536, "type": "text", "text": "Type  list[int] ", "page_idx": 192, "bbox": [137, 232.21881103515625, 192, 246.75424194335938], "page_size": [612.0, 792.0]}
{"layout": 1537, "type": "text", "text": "property num base priors The number of priors (anchors) at a point on the feature grid ", "page_idx": 192, "bbox": [96, 252.50296020507812, 359.2513732910156, 276.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 1538, "type": "text", "text": "Type  list[int] ", "page_idx": 192, "bbox": [137, 280.0398254394531, 192, 294.57525634765625], "page_size": [612.0, 792.0]}
{"layout": 1539, "type": "text", "text": "property num_levels number of feature levels that the generator will be applied ", "page_idx": 192, "bbox": [96, 300.3240051269531, 348.65216064453125, 323.8655090332031], "page_size": [612.0, 792.0]}
{"layout": 1540, "type": "text", "text": "Type  int ", "page_idx": 192, "bbox": [137, 327.86083984375, 173.49949645996094, 342.3962707519531], "page_size": [612.0, 792.0]}
{"layout": 1541, "type": "text", "text": "single level grid anchors ( base anchors ,  feat map size ,  stride=(16, 16) ,  device  $=$  'cuda' ) Generate grid anchors of a single level. ", "page_idx": 192, "bbox": [96, 346.2720642089844, 479, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 1542, "type": "text", "text": "Note:  This function is usually called by method  self.grid anchors . ", "page_idx": 192, "bbox": [118, 387.6368103027344, 409, 402.1722412109375], "page_size": [612.0, 792.0]}
{"layout": 1543, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 192, "bbox": [136, 425, 187, 437], "page_size": [612.0, 792.0]}
{"layout": 1544, "type": "text", "text": "•  base anchors  ( torch.Tensor ) – The base anchors of a feature grid. ", "page_idx": 192, "bbox": [152, 442.0625, 447.21173095703125, 455.3725280761719], "page_size": [612.0, 792.0]}
{"layout": 1545, "type": "text", "text": "•  feat map size  ( tuple[int] ) – Size of the feature maps. •  stride  ( tuple[int], optional ) – Stride of the feature map in order (w, h). Defaults to (16, 16). •  device  ( str, optional ) – Device the tensor will be put on. Defaults to ‘cuda’. Returns  Anchors in the overall feature maps. ", "page_idx": 192, "bbox": [137, 459.9955139160156, 521, 539.65625], "page_size": [612.0, 792.0]}
{"layout": 1546, "type": "text", "text": "", "page_idx": 192, "bbox": [136, 548.25, 244, 556], "page_size": [612.0, 792.0]}
{"layout": 1547, "type": "text", "text": "single level grid priors ( feat map size ,  level_idx ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $=$  'cuda' ) Generate grid anchors of a single level. ", "page_idx": 192, "bbox": [96, 567.4420776367188, 470.435302734375, 592.8565673828125], "page_size": [612.0, 792.0]}
{"layout": 1548, "type": "text", "text": "Note:  This function is usually called by method  self.grid priors . ", "page_idx": 192, "bbox": [118, 608.8068237304688, 401.68365478515625, 623.3422241210938], "page_size": [612.0, 792.0]}
{"layout": 1549, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 192, "bbox": [136, 646, 188, 658], "page_size": [612.0, 792.0]}
{"layout": 1550, "type": "text", "text": "•  feat map size  ( tuple[int] ) – Size of the feature maps. •  level_idx  ( int ) – The index of corresponding feature map level. •  (obj  ( dtype ) –  torch.dtype ): Date type of points.Defaults to  torch.float32 . ", "page_idx": 192, "bbox": [152, 663.232421875, 479, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 1551, "type": "text", "text": "•  device  ( str, optional ) – The device the tensor will be put on. Defaults to ‘cuda’. ", "page_idx": 193, "bbox": [154, 71.45246887207031, 505.18927001953125, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 1552, "type": "text", "text": "Returns  Anchors in the overall feature maps. ", "page_idx": 193, "bbox": [137, 88.7568359375, 320.22821044921875, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 1553, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 193, "bbox": [137, 106.6898193359375, 242, 121.22525024414062], "page_size": [612.0, 792.0]}
{"layout": 1554, "type": "text", "text": "single level valid flags ( feat map size ,  valid_size ,  num base anchors ,  device  $=$  'cuda' ) Generate the valid flags of anchor in a single feature map. ", "page_idx": 193, "bbox": [96, 131.07798767089844, 472, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 1555, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 193, "bbox": [136, 162, 188, 174], "page_size": [612.0, 792.0]}
{"layout": 1556, "type": "text", "text": "•  feat map size  ( tuple[int] ) – The size of feature maps, arrange as (h, w). •  valid_size  ( tuple[int] ) – The valid size of the feature maps. •  num base anchors  ( int ) – The number of base anchors. •  device  ( str, optional ) – Device where the flags will be put on. Defaults to ‘cuda’. ", "page_idx": 193, "bbox": [154, 179.0484161376953, 510, 246.1564178466797], "page_size": [612.0, 792.0]}
{"layout": 1557, "type": "text", "text": "Returns  The valid flags of each anchor in a single level feature map. ", "page_idx": 193, "bbox": [137, 250.1517333984375, 412.8902893066406, 264.6871643066406], "page_size": [612.0, 792.0]}
{"layout": 1558, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 193, "bbox": [137, 268.084716796875, 242, 282.6201477050781], "page_size": [612.0, 792.0]}
{"layout": 1559, "type": "text", "text": "sparse priors ( prior_idxs ,  feat map size ,  level_idx ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $\\mathbf{=}$  'cuda' ) Generate sparse anchors according to the  prior_idxs . ", "page_idx": 193, "bbox": [96, 286.4959411621094, 459.39727783203125, 311.910400390625], "page_size": [612.0, 792.0]}
{"layout": 1560, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 193, "bbox": [136, 318, 188, 329], "page_size": [612.0, 792.0]}
{"layout": 1561, "type": "text", "text": "•  prior_idxs  ( Tensor ) – The index of corresponding anchors in the feature map. •  feat map size  ( tuple[int] ) – feature map size arrange as (h, w). •  level_idx  ( int ) – The level index of corresponding feature map. •  (obj  ( device ) –  torch.dtype ): Date type of points.Defaults to  torch.float32 . •  (obj  –  torch.device ): The device where the points is located. ", "page_idx": 193, "bbox": [154, 334.46539306640625, 487, 419.5064697265625], "page_size": [612.0, 792.0]}
{"layout": 1562, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 193, "bbox": [137, 425, 172, 437], "page_size": [612.0, 792.0]}
{"layout": 1563, "type": "text", "text": "Anchor with shape (N, 4), N should be equal to  the length of  prior_idxs . Return type  Tensor ", "page_idx": 193, "bbox": [137, 441.434814453125, 465.93255615234375, 473.90325927734375], "page_size": [612.0, 792.0]}
{"layout": 1564, "type": "text", "text": "valid flags ( feat map sizes ,  pad_shape ,  device  $=$  'cuda' ) Generate valid flags of anchors in multiple feature levels. ", "page_idx": 193, "bbox": [96, 477.7780456542969, 348, 503.1935119628906], "page_size": [612.0, 792.0]}
{"layout": 1565, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 193, "bbox": [136, 509, 188, 521], "page_size": [612.0, 792.0]}
{"layout": 1566, "type": "text", "text": "•  feat map sizes  ( list(tuple) ) – List of feature map sizes in multiple feature levels. •  pad_shape  ( tuple ) – The padded shape of the image. •  device  ( str ) – Device where the anchors will be put on. ", "page_idx": 193, "bbox": [154, 525.7484741210938, 510, 574.9244995117188], "page_size": [612.0, 792.0]}
{"layout": 1567, "type": "text", "text": "Returns  Valid flags of anchors in multiple levels. ", "page_idx": 193, "bbox": [137, 578.9188232421875, 339, 593.4542236328125], "page_size": [612.0, 792.0]}
{"layout": 1568, "type": "text", "text": "Return type  list(torch.Tensor) ", "page_idx": 193, "bbox": [137, 596.851806640625, 261.6284484863281, 611.38720703125], "page_size": [612.0, 792.0]}
{"layout": 1569, "type": "text", "text": "class  mmdet.core.anchor. Legacy Anchor Generator ( strides ,  ratios ,  scale  $\\mathfrak{z}{=}$  None ,  base_size  $\\leftrightharpoons$  None scale major  $\\mathbf{=}$  True ,  octave base scale=None , scales per octave  $\\mathbf{\\dot{\\rho}}$  None ,  centers  $\\mathbf{\\check{\\Sigma}}$  None , center off se  $\\mathord{\\tan}=\\!O.O.$  ) ", "page_idx": 193, "bbox": [71.99986267089844, 615.2630004882812, 498, 664.4779052734375], "page_size": [612.0, 792.0]}
{"layout": 1570, "type": "text", "text": "Legacy anchor generator used in MM Detection V1.x. ", "page_idx": 193, "bbox": [96, 663.2324829101562, 309.8972473144531, 676.5425415039062], "page_size": [612.0, 792.0]}
{"layout": 1571, "type": "text", "text": "Note:  Difference to the V2.0 anchor generator: ", "page_idx": 193, "bbox": [96, 692.4927978515625, 287.581298828125, 707.0281982421875], "page_size": [612.0, 792.0]}
{"layout": 1572, "type": "text", "text": "1. The center offset of V1.x anchors are set to be 0.5 rather than 0. ", "page_idx": 194, "bbox": [106, 71.45246887207031, 372.5417785644531, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 1573, "type": "text", "text": "2. The width/height are minused by 1 when calculating the anchors’ centers and corners to meet the V1.x coordinate system. ", "page_idx": 194, "bbox": [106, 89.38447570800781, 540, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 1574, "type": "text", "text": "3. The anchors’ corners are quantized. ", "page_idx": 194, "bbox": [106, 119.27247619628906, 260, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 1575, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 194, "bbox": [117, 156, 169, 168], "page_size": [612.0, 792.0]}
{"layout": 1576, "type": "text", "text": "•  strides  ( list[int] | list[tuple[int]] ) – Strides of anchors in multiple feature lev- ", "page_idx": 194, "bbox": [145, 173.07142639160156, 518, 186.38145446777344], "page_size": [612.0, 792.0]}
{"layout": 1577, "type": "text", "text": "els. •  ratios  ( list[float] ) – The list of ratios between the height and width of anchors in a single level. •  scales  ( list[int] | None ) – Anchor scales for anchors in a single level. It cannot be set at the same time if  octave base scale  and  scales per octave  are set. •  base_sizes  ( list[int] ) – The basic sizes of anchors in multiple levels. If None is given, strides will be used to generate base_sizes. •  scale major  ( bool ) – Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 •  octave base scale  ( int ) – The base scale of octave. •  scales per octave  ( int ) – Number of scales for each octave.  octave base scale  and scales per octave  are usually used in retinanet and the  scales  should be None when they are set. •  centers  ( list[tuple[float, float]] | None ) – The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. It a list of float is given, this list will be used to shift the centers of anchors. •  center offset  ( float ) – The offset of center in proportion to anchors’ width and height. By default it is 0.5 in V2.0 but it should be 0.5 in v1.x models.\n\n ", "page_idx": 194, "bbox": [145, 185.0264434814453, 518, 449.39447021484375], "page_size": [612.0, 792.0]}
{"layout": 1578, "type": "text", "text": "Examples ", "text_level": 1, "page_idx": 194, "bbox": [95, 469, 144, 481], "page_size": [612.0, 792.0]}
{"layout": 1579, "type": "text", "text": " $>>$   from  mmdet.core  import  Legacy Anchor Generator\n\n  $>>$   self  $=$   Legacy Anchor Generator(\n\n  $>>$  [ 16 ], [ 1. ], [ 1. ], [ 9 ], center offset  $\\scriptstyle\\cdot=0\\,.\\;5$  )\n\n  $>>$   all anchors  $=$   self . grid anchors((( 2 ,  2 ),), device  $=\"$  ' cpu ' )\n\n  $>>$   print (all anchors)\n\n [tensor([[ 0., 0., 8., 8.], [16., 0., 24., 8.], [ 0., 16., 8., 24.], [16., 16., 24., 24.]])] ", "page_idx": 194, "bbox": [95, 496.8329772949219, 411, 603.0851440429688], "page_size": [612.0, 792.0]}
{"layout": 1580, "type": "text", "text": "gen single level base anchors ( base_size ,  scales ,  ratios ,  center  $=$  None ) Generate base anchors of a single level. ", "page_idx": 194, "bbox": [95, 615.5079956054688, 411, 640.9224853515625], "page_size": [612.0, 792.0]}
{"layout": 1581, "type": "text", "text": "Note:  The width/height of anchors are minused by 1 when calculating the centers and corners to meet the V1.x coordinate system. ", "page_idx": 194, "bbox": [118, 656.872802734375, 540, 682.7664794921875], "page_size": [612.0, 792.0]}
{"layout": 1582, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 194, "bbox": [136, 706, 187, 718], "page_size": [612.0, 792.0]}
{"layout": 1583, "type": "text", "text": "•  base_size  ( int | float ) – Basic size of an anchor. ", "page_idx": 195, "bbox": [154, 71.45246887207031, 382.64434814453125, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 1584, "type": "text", "text": "•  scales  ( torch.Tensor ) – Scales of the anchor. •  ratios  ( torch.Tensor ) – The ratio between between the height. and width of anchors in a single level. •  center  ( tuple[float], optional ) – The center of the base anchor related to a single feature grid. Defaults to None. ", "page_idx": 195, "bbox": [154, 89.38447570800781, 521, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 1585, "type": "text", "text": "Returns  Anchors in a single-level feature map. ", "page_idx": 195, "bbox": [137, 166.4658203125, 327.5208435058594, 181.00125122070312], "page_size": [612.0, 792.0]}
{"layout": 1586, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 195, "bbox": [137, 184.3988037109375, 242.95864868164062, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 1587, "type": "text", "text": "class  mmdet.core.anchor. Ml vl Point Generator ( strides ,  offset=0.5 ) Standard points generator for multi-level (Mlvl) feature maps in 2D points-based detectors. ", "page_idx": 195, "bbox": [71.99998474121094, 208.78697204589844, 458.0910339355469, 234.2014617919922], "page_size": [612.0, 792.0]}
{"layout": 1588, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 195, "bbox": [117, 240, 169, 252], "page_size": [612.0, 792.0]}
{"layout": 1589, "type": "text", "text": "•  strides  ( list[int] | list[tuple[int, int]] ) – Strides of anchors in multiple fea- ture levels in order (w, h). •  offset  ( float ) – The offset of points, the value is normalized with corresponding stride. Defaults to 0.5. ", "page_idx": 195, "bbox": [145, 256.7574157714844, 521, 311.91046142578125], "page_size": [612.0, 792.0]}
{"layout": 1590, "type": "text", "text": "grid priors ( feat map sizes ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $=$  'cuda' ,  with stride  $\\mathbf{\\dot{\\rho}}$  False ) Generate grid points of multiple feature levels. ", "page_idx": 195, "bbox": [96, 316.3840026855469, 442.8593444824219, 341.7984619140625], "page_size": [612.0, 792.0]}
{"layout": 1591, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 195, "bbox": [136, 348, 188, 359], "page_size": [612.0, 792.0]}
{"layout": 1592, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – List of feature map sizes in multiple feature levels, each size arrange as as (h, w). ", "page_idx": 195, "bbox": [154, 364.35345458984375, 521, 389.61846923828125], "page_size": [612.0, 792.0]}
{"layout": 1593, "type": "text", "text": "•  dtype  ( dtype ) – Dtype of priors. Default: torch.float32.  device  ( str ) – The device where the anchors will be put on.  with stride  ( bool ) – Whether to concatenate the stride to the last dimension of points. ", "page_idx": 195, "bbox": [154, 394.241455078125, 391.32196044921875, 407.5514831542969], "page_size": [612.0, 792.0]}
{"layout": 1594, "type": "text", "text": "", "page_idx": 195, "bbox": [159, 412.1744689941406, 409.5235595703125, 425.4844970703125], "page_size": [612.0, 792.0]}
{"layout": 1595, "type": "text", "text": "", "page_idx": 195, "bbox": [159, 430.10748291015625, 521, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 1596, "type": "text", "text": "Returns  Points of multiple feature levels. The sizes of each tensor should be (N, 2) when with stride is  False , where  $\\mathbf{N}=$   width \\* height, width and height are the sizes of the corresponding feature level, and the last dimension 2 represent (coord_x, coord_y), otherwise the shape should be (N, 4), and the last dimension 4 represent (coord_x, coord_y, stride_w, stride_h). ", "page_idx": 195, "bbox": [137, 447.4118347167969, 521, 497.2154846191406], "page_size": [612.0, 792.0]}
{"layout": 1597, "type": "text", "text": "Return type  list[torch.Tensor] ", "page_idx": 195, "bbox": [137, 501.2108154296875, 261.6285705566406, 515.7462158203125], "page_size": [612.0, 792.0]}
{"layout": 1598, "type": "text", "text": "property num base priors The number of priors (points) at a point on the feature grid ", "page_idx": 195, "bbox": [96, 521.4949951171875, 352.7766418457031, 545.0364990234375], "page_size": [612.0, 792.0]}
{"layout": 1599, "type": "text", "text": "Type  list[int] ", "page_idx": 195, "bbox": [137, 549.0308227539062, 192.16943359375, 563.5662231445312], "page_size": [612.0, 792.0]}
{"layout": 1600, "type": "text", "text": "property num_levels number of feature levels that the generator will be applied ", "page_idx": 195, "bbox": [96, 569.3150024414062, 348.65216064453125, 592.8565063476562], "page_size": [612.0, 792.0]}
{"layout": 1601, "type": "text", "text": "Type  int ", "page_idx": 195, "bbox": [137, 596.851806640625, 173.49949645996094, 611.38720703125], "page_size": [612.0, 792.0]}
{"layout": 1602, "type": "text", "text": "single level grid priors ( feat map size ,  level_idx ,  dtyp  $\\scriptstyle{2=}$  torch.float32 ,  device  $=$  'cuda' , with stride  $\\mathbf{\\dot{\\rho}}=$  False ) ", "page_idx": 195, "bbox": [96, 615.2630004882812, 467.1476135253906, 640.56787109375], "page_size": [612.0, 792.0]}
{"layout": 1603, "type": "text", "text": "Generate grid Points of a single level. ", "page_idx": 195, "bbox": [118, 639.3224487304688, 268.52301025390625, 652.6325073242188], "page_size": [612.0, 792.0]}
{"layout": 1604, "type": "text", "text": "Note:  This function is usually called by method  self.grid priors . ", "page_idx": 195, "bbox": [118, 668.5828247070312, 401.68365478515625, 683.1182250976562], "page_size": [612.0, 792.0]}
{"layout": 1605, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 195, "bbox": [136, 707, 187, 718], "page_size": [612.0, 792.0]}
{"layout": 1606, "type": "text", "text": "•  feat map size  ( tuple[int] ) – Size of the feature maps, arrange as (h, w). •  level_idx  ( int ) – The index of corresponding feature map level. •  dtype  ( dtype ) – Dtype of priors. Default: torch.float32. •  device  ( str, optional ) – The device the tensor will be put on. Defaults to ‘cuda’. •  with stride  ( bool ) – Concatenate the stride to the last dimension of points. ", "page_idx": 196, "bbox": [154, 71.45246887207031, 505.18927001953125, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 1607, "type": "text", "text": "Returns  Points of single feature levels. The shape of tensor should be (N, 2) when with stride is False , where  $\\mathbf{N}=$  width \\* height, width and height are the sizes of the corresponding feature level, and the last dimension 2 represent (coord_x, coord_y), otherwise the shape should be (N, 4), and the last dimension 4 represent (coord_x, coord_y, stride_w, stride_h). ", "page_idx": 196, "bbox": [137, 160.48779296875, 521, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 1608, "type": "text", "text": "Return type  Tensor ", "page_idx": 196, "bbox": [137, 214.28680419921875, 220, 228.82223510742188], "page_size": [612.0, 792.0]}
{"layout": 1609, "type": "text", "text": "single level valid flags ( feat map size ,  valid_size ,  device  $=$  'cuda' ) Generate the valid flags of points of a single feature map. ", "page_idx": 196, "bbox": [96, 238.6749725341797, 391, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 1610, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 196, "bbox": [136, 270, 188, 282], "page_size": [612.0, 792.0]}
{"layout": 1611, "type": "text", "text": " feat map size  ( tuple[int] ) – The size of feature maps, arrange as as (h, w). •  valid_size  ( tuple[int] ) – The valid size of the feature maps. The size arrange as as (h, w). •  device  ( str, optional ) – The device where the flags will be put on. Defaults to ‘cuda’. ", "page_idx": 196, "bbox": [154, 286.64544677734375, 521, 347.7754821777344], "page_size": [612.0, 792.0]}
{"layout": 1612, "type": "text", "text": "Returns  The valid flags of each points in a single level feature map. Return type  torch.Tensor ", "page_idx": 196, "bbox": [137, 351.77081298828125, 410, 384.2392578125], "page_size": [612.0, 792.0]}
{"layout": 1613, "type": "text", "text": "sparse priors ( prior_idxs ,  feat map size ,  level_idx ,  dtype  $=$  torch.float32 ,  device  $\\mathbf{=}$  'cuda' ) Generate sparse points according to the  prior_idxs . ", "page_idx": 196, "bbox": [96, 388.11505126953125, 459.39727783203125, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 1614, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 196, "bbox": [136, 419, 188, 431], "page_size": [612.0, 792.0]}
{"layout": 1615, "type": "text", "text": "•  prior_idxs  ( Tensor ) – The index of corresponding anchors in the feature map. •  feat map size  ( tuple[int] ) – feature map size arrange as (w, h). •  level_idx  ( int ) – The level index of corresponding feature map. •  (obj  ( device ) –  torch.dtype ): Date type of points. Defaults to  torch.float32 . •  (obj  –  torch.device ): The device where the points is located. Returns  Anchor with shape (N, 2), N should be equal to the length of  prior_idxs . And last dimension 2 represent (coord_x, coord_y). Return type  Tensor ", "page_idx": 196, "bbox": [137, 436.0845031738281, 521, 569.5443115234375], "page_size": [612.0, 792.0]}
{"layout": 1616, "type": "text", "text": "valid flags ( feat map sizes ,  pad_shape ,  device  $\\mathbf{=}$  'cuda' ) Generate valid flags of points of multiple feature levels. ", "page_idx": 196, "bbox": [96, 573.4201049804688, 339.8152770996094, 598.8345947265625], "page_size": [612.0, 792.0]}
{"layout": 1617, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 196, "bbox": [136, 604, 188, 617], "page_size": [612.0, 792.0]}
{"layout": 1618, "type": "text", "text": "•  feat map sizes  ( list(tuple) ) – List of feature map sizes in multiple feature levels, each size arrange as as (h, w). •  pad_shape  ( tuple(int) ) – The padded shape of the image, arrange as (h, w). •  device  ( str ) – The device where the anchors will be put on. ", "page_idx": 196, "bbox": [154, 621.3895263671875, 521, 682.5205688476562], "page_size": [612.0, 792.0]}
{"layout": 1619, "type": "text", "text": "Returns  Valid flags of points of multiple levels. ", "page_idx": 196, "bbox": [137, 686.515869140625, 333, 701.05126953125], "page_size": [612.0, 792.0]}
{"layout": 1620, "type": "text", "text": "Return type  list(torch.Tensor) ", "page_idx": 196, "bbox": [137, 704.4489135742188, 261.62847900390625, 718.9843139648438], "page_size": [612.0, 792.0]}
{"layout": 1621, "type": "text", "text": "class  mmdet.core.anchor. YO LO Anchor Generator ( strides ,  base_sizes ) Anchor generator for YOLO. ", "page_idx": 197, "bbox": [72.0, 71.30303192138672, 380, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1622, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 197, "bbox": [118, 102, 169, 114], "page_size": [612.0, 792.0]}
{"layout": 1623, "type": "text", "text": "•  strides  ( list[int] | list[tuple[int, int]] ) – Strides of anchors in multiple fea- ture levels. •  base_sizes  ( list[list[tuple[int, int]]] ) – The basic sizes of anchors in multiple levels. ", "page_idx": 197, "bbox": [145, 119.27247619628906, 518, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 1624, "type": "text", "text": "gen base anchors () ", "page_idx": 197, "bbox": [96, 180.77200317382812, 191.0537109375, 191.68106079101562], "page_size": [612.0, 792.0]}
{"layout": 1625, "type": "text", "text": "Returns  Base anchors of a feature grid in multiple feature levels. ", "page_idx": 197, "bbox": [137, 208.308837890625, 400, 222.84426879882812], "page_size": [612.0, 792.0]}
{"layout": 1626, "type": "text", "text": "Return type  list(torch.Tensor) ", "page_idx": 197, "bbox": [137, 226.2418212890625, 261, 240.77725219726562], "page_size": [612.0, 792.0]}
{"layout": 1627, "type": "text", "text": "gen single level base anchors ( base sizes per level ,  center=None ) Generate base anchors of a single level. ", "page_idx": 197, "bbox": [96, 244.6529998779297, 400, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 1628, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 197, "bbox": [136, 276, 188, 288], "page_size": [612.0, 792.0]}
{"layout": 1629, "type": "text", "text": "•  base sizes per level  ( list[tuple[int, int]] ) – Basic sizes of anchors. •  center  ( tuple[float], optional ) – The center of the base anchor related to a single feature grid. Defaults to None. Returns  Anchors in a single-level feature maps. Return type  torch.Tensor ", "page_idx": 197, "bbox": [137, 292.62249755859375, 521.3729858398438, 372.2843017578125], "page_size": [612.0, 792.0]}
{"layout": 1630, "type": "text", "text": "property num_levels number of feature levels that the generator will be applied ", "page_idx": 197, "bbox": [96, 378.03204345703125, 348.652099609375, 401.5745544433594], "page_size": [612.0, 792.0]}
{"layout": 1631, "type": "text", "text": "Type  int ", "text_level": 1, "page_idx": 197, "bbox": [137, 408, 174, 420], "page_size": [612.0, 792.0]}
{"layout": 1632, "type": "text", "text": "responsible flags ( feat map sizes ,  gt_bboxes ,  device  $=$  cuda' ) Generate responsible anchor flags of grid cells in multiple scales. ", "page_idx": 197, "bbox": [96, 423.9801025390625, 380, 449.3945617675781], "page_size": [612.0, 792.0]}
{"layout": 1633, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 197, "bbox": [136, 455, 187, 467], "page_size": [612.0, 792.0]}
{"layout": 1634, "type": "text", "text": "•  feat map sizes  ( list(tuple) ) – List of feature map sizes in multiple feature levels. •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (n, 4). •  device  ( str ) – Device where the anchors will be put on. Returns  responsible flags of anchors in multiple level Return type  list(torch.Tensor) ", "page_idx": 197, "bbox": [137, 471.9505615234375, 510, 557.58935546875], "page_size": [612.0, 792.0]}
{"layout": 1635, "type": "text", "text": "single level responsible flags ( feat map size ,  gt_bboxes ,  stride ,  num base anchors ,  device  $=$  'cuda' ) Generate the responsible flags of anchor in a single feature map. ", "page_idx": 197, "bbox": [96, 561.4651489257812, 532.0042114257812, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 1636, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 197, "bbox": [136, 593, 188, 604], "page_size": [612.0, 792.0]}
{"layout": 1637, "type": "text", "text": "•  feat map size  ( tuple[int] ) – The size of feature maps. •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (n, 4). •  stride  ( tuple(int) ) – stride of current level •  num base anchors  ( int ) – The number of base anchors. •  device  ( str, optional ) – Device where the flags will be put on. Defaults to ‘cuda’. Returns  The valid flags of each anchor in a single level feature map. ", "page_idx": 197, "bbox": [137, 609.4345703125, 510, 713.00634765625], "page_size": [612.0, 792.0]}
{"layout": 1638, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 198, "bbox": [137, 70.8248291015625, 245, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 1639, "type": "text", "text": "mmdet.core.anchor. anchor inside flags ( flat anchors ,  valid flags ,  img_shape ,  allowed border  $\\mathord{\\leftrightharpoons}\\!O_{.}$  ) Check whether the anchors are inside the border. ", "page_idx": 198, "bbox": [71, 89.23503875732422, 501.8023376464844, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 1640, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 198, "bbox": [117, 121, 170, 132], "page_size": [612.0, 792.0]}
{"layout": 1641, "type": "text", "text": "•  flat anchors  ( torch.Tensor ) – Flatten anchors, shape (n, 4). •  valid flags  ( torch.Tensor ) – An existing valid flags of anchors. •  img_shape  ( tuple(int) ) – Shape of current image. •  allowed border  ( int, optional ) – The border to allow the valid anchor. Defaults to 0. ", "page_idx": 198, "bbox": [145, 137.20545959472656, 518.0787963867188, 204.31346130371094], "page_size": [612.0, 792.0]}
{"layout": 1642, "type": "text", "text": "Returns  Flags indicating whether the anchors are inside a valid range. ", "page_idx": 198, "bbox": [118, 208.30877685546875, 401, 222.84420776367188], "page_size": [612.0, 792.0]}
{"layout": 1643, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 198, "bbox": [118, 226.24176025390625, 224.32859802246094, 240.77719116210938], "page_size": [612.0, 792.0]}
{"layout": 1644, "type": "text", "text": "mmdet.core.anchor. calc region ( bbox ,  ratio ,  feat map size  $=$  None ) Calculate a proportional bbox region. ", "page_idx": 198, "bbox": [71, 244.65293884277344, 361, 270.06744384765625], "page_size": [612.0, 792.0]}
{"layout": 1645, "type": "text", "text": "The bbox center are fixed and the new h’ and w’ is h \\* ratio and w \\* ratio. ", "page_idx": 198, "bbox": [96, 274.6904296875, 390, 288.0004577636719], "page_size": [612.0, 792.0]}
{"layout": 1646, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 198, "bbox": [117, 294, 170, 306], "page_size": [612.0, 792.0]}
{"layout": 1647, "type": "text", "text": "•  bbox  ( Tensor ) – Bboxes to calculate regions, shape (n, 4). •  ratio  ( float ) – Ratio of the output region. •  feat map size  ( tuple ) – Feature map size used for clipping the boundary. Returns  x1, y1, x2, y2 Return type  tuple ", "page_idx": 198, "bbox": [118, 310.5554504394531, 456.2431335449219, 396.19427490234375], "page_size": [612.0, 792.0]}
{"layout": 1648, "type": "text", "text": "Convert targets by image to targets by feature level. [target img 0, target img 1]  $->$   [target level 0, target level 1, ...] ", "page_idx": 198, "bbox": [96, 412.17449951171875, 347.78515625, 443.41754150390625], "page_size": [612.0, 792.0]}
{"layout": 1649, "type": "text", "text": "37.2 bbox ", "text_level": 1, "page_idx": 198, "bbox": [71, 466, 142, 482], "page_size": [612.0, 792.0]}
{"layout": 1650, "type": "text", "text": "class  mmdet.core.bbox. Assign Result ( num_gts ,  gt_inds ,  max overlaps ,  labels  $:=$  None ) Stores assignments between predicted and truth boxes. ", "page_idx": 198, "bbox": [71, 499.4630432128906, 444.8412780761719, 524.8775024414062], "page_size": [612.0, 792.0]}
{"layout": 1651, "type": "text", "text": "num_gts the number of truth boxes considered when computing this assignment ", "page_idx": 198, "bbox": [96, 531.2239990234375, 401, 554.7655029296875], "page_size": [612.0, 792.0]}
{"layout": 1652, "type": "text", "text": "Type  int gt_inds for each predicted box indicates the 1-based index of the assigned truth box. 0 means unassigned and -1 means ignore. Type  LongTensor max overlaps the iou between the predicted box and its assigned truth box. Type  Float Tensor labels If specified, for each predicted box indicates the category label of the assigned truth box. ", "page_idx": 198, "bbox": [96, 558.7608032226562, 540.0038452148438, 710.1824951171875], "page_size": [612.0, 792.0]}
{"layout": 1653, "type": "text", "text": "Type  None | LongTensor\n\n ", "page_idx": 199, "bbox": [137, 70.8248291015625, 239.18289184570312, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 1654, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 199, "bbox": [96, 105, 139, 116], "page_size": [612.0, 792.0]}
{"layout": 1655, "type": "text", "text": " $>>$   # An assign result between 4 predicted boxes and 9 true boxes\n\n  $>>$   # where only two boxes were assigned.\n\n  $>>$   num_gts  $\\c=~9\n\n$   $>>$   max overlaps  $=$   torch . LongTensor([ 0 ,  .5 ,  .9 ,  0 ])\n\n  $>>$   gt_inds  $=$   torch . LongTensor([ - 1 ,  1 ,  2 ,  0 ])\n\n  $>>$   labels  $=$   torch . LongTensor([ 0 ,  3 ,  4 ,  0 ])\n\n  $>>$   self  $=$   Assign Result(num_gts, gt_inds, max overlaps, labels)\n\n >>>  print ( str ( self )) # xdoctest: +IGNORE WANT\n\n <Assign Result(num_gts  $\\scriptstyle{\\varepsilon=9}$  , gt_inds.shape  $=$  (4,), max overlaps.shape  $=$  (4,), labels.shape  $=$  (4,))>\n\n >>>  # Force addition of gt labels (when adding gt as proposals)\n\n  $>>$   new_labels  $=$   torch . LongTensor([ 3 ,  4 ,  5 ])\n\n  $>>$   self . add_gt_(new_labels)\n\n  $>>$   print ( str ( self )) # xdoctest:   $^+$  IGNORE WANT\n\n <Assign Result(num_gts  $_{;=9}$  , gt_inds.shape  $\\mathord{\\mathfrak{z}}(7\\,,)$  , max overlaps.shape  $=$  (7,), labels.shape  $=\\!(7,))\\!>$  ", "page_idx": 199, "bbox": [95, 132.79872131347656, 457, 323], "page_size": [612.0, 792.0]}
{"layout": 1656, "type": "text", "text": "Type  dict ", "page_idx": 199, "bbox": [137, 441.6798095703125, 177, 456.2152404785156], "page_size": [612.0, 792.0]}
{"layout": 1657, "type": "text", "text": "Create random Assign Result for tests or debugging. Parameters •  num_preds  – number of predicted boxes •  num_gts  – number of true boxes •  p_ignore  ( float ) – probability of a predicted box assigned to an ignored truth •  p_assigned  ( float ) – probability of a predicted box not being assigned •  p use label  ( float | bool ) – with labels or not •  rng  ( None | int | numpy.random.Random State ) – seed or state Returns  Randomly generated assign results. Return type  Assign Result ", "page_idx": 199, "bbox": [118.82400512695312, 520.0164184570312, 483.236083984375, 695.3192138671875], "page_size": [612.0, 792.0]}
{"layout": 1658, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 200, "bbox": [117, 72, 161, 85], "page_size": [612.0, 792.0]}
{"layout": 1659, "type": "table", "page_idx": 200, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_120.jpg", "bbox": [115, 96, 544, 139], "page_size": [612.0, 792.0], "ocr_text": ">>> from mmdet.core.bbox.assigners.assign_result import * # NOQA\n>>> self = AssignResult.random()\n>>> print(self.info)\n", "vlm_text": "The image displays a snippet of Python code set within a table-like format. The code is as follows:\n\n```python\n>>> from mmdet.core.bbox.assigners.assign_result import *  # NOQA\n>>> self = AssignResult.random()\n>>> print(self.info)\n```\n\nThis code is likely part of a demonstration or example related to object detection or bounding box assignment, possibly utilizing the MMDetection library (a PyTorch-based object detection toolbox). Here's a breakdown of what each line does:\n\n1. `from mmdet.core.bbox.assigners.assign_result import *  # NOQA`: Imports all classes and functions from the `assign_result` module within the `bbox` (bounding box) package of the MMDetection library. The `# NOQA` comment is typically used to tell linters to ignore this line, possibly because wildcard imports are generally discouraged for clarity reasons.\n\n2. `self = AssignResult.random()`: Creates an instance of the `AssignResult` class using a method `random()`. This suggests that `AssignResult` has a static or class method named `random` that generates a randomized instance, likely for testing or demonstration purposes.\n\n3. `print(self.info)`: Prints the `info` attribute or method of the `AssignResult` instance. This could display diagnostic information or a summary of the properties associated with the `AssignResult` instance."}
{"layout": 1660, "type": "text", "text": "set extra property ( key ,  value ) Set user-defined new property. ", "page_idx": 200, "bbox": [96, 147.27403259277344, 241, 172.6894989013672], "page_size": [612.0, 792.0]}
{"layout": 1661, "type": "text", "text": "class  mmdet.core.bbox. Base As signer Base assigner that assigns boxes to ground truth boxes. ", "page_idx": 200, "bbox": [72, 179.03500366210938, 315.346923828125, 202.57749938964844], "page_size": [612.0, 792.0]}
{"layout": 1662, "type": "text", "text": "abstract assign ( bboxes ,  gt_bboxes ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_labels=None ) Assign boxes to either a ground truth boxes or a negative boxes. ", "page_idx": 200, "bbox": [96, 207.05003356933594, 430.6153869628906, 232.4654998779297], "page_size": [612.0, 792.0]}
{"layout": 1663, "type": "text", "text": "class  mmdet.core.bbox. Base B Box Code r ( \\*\\*kwargs ) Base bounding box coder. ", "page_idx": 200, "bbox": [72, 236.9380340576172, 303.8743896484375, 262.3525390625], "page_size": [612.0, 792.0]}
{"layout": 1664, "type": "text", "text": "abstract decode ( bboxes ,  b boxes p red ) Decode the predicted bboxes according to prediction and base boxes. ", "page_idx": 200, "bbox": [96, 266.8260498046875, 393.0943298339844, 292.2405090332031], "page_size": [612.0, 792.0]}
{"layout": 1665, "type": "text", "text": "abstract encode ( bboxes ,  gt_bboxes ) Encode deltas between bboxes and ground truth boxes. ", "page_idx": 200, "bbox": [96, 296.71405029296875, 336.66619873046875, 322.1285095214844], "page_size": [612.0, 792.0]}
{"layout": 1666, "type": "text", "text": "class  mmdet.core.bbox. Base Sampler ( num ,  pos fraction ,  neg_pos_ub=- 1 ,  add gt as proposals  $\\mathbf{\\tilde{=}}$  True , \\*\\*kwargs ) ", "page_idx": 200, "bbox": [72, 326.60205078125, 510.5546875, 351.90692138671875], "page_size": [612.0, 792.0]}
{"layout": 1667, "type": "text", "text": "(  $\\leftrightharpoons$  ) Sample positive and negative bboxes. This is a simple implementation of bbox sampling given candidates, assigning results and ground truth bboxes. ", "page_idx": 200, "bbox": [118, 370.9157409667969, 540.0029296875, 423.74749755859375], "page_size": [612.0, 792.0]}
{"layout": 1668, "type": "text", "text": "Parameters •  assign result  ( Assign Result ) – Bbox assigning results. •  bboxes  ( Tensor ) – Boxes to be sampled from. •  gt_bboxes  ( Tensor ) – Ground truth bboxes. •  gt_labels  ( Tensor, optional ) – Class labels of ground truth bboxes. Returns  Sampling result. Return type  Sampling Result ", "page_idx": 200, "bbox": [137, 427.7428283691406, 457.9762878417969, 549.874267578125], "page_size": [612.0, 792.0]}
{"layout": 1669, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 200, "bbox": [117, 569, 161, 581], "page_size": [612.0, 792.0]}
{"layout": 1670, "type": "table", "page_idx": 200, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_121.jpg", "bbox": [113, 592, 543, 711], "page_size": [612.0, 792.0], "ocr_text": ">>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n\nfrom mndet.core.bbox import RandomSampler\n\nfrom mmdet.core.bbox import AssignResult\n\nfrom mndet.core.bbox.demodata import ensure_rng, random_boxes\nrng = ensure_rng (None)\n\nassign_result = AssignResult.random(rng=rng)\n\nbboxes = random_boxes(assign_result .num_preds, rng=rng)\ngt_bboxes = random_boxes(assign_result.num_gts, rng=rng)\ngt_labels = None\n\nself = RandomSampler(num=32, pos_fraction=0.5, neg_pos_ub=-1,\n\n", "vlm_text": "The image shows a Python code snippet related to a machine learning or computer vision framework, likely used for object detection. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `RandomSampler` and `AssignResult` from `mmdet.core.bbox`.\n   - `ensure_rng` and `random_boxes` from `mmdet.core.bbox.demodata`.\n\n2. **Code**:\n   - Initializes a random number generator `rng` using `ensure_rng(None)`.\n   - Creates a random assignment result using `AssignResult.random(rng=rng)`.\n   - Generates random bounding boxes (`bboxes`) and ground truth bounding boxes (`gt_bboxes`) based on the numbers of predictions and ground truths, respectively.\n   - `gt_labels` is set to `None`, indicating no ground truth labels are assigned.\n   - Initializes a `RandomSampler` object with parameters `num=32`, `pos_fraction=0.5`, and `neg_pos_ub=-1`.\n\nThis snippet appears to be part of a setup script for testing or simulating detection models."}
{"layout": 1671, "type": "table", "page_idx": 201, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_122.jpg", "bbox": [114, 81, 543, 114], "page_size": [612.0, 792.0], "ocr_text": ">>> add_gt_as_proposals=False)\n>>> self = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n", "vlm_text": "The image shows a snippet of Python code. It includes:\n\n```python\nadd_gt_as_proposals=False\nself = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n```\n\n- `add_gt_as_proposals=False` likely indicates a parameter setting where ground truth is not added as proposals.\n- `self = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)` is a method call for sampling with the inputs `assign_result`, `bboxes`, `gt_bboxes`, and `gt_labels`."}
{"layout": 1672, "type": "text", "text": "class  mmdet.core.bbox. B box Overlaps 2 D ( scale  $\\mathrm{=}l.0$  ,  dtype  $=$  None ) 2D Overlaps (e.g. IoUs, GIoUs) Calculator. ", "page_idx": 201, "bbox": [71, 122.06401824951172, 364.47235107421875, 147.47850036621094], "page_size": [612.0, 792.0]}
{"layout": 1673, "type": "text", "text": "class  mmdet.core.bbox. Center Region As signer ( pos_scale ,  neg_scale ,  min_pos_io  $\\it\\Delta\\it{f}=0.01$  , ignore gt scale  $\\mathrm{\\Sigma=}0.5$  ,  foreground dominate  $\\mathbf{\\dot{\\rho}}=$  False , i ou calculator  $\\mathbf{=}$  {'type': 'B box Overlaps 2 D'} ) Assign pixels at the center region of a bbox as positive. ", "page_idx": 201, "bbox": [71, 151.95201110839844, 501.2696533203125, 201.2775115966797], "page_size": [612.0, 792.0]}
{"layout": 1674, "type": "text", "text": "Each proposals will be assigned with    $\\cdot l,\\theta$  , or a positive integer indicating the ground truth index. - -1: negative samples - semi-positive numbers: positive sample, index (0-based) of assigned gt ", "page_idx": 201, "bbox": [96, 205.7500457763672, 540, 231.16551208496094], "page_size": [612.0, 792.0]}
{"layout": 1675, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 201, "bbox": [117, 237, 170, 249], "page_size": [612.0, 792.0]}
{"layout": 1676, "type": "text", "text": "•  pos_scale  ( float ) – Threshold within which pixels are labelled as positive. •  neg_scale  ( float ) – Threshold above which pixels are labelled as positive. •  min pos i of  ( float ) – Minimum iof of a pixel with a gt to be labelled as positive. Default: 1e-2 •  ignore gt scale  ( float ) – Threshold within which the pixels are ignored when the gt is labelled as shadowed. Default: 0.5 •  foreground dominate  ( bool ) – If True, the bbox will be assigned as positive when a gt’s kernel region overlaps with another’s shadowed (ignored) region, otherwise it is set as ignored. Default to False. ", "page_idx": 201, "bbox": [145, 253.72047424316406, 518, 386.58251953125], "page_size": [612.0, 792.0]}
{"layout": 1677, "type": "text", "text": "assign ( bboxes ,  gt_bboxes ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_labels  $\\mathbf{\\check{\\Sigma}}$  None ) Assign gt to bboxes. ", "page_idx": 201, "bbox": [96, 391.0560607910156, 383.541259765625, 416.47052001953125], "page_size": [612.0, 792.0]}
{"layout": 1678, "type": "text", "text": "This method assigns gts to every bbox (proposal/anchor), each bbox will be assigned with -1, or a semi- positive number. -1 means negative sample, semi-positive number is the index (0-based) of assigned gt. ", "page_idx": 201, "bbox": [118, 421.093505859375, 540, 446.3585205078125], "page_size": [612.0, 792.0]}
{"layout": 1679, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 201, "bbox": [136, 452, 187, 464], "page_size": [612.0, 792.0]}
{"layout": 1680, "type": "text", "text": "•  bboxes  ( Tensor ) – Bounding boxes to be assigned, shape(n, 4). •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (k, 4). •  gt b boxes ignore  ( tensor, optional ) – Ground truth bboxes that are labelled as ignored , e.g., crowd boxes in COCO. •  gt_labels  ( tensor, optional ) – Label of gt_bboxes, shape (num_gts,). ", "page_idx": 201, "bbox": [154, 468.91351318359375, 521, 547.9775390625], "page_size": [612.0, 792.0]}
{"layout": 1681, "type": "text", "text": "Returns  The assigned result. Note that shadowed labels of shape (N, 2) is also added as an assign result  attribute.  shadowed labels  is a tensor composed of N pairs of anchor_ind, class label], where N is the number of anchors that lie in the outer region of a gt, anchor_ind is the shadowed anchor index and class label is the shadowed class label. ", "page_idx": 201, "bbox": [137, 551.9718627929688, 521, 601.7755737304688], "page_size": [612.0, 792.0]}
{"layout": 1682, "type": "text", "text": "Return type  Assign Result ", "page_idx": 201, "bbox": [137, 605.7708740234375, 256, 620.3062744140625], "page_size": [612.0, 792.0]}
{"layout": 1683, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 202, "bbox": [117, 72, 161, 85], "page_size": [612.0, 792.0]}
{"layout": 1684, "type": "text", "text": ">>>  self  $=$   Center Region As signer( 0.2 ,  0.2 ) >>>  bboxes  $=$   torch . Tensor([[ 0 ,  0 ,  10 ,  10 ], [ 10 ,  10 ,  20 ,  20 ]]) >>>  gt_bboxes  $=$   torch . Tensor([[ 0 ,  0 ,  10 ,  10 ]]) >>>  assign result  $=$   self . assign(bboxes, gt_bboxes) >>>  expected gt in ds  $=$   torch . LongTensor([ 1 ,  0 ])  $>>$   assert  torch . all(assign result . gt_inds  $==$   expected gt in ds) ", "page_idx": 202, "bbox": [117, 100.33097076416016, 448.33837890625, 171.01510620117188], "page_size": [612.0, 792.0]}
{"layout": 1685, "type": "text", "text": "assign one hot gt indices ( is b box in gt core ,  is b box in gt shadow ,  gt priority  $\\scriptstyle\\sum=.$  None ) Assign only one gt index to each prior box. ", "page_idx": 202, "bbox": [96, 183.13999938964844, 488.75634765625, 208.5544891357422], "page_size": [612.0, 792.0]}
{"layout": 1686, "type": "text", "text": "Gts with large gt priority are more likely to be assigned. ", "page_idx": 202, "bbox": [117, 213.1774444580078, 344, 226.4874725341797], "page_size": [612.0, 792.0]}
{"layout": 1687, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 202, "bbox": [136, 232, 187, 244], "page_size": [612.0, 792.0]}
{"layout": 1688, "type": "text", "text": "•  is b box in gt core  ( Tensor ) – Bool tensor indicating the bbox center is in the core area of a gt (e.g. 0-0.2). Shape: (num_prior, num_gt). •  is b box in gt shadow  ( Tensor ) – Bool tensor indicating the bbox center is in the shad- owed area of a gt (e.g. 0.2-0.5). Shape: (num_prior, num_gt). •  gt priority  ( Tensor ) – Priorities of gts. The gt with a higher priority is more likely to be assigned to the bbox when the bbox match with multiple gts. Shape: (num_gt, ). ", "page_idx": 202, "bbox": [154, 249.0424346923828, 521, 334.0834655761719], "page_size": [612.0, 792.0]}
{"layout": 1689, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 202, "bbox": [136, 340, 173, 352], "page_size": [612.0, 792.0]}
{"layout": 1690, "type": "text", "text": "Returns (assigned gt in ds, shadowed gt in ds). • assigned gt in ds: The assigned gt index of each prior bbox (i.e. index from 1 to num_gts). Shape: (num_prior, ). • shadowed gt in ds: shadowed gt indices. It is a tensor of shape (num_ignore, 2) with first column being the shadowed prior bbox indices and the second column the shadowed gt indices (1-based). ", "page_idx": 202, "bbox": [154, 356.63946533203125, 521, 441.68048095703125], "page_size": [612.0, 792.0]}
{"layout": 1691, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 202, "bbox": [137, 447, 214, 459], "page_size": [612.0, 792.0]}
{"layout": 1692, "type": "text", "text": "get gt priorities ( gt_bboxes ) Get gt priorities according to their areas. ", "page_idx": 202, "bbox": [96, 464.0870361328125, 280.5269775390625, 489.5014953613281], "page_size": [612.0, 792.0]}
{"layout": 1693, "type": "text", "text": "Smaller gt has higher priority. ", "page_idx": 202, "bbox": [117, 494.12347412109375, 238, 507.4335021972656], "page_size": [612.0, 792.0]}
{"layout": 1694, "type": "text", "text": "Parameters  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (k, 4). ", "page_idx": 202, "bbox": [137, 511.4288330078125, 420.5722961425781, 525.9642333984375], "page_size": [612.0, 792.0]}
{"layout": 1695, "type": "text", "text": "Returns  The priority of gts so that gts with larger priority is more likely to be assigned. Shape (k, ) ", "page_idx": 202, "bbox": [137, 529.36181640625, 521, 555.2545166015625], "page_size": [612.0, 792.0]}
{"layout": 1696, "type": "text", "text": "Return type  Tensor ", "page_idx": 202, "bbox": [137, 559.2498168945312, 220.1442413330078, 573.7852172851562], "page_size": [612.0, 792.0]}
{"layout": 1697, "type": "text", "text": "class  mmdet.core.bbox. Combined Sampler ( pos sampler ,  ne g sampler ,  \\*\\*kwargs ) A sampler that combines positive sampler and negative sampler. ", "page_idx": 202, "bbox": [71, 577.6610717773438, 426.684326171875, 603.0755004882812], "page_size": [612.0, 792.0]}
{"layout": 1698, "type": "text", "text": "class  mmdet.core.bbox. Delta XY WH B Box Code r ( target means  $=$  (0.0, 0.0, 0.0, 0.0) ,  target stds  $\\mathbf{\\tilde{=}}$  (1.0, 1.0, 1.0, 1.0) ,  clip border  $\\bf{\\underline{{\\delta}}}$  True ,  add c tr clamp  $\\leftrightharpoons$  False ,  ctr_clamp  $\\scriptstyle{=32}$  ) ", "page_idx": 202, "bbox": [71, 607.5490112304688, 540, 632.9635009765625], "page_size": [612.0, 792.0]}
{"layout": 1699, "type": "text", "text": "Delta XYWH BBox coder. ", "page_idx": 202, "bbox": [96, 631.6084594726562, 203.9650115966797, 644.9185180664062], "page_size": [612.0, 792.0]}
{"layout": 1700, "type": "text", "text": "Following the practice in  R-CNN , this coder encodes bbox (x1, y1, x2, y2) into delta (dx, dy, dw, dh) and decodes delta (dx, dy, dw, dh) back to original bbox (x1, y1, x2, y2). ", "page_idx": 202, "bbox": [96, 649.5414428710938, 540, 674.8065185546875], "page_size": [612.0, 792.0]}
{"layout": 1701, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 202, "bbox": [117, 681, 169, 693], "page_size": [612.0, 792.0]}
{"layout": 1702, "type": "text", "text": "•  target means  ( Sequence[float] ) – De normalizing means of target for delta coordinates ", "page_idx": 202, "bbox": [145.92295837402344, 697.3614501953125, 518.0858154296875, 710.6715087890625], "page_size": [612.0, 792.0]}
{"layout": 1703, "type": "text", "text": "•  target stds  ( Sequence[float] ) – De normalizing standard deviation of target for delta coordinates •  clip border  ( bool, optional ) – Whether clip the objects outside the border of the im- age. Defaults to True. •  add c tr clamp  ( bool ) – Whether to add center clamp, when added, the predicted box is clamped is its center is too far away from the original anchor’s center. Only used by YOLOF. Default False. •  ctr_clamp  ( int ) – the maximum pixel shift to clamp. Only used by YOLOF. Default 32. ", "page_idx": 203, "bbox": [145, 71.45246887207031, 518, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 1704, "type": "text", "text": "decode ( bboxes ,  p red b boxes ,  max_shape  $=$  None ,  wh ratio clip=0.016 ) Apply transformation  p red b boxes  to  boxes . ", "page_idx": 203, "bbox": [96, 190.8540496826172, 388.9963684082031, 216.26951599121094], "page_size": [612.0, 792.0]}
{"layout": 1705, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 203, "bbox": [137, 223, 188, 234], "page_size": [612.0, 792.0]}
{"layout": 1706, "type": "text", "text": "•  bboxes  ( torch.Tensor ) – Basic boxes. Shape (B, N, 4) or (N, 4) ", "page_idx": 203, "bbox": [154, 238.82447814941406, 430.0462951660156, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 1707, "type": "text", "text": "•  p red b boxes  ( Tensor ) – Encoded offsets with respect to each roi. Has shape (B, N, num classes   $^{*}\\,4$  ) or (B, N, 4) or (N, num classes  $^{*}\\,4$  ) or (N, 4). Note  $\\mathbf{N}=$   num anchors \\*  $\\textrm{W}^{*}\\textrm{H}$   when rois is a grid of anchors.Offset encoding follows 1 . •  (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If bboxes shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and the length of max_shape should also be B. •  wh ratio clip  ( float, optional ) – The allowed ratio between width and height. ", "page_idx": 203, "bbox": [154, 256.7574768066406, 521, 365.70849609375], "page_size": [612.0, 792.0]}
{"layout": 1708, "type": "text", "text": "Returns  Decoded boxes. ", "page_idx": 203, "bbox": [137, 369.7038269042969, 242, 384.2392578125], "page_size": [612.0, 792.0]}
{"layout": 1709, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 203, "bbox": [137, 387.6368408203125, 242, 402.1722717285156], "page_size": [612.0, 792.0]}
{"layout": 1710, "type": "text", "text": "encode ( bboxes ,  gt_bboxes ) Get box regression transformation deltas that can be used to transform the  bboxes  into the  gt_bboxes . ", "page_idx": 203, "bbox": [96, 406.04705810546875, 531, 431.4625244140625], "page_size": [612.0, 792.0]}
{"layout": 1711, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 203, "bbox": [136, 438, 187, 449], "page_size": [612.0, 792.0]}
{"layout": 1712, "type": "text", "text": "•  bboxes  ( torch.Tensor ) – Source boxes, e.g., object proposals. •  gt_bboxes  ( torch.Tensor ) – Target of the transformation, e.g., ground-truth boxes. Returns  Box transformation deltas Return type  torch.Tensor ", "page_idx": 203, "bbox": [137, 454.01751708984375, 507, 521.7232666015625], "page_size": [612.0, 792.0]}
{"layout": 1713, "type": "text", "text": "class mmdet.core.bbox.Distance Point B Box Code r(clip border $\\bf{=}$ True)Distance Point BBox coder. ", "page_idx": 203, "bbox": [71.99989318847656, 525.59912109375, 384.1382751464844, 551.0136108398438], "page_size": [612.0, 792.0]}
{"layout": 1714, "type": "text", "text": "This coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left, right) and decode it back to the original. ", "page_idx": 203, "bbox": [96, 555.6365356445312, 531, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 1715, "type": "text", "text": "Parameters  clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. ", "page_idx": 203, "bbox": [118, 572.94189453125, 518, 598.8345947265625], "page_size": [612.0, 792.0]}
{"layout": 1716, "type": "text", "text": "decode ( points ,  p red b boxes ,  max_shape=None ) Decode distance prediction to bounding box. ", "page_idx": 203, "bbox": [96, 603.30810546875, 297, 628.7225952148438], "page_size": [612.0, 792.0]}
{"layout": 1717, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 203, "bbox": [136, 634, 188, 646], "page_size": [612.0, 792.0]}
{"layout": 1718, "type": "text", "text": "•  points  ( Tensor ) – Shape (B, N, 2) or (N, 2). •  p red b boxes  ( Tensor ) – Distance from the given point to 4 boundaries (left, top, right, bottom). Shape (B, N, 4) or (N, 4) ", "page_idx": 203, "bbox": [154, 651.2775268554688, 521, 694.4755859375], "page_size": [612.0, 792.0]}
{"layout": 1719, "type": "text", "text": " (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If priors shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]], and the length of max_shape should also be B. Default None. ", "page_idx": 204, "bbox": [159.3719024658203, 71.45246887207031, 521, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 1720, "type": "text", "text": "Returns  Boxes with shape (N, 4) or (B, N, 4) Return type  Tensor ", "page_idx": 204, "bbox": [137, 124.62286376953125, 320.7662048339844, 157.09127807617188], "page_size": [612.0, 792.0]}
{"layout": 1721, "type": "text", "text": "encode ( points ,  gt_bboxes ,  max_dis  $\\leftrightharpoons$  None ,  eps=0.1 ) Encode bounding box to distances. ", "page_idx": 204, "bbox": [96, 160.96604919433594, 311.32232666015625, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 1722, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 204, "bbox": [136, 193, 188, 203], "page_size": [612.0, 792.0]}
{"layout": 1723, "type": "text", "text": "•  points  ( Tensor ) – Shape (N, 2), The format is [x, y]. •  gt_bboxes  ( Tensor ) – Shape (N, 4), The format is “xyxy” •  max_dis  ( float ) – Upper bound of the distance. Default None. •  eps  ( float ) – a small value to ensure target  $<$   max_dis, instead  $<=$  . Default 0.1. ", "page_idx": 204, "bbox": [154, 208.9364776611328, 487, 276.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 1724, "type": "text", "text": "Returns  Box transformation deltas. The shape is (N, 4). ", "page_idx": 204, "bbox": [137, 280.0398254394531, 363.6252746582031, 294.57525634765625], "page_size": [612.0, 792.0]}
{"layout": 1725, "type": "text", "text": "Return type  Tensor ", "page_idx": 204, "bbox": [137, 297.97283935546875, 220, 312.5082702636719], "page_size": [612.0, 792.0]}
{"layout": 1726, "type": "text", "text": "class  mmdet.core.bbox. Instance Balanced Pos Sampler ( num ,  pos fraction ,  neg_pos_ub=- 1 , add gt as proposals  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ,  \\*\\*kwargs ) Instance balanced sampler that samples equal number of positive samples for each instance. ", "page_idx": 204, "bbox": [71, 316.3840637207031, 487, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 1727, "type": "text", "text": "class  mmdet.core.bbox. I oU Balanced Ne g Sampler ( num ,  pos fraction ,  floor_thr  $\\mathbf{=}\\mathbf{\\cdot}$  - 1 ,  floor fraction  $\\mathbf{\\chi}_{:=0}$  , num_bins  ${}^{\\ast}\\mathrm{=}3$  ,  \\*\\*kwargs ) IoU Balanced Sampling. ", "page_idx": 204, "bbox": [71, 358.22705078125, 506.4996032714844, 395.59649658203125], "page_size": [612.0, 792.0]}
{"layout": 1728, "type": "text", "text": "arXiv:  https://arxiv.org/pdf/1904.02701.pdf  (CVPR 2019) ", "page_idx": 204, "bbox": [96, 400.219482421875, 330.02166748046875, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 1729, "type": "text", "text": "Sampling proposals according to their IoU.  floor fraction  of needed RoIs are sampled from proposals whose IoU are lower than  floor_thr  randomly. The others are sampled from proposals whose IoU are higher than  floor_thr . These proposals are sampled from some bins evenly, which are split by  num_bins  via IoU evenly. ", "page_idx": 204, "bbox": [96, 418.0030517578125, 540, 455.37249755859375], "page_size": [612.0, 792.0]}
{"layout": 1730, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 204, "bbox": [117, 462, 169, 473], "page_size": [612.0, 792.0]}
{"layout": 1731, "type": "text", "text": "•  num  ( int ) – number of proposals. •  pos fraction  ( float ) – fraction of positive proposals. •  floor_thr  ( float ) – threshold (minimum) IoU for IoU balanced sampling, set to -1 if all using IoU balanced sampling. •  floor fraction  ( float ) – sampling fraction of proposals under floor_thr. •  num_bins  ( int ) – number of bins in IoU balanced sampling. ", "page_idx": 204, "bbox": [145, 477.927490234375, 518.081787109375, 574.924560546875], "page_size": [612.0, 792.0]}
{"layout": 1732, "type": "text", "text": "sample via interval ( max overlaps ,  full_set ,  num expected ) Sample according to the iou interval. ", "page_idx": 204, "bbox": [96, 579.3970336914062, 358.4803161621094, 604.8125610351562], "page_size": [612.0, 792.0]}
{"layout": 1733, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 204, "bbox": [136, 611, 188, 622], "page_size": [612.0, 792.0]}
{"layout": 1734, "type": "text", "text": "•  max overlaps  ( torch.Tensor ) – IoU between bounding boxes and ground truth boxes. •  full_set  ( set(int) ) – A full set of indices of boxes •  num expected  ( int ) – Number of expected samples Returns  Indices of samples Return type  np.ndarray ", "page_idx": 204, "bbox": [137, 627.3674926757812, 521, 713.0062255859375], "page_size": [612.0, 792.0]}
{"layout": 1735, "type": "text", "text": "class  mmdet.core.bbox. MaxI oU As signer ( pos i ou thr ,  ne g i ou thr ,  min pos i ou  $\\mathrm{\\Sigma=}0.0$  , gt max assign all  $\\leftrightharpoons$  True ,  ignore i of  $\\mathit{t h r}{=}{\\mathit{-1}}$  , ignore w rt candidates  $=$  True ,  match low quality  $\\mathbf{=}$  True , gpu assign thr  $=$  - 1 ,  i ou calculator  $\\bf{=}$  {'type': 'B box Overlaps 2 D'} ) ", "page_idx": 205, "bbox": [72.0, 71.30303192138672, 528.5473022460938, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 1736, "type": "text", "text": "Assign a corresponding gt bbox or background to each bbox. Each proposals will be assigned with  -1 , or a semi-positive integer indicating the ground truth index. ", "page_idx": 205, "bbox": [96, 119.27253723144531, 339.0478515625, 132.5825653076172], "page_size": [612.0, 792.0]}
{"layout": 1737, "type": "text", "text": "", "page_idx": 205, "bbox": [96, 137.0560760498047, 497.0652160644531, 150.5155487060547], "page_size": [612.0, 792.0]}
{"layout": 1738, "type": "text", "text": "• -1: negative sample, no assigned gt • semi-positive integer: positive sample, index (0-based) of assigned gt ", "page_idx": 205, "bbox": [110, 155.1385040283203, 394, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 1739, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 205, "bbox": [117, 198, 169, 210], "page_size": [612.0, 792.0]}
{"layout": 1740, "type": "text", "text": "•  pos i ou thr  ( float ) – IoU threshold for positive bboxes. •  ne g i ou thr  ( float or tuple ) – IoU threshold for negative bboxes. •  min pos i ou  ( float ) – Minimum iou for a bbox to be considered as a positive bbox. Pos- itive samples can have smaller IoU than pos i ou thr due to the 4th step (assign max IoU sample to each gt). •  gt max assign all  ( bool ) – Whether to assign all bboxes with the same highest overlap with some gt to that gt. •  ignore i of thr  ( float ) – IoF threshold for ignoring bboxes (if  gt b boxes ignore  is spec- ified). Negative values mean not ignoring any bboxes. •  ignore w rt candidates  ( bool ) – Whether to compute the iof between  bboxes  and gt b boxes ignore , or the contrary. •  match low quality  ( bool ) – Whether to allow low quality matches. This is usually al- lowed for RPN and single stage detectors, but not allowed in the second stage. Details are demonstrated in Step 4. •  gpu assign thr  ( int ) – The upper bound of the number of GT for GPU assign. When the number of gt is above this threshold, will assign on CPU device. Negative values mean not assign on CPU. ", "page_idx": 205, "bbox": [145, 214.91444396972656, 518, 461.3494567871094], "page_size": [612.0, 792.0]}
{"layout": 1741, "type": "text", "text": "assign ( bboxes ,  gt_bboxes ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_labels  $=$  None ) Assign gt to bboxes. ", "page_idx": 205, "bbox": [96, 471.8009948730469, 383.5413513183594, 497.2154541015625], "page_size": [612.0, 792.0]}
{"layout": 1742, "type": "text", "text": "This method assign a gt bbox to every bbox (proposal/anchor), each bbox will be assigned with  $^{-1}$  , or a semi-positive number. -1 means negative sample, semi-positive number is the index (0-based) of assigned gt. The assignment is done in following steps, the order matters. ", "page_idx": 205, "bbox": [118, 501.8384704589844, 540, 539.0584716796875], "page_size": [612.0, 792.0]}
{"layout": 1743, "type": "text", "text": "1. assign every bbox to the background 2. assign proposals whose iou with all gts  $<$   ne g i ou thr to 0 3. for each bbox, if the iou with its nearest gt  $>=$   pos i ou thr, assign it to that bbox 4. for each gt bbox, assign its nearest proposals (may be more than one) to itself ", "page_idx": 205, "bbox": [125, 543.681396484375, 456.2771911621094, 610.7894287109375], "page_size": [612.0, 792.0]}
{"layout": 1744, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 205, "bbox": [136, 623, 188, 634], "page_size": [612.0, 792.0]}
{"layout": 1745, "type": "text", "text": "•  bboxes  ( Tensor ) – Bounding boxes to be assigned, shape(n, 4). •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (k, 4). •  gt b boxes ignore  ( Tensor, optional ) – Ground truth bboxes that are labelled as ignored , e.g., crowd boxes in COCO. •  gt_labels  ( Tensor, optional ) – Label of gt_bboxes, shape (k, ). ", "page_idx": 205, "bbox": [154, 639.3223876953125, 521.3682250976562, 718.3864135742188], "page_size": [612.0, 792.0]}
{"layout": 1746, "type": "text", "text": "Returns  The assign result. Return type  Assign Result ", "page_idx": 206, "bbox": [137, 70.8248291015625, 256, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 1747, "type": "text", "text": "Example ", "page_idx": 206, "bbox": [118, 120.55806732177734, 159.75035095214844, 134.8245086669922], "page_size": [612.0, 792.0]}
{"layout": 1748, "type": "text", "text": ">>>  self  $=$   MaxI oU As signer( 0.5 ,  0.5 ) >>>  bboxes  $=$   torch . Tensor([[ 0 ,  0 ,  10 ,  10 ], [ 10 ,  10 ,  20 ,  20 ]]) >>>  gt_bboxes  $=$  torch . Tensor([[ 0 ,  0 ,  10 ,  9 ]]) >>>  assign result  $=$   self . assign(bboxes, gt_bboxes) >>>  expected gt in ds    $=$   torch . LongTensor([ 1 ,  0 ]) >>>  assert  torch . all(assign result . gt_inds  $==$  expected gt in ds) ", "page_idx": 206, "bbox": [118, 150, 448.33837890625, 220.81808471679688], "page_size": [612.0, 792.0]}
{"layout": 1749, "type": "text", "text": "assign w rt overlaps ( overlaps ,  gt_labels  $\\mathbf{\\check{\\Sigma}}$  None ) Assign w.r.t. the overlaps of bboxes with gts. ", "page_idx": 206, "bbox": [96, 232.9430389404297, 314, 258.3575439453125], "page_size": [612.0, 792.0]}
{"layout": 1750, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 206, "bbox": [136, 264, 188, 276], "page_size": [612.0, 792.0]}
{"layout": 1751, "type": "text", "text": "•  overlaps  ( Tensor ) – Overlaps between k gt_bboxes and n bboxes, shape  $(\\mathbf{k},\\mathbf{n})$  . •  gt_labels  ( Tensor, optional ) – Labels of k gt_bboxes, shape (k, ). Returns  The assign result. Return type  Assign Result ", "page_idx": 206, "bbox": [137, 280.91351318359375, 484.2930603027344, 348.61932373046875], "page_size": [612.0, 792.0]}
{"layout": 1752, "type": "text", "text": "class  mmdet.core.bbox. OH EM Sampler ( num ,  pos fraction ,  context ,  neg_pos_ub=- 1 , add gt as proposals  $\\mathbf{:=}$  True ,  \\*\\*kwargs ) Online Hard Example Mining Sampler described in  Training Region-based Object Detectors with Online Hard Example Mining . ", "page_idx": 206, "bbox": [71, 352.4951171875, 540.0033569335938, 401.8195495605469], "page_size": [612.0, 792.0]}
{"layout": 1753, "type": "text", "text": "class  mmdet.core.bbox. Pseudo B Box Code r ( \\*\\*kwargs ) Pseudo bounding box coder. ", "page_idx": 206, "bbox": [71, 406.2930908203125, 314, 431.7075500488281], "page_size": [612.0, 792.0]}
{"layout": 1754, "type": "text", "text": "decode ( bboxes ,  p red b boxes ) torch.Tensor: return the given  p red b boxes encode ( bboxes ,  gt_bboxes ) torch.Tensor: return the given  bboxes ", "page_idx": 206, "bbox": [96, 436.18109130859375, 297, 491.4835510253906], "page_size": [612.0, 792.0]}
{"layout": 1755, "type": "text", "text": "class  mmdet.core.bbox. Pseudo Sampler ( \\*\\*kwargs ) A pseudo sampler that does not do sampling actually. ", "page_idx": 206, "bbox": [71, 495.95709228515625, 314, 521.3715209960938], "page_size": [612.0, 792.0]}
{"layout": 1756, "type": "text", "text": "sample ( assign result ,  bboxes ,  gt_bboxes ,  \\*\\*kwargs ) Directly returns the positive and negative indices of samples. ", "page_idx": 206, "bbox": [96, 525.8450927734375, 361.094482421875, 551.2595825195312], "page_size": [612.0, 792.0]}
{"layout": 1757, "type": "text", "text": "Parameters •  assign result  ( Assign Result ) – Assigned results •  bboxes  ( torch.Tensor ) – Bounding boxes •  gt_bboxes  ( torch.Tensor ) – Ground truth boxes Returns  sampler results Return type  Sampling Result ", "page_idx": 206, "bbox": [137, 555.2548828125, 380, 659.4532470703125], "page_size": [612.0, 792.0]}
{"layout": 1758, "type": "text", "text": "class  mmdet.core.bbox. Random Sampler ( num ,  pos fraction ,  neg_pos_  $u b\\!\\!=\\!\\!-\\!\\!\\;I$  ,  add gt as proposals  $\\mathbf{\\tilde{=}}$  True , \\*\\*kwargs ) ", "page_idx": 206, "bbox": [71, 663.3290405273438, 521.015625, 688.6339111328125], "page_size": [612.0, 792.0]}
{"layout": 1759, "type": "text", "text": "Random sampler. ", "page_idx": 206, "bbox": [96, 687.3884887695312, 166, 700.6985473632812], "page_size": [612.0, 792.0]}
{"layout": 1760, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 206, "bbox": [117, 706, 168, 718], "page_size": [612.0, 792.0]}
{"layout": 1761, "type": "text", "text": "•  num  ( int ) – Number of samples •  pos fraction  ( float ) – Fraction of positive samples •  neg_pos_up  ( int, optional ) – Upper bound number of negative and positive samples. Defaults to -1. •  add gt as proposals  ( bool, optional ) – Whether to add ground truth boxes as pro- posals. Defaults to True. ", "page_idx": 207, "bbox": [145, 71.45246887207031, 518, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 1762, "type": "text", "text": "random choice ( gallery ,  num ) Random select some elements from the gallery. If  gallery  is a Tensor, the returned indices will be a Tensor; If  gallery  is a ndarray or list, the returned indices will be a ndarray. ", "page_idx": 207, "bbox": [96, 166.9440155029297, 540, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 1763, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 207, "bbox": [136, 228, 188, 239], "page_size": [612.0, 792.0]}
{"layout": 1764, "type": "text", "text": "•  gallery  ( Tensor | ndarray | list ) – indices pool. •  num  ( int ) – expected sample num. Returns  sampled indices. Return type  Tensor or ndarray ", "page_idx": 207, "bbox": [137, 244.8024444580078, 391.6209716796875, 312.5082702636719], "page_size": [612.0, 792.0]}
{"layout": 1765, "type": "text", "text": "class  mmdet.core.bbox. Region As signer ( center ratio=0.2 ,  ignore ratio  $\\bullet{=}0.5$  ) Assign a corresponding gt bbox or background to each bbox. ", "page_idx": 207, "bbox": [71.99998474121094, 316.3840637207031, 412.9653625488281, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 1766, "type": "text", "text": "Each proposals will be assigned with  $\\it-1,0.$  , or a positive integer indicating the ground truth index. ", "page_idx": 207, "bbox": [96, 346.4215087890625, 484.8903503417969, 359.7315368652344], "page_size": [612.0, 792.0]}
{"layout": 1767, "type": "text", "text": "• -1: don’t care • 0: negative sample, no assigned gt • positive integer: positive sample, index (1-based) of assigned gt ", "page_idx": 207, "bbox": [110, 364.353515625, 377, 413.5295715332031], "page_size": [612.0, 792.0]}
{"layout": 1768, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 207, "bbox": [117, 425, 169, 437], "page_size": [612.0, 792.0]}
{"layout": 1769, "type": "text", "text": "•  center ratio  – ratio of the region in the center of the bbox to define positive sample. •  ignore ratio  – ratio of the region to define ignore samples. ", "page_idx": 207, "bbox": [145, 442.06256103515625, 502.0158996582031, 473.30560302734375], "page_size": [612.0, 792.0]}
{"layout": 1770, "type": "text", "text": "assign ( ml vl anchors ,  ml vl valid flags ,  gt_bboxes ,  img_meta ,  feat map sizes ,  anchor scale ,  anchor strides , gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_labels  $\\mathbf{\\check{\\Sigma}}$  None ,  allowed border  $\\mathord{:=}\\!\\!O_{.}$  ) Assign gt to anchors. ", "page_idx": 207, "bbox": [96, 483.7561340332031, 532.0446166992188, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 1771, "type": "text", "text": "This method assign a gt bbox to every bbox (proposal/anchor), each bbox will be assigned with -1, 0, or a positive number. -1 means don’t care, 0 means negative sample, positive number is the index (1-based) of assigned gt. ", "page_idx": 207, "bbox": [118, 525.74853515625, 540, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 1772, "type": "text", "text": "The assignment is done in following steps, and the order matters. ", "page_idx": 207, "bbox": [118, 567.5914916992188, 377, 580.9015502929688], "page_size": [612.0, 792.0]}
{"layout": 1773, "type": "text", "text": "1. Assign every anchor to 0 (negative) 2. (For each gt_bboxes) Compute ignore flags based on ignore region then assign  $^{-1}$   to anchors w.r.t. ignore flags 3. (For each gt_bboxes) Compute pos flags based on center region then assign gt_bboxes to anchors w.r.t. pos flags 4. (For each gt_bboxes) Compute ignore flags based on adjacent anchor level then assign   $^{-1}$   to anchors w.r.t. ignore flags 5. Assign anchor outside of image to -1 ", "page_idx": 207, "bbox": [125, 585.5245361328125, 540, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 1774, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 208, "bbox": [136, 73, 188, 84], "page_size": [612.0, 792.0]}
{"layout": 1775, "type": "text", "text": "•  ml vl anchors  ( list[Tensor] ) – Multi level anchors. •  ml vl valid flags  ( list[Tensor] ) – Multi level valid flags. •  gt_bboxes  ( Tensor ) – Ground truth bboxes of image •  img_meta  ( dict ) – Meta info of image. •  feat map sizes  ( list[Tensor] ) – Feature mapsize each level •  anchor scale  ( int ) – Scale of the anchor. •  anchor strides  ( list[int] ) – Stride of the anchor. •  gt_bboxes  – Ground truth boxes, shape (k, 4). •  gt b boxes ignore  ( Tensor, optional ) – Ground truth bboxes that are labelled as ignored , e.g., crowd boxes in COCO. •  gt_labels  ( Tensor, optional ) – Label of gt_bboxes, shape (k, ). •  allowed border  ( int, optional ) – The border to allow the valid anchor. Defaults to 0. Returns  The assign result. Return type  Assign Result ", "page_idx": 208, "bbox": [137, 89.38447570800781, 521, 342.3962097167969], "page_size": [612.0, 792.0]}
{"layout": 1776, "type": "text", "text": "class  mmdet.core.bbox. Sampling Result ( pos_inds ,  neg_inds ,  bboxes ,  gt_bboxes ,  assign result ,  gt_flags ) Bbox sampling result. ", "page_idx": 208, "bbox": [71.99995422363281, 352.2489929199219, 521, 377.6634521484375], "page_size": [612.0, 792.0]}
{"layout": 1777, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 208, "bbox": [96, 397, 139, 409], "page_size": [612.0, 792.0]}
{"layout": 1778, "type": "text", "text": ">>>  # xdoctest:   $^+$  IGNORE WANT  $>>$   from  mmdet.core.bbox.samplers.sampling result  import  \\* # NOQA  $>>$   self  $=$   Sampling Result . random(  $\\scriptstyle{\\mathbf{r}}{\\mathfrak{n}}{\\mathfrak{g}}=10.$  )  $>>$   print ( f ' self  $=$   { self } ' ) self  $=$   <Sampling Result({ ' neg_bboxes ' : torch.Size([12, 4]), ' neg_inds ' : tensor([ 0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12]), ' num_gts ' : 4, ' pos assigned gt in ds ' : tensor([], dtype  $=$  torch.int64), ' pos_bboxes ' : torch.Size([0, 4]), ' pos_inds ' : tensor([], dtype  $=$  torch.int64), ' pos_is_gt ' : tensor([], dtype  $=$  torch.uint8) })> ", "page_idx": 208, "bbox": [95, 425.10198974609375, 478.7237854003906, 579.1741333007812], "page_size": [612.0, 792.0]}
{"layout": 1779, "type": "text", "text": "property bboxes ", "text_level": 1, "page_idx": 208, "bbox": [95, 593, 177, 605], "page_size": [612.0, 792.0]}
{"layout": 1780, "type": "text", "text": "concatenated positive and negative boxes Type  torch.Tensor ", "page_idx": 208, "bbox": [117, 603.7024536132812, 282.18072509765625, 635.543212890625], "page_size": [612.0, 792.0]}
{"layout": 1781, "type": "text", "text": "property info Returns a dictionary of info about the object. ", "page_idx": 208, "bbox": [95, 641.2919921875, 297, 664.83349609375], "page_size": [612.0, 792.0]}
{"layout": 1782, "type": "text", "text": "class method random (  $.r n g{=}N o n e$  ,  \\*\\*kwargs ) Parameters ", "page_idx": 208, "bbox": [95, 669.3070068359375, 286.5603942871094, 713.251220703125], "page_size": [612.0, 792.0]}
{"layout": 1783, "type": "text", "text": "•  rng  ( None | int | numpy.random.Random State ) – seed or state. ", "page_idx": 209, "bbox": [154, 71.45246887207031, 444.447509765625, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 1784, "type": "text", "text": "•  kwargs  ( keyword arguments ) – –  num_preds: number of predicted boxes –  num_gts: number of true boxes –  p_ignore (float): probability of a predicted box assigned to an ignored truth. –  p_assigned (float): probability of a predicted box not being assigned. –  p use label (float | bool): with labels or not. ", "page_idx": 209, "bbox": [154, 89.38447570800781, 476.60107421875, 192.95620727539062], "page_size": [612.0, 792.0]}
{"layout": 1785, "type": "text", "text": "Returns  Randomly generated sampling result. Return type  Sampling Result ", "page_idx": 209, "bbox": [137, 196.353759765625, 324.8209533691406, 228.82217407226562], "page_size": [612.0, 792.0]}
{"layout": 1786, "type": "text", "text": "Example ", "page_idx": 209, "bbox": [118, 246.0869903564453, 159.75033569335938, 260.35345458984375], "page_size": [612.0, 792.0]}
{"layout": 1787, "type": "table", "page_idx": 209, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_123.jpg", "bbox": [114, 272, 544, 315], "page_size": [612.0, 792.0], "ocr_text": ">>> from mmdet.core.bbox.samplers.sampling_result import * # NOQA\n>>> self = SamplingResult.random()\n>>> print(self.__dict__)\n", "vlm_text": "The image is not actually a table, but rather a snippet of Python code, likely produced using a library that offers syntax coloring for better readability. Here's a description of the code:\n\n1. The code imports all symbols from the `sampling_result` module in the `mmdet.core.bbox.samplers` package. This is indicated by the `from mmdet.core.bbox.samplers.sampling_result import *` line. It also includes a comment `# NOQA`, which often signifies to linters to ignore warnings or errors on that line.\n\n2. A `SamplingResult` object is created using a method called `random()` from the `SamplingResult` class. It is assigned to the variable `self`.\n\n3. The code then prints the dictionary representation (`__dict__`) of the `self` object, which outputs the internal dictionary used to store an object's (in this case, `SamplingResult`) attributes. \n\nThis example seems to be illustrating a way to use certain features from the MMDetection framework, which is a popular object detection library in Python."}
{"layout": 1788, "type": "text", "text": "to ( device ) Change the device of the data inplace. ", "page_idx": 209, "bbox": [96, 322.6070251464844, 270.06622314453125, 348.021484375], "page_size": [612.0, 792.0]}
{"layout": 1789, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 209, "bbox": [117, 367, 161, 379], "page_size": [612.0, 792.0]}
{"layout": 1790, "type": "image", "page_idx": 209, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_124.jpg", "bbox": [114, 391, 543, 447], "page_size": [612.0, 792.0], "ocr_text": ">>> self = SamplingResult.random()\n>>> print(f'self = fself.to(None) }')\n>>> # xdoctest: +REQUIRES(--gpu)\n>>> print(f'self = {self.to(0)}')\n\n", "vlm_text": "The image contains a snippet of Python code that seems to be a part of a documentation example or a doctest. Here’s a breakdown of what is happening in each line:\n\n1. `self = SamplingResult.random()`: It initializes a variable `self` with a random sampling result generated by calling a `random()` method on `SamplingResult`.\n\n2. `print(f'self = {self.to(None)}')`: This line prints the result of calling the `to()` method on `self` with `None` as an argument, formatting it into the string 'self = ...'.\n\n3. `# xdoctest: +REQUIRES(--gpu)`: This is a comment line intended for `xdoctest`, a documentation testing tool. It specifies that the following test requires a `--gpu` flag, likely because it needs GPU resources to run.\n\n4. `print(f'self = {self.to(0)}')`: Similar to the second line, this line prints the result of calling the `to()` method on `self`, but this time using `0` as an argument, again formatted into the string 'self = ...'.\n\nOverall, this code appears to be testing or demonstrating the `to()` method of a `SamplingResult` object, showing its output under different conditions."}
{"layout": 1791, "type": "text", "text": "class  mmdet.core.bbox. Score HL R Sampler ( num ,  pos fraction ,  context ,  neg_pos_ub=- 1 , add gt as proposal  $\\backsimeq$  True ,  $k{=}0.5$  ,  bias  $\\mathrel{\\mathop:}=\\!\\!O$  ,  score_  $t h r{=}0.05$  , iou_th  $\\scriptstyle{r=0.5}$  ,  \\*\\*kwargs ) ", "page_idx": 209, "bbox": [72.0, 454.3590393066406, 512.3975830078125, 491.7284851074219], "page_size": [612.0, 792.0]}
{"layout": 1792, "type": "text", "text": "Importance-based Sample Re weighting (ISR_N), described in  Prime Sample Attention in Object Detection ", "page_idx": 209, "bbox": [96, 490.3734436035156, 523.1867065429688, 503.6834716796875], "page_size": [612.0, 792.0]}
{"layout": 1793, "type": "text", "text": "Score hierarchical local rank (HLR) differentiates with Random Sampler in negative part. It firstly computes Score-HLR in a two-step way, then linearly maps score hlr to the loss weights. ", "page_idx": 209, "bbox": [96, 508.3064880371094, 540.0034790039062, 533.5714721679688], "page_size": [612.0, 792.0]}
{"layout": 1794, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 209, "bbox": [117, 539, 169, 551], "page_size": [612.0, 792.0]}
{"layout": 1795, "type": "text", "text": "•  num  ( int ) – Total number of sampled RoIs. •  pos fraction  ( float ) – Fraction of positive samples. •  context  ( Base RoI Head ) – RoI head that the sampler belongs to. •  neg_pos_ub  ( int ) – Upper bound of the ratio of num negative to num positive, -1 means no upper bound. •  add gt as proposals  ( bool ) – Whether to add ground truth as proposals. •  k  ( float ) – Power of the non-linear mapping. •  bias  ( float ) – Shift of the non-linear mapping. •  score_thr  ( float ) – Minimum score that a negative sample is to be considered as valid bbox. ", "page_idx": 209, "bbox": [145, 556.1273803710938, 518, 718.87646484375], "page_size": [612.0, 792.0]}
{"layout": 1796, "type": "text", "text": "static random choice ( gallery ,  num ) Randomly select some elements from the gallery. ", "page_idx": 210, "bbox": [96, 71.30303192138672, 315.11712646484375, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1797, "type": "text", "text": "If  gallery  is a Tensor, the returned indices will be a Tensor; If  gallery  is a ndarray or list, the returned indices will be a ndarray. ", "page_idx": 210, "bbox": [118, 101.19103240966797, 540, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 1798, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 210, "bbox": [136, 133, 188, 144], "page_size": [612.0, 792.0]}
{"layout": 1799, "type": "text", "text": "•  gallery  ( Tensor | ndarray | list ) – indices pool. •  num  ( int ) – expected sample num. Returns  sampled indices. Return type  Tensor or ndarray ", "page_idx": 210, "bbox": [137, 149.1604766845703, 391.6209411621094, 216.86721801757812], "page_size": [612.0, 792.0]}
{"layout": 1800, "type": "text", "text": "sample ( assign result ,  bboxes ,  gt_bboxes ,  gt_labels  $\\leftrightharpoons$  None ,  img_meta  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Sample positive and negative bboxes. ", "page_idx": 210, "bbox": [96, 220.7419891357422, 453.0063171386719, 246.15647888183594], "page_size": [612.0, 792.0]}
{"layout": 1801, "type": "text", "text": "This is a simple implementation of bbox sampling given candidates, assigning results and ground truth bboxes. ", "page_idx": 210, "bbox": [118, 250.77943420410156, 540, 276.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 1802, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 210, "bbox": [136, 283, 188, 293], "page_size": [612.0, 792.0]}
{"layout": 1803, "type": "text", "text": "•  assign result  ( Assign Result ) – Bbox assigning results. •  bboxes  ( Tensor ) – Boxes to be sampled from. •  gt_bboxes  ( Tensor ) – Ground truth bboxes. •  gt_labels  ( Tensor, optional ) – Class labels of ground truth bboxes. ", "page_idx": 210, "bbox": [154, 298.6004943847656, 457.9762268066406, 365.70855712890625], "page_size": [612.0, 792.0]}
{"layout": 1804, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 210, "bbox": [136, 372, 173, 383], "page_size": [612.0, 792.0]}
{"layout": 1805, "type": "text", "text": "Sampling result and negative  label weights. Return type  tuple[ Sampling Result , Tensor] ", "page_idx": 210, "bbox": [137, 387.63690185546875, 337.38427734375, 420.1043395996094], "page_size": [612.0, 792.0]}
{"layout": 1806, "type": "text", "text": "class mmdet.core.bbox.T BL RB Box Code r(normalize $\\tt:=\\!4.0$ , clip border $\\mathbf{=}$ True)TBLR BBox coder. ", "page_idx": 210, "bbox": [71.99993896484375, 423.9801330566406, 406, 449.39459228515625], "page_size": [612.0, 792.0]}
{"layout": 1807, "type": "text", "text": "Following the practice in  FSAF , this coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left, right) and decode it back to the original. ", "page_idx": 210, "bbox": [96, 454.017578125, 540, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 1808, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 210, "bbox": [117, 485, 169, 497], "page_size": [612.0, 792.0]}
{"layout": 1809, "type": "text", "text": "•  normalizer  ( list | float ) – Normalization factor to be divided with when coding the coordinates. If it is a list, it should have length of 4 indicating normalization factor in tblr dims. Otherwise it is a unified float factor for all dims. Default: 4.0 •  clip border  ( bool, optional ) – Whether clip the objects outside the border of the im- age. Defaults to True. ", "page_idx": 210, "bbox": [145, 501.8385925292969, 518, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 1810, "type": "text", "text": "decode ( bboxes ,  p red b boxes ,  max_shape  $=$  None ) Apply transformation  p red b boxes  to  boxes . ", "page_idx": 210, "bbox": [96, 573.4201049804688, 298.7192687988281, 598.8345947265625], "page_size": [612.0, 792.0]}
{"layout": 1811, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 210, "bbox": [136, 604, 188, 616], "page_size": [612.0, 792.0]}
{"layout": 1812, "type": "text", "text": "•  bboxes  ( torch.Tensor ) – Basic boxes.Shape (B, N, 4) or (N, 4) •  p red b boxes  ( torch.Tensor ) – Encoded boxes with shape (B, N, 4) or (N, 4) •  (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If bboxes shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and the length of max_shape should also be B. ", "page_idx": 210, "bbox": [154, 621.3895874023438, 521, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 1813, "type": "text", "text": "Returns  Decoded boxes. Return type  torch.Tensor ", "page_idx": 211, "bbox": [137, 70.8248291015625, 246, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 1814, "type": "text", "text": "encode ( bboxes ,  gt_bboxes ) Get box regression transformation deltas that can be used to transform the  bboxes  into the  gt_bboxes  in the (top, left, bottom, right) order. ", "page_idx": 211, "bbox": [96, 107.16802215576172, 539.9999389648438, 144.5375213623047], "page_size": [612.0, 792.0]}
{"layout": 1815, "type": "text", "text": "Parameters •  bboxes  ( torch.Tensor ) – source boxes, e.g., object proposals. •  gt_bboxes  ( torch.Tensor ) – target of the transformation, e.g., ground truth boxes. ", "page_idx": 211, "bbox": [137, 148.5328369140625, 504.1280822753906, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 1816, "type": "text", "text": "Returns  Box transformation deltas ", "page_idx": 211, "bbox": [137, 202.330810546875, 279.3915710449219, 216.86624145507812], "page_size": [612.0, 792.0]}
{"layout": 1817, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 211, "bbox": [137, 220.2637939453125, 246, 234.79922485351562], "page_size": [612.0, 792.0]}
{"layout": 1818, "type": "text", "text": "mmdet.core.bbox. b box 2 distance ( points ,  bbox ,  max_dis  $\\mathbf{\\check{\\Sigma}}$  None ,  eps=0.1 ) Decode bounding box based on distances. ", "page_idx": 211, "bbox": [71, 238.6749725341797, 385, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 1819, "type": "text", "text": "•  points  ( Tensor ) – Shape (n, 2), [x, y]. •  bbox  ( Tensor ) – Shape (n, 4), “xyxy” format •  max_dis  ( float ) – Upper bound of the distance. •  eps  ( float ) – a small value to ensure target   $<$   max_dis, instead  $<=$  ", "page_idx": 211, "bbox": [145, 286.64544677734375, 424, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 1820, "type": "text", "text": "mmdet.core.bbox. b box 2 result ( bboxes ,  labels ,  num classes ) Convert detection results to a list of numpy arrays. ", "page_idx": 211, "bbox": [71, 394.0920715332031, 339, 419.50653076171875], "page_size": [612.0, 792.0]}
{"layout": 1821, "type": "text", "text": "Parameters •  bboxes  ( torch.Tensor | np.ndarray ) – shape (n, 5) •  labels  ( torch.Tensor | np.ndarray ) – shape (n, ) •  num classes  ( int ) – class number, including background class ", "page_idx": 211, "bbox": [118, 423.5018615722656, 412.75103759765625, 491.2375793457031], "page_size": [612.0, 792.0]}
{"layout": 1822, "type": "text", "text": "Returns  bbox results of each class Return type  list(ndarray) ", "page_idx": 211, "bbox": [118, 495.23291015625, 259.1376037597656, 527.7012939453125], "page_size": [612.0, 792.0]}
{"layout": 1823, "type": "text", "text": "mmdet.core.bbox. bbox2roi ( bbox_list ) Convert a list of bboxes to roi format. Parameters  bbox_list  ( list[Tensor] ) – a list of bboxes corresponding to a batch of images. Returns  shape (n, 5), [batch_ind, x1, y1, x2, y2] Return type  Tensor ", "page_idx": 211, "bbox": [71, 531.5771484375, 507.1618347167969, 611.3873291015625], "page_size": [612.0, 792.0]}
{"layout": 1824, "type": "text", "text": "mmdet.core.bbox. b box cx cy wh to xy xy ( bbox ) Convert bbox coordinates from (cx, cy, w, h) to (x1, y1, x2, y2). ", "page_idx": 211, "bbox": [71, 615.2631225585938, 351, 640.6776123046875], "page_size": [612.0, 792.0]}
{"layout": 1825, "type": "text", "text": "Parameters  bbox  ( Tensor ) – Shape (n, 4) for bboxes. Returns  Converted bboxes. Return type  Tensor ", "page_idx": 211, "bbox": [118, 644.6729125976562, 339, 695.0733032226562], "page_size": [612.0, 792.0]}
{"layout": 1826, "type": "text", "text": "mmdet.core.bbox. bbox_flip ( bboxes ,  img_shape ,  direction  $=$  'horizontal' ) Flip bboxes horizontally or vertically. ", "page_idx": 212, "bbox": [71, 71.30303192138672, 381.8973693847656, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1827, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 212, "bbox": [118, 103, 169, 114], "page_size": [612.0, 792.0]}
{"layout": 1828, "type": "text", "text": "•  img_shape  ( tuple ) – Image shape. •  direction  ( str ) – Flip direction, options are “horizontal”, “vertical”, “diagonal”. Default: “horizontal” ", "page_idx": 212, "bbox": [145, 137.20545959472656, 518.0811157226562, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 1829, "type": "text", "text": "Returns  Flipped bboxes. ", "page_idx": 212, "bbox": [118, 184.3988037109375, 220.8214569091797, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 1830, "type": "text", "text": "Return type  Tensor ", "page_idx": 212, "bbox": [118, 202.331787109375, 203, 216.86721801757812], "page_size": [612.0, 792.0]}
{"layout": 1831, "type": "text", "text": "mmdet.core.bbox. b box mapping ( bboxes ,  img_shape ,  scale factor ,  flip ,  flip direction  $=$  'horizontal' ) Map bboxes from the original image scale to testing scale. ", "page_idx": 212, "bbox": [71, 220.7419891357422, 486.9033508300781, 246.15647888183594], "page_size": [612.0, 792.0]}
{"layout": 1832, "type": "text", "text": "mmdet.core.bbox. b box mapping back ( bboxes ,  img_shape ,  scale factor ,  flip ,  flip direction  $.=$  'horizontal' ) Map bboxes from testing scale to original image scale. ", "page_idx": 212, "bbox": [71, 250.62998962402344, 513.0552978515625, 276.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 1833, "type": "text", "text": "mmdet.core.bbox. b box overlaps ( bboxes1 ,  bboxes2 ,  mode  $=$  'iou' ,  is_aligned  $\\leftrightharpoons$  False ,  eps=1e-06 ) Calculate overlap between two set of bboxes. ", "page_idx": 212, "bbox": [71, 280.5180358886719, 476.8761901855469, 305.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 1834, "type": "text", "text": "FP16 Contributed by  https://github.com/open-mmlab/mm detection/pull/4889  .. note: ", "page_idx": 212, "bbox": [96, 310.55548095703125, 437.6275939941406, 323.8655090332031], "page_size": [612.0, 792.0]}
{"layout": 1835, "type": "image", "page_idx": 212, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_125.jpg", "bbox": [93, 329, 545, 711], "page_size": [612.0, 792.0], "ocr_text": "Assume bboxes1 is M x 4, bboxes2 is N x 4, when mode is ‘iou',\nthere are some new generated variable when calculating IOU\nusing bbox_overlaps function:\n\n1) is_aligned is False\nareal: Mx 1\narea2: N x 1\nIt: MxNx2\nrb: Mx N x 2\nwh: Mx N x 2\noverlap: Mx Nx1\nunion: Mx Nxl1\nious: Mx Nx1\n\nTotal memory:\nS=(9 xNxM+N+M) * 4 Byte,\n\nWhen using FP16, we can reduce:\nR= (9 xNxM+N+M) * 4 / 2 Byte\nR large than (N + M) * 4 * 2 is always true when N and M >= 1.\nObviously, N+ M<=N* M< 3 * N * M, when N >=2 and M >=2,\nN+ 1< 3 * N, when N or M is 1.\n\nGiven M = 40 (ground truth), N = 400000 (three anchor boxes\nin per grid, FPN, R-CNNs),\nR = 275 MB (Cone times)\n\nA special case (dense detection), M = 512 (ground truth),\nR = 3516 MB = 3.43 GB\n\nWhen the batch size is B, reduce:\n\nPIER TS pre AE AY II ARTE\n\n", "vlm_text": "The image contains text related to calculating the Intersection over Union (IoU) between two sets of bounding boxes in computer vision. It covers the following points:\n\n- Assumptions of bounding box dimensions: `bboxes1` is \\(M \\times 4\\), `bboxes2` is \\(N \\times 4\\).\n- Variables generated during the IoU calculation in the `bbox_overlaps` function are listed.\n- Calculation details when `is_aligned` is `False`:\n  - Several variable dimensions: `area1`, `area2`, `lt`, `rb`, `wh`, `overlap`, `union`, and `ious`.\n- Total memory calculation formula: \\(S = (9 \\times N \\times M + N + M) \\times 4\\) Bytes.\n- Memory reduction explanation using FP16: \\(R = (9 \\times N \\times M + N + M) \\times 4 / 2\\) Bytes.\n- Special cases for memory calculations:\n  - Given \\(M = 40\\) and \\(N = 400,000\\), resulting in \\(R = 275\\) MB.\n  - Dense detection with \\(M = 512\\), resulting in \\(R = 3.516\\) MB or 3.43 GB.\n- Consideration of batch size \\(B\\) for further reduction."}
{"layout": 1836, "type": "image", "page_idx": 213, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_126.jpg", "bbox": [92, 80, 544, 518], "page_size": [612.0, 792.0], "ocr_text": "Die Sree aioe: — leet\n\nBxR\nTherefore, CUDA memory runs out frequently.\n\nExperiments on GeForce RTX 2080Ti (11019 MiB):\n\n| dtype | M | N | Use | Real | Ideal\n\n| :----1 | :----1 |) :----1 | :----1 | :----:|:----:\n\n| FP32 | 512 | 400000 | 8020 MiB | -- | -- |\n\n| FP16 | 512 | 400000 | 4504 MiB | 3516 MiB | 3516 MiB |\n| FP32 | 40 | 400000 | 1540 MiB | -- | = |\n\n| FP16 | 40 | 400000 | 1264 MiB | 276MiB | 275 MiB |\n\n2) is_aligned is True\nareal: N x 1\narea2: N x 1\nlt: N x2\nrb: N x 2\nwh: N x 2\noverlap: Nx 1\nunion: N x 1\nious: N x 1\n\nTotal memory:\nS = 11x N * 4 Byte\n\nWhen using FP16, we can reduce:\nR= 11xN* 4 / 2 Byte\n\nSo do the 'giou' (large than 'iou').\nTime-wise, FP16 is generally faster than FP32.\nWhen gpu_assign_thr is not -1, it takes more time on cpu\n\nbut not reduce memory.\nThere, we can reduce half the memory and keep the speed.\n\n", "vlm_text": "The image appears to be a screenshot of a text document or slide that explains CUDA memory usage on a GeForce RTX 2080 Ti graphics card with 11,019 MiB of memory. It contains a table that shows memory usage for different data types (FP32 and FP16) and tensor sizes.\n\nKey points from the image include:\n- The table shows memory usage in MiB for tensors when processed using FP32 and FP16 precision. Higher precision (FP32) uses more memory than lower precision (FP16).\n- It suggests that using FP16 precision can reduce memory usage significantly.\n- The \"is_aligned is True\" section lists different variables and their sizes in terms of N (presumably representing some dimension).\n- The section also provides a formula for calculating total memory usage, denoted as \"S,\" and shows that this can be reduced when using FP16, denoted as \"R.\"\n- A note mentions that FP16 is generally faster than FP32.\n- There is also a note that adjusting `gpu_assign_thr` can affect CPU time but not memory reduction.\n- The document concludes with a recommendation to reduce half the memory while keeping speed.\n\nOverall, the image seems to focus on optimizing CUDA memory usage using lower precision (FP16) to make the best use of GPU resources."}
{"layout": 1837, "type": "text", "text": "If  is_aligned  is  False , then calculate the overlaps between each bbox of bboxes1 and bboxes2, otherwise the overlaps between each aligned pair of bboxes1 and bboxes2. ", "page_idx": 213, "bbox": [96, 528.689453125, 539.9968872070312, 553.9545288085938], "page_size": [612.0, 792.0]}
{"layout": 1838, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 213, "bbox": [117, 559, 168, 572], "page_size": [612.0, 792.0]}
{"layout": 1839, "type": "text", "text": "•  bboxes1  ( Tensor ) – shape (B, m, 4) in  $_{<\\mathbf{X}1}$  , y1, x2,   $_{\\mathrm{y}2>}$   format or empty. •  bboxes2  ( Tensor ) – shape (B, n, 4) in <x1, y1, x2,   ${\\mathrm y}2\\!>$   format or empty. B indicates the batch dim, in shape (B1, B2, ..., Bn). If  is_aligned  is  True , then m and n must be equal. •  mode  ( str ) – “iou” (intersection over union), “iof” (intersection over foreground) or “giou” (generalized intersection over union). Default “iou”. •  is_aligned  ( bool, optional ) – If True, then m and n must be equal. Default False. •  eps  ( float, optional ) – A value added to the denominator for numerical stability. De- fault 1e-6. ", "page_idx": 213, "bbox": [145, 576.5104370117188, 518, 697.41650390625], "page_size": [612.0, 792.0]}
{"layout": 1840, "type": "text", "text": "Returns  shape   $(\\mathrm{m},\\mathrm{n})$   if  is_aligned  is False else shape (m,) ", "page_idx": 213, "bbox": [118.823974609375, 701.4118041992188, 366.32537841796875, 715.9472045898438], "page_size": [612.0, 792.0]}
{"layout": 1841, "type": "text", "text": "Return type  Tensor ", "page_idx": 214, "bbox": [118, 70.8248291015625, 201, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 1842, "type": "text", "text": "Example ", "page_idx": 214, "bbox": [95, 102.62508392333984, 137.83335876464844, 116.89152526855469], "page_size": [612.0, 792.0]}
{"layout": 1843, "type": "text", "text": ">>>  bboxes1  $=$   torch . Float Tensor([ >>> [ 0 ,  0 ,  10 ,  10 ], >>> [ 10 ,  10 ,  20 ,  20 ], >>> [ 32 ,  32 ,  38 ,  42 ], >>>  ]) >>>  bboxes2  $=$   torch . Float Tensor([ >>> [ 0 ,  0 ,  10 ,  20 ], >>> [ 0 ,  10 ,  10 ,  19 ], >>> [ 10 ,  10 ,  20 ,  20 ], >>>  ])  $>>$   overlaps  $=$   b box overlaps(bboxes1, bboxes2)  $>>$   assert  overlaps . shape  $==$   ( 3 ,  3 )  $>>$   overlaps  $=$   b box overlaps(bboxes1, bboxes2, is_aligned = True )  $>>$   assert  overlaps . shape  ==  ( 3 , ) ", "page_idx": 214, "bbox": [95, 132.20095825195312, 426.4203796386719, 298.5271301269531], "page_size": [612.0, 792.0]}
{"layout": 1844, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 214, "bbox": [96, 325, 139, 337], "page_size": [612.0, 792.0]}
{"layout": 1845, "type": "text", "text": " $>>$   empty  $=$   torch . empty( 0 ,  4 )  $>>$   nonempty  $=$   torch . Float Tensor([[ 0 ,  0 ,  10 ,  9 ]])  $>>$   assert  tuple (b box overlaps(empty, nonempty) . shape)  ==  ( 0 ,  1 )  $>>$   assert  tuple (b box overlaps(nonempty, empty) . shape)  ==  ( 1 ,  0 )  $>>$   assert  tuple (b box overlaps(empty, empty) . shape)  ==  ( 0 ,  0 ) ", "page_idx": 214, "bbox": [95, 354, 431, 412.3459777832031], "page_size": [612.0, 792.0]}
{"layout": 1846, "type": "text", "text": "mmdet.core.bbox. b box re scale ( bboxes ,  scale factor  ${\\simeq}I.0$  ) ", "page_idx": 214, "bbox": [72, 424.4710388183594, 329, 437.9305114746094], "page_size": [612.0, 792.0]}
{"layout": 1847, "type": "text", "text": "Parameters •  bboxes  ( Tensor ) – Shape (n, 4) for bboxes or (n, 5) for rois •  scale factor  ( float ) – rescale factor Returns  Rescaled bboxes. Return type  Tensor mmdet.core.bbox. b box xy xy to cx cy wh ( bbox ) Convert bbox coordinates from (x1, y1, x2, y2) to (cx, cy, w, h). Parameters  bbox  ( Tensor ) – Shape (n, 4) for bboxes. Returns  Converted bboxes. Return type  Tensor mmdet.core.bbox. build as signer ( cfg ,  \\*\\*default arg s ) Builder of box assigner. mmdet.core.bbox. build b box code r ( cfg ,  \\*\\*default arg s ) Builder of box coder. mmdet.core.bbox. build sampler ( cfg ,  \\*\\*default arg s ) Builder of box sampler. ", "page_idx": 214, "bbox": [72, 453.8808288574219, 394.8382263183594, 712.8995361328125], "page_size": [612.0, 792.0]}
{"layout": 1848, "type": "text", "text": "mmdet.core.bbox. distance 2 b box ( points ,  distance ,  max_shape=None ) Decode distance prediction to bounding box. ", "page_idx": 215, "bbox": [71, 71.30303192138672, 373.8973388671875, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1849, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 215, "bbox": [118, 103, 169, 114], "page_size": [612.0, 792.0]}
{"layout": 1850, "type": "text", "text": "•  points  ( Tensor ) – Shape (B, N, 2) or (N, 2). •  distance  ( Tensor ) – Distance from the given point to 4 boundaries (left, top, right, bot- tom). Shape (B, N, 4) or (N, 4) •  (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If priors shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and the length of max_shape should also be B. ", "page_idx": 215, "bbox": [145, 119.27247619628906, 518, 216.26951599121094], "page_size": [612.0, 792.0]}
{"layout": 1851, "type": "text", "text": "Returns  Boxes with shape (N, 4) or (B, N, 4) ", "page_idx": 215, "bbox": [118, 220.26385498046875, 302.1361083984375, 234.79928588867188], "page_size": [612.0, 792.0]}
{"layout": 1852, "type": "text", "text": "Return type  Tensor ", "text_level": 1, "page_idx": 215, "bbox": [118, 240, 202, 252], "page_size": [612.0, 792.0]}
{"layout": 1853, "type": "text", "text": "mmdet.core.bbox. roi2bbox ( rois ) Convert rois to bounding box format. ", "page_idx": 215, "bbox": [71, 256.6080322265625, 245, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 1854, "type": "text", "text": "Parameters  rois  ( torch.Tensor ) – RoIs with the shape (n, 5) where the first column indicates batch id of each RoI. ", "page_idx": 215, "bbox": [118, 286.0178527832031, 518, 311.9105224609375], "page_size": [612.0, 792.0]}
{"layout": 1855, "type": "text", "text": "Returns  Converted boxes of corresponding rois. ", "page_idx": 215, "bbox": [118, 315.9058532714844, 314.45977783203125, 330.4412841796875], "page_size": [612.0, 792.0]}
{"layout": 1856, "type": "text", "text": "Return type  list[torch.Tensor] ", "page_idx": 215, "bbox": [118, 333.8378601074219, 245, 348.373291015625], "page_size": [612.0, 792.0]}
{"layout": 1857, "type": "text", "text": "37.3 export ", "text_level": 1, "page_idx": 215, "bbox": [70, 371, 153, 388], "page_size": [612.0, 792.0]}
{"layout": 1858, "type": "text", "text": "mmdet.core.export. add dummy nm s for on nx ( boxes ,  scores ,  max output boxes per class  $\\mathbf{=}$  1000 , i ou threshold  $\\mathbf{\\chi}{=}0.5$  ,  score threshold  $\\it{=}0.05$  ,  pre_top_  $k{=}{\\mathfrak{-}}\\,I$  , after_top_  $k{=}{\\mathfrak{-}}\\,I$  ,  labels  $\\scriptstyle\\sum=I$  None ) Create a dummy onnx::Non Max Suppression op while exporting to ONNX. ", "page_idx": 215, "bbox": [71, 402.5120849609375, 518, 451.8365173339844], "page_size": [612.0, 792.0]}
{"layout": 1859, "type": "text", "text": "This function helps exporting to onnx with batch and multiclass NMS op. It only supports class-agnostic de- tection results. That is, the scores is of shape (N, num_bboxes, num classes) and the boxes is of shape (N, num_boxes, 4). ", "page_idx": 215, "bbox": [96, 456.4595031738281, 540, 493.67950439453125], "page_size": [612.0, 792.0]}
{"layout": 1860, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 215, "bbox": [117, 500, 168, 511], "page_size": [612.0, 792.0]}
{"layout": 1861, "type": "text", "text": "•  boxes  ( Tensor ) – The bounding boxes of shape [N, num_boxes, 4] •  scores  ( Tensor ) – The detection scores of shape [N, num_boxes, num classes] •  max output boxes per class  ( int ) – Maximum number of output boxes per class of nms. Defaults to 1000. •  i ou threshold  ( float ) – IOU threshold of nms. Defaults to 0.5 •  score threshold  ( float ) – score threshold of nms. Defaults to 0.05. •  pre_top_k  ( bool ) – Number of top K boxes to keep before nms. Defaults to -1. •  after top k  ( int ) – Number of top K boxes to keep after nms. Defaults to -1. •  labels  ( Tensor, optional ) – It not None, explicit labels would be used. Otherwise, labels would be automatically generated using num classed. Defaults to None. ", "page_idx": 215, "bbox": [145, 516.2354736328125, 518, 678.9855346679688], "page_size": [612.0, 792.0]}
{"layout": 1862, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 215, "bbox": [118, 685, 154, 695], "page_size": [612.0, 792.0]}
{"layout": 1863, "type": "text", "text": "dets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. ", "page_idx": 215, "bbox": [137, 700.912841796875, 424, 715.4482421875], "page_size": [612.0, 792.0]}
{"layout": 1864, "type": "text", "text": "Return type  tuple[Tensor, Tensor] ", "page_idx": 216, "bbox": [118, 70.8248291015625, 261, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 1865, "type": "text", "text": "mmdet.core.export. build model from cf g ( config path ,  checkpoint path ,  cf g options  $=$  None ) Build a model from config and load the given checkpoint. ", "page_idx": 216, "bbox": [71, 89.23503875732422, 477.13037109375, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 1866, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 216, "bbox": [117, 121, 170, 132], "page_size": [612.0, 792.0]}
{"layout": 1867, "type": "text", "text": "•  config path  ( str ) – the OpenMMLab config for the model we want to export to ONNX •  checkpoint path  ( str ) – Path to the corresponding checkpoint ", "page_idx": 216, "bbox": [145, 137.20545959472656, 514, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 1868, "type": "text", "text": "Returns  the built model ", "page_idx": 216, "bbox": [118, 172.44378662109375, 218.15147399902344, 186.97921752929688], "page_size": [612.0, 792.0]}
{"layout": 1869, "type": "text", "text": "Return type  torch.nn.Module ", "page_idx": 216, "bbox": [118, 190.37579345703125, 240.95619201660156, 204.91122436523438], "page_size": [612.0, 792.0]}
{"layout": 1870, "type": "text", "text": "mmdet.core.export. dynamic clip for on nx ( x1 ,  y1 ,  x2 ,  y2 ,  max_shape ) Clip boxes dynamically for onnx. ", "page_idx": 216, "bbox": [71, 208.78697204589844, 388, 234.2014617919922], "page_size": [612.0, 792.0]}
{"layout": 1871, "type": "text", "text": "Since torch.clamp cannot have dynamic  min  and  max , we scale the  boxes by 1/max_shape and clamp in the range [0, 1]. ", "page_idx": 216, "bbox": [96, 238.19677734375, 540, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 1872, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 216, "bbox": [117, 276, 170, 287], "page_size": [612.0, 792.0]}
{"layout": 1873, "type": "text", "text": "•  x1  ( Tensor ) – The x1 for bounding boxes. •  y1  ( Tensor ) – The y1 for bounding boxes. •  x2  ( Tensor ) – The x2 for bounding boxes. •  y2  ( Tensor ) – The y2 for bounding boxes. •  max_shape  ( Tensor or torch.Size ) – The (H,W) of original image. ", "page_idx": 216, "bbox": [145, 292.6224670410156, 443.4208068847656, 377.6635437011719], "page_size": [612.0, 792.0]}
{"layout": 1874, "type": "text", "text": "Returns  The clipped x1, y1, x2, y2. ", "page_idx": 216, "bbox": [118, 381.65887451171875, 264.90576171875, 396.1943054199219], "page_size": [612.0, 792.0]}
{"layout": 1875, "type": "text", "text": "Return type  tuple(Tensor) ", "page_idx": 216, "bbox": [118, 399.5918884277344, 229, 414.1273193359375], "page_size": [612.0, 792.0]}
{"layout": 1876, "type": "text", "text": "mmdet.core.export. generate inputs and wrap model ( config path ,  checkpoint path ,  input config , cf g options  $\\mathbf{:=}$  None ) ", "page_idx": 216, "bbox": [71, 423.9801025390625, 502.6545104980469, 449.28497314453125], "page_size": [612.0, 792.0]}
{"layout": 1877, "type": "text", "text": "Prepare sample input and wrap model for ONNX export. ", "page_idx": 216, "bbox": [96, 448.0395202636719, 326, 461.34954833984375], "page_size": [612.0, 792.0]}
{"layout": 1878, "type": "text", "text": "The ONNX export API only accept args, and all inputs should be torch.Tensor or corresponding types (such as tuple of tensor). So we should call this function before exporting. This function will: ", "page_idx": 216, "bbox": [96, 465.9725341796875, 540, 491.237548828125], "page_size": [612.0, 792.0]}
{"layout": 1879, "type": "text", "text": "1. generate corresponding inputs which are used to execute the model. 2. Wrap the model’s forward function. ", "page_idx": 216, "bbox": [106, 495.86053466796875, 388, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 1880, "type": "text", "text": "For example, the MMDet models’ forward function has a parameter  return loss:bool . As we want to set it as False while export API supports neither bool type or kwargs. So we have to replace the forward method like model.forward  $=$   partial(model.forward, return loss  $=$  False) . ", "page_idx": 216, "bbox": [96, 531.7265014648438, 540, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 1881, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 216, "bbox": [117, 575, 169, 586], "page_size": [612.0, 792.0]}
{"layout": 1882, "type": "text", "text": "•  config path  ( str ) – the OpenMMLab config for the model we want to export to ONNX •  checkpoint path  ( str ) – Path to the corresponding checkpoint •  input config  ( dict ) – the exactly data in this dict depends on the framework. For MMSeg, we can just declare the input shape, and generate the dummy data accordingly. However, for MMDet, we may pass the real img path, or the NMS will return None as there is no legal bbox. ", "page_idx": 216, "bbox": [145, 591.5015258789062, 518, 676.5426025390625], "page_size": [612.0, 792.0]}
{"layout": 1883, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 216, "bbox": [117, 682, 154, 694], "page_size": [612.0, 792.0]}
{"layout": 1884, "type": "text", "text": "(model, tensor data) wrapped model which can be called by  model(\\*tensor data)  and a list of inputs which are used to execute the model while exporting. ", "page_idx": 217, "bbox": [137, 70.8248291015625, 518, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1885, "type": "text", "text": "Return type  tuple ", "page_idx": 217, "bbox": [118, 100.71282958984375, 194, 115.24826049804688], "page_size": [612.0, 792.0]}
{"layout": 1886, "type": "text", "text": "mmdet.core.export. get k for top k ( k ,  size ) Get k of TopK for onnx exporting. ", "page_idx": 217, "bbox": [71, 119.12303924560547, 274.1813659667969, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 1887, "type": "text", "text": "The K of TopK in TensorRT should not be a Tensor, while in ONNX Runtime  it could be a Tensor.Due to dynamic shape feature, we have to decide whether to do TopK and what K it should be while exporting to ONNX. ", "page_idx": 217, "bbox": [96, 148.5328369140625, 540, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 1888, "type": "text", "text": "If returned K is less than zero, it means we do not have to do  TopK operation. ", "page_idx": 217, "bbox": [96, 190.3758544921875, 425, 204.91128540039062], "page_size": [612.0, 792.0]}
{"layout": 1889, "type": "text", "text": "Parameters ", "page_idx": 217, "bbox": [118, 214.286865234375, 167.76031494140625, 228.82229614257812], "page_size": [612.0, 792.0]}
{"layout": 1890, "type": "text", "text": "•  k  ( int or Tensor ) – The set k value for nms from config file. •  size  ( Tensor or torch.Size ) – The number of elements of TopK’s input tensor ", "page_idx": 217, "bbox": [145, 232.8465118408203, 489.1590270996094, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 1891, "type": "text", "text": "Returns  (int or Tensor): The final K for TopK. ", "page_idx": 217, "bbox": [118, 268.0848388671875, 307.5757751464844, 282.6202697753906], "page_size": [612.0, 792.0]}
{"layout": 1892, "type": "text", "text": "Return type  tuple ", "page_idx": 217, "bbox": [118, 286.0178527832031, 194, 300.55328369140625], "page_size": [612.0, 792.0]}
{"layout": 1893, "type": "text", "text": "mmdet.core.export. pre process example input ( input config ) Prepare an example input image for  generate inputs and wrap model ", "page_idx": 217, "bbox": [71, 310.40606689453125, 397.8482360839844, 335.8205261230469], "page_size": [612.0, 792.0]}
{"layout": 1894, "type": "text", "text": "Parameters  input config  ( dict ) – customized config describing the example input. ", "page_idx": 217, "bbox": [118, 339.81585693359375, 468.4620056152344, 354.3512878417969], "page_size": [612.0, 792.0]}
{"layout": 1895, "type": "text", "text": "Returns  (one_img, one_meta), tensor of the example input image and meta information for the ex- ample input image. ", "page_idx": 217, "bbox": [118, 357.7488708496094, 518, 383.64154052734375], "page_size": [612.0, 792.0]}
{"layout": 1896, "type": "text", "text": "Return type  tuple ", "page_idx": 217, "bbox": [118, 387.6368713378906, 194, 402.17230224609375], "page_size": [612.0, 792.0]}
{"layout": 1897, "type": "text", "text": "Examples ", "text_level": 1, "page_idx": 217, "bbox": [95, 421, 144, 433], "page_size": [612.0, 792.0]}
{"layout": 1898, "type": "table", "page_idx": 217, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_127.jpg", "bbox": [91, 444, 544, 680], "page_size": [612.0, 792.0], "ocr_text": ">>> from mmdet.core.export import preprocess_example_input\n>>> input_config = {\n\n>>> ‘input_shape': (1,3,224,224),\n\n>>> \"input_path': 'demo/demo.jpg',\n\n>>> \"‘normalize_cfg': {\n\n>>> ‘mean’: (123.675, 116.28, 103.53),\n>>> ‘std’: (98.395, 57.12, 57.375)\n>> }\n\n>> }\n\n>>> one_img, one_meta = preprocess_example_input (input_config)\n>>> print (one_img. shape)\n\ntorch.Size([1, 3, 224, 224])\n\n>>> print (one_meta)\n\n{'img_shape': (224, 224, 3),\n\n\"ori_shape': (224, 224, 3),\n\n\"pad_shape': (224, 224, 3),\n\n\"filename': '<demo>.png',\n\n\"scale_factor': 1.0,\n\n\"flip': False}\n\n", "vlm_text": "This image shows a Python code snippet used for preprocessing input data, likely for a machine learning application. Here's a breakdown of what it does:\n\n1. **Import**:\n   - A function `preprocess_example_input` is imported from `mmdet.core.export`.\n\n2. **Input Configuration**:\n   - `input_config` is a dictionary defining preprocessing parameters:\n     - `'input_shape'`: `(1, 3, 224, 224)`, which specifies the size and channels of the input image tensor.\n     - `'input_path'`: `'demo/demo.jpg'`, indicating the path to the input image.\n     - `'normalize_cfg'`: A dictionary containing:\n       - `'mean'`: `(123.675, 116.28, 103.53)`, mean values for normalization.\n       - `'std'`: `(58.395, 57.12, 57.375)`, standard deviation values for normalization.\n\n3. **Processing the Input**:\n   - `one_img, one_meta = preprocess_example_input(input_config)`: This line calls the imported function with `input_config` and stores the results in `one_img` and `one_meta`.\n\n4. **Printing Results**:\n   - `print(one_img.shape)`: This prints the shape of the processed image tensor, which is `torch.Size([1, 3, 224, 224])`.\n   - `print(one_meta)`: This prints metadata associated with the image, which includes:\n     - `'img_shape'`: Original image shape `(224, 224, 3)`\n     - `'ori_shape'`: Original shape before preprocessing `(224, 224, 3)`\n     - `'pad_shape'`: Shape after any padding `(224, 224, 3)`\n     - `'filename'`: Image filename `'<demo>.png'`\n     - `'scale_factor'`: Scaling factor `1.0`\n     - `'flip'`: Indicates whether the image was flipped `False`"}
{"layout": 1899, "type": "text", "text": "37.4 mask ", "text_level": 1, "page_idx": 218, "bbox": [71, 72, 145, 87], "page_size": [612.0, 792.0]}
{"layout": 1900, "type": "text", "text": "class  mmdet.core.mask. Base Instance Masks Base class for instance masks. ", "page_idx": 218, "bbox": [72.0, 106.35196685791016, 275.98419189453125, 129.8934783935547], "page_size": [612.0, 792.0]}
{"layout": 1901, "type": "text", "text": "abstract property areas areas of each instance. Type  ndarray ", "page_idx": 218, "bbox": [96, 136.23995971679688, 217, 178.31124877929688], "page_size": [612.0, 792.0]}
{"layout": 1902, "type": "text", "text": "abstract crop ( bbox ) Crop each mask by the given bbox. Parameters  bbox  ( ndarray ) – Bbox in format [x1, y1, x2, y2], shape (4, ). Returns  The cropped masks. Return type  Base Instance Masks ", "page_idx": 218, "bbox": [96, 182.18699645996094, 442, 261.9981994628906], "page_size": [612.0, 792.0]}
{"layout": 1903, "type": "text", "text": "abstract crop and resize ( bboxes ,  out_shape ,  inds ,  device ,  interpolation  $.=$  'bilinear' ,  binarize  $\\mathbf{=}$  True ) Crop and resize masks by the given bboxes. ", "page_idx": 218, "bbox": [96, 265.87298583984375, 521, 291.2884521484375], "page_size": [612.0, 792.0]}
{"layout": 1904, "type": "text", "text": "This function is mainly used in mask targets computation. It firstly align mask to bboxes by assigned in ds, then crop mask by the assigned bbox and resize to the size of (mask_h, mask_w) ", "page_idx": 218, "bbox": [118, 295.9104309082031, 540.0037231445312, 321.1754455566406], "page_size": [612.0, 792.0]}
{"layout": 1905, "type": "text", "text": "Parameters •  bboxes  ( Tensor ) – Bboxes in format [x1, y1, x2, y2], shape (N, 4) •  out_shape  ( tuple[int] ) – Target (h, w) of resized mask •  inds  ( ndarray ) – Indexes to assign masks to each bbox, shape (N,) and values should be between [0, num_masks - 1]. •  device  ( str ) – Device of bboxes •  interpolation  ( str ) – See  mmcv.imresize •  binarize  ( bool ) – if True fractional values are rounded to 0 or 1 after the resize operation. if False and unsupported an error will be raised. Defaults to True. Returns  the cropped and resized masks. Return type  Base Instance Masks ", "page_idx": 218, "bbox": [137, 325.1707763671875, 521, 507.07928466796875], "page_size": [612.0, 792.0]}
{"layout": 1906, "type": "text", "text": "abstract expand(expanded_h, expanded_w, top, left)see  Expand . ", "page_idx": 218, "bbox": [96, 510.95404052734375, 324.7423095703125, 536.3685302734375], "page_size": [612.0, 792.0]}
{"layout": 1907, "type": "text", "text": "abstract flip ( flip direction  $.=$  'horizontal' ) Flip masks alone the given direction. Parameters  flip direction  ( str ) – Either ‘horizontal’ or ‘vertical’. Returns  The flipped masks. Return type  Base Instance Masks abstract pad(out_shape, pad_val)Pad masks to the given size of (h, w). Parameters •  out_shape  ( tuple[int] ) – Target (h, w) of padded mask. •  pad_val  ( int ) – The padded value. Returns  The padded masks. ", "page_idx": 218, "bbox": [96, 540.842041015625, 425, 722.272216796875], "page_size": [612.0, 792.0]}
{"layout": 1908, "type": "text", "text": "Return type  Base Instance Masks ", "page_idx": 219, "bbox": [137, 70.8248291015625, 271, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 1909, "type": "text", "text": "abstract rescale ( scale ,  interpolation  $=$  'nearest' ) Rescale masks as large as possible while keeping the aspect ratio. For details can refer to  mmcv.imrescale . ", "page_idx": 219, "bbox": [96, 89.23503875732422, 539.9996337890625, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 1910, "type": "text", "text": "Parameters •  scale  ( tuple[int] ) – The maximum size (h, w) of rescaled mask. •  interpolation  ( str ) – Same as  mmcv.imrescale() . Returns  The rescaled masks. Return type  Base Instance Masks ", "page_idx": 219, "bbox": [137, 118.64483642578125, 437.4188537597656, 204.91122436523438], "page_size": [612.0, 792.0]}
{"layout": 1911, "type": "text", "text": "abstract resize ( out_shape ,  interpolation  $=$  'nearest' ) Resize masks to the given out_shape. ", "page_idx": 219, "bbox": [96, 208.78697204589844, 324.2633361816406, 234.2014617919922], "page_size": [612.0, 792.0]}
{"layout": 1912, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 219, "bbox": [136, 241, 188, 251], "page_size": [612.0, 792.0]}
{"layout": 1913, "type": "text", "text": "•  out_shape  – Target (h, w) of resized mask. •  interpolation  ( str ) – See  mmcv.imresize() . Returns  The resized masks. Return type  Base Instance Masks ", "page_idx": 219, "bbox": [137, 256.7574157714844, 365.4686279296875, 324.4632263183594], "page_size": [612.0, 792.0]}
{"layout": 1914, "type": "text", "text": "abstract rotate ( out_shape ,  angle ,  center  $\\leftrightharpoons$  None ,  scale  $\\mathbf{=}$  1.0 ,  fill_val=0 ) Rotate the masks. ", "page_idx": 219, "bbox": [96, 328.3390197753906, 399.68634033203125, 353.75347900390625], "page_size": [612.0, 792.0]}
{"layout": 1915, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 219, "bbox": [136, 359, 188, 371], "page_size": [612.0, 792.0]}
{"layout": 1916, "type": "text", "text": "•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  angle  ( int | float ) – Rotation angle in degrees. Positive values mean counter- clockwise rotation. •  center  ( tuple[float], optional ) – Center point (w, h) of the rotation in source im- age. If not specified, the center of the image will be used. •  scale  ( int | float ) – Isotropic scale factor. •  fill_val  ( int | float ) – Border value. Default 0 for masks. ", "page_idx": 219, "bbox": [155, 376.3084716796875, 521, 485.2605285644531], "page_size": [612.0, 792.0]}
{"layout": 1917, "type": "text", "text": "Returns  Rotated masks. ", "page_idx": 219, "bbox": [137, 489.255859375, 239, 503.7912902832031], "page_size": [612.0, 792.0]}
{"layout": 1918, "type": "text", "text": "shear ( out_shape ,  magnitude ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  border value  $\\mathrm{=}0$  ,  interpolation  $=$  'bilinear' ) Shear the masks. ", "page_idx": 219, "bbox": [96, 507.66607666015625, 484.0542297363281, 533.0805053710938], "page_size": [612.0, 792.0]}
{"layout": 1919, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 219, "bbox": [136, 539, 188, 550], "page_size": [612.0, 792.0]}
{"layout": 1920, "type": "text", "text": "•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  magnitude  ( int | float ) – The magnitude used for shear. •  direction  ( str ) – The shear direction, either “horizontal” or “vertical”. •  border value  ( int   $I$   tuple[int] ) – Value used in case of a constant border. Default 0. •  interpolation  ( str ) – Same as in  mmcv.imshear() . Returns  Sheared masks. ", "page_idx": 219, "bbox": [137, 555.636474609375, 521, 671.1632080078125], "page_size": [612.0, 792.0]}
{"layout": 1921, "type": "text", "text": "Return type  ndarray ", "page_idx": 219, "bbox": [137, 674.5608520507812, 223.70083618164062, 689.0962524414062], "page_size": [612.0, 792.0]}
{"layout": 1922, "type": "text", "text": "abstract to_ndarray () Convert masks to the format of ndarray. ", "page_idx": 219, "bbox": [96, 694.8440551757812, 277.0807189941406, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 1923, "type": "text", "text": "Returns  Converted masks in the format of ndarray. ", "page_idx": 220, "bbox": [137, 70.8248291015625, 344.1085510253906, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 1924, "type": "text", "text": "Return type  ndarray ", "page_idx": 220, "bbox": [137, 88.7568359375, 223.7009735107422, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 1925, "type": "text", "text": "abstract to_tensor ( dtype ,  device ) Convert masks to the format of Tensor. ", "page_idx": 220, "bbox": [96, 107.16802215576172, 276, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 1926, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 220, "bbox": [136, 138, 188, 150], "page_size": [612.0, 792.0]}
{"layout": 1927, "type": "text", "text": "•  dtype  ( str ) – Dtype of converted mask. •  device  ( torch.device ) – Device of converted masks. Returns  Converted masks in the format of Tensor. ", "page_idx": 220, "bbox": [137, 155.13844299316406, 387.8101501464844, 204.91122436523438], "page_size": [612.0, 792.0]}
{"layout": 1928, "type": "text", "text": "Return type  Tensor ", "page_idx": 220, "bbox": [137, 208.30877685546875, 220.14431762695312, 222.84420776367188], "page_size": [612.0, 792.0]}
{"layout": 1929, "type": "text", "text": "abstract translate ( out_shape ,  offset ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  fill_va  $\\imath\\!\\!=\\!\\!O$  ,  interpolation  $.=$  'bilinear' ) Translate the masks. ", "page_idx": 220, "bbox": [96, 226.71995544433594, 505.4943542480469, 252.1344451904297], "page_size": [612.0, 792.0]}
{"layout": 1930, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 220, "bbox": [136, 258, 188, 269], "page_size": [612.0, 792.0]}
{"layout": 1931, "type": "text", "text": "•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  offset  ( int   $I$   float ) – The offset for translate. •  direction  ( str ) – The translate direction, either “horizontal” or “vertical”. •  fill_val  ( int | float ) – Border value. Default 0. •  interpolation  ( str ) – Same as  mmcv.im translate() . Returns  Translated masks. ", "page_idx": 220, "bbox": [137, 274.6894226074219, 470.28546142578125, 378.2612609863281], "page_size": [612.0, 792.0]}
{"layout": 1932, "type": "text", "text": "class  mmdet.core.mask. Bitmap Masks ( masks ,  height ,  width ) This class represents masks in the form of bitmaps. ", "page_idx": 220, "bbox": [71.99998474121094, 382.1370544433594, 336.38336181640625, 407.551513671875], "page_size": [612.0, 792.0]}
{"layout": 1933, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 220, "bbox": [117, 414, 169, 424], "page_size": [612.0, 792.0]}
{"layout": 1934, "type": "text", "text": "•  masks  ( ndarray ) – ndarray of masks in shape (N, H, W), where N is the number of objects. •  height  ( int ) – height of masks •  width  ( int ) – width of masks ", "page_idx": 220, "bbox": [145, 430.1075134277344, 518.0838623046875, 479.2825622558594], "page_size": [612.0, 792.0]}
{"layout": 1935, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 220, "bbox": [96, 499, 138, 510], "page_size": [612.0, 792.0]}
{"layout": 1936, "type": "table", "page_idx": 220, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_128.jpg", "bbox": [91, 522, 545, 715], "page_size": [612.0, 792.0], "ocr_text": ">>>\n>>>\n>>>\n>>>\n>>>\n\nfrom mmdet.core.mask.structures import * # NOQA\nnum_masks, H, W = 3, 32, 32\n\nrng = np.random.RandomState(0)\n\nmasks = (rng.rand(num_masks, H, W) > 0.1).astype(np.int)\nself = BitmapMasks(masks, height=H, width=W)\n\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n\n# demo crop_and_resize\nnum_boxes = 5\nbboxes = np.array([[9, 9, 30, 10.0]] * num_boxes)\nout_shape = (14, 14)\ninds = torch.randint(0, len(self), size=(num_boxes,))\ndevice = 'cpu\ninterpolation = 'bilinear'\nnew = self.crop_and_resize(\n\nbboxes, out_shape, inds, device, interpolation)\n\n", "vlm_text": "The image shows a Python code snippet involving image processing tasks, likely related to computer vision. The code imports a module and demonstrates mask creation and resizing operations. Here's a summary of the key parts:\n\n1. Import statement:\n   - Imports functions from `mmdet.core.mask.structures`.\n\n2. Mask creation:\n   - `num_masks`, `H`, `W` are set to 3, 32, and 32, respectively.\n   - A random number generator `rng` is initialized.\n   - A `masks` array is created where values are greater than 0.1.\n   - `BitmapMasks` is instantiated with the `masks`, with specified height `H` and width `W`.\n\n3. Cropping and resizing:\n   - `num_boxes` is set to 5.\n   - `bboxes` is an array indicating bounding box dimensions.\n   - `out_shape` is set to (14, 14).\n   - `inds` generates random indices based on `self`.\n   - Specifies the device as 'cpu' and interpolation method as 'bilinear'.\n   - Calls a method `crop_and_resize` on `self` with the specified parameters.\n\nThis code seems to demonstrate patch-based image processing, useful for tasks like segmentation or detection using bounding boxes."}
{"layout": 1937, "type": "table", "page_idx": 221, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_129.jpg", "bbox": [92, 82, 544, 114], "page_size": [612.0, 792.0], "ocr_text": ">>> assert len(new) == num_boxes\n>>> assert new.height, new.width == out_shape\n", "vlm_text": "The image shows Python code with two assert statements:\n\n1. `assert len(new) == num_boxes`\n2. `assert new.height, new.width == out_shape`\n\nThese statements are likely checking conditions within a program to ensure that the length of a collection `new` equals `num_boxes`, and that the `height` and `width` attributes of `new` match `out_shape`. If these conditions are not met, the program will raise an AssertionError."}
{"layout": 1938, "type": "text", "text": "property areas See  Base Instance Masks.areas . ", "page_idx": 221, "bbox": [96, 123.9369888305664, 263, 147.47850036621094], "page_size": [612.0, 792.0]}
{"layout": 1939, "type": "text", "text": "crop ( bbox ) See  Base Instance Masks.crop() ", "page_idx": 221, "bbox": [96, 151.95201110839844, 263, 177.3665008544922], "page_size": [612.0, 792.0]}
{"layout": 1940, "type": "text", "text": "crop and resize ( bboxes ,  out_shape ,  inds ,  device  $\\mathbf{=}$  'cpu' ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'bilinear' ,  binarize  $\\mathbf{\\tilde{=}}$  True ) See  Base Instance Masks.crop and resize() . ", "page_idx": 221, "bbox": [96, 181.8400115966797, 498.5103454589844, 207.25450134277344], "page_size": [612.0, 792.0]}
{"layout": 1941, "type": "text", "text": "expand(expanded_h, expanded_w, top, left)See  Base Instance Masks.expand() . ", "page_idx": 221, "bbox": [96, 211.72801208496094, 277, 237.1425018310547], "page_size": [612.0, 792.0]}
{"layout": 1942, "type": "text", "text": "flip ( flip direction  $=$  'horizontal' ) See  Base Instance Masks.flip() . ", "page_idx": 221, "bbox": [96, 241.6160125732422, 263, 267.030517578125], "page_size": [612.0, 792.0]}
{"layout": 1943, "type": "text", "text": "pad ( out_shape ,  pad_va  $\\mathit{l}\\mathrm{=}\\mathit{0.}$  ) See  Base Instance Masks.pad() . ", "page_idx": 221, "bbox": [96, 271.5040588378906, 263, 296.91851806640625], "page_size": [612.0, 792.0]}
{"layout": 1944, "type": "text", "text": "class method random ( num_mask  $\\scriptstyle{:=3}$  ,  height=32 ,  width  $\\iota{=}32$  ,  dtyp  $\\scriptstyle{\\mathcal{S}}=$  <class 'numpy.uint8  $^{\\prime}{>}$  ,  $r n g{=}N o n e)$  ) Generate random bitmap masks for demo / testing purposes. ", "page_idx": 221, "bbox": [96, 301.3920593261719, 521, 326.8065185546875], "page_size": [612.0, 792.0]}
{"layout": 1945, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 221, "bbox": [117, 346, 161, 358], "page_size": [612.0, 792.0]}
{"layout": 1946, "type": "text", "text": " $>>$   from  mmdet.core.mask.structures  import  Bitmap Masks >>> self $=$  Bitmap Masks.random() $>>$   print ( ' self  $\\begin{array}{r l}{\\mathbf{\\Sigma}=}&{{}\\{\\mathcal{Y}}}\\end{array}$  ' . format( self )) self  $=$   Bitmap Masks(num_masks  $^{=3}$  , height  $.=32$  , width  $\\scriptstyle{1=32}$  ) ", "page_idx": 221, "bbox": [117, 374.2449951171875, 401.4741516113281, 420.7201232910156], "page_size": [612.0, 792.0]}
{"layout": 1947, "type": "text", "text": "rescale ( scale ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'nearest' ) See Base Instance Masks.rescale()", "page_idx": 221, "bbox": [96, 433.1440124511719, 277, 458.5584716796875], "page_size": [612.0, 792.0]}
{"layout": 1948, "type": "text", "text": "resize ( out_shape ,  interpolation  $=$  'nearest' ) See  Base Instance Masks.resize() . ", "page_idx": 221, "bbox": [96, 463.0320129394531, 277, 488.44647216796875], "page_size": [612.0, 792.0]}
{"layout": 1949, "type": "text", "text": "rotate ( out_shape ,  angle ,  center  $\\mathbf{\\hat{\\Sigma}}$  None ,  scale  $\\mathbf{\\tilde{=}}$  1.0 ,  fill_va  $l{=}0.$  ) Rotate the Bitmap Masks. ", "page_idx": 221, "bbox": [96, 492.9200134277344, 352, 518.33447265625], "page_size": [612.0, 792.0]}
{"layout": 1950, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 221, "bbox": [136, 524, 188, 535], "page_size": [612.0, 792.0]}
{"layout": 1951, "type": "text", "text": "•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  angle  ( int | float ) – Rotation angle in degrees. Positive values mean counter- clockwise rotation. •  center  ( tuple[float], optional ) – Center point (w, h) of the rotation in source im- age. If not specified, the center of the image will be used. •  scale  ( int | float ) – Isotropic scale factor. •  fill_val  ( int | float ) – Border value. Default 0 for masks. Returns  Rotated Bitmap Masks. Return type Bitmap Masks", "page_idx": 221, "bbox": [137, 540.8904418945312, 521, 686.30419921875], "page_size": [612.0, 792.0]}
{"layout": 1952, "type": "text", "text": "shear ( out_shape ,  magnitude ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  border value  $\\scriptstyle{\\cdot=0}$  ,  interpolation  $\\mathbf{\\beta}=$  'bilinear' ) Shear the Bitmap Masks. ", "page_idx": 221, "bbox": [96, 690.1799926757812, 484.0542907714844, 715.594482421875], "page_size": [612.0, 792.0]}
{"layout": 1953, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 222, "bbox": [136, 73, 188, 84], "page_size": [612.0, 792.0]}
{"layout": 1954, "type": "text", "text": "•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  magnitude  ( int | float ) – The magnitude used for shear. •  direction  ( str ) – The shear direction, either “horizontal” or “vertical”. •  border value  ( int | tuple[int] ) – Value used in case of a constant border. •  interpolation  ( str ) – Same as in  mmcv.imshear() . ", "page_idx": 222, "bbox": [155, 89.38447570800781, 488.08880615234375, 174.4254608154297], "page_size": [612.0, 792.0]}
{"layout": 1955, "type": "text", "text": "Returns  The sheared masks. Return type Bitmap Masks", "page_idx": 222, "bbox": [137, 178.4207763671875, 254.7440643310547, 210.88919067382812], "page_size": [612.0, 792.0]}
{"layout": 1956, "type": "text", "text": "to_ndarray () See  Base Instance Masks.to_ndarray() ", "page_idx": 222, "bbox": [96, 216.63790893554688, 292, 240.17942810058594], "page_size": [612.0, 792.0]}
{"layout": 1957, "type": "text", "text": "to_tensor ( dtype ,  device ) See  Base Instance Masks.to_tensor() . ", "page_idx": 222, "bbox": [96, 244.65293884277344, 292, 270.06744384765625], "page_size": [612.0, 792.0]}
{"layout": 1958, "type": "text", "text": "translate ( out_shape ,  offset ,  direction  $.=$  'horizontal' ,  fill_val  $\\mathbf{\\chi}_{=0}$  ,  interpolation  $=$  'bilinear' ) Translate the Bitmap Masks. ", "page_idx": 222, "bbox": [96, 274.5409851074219, 458, 299.9554443359375], "page_size": [612.0, 792.0]}
{"layout": 1959, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 222, "bbox": [136, 306, 188, 317], "page_size": [612.0, 792.0]}
{"layout": 1960, "type": "text", "text": "•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  offset  ( int | float ) – The offset for translate. •  direction  ( str ) – The translate direction, either “horizontal” or “vertical”. •  fill_val  ( int | float ) – Border value. Default 0 for masks. •  interpolation  ( str ) – Same as  mmcv.im translate() . ", "page_idx": 222, "bbox": [155, 322.51043701171875, 470.2854309082031, 407.551513671875], "page_size": [612.0, 792.0]}
{"layout": 1961, "type": "text", "text": "Returns  Translated Bitmap Masks. ", "page_idx": 222, "bbox": [137, 411.5468444824219, 278, 426.082275390625], "page_size": [612.0, 792.0]}
{"layout": 1962, "type": "text", "text": "Return type Bitmap Masks", "page_idx": 222, "bbox": [137, 429.4798583984375, 247, 444.0152893066406], "page_size": [612.0, 792.0]}
{"layout": 1963, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 222, "bbox": [117, 463, 161, 474], "page_size": [612.0, 792.0]}
{"layout": 1964, "type": "text", "text": " $>>$   from  mmdet.core.mask.structures  import  Bitmap Masks >>>  self  $=$   Bitmap Masks . random(dtype = np . uint8) >>>  out_shape  $=$   ( 32 ,  32 )  $>>$   offset  $\\c=4$   $>>$   direction  $=$   ' horizontal '  $>>$   fill_val  $\\mathbf{\\varepsilon}=\\mathbf{\\varepsilon}\\,\\Updownarrow$   $>>$   interpolation  $=$  ' bilinear '  $>>$   # Note, There seem to be issues when: >>>  #   $^*$   out_shape is different than self ' s shape >>>  # \\* the mask dtype is not supported by cv2.AffineWarp >>>  new    $=$   self . translate(out_shape, offset, direction, fill_val, >>> interpolation)  $>>$   assert  len (new)  $==$   len ( self ) >>>  assert  new . height, new . width  $==$   out_shape ", "page_idx": 222, "bbox": [117, 490.8559875488281, 453.56829833984375, 657.1819458007812], "page_size": [612.0, 792.0]}
{"layout": 1965, "type": "text", "text": "class  mmdet.core.mask. Polygon Masks ( masks ,  height ,  width ) This class represents masks in the form of polygons. ", "page_idx": 222, "bbox": [72.0, 669.3070068359375, 341.61334228515625, 694.7214965820312], "page_size": [612.0, 792.0]}
{"layout": 1966, "type": "text", "text": "Polygons is a list of three levels. The first level of the list corresponds to objects, the second level to the polys that compose the object, the third level to the poly coordinates ", "page_idx": 223, "bbox": [96, 71.45246887207031, 540, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1967, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 223, "bbox": [118, 103, 169, 114], "page_size": [612.0, 792.0]}
{"layout": 1968, "type": "text", "text": "•  masks  ( list[list[ndarray]] ) – The first level of the list corresponds to objects, the second level to the polys that compose the object, the third level to the poly coordinates •  height  ( int ) – height of masks •  width  ( int ) – width of masks ", "page_idx": 223, "bbox": [145, 119.27247619628906, 518.0841674804688, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 1969, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 223, "bbox": [96, 200, 139, 212], "page_size": [612.0, 792.0]}
{"layout": 1970, "type": "image", "page_idx": 223, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_130.jpg", "bbox": [92, 220, 544, 515], "page_size": [612.0, 792.0], "ocr_text": ">>> from mmdet.core.mask.structures import * # NOQA\n\n>>> masks = [\n\n>>> [ np.array([0, 0, 10, 0, 10, 10., 0, 10, 0, 9]) ]\n>>> J\n\n>>> height, width = 16, 16\n\n>>> self = PolygonMasks(masks, height, width)\n\n>>> # demo translate\n\n>>> new = self.translate((16, 16), 4., direction='horizontal')\n>>> assert np.all(new.masks[0][0][1::2] == masks[0][0][1::2])\n>>> assert np.all(new.masks[0][0][0::2] == masks[0][0][0::2] + 4)\n\n>>> # demo crop_and_resize\n\n>>> num_boxes = 3\n\n>>> bboxes = np.array([[0, 9, 30, 10.0]] * num_boxes)\n>>> out_shape = (16, 16)\n\n>>> inds = torch.randint(9, len(self), size=(num_boxes, ))\n>>> device = 'cpu'\n\n>>> interpolation = 'bilinear'\n\n>>> new = self.crop_and_resize(\n\n— bboxes, out_shape, inds, device, interpolation)\n>>> assert len(new) == num_boxes\n\n>>> assert new.height, new.width == out_shape\n\n", "vlm_text": "The image shows a Python script using the `mmdet` library to demonstrate operations on polygon masks. It includes:\n\n1. **Import and Initialization:**\n   - Importing `PolygonMasks`.\n   - Creating a mask using a NumPy array.\n   - Setting height and width to 16.\n   - Initializing `PolygonMasks`.\n\n2. **Translate Operation:**\n   - Demonstrating translation by modifying the mask coordinates horizontally.\n   - Asserting conditions to verify the operation's correctness.\n\n3. **Crop and Resize Operation:**\n   - Setting the number of boxes to 3 and defining bounding boxes.\n   - Specifying output shape, indices, and device ('cpu').\n   - Using bilinear interpolation for resizing.\n   - Asserting that the new mask dimensions match the expected output."}
{"layout": 1971, "type": "text", "text": "property areas ", "text_level": 1, "page_idx": 223, "bbox": [95, 524, 171, 535], "page_size": [612.0, 792.0]}
{"layout": 1972, "type": "text", "text": "Compute areas of masks. This func is modified from  detectron2 . The function only works with Polygons using the shoelace formula. ", "page_idx": 223, "bbox": [118, 533.5654296875, 540, 564.8085327148438], "page_size": [612.0, 792.0]}
{"layout": 1973, "type": "text", "text": "Returns  areas of each instance ", "page_idx": 223, "bbox": [137.4550018310547, 568.8038330078125, 263.28204345703125, 583.3392333984375], "page_size": [612.0, 792.0]}
{"layout": 1974, "type": "text", "text": "crop ( bbox ) see  Base Instance Masks.crop() ", "page_idx": 223, "bbox": [96, 605.1470336914062, 259, 630.5614624023438], "page_size": [612.0, 792.0]}
{"layout": 1975, "type": "text", "text": "crop and resize ( bboxes ,  out_shape ,  inds ,  device  $=$  'cpu' ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'bilinear' ,  binarize  $\\mathbf{\\tilde{=}}$  True ) see  Base Instance Masks.crop and resize() ", "page_idx": 223, "bbox": [96, 635.0350341796875, 498.5102844238281, 660.4495239257812], "page_size": [612.0, 792.0]}
{"layout": 1976, "type": "text", "text": "expand ( \\*args ,  \\*\\*kwargs ) TODO: Add expand for polygon ", "page_idx": 223, "bbox": [96, 664.9230346679688, 247.63027954101562, 690.3374633789062], "page_size": [612.0, 792.0]}
{"layout": 1977, "type": "text", "text": "flip ( flip direction  $=$  'horizontal' ) see  Base Instance Masks.flip() ", "page_idx": 223, "bbox": [96, 694.81103515625, 259, 720.2255249023438], "page_size": [612.0, 792.0]}
{"layout": 1978, "type": "text", "text": "pad ( out_shape ,  pad_va  $\\mathbf{\\chi}_{}^{}$  ) padding has no effect on polygons\\` ", "page_idx": 224, "bbox": [96, 71.30303192138672, 257.70355224609375, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 1979, "type": "text", "text": "class method random ( num_mask  $\\backprime{=}3$  ,  heigh  $t{=}32$  ,  width  $\\iota{=}32$  ,  n_vert  $\\wp{=}5$  ,  dtype  $<<$  class 'numpy.float32'> ,  $\\scriptstyle{r n g=N o n e})$  Generate random polygon masks for demo / testing purposes. Adapted from ", "page_idx": 224, "bbox": [96, 101.19103240966797, 523.1875610351562, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 1980, "type": "text", "text": "References ", "page_idx": 224, "bbox": [117, 174.35609436035156, 174, 188.62254333496094], "page_size": [612.0, 792.0]}
{"layout": 1981, "type": "text", "text": "Example ", "page_idx": 224, "bbox": [117, 198.72108459472656, 159, 212.98753356933594], "page_size": [612.0, 792.0]}
{"layout": 1982, "type": "text", "text": " $>>$   from  mmdet.core.mask.structures  import  Polygon Masks >>>  self  $=$   Polygon Masks . random() >>>  print ( ' self =  {} ' . format( self )) ", "page_idx": 224, "bbox": [117, 228.29696655273438, 406.70452880859375, 263.1160583496094], "page_size": [612.0, 792.0]}
{"layout": 1983, "type": "text", "text": "rescale ( scale ,  interpolation  $\\scriptstyle\\varepsilon$  None ) see  Base Instance Masks.rescale() ", "page_idx": 224, "bbox": [96, 275.24102783203125, 277, 300.6554870605469], "page_size": [612.0, 792.0]}
{"layout": 1984, "type": "text", "text": "resize ( out_shape ,  interpolation  $\\scriptstyle.\\equiv$  None ) see  Base Instance Masks.resize() ", "page_idx": 224, "bbox": [96, 305.1290283203125, 270, 330.5434875488281], "page_size": [612.0, 792.0]}
{"layout": 1985, "type": "text", "text": "rotate ( out_shape ,  angle ,  center  $\\leftrightharpoons$  None ,  scale  $\\mathbf{\\tilde{=}}$  1.0 ,  fill_va  $l{=}0.$  ) See  Base Instance Masks.rotate() . ", "page_idx": 224, "bbox": [96, 335.01702880859375, 352.6123352050781, 360.4314880371094], "page_size": [612.0, 792.0]}
{"layout": 1986, "type": "text", "text": "shear ( out_shape ,  magnitude ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  border value  $\\mathrm{=}0$  ,  interpolation  $\\mathbf{\\beta}=$  'bilinear' ) See  Base Instance Masks.shear() . ", "page_idx": 224, "bbox": [96, 364.905029296875, 484.0543212890625, 390.3194885253906], "page_size": [612.0, 792.0]}
{"layout": 1987, "type": "text", "text": "to_bitmap () convert polygon masks to bitmap masks. ", "page_idx": 224, "bbox": [96, 396.6659851074219, 280.39837646484375, 420.2074890136719], "page_size": [612.0, 792.0]}
{"layout": 1988, "type": "text", "text": "to_ndarray () Convert masks to the format of ndarray. ", "page_idx": 224, "bbox": [96, 426.5539855957031, 277, 450.0954895019531], "page_size": [612.0, 792.0]}
{"layout": 1989, "type": "text", "text": "to_tensor ( dtype ,  device ) See  Base Instance Masks.to_tensor() . ", "page_idx": 224, "bbox": [96, 454.56903076171875, 289.8725891113281, 479.9834899902344], "page_size": [612.0, 792.0]}
{"layout": 1990, "type": "text", "text": "translate ( out_shape ,  offset ,  direction  $.=$  'horizontal' ,  fill_val  $\\leftrightharpoons$  None ,  interpolation  $\\scriptstyle.\\equiv$  None ) Translate the Polygon Masks. ", "page_idx": 224, "bbox": [96, 484.45703125, 458.6402282714844, 509.8714904785156], "page_size": [612.0, 792.0]}
{"layout": 1991, "type": "text", "text": "Example ", "page_idx": 224, "bbox": [117, 527.7340698242188, 159, 542.0005493164062], "page_size": [612.0, 792.0]}
{"layout": 1992, "type": "text", "text": ">>>  self  $=$   Polygon Masks . random(dtype = np . int)  $>>$   out_shape  $=$   ( self . height,  self . width)  $>>$   new  $=$   self . translate(out_shape,  4. , direction  $\\c=$  ' horizontal ' )  $>>$   assert  np . all(new . masks[ 0 ][ 0 ][ 1 :: 2 ]  $==$   self . masks[ 0 ][ 0 ][ 1 :: 2 ]) >>>  assert  np . all(new . masks[ 0 ][ 0 ][ 0 :: 2 ]  $==$   self . masks[ 0 ][ 0 ][ 0 :: 2 ]  +  4 ) # noqa: ␣  $\\substack{\\mathrm{\\Large~\\hookrightarrow\\,}E501}$  ", "page_idx": 224, "bbox": [117, 557.3099975585938, 537.25439453125, 628], "page_size": [612.0, 792.0]}
{"layout": 1993, "type": "text", "text": "mmdet.core.mask. encode mask results ( mask results ) Encode bitmap mask to RLE code. ", "page_idx": 224, "bbox": [72.0, 640.1190185546875, 317.30438232421875, 665.5335083007812], "page_size": [612.0, 792.0]}
{"layout": 1994, "type": "text", "text": "Parameters  mask results  ( list | tuple[list] ) – bitmap mask results. In mask scoring rcnn, mask results is a tuple of (seg m results, seg m cls score). ", "page_idx": 224, "bbox": [117, 669.52880859375, 518.078369140625, 695.4215087890625], "page_size": [612.0, 792.0]}
{"layout": 1995, "type": "text", "text": "Returns  RLE encoded mask. ", "page_idx": 224, "bbox": [117, 699.4168090820312, 238, 713.9522094726562], "page_size": [612.0, 792.0]}
{"layout": 1996, "type": "text", "text": "Return type  list | tuple ", "page_idx": 225, "bbox": [118, 70.8248291015625, 213.62884521484375, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 1997, "type": "text", "text": "mmdet.core.mask. mask target ( pos proposals list ,  pos assigned gt in ds list ,  gt masks list ,  cfg ) Compute mask target for positive proposals in multiple images. ", "page_idx": 225, "bbox": [71.99999237060547, 89.23503875732422, 484, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 1998, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 225, "bbox": [118, 121, 168, 132], "page_size": [612.0, 792.0]}
{"layout": 1999, "type": "text", "text": "•  pos proposals list  ( list[Tensor] ) – Positive proposals in multiple images. •  pos assigned gt in ds list  ( list[Tensor] ) – Assigned GT indices for each positive proposals. •  gt masks list  (list[ Base Instance Masks ]) – Ground truth masks of each image. •  cfg  ( dict ) – Config dict that specifies the mask size. ", "page_idx": 225, "bbox": [145, 137.20545959472656, 518.0833129882812, 216.2694549560547], "page_size": [612.0, 792.0]}
{"layout": 2000, "type": "text", "text": "Returns  Mask target of each image. ", "page_idx": 225, "bbox": [118, 220.2637939453125, 265.1948547363281, 234.79922485351562], "page_size": [612.0, 792.0]}
{"layout": 2001, "type": "text", "text": "Return type  list[Tensor] ", "page_idx": 225, "bbox": [118, 238.19677734375, 220.1841583251953, 252.73220825195312], "page_size": [612.0, 792.0]}
{"layout": 2002, "type": "text", "text": "Example ", "page_idx": 225, "bbox": [96, 269.9970397949219, 137.83326721191406, 284.26348876953125], "page_size": [612.0, 792.0]}
{"layout": 2003, "type": "table", "page_idx": 225, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_131.jpg", "bbox": [92, 295, 544, 699], "page_size": [612.0, 792.0], "ocr_text": ">>> import mmcv\n\n>>> import mmdet\n\n>>> from mmdet.core.mask import BitmapMasks\n\n>>> from mmdet.core.mask.mask_target import *\n\n>>> H, W= 17, 18\n\n>>> cfg = mmcv.Config({'mask_size': (13, 14)})\n\n>>> rng = np.random.RandomState(0)\n\n>>> # Positive proposals (tl_x, tl_y, br_x, br_y) for each image\n>>> pos_proposals_list = [\n\n>>> torch. Tensor([\n\n>>> [ 7.2425, 5.5929, 13.9414, 14.9541],\n>>> [ 7.3241, 3.6170, 16.3850, 15.3102],\nam )),\n\n>>> torch. Tensor([\n\n>>> [ 4.8448, 6.4010, 7.0314, 9.7681],\n>>> [ 5.9790, 2.6989, 7.4416, 4.8580],\n>>> [L 0.0000, 0.0000, 0.1398, 9.8232],\n>>> ),\n\n>> ]\n\n>>> # Corresponding class index for each proposal for each image\n>>> pos_assigned_gt_inds_list = [\n\n>>> torch.LongTensor([7, 0]),\n>> torch.LongTensor([5, 4, 1]),\n>> ]\n\n>>> # Ground truth mask for each true object for each image\n>>> gt_masks_list = [\n\n>>> BitmapMasks(rng.rand(8, H, W), height=H, width=W),\n>>> BitmapMasks(rng.rand(6, H, W), height=H, width=W),\n>> ]\n\n>>> mask_targets = mask_target(\n\n>>> pos_proposals_list, pos_assigned_gt_inds_list,\n\n>>> gt_masks_list, cfg)\n\n>>> assert mask_targets.shape == (5,) + cfg['mask_size']\n\n", "vlm_text": "The image contains a Python script involving image processing and mask creation for object detection using libraries such as `mmcv` and `mmdet`. Here's a breakdown of the code:\n\n1. **Imports:**\n   - Various modules from `mmcv` and `mmdet` for handling configuration and masks.\n\n2. **Configuration:**\n   - Sets image dimensions `H` and `W` to 17 and 18, respectively.\n   - Defines a configuration `cfg` with `mask_size` set to `(13, 14)`.\n\n3. **Random State:**\n   - Initializes a random state using `np.random.RandomState(0)`.\n\n4. **Positive Proposals:**\n   - Defines lists of tensors representing positive proposals with coordinates `(tL_x, tL_y, br_x, br_y)`.\n\n5. **Class Index Assignment:**\n   - Maps proposals to corresponding class indices in `pos_assigned_gt_inds_list`.\n\n6. **Ground Truth Masks:**\n   - Creates a list of `BitmapMasks` representing ground truth masks for each object in the images.\n   - The masks use random values of shape `(H, W)`.\n\n7. **Mask Targeting:**\n   - Computes `mask_targets` using the `mask_target` function with proposals, indices, ground truth masks, and configuration.\n\n8. **Assertion:**\n   - Asserts that the shape of `mask_targets` is `(5,) + `mask_size`, verifying the expected dimensions.\n\nThis script is likely part of a larger system for training or evaluating visual recognition models."}
{"layout": 2004, "type": "text", "text": "mmdet.core.mask. split combined poly s ( polys ,  poly_lens ,  poly s per mask ) Split the combined 1-D polys into masks. ", "page_idx": 226, "bbox": [71, 71.30303192138672, 404.7263488769531, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2005, "type": "text", "text": "A mask is represented as a list of polys, and a poly is represented as a 1-D array. In dataset, all masks are concatenated into a single 1-D tensor. Here we need to split the tensor into original representations. ", "page_idx": 226, "bbox": [96, 101.34046936035156, 540.003662109375, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 2006, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 226, "bbox": [117, 133, 169, 144], "page_size": [612.0, 792.0]}
{"layout": 2007, "type": "text", "text": "•  polys  ( list ) – a list (length  $=$  image num) of 1-D tensors •  poly_lens    $(I i s t)-\\mathbf{a}$   list (length  $=$   image num) of poly length •  poly s per mask  ( list ) – a list (length  $=$   image num) of poly number of each mask ", "page_idx": 226, "bbox": [145, 149.1604766845703, 494, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 2008, "type": "text", "text": "Returns  a list (length  $=$   image num) of list (length  $=$   mask num) of list (length  $=$   poly num) of numpy array. ", "page_idx": 226, "bbox": [118, 202.331787109375, 518, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 2009, "type": "text", "text": "Return type  list ", "page_idx": 226, "bbox": [118, 232.21881103515625, 186.72984313964844, 246.75424194335938], "page_size": [612.0, 792.0]}
{"layout": 2010, "type": "text", "text": "37.5 evaluation ", "text_level": 1, "page_idx": 226, "bbox": [70, 268, 180, 286], "page_size": [612.0, 792.0]}
{"layout": 2011, "type": "text", "text": "class  mmdet.core.evaluation. Di stEv al Hook ( dataloader ,  start=None ,  interval=1 ,  by_epoch=True , save_bes  $t{=}$  None ,  rule=None ,  test_fn=None , greater keys  $\\mathbf{\\check{\\Sigma}}$  None ,  less_keys=None , broadcast bn buffer=True ,  tmpdir  $\\leftrightharpoons$  None ,  gpu collect=False , out_dir  $\\leftrightharpoons$  None ,  file client arg s=None ,  \\*\\*e val k war gs ) ", "page_idx": 226, "bbox": [71, 300.89300537109375, 535.3716430664062, 362.1734313964844], "page_size": [612.0, 792.0]}
{"layout": 2012, "type": "text", "text": "class  mmdet.core.evaluation. EvalHook ( dataloader ,  start  $=$  None ,  interval=1 ,  by_epoch=True , save_best  $=$  None ,  rule  $=$  None ,  test_fn=None ,  greater keys=None , less_keys  $\\mathbf{\\hat{\\rho}}$  None ,  out_dir=None ,  file client arg s=None , \\*\\*e val k war gs)", "page_idx": 226, "bbox": [71, 366.6459655761719, 524.402587890625, 415.86181640625], "page_size": [612.0, 792.0]}
{"layout": 2013, "type": "text", "text": "mmdet.core.evaluation. average precision ( recalls ,  precisions ,  mode  $=$  'area' ) Calculate average precision (for single or multiple scales). ", "page_idx": 226, "bbox": [71, 420.4449462890625, 415.00323486328125, 445.8594055175781], "page_size": [612.0, 792.0]}
{"layout": 2014, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 226, "bbox": [117, 452, 169, 463], "page_size": [612.0, 792.0]}
{"layout": 2015, "type": "text", "text": "•  recalls  ( ndarray ) – shape (num_scales, num_dets) or (num_dets, ) •  precisions  ( ndarray ) – shape (num_scales, num_dets) or (num_dets, ) •  mode  ( str ) – ‘area’ or ‘11points’, ‘area’ means calculating the area under precision-recall curve, ‘11points’ means calculating the average precision of recalls at [0, 0.1, ..., 1] ", "page_idx": 226, "bbox": [145, 468.4143981933594, 518, 529.5454711914062], "page_size": [612.0, 792.0]}
{"layout": 2016, "type": "text", "text": "Returns  calculated average precision ", "page_idx": 226, "bbox": [118, 533.540771484375, 269.78753662109375, 548.076171875], "page_size": [612.0, 792.0]}
{"layout": 2017, "type": "text", "text": "Return type  float or ndarray ", "page_idx": 226, "bbox": [118, 551.4727783203125, 236.064453125, 566.0081787109375], "page_size": [612.0, 792.0]}
{"layout": 2018, "type": "text", "text": "mmdet.core.evaluation. eval_map ( det results ,  annotations ,  scale ranges  $=$  None ,  iou_th  $\\scriptstyle{r=0.5}$  ,  dataset  $\\fallingdotseq$  None , logger  $=$  None ,  tpfp_fn  $\\scriptstyle\\cdot\\equiv$  None ,  nproc  $\\scriptstyle\\bullet=4$  ,  use legacy coordinate  $\\mathbf{\\beta}=$  False ) ", "page_idx": 226, "bbox": [71, 569.8839721679688, 530.6795043945312, 595.2984008789062], "page_size": [612.0, 792.0]}
{"layout": 2019, "type": "text", "text": "Evaluate mAP of a dataset. ", "page_idx": 226, "bbox": [96, 593.9443969726562, 204.84161376953125, 607.2544555664062], "page_size": [612.0, 792.0]}
{"layout": 2020, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 226, "bbox": [117, 613, 169, 625], "page_size": [612.0, 792.0]}
{"layout": 2021, "type": "text", "text": "•  det results  ( list[list] ) – [[cls1_det, cls2_det, ...], ...]. The outer list indicates im- ages, and the inner list indicates per-class detected bboxes. •  annotations  ( list[dict] ) – Ground truth annotations where each item of the list indi- cates an image. Keys of annotations are: –  bboxes : numpy array of shape (n, 4) –  labels : numpy array of shape (n, ) ", "page_idx": 226, "bbox": [145, 629.8093872070312, 518, 721.4261474609375], "page_size": [612.0, 792.0]}
{"layout": 2022, "type": "text", "text": "–  b boxes ignore  (optional): numpy array of shape (k, 4) –  labels ignore  (optional): numpy array of shape (k, ) •  scale ranges  ( list[tuple] | None ) – Range of scales to be evaluated, in the format [(min1, max1), (min2, max2), ...]. A range of (32, 64) means the area range between   $(32^{**}2$  ,  $64^{**}2_{c}$  ). Default: None. •  iou_thr  ( float ) – IoU threshold to be considered as matched. Default: 0.5. •  dataset  ( list[str] | str | None ) – Dataset name or dataset classes, there are minor differences in metrics for different datasets, e.g. “voc07”, “image net det”, etc. Default: None. •  logger  ( logging.Logger | str | None ) – The way to print the mAP summary. See mmcv.utils.print_log()  for details. Default: None. •  tpfp_fn  ( callable | None ) – The function used to determine true/ false posi- tives. If None,  tp fp default()  is used as default unless dataset is ‘det’ or ‘vid’ ( tp fp image net()  in this case). If it is given as a function, then this function is used to evaluate tp & fp. Default None. •  nproc  ( int ) – Processes used for computing TP and FP. Default: 4. •  use legacy coordinate  ( bool ) – Whether to use coordinate system in mmdet v1.x. which means width, height should be calculated as   $\\mathbf{\\dot{x}}2-\\mathbf{x}1+1\\mathbf{\\hat{\\$   and   $\\mathbf{\\dot{y}}2-\\mathbf{y}1+1^{\\prime}$  ’ respectively. Default: False. ", "page_idx": 227, "bbox": [145, 70.8248291015625, 518, 347.7755432128906], "page_size": [612.0, 792.0]}
{"layout": 2023, "type": "text", "text": "Returns  (mAP, [dict, dict, ...]) ", "page_idx": 227, "bbox": [118, 351.7708740234375, 248, 366.3063049316406], "page_size": [612.0, 792.0]}
{"layout": 2024, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 227, "bbox": [117, 372, 197, 384], "page_size": [612.0, 792.0]}
{"layout": 2025, "type": "text", "text": "mmdet.core.evaluation. e val recalls ( gts ,  proposals ,  proposal num s  $\\leftrightharpoons$  None ,  iou_thrs  $\\mathrm{\\Sigma=}0.5$  ,  logger=None , use legacy coordinate  $\\mathbf{\\beta}=$  False ) ", "page_idx": 227, "bbox": [71, 388.1151123046875, 522.0716552734375, 413.41998291015625], "page_size": [612.0, 792.0]}
{"layout": 2026, "type": "text", "text": "Calculate recalls. ", "page_idx": 227, "bbox": [96, 412.1745300292969, 165.51943969726562, 425.48455810546875], "page_size": [612.0, 792.0]}
{"layout": 2027, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 227, "bbox": [117, 432, 169, 443], "page_size": [612.0, 792.0]}
{"layout": 2028, "type": "text", "text": "•  gts  ( list[ndarray] ) – a list of arrays of shape (n, 4) •  proposals  ( list[ndarray] ) – a list of arrays of shape (k, 4) or (k, 5) •  proposal num s  ( int | Sequence[int] ) – Top N proposals to be evaluated. •  iou_thrs  ( float | Sequence[float] ) – IoU thresholds. Default: 0.5. •  logger  ( logging.Logger | str   $I$   None ) – The way to print the recall summary. See mmcv.utils.print_log()  for details. Default: None. •  use legacy coordinate  ( bool ) – Whether use coordinate system in mmdet v1.x. “1” was added to both height and width which means w, h should be computed as   $\\phantom{0}\\phantom{0}\\cdot\\phantom{0}\\times1+1\\phantom{0}\\phantom{0}\\cdot$  and   $\\mathbf{\\dot{y}}2\\mathbf{\\dot{-}y}1+1^{\\mathbf{\\dot{r}}}$  ’. Default: False. ", "page_idx": 227, "bbox": [145, 448.03955078125, 518, 586.879638671875], "page_size": [612.0, 792.0]}
{"layout": 2029, "type": "text", "text": "Returns  recalls of different ious and proposal nums ", "page_idx": 227, "bbox": [118, 590.8739013671875, 329, 605.4093017578125], "page_size": [612.0, 792.0]}
{"layout": 2030, "type": "text", "text": "Return type  ndarray ", "page_idx": 227, "bbox": [118, 608.8069458007812, 205.07093811035156, 623.3423461914062], "page_size": [612.0, 792.0]}
{"layout": 2031, "type": "text", "text": "mmdet.core.evaluation. get classes ( dataset ) Get class names of a dataset. ", "page_idx": 227, "bbox": [71, 627.2181396484375, 285, 652.6326293945312], "page_size": [612.0, 792.0]}
{"layout": 2032, "type": "text", "text": "mmdet.core.evaluation. plot i ou recall ( recalls ,  iou_thrs ) Plot IoU-Recalls curve. ", "page_idx": 227, "bbox": [71, 657.1061401367188, 340.80633544921875, 682.5206298828125], "page_size": [612.0, 792.0]}
{"layout": 2033, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 227, "bbox": [117, 688, 170, 700], "page_size": [612.0, 792.0]}
{"layout": 2034, "type": "text", "text": "•  recalls  ( ndarray or list ) – shape (k,) ", "page_idx": 227, "bbox": [145, 705.0765380859375, 329, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 2035, "type": "text", "text": "•  iou_thrs  ( ndarray or list ) – same shape as  recalls ", "page_idx": 228, "bbox": [145, 71.30303192138672, 380, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 2036, "type": "text", "text": "mmdet.core.evaluation. plot num recall ( recalls ,  proposal num s ) Plot Proposal num-Recalls curve. ", "page_idx": 228, "bbox": [71, 89.23503875732422, 369.0303649902344, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 2037, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 228, "bbox": [117, 120, 169, 132], "page_size": [612.0, 792.0]}
{"layout": 2038, "type": "text", "text": "•  recalls  ( ndarray or list ) – shape (k,) •  proposal num s  ( ndarray or list ) – same shape as  recalls ", "page_idx": 228, "bbox": [145, 137.20545959472656, 406.9432678222656, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 2039, "type": "text", "text": "mmdet.core.evaluation. print map summary ( mean_ap ,  results ,  datase  $\\fallingdotseq$  None ,  scale ranges  $=$  None , logger=None ) ", "page_idx": 228, "bbox": [71, 172.92198181152344, 498, 198.2268829345703], "page_size": [612.0, 792.0]}
{"layout": 2040, "type": "text", "text": "Print mAP and results of each class. ", "page_idx": 228, "bbox": [96, 196.98146057128906, 240.56759643554688, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 2041, "type": "text", "text": "A table will be printed to show the gts/dets/recall/AP of each class and the mAP. ", "page_idx": 228, "bbox": [96, 214.91444396972656, 417.4237060546875, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 2042, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 228, "bbox": [117, 234, 169, 246], "page_size": [612.0, 792.0]}
{"layout": 2043, "type": "text", "text": "•  mean_ap  ( float ) – Calculated from  eval_map() . •  results  ( list[dict] ) – Calculated from  eval_map() . •  dataset  ( list[str] | str | None ) – Dataset name or dataset classes. •  scale ranges  ( list[tuple] | None ) – Range of scales to be evaluated. •  logger  ( logging.Logger | str | None ) – The way to print the mAP summary. See mmcv.utils.print_log()  for details. Default: None. ", "page_idx": 228, "bbox": [145, 250.62998962402344, 518, 347.7754821777344], "page_size": [612.0, 792.0]}
{"layout": 2044, "type": "text", "text": "mmdet.core.evaluation. print recall summary ( recalls ,  proposal num s ,  iou_thrs ,  row_idxs  $:=$  None , col_idxs  $=$  None ,  logger=None ) Print recalls in a table. ", "page_idx": 228, "bbox": [71, 352.2490234375, 498, 389.61846923828125], "page_size": [612.0, 792.0]}
{"layout": 2045, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 228, "bbox": [117, 396, 169, 407], "page_size": [612.0, 792.0]}
{"layout": 2046, "type": "text", "text": "•  recalls  ( ndarray ) – calculated from  b box recalls •  proposal num s  ( ndarray or list ) – top N proposals •  iou_thrs  ( ndarray or list ) – iou thresholds •  row_idxs  ( ndarray ) – which rows(proposal nums) to print •  col_idxs  ( ndarray ) – which cols(iou thresholds) to print •  logger  ( logging.Logger | str | None ) – The way to print the recall summary. See mmcv.utils.print_log()  for details. Default: None. ", "page_idx": 228, "bbox": [145, 412.0250244140625, 518, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 2047, "type": "text", "text": "37.6 post processing ", "text_level": 1, "page_idx": 228, "bbox": [70, 550, 224, 567], "page_size": [612.0, 792.0]}
{"layout": 2048, "type": "text", "text": "mmdet.core.post processing. fast_nms ( multi b boxes ,  multi scores ,  multi coe ffs ,  score_thr ,  iou_thr ,  top_k , max_num  $\\scriptstyle-I$  ) ", "page_idx": 228, "bbox": [71, 581.8390502929688, 528.6863403320312, 607.1439208984375], "page_size": [612.0, 792.0]}
{"layout": 2049, "type": "text", "text": "Fast NMS in YOLACT.", "page_idx": 228, "bbox": [96, 605.8994750976562, 191.4716033935547, 619.2095336914062], "page_size": [612.0, 792.0]}
{"layout": 2050, "type": "text", "text": "Fast NMS allows already-removed detections to suppress other detections so that every instance can be decided to be kept or discarded in parallel, which is not possible in traditional NMS. This relaxation allows us to implement Fast NMS entirely in standard GPU-accelerated matrix operations. ", "page_idx": 228, "bbox": [96, 623.8314819335938, 540, 661.0525512695312], "page_size": [612.0, 792.0]}
{"layout": 2051, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 228, "bbox": [117, 666, 169, 678], "page_size": [612.0, 792.0]}
{"layout": 2052, "type": "text", "text": "•  multi b boxes  ( Tensor ) – shape (n, #class\\*4) or (n, 4) ", "page_idx": 228, "bbox": [145, 683.6074829101562, 380, 696.9175415039062], "page_size": [612.0, 792.0]}
{"layout": 2053, "type": "text", "text": "•  multi scores  ( Tensor ) – shape (n, #class  $_{;+1}$  ), where the last column contains scores of the background class, but this will be ignored. •  multi coe ffs  ( Tensor ) – shape (n, #class\\*coeffs_dim). •  score_thr  ( float ) – bbox threshold, bboxes with scores lower than it will not be consid- ered. •  iou_thr  ( float ) – IoU threshold to be considered as conflicted. •  top_k  ( int ) – if there are more than top_k bboxes before NMS, only top top_k will be kept. •  max_num  ( int ) – if there are more than max_num bboxes after NMS, only top max_num will be kept. If -1, keep all the bboxes. Default: -1. ", "page_idx": 229, "bbox": [145, 71.45246887207031, 518, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 2054, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 229, "bbox": [118, 216, 154, 228], "page_size": [612.0, 792.0]}
{"layout": 2055, "type": "text", "text": "(dets, labels, coefficients), tensors of shape  $(\\mathbf{k},\\mathsf{5}),(\\mathbf{k},\\mathbf{1})$  ,  and (k, coeffs_dim). Dets are boxes with scores. Labels are 0-based. ", "page_idx": 229, "bbox": [137, 232.21881103515625, 518, 258.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 2056, "type": "text", "text": "Return type  tuple ", "page_idx": 229, "bbox": [118, 262.1068115234375, 194.62017822265625, 276.6422424316406], "page_size": [612.0, 792.0]}
{"layout": 2057, "type": "text", "text": "mmdet.core.post processing. mask matrix nm s ( masks ,  labels ,  scores ,  filter_th  $r{=}{-}\\ I$  ,  nms_pre=- 1 , max_num=- 1 ,  kernel  $'='$  gaussian' ,  sigma  $=\\!2.0$  , mask_area  $=$  None ) ", "page_idx": 229, "bbox": [71, 280.5180358886719, 498, 317.77789306640625], "page_size": [612.0, 792.0]}
{"layout": 2058, "type": "text", "text": "Matrix NMS for multi-class masks. ", "page_idx": 229, "bbox": [96, 316.533447265625, 238.11680603027344, 329.8434753417969], "page_size": [612.0, 792.0]}
{"layout": 2059, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 229, "bbox": [117, 336, 169, 347], "page_size": [612.0, 792.0]}
{"layout": 2060, "type": "text", "text": "•  masks  ( Tensor ) – Has shape (num instances, h, w) •  labels  ( Tensor ) – Labels of corresponding masks, has shape (num instances,). •  scores  ( Tensor ) – Mask scores of corresponding masks, has shape (num instances). •  filter_thr  ( float ) – Score threshold to filter the masks after matrix nms. Default: -1, which means do not use filter_thr. •  nms_pre  ( int ) – The max number of instances to do the matrix nms. Default: -1, which means do not use nms_pre. •  max_num  ( int, optional ) – If there are more than max_num masks after matrix, only top max_num will be kept. Default:  $^{-1}$  , which means do not use max_num. •  kernel  ( str ) – ‘linear’ or ‘gaussian’. •  sigma  ( float ) – std in gaussian method. •  mask_area  ( Tensor ) – The sum of seg_masks. ", "page_idx": 229, "bbox": [145, 352.3984680175781, 518, 545.0365600585938], "page_size": [612.0, 792.0]}
{"layout": 2061, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 229, "bbox": [117, 551, 154, 562], "page_size": [612.0, 792.0]}
{"layout": 2062, "type": "text", "text": "Processed mask results. ", "page_idx": 229, "bbox": [137, 567.5914916992188, 231.8107452392578, 580.9015502929688], "page_size": [612.0, 792.0]}
{"layout": 2063, "type": "text", "text": "• scores (Tensor): Updated scores, has shape (n,). • labels (Tensor): Remained labels, has shape (n,). • masks (Tensor): Remained masks, has shape (n, w, h). •  keep_inds (Tensor): The indices number of  the remaining mask in the input mask, has shape (n,). ", "page_idx": 229, "bbox": [145, 585.5244750976562, 518, 664.5875244140625], "page_size": [612.0, 792.0]}
{"layout": 2064, "type": "text", "text": "Return type  tuple(Tensor) ", "page_idx": 229, "bbox": [118, 668.5828247070312, 228.07455444335938, 683.1182250976562], "page_size": [612.0, 792.0]}
{"layout": 2065, "type": "text", "text": "mmdet.core.post processing. merge aug b boxes ( aug_bboxes ,  aug_scores ,  img_metas ,  r cnn test cf g ) Merge augmented detection bboxes and scores. ", "page_idx": 229, "bbox": [71, 686.9940795898438, 511.4413146972656, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 2066, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 230, "bbox": [117, 73, 170, 84], "page_size": [612.0, 792.0]}
{"layout": 2067, "type": "text", "text": "•  aug_bboxes  ( list[Tensor] ) – shape (n, 4\\*#class) •  aug_scores  ( list[Tensor] or None ) – shape (n, #class) •  img_shapes  ( list[Tensor] ) – shape (3, ). •  r cnn test cf g  ( dict ) – rcnn test config. ", "page_idx": 230, "bbox": [145, 89.38447570800781, 398.24591064453125, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 2068, "type": "text", "text": "Returns  (bboxes, scores) ", "page_idx": 230, "bbox": [118, 160.48779296875, 221, 175.02322387695312], "page_size": [612.0, 792.0]}
{"layout": 2069, "type": "text", "text": "Return type  tuple ", "page_idx": 230, "bbox": [118, 178.4207763671875, 194, 192.95620727539062], "page_size": [612.0, 792.0]}
{"layout": 2070, "type": "text", "text": "mmdet.core.post processing. merge aug masks ( aug_masks ,  img_metas ,  r cnn test cf g ,  weights  $\\mathbf{\\hat{\\rho}}$  None ) Merge augmented mask prediction. ", "page_idx": 230, "bbox": [71, 196.8319549560547, 518, 222.24644470214844], "page_size": [612.0, 792.0]}
{"layout": 2071, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 230, "bbox": [117, 228, 170, 239], "page_size": [612.0, 792.0]}
{"layout": 2072, "type": "text", "text": "•  aug_masks  ( list[ndarray] ) – shape (n, #class, h, w) •  img_shapes  ( list[ndarray] ) – shape (3, ). •  r cnn test cf g  ( dict ) – rcnn test config. ", "page_idx": 230, "bbox": [145, 244.80238342285156, 381, 293.9774475097656], "page_size": [612.0, 792.0]}
{"layout": 2073, "type": "text", "text": "Returns  (bboxes, scores) ", "page_idx": 230, "bbox": [118, 297.9727783203125, 221, 312.5082092285156], "page_size": [612.0, 792.0]}
{"layout": 2074, "type": "text", "text": "Return type  tuple ", "page_idx": 230, "bbox": [118, 315.9057922363281, 194, 330.44122314453125], "page_size": [612.0, 792.0]}
{"layout": 2075, "type": "text", "text": "mmdet.core.post processing. merge aug proposals ( aug proposals ,  img_metas ,  cfg ) Merge augmented proposals (multiscale, flip, etc.) ", "page_idx": 230, "bbox": [71, 334.3160095214844, 447.19232177734375, 359.73046875], "page_size": [612.0, 792.0]}
{"layout": 2076, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 230, "bbox": [117, 365, 170, 377], "page_size": [612.0, 792.0]}
{"layout": 2077, "type": "text", "text": "•  aug proposals  ( list[Tensor] ) – proposals from different testing schemes, shape (n, 5). Note that they are not rescaled to the original image size. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  cfg  ( dict ) – rpn test config. ", "page_idx": 230, "bbox": [145, 382.2864685058594, 518, 479.282470703125], "page_size": [612.0, 792.0]}
{"layout": 2078, "type": "text", "text": "Returns  shape (n, 4), proposals corresponding to original image scale. ", "page_idx": 230, "bbox": [118, 483.2778015136719, 402.8878479003906, 497.813232421875], "page_size": [612.0, 792.0]}
{"layout": 2079, "type": "text", "text": "Return type  Tensor ", "page_idx": 230, "bbox": [118, 501.2108154296875, 201.51425170898438, 515.7462158203125], "page_size": [612.0, 792.0]}
{"layout": 2080, "type": "text", "text": "mmdet.core.post processing. merge aug scores ( aug_scores ) Merge augmented bbox scores. ", "page_idx": 230, "bbox": [71, 519.6210327148438, 352.5023498535156, 545.0364990234375], "page_size": [612.0, 792.0]}
{"layout": 2081, "type": "text", "text": "mmdet.core.post processing. multi class nm s ( multi b boxes ,  multi scores ,  score_thr ,  nms_cfg ,  max_num=- 1 ,  score factors  $\\mathbf{\\hat{\\Sigma}}$  None ,  return in ds  $\\mathbf{:=}$  False ) NMS for multi-class bboxes. ", "page_idx": 230, "bbox": [71, 549.509033203125, 534.8238525390625, 586.8795166015625], "page_size": [612.0, 792.0]}
{"layout": 2082, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 230, "bbox": [117, 593, 169, 604], "page_size": [612.0, 792.0]}
{"layout": 2083, "type": "text", "text": "•  multi b boxes  ( Tensor ) – shape (n, #class\\*4) or (n, 4) •  multi scores  ( Tensor ) – shape (n, #class), where the last column contains scores of the background class, but this will be ignored. •  score_thr  ( float ) – bbox threshold, bboxes with scores lower than it will not be consid- ered. •  nms_thr  ( float ) – NMS IoU threshold ", "page_idx": 230, "bbox": [145, 609.4344482421875, 518, 700.4534912109375], "page_size": [612.0, 792.0]}
{"layout": 2084, "type": "text", "text": "•  max_num  ( int, optional ) – if there are more than max_num bboxes after NMS, only top max_num will be kept. Default to -1. •  score factors  ( Tensor, optional ) – The factors multiplied to scores before applying NMS. Default to None. •  return in ds  ( bool, optional ) – Whether return the indices of kept bboxes. Default to False. ", "page_idx": 231, "bbox": [145, 71.45246887207031, 518, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 2085, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 231, "bbox": [118, 162, 155, 174], "page_size": [612.0, 792.0]}
{"layout": 2086, "type": "text", "text": "(dets, labels, indices (optional)), tensors of shape   $({\\bf k},{\\bf5}),~({\\bf k})$  , and   $(\\mathrm{k})$  . Dets are boxes with scores. Labels are 0-based. ", "page_idx": 231, "bbox": [137, 178.42083740234375, 518, 204.3135223388672], "page_size": [612.0, 792.0]}
{"layout": 2087, "type": "text", "text": "Return type  tuple ", "page_idx": 231, "bbox": [118, 208.308837890625, 194.62017822265625, 222.84426879882812], "page_size": [612.0, 792.0]}
{"layout": 2088, "type": "text", "text": "37.7 utils ", "text_level": 1, "page_idx": 231, "bbox": [70, 245, 139, 261], "page_size": [612.0, 792.0]}
{"layout": 2089, "type": "text", "text": "class  mmdet.core.utils. Dist Optimizer Hook ( \\*args ,  \\*\\*kwargs ) Deprecated optimizer hook for distributed training. ", "page_idx": 231, "bbox": [71, 278.29302978515625, 358, 303.7074890136719], "page_size": [612.0, 792.0]}
{"layout": 2090, "type": "text", "text": "mmdet.core.utils. all reduce dic t ( py_dict ,  $\\scriptstyle o p='s u m`$  ,  group  $=$  None ,  to_float=True ) Apply all reduce function for python dict object. The code is modified from  https://github.com/Megvii - Base Detection/YOLOX/blob/main/yolox/utils/all reduce norm.py. NOTE: make sure that py_dict in different ranks has the same keys and the values should be in the same shape. ", "page_idx": 231, "bbox": [71, 308.1810302734375, 572.7705688476562, 369.46051025390625], "page_size": [612.0, 792.0]}
{"layout": 2091, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 231, "bbox": [117, 375, 169, 387], "page_size": [612.0, 792.0]}
{"layout": 2092, "type": "text", "text": "•  py_dict  ( dict ) – Dict to be applied all reduce op. •  op  ( str ) – Operator, could be ‘sum’ or ‘mean’. Default: ‘sum’ •  group  ( torch.distributed.group , optional) – Distributed group, Default: None. •  to_float  ( bool ) – Whether to convert all values of dict to float. Default: True. ", "page_idx": 231, "bbox": [145, 392.0165100097656, 492.850830078125, 459.12457275390625], "page_size": [612.0, 792.0]}
{"layout": 2093, "type": "text", "text": "", "page_idx": 231, "bbox": [117, 469.25, 266, 476], "page_size": [612.0, 792.0]}
{"layout": 2094, "type": "text", "text": "Return type  Ordered Dic t ", "page_idx": 231, "bbox": [118, 481.0519104003906, 224.488037109375, 495.58734130859375], "page_size": [612.0, 792.0]}
{"layout": 2095, "type": "text", "text": "mmdet.core.utils. all reduce grads ( params ,  coalesce  $\\mathbf{=}$  True ,  bucket size mb=- 1 ) Allreduce gradients. ", "page_idx": 231, "bbox": [71, 499.463134765625, 428.6822814941406, 524.8776245117188], "page_size": [612.0, 792.0]}
{"layout": 2096, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 231, "bbox": [117, 531, 169, 543], "page_size": [612.0, 792.0]}
{"layout": 2097, "type": "text", "text": "•  params  ( list[torch.Parameters] ) – List of parameters of a model •  coalesce  ( bool, optional ) – Whether allreduce parameters as a whole. Defaults to True. •  bucket size mb  ( int, optional ) – Size of bucket, the unit is MB. Defaults to -1. ", "page_idx": 231, "bbox": [145, 547.4335327148438, 518, 596.6085815429688], "page_size": [612.0, 792.0]}
{"layout": 2098, "type": "text", "text": "mmdet.core.utils. center of mass ( mask ,  $e s p{=}l e$  -06 ) Calculate the centroid coordinates of the mask. ", "page_idx": 231, "bbox": [71, 601.0820922851562, 313.2402648925781, 626.49658203125], "page_size": [612.0, 792.0]}
{"layout": 2099, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 231, "bbox": [117, 632, 169, 644], "page_size": [612.0, 792.0]}
{"layout": 2100, "type": "text", "text": "•  mask  ( Tensor ) – The mask to be calculated, shape (h, w). •  esp  ( float ) – Avoid dividing by zero. Default: 1e-6. ", "page_idx": 231, "bbox": [145, 649.0525512695312, 386.649658203125, 680.2955932617188], "page_size": [612.0, 792.0]}
{"layout": 2101, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 231, "bbox": [118, 687, 154, 697], "page_size": [612.0, 792.0]}
{"layout": 2102, "type": "text", "text": "the coordinates of the center point of the mask. ", "page_idx": 231, "bbox": [137, 702.8505249023438, 324.7716979980469, 716.1605834960938], "page_size": [612.0, 792.0]}
{"layout": 2103, "type": "text", "text": "• center_h (Tensor): the center point of the height. • center_w (Tensor): the center point of the width. ", "page_idx": 232, "bbox": [145, 71.45246887207031, 349, 102.69451141357422], "page_size": [612.0, 792.0]}
{"layout": 2104, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 232, "bbox": [118, 106.6898193359375, 228, 121.22525024414062], "page_size": [612.0, 792.0]}
{"layout": 2105, "type": "text", "text": "mmdet.core.utils. filter scores and top k ( scores ,  score_thr ,  topk ,  results  $\\mathbf{\\hat{\\rho}}$  None ) Filter results using score threshold and topk candidates. ", "page_idx": 232, "bbox": [71, 125.10100555419922, 435.1673278808594, 150.51548767089844], "page_size": [612.0, 792.0]}
{"layout": 2106, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 232, "bbox": [117, 157, 169, 168], "page_size": [612.0, 792.0]}
{"layout": 2107, "type": "text", "text": "•  scores  ( Tensor ) – The scores, shape (num_bboxes, K). •  score_thr  ( float ) – The score filter threshold. •  topk  ( int ) – The number of topk candidates. •  results  ( dict or list or Tensor, Optional ) – The results to which the filtering rule is to be applied. The shape of each item is (num_bboxes, N). ", "page_idx": 232, "bbox": [145, 173.07142639160156, 518, 252.1344451904297], "page_size": [612.0, 792.0]}
{"layout": 2108, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 232, "bbox": [117, 258, 154, 270], "page_size": [612.0, 792.0]}
{"layout": 2109, "type": "text", "text": "Filtered results • scores (Tensor): The scores after being filtered, shape (num b boxes filtered, ). • labels (Tensor): The class labels, shape (num b boxes filtered, ). • anchor i dx s (Tensor): The anchor indexes, shape (num b boxes filtered, ). • filtered results (dict or list or Tensor, Optional): The filtered results. The shape of each item is (num b boxes filtered, N). ", "page_idx": 232, "bbox": [137.45498657226562, 274.6904296875, 518, 371.6864929199219], "page_size": [612.0, 792.0]}
{"layout": 2110, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 232, "bbox": [117, 377, 197, 389], "page_size": [612.0, 792.0]}
{"layout": 2111, "type": "text", "text": "mmdet.core.utils. flip tensor ( src_tensor ,  flip direction ) flip tensor base on flip direction. ", "page_idx": 232, "bbox": [71, 394.092041015625, 328.9463195800781, 419.5065002441406], "page_size": [612.0, 792.0]}
{"layout": 2112, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 232, "bbox": [117, 425, 169, 437], "page_size": [612.0, 792.0]}
{"layout": 2113, "type": "text", "text": "•  src_tensor  ( Tensor ) – input feature map, shape (B, C, H, W). •  flip direction  ( str ) – The flipping direction. Options are ‘horizontal’, ‘vertical’, ‘diag- onal’. ", "page_idx": 232, "bbox": [145, 442.0625, 518, 485.2605285644531], "page_size": [612.0, 792.0]}
{"layout": 2114, "type": "text", "text": "Returns  Flipped tensor. ", "page_idx": 232, "bbox": [118, 489.255859375, 217.09539794921875, 503.7912902832031], "page_size": [612.0, 792.0]}
{"layout": 2115, "type": "text", "text": "Return type  out_tensor (Tensor) ", "page_idx": 232, "bbox": [118, 507.1878662109375, 255, 521.7232666015625], "page_size": [612.0, 792.0]}
{"layout": 2116, "type": "text", "text": "mmdet.core.utils. generate coordinate ( feat map sizes ,  device  $=$  'cuda' ) Generate the coordinate. ", "page_idx": 232, "bbox": [71, 525.5990600585938, 387.6852722167969, 551.0135498046875], "page_size": [612.0, 792.0]}
{"layout": 2117, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 232, "bbox": [117, 557, 169, 569], "page_size": [612.0, 792.0]}
{"layout": 2118, "type": "text", "text": "•  feat map sizes  ( tuple ) – The feature to be calculated, of shape (N, C, W, H). •  device  ( str ) – The device where the feature will be put on. Returns  The coordinate feature, of shape (N, 2, W, H). Return type  coord_feat (Tensor) ", "page_idx": 232, "bbox": [118, 573.5695190429688, 472.5416259765625, 641.2752685546875], "page_size": [612.0, 792.0]}
{"layout": 2119, "type": "text", "text": "mmdet.core.utils.mask 2 nd array(mask)Convert Mask to ndarray.. ", "page_idx": 232, "bbox": [71, 645.1510620117188, 255, 670.5655517578125], "page_size": [612.0, 792.0]}
{"layout": 2120, "type": "text", "text": ":param mask ( Bitmap Masks  or  Polygon Masks  or: :param torch.Tensor or np.ndarray): The mask to be con- verted. ", "page_idx": 232, "bbox": [96, 675.1884765625, 539.9967041015625, 700.4535522460938], "page_size": [612.0, 792.0]}
{"layout": 2121, "type": "text", "text": "Returns  Ndarray mask of shape (n, h, w) that has been converted ", "page_idx": 232, "bbox": [118, 704.4488525390625, 382, 718.9842529296875], "page_size": [612.0, 792.0]}
{"layout": 2122, "type": "text", "text": "Return type  np.ndarray ", "page_idx": 233, "bbox": [118, 70.8248291015625, 220, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 2123, "type": "text", "text": "mmdet.core.utils. multi apply ( func ,  \\*args ,  \\*\\*kwargs ) Apply function to a list of arguments. ", "page_idx": 233, "bbox": [71, 89.23503875732422, 316.8763732910156, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 2124, "type": "text", "text": "Note:  This function applies the  func  to multiple inputs and map the multiple outputs of the  func  into different list. Each list contains the same type of outputs corresponding to different inputs. ", "page_idx": 233, "bbox": [96, 130.59979248046875, 540, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 2125, "type": "text", "text": "Parameters  func  ( Function ) – A function that will be applied to a list of arguments ", "page_idx": 233, "bbox": [118, 178.42083740234375, 463.6199035644531, 192.95626831054688], "page_size": [612.0, 792.0]}
{"layout": 2126, "type": "text", "text": "Returns  A tuple containing multiple list, each list contains a kind of returned results by the function ", "page_idx": 233, "bbox": [118, 196.35382080078125, 518.0853881835938, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 2127, "type": "text", "text": "Return type  tuple(list) ", "page_idx": 233, "bbox": [118, 214.28680419921875, 213.29014587402344, 228.82223510742188], "page_size": [612.0, 792.0]}
{"layout": 2128, "type": "text", "text": "mmdet.core.utils. reduce mean ( tensor ) “Obtain the mean of tensor on different GPUs. ", "page_idx": 233, "bbox": [71, 238.6749725341797, 284, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 2129, "type": "text", "text": "mmdet.core.utils. select single ml vl ( ml vl tensors ,  batch_id ,  detach=True ) Extract a multi-scale single image tensor from a multi-scale batch tensor based on batch index. ", "page_idx": 233, "bbox": [71, 268.56298828125, 473.69256591796875, 293.9774475097656], "page_size": [612.0, 792.0]}
{"layout": 2130, "type": "text", "text": "Note: The default value of detach is True, because the proposal gradient needs to be detached during the training of the two-stage model. E.g Cascade Mask R-CNN. ", "page_idx": 233, "bbox": [96, 298.6004333496094, 540, 323.8654479980469], "page_size": [612.0, 792.0]}
{"layout": 2131, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 233, "bbox": [117, 330, 169, 342], "page_size": [612.0, 792.0]}
{"layout": 2132, "type": "text", "text": "•  ml vl tensors  ( list[Tensor] ) – Batch tensor for all scale levels, each is a 4D-tensor. ", "page_idx": 233, "bbox": [145, 346.42144775390625, 507.23175048828125, 359.7314758300781], "page_size": [612.0, 792.0]}
{"layout": 2133, "type": "text", "text": "•  batch_id  ( int ) – Batch index. •  detach  ( bool ) – Whether detach gradient. Default True. ", "page_idx": 233, "bbox": [145, 364.35345458984375, 383.0174255371094, 395.59649658203125], "page_size": [612.0, 792.0]}
{"layout": 2134, "type": "text", "text": "Returns  Multi-scale single image tensor. ", "page_idx": 233, "bbox": [118, 399.5918273925781, 284, 414.12725830078125], "page_size": [612.0, 792.0]}
{"layout": 2135, "type": "text", "text": "Return type  list[Tensor] ", "page_idx": 233, "bbox": [118, 417.52484130859375, 220, 432.0602722167969], "page_size": [612.0, 792.0]}
{"layout": 2136, "type": "text", "text": "mmdet.core.utils. unmap ( data ,  count ,  inds ,  fill=0 ) Unmap a subset of item (data) back to the original set of items (of size count) ", "page_idx": 233, "bbox": [71, 435.93505859375, 405.11993408203125, 461.3495178222656], "page_size": [612.0, 792.0]}
{"layout": 2137, "type": "text", "text": "MMDET.DATASETS ", "text_level": 1, "page_idx": 234, "bbox": [409, 164, 541, 181], "page_size": [612.0, 792.0]}
{"layout": 2138, "type": "text", "text": "38.1 datasets ", "text_level": 1, "page_idx": 234, "bbox": [71, 229, 167, 245], "page_size": [612.0, 792.0]}
{"layout": 2139, "type": "text", "text": "class  mmdet.datasets. Cityscape s Data set ( ann_file ,  pipeline ,  classes=None ,  data_root=None ,  img_prefix='' , seg_prefix=None ,  proposal file=None ,  test_mode=False , filter empty gt=True ) ", "page_idx": 234, "bbox": [72.0, 260.65301513671875, 538, 297.9128723144531], "page_size": [612.0, 792.0]}
{"layout": 2140, "type": "text", "text": "evaluate ( results ,  metric  $=$  'bbox' ,  logger  $\\leftrightharpoons$  None ,  out file prefix  $\\mathbf{\\beta}=$  None ,  classwise  $\\mathbf{=}$  False ,  proposal num s=(100, 300, 1000) ,  iou_thrs  $=$  array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]) ) Evaluation in Cityscapes/COCO protocol. ", "page_idx": 234, "bbox": [96, 314.45098876953125, 538, 351.8214416503906], "page_size": [612.0, 792.0]}
{"layout": 2141, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 234, "bbox": [137, 358, 188, 369], "page_size": [612.0, 792.0]}
{"layout": 2142, "type": "text", "text": "•  results  ( list[list | tuple] ) – Testing results of the dataset. ", "page_idx": 234, "bbox": [155, 374.3764343261719, 430, 387.68646240234375], "page_size": [612.0, 792.0]}
{"layout": 2143, "type": "text", "text": " metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro- posal’, ‘proposal fast’. ", "page_idx": 234, "bbox": [159, 392.3094482421875, 521, 417.574462890625], "page_size": [612.0, 792.0]}
{"layout": 2144, "type": "text", "text": "•  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. ", "page_idx": 234, "bbox": [155, 422.19744873046875, 521, 447.46246337890625], "page_size": [612.0, 792.0]}
{"layout": 2145, "type": "text", "text": " out file prefix  ( str | None ) – The prefix of output file. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If results are evaluated with COCO protocol, it would be the prefix of output json file. For example, the metric is ‘bbox’ and ‘segm’, then json files would be “a/b/prefix.bbox.json” and “a/b/prefix.segm.json”. If results are evaluated with cityscapes protocol, it would be the prefix of output txt/png files. The out- put files would be png images under folder “a/b/prefix/xxx/” and the file name of images would be written into a txt file “a/b/prefix/xxx_pred.txt”, where “xxx” is the video name of cityscapes. If not specified, a temp file will be created. Default: None. ", "page_idx": 234, "bbox": [159, 452.08544921875, 521, 549.0814208984375], "page_size": [612.0, 792.0]}
{"layout": 2146, "type": "text", "text": "•  classwise  ( bool ) – Whether to evaluating the AP for each class. ", "page_idx": 234, "bbox": [155, 553.704345703125, 430, 567.014404296875], "page_size": [612.0, 792.0]}
{"layout": 2147, "type": "text", "text": "•  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thrs  ( Sequence[float] ) – IoU threshold used for evaluating recalls. If set to a list, the average recall of all IoUs will also be computed. Default: 0.5. ", "page_idx": 234, "bbox": [155, 571.6363525390625, 521, 626.7904052734375], "page_size": [612.0, 792.0]}
{"layout": 2148, "type": "text", "text": "Returns  COCO style evaluation metric or cityscapes mAP and  AP  $@50$  . ", "page_idx": 234, "bbox": [137, 630.7847290039062, 430, 645.3201293945312], "page_size": [612.0, 792.0]}
{"layout": 2149, "type": "text", "text": "Return type  dict[str, float] ", "page_idx": 234, "bbox": [137, 648.7177124023438, 247.01341247558594, 663.2531127929688], "page_size": [612.0, 792.0]}
{"layout": 2150, "type": "text", "text": "format results ( results ,  txt file prefix  $\\mathbf{\\dot{\\rho}}=$  None ) Format the results to txt (standard format for Cityscapes evaluation). ", "page_idx": 234, "bbox": [96, 667.1289672851562, 390, 692.5433959960938], "page_size": [612.0, 792.0]}
{"layout": 2151, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 234, "bbox": [136, 698, 187, 710], "page_size": [612.0, 792.0]}
{"layout": 2152, "type": "text", "text": "•  txt file prefix  ( str | None ) – The prefix of txt files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. Default: None. ", "page_idx": 235, "bbox": [153, 89.38447570800781, 521, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 2153, "type": "text", "text": "Returns  (result files, tmp_dir), result files is a dict containing the json filepaths, tmp_dir is the temporal directory created for saving txt/png files when txt file prefix is not specified. ", "page_idx": 235, "bbox": [137, 130.600830078125, 521, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 2154, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 235, "bbox": [137, 163, 214, 174], "page_size": [612.0, 792.0]}
{"layout": 2155, "type": "text", "text": "results 2 txt ( results ,  out file prefix ) Dump the detection results to a txt file. ", "page_idx": 235, "bbox": [96, 178.89903259277344, 273.792236328125, 204.3135223388672], "page_size": [612.0, 792.0]}
{"layout": 2156, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 235, "bbox": [136, 211, 187, 222], "page_size": [612.0, 792.0]}
{"layout": 2157, "type": "text", "text": "•  results  ( list[list | tuple] ) – Testing results of the dataset. ", "page_idx": 235, "bbox": [153, 226.8694610595703, 432, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 2158, "type": "text", "text": " out file prefix  ( str ) – The filename prefix of the json files. If the prefix is “somepath/xxx”, the txt files will be named “somepath/xxx.txt”. ", "page_idx": 235, "bbox": [159.37184143066406, 244.8024444580078, 521, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 2159, "type": "text", "text": "Returns  Result txt files which contains corresponding instance segmentation images. ", "page_idx": 235, "bbox": [137, 274.0628356933594, 482, 288.5982666015625], "page_size": [612.0, 792.0]}
{"layout": 2160, "type": "text", "text": "Return type  list[str] ", "page_idx": 235, "bbox": [137, 291.9948425292969, 221.80804443359375, 306.5302734375], "page_size": [612.0, 792.0]}
{"layout": 2161, "type": "text", "text": "class  mmdet.datasets. Class Balanced Data set ( dataset ,  over sample thr ,  filter empty g  $\\leftleftarrows$  True ) A wrapper of repeated dataset with repeat factor. ", "page_idx": 235, "bbox": [71, 310.40606689453125, 482, 335.8205261230469], "page_size": [612.0, 792.0]}
{"layout": 2162, "type": "text", "text": "Suitable for training on class imbalanced datasets like LVIS. Following the sampling strategy in the  paper , in each epoch, an image may appear multiple times based on its “repeat factor”. The repeat factor for an image is a function of the frequency the rarest category labeled in that image. The “frequency of category   $\\mathbf{c}^{\\ast}$   in [0, 1] is defined by the fraction of images in the training set (without repeats) in which category c appears. The dataset needs to instantiate  self.get cat ids()  to support Class Balanced Data set. ", "page_idx": 235, "bbox": [96, 340.4435119628906, 540, 401.5744934082031], "page_size": [612.0, 792.0]}
{"layout": 2163, "type": "text", "text": "The repeat factor is computed as followed. ", "page_idx": 235, "bbox": [96, 406.19647216796875, 265.7630310058594, 419.5065002441406], "page_size": [612.0, 792.0]}
{"layout": 2164, "type": "text", "text": "1. For each category c, compute the fraction # of images that contain it:    $f(c)$  ", "page_idx": 235, "bbox": [106, 424.1294860839844, 414, 437.43951416015625], "page_size": [612.0, 792.0]}
{"layout": 2165, "type": "text", "text": "2. For each category c, compute the category-level repeat factor:    $r(c)=m a x(1,s q r t(t/f(c)))$  ", "page_idx": 235, "bbox": [106, 442.0625, 487, 456], "page_size": [612.0, 792.0]}
{"layout": 2166, "type": "text", "text": "3. For each image I, compute the image-level repeat factor:    $r(I)=m a x_{c i n I}r(c)$  ", "page_idx": 235, "bbox": [106, 459.9955139160156, 432, 473.3055419921875], "page_size": [612.0, 792.0]}
{"layout": 2167, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 235, "bbox": [117, 485, 169, 497], "page_size": [612.0, 792.0]}
{"layout": 2168, "type": "text", "text": "•  dataset  ( Custom Data set ) – The dataset to be repeated. ", "page_idx": 235, "bbox": [143, 501.8385314941406, 384.9452209472656, 515.1485595703125], "page_size": [612.0, 792.0]}
{"layout": 2169, "type": "text", "text": "•  over sample thr  ( float ) – frequency threshold below which data is repeated. For cat- egories with    $\\tt f\\_c\\;>=\\;$   over sample thr , there is no oversampling. For categories with  $\\tt f\\_c\\ <\\$   over sample thr , the degree of oversampling following the square-root inverse fre- quency heuristic above. ", "page_idx": 235, "bbox": [143, 519.7705078125, 518, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 2170, "type": "text", "text": "•  filter empty gt  ( bool, optional ) – If set true, images without bounding boxes will not be over sampled. Otherwise, they will be categorized as the pure background class and involved into the oversampling. Default: True. ", "page_idx": 235, "bbox": [143, 573.5695190429688, 518, 610.78955078125], "page_size": [612.0, 792.0]}
{"layout": 2171, "type": "text", "text": "class  mmdet.datasets. Coco Data set ( ann_file ,  pipeline ,  classes  $\\leftrightharpoons$  None ,  data_root  $=$  None ,  img_prefix  $\\mathrel{\\mathop:}=^{\\prime\\prime}$  , seg_prefix  $\\mathbf{\\beta}=$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None ,  test_mode  $:=$  False , filter empty g  $t{=}$  True ) ", "page_idx": 235, "bbox": [71, 621.2401123046875, 506.6594543457031, 658.5009155273438], "page_size": [612.0, 792.0]}
{"layout": 2172, "type": "text", "text": "evaluate ( results ,  metric  $\\mathbf{=}$  'bbox' ,  logge  $\\leftrightharpoons$  None ,  json file prefix  $\\mathbf{\\dot{\\rho}}=$  None ,  classwise  $=$  False , proposal num s  $=$  (100, 300, 1000) ,  iou_thrs  $\\mathbf{\\check{\\Sigma}}$  None ,  metric items=None ) Evaluation in COCO protocol. ", "page_idx": 235, "bbox": [96, 675.0391235351562, 450.51043701171875, 712.4085693359375], "page_size": [612.0, 792.0]}
{"layout": 2173, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 236, "bbox": [136, 73, 188, 84], "page_size": [612.0, 792.0]}
{"layout": 2174, "type": "text", "text": "•  results  ( list[list | tuple] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro- posal’, ‘proposal fast’. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. •  json file prefix  ( str | None ) – The prefix of json files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De- fault: None. •  classwise  ( bool ) – Whether to evaluating the AP for each class. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thrs  ( Sequence[float], optional ) – IoU threshold used for evaluating re- calls/mAPs. If set to a list, the average of all IoUs will also be computed. If not specified, [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used. Default: None. •  metric items  ( list[str] | str, optional ) – Metric items that will be returned. If not specified,  [ ' AR@100 ' ,  ' AR@300 ' ,  ' AR@1000 ' ,  ' AR_s@1000 ' ,  ' AR_m@1000 ' , ' AR_l@1000 '  ]  will be used when  metric  $==$  ' proposal ' ,  [ ' mAP ' ,    $\\,\\cdot\\,\\mathtt{m A P}\\_50\\,^{\\bullet}$  , ' mAP_75 ' ,  ' mAP_s ' ,  ' mAP_m ' ,  ' mAP_l ' ]  will be used when  metric  $==$  ' bbox '  or metric  $==$  ' segm ' . ", "page_idx": 236, "bbox": [154, 89.38447570800781, 521, 359.7314758300781], "page_size": [612.0, 792.0]}
{"layout": 2175, "type": "text", "text": "Returns  COCO style evaluation metric. ", "page_idx": 236, "bbox": [137, 363.7257995605469, 298, 378.26123046875], "page_size": [612.0, 792.0]}
{"layout": 2176, "type": "text", "text": "Return type  dict[str, float] ", "page_idx": 236, "bbox": [137, 381.6588134765625, 247, 396.1942443847656], "page_size": [612.0, 792.0]}
{"layout": 2177, "type": "text", "text": "format results ( results ,  json file prefix  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Format the results to json (standard format for COCO evaluation). ", "page_idx": 236, "bbox": [96, 400.0700378417969, 381.359375, 425.4844970703125], "page_size": [612.0, 792.0]}
{"layout": 2178, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 236, "bbox": [136, 431, 188, 443], "page_size": [612.0, 792.0]}
{"layout": 2179, "type": "text", "text": "•  results  ( list[tuple | numpy.ndarray] ) – Testing results of the dataset. ", "page_idx": 236, "bbox": [154, 448.03948974609375, 478.0215148925781, 461.3495178222656], "page_size": [612.0, 792.0]}
{"layout": 2180, "type": "text", "text": "•  json file prefix  ( str | None ) – The prefix of json files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De- fault: None. ", "page_idx": 236, "bbox": [154, 465.9725036621094, 521, 503.1935119628906], "page_size": [612.0, 792.0]}
{"layout": 2181, "type": "text", "text": "Returns  (result files, tmp_dir), result files is a dict containing the json filepaths, tmp_dir is the temporal directory created for saving json files when json file prefix is not specified. ", "page_idx": 236, "bbox": [137, 507.1878662109375, 521, 533.0805053710938], "page_size": [612.0, 792.0]}
{"layout": 2182, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 236, "bbox": [137, 539, 213, 551], "page_size": [612.0, 792.0]}
{"layout": 2183, "type": "text", "text": "get ann info ( idx ) Get COCO annotation by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict ", "page_idx": 236, "bbox": [96, 555.4869995117188, 317, 635.2972412109375], "page_size": [612.0, 792.0]}
{"layout": 2184, "type": "text", "text": "get cat ids ( idx ) Get COCO category ids by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] ", "page_idx": 236, "bbox": [96, 639.1730346679688, 359.222900390625, 718.9841918945312], "page_size": [612.0, 792.0]}
{"layout": 2185, "type": "text", "text": "load annotations ( ann_file ) Load annotation from COCO style annotation file. ", "page_idx": 237, "bbox": [96, 71.30303192138672, 318.8031921386719, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2186, "type": "text", "text": "Parameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from COCO api. Return type  list[dict] ", "page_idx": 237, "bbox": [137, 100.71282958984375, 360, 151.11325073242188], "page_size": [612.0, 792.0]}
{"layout": 2187, "type": "text", "text": "results 2 json ( results ,  out file prefix ) Dump the detection results to a COCO style json file. There are 3 types of results: proposals, bbox predictions, mask predictions, and they have different data types. This method will automatically recognize the type, and dump them to json files. ", "page_idx": 237, "bbox": [96, 154.98899841308594, 540.0028686523438, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 2188, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 237, "bbox": [136, 216, 188, 228], "page_size": [612.0, 792.0]}
{"layout": 2189, "type": "text", "text": "•  results  ( list[list | tuple | ndarray] ) – Testing results of the dataset. ", "page_idx": 237, "bbox": [154, 232.84645080566406, 483.25152587890625, 246.15647888183594], "page_size": [612.0, 792.0]}
{"layout": 2190, "type": "text", "text": "•  out file prefix  ( str ) – The filename prefix of the json files. If the pre- fix is “somepath/xxx”, the json files will be named “somepath/xxx.bbox.json”, “somepath/xxx.segm.json”, “somepath/xxx.proposal.json”. Returns  str]: Possible keys are “bbox”, “segm”, “proposal”, and values are corresponding file- names. ", "page_idx": 237, "bbox": [137, 250.77943420410156, 521, 317.8874816894531], "page_size": [612.0, 792.0]}
{"layout": 2191, "type": "text", "text": "xyxy2xywh ( bbox ) Convert  xyxy  style bounding boxes to  xywh  style for COCO evaluation. Parameters  bbox  ( numpy.ndarray ) – The bounding boxes, shape (4, ), in  xyxy  order. Returns  The converted bounding boxes, in  xywh  order. Return type  list[float] ", "page_idx": 237, "bbox": [96, 340.2940368652344, 490, 420.1042785644531], "page_size": [612.0, 792.0]}
{"layout": 2192, "type": "text", "text": "class  mmdet.datasets. Coco Pan optic Data set ( ann_file ,  pipeline ,  classes  $:=$  None ,  data_root  $\\leftrightharpoons$  None , img_pref  $\\scriptstyle{x={'}{'}}$  ,  seg_prefix  $\\mathbf{\\beta}=$  None ,  proposal file  $\\mathbf{\\hat{\\rho}}=$  None , test_mode  $=$  False ,  filter empty gt=True ) ", "page_idx": 237, "bbox": [71.99992370605469, 423.9800720214844, 500.19354248046875, 461.3495178222656], "page_size": [612.0, 792.0]}
{"layout": 2193, "type": "text", "text": "Coco dataset for Panoptic segmentation. ", "page_idx": 237, "bbox": [96, 459.9954833984375, 257.0756530761719, 473.3055114746094], "page_size": [612.0, 792.0]}
{"layout": 2194, "type": "text", "text": "The annotation format is shown as follows. The  ann  field is optional for testing. ", "page_idx": 237, "bbox": [96, 477.7780456542969, 413.5987548828125, 491.2375183105469], "page_size": [612.0, 792.0]}
{"layout": 2195, "type": "image", "page_idx": 237, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_132.jpg", "bbox": [93, 496, 543, 708], "page_size": [612.0, 792.0], "ocr_text": "\"filename': f'{image_id:012}.png',\n‘image_id':9\n\"segments_info': {\n\n[\n\n{\n\n\"id': 8345037, (segment_id in panoptic png,\nconvert from rgb)\n\n\"category_id': 51,\n\"iscrowd': Q,\n\"bbox': (x1, yl, w, h),\n\"area': 24315,\n\"segmentation': list, (coded mask)\n\n},\n\n", "vlm_text": "The image shows a JSON-like data structure typically used in image segmentation or annotation tasks. It contains:\n\n- `filename`: The name of the image file, formatted with an `image_id`.\n- `image_id`: An identifier for the image, here set to `9`.\n- `segments_info`: A dictionary that includes details about image segments:\n  - `id`: A unique identifier for a segment.\n  - `category_id`: An identifier for the category of the segment.\n  - `iscrowd`: A binary flag indicating if the segment is a crowd (0 means no).\n  - `bbox`: The bounding box coordinates and dimensions: `(x1, y1, w, h)`.\n  - `area`: The area covered by the segment.\n  - `segmentation`: A list representing a coded mask for the segment."}
{"layout": 2196, "type": "image", "page_idx": 238, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_133.jpg", "bbox": [93, 81, 544, 139], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image appears to be a snippet of code or data structure that is continuing from a previous page. It contains what seems to be closing brackets and commas, likely indicating the end of a list, array, or object in a programming or data context. The brackets include closing curly braces (`}`) and a closing square bracket (`]`). There are ellipses (`...`) implying the truncation or continuation of content not fully visible in this image."}
{"layout": 2197, "type": "text", "text": "evaluate ( results ,  metric  $\\mathbf{\\Sigma=}^{\\prime}P\\mathcal{Q}^{\\prime}$  ,  logger  $\\leftrightharpoons$  None ,  json file prefix  $\\mathbf{\\beta}=$  None ,  classwise  $\\mathbf{\\dot{\\rho}}=$  False ,  \\*\\*kwargs ) Evaluation in COCO Panoptic protocol. ", "page_idx": 238, "bbox": [96, 145.9750213623047, 490.79437255859375, 171.38951110839844], "page_size": [612.0, 792.0]}
{"layout": 2198, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 238, "bbox": [136, 177, 187, 189], "page_size": [612.0, 792.0]}
{"layout": 2199, "type": "text", "text": "•  results  ( list[dict] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Only support ‘PQ’ at present. ‘pq’ will be regarded as ‘PQ. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. •  json file prefix  ( str | None ) – The prefix of json files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De- fault: None. •  classwise  ( bool ) – Whether to print classwise evaluation results. Default: False. Returns  COCO Panoptic style evaluation metric. Return type  dict[str, float] ", "page_idx": 238, "bbox": [137, 193.94447326660156, 521, 363.269287109375], "page_size": [612.0, 792.0]}
{"layout": 2200, "type": "text", "text": "evaluate pan json ( result files ,  out file prefix ,  logger  $=$  None ,  classwise  $\\mathbf{\\beta}=$  False ) Evaluate PQ according to the panoptic results json file. ", "page_idx": 238, "bbox": [96, 367.14508056640625, 425.7932434082031, 392.5595397949219], "page_size": [612.0, 792.0]}
{"layout": 2201, "type": "text", "text": "get ann info ( idx ) Get COCO annotation by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict ", "page_idx": 238, "bbox": [96, 397.0330810546875, 317, 476.8443298339844], "page_size": [612.0, 792.0]}
{"layout": 2202, "type": "text", "text": "load annotations ( ann_file ) Load annotation from COCO Panoptic style annotation file. ", "page_idx": 238, "bbox": [96, 480.7191162109375, 358, 506.1335754394531], "page_size": [612.0, 792.0]}
{"layout": 2203, "type": "text", "text": "Parameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from COCO api. Return type  list[dict] ", "page_idx": 238, "bbox": [137, 510.12890625, 358, 560.5302734375], "page_size": [612.0, 792.0]}
{"layout": 2204, "type": "text", "text": "results 2 json ( results ,  out file prefix ) Dump the panoptic results to a COCO panoptic style json file. ", "page_idx": 238, "bbox": [96, 564.4050903320312, 365.29949951171875, 589.8206176757812], "page_size": [612.0, 792.0]}
{"layout": 2205, "type": "text", "text": "Parameters •  results  ( dict ) – Testing results of the dataset. •  out file prefix  ( str ) – The filename prefix of the json files. If the prefix is “somepath/xxx”, the json files will be named “somepath/xxx.panoptic.json” Returns str]: The key is ‘panoptic’ and the value is  corresponding filename. Return type  dict[str ", "page_idx": 238, "bbox": [137, 593.81494140625, 521, 709.9692993164062], "page_size": [612.0, 792.0]}
{"layout": 2206, "type": "text", "text": "class  mmdet.datasets. Con cat Data set ( datasets ,  separate eva  $\\leftrightharpoons$  True ) A wrapper of concatenated dataset. ", "page_idx": 239, "bbox": [71, 71.30303192138672, 377.8223876953125, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2207, "type": "text", "text": "Same as  torch.utils.data.dataset.Con cat Data set , but concat the group flag for image aspect ratio. ", "page_idx": 239, "bbox": [96, 101.34046936035156, 526.145751953125, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 2208, "type": "text", "text": "•  datasets  (list[ Dataset ]) – A list of datasets. •  separate e val  ( bool ) – Whether to evaluate the results separately if it is used as validation dataset. Defaults to True. ", "page_idx": 239, "bbox": [145, 137.20545959472656, 518, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 2209, "type": "text", "text": "evaluate ( results ,  logger  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) Evaluate the results. ", "page_idx": 239, "bbox": [96, 184.8769989013672, 276.86639404296875, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 2210, "type": "text", "text": "•  results  ( list[list | tuple] ) – Testing results of the dataset. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. Returns  float]: AP results of the total dataset or each separate dataset if  self.separate e val  $\\leftrightharpoons$  True . ", "page_idx": 239, "bbox": [137, 232.84645080566406, 521, 294.57525634765625], "page_size": [612.0, 792.0]}
{"layout": 2211, "type": "text", "text": "get cat ids ( idx ) Get category ids of concatenated dataset by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. ", "page_idx": 239, "bbox": [96, 316.3840637207031, 359.22186279296875, 378.26129150390625], "page_size": [612.0, 792.0]}
{"layout": 2212, "type": "text", "text": "Return type  list[int] ", "page_idx": 239, "bbox": [137, 381.65887451171875, 222.51539611816406, 396.1943054199219], "page_size": [612.0, 792.0]}
{"layout": 2213, "type": "text", "text": "class  mmdet.datasets. Custom Data set ( ann_file ,  pipeline ,  classes  $\\leftrightharpoons$  None ,  data_root  $\\mathbf{\\dot{=}}$  None ,  img_prefix='' , seg_prefix  $\\mathbf{\\dot{\\rho}}$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None ,  test_mode  $\\mathbf{\\beta}=$  False , filter empty g  $t{=}$  True ) ", "page_idx": 239, "bbox": [71, 400.0700988769531, 518, 437.3299560546875], "page_size": [612.0, 792.0]}
{"layout": 2214, "type": "text", "text": "Custom dataset for detection. ", "page_idx": 239, "bbox": [96, 436.0845031738281, 213.130615234375, 449.39453125], "page_size": [612.0, 792.0]}
{"layout": 2215, "type": "text", "text": "The annotation format is shown as follows. The  ann  field is optional for testing. ", "page_idx": 239, "bbox": [96, 453.8680725097656, 413.5987548828125, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 2216, "type": "image", "page_idx": 239, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_134.jpg", "bbox": [93, 472, 543, 647], "page_size": [612.0, 792.0], "ocr_text": "\"filename': 'a.jpg',\n\"width': 1280,\n\"height': 720,\n‘ann': {\n\n\"bboxes': <np.ndarray> (n, 4)\n\"labels': <np.ndarray> (n, ),\n\"bboxes_ignore': <np.ndarray>\n\"labels_ignore': <np.ndarray>\n\nin (x1, yl, x2, y2) order.\n\n(k, 4), (optional field)\n(k, 4) (optional field)\n", "vlm_text": "The image shows a snippet of JSON-like data, probably used for image annotation. Here's a breakdown of the structure:\n\n- `filename`: 'a.jpg'\n- `width`: 1280\n- `height`: 720\n- `ann` (annotations):\n  - `bboxes`: `<np.ndarray>` (n, 4) representing bounding boxes in (x1, y1, x2, y2) order.\n  - `labels`: `<np.ndarray>` (n,) indicating labels.\n  - `bboxes_ignore`: `<np.ndarray>` (k, 4), an optional field for ignored bounding boxes.\n  - `labels_ignore`: `<np.ndarray>` (k, 4), an optional field for ignored labels.\n\nThis format suggests usage for image processing tasks like object detection."}
{"layout": 2217, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 239, "bbox": [117, 662, 169, 674], "page_size": [612.0, 792.0]}
{"layout": 2218, "type": "text", "text": "•  ann_file  ( str ) – Annotation file path. •  pipeline  ( list[dict] ) – Processing pipeline. ", "page_idx": 239, "bbox": [145, 678.9734497070312, 348.95587158203125, 710.2164916992188], "page_size": [612.0, 792.0]}
{"layout": 2219, "type": "text", "text": "•  classes  ( str | Sequence[str], optional ) – Specify classes to load. If is None,  cls. CLASSES  will be used. Default: None. •  data_root  ( str, optional ) – Data root for  ann_file ,  img_prefix ,  seg_prefix , proposal file  if specified. •  test_mode  ( bool, optional ) – If set True, annotation will not be loaded. •  filter empty gt  ( bool, optional ) – If set true, images without bounding boxes of the dataset’s classes will be filtered out. This option only works when  test_mode  $\\mathbf{=}$  False , i.e., we never filter images during tests. ", "page_idx": 240, "bbox": [143, 71.45246887207031, 518, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 2220, "type": "text", "text": "evaluate ( results ,  metri  $\\scriptstyle c=m A P^{\\prime}$  ,  logger  $\\leftrightharpoons$  None ,  proposal_nums=(100, 300, 1000) ,  iou_thr  $\\mathord{\\simeq}\\!O.5$  , scale range  $\\leftrightharpoons$  None ) Evaluate the dataset. ", "page_idx": 240, "bbox": [96, 196.83201599121094, 484.1446228027344, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 2221, "type": "text", "text": "", "text_level": 1, "page_idx": 240, "bbox": [136, 241, 188, 247.75], "page_size": [612.0, 792.0]}
{"layout": 2222, "type": "text", "text": "•  results  ( list ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. •  logger  ( logging.Logger | None | str ) – Logger used for printing related informa- tion during evaluation. Default: None. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thr  ( float | list[float] ) – IoU threshold. Default: 0.5. •  scale ranges  ( list[tuple] | None ) – Scale ranges for evaluating mAP. Default: None. ", "page_idx": 240, "bbox": [154, 256.7574768066406, 521, 395.5965270996094], "page_size": [612.0, 792.0]}
{"layout": 2223, "type": "text", "text": "format results ( results ,  \\*\\*kwargs ) Place holder to format result to dataset specific output. ", "page_idx": 240, "bbox": [96, 400.070068359375, 335.8403015136719, 425.4845275878906], "page_size": [612.0, 792.0]}
{"layout": 2224, "type": "text", "text": "get ann info ( idx ) Get annotation by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict ", "page_idx": 240, "bbox": [96, 429.95806884765625, 317, 509.768310546875], "page_size": [612.0, 792.0]}
{"layout": 2225, "type": "text", "text": "get cat ids ( idx ) Get category ids by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] ", "page_idx": 240, "bbox": [96, 513.6441040039062, 359, 593.4542846679688], "page_size": [612.0, 792.0]}
{"layout": 2226, "type": "text", "text": "class method get classes (  $=$  ) Get class names of current dataset. Parameters  classes  ( Sequence[str] | str | None ) – If classes is None, use default CLASSES defined by builtin dataset. If classes is a string, take it as a file name. The file contains the name of classes where each line contains one class name. If classes is a tuple or list, override the CLASSES defined by the dataset. Returns  Names of categories of the dataset. Return type  tuple[str] or list[str] ", "page_idx": 240, "bbox": [96, 599.2030639648438, 521, 713.0062866210938], "page_size": [612.0, 792.0]}
{"layout": 2227, "type": "text", "text": "load annotations ( ann_file ) Load annotation from annotation file. ", "page_idx": 241, "bbox": [96, 71.30303192138672, 267.9740295410156, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2228, "type": "text", "text": "load proposals ( proposal file ) Load proposal from proposal file. ", "page_idx": 241, "bbox": [96, 101.19103240966797, 252.4722137451172, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 2229, "type": "text", "text": "pre pipeline ( results ) Prepare results dict for pipeline. ", "page_idx": 241, "bbox": [96, 131.0790252685547, 245.97665405273438, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 2230, "type": "text", "text": "prepare test img ( idx ) Get testing data after pipeline. Parameters  idx  ( int ) – Index of data. Returns  Testing data after pipeline with new keys introduced by pipeline. Return type  dict prepare train img ( idx ) Get training data and annotations after pipeline. Parameters  idx  ( int ) – Index of data. Returns  Training data and annotation after pipeline with new keys introduced by pipeline. Return type  dict ", "page_idx": 241, "bbox": [96, 160.96604919433594, 499.4452209472656, 324.4632873535156], "page_size": [612.0, 792.0]}
{"layout": 2231, "type": "text", "text": "class  mmdet.datasets. Deep Fashion Data set ( ann_file ,  pipeline ,  classes  $\\leftrightharpoons$  None ,  data_root  $=$  None , img_pref  $\\acute{\\iota}={}^{\\prime\\prime}$  ,  seg_prefix  $\\mathbf{\\hat{\\rho}}$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None , test_mode  $\\mathbf{=}$  False ,  filter empty gt=True ) ", "page_idx": 241, "bbox": [71, 328.3390808105469, 494.9635925292969, 365.7085266113281], "page_size": [612.0, 792.0]}
{"layout": 2232, "type": "text", "text": "class  mmdet.datasets. Distributed Group Sampler ( dataset ,  samples per gpu  $\\mathsf{\\Pi}_{=I}$  ,  num replicas  $\\mathbf{=}$  None , rank  $\\leftrightharpoons$  None ,  seed  $\\mathrm{\\Lambda}{}=\\!\\!O_{.}$  ) ", "page_idx": 241, "bbox": [71, 370.18206787109375, 511.8895568847656, 395.5965270996094], "page_size": [612.0, 792.0]}
{"layout": 2233, "type": "text", "text": "Sampler that restricts data loading to a subset of the dataset. ", "page_idx": 241, "bbox": [96, 394.2414855957031, 335.7502746582031, 407.551513671875], "page_size": [612.0, 792.0]}
{"layout": 2234, "type": "text", "text": "It is especially useful in conjunction with  torch.nn.parallel.Distributed Data Parallel . In such case, each process can pass a Distributed Sampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it. ", "page_idx": 241, "bbox": [96, 412.17449951171875, 540, 449.3945007324219], "page_size": [612.0, 792.0]}
{"layout": 2235, "type": "text", "text": "Note:  Dataset is assumed to be of constant size. ", "page_idx": 241, "bbox": [96, 465.3448181152344, 291.2875061035156, 479.8802490234375], "page_size": [612.0, 792.0]}
{"layout": 2236, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 241, "bbox": [117, 503, 169, 515], "page_size": [612.0, 792.0]}
{"layout": 2237, "type": "text", "text": "•  dataset  – Dataset used for sampling. •  num replicas  ( optional ) – Number of processes participating in distributed training. •  rank  ( optional ) – Rank of the current process within num replicas. •  seed  ( int, optional ) – random seed used to shuffle the sampler if  shuffle  $=$  True . This number should be identical across all processes in the distributed group. Default: 0. ", "page_idx": 241, "bbox": [145, 519.7704467773438, 518.0859375, 598.8345336914062], "page_size": [612.0, 792.0]}
{"layout": 2238, "type": "text", "text": "class  mmdet.datasets. Distributed Sampler ( dataset ,  num replicas  $=$  None ,  rank  $\\mathbf{\\beta}=$  None ,  shuffle  $\\mathbf{\\Psi}=$  True , seed  $\\mathrm{\\Lambda}{}=\\!\\!O_{.}$  ) ", "page_idx": 241, "bbox": [71, 609.2850341796875, 506, 634.5899047851562], "page_size": [612.0, 792.0]}
{"layout": 2239, "type": "text", "text": "class  mmdet.datasets. Group Sampler ( dataset ,  samples per gpu  $\\scriptstyle{I=I}$  ) ", "page_idx": 241, "bbox": [71, 639.1730346679688, 370.6693115234375, 652.6325073242188], "page_size": [612.0, 792.0]}
{"layout": 2240, "type": "text", "text": "mmdet.datasets. LV IS Data set alias of  mmdet.datasets.lvis.LV IS V 05 Data set ", "page_idx": 241, "bbox": [71, 658.97900390625, 308, 682.5205078125], "page_size": [612.0, 792.0]}
{"layout": 2241, "type": "text", "text": "class  mmdet.datasets. LV IS V 05 Data set ( ann_file ,  pipeline ,  classes  $\\mathbf{\\check{\\Sigma}}$  None ,  data_root  $=$  None ,  img_prefix  $\\mathrel{=}^{\\prime}$  ' , seg_prefix  $\\mathbf{\\beta}=$  None ,  proposal file  $\\leftrightharpoons$  None ,  test_mode  $\\mathbf{\\Pi}^{*}=$  False , filter empty gt=True ) ", "page_idx": 242, "bbox": [71, 71.30303192138672, 522, 108.56295013427734], "page_size": [612.0, 792.0]}
{"layout": 2242, "type": "text", "text": "evaluate ( results ,  metric  $=$  'bbox' ,  logger  $\\mathbf{\\hat{\\Sigma}}$  None ,  json file prefix  $\\mathbf{\\hat{\\rho}}$  None ,  classwise  $\\mathbf{\\dot{\\rho}}=$  False , proposal num s  $=$  (100, 300, 1000) ,  iou_thrs  $\\mathbf{\\tilde{=}}$  array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]) ) ", "page_idx": 242, "bbox": [96, 125.10106658935547, 526.4054565429688, 162.36097717285156], "page_size": [612.0, 792.0]}
{"layout": 2243, "type": "text", "text": "Evaluation in LVIS protocol. ", "page_idx": 242, "bbox": [118, 161.1155548095703, 233.6538543701172, 174.4255828857422], "page_size": [612.0, 792.0]}
{"layout": 2244, "type": "text", "text": "Parameters •  results  ( list[list | tuple] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro- posal’, ‘proposal fast’. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. •  json file prefix  ( str | None ) – •  classwise  ( bool ) – Whether to evaluating the AP for each class. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thrs  ( Sequence[float] ) – IoU threshold used for evaluating recalls. If set to a list, the average recall of all IoUs will also be computed. Default: 0.5. Returns  LVIS style metrics. Return type  dict[str, float] ( ann_file ) Load annotation from lvis style annotation file. ", "page_idx": 242, "bbox": [118, 178.4208984375, 522, 431.4626159667969], "page_size": [612.0, 792.0]}
{"layout": 2245, "type": "text", "text": "Parameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from LVIS api. Return type  list[dict] ", "page_idx": 242, "bbox": [137, 435.4569396972656, 358, 485.8583984375], "page_size": [612.0, 792.0]}
{"layout": 2246, "type": "text", "text": "class  mmdet.datasets. LV IS V 1 Data set ( ann_file ,  pipeline ,  classes  $\\mathbf{=}$  None ,  data_root  $\\leftrightharpoons$  None ,  img_prefix='' , seg_pref  $\\leftleftarrows$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None ,  test_mode  $\\mathbf{\\beta}=$  False , filter empty g  $t{=}$  True ) ", "page_idx": 242, "bbox": [71, 489.73419189453125, 517.1204833984375, 526.9940185546875], "page_size": [612.0, 792.0]}
{"layout": 2247, "type": "text", "text": "Load annotation from lvis style annotation file. Parameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from LVIS api. Return type  list[dict] ", "page_idx": 242, "bbox": [118, 555.6365966796875, 358, 623.3423461914062], "page_size": [612.0, 792.0]}
{"layout": 2248, "type": "text", "text": "class  mmdet.datasets. Multi Image Mix Data set ( dataset ,  pipeline ,  dynamic scale=None , ", "page_idx": 242, "bbox": [71, 627.2181396484375, 451, 640.6776123046875], "page_size": [612.0, 792.0]}
{"layout": 2249, "type": "text", "text": "A wrapper of multiple images mixed dataset. ", "page_idx": 242, "bbox": [96, 651.277587890625, 275.9347229003906, 664.587646484375], "page_size": [612.0, 792.0]}
{"layout": 2250, "type": "text", "text": "Suitable for training on multiple images mixed data augmentation like mosaic and mixup. For the augmentation pipeline of mixed image data, the  get indexes  method needs to be provided to obtain the image indexes, and you can set  skip_flags  to change the pipeline running process. At the same time, we provide the  dynamic scale parameter to dynamically change the output image size. ", "page_idx": 242, "bbox": [96, 669.2105712890625, 540, 718.3866577148438], "page_size": [612.0, 792.0]}
{"layout": 2251, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 243, "bbox": [117, 72, 169, 85], "page_size": [612.0, 792.0]}
{"layout": 2252, "type": "text", "text": "•  dataset  ( Custom Data set ) – The dataset to be mixed. •  pipeline  ( Sequence[dict] ) – Sequence of transform object or config dict to be com- posed. •  dynamic scale  ( tuple[int], optional ) – The image scale can be changed dynami- cally. Default to None. •  skip type keys  ( list[str], optional ) – Sequence of type string to be skip pipeline. Default to None. ", "page_idx": 243, "bbox": [143, 89.38447570800781, 518, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 2253, "type": "text", "text": "", "page_idx": 243, "bbox": [95, 198, 273, 204.75], "page_size": [612.0, 792.0]}
{"layout": 2254, "type": "text", "text": "Update dynamic scale. It is called by an external hook. Parameters  dynamic scale  ( tuple[int] ) – The image scale can be changed dynamically. ", "page_idx": 243, "bbox": [118, 208.9364776611328, 512.9258422851562, 240.77725219726562], "page_size": [612.0, 792.0]}
{"layout": 2255, "type": "text", "text": "update skip type keys(skip type keys)Update skip type keys. It is called by an external hook. Parameters  skip type keys  ( list[str], optional ) – Sequence of type string to be skip pipeline. ", "page_idx": 243, "bbox": [96, 244.6529998779297, 521, 299.95550537109375], "page_size": [612.0, 792.0]}
{"layout": 2256, "type": "text", "text": "class  mmdet.datasets. Repeat Data set ( dataset ,  times ) A wrapper of repeated dataset. ", "page_idx": 243, "bbox": [71, 304.42803955078125, 314.9933166503906, 329.843505859375], "page_size": [612.0, 792.0]}
{"layout": 2257, "type": "text", "text": "The length of repeated dataset will be  times  larger than the original dataset. This is useful when the data loading time is long but the dataset is small. Using Repeat Data set can reduce the data loading time between epochs. ", "page_idx": 243, "bbox": [96, 334.3160400390625, 539.9945068359375, 359.73150634765625], "page_size": [612.0, 792.0]}
{"layout": 2258, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 243, "bbox": [117, 365, 170, 377], "page_size": [612.0, 792.0]}
{"layout": 2259, "type": "text", "text": "•  dataset  ( Dataset ) – The dataset to be repeated. •  times  ( int ) – Repeat times. ", "page_idx": 243, "bbox": [143, 382.2864990234375, 353.2142639160156, 413.529541015625], "page_size": [612.0, 792.0]}
{"layout": 2260, "type": "text", "text": "get cat ids ( idx ) Get category ids of repeat dataset by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] ", "page_idx": 243, "bbox": [96, 418.0030822753906, 359.22186279296875, 497.8133239746094], "page_size": [612.0, 792.0]}
{"layout": 2261, "type": "text", "text": "class  mmdet.datasets. VOCDataset ( \\*\\*kwargs ) ", "page_idx": 243, "bbox": [71, 501.68914794921875, 282.9533386230469, 515.0390014648438], "page_size": [612.0, 792.0]}
{"layout": 2262, "type": "text", "text": "evaluate ( results ,  metric  $\\scriptstyle=m A P^{\\prime}$  ,  logger  $\\leftrightharpoons$  None ,  proposal_nums=(100, 300, 1000) ,  iou_thr=0.5 , scale ranges  $=$  None ) Evaluate in VOC protocol. ", "page_idx": 243, "bbox": [96, 531.5770874023438, 484.1446228027344, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 2263, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 243, "bbox": [135, 574, 188, 586], "page_size": [612.0, 792.0]}
{"layout": 2264, "type": "text", "text": "•  results  ( list[list | tuple] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘mAP’, ‘recall’. •  logger  ( logging.Logger | str, optional ) – Logger used for printing related infor- mation during evaluation. Default: None. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thr  ( float | list[float] ) – IoU threshold. Default: 0.5. ", "page_idx": 243, "bbox": [154, 591.5025024414062, 521, 700.45361328125], "page_size": [612.0, 792.0]}
{"layout": 2265, "type": "text", "text": "•  scale ranges  ( list[tuple], optional ) – Scale ranges for evaluating mAP. If not specified, all bounding boxes would be included in evaluation. Default: None. Returns  AP/recall metrics. Return type  dict[str, float] ", "page_idx": 244, "bbox": [137, 71.45246887207031, 522, 133.18026733398438], "page_size": [612.0, 792.0]}
{"layout": 2266, "type": "text", "text": "class  mmdet.datasets. WIDER Face Data set ( \\*\\*kwargs ) Reader for the WIDER Face dataset in PASCAL VOC format. ", "page_idx": 244, "bbox": [71, 137.05601501464844, 343.4913330078125, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 2267, "type": "text", "text": "Conversion scripts can be found in  https://github.com/sovrasov/wider-face-pascal-voc-annotations ", "page_idx": 244, "bbox": [96, 167.0934600830078, 490.05096435546875, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 2268, "type": "text", "text": "load annotations ( ann_file ) Load annotation from WIDERFace XML style annotation file. Parameters  ann_file  ( str ) – Path of XML file. Returns  Annotation info from XML file. Return type  list[dict] ", "page_idx": 244, "bbox": [96, 184.8769989013672, 367.06207275390625, 264.6872253417969], "page_size": [612.0, 792.0]}
{"layout": 2269, "type": "text", "text": "class  mmdet.datasets. XMLDataset ( min_size  $=$  None ,  img_subdir  $\\bf{=}$  JPEGImages' ,  ann_subdir  $\\mathbf{\\dot{\\rho}}$  'Annotations' , \\*\\*kwargs ) XML dataset for detection. ", "page_idx": 244, "bbox": [71, 268.56298828125, 527.0825805664062, 305.93243408203125], "page_size": [612.0, 792.0]}
{"layout": 2270, "type": "text", "text": "•  min_size  ( int | float, optional ) – The minimum size of bounding boxes in the im- ages. If the size of a bounding box is less than  min_size , it would be add to ignored field. •  img_subdir  ( str ) – Subdir where images are stored. Default: JPEGImages. •  ann_subdir    $(s t r)-S$  ubdir where annotations are. Default: Annotations. ", "page_idx": 244, "bbox": [145, 328.4884338378906, 518.0816650390625, 389.61846923828125], "page_size": [612.0, 792.0]}
{"layout": 2271, "type": "text", "text": "get ann info ( idx ) Get annotation from XML file by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict get cat ids ( idx ) Get category ids in XML file by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] load annotations ( ann_file ) Load annotation from XML style ann_file. Parameters  ann_file  ( str ) – Path of XML file. Returns  Annotation info from XML file. Return type  list[dict] ", "page_idx": 244, "bbox": [96, 394.0920104980469, 359.2218017578125, 641.2752685546875], "page_size": [612.0, 792.0]}
{"layout": 2272, "type": "text", "text": "mmdet.datasets. build data loader ( dataset ,  samples per gpu ,  workers per gpu ,  num_gpus  $\\mathord{:=}I$  ,  dist  $\\mathbf{\\dot{\\rho}}$  True , shuffle  $\\mathbf{=}$  True ,  seed  $\\leftrightharpoons$  None ,  runner type  $\\mathbf{\\tilde{=}}$  'Epoch Based Runner' ,  \\*\\*kwargs ) Build PyTorch DataLoader. ", "page_idx": 244, "bbox": [71, 645.1510620117188, 532.378173828125, 682.5205078125], "page_size": [612.0, 792.0]}
{"layout": 2273, "type": "text", "text": "In distributed training, each GPU/process has a dataloader. In non-distributed training, there is only one dat- aloader for all GPUs. ", "page_idx": 244, "bbox": [96, 687.1434936523438, 540.0032348632812, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 2274, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 245, "bbox": [118, 72, 169, 84], "page_size": [612.0, 792.0]}
{"layout": 2275, "type": "text", "text": "•  dataset  ( Dataset ) – A PyTorch dataset. •  samples per gpu  ( int ) – Number of training samples on each GPU, i.e., batch size of each GPU. •  workers per gpu  ( int ) – How many sub processes to use for data loading for each GPU. •  num_gpus  ( int ) – Number of GPUs. Only used in non-distributed training. •  dist  ( bool ) – Distributed training/test or not. Default: True. •  shuffle  ( bool ) – Whether to shuffle the data at every epoch. Default: True. •  runner type  ( str ) – Type of runner. Default:  Epoch Based Runner •  kwargs  – any keyword argument to be used to initialize DataLoader ", "page_idx": 245, "bbox": [145, 89.38447570800781, 518, 240.17942810058594], "page_size": [612.0, 792.0]}
{"layout": 2276, "type": "text", "text": "Returns  A PyTorch dataloader. Return type  DataLoader ", "page_idx": 245, "bbox": [118, 244.17474365234375, 246.744140625, 276.6421813964844], "page_size": [612.0, 792.0]}
{"layout": 2277, "type": "text", "text": "mmdet.datasets. get loading pipeline ( pipeline ) Only keep loading image and annotations related configuration. ", "page_idx": 245, "bbox": [71, 280.5179748535156, 349.6680908203125, 305.93243408203125], "page_size": [612.0, 792.0]}
{"layout": 2278, "type": "text", "text": "Parameters  pipeline  ( list[dict] ) – Data pipeline configs. ", "page_idx": 245, "bbox": [118, 309.9277648925781, 374.220947265625, 324.46319580078125], "page_size": [612.0, 792.0]}
{"layout": 2279, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 245, "bbox": [118, 330, 154, 341], "page_size": [612.0, 792.0]}
{"layout": 2280, "type": "text", "text": "The new pipeline list with only keep  loading image and annotations related configuration. ", "page_idx": 245, "bbox": [137, 345.79278564453125, 503.3114929199219, 360.3282165527344], "page_size": [612.0, 792.0]}
{"layout": 2281, "type": "text", "text": "Return type  list[dict] ", "page_idx": 245, "bbox": [118, 363.7257995605469, 208.30886840820312, 378.26123046875], "page_size": [612.0, 792.0]}
{"layout": 2282, "type": "text", "text": "Examples ", "text_level": 1, "page_idx": 245, "bbox": [96, 398, 144, 410], "page_size": [612.0, 792.0]}
{"layout": 2283, "type": "image", "page_idx": 245, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_135.jpg", "bbox": [92, 421, 543, 606], "page_size": [612.0, 792.0], "ocr_text": ">>> pipelines = [\ndict (type='LoadImageFromFile'),\ndict (type='LoadAnnotations', with_bbox=True) ,\ndict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\ndict(type='RandomFlip', flip_ratio=0.5),\ndict(type='Normalize', **img_norm_cfg) ,\ndict(type='Pad', size_divisor=32),\ndict (type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\naime ]\n>>> expected_pipelines = [\ndict (type='LoadImageFromFile'),\ndict (type='LoadAnnotations', with_bbox=True)\n]\n\n>>> assert expected_pipelines == wee get_loading_pipeline(pipelines)\n", "vlm_text": "This image is a screenshot of a Python code snippet. The code is setting up two lists called `pipelines` and `expected_pipelines`. Both lists contain dictionaries that specify different processing steps, with a `type` key indicating the kind of operation, such as 'LoadImageFromFile', 'LoadAnnotations', 'Resize', and more.\n\n- The `pipelines` list includes a series of data processing steps for handling images and their annotations, including loading, resizing, flipping, normalizing, padding, formatting, and collecting data.\n- The `expected_pipelines` list includes only two elements, both related to loading images and annotations with bounding boxes.\n  \nFinally, there is an assertion statement that checks whether the `expected_pipelines` list is equal to the result of the function call `get_loading_pipeline(pipelines)`. \n\nThe code appears to be part of a data preprocessing step, potentially for a machine learning or computer vision project involving image data and annotations."}
{"layout": 2284, "type": "text", "text": "mmdet.datasets. replace Image To Tensor ( pipelines ) Replace the Image To Tensor transform in a data pipeline to Default Format Bundle, which is normally useful in batch inference. ", "page_idx": 245, "bbox": [71, 615.5079956054688, 540.0034790039062, 652.8784790039062], "page_size": [612.0, 792.0]}
{"layout": 2285, "type": "text", "text": "Parameters  pipelines  ( list[dict] ) – Data pipeline configs. ", "page_idx": 245, "bbox": [118, 656.872802734375, 379.4508972167969, 671.408203125], "page_size": [612.0, 792.0]}
{"layout": 2286, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 245, "bbox": [118, 676, 154, 688], "page_size": [612.0, 792.0]}
{"layout": 2287, "type": "text", "text": "The new pipeline list with all Image To Tensor replaced by  Default Format Bundle. ", "page_idx": 245, "bbox": [137, 692.7388305664062, 473.13525390625, 707.2742309570312], "page_size": [612.0, 792.0]}
{"layout": 2288, "type": "text", "text": "Return type  list ", "page_idx": 246, "bbox": [118.8239974975586, 70.8248291015625, 186.72982788085938, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 2289, "type": "text", "text": "Examples ", "page_idx": 246, "bbox": [96, 102.62508392333984, 143.37255859375, 116.89152526855469], "page_size": [612.0, 792.0]}
{"layout": 2290, "type": "table", "page_idx": 246, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_136.jpg", "bbox": [93, 127, 544, 507], "page_size": [612.0, 792.0], "ocr_text": ">>> pipelines = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntrans forms=[\ndict(type='Resize', keep_ratio=True),\ndict(type='RandomFlip'),\ndict(type='Normalize', mean=[0, 9, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n])\n]\n>>> expected_pipelines = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntrans forms=[\ndict(type='Resize', keep_ratio=True),\ndict(type='RandomFlip'),\ndict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict(type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img']),\n])\n]\n\n>>> assert expected_pipelines == replace_ImageToTensor (pipelines)\n\n", "vlm_text": "The image shows a Python code block, which defines two lists of dictionaries named `pipelines` and `expected_pipelines`. These dictionaries specify image processing operations for a machine learning pipeline. \n\nHere's a breakdown of the operations within each list:\n\n1. **`pipelines`:**\n   - `LoadImageFromFile`: Loads an image from a file.\n   - `MultiScaleFlipAug`: Performs augmentation with multiscale flipping.\n     - `img_scale`: Sets image scale to (1333, 800).\n     - `flip`: Sets flipping to False.\n     - `transforms`: Applies a series of transformations:\n       - `Resize`: Resizes image while keeping ratio.\n       - `RandomFlip`: Applies random flipping.\n       - `Normalize`: Normalizes the image with mean [0, 0, 0] and std [1, 1, 1].\n       - `Pad`: Pads the image with a size divisor of 32.\n       - `ImageToTensor`: Converts the image to a tensor.\n       - `Collect`: Collects the image data with key 'img'.\n\n2. **`expected_pipelines`:**\n   - Similar to `pipelines` but with a slight difference:\n     - Instead of `ImageToTensor`, it uses `DefaultFormatBundle`.\n\nThe code ends with an assertion:\n```python\nassert expected_pipelines == replace_ImageToTensor(pipelines)\n```\nThis checks if the `expected_pipelines` is the same as `pipelines` after replacing `ImageToTensor` with the appropriate transformation."}
{"layout": 2291, "type": "text", "text": "38.2 pipelines ", "text_level": 1, "page_idx": 246, "bbox": [71, 531, 170, 549], "page_size": [612.0, 792.0]}
{"layout": 2292, "type": "text", "text": "class  mmdet.datasets.pipelines. Albu ( transforms ,  b box params  $\\mathbf{\\hat{\\rho}}$  None ,  keymap  $\\mathbf{\\varepsilon}=$  None , update pad shape  $\\mathbf{=}$  False ,  skip img without anno=False ) ", "page_idx": 246, "bbox": [72.0, 564.1520385742188, 497.51336669921875, 589.5664672851562], "page_size": [612.0, 792.0]}
{"layout": 2293, "type": "text", "text": "Album ent ation augmentation. ", "page_idx": 246, "bbox": [96, 588.21142578125, 215.9002685546875, 601.521484375], "page_size": [612.0, 792.0]}
{"layout": 2294, "type": "text", "text": "Adds custom transformations from Albumen tat ions library. Please, visit  https://albumen tat ions.read the docs.io to get more information. ", "page_idx": 246, "bbox": [96, 605.9950561523438, 540, 631.4094848632812], "page_size": [612.0, 792.0]}
{"layout": 2295, "type": "text", "text": "An example of  transforms  is as followed: ", "page_idx": 246, "bbox": [96, 636.0324096679688, 270.79376220703125, 649.3424682617188], "page_size": [612.0, 792.0]}
{"layout": 2296, "type": "table", "page_idx": 246, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_137.jpg", "bbox": [94, 654, 543, 708.75], "page_size": [612.0, 792.0], "ocr_text": "dict(\ntype='ShiftScaleRotate',\nshift_limit=0.0625,\n\n", "vlm_text": "The table contains code that appears to be part of a configuration or dictionary in Python, likely related to image data augmentation in machine learning:\n\n- `dict`: Indicates a dictionary structure.\n- `type='ShiftScaleRotate'`: Specifies the type of augmentation to be applied, which involves shifting, scaling, and rotating an image.\n- `shift_limit=0.0625`: Specifies the limit for shifting, probably referring to the maximum fraction of total image width or height."}
{"layout": 2297, "type": "text", "text": "(continues on next page) ", "page_idx": 246, "bbox": [461.9010009765625, 707.0835571289062, 540, 717.7316284179688], "page_size": [612.0, 792.0]}
{"layout": 2298, "type": "image", "page_idx": 247, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_138.jpg", "bbox": [93, 82, 543, 307], "page_size": [612.0, 792.0], "ocr_text": "scale_limit=0.0,\nrotate_limit=0,\ninterpolation=1,\np=0.5),\ndict(\ntype='RandomBrightnessContrast',\nbrightness_limit=[0.1, 0.3],\ncontrast_limit=[0.1, 0.3],\np=0.2),\ndict(type='ChannelShuffle', p=0.1),\ndict(\ntype='OneOf',\ntransforms=[\ndict(type='Blur', blur_limit=3, p=1.9),\ndict(type='MedianBlur', blur_limit=3, p=1.0)\n1,\np=0.1),\n\n", "vlm_text": "This image shows a snippet of code that appears to define image augmentation operations, possibly using a library like Albumentations. Here's a breakdown of the elements:\n\n1. **Scale, Rotate, Interpolation, Probability (`p`)**:\n   - `scale_limit=0.0`\n   - `rotate_limit=0`\n   - `interpolation=1`\n   - Applied with a probability of `p=0.5`\n\n2. **Random Brightness and Contrast**:\n   - `brightness_limit=[0.1, 0.3]`\n   - `contrast_limit=[0.1, 0.3]`\n   - Probability of applying: `p=0.2`\n\n3. **Channel Shuffle**:\n   - Probability: `p=0.1`\n\n4. **OneOf Transformations**:\n   - Types of blurs applied with different probabilities:\n     - `Blur`: `blur_limit=3`, probability `p=1.0`\n     - `MedianBlur`: `blur_limit=3`, probability `p=1.0`\n   - Overall probability for `OneOf`: `p=0.1`\n\nThese transformations are typically used in data preprocessing to enhance the diversity of training images."}
{"layout": 2299, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 247, "bbox": [117, 320, 169, 332], "page_size": [612.0, 792.0]}
{"layout": 2300, "type": "text", "text": "•  transforms  ( list[dict] ) – A list of albu transformations •  b box params  ( dict ) – B box params for album ent ation  Compose •  keymap  ( dict ) – Contains {‘input key’:’album ent ation-style key’} •  skip img without anno  ( bool ) – Whether to skip the image if no ann left after aug ", "page_idx": 247, "bbox": [145, 337.4064636230469, 500.8155822753906, 404.5155334472656], "page_size": [612.0, 792.0]}
{"layout": 2301, "type": "text", "text": "alb u builder ( cfg ) Import a module from albumen tat ions. It inherits some of  build from cf g()  logic. Parameters  cfg  ( dict ) – Config dict. It should at least contain the key “type”. Returns  The constructed object. Return type  obj static mapper ( d ,  keymap ) Dictionary mapper. Renames keys according to keymap provided. Parameters  $\\begin{array}{r l}&{\\bullet\\,\\,\\,{\\mathsf{d}}\\,(d i c t)-{\\mathrm{old~dict}}}\\\\ &{\\bullet\\,\\,\\,{\\mathsf{k e y m a p}}\\,(d i c t)-\\{\\,^{\\ast}{\\mathrm{old}}\\_{\\mathrm{kcy}}^{\\ast}\\colon{\\mathrm{new}}\\_{\\mathrm{kcy}}^{\\ast}\\}}\\end{array}$  Returns  new dict. Return type  dict ", "page_idx": 247, "bbox": [96, 414.966064453125, 457.0750427246094, 632.2612915039062], "page_size": [612.0, 792.0]}
{"layout": 2302, "type": "text", "text": "class  mmdet.datasets.pipelines. Auto Augment ( policies ) Auto augmentation. ", "page_idx": 247, "bbox": [72.00001525878906, 636.1361083984375, 333.1854248046875, 661.5515747070312], "page_size": [612.0, 792.0]}
{"layout": 2303, "type": "text", "text": "This data augmentation is proposed in  Learning Data Augmentation Strategies for Object Detection . TODO: Implement ‘Shear’, ‘Sharpness’ and ‘Rotate’ transforms ", "page_idx": 247, "bbox": [96, 666.1735229492188, 496.03857421875, 679.4835815429688], "page_size": [612.0, 792.0]}
{"layout": 2304, "type": "text", "text": "", "page_idx": 247, "bbox": [96, 684.1065063476562, 352.0492248535156, 697.4165649414062], "page_size": [612.0, 792.0]}
{"layout": 2305, "type": "text", "text": "Parameters  policies  ( list[list[dict]] ) – The policies of auto augmentation. Each policy in policies  is a specific augmentation policy, and is composed by several augmentations (dict). When Auto Augment is called, a random policy in  policies  will be selected to augment images. ", "page_idx": 248, "bbox": [118, 70.8248291015625, 518, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 2306, "type": "text", "text": "Examples ", "text_level": 1, "page_idx": 248, "bbox": [96, 128, 144, 140], "page_size": [612.0, 792.0]}
{"layout": 2307, "type": "table", "page_idx": 248, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_139.jpg", "bbox": [92, 149, 545, 460], "page_size": [612.0, 792.0], "ocr_text": ">>> replace = (104, 116, 124)\n>>> policies = [\n\n>> [\n\n>>> dict(type='Sharpness', prob=0.0, level=8),\n>> dict(\n\n>>> type='Shear',\n\n>>> prob=0.4,\n\n>>> level=0,\n\n>>> replace=replace,\n\n>>> axis='x')\n\n>> 1,\n\n>> [\n\n>>> dict(\n\n>>> type='Rotate',\n\n>>> prob=0.6,\n\n>>> level=10,\n\n>>> replace=replace),\n\n>>> dict(type='Color', prob=1.0, level=6)\n>> ]\n\n>> ]\n\n>>> augmentation = AutoAugment (policies)\n\n>>> img = np.ones(100, 100, 3)\n\n>>> gt_bboxes = np.ones(10, 4)\n\n>>> results = dict(img=img, gt_bboxes=gt_bboxes)\n>>> results = augmentation(results)\n\n", "vlm_text": "This image shows a code snippet that demonstrates how to use an auto-augmentation technique for image data. Here's a breakdown:\n\n1. **Replace Values**: \n   ```python\n   replace = (104, 116, 124)\n   ```\n   - This tuple is used for color replacement during augmentation.\n\n2. **Policies**:\n   ```python\n   policies = [\n       [\n           dict(type='Sharpness', prob=0.0, level=8),\n           dict(type='Shear', prob=0.4, level=0, replace=replace, axis='x')\n       ],\n       [\n           dict(type='Rotate', prob=0.6, level=10, replace=replace),\n           dict(type='Color', prob=1.0, level=6)\n       ]\n   ]\n   ```\n   - A list of policies is defined, with each policy containing different transformation operations such as 'Sharpness', 'Shear', 'Rotate', and 'Color'.\n   - Each operation specifies a type, probability (`prob`), level, and additional parameters specific to the transformation.\n\n3. **Applying AutoAugment**:\n   ```python\n   augmentation = AutoAugment(policies)\n   ```\n   - An `AutoAugment` object is created using the defined policies.\n\n4. **Creating Image Data**:\n   ```python\n   img = np.ones((100, 100, 3))\n   gt_bboxes = np.ones((10, 4))\n   ```\n   - Sample image data (`img`) and ground truth bounding boxes (`gt_bboxes`) are created using `numpy`.\n\n5. **Applying Augmentation**:\n   ```python\n   results = dict(img=img, gt_bboxes=gt_bboxes)\n   results = augmentation(results)\n   ```\n   - The image data and bounding boxes are passed into a dictionary called `results`.\n   - The augmentation is applied to this data. \n\nThis code provides a basic example of setting up auto-augmentation with specific policies to enhance image datasets."}
{"layout": 2308, "type": "text", "text": "class  mmdet.datasets.pipelines. Brightness Transform ( level ,  pro  $\\wp{=}0.5.$  ) Apply Brightness transformation to image. The bboxes, masks and segmentation s are not modified. ", "page_idx": 248, "bbox": [72, 466.06903076171875, 492.5717468261719, 491.4834899902344], "page_size": [612.0, 792.0]}
{"layout": 2309, "type": "text", "text": "Parameters ", "page_idx": 248, "bbox": [118, 495.47882080078125, 167.7602996826172, 510.0142517089844], "page_size": [612.0, 792.0]}
{"layout": 2310, "type": "text", "text": "•  level  ( int | float ) – Should be in range [0,_MAX_LEVEL]. ", "page_idx": 248, "bbox": [145, 514.0384521484375, 416.0888366699219, 527.3485107421875], "page_size": [612.0, 792.0]}
{"layout": 2311, "type": "text", "text": "•  prob  ( float ) – The probability for performing Brightness transformation. ", "page_idx": 248, "bbox": [145, 531.971435546875, 452.3819580078125, 545.281494140625], "page_size": [612.0, 792.0]}
{"layout": 2312, "type": "text", "text": "class  mmdet.datasets.pipelines. Collect ( keys ,  meta_keys  $=$  ('filename', 'ori filename', 'ori_shape', 'img_shape', 'pad_shape', 'scale factor', 'flip', 'flip direction', 'img norm cf g'))", "page_idx": 248, "bbox": [72, 549.7550659179688, 521.08544921875, 587.014892578125], "page_size": [612.0, 792.0]}
{"layout": 2313, "type": "text", "text": "Collect data from the loader relevant to the specific task. ", "page_idx": 248, "bbox": [96, 585.7694702148438, 321.6332092285156, 599.0795288085938], "page_size": [612.0, 792.0]}
{"layout": 2314, "type": "text", "text": "This is usually the last stage of the data loader pipeline. Typically keys is set to some subset of “img”, “proposals”, “gt_bboxes”, “gt b boxes ignore”, “gt_labels”, and/or “gt_masks”. ", "page_idx": 248, "bbox": [96, 603.7024536132812, 540, 628.967529296875], "page_size": [612.0, 792.0]}
{"layout": 2315, "type": "text", "text": "The “img_meta” item is always populated. The contents of the “img_meta” dictionary depends on “meta_keys”. By default this includes: ", "page_idx": 248, "bbox": [96, 633.5904541015625, 540, 658.8555297851562], "page_size": [612.0, 792.0]}
{"layout": 2316, "type": "text", "text": "• “img_shape”: shape of the image input to the network as a tuple (h, w, c). Note that images may be zero padded on the bottom/right if the batch tensor is larger than this shape. • “scale factor”: a float indicating the preprocessing scale ", "page_idx": 248, "bbox": [110, 663.4784545898438, 540, 706.676513671875], "page_size": [612.0, 792.0]}
{"layout": 2317, "type": "text", "text": "• “flip”: a boolean indicating if image flip transform was used • “filename”: path to the image file • “ori_shape”: original shape of the image as a tuple (h, w, c) • “pad_shape”: image shape after padding • “img norm cf g”: a dict of normalization information: –  mean - per channel mean subtraction –  std - per channel std divisor –  to_rgb - bool indicating if bgr was converted to rgb ", "page_idx": 249, "bbox": [110, 71.45246887207031, 358, 210.88919067382812], "page_size": [612.0, 792.0]}
{"layout": 2318, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 249, "bbox": [117, 222, 170, 234], "page_size": [612.0, 792.0]}
{"layout": 2319, "type": "text", "text": "•  keys  ( Sequence[str] ) – Keys of results to be collected in  data . •  meta_keys ( Sequence[str], optional ) – Meta keys to be converted to mmcv.Data Container and collected in data[img_metas] . Default: ( ' filename ' ,  ' ori filename ' ,  ' ori_shape ' ,  ' img_shape ' ,  ' pad_shape ' , ' scale factor ' ,  ' flip ' ,  ' flip direction ' ,  ' img norm cf g ' ) ", "page_idx": 249, "bbox": [145, 238.82435607910156, 518, 304.9826354980469], "page_size": [612.0, 792.0]}
{"layout": 2320, "type": "text", "text": "class  mmdet.datasets.pipelines. Color Transform ( level ,  pro  $\\jmath{=}0.5$  ) Apply Color transformation to image. The bboxes, masks, and segmentation s are not modified. ", "page_idx": 249, "bbox": [71, 316.3839111328125, 474.9378662109375, 341.7983703613281], "page_size": [612.0, 792.0]}
{"layout": 2321, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 249, "bbox": [117, 348, 169, 359], "page_size": [612.0, 792.0]}
{"layout": 2322, "type": "text", "text": "•  level  ( int | float ) – Should be in range [0,_MAX_LEVEL]. •  prob  ( float ) – The probability for performing Color transformation. ", "page_idx": 249, "bbox": [145, 364.3533630371094, 432.2574462890625, 395.5964050292969], "page_size": [612.0, 792.0]}
{"layout": 2323, "type": "text", "text": "class  mmdet.datasets.pipelines. Compose ( transforms ) Compose multiple transforms sequentially. ", "page_idx": 249, "bbox": [71, 400.0699462890625, 324.0492858886719, 425.4844055175781], "page_size": [612.0, 792.0]}
{"layout": 2324, "type": "text", "text": "Parameters  transforms  ( Sequence[dict | callable] ) – Sequence of transform object or con- fig dict to be composed. ", "page_idx": 249, "bbox": [118.82490539550781, 429.479736328125, 518, 455.3724060058594], "page_size": [612.0, 792.0]}
{"layout": 2325, "type": "text", "text": "class  mmdet.datasets.pipelines. Contrast Transform ( level ,  prob=0.5 ) Apply Contrast transformation to image. The bboxes, masks and segmentation s are not modified. ", "page_idx": 249, "bbox": [71, 459.845947265625, 483.3661804199219, 485.2604064941406], "page_size": [612.0, 792.0]}
{"layout": 2326, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 249, "bbox": [117, 491, 169, 503], "page_size": [612.0, 792.0]}
{"layout": 2327, "type": "text", "text": "•  level  ( int | float ) – Should be in range [0,_MAX_LEVEL]. •  prob  ( float ) – The probability for performing Contrast transformation. ", "page_idx": 249, "bbox": [145, 507.8153991699219, 443.1763916015625, 539.0584106445312], "page_size": [612.0, 792.0]}
{"layout": 2328, "type": "text", "text": "class  mmdet.datasets.pipelines. CutOut ( n_holes ,  cut out shape  $=$  None ,  cut out ratio  $=$  None ,  fill_in=(0, 0, 0) ) CutOut operation. ", "page_idx": 249, "bbox": [71, 543.5319213867188, 540.000244140625, 568.9464111328125], "page_size": [612.0, 792.0]}
{"layout": 2329, "type": "text", "text": "Randomly drop some regions of image used in  Cutout . ", "page_idx": 249, "bbox": [96, 573.5693969726562, 314.7490539550781, 586.8794555664062], "page_size": [612.0, 792.0]}
{"layout": 2330, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 249, "bbox": [117, 593, 168, 604], "page_size": [612.0, 792.0]}
{"layout": 2331, "type": "text", "text": "•  n_holes  ( int | tuple[int, int] ) – Number of regions to be dropped. If it is given as a list, number of holes will be randomly selected from the closed interval [ n_holes[0] , n_holes[1] ]. •  cut out shape  ( tuple[int, int] | list[tuple[int, int]] ) – The candidate shape of dropped regions. It can be  tuple[int, int]  to use a fixed cutout shape, or  list[tuple[int, int]]  to randomly choose shape from the list. ", "page_idx": 249, "bbox": [145, 609.4343872070312, 518, 688.4984130859375], "page_size": [612.0, 792.0]}
{"layout": 2332, "type": "text", "text": "•  cut out ratio  ( tuple[float, float] | list[tuple[float, float]] ) – The can- didate ratio of dropped regions. It can be  tuple[float, float]  to use a fixed ratio or list[tuple[float, float]]  to randomly choose ratio from the list. Please note that  cut out shape and  cut out ratio  cannot be both given at the same time. •  fill_in  ( tuple[float, float, float] | tuple[int, int, int] ) – The value of pixel to fill in the dropped regions. Default:   $(0,\\,0,\\,0)$  . ", "page_idx": 250, "bbox": [145, 71.45246887207031, 518, 150.5155487060547], "page_size": [612.0, 792.0]}
{"layout": 2333, "type": "text", "text": "class  mmdet.datasets.pipelines. Default Format Bundle Default formatting bundle. ", "page_idx": 250, "bbox": [72, 156.86203002929688, 333.5181579589844, 180.40354919433594], "page_size": [612.0, 792.0]}
{"layout": 2334, "type": "text", "text": "It simplifies the pipeline of formatting common fields, including “img”, “proposals”, “gt_bboxes”, “gt_labels”, “gt_masks” and “gt semantic seg”. These fields are formatted as follows. ", "page_idx": 250, "bbox": [96, 185.02650451660156, 540, 210.2915496826172], "page_size": [612.0, 792.0]}
{"layout": 2335, "type": "text", "text": "• img: (1)transpose, (2)to tensor, (3)to Data Container (stack  $\\risingdotseq$  True) • proposals: (1)to tensor, (2)to Data Container • gt_bboxes: (1)to tensor, (2)to Data Container • gt b boxes ignore: (1)to tensor, (2)to Data Container • gt_labels: (1)to tensor, (2)to Data Container • gt_masks: (1)to tensor, (2)to Data Container (cpu_only  $\\mathbf{\\acute{=}}$  True) • gt semantic seg: (1)unsqueeze dim-0 (2)to tensor, (3)to Data Container (stack  $\\risingdotseq$  True) ", "page_idx": 250, "bbox": [110, 214.9145050048828, 457.2434997558594, 335.820556640625], "page_size": [612.0, 792.0]}
{"layout": 2336, "type": "text", "text": "class  mmdet.datasets.pipelines. Equalize Transform  $(p r o b{=}0.5)$  ) ", "page_idx": 250, "bbox": [72, 342, 371.6704406738281, 353.07611083984375], "page_size": [612.0, 792.0]}
{"layout": 2337, "type": "text", "text": "Apply Equalize transformation to image. The bboxes, masks and segmentation s are not modified. Parameters  prob  ( float ) – The probability for performing Equalize transformation. ", "page_idx": 250, "bbox": [96, 352.3985290527344, 485, 384.23931884765625], "page_size": [612.0, 792.0]}
{"layout": 2338, "type": "text", "text": "class  mmdet.datasets.pipelines. Expand ( mean=(0, 0, 0) ,  to_rgb  $\\mathbf{\\delta}){=}\\mathbf{\\delta}$  True ,  ratio range=(1, 4) , seg ignore la be  $\\leftrightharpoons$  None ,  pro  $\\scriptstyle b=0.5$  ) ", "page_idx": 250, "bbox": [72, 388.1151123046875, 469.0906982421875, 413.5295715332031], "page_size": [612.0, 792.0]}
{"layout": 2339, "type": "text", "text": "Random expand the image & bboxes. ", "page_idx": 250, "bbox": [96, 412.1745300292969, 245.6386260986328, 425.48455810546875], "page_size": [612.0, 792.0]}
{"layout": 2340, "type": "text", "text": "Randomly place the original image on a canvas of ‘ratio’  $\\mathbf{X}$   original image size filled with mean values. The ratio is in the range of ratio range. ", "page_idx": 250, "bbox": [96, 430.1075439453125, 540, 455.37255859375], "page_size": [612.0, 792.0]}
{"layout": 2341, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 250, "bbox": [117, 461, 169, 473], "page_size": [612.0, 792.0]}
{"layout": 2342, "type": "text", "text": "•  mean  ( tuple ) – mean value of dataset. •  to_rgb  ( bool ) – if need to convert the order of mean to align with RGB. •  ratio range  ( tuple ) – range of expand ratio. •  prob  ( float ) – probability of applying this transformation ", "page_idx": 250, "bbox": [145, 477.92755126953125, 446, 545.03662109375], "page_size": [612.0, 792.0]}
{"layout": 2343, "type": "text", "text": "class  mmdet.datasets.pipelines. Image To Tensor ( keys ) ", "page_idx": 250, "bbox": [72, 549.5090942382812, 329.598388671875, 562.8589477539062], "page_size": [612.0, 792.0]}
{"layout": 2344, "type": "text", "text": "Convert image to  torch.Tensor  by given keys. ", "page_idx": 250, "bbox": [96, 561.6145629882812, 295, 574.9246215820312], "page_size": [612.0, 792.0]}
{"layout": 2345, "type": "text", "text": "The dimension order of input image is (H, W, C). The pipeline will convert it to (C, H, W). If only 2 dimension (H, W) is given, the output would be (1, H, W). ", "page_idx": 250, "bbox": [96, 579.5465698242188, 540, 604.8125610351562], "page_size": [612.0, 792.0]}
{"layout": 2346, "type": "text", "text": "Parameters  keys  ( Sequence[str] ) – Key of images to be converted to Tensor. ", "page_idx": 250, "bbox": [118, 608.806884765625, 446, 623.34228515625], "page_size": [612.0, 792.0]}
{"layout": 2347, "type": "text", "text": "class  mmdet.datasets.pipelines. InstaBoost ( action candidate  $\\mathbf{=}$  ('normal', 'horizontal', 'skip') , action_prob=(1, 0, 0) ,  scale  $\\mathbf{=}$  (0.8, 1.2) ,    $d x{=}I5$  ,    $\\scriptstyle d y=I S$  , ", "page_idx": 250, "bbox": [72, 627.2181396484375, 511.7406311035156, 652.6326293945312], "page_size": [612.0, 792.0]}
{"layout": 2348, "type": "text", "text": "theta=(- 1, 1) ,  color_prob  $\\backsimeq\\!O.5$  ,  hflag  $\\mathbf{\\hat{\\mu}}$  False ,  aug_ratio  $\\backsimeq\\!O.5$  ) ", "page_idx": 250, "bbox": [291, 651.1281127929688, 535.8652954101562, 664.5875854492188], "page_size": [612.0, 792.0]}
{"layout": 2349, "type": "text", "text": "Data augmentation method in  InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy- Pasting . ", "page_idx": 250, "bbox": [96, 663.2335205078125, 540, 688.4985961914062], "page_size": [612.0, 792.0]}
{"layout": 2350, "type": "text", "text": "Refer to  https://github.com/GothicAi/Instaboost  for implementation details. ", "page_idx": 250, "bbox": [96, 693.1205444335938, 398.3751525878906, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 2351, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 251, "bbox": [117, 72, 169, 84], "page_size": [612.0, 792.0]}
{"layout": 2352, "type": "text", "text": "•  action candidate  ( tuple ) – Action candidates. “normal”, “horizontal”, “vertical”, “skip” are supported. Default: (‘normal’, ‘horizontal’, ‘skip’). •  action pro b  ( tuple ) – Corresponding action probabilities. Should be the same length as action candidate. Default: (1, 0, 0). •  scale  ( tuple ) – (min scale, max scale). Default: (0.8, 1.2). •  dx  ( int ) – The maximum x-axis shift will be (instance width) / dx. Default 15. •  dy  ( int ) – The maximum y-axis shift will be (instance height) / dy. Default 15. •  theta  ( tuple ) – (min rotation degree, max rotation degree). Default: (-1, 1). •  color_prob  ( float ) – Probability of images for color augmentation. Default 0.5. •  heat map flag  ( bool ) – Whether to use heatmap guided. Default False. •  aug_ratio  ( float ) – Probability of applying this transformation. Default 0.5. ", "page_idx": 251, "bbox": [145, 89.38447570800781, 518, 270.06744384765625], "page_size": [612.0, 792.0]}
{"layout": 2353, "type": "text", "text": "class  mmdet.datasets.pipelines. Load Annotations ( with_bbox  $\\mathbf{\\beta}=$  True ,  with_labe  $\\acute{=}$  True ,  with_mask  $\\leftrightharpoons$  False , with_seg  $\\mathbf{\\hat{\\mu}}$  False ,  poly2mask  $\\mathbf{\\tilde{\\rho}}$  True , file client arg s  $=$  {'backend': 'disk'} ) ", "page_idx": 251, "bbox": [71, 274.53997802734375, 533.5986328125, 311.80084228515625], "page_size": [612.0, 792.0]}
{"layout": 2354, "type": "text", "text": "Load multiple types of annotations. ", "page_idx": 251, "bbox": [96, 310.5553894042969, 238.31602478027344, 323.86541748046875], "page_size": [612.0, 792.0]}
{"layout": 2355, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 251, "bbox": [117, 330, 169, 341], "page_size": [612.0, 792.0]}
{"layout": 2356, "type": "text", "text": "•  with_bbox  ( bool ) – Whether to parse and load the bbox annotation. Default: True. •  with_label  ( bool ) – Whether to parse and load the label annotation. Default: True. •  with_mask  ( bool ) – Whether to parse and load the mask annotation. Default: False. •  with_seg  ( bool ) – Whether to parse and load the semantic segmentation annotation. De- fault: False. •  poly2mask  ( bool ) – Whether to convert the instance masks from polygons to bitmaps. De- fault: True. •  file client arg s  ( dict ) – Arguments to instantiate a FileClient. See  mmcv.fileio. FileClient  for details. Defaults to  dict(backend= ' disk ' ) . ", "page_idx": 251, "bbox": [145, 346.4214172363281, 518, 485.2604675292969], "page_size": [612.0, 792.0]}
{"layout": 2357, "type": "text", "text": "process polygons ( polygons ) Convert polygons to list of ndarray and filter invalid polygons. ", "page_idx": 251, "bbox": [96, 489.7340087890625, 365.7966003417969, 515.1484985351562], "page_size": [612.0, 792.0]}
{"layout": 2358, "type": "text", "text": "Parameters  polygons  ( list[list] ) – Polygons of one instance. Returns  Processed polygons. Return type  list[numpy.ndarray] ", "page_idx": 251, "bbox": [137, 519.143798828125, 408, 569.544189453125], "page_size": [612.0, 792.0]}
{"layout": 2359, "type": "text", "text": "class  mmdet.datasets.pipelines. Load Image From File ( to_float32  $\\leftrightharpoons$  False ,  color_type  $=$  'color' , file client arg  $s{=}i$  {'backend': 'disk'} ) ", "page_idx": 251, "bbox": [71, 573.4199829101562, 478.8134460449219, 598.724853515625], "page_size": [612.0, 792.0]}
{"layout": 2360, "type": "text", "text": "Load an image from file. ", "page_idx": 251, "bbox": [96, 597.4794311523438, 195.4368133544922, 610.7894897460938], "page_size": [612.0, 792.0]}
{"layout": 2361, "type": "text", "text": "Required keys are “img_prefix” and “img_info” (a dict that must contain the key “filename”). Added or updated keys are “filename”, “img”, “img_shape”, “ori_shape” (same as  img_shape ), “pad_shape” (same as  img_shape ), “scale factor” (1.0) and “img norm cf g” (means  $\\mathrm{{=}0}$   and stds  $_{\\ast=1}$  ). ", "page_idx": 251, "bbox": [96, 615.4124145507812, 540, 652.6325073242188], "page_size": [612.0, 792.0]}
{"layout": 2362, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 251, "bbox": [117, 658, 169, 670], "page_size": [612.0, 792.0]}
{"layout": 2363, "type": "text", "text": "•  to_float32  ( bool ) – Whether to convert the loaded image to a float32 numpy array. If set to False, the loaded image is an uint8 array. Defaults to False. •  color_type  ( str ) – The flag argument for  mmcv.im from bytes() . Defaults to ‘color’. ", "page_idx": 251, "bbox": [145, 675.1884155273438, 518, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 2364, "type": "text", "text": "•  file client arg s  ( dict ) – Arguments to instantiate a FileClient. See  mmcv.fileio. FileClient  for details. Defaults to  dict(backend  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' disk ' ) . ", "page_idx": 252, "bbox": [145, 71.45246887207031, 518, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2365, "type": "text", "text": "class  mmdet.datasets.pipelines. Load Image From Webcam ( to_float  $32{=}$  False ,  color_type  $\\mathbf{=}$  'color' , file client arg  $s{=}$  {'backend': 'disk'} ) ", "page_idx": 252, "bbox": [71, 101.19103240966797, 489.4836730957031, 126.49593353271484], "page_size": [612.0, 792.0]}
{"layout": 2366, "type": "text", "text": "Load an image from webcam. ", "page_idx": 252, "bbox": [96, 125.25050354003906, 215.60133361816406, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 2367, "type": "text", "text": "Similar with  Load Image From File , but the image read from webcam is in  results[ ' img ' ] . ", "page_idx": 252, "bbox": [96, 143.18348693847656, 471.6255798339844, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 2368, "type": "text", "text": "class  mmdet.datasets.pipelines. Load MultiChannel Image From Files ( to_float3  $\\boldsymbol{\\mathrm{2}}\\!=\\!\\boldsymbol{\\mathrm{1}}$  False , color_type  $\\mathbf{=}$  'unchanged' , file client arg s={'backend': 'disk'} ) ", "page_idx": 252, "bbox": [71, 160.96604919433594, 509.2593994140625, 210.1819610595703], "page_size": [612.0, 792.0]}
{"layout": 2369, "type": "text", "text": "Load multi-channel images from a list of separate channel files. ", "page_idx": 252, "bbox": [96, 208.93653869628906, 349, 222.24656677246094], "page_size": [612.0, 792.0]}
{"layout": 2370, "type": "text", "text": "Required keys are “img_prefix” and “img_info” (a dict that must contain the key “filename”, which is expected to be a list of filenames). Added or updated keys are “filename”, “img”, “img_shape”, “ori_shape” (same as  img_shape ), “pad_shape” (same as  img_shape ), “scale factor” (1.0) and “img norm cf g” (means  $\\mathrm{=}0$   and stds  $^{=1}$  ). ", "page_idx": 252, "bbox": [96, 226.86952209472656, 540, 276.04461669921875], "page_size": [612.0, 792.0]}
{"layout": 2371, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 252, "bbox": [117, 282, 170, 294], "page_size": [612.0, 792.0]}
{"layout": 2372, "type": "text", "text": "•  to_float32  ( bool ) – Whether to convert the loaded image to a float32 numpy array. If set to False, the loaded image is an uint8 array. Defaults to False. •  color_type  ( str ) – The flag argument for  mmcv.im from bytes() . Defaults to ‘color’. •  file client arg s  ( dict ) – Arguments to instantiate a FileClient. See  mmcv.fileio. FileClient  for details. Defaults to  dict(backend= ' disk ' ) . ", "page_idx": 252, "bbox": [145, 298.6006164550781, 518, 371.6866455078125], "page_size": [612.0, 792.0]}
{"layout": 2373, "type": "text", "text": "class  mmdet.datasets.pipelines. Load Proposals ( num max proposals  $:=$  None ) Load proposal pipeline. ", "page_idx": 252, "bbox": [71, 376.1601867675781, 425, 401.57464599609375], "page_size": [612.0, 792.0]}
{"layout": 2374, "type": "text", "text": "Required key is “proposals”. Updated keys are “proposals”, “b box fields”. ", "page_idx": 252, "bbox": [96, 406.1966247558594, 396, 419.50665283203125], "page_size": [612.0, 792.0]}
{"layout": 2375, "type": "text", "text": "Parameters  num max proposals  ( int, optional ) – Maximum number of proposals to load. If not specified, all proposals will be loaded. ", "page_idx": 252, "bbox": [118.82392120361328, 423.5019836425781, 518, 449.3946533203125], "page_size": [612.0, 792.0]}
{"layout": 2376, "type": "text", "text": "class  mmdet.datasets.pipelines. MinI oU Random Crop ( min_ious=(0.1, 0.3, 0.5, 0.7, 0.9) ,  min crop s iz  $\\it{z}{=}0.3$  , b box clip border=True)Random crop the image & bboxes, the cropped patches have minimum IoU requirement with original image & bboxes, the IoU threshold is randomly selected from min_ious. ", "page_idx": 252, "bbox": [71, 453.8681945800781, 540, 503.1936340332031], "page_size": [612.0, 792.0]}
{"layout": 2377, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 252, "bbox": [117, 509, 169, 521], "page_size": [612.0, 792.0]}
{"layout": 2378, "type": "text", "text": "•  min_ious  ( tuple ) – minimum IoU threshold for all intersections with •  boxes  ( bounding ) – •  min crop size  ( float ) – minimum crop’s size (i.e. h,w :  $=\\mathrm{a}^{*}\\mathrm{h}$  ,  $\\mathrm{a}^{*}\\mathrm{w}$  , •  a  $>=$   min crop size)  ( where ) – •  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. ", "page_idx": 252, "bbox": [145, 525.7485961914062, 518, 622.74462890625], "page_size": [612.0, 792.0]}
{"layout": 2379, "type": "text", "text": "Note:  The keys for bboxes, labels and masks should be paired. That is,  gt_bboxes  corresponds to  gt_labels  and gt_masks , and  gt b boxes ignore  to  gt labels ignore  and  gt masks ignore . ", "page_idx": 252, "bbox": [96, 638.69482421875, 540, 664.5874633789062], "page_size": [612.0, 792.0]}
{"layout": 2380, "type": "text", "text": "class  mmdet.datasets.pipelines. MixUp ( img_scale  $\\scriptstyle\\sum=$  (640, 640) ,  ratio range  $\\scriptstyle\\left=|\\begin{array}{l}{\\begin{array}{r l}\\end{array}}\\end{array}\\right.$  (0.5, 1.5) ,  flip_ratio  $\\bullet{=}0.5$  , pad_va  $\\scriptstyle{l=l}I4$  ,  max_iters  $\\scriptstyle{\\prime=}I S$  ,  min b box s iz  $e{=}5$  , min area ratio  $\\scriptstyle{\\iota=0.2}$  ,  max aspect ratio  $\\bullet{=}2O.$  ) ", "page_idx": 253, "bbox": [71, 71.30303192138672, 512.5276489257812, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 2381, "type": "text", "text": "MixUp data augmentation. ", "page_idx": 253, "bbox": [96, 107.31752014160156, 208, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 2382, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 253, "bbox": [117, 126, 169, 138], "page_size": [612.0, 792.0]}
{"layout": 2383, "type": "text", "text": "•  img_scale  ( Sequence[int] ) – Image output size after mixup pipeline. Default: (640, 640). •  ratio range  ( Sequence[float] ) – Scale ratio of mixup image. Default: (0.5, 1.5). •  flip_ratio  ( float ) – Horizontal flip ratio of mixup image. Default: 0.5. •  pad_val  ( int ) – Pad value. Default: 114. •  max_iters  ( int ) – The maximum number of iterations. If the number of iterations is greater than  max_iters , but gt_bbox is still empty, then the iteration is terminated. Default: 15. •  min b box size  ( float ) – Width and height threshold to filter bboxes. If the height or width of a box is smaller than this value, it will be removed. Default: 5. •  min area ratio  ( float ) – Threshold of area ratio between original bboxes and wrapped bboxes. If smaller than this value, the box will be removed. Default: 0.2. •  max aspect ratio  ( float ) – Aspect ratio of width and height threshold to filter bboxes. If max(h/w, w/h) larger than this value, the box will be removed. Default: 20. ", "page_idx": 253, "bbox": [145, 143.18348693847656, 518, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 2384, "type": "text", "text": "get indexes ( dataset ) Call function to collect indexes. Parameters  dataset  ( Multi Image Mix Data set ) – The dataset. Returns  indexes. Return type  list ", "page_idx": 253, "bbox": [96, 346.2720642089844, 399.8204040527344, 426.0823059082031], "page_size": [612.0, 792.0]}
{"layout": 2385, "type": "text", "text": "class  mmdet.datasets.pipelines. Mosaic ( img_scale  $=$  (640, 640) ,  center ratio range  $\\mathbf{\\tilde{=}}$  (0.5, 1.5) , min b box size  $\\mathrm{=}0$  ,  pad_va  $\\scriptstyle{l=I I4.}$  ) ", "page_idx": 253, "bbox": [71, 429.9580993652344, 486.484619140625, 455.37255859375], "page_size": [612.0, 792.0]}
{"layout": 2386, "type": "text", "text": "Mosaic augmentation. ", "page_idx": 253, "bbox": [96, 454.01751708984375, 185.45452880859375, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 2387, "type": "text", "text": "Given 4 images, mosaic transform combines them into one output image. The output image is composed of the parts from each sub- image. ", "page_idx": 253, "bbox": [96, 471.9505310058594, 540.0034790039062, 497.2155456542969], "page_size": [612.0, 792.0]}
{"layout": 2388, "type": "table", "page_idx": 253, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_140.jpg", "bbox": [93, 501, 544, 714], "page_size": [612.0, 792.0], "ocr_text": "mosaic transform\n\ncenter_x\n4$------------------------------ +\npad pad\n$o---------- +\n|\n| imagel  |-------- +\n| |\n| image2 |\ncenter_y ala Reena pone enna\n| cropped\npad | image3 image4\n|\n+---- |------------- +----------- +\n|\n\n", "vlm_text": "The table illustrates a \"mosaic transform\" layout commonly used in image augmentation for machine learning. It is organized as a 2x2 grid:\n\n- **Upper-left**: Padding (\"pad\").\n- **Upper-right**: Image 1 and Image 2, with padding on the left and top.\n- **Lower-left**: Cropped Image 3 and padding on the left.\n- **Lower-right**: Image 4.\n\nThe transformation involves positioning images in a grid format with potential padding and cropping adjustments. This can help improve the diversity of the training data by combining different images."}
{"layout": 2389, "type": "table", "page_idx": 254, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_141.jpg", "bbox": [92, 82, 543, 163], "page_size": [612.0, 792.0], "ocr_text": "The mosaic transform steps are as follows:\n\n1. Choose the mosaic center as the intersections of 4 images\n\n2. Get the left top image according to the index, and randomly\nsample another 3 images from the custom dataset.\n\n3. Sub image will be cropped if image is larger than mosaic patch\n\n", "vlm_text": "The table contains steps for executing a mosaic transform:\n\n1. Choose the mosaic center as the intersections of 4 images.\n2. Get the top left image according to the index, and randomly sample another 3 images from the custom dataset.\n3. Sub image will be cropped if the image is larger than the mosaic patch."}
{"layout": 2390, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 254, "bbox": [117, 177, 168, 189], "page_size": [612.0, 792.0]}
{"layout": 2391, "type": "text", "text": "•  img_scale  ( Sequence[int] ) – Image size after mosaic pipeline of single image. Default to (640, 640). •  center ratio range  ( Sequence[float] ) – Center ratio range of mosaic output. Default to (0.5, 1.5). •  min b box size  ( int | float ) – The minimum pixel for filtering invalid bboxes after the mosaic pipeline. Default to 0. •  pad_val  ( int ) – Pad value. Default to 114. ", "page_idx": 254, "bbox": [145, 193.94447326660156, 518, 296.9185485839844], "page_size": [612.0, 792.0]}
{"layout": 2392, "type": "text", "text": "get indexes ( dataset ) Call function to collect indexes. Parameters  dataset  ( Multi Image Mix Data set ) – The dataset. ", "page_idx": 254, "bbox": [96, 307.36907958984375, 399.8204040527344, 351.3143005371094], "page_size": [612.0, 792.0]}
{"layout": 2393, "type": "text", "text": "Returns  indexes. ", "page_idx": 254, "bbox": [137, 354.7118835449219, 208, 369.247314453125], "page_size": [612.0, 792.0]}
{"layout": 2394, "type": "text", "text": "Return type  list ", "page_idx": 254, "bbox": [137, 372.6448974609375, 205, 387.1803283691406], "page_size": [612.0, 792.0]}
{"layout": 2395, "type": "text", "text": "class  mmdet.datasets.pipelines. Multi Scale Flip Aug ( transforms ,  img_scale  $\\mathbf{\\dot{\\rho}}$  None ,  scale factor=None , flip=False ,  flip direction  $=$  'horizontal' ) ", "page_idx": 254, "bbox": [71.99993896484375, 391.0561218261719, 527.839599609375, 416.4705810546875], "page_size": [612.0, 792.0]}
{"layout": 2396, "type": "text", "text": "Test-time augmentation with multiple scales and flipping. ", "page_idx": 254, "bbox": [96, 415.11553955078125, 325.8773498535156, 428.4255676269531], "page_size": [612.0, 792.0]}
{"layout": 2397, "type": "text", "text": "An example configuration is as followed: ", "page_idx": 254, "bbox": [96, 433.0485534667969, 260.054443359375, 446.35858154296875], "page_size": [612.0, 792.0]}
{"layout": 2398, "type": "table", "page_idx": 254, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_142.jpg", "bbox": [94, 451, 543, 578], "page_size": [612.0, 792.0], "ocr_text": "img_scale=[(1333, 400), (1333, 800)],\n\nflip=True,\n\ntrans forms=[\ndict(type='Resize', keep_ratio=True),\ndict(type='RandomFlip'),\ndict(type='Normalize', **img_norm_cfg),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n", "vlm_text": "The image you provided is a code snippet that appears to define image processing settings typically used in a deep learning or computer vision pipeline. Here's a breakdown of the contents:\n\n- `img_scale`: A list of tuples defining the target sizes for image scaling. In this case, it includes two tuples: `(1333, 400)` and `(1333, 800)`.\n\n- `flip`: A Boolean setting, `True`, indicating that flipping the images is enabled.\n\n- `transforms`: A list of dictionaries, each specifying a different image transformation type and its corresponding parameters:\n  - `dict(type='Resize', keep_ratio=True)`: Configures the resizing transformation while keeping the aspect ratio of images.\n  - `dict(type='RandomFlip')`: Configures a random image flip transformation.\n  - `dict(type='Normalize', **img_norm_cfg)`: Configures normalization of the image using settings in `img_norm_cfg`.\n  - `dict(type='Pad', size_divisor=32)`: Configures padding of images to a size that is divisible by 32.\n  - `dict(type='ImageToTensor', keys=['img'])`: Converts the image to a tensor format.\n  - `dict(type='Collect', keys=['img'])`: Collects the image tensor for further processing.\n\nThis setup is typically used in configuring data preprocessing for image-based machine learning models."}
{"layout": 2399, "type": "text", "text": "After Multi Scale FLip Aug with above configuration, the results are wrapped into lists of the same length as followed: ", "page_idx": 254, "bbox": [96, 586.2734375, 540.0034790039062, 611.5385131835938], "page_size": [612.0, 792.0]}
{"layout": 2400, "type": "image", "page_idx": 254, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_143.jpg", "bbox": [92, 617, 542, 706], "page_size": [612.0, 792.0], "ocr_text": "dict(\nimg=[...],\nimg_shape=[...],\nscale=[(1333, 400), (1333, 400), (1333, 800), (1333, 800)]\nflip=[False, True, False, True]\n", "vlm_text": "The image shows a Python dictionary with the following structure:\n\n- `img`: Holds an unspecified list (denoted by `[...]`).\n- `img_shape`: Holds another unspecified list.\n- `scale`: Contains a list of tuples representing different scales: `(1333, 400)`, `(1333, 400)`, `(1333, 800)`, `(1333, 800)`.\n- `flip`: Contains a list of boolean values: `[False, True, False, True]`.\n\nThe dictionary seems to define some image processing parameters such as image size and whether to flip the image or not."}
{"layout": 2401, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 255, "bbox": [117, 73, 169, 84], "page_size": [612.0, 792.0]}
{"layout": 2402, "type": "text", "text": "•  transforms  ( list[dict] ) – Transforms to apply in each augmentation. •  img_scale  ( tuple | list[tuple] | None ) – Images scales for resizing. •  scale factor  ( float | list[float] | None ) – Scale factors for resizing. •  flip  ( bool ) – Whether apply flip augmentation. Default: False. •  flip direction  ( str | list[str] ) – Flip augmentation directions, options are “hori- zontal”, “vertical” and “diagonal”. If flip direction is a list, multiple flip augmentations will be applied. It has no effect when flip  $==$   False. Default: “horizontal”. ", "page_idx": 255, "bbox": [145, 89.38447570800781, 518, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 2403, "type": "text", "text": "class  mmdet.datasets.pipelines. Normalize ( mean ,  std ,  to_rg  $b{=}$  True ) Normalize the image. ", "page_idx": 255, "bbox": [72, 208.78697204589844, 385.8124084472656, 234.2014617919922], "page_size": [612.0, 792.0]}
{"layout": 2404, "type": "text", "text": "Added key is “img norm cf g”. ", "page_idx": 255, "bbox": [96, 238.8244171142578, 221, 252.1344451904297], "page_size": [612.0, 792.0]}
{"layout": 2405, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 255, "bbox": [116, 258, 170, 269], "page_size": [612.0, 792.0]}
{"layout": 2406, "type": "text", "text": "•  mean  ( sequence ) – Mean values of 3 channels. •  std  ( sequence ) – Std values of 3 channels. •  to_rgb  ( bool ) – Whether to convert the image from BGR to RGB, default is true. ", "page_idx": 255, "bbox": [145, 274.6904296875, 483, 323.865478515625], "page_size": [612.0, 792.0]}
{"layout": 2407, "type": "text", "text": "class  mmdet.datasets.pipelines. Pad ( size  $\\leftrightharpoons$  None ,  size divisor  $=$  None ,  pad to square  $\\mathbf{\\dot{\\rho}}=$  False ,  pad_val={'img': 0, 'masks': 0, 'seg': 255} ) ", "page_idx": 255, "bbox": [72, 328.3390197753906, 540, 353.6438903808594], "page_size": [612.0, 792.0]}
{"layout": 2408, "type": "text", "text": "Pad the image & masks & segmentation map. ", "page_idx": 255, "bbox": [96, 352.3984375, 278.545166015625, 365.7084655761719], "page_size": [612.0, 792.0]}
{"layout": 2409, "type": "text", "text": "There are two padding modes: (1) pad to a fixed size and (2) pad to the minimum size that is divisible by some number. Added keys are “pad_shape”, “pad fixed size”, “pad size divisor”, ", "page_idx": 255, "bbox": [96, 370.3314514160156, 540, 395.5964660644531], "page_size": [612.0, 792.0]}
{"layout": 2410, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 255, "bbox": [117, 402, 169, 413], "page_size": [612.0, 792.0]}
{"layout": 2411, "type": "text", "text": "•  size  ( tuple, optional ) – Fixed padding size. •  size divisor  ( int, optional ) – The divisor of padded size. •  pad to square  ( bool ) – Whether to pad the image into a square. Currently only used for YOLOX. Default: False. •  pad_val  ( dict, optional ) – A dict for padding value, the default value is  dict(img  $\\scriptstyle{=}0,$  , mask  $\\wp{=}0$  ,  $s e g{=}255,$  ) . ", "page_idx": 255, "bbox": [145, 418.1524658203125, 518, 509.1705017089844], "page_size": [612.0, 792.0]}
{"layout": 2412, "type": "text", "text": "class  mmdet.datasets.pipelines. PhotoMetric Distortion ( brightness delta  $\\scriptstyle{=32}$  ,  contrast range=(0.5, 1.5) ,  saturation range  $=$  (0.5, 1.5) , hue_delta  $\\scriptstyle{\\varepsilon=I8.}$  ) ", "page_idx": 255, "bbox": [72, 513.64404296875, 522.1605834960938, 550.9038696289062], "page_size": [612.0, 792.0]}
{"layout": 2413, "type": "text", "text": "Apply photometric distortion to image sequentially, every transformation is applied with a probability of 0.5. The position of random contrast is in second or second to last. ", "page_idx": 255, "bbox": [96, 549.658447265625, 540, 574.9244995117188], "page_size": [612.0, 792.0]}
{"layout": 2414, "type": "text", "text": "1. random brightness 2. random contrast (mode 0) 3. convert color from BGR to HSV 4. random saturation 5. random hue 6. convert color from HSV to BGR 7. random contrast (mode 1) 8. randomly swap channels ", "page_idx": 255, "bbox": [106, 579.5464477539062, 248, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 2415, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 256, "bbox": [118, 73, 169, 85], "page_size": [612.0, 792.0]}
{"layout": 2416, "type": "text", "text": "•  brightness delta  ( int ) – delta of brightness. •  contrast range  ( tuple ) – range of contrast. •  saturation range  ( tuple ) – range of saturation. •  hue_delta  ( int ) – delta of hue. ", "page_idx": 256, "bbox": [145, 89.38447570800781, 360.4623107910156, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 2417, "type": "text", "text": "class mmdet.datasets.pipelines.Random Affine(max rotate de gre $\\it{z}{=}10.0$ , max translate ratio $\\mathord{\\leftrightharpoons}0.I$ ,scaling ratio range $\\mathbf{\\Psi}=$ (0.5, 1.5), max shear de gre $_{z=2.0}$ ,border=(0, 0) ,  border_val  $\\leftrightharpoons$  (114, 114, 114) , min b box size  $_{:=2}$  ,  min area ratio  $\\backprime{=}0.2$  , max aspect ratio  $\\bullet{=}2O$  ) ", "page_idx": 256, "bbox": [71, 166.94395446777344, 527, 228.11488342285156], "page_size": [612.0, 792.0]}
{"layout": 2418, "type": "text", "text": "Random affine transform data augmentation. ", "page_idx": 256, "bbox": [96, 226.8694610595703, 274.56005859375, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 2419, "type": "text", "text": "This operation randomly generates affine transform matrix which including rotation, translation, shear and scaling transforms. ", "page_idx": 256, "bbox": [96, 244.8024444580078, 540, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 2420, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 256, "bbox": [117, 276, 169, 287], "page_size": [612.0, 792.0]}
{"layout": 2421, "type": "text", "text": "•  max rotate degree  ( float ) – Maximum degrees of rotation transform. Default: 10. •  max translate ratio  ( float ) – Maximum ratio of translation. Default: 0.1. •  scaling ratio range  ( tuple[float] ) – Min and max ratio of scaling transform. De- fault: (0.5, 1.5). •  max shear degree  ( float ) – Maximum degrees of shear transform. Default: 2. •  border  ( tuple[int] ) – Distance from height and width sides of input image to adjust output shape. Only used in mosaic dataset. Default:   $(0,0)$  . •  border_val  ( tuple[int] ) – Border padding values of 3 channels. Default: (114, 114, 114). •  min b box size  ( float ) – Width and height threshold to filter bboxes. If the height or width of a box is smaller than this value, it will be removed. Default: 2. •  min area ratio  ( float ) – Threshold of area ratio between original bboxes and wrapped bboxes. If smaller than this value, the box will be removed. Default: 0.2. •  max aspect ratio  ( float ) – Aspect ratio of width and height threshold to filter bboxes. If max(h/w, w/h) larger than this value, the box will be removed. ", "page_idx": 256, "bbox": [145, 292.62249755859375, 518, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 2422, "type": "text", "text": "class  mmdet.datasets.pipelines. Random Center Crop Pad ( crop_size  $\\mathbf{\\varepsilon}=$  None ,  ratios=(0.9, 1.0, 1.1) , border=128 ,  mean  $=$  None ,  std=None , to_rgb  $=$  None ,  test_mode  $:=$  False , test pad mode=('logical_or', 127) , test pad add pix $\\mathbf{\\varepsilon}_{:=0}$ , b box clip border=True)", "page_idx": 256, "bbox": [71, 525.5990600585938, 527, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 2423, "type": "text", "text": "Random center crop and random around padding for CornerNet. ", "page_idx": 256, "bbox": [96, 585.5244750976562, 353.0351867675781, 598.8345336914062], "page_size": [612.0, 792.0]}
{"layout": 2424, "type": "text", "text": "This operation generates randomly cropped image from the original image and pads it simultaneously. Different from  RandomCrop , the output shape may not equal to  crop_size  strictly. We choose a random value from ratios  and the output shape could be larger or smaller than  crop_size . The padding operation is also different from  Pad , here we use around padding instead of right-bottom padding. ", "page_idx": 256, "bbox": [96, 603.4574584960938, 540, 652.632568359375], "page_size": [612.0, 792.0]}
{"layout": 2425, "type": "text", "text": "The relation between output image (padding image) and original image: ", "page_idx": 256, "bbox": [96, 657.2554931640625, 383, 670.5655517578125], "page_size": [612.0, 792.0]}
{"layout": 2426, "type": "table", "page_idx": 256, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_144.jpg", "bbox": [92, 675, 543, 711], "page_size": [612.0, 792.0], "ocr_text": "output image\n\nFFP AR re RRR a SG\n", "vlm_text": "The table contains the text \"output image.\" There doesn't appear to be any other content in the table."}
{"layout": 2427, "type": "image", "page_idx": 257, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_145.jpg", "bbox": [92, 82, 542, 223], "page_size": [612.0, 792.0], "ocr_text": "|\n|\n| original image\n|\n|\n\n", "vlm_text": "This image is a diagram illustrating the concept of cropping and padding an image. It shows an \"original image\" section on the right and indicates how the image can be manipulated. The diagram is divided into sections labeled as \"padded area\" at the top and bottom, and a \"cropped area\" with a \"center range\" marked by a dot to suggest where the cropping might be focused within the original image. This visual representation is typically used in image processing to explain how an image might be resized or adjusted by adding padding (extra space) or cropping (cutting out parts of the image)."}
{"layout": 2428, "type": "text", "text": "There are 5 main areas in the figure: ", "page_idx": 257, "bbox": [96, 229.81044006347656, 241.0458984375, 243.12046813964844], "page_size": [612.0, 792.0]}
{"layout": 2429, "type": "text", "text": "• output image: output image of this operation, also called padding image in following instruction. • original image: input image of this operation. • padded area: non-intersect area of output image and original image. • cropped area: the overlap of output image and original image. • center range: a smaller area where random center chosen from. center range is computed by  border  and original image’s shape to avoid our random center is too close to original image’s border. ", "page_idx": 257, "bbox": [110, 247.74342346191406, 540, 344.739501953125], "page_size": [612.0, 792.0]}
{"layout": 2430, "type": "text", "text": "Also this operation act differently in train and test mode, the summary pipeline is listed below. ", "page_idx": 257, "bbox": [96, 349.3614807128906, 472, 362.6715087890625], "page_size": [612.0, 792.0]}
{"layout": 2431, "type": "text", "text": "Train pipeline: ", "page_idx": 257, "bbox": [96, 367.29449462890625, 155.34762573242188, 380.6045227050781], "page_size": [612.0, 792.0]}
{"layout": 2432, "type": "text", "text": "1. Choose a random ratio from ratios, the shape of padding image will be random ratio \\*crop_size . 2. Choose a  random center  in center range. 3. Generate padding image with center matches the  random center . 4. Initialize the padding image with pixel value equals to  mean . 5. Copy the cropped area to padding image. 6. Refine annotations. ", "page_idx": 257, "bbox": [106, 385.2275085449219, 540, 500.1565856933594], "page_size": [612.0, 792.0]}
{"layout": 2433, "type": "text", "text": "Test pipeline: ", "page_idx": 257, "bbox": [96, 504.7795715332031, 150.42604064941406, 518.089599609375], "page_size": [612.0, 792.0]}
{"layout": 2434, "type": "text", "text": "1. Compute output shape according to  test pad mode . 2. Generate padding image with center matches the original image center. 3. Initialize the padding image with pixel value equals to  mean . 4. Copy the  cropped area  to padding image. ", "page_idx": 257, "bbox": [106, 522.7115478515625, 402, 589.8206176757812], "page_size": [612.0, 792.0]}
{"layout": 2435, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 257, "bbox": [117, 602, 169, 613], "page_size": [612.0, 792.0]}
{"layout": 2436, "type": "text", "text": "•  crop_size  ( tuple | None ) – expected size after crop, final size will computed according to ratio. Requires (h, w) in train mode, and None in test mode. •  ratios  ( tuple ) – random select a ratio from tuple and crop image to (crop_size[0] \\* ratio) \\* (crop_size[1] \\* ratio). Only available in train mode. •  border  ( int ) – max distance from center select area to image border. Only available in train mode. •  mean  ( sequence ) – Mean values of 3 channels. ", "page_idx": 257, "bbox": [145, 618.3535766601562, 518, 721.3275756835938], "page_size": [612.0, 792.0]}
{"layout": 2437, "type": "text", "text": "•  std  ( sequence ) – Std values of 3 channels. ", "page_idx": 258, "bbox": [145, 71.45246887207031, 329.96746826171875, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 2438, "type": "text", "text": "•  to_rgb  ( bool ) – Whether to convert the image from BGR to RGB. •  test_mode  ( bool ) – whether involve random variables in transform. In train mode, crop_size is fixed, center coords and ratio is random selected from predefined lists. In test mode, crop_size is image’s original shape, center coords and ratio is fixed. •  test pad mode  ( tuple ) – padding method and padding shape value, only available in test mode. Default is using ‘logical_or’ with 127 as padding shape value. –  ’logical_or’: final shape  $=$   input shape | padding shape value –  ’size divisor’: final shape  $=$  int( ceil(input shape / padding shape value) \\* padding shape value)•  test pad add pix  ( int ) – Extra padding pixel in test mode. Default 0. •  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. ", "page_idx": 258, "bbox": [145, 89.38447570800781, 518, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 2439, "type": "text", "text": "class  mmdet.datasets.pipelines. RandomCrop ( crop_size ,  crop_type  $=$  'absolute' ,  allow negative crop  $\\leftrightharpoons$  False , re compute b box  $\\mathbf{\\beta}=$  False ,  b box clip border=True ) ", "page_idx": 258, "bbox": [72, 280.5180358886719, 540, 305.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 2440, "type": "text", "text": "Random crop the image & bboxes & masks. ", "page_idx": 258, "bbox": [96, 304.57745361328125, 275, 317.8874816894531], "page_size": [612.0, 792.0]}
{"layout": 2441, "type": "text", "text": "The absolute  crop_size  is sampled based on  crop_type  and  image_size , then the cropped results are generated. ", "page_idx": 258, "bbox": [96, 322.36102294921875, 534, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 2442, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 258, "bbox": [118, 342, 168, 353], "page_size": [612.0, 792.0]}
{"layout": 2443, "type": "text", "text": "•  crop_size  ( tuple ) – The relative ratio or absolute pixels of height and width. ", "page_idx": 258, "bbox": [145, 358.3764953613281, 470, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 2444, "type": "text", "text": "•  crop_type  ( str, optional ) – one of “relative range”, “relative”, “absolute”, “abso- lute_range”. “relative” randomly crops (h \\* crop_size[0], w \\* crop_size[1]) part from an input of size (h, w). “relative range” uniformly samples relative crop size from range [crop_size[0], 1] and [crop_size[1], 1] for height and width respectively. “absolute” crops from an input with absolute size (crop_size[0], crop_size[1]). “absolute range” uni- formly samples crop_h in range [crop_size[0], min(h, crop_size[1])] and crop_w in range [crop_size[0], min(w, crop_size[1])]. Default “absolute”. ", "page_idx": 258, "bbox": [145, 376.3085021972656, 518, 461.3494567871094], "page_size": [612.0, 792.0]}
{"layout": 2445, "type": "text", "text": "•  allow negative crop  ( bool, optional ) – Whether to allow a crop that does not con- tain any bbox area. Default False. ", "page_idx": 258, "bbox": [145, 465.9724426269531, 518, 491.2374572753906], "page_size": [612.0, 792.0]}
{"layout": 2446, "type": "text", "text": "•  re compute b box  ( bool, optional ) – Whether to re-compute the boxes based on cropped instance masks. Default False. ", "page_idx": 258, "bbox": [145, 495.8604431152344, 518, 521.1254272460938], "page_size": [612.0, 792.0]}
{"layout": 2447, "type": "text", "text": "•  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. ", "page_idx": 258, "bbox": [145, 525.7484130859375, 518, 551.0134887695312], "page_size": [612.0, 792.0]}
{"layout": 2448, "type": "text", "text": "Note: ", "text_level": 1, "page_idx": 258, "bbox": [96, 569, 120, 580], "page_size": [612.0, 792.0]}
{"layout": 2449, "type": "text", "text": "•  If the image is smaller than the absolute crop size, return the  original image. ", "text_level": 1, "page_idx": 258, "bbox": [109, 587, 441, 599], "page_size": [612.0, 792.0]}
{"layout": 2450, "type": "text", "text": "• The keys for bboxes, labels and masks must be aligned. That is,  gt_bboxes  corresponds to  gt_labels  and gt_masks , and  gt b boxes ignore  corresponds to  gt labels ignore  and  gt masks ignore . • If the crop does not contain any gt-bbox region and  allow negative crop  is set to False, skip this image. ", "page_idx": 258, "bbox": [110, 603.3080444335938, 540, 646.655517578125], "page_size": [612.0, 792.0]}
{"layout": 2451, "type": "text", "text": "class  mmdet.datasets.pipelines. RandomFlip ( flip_ratio  $\\leftrightharpoons$  None ,  direction  $.=$  'horizontal' ) Flip the image & bbox & mask. ", "page_idx": 258, "bbox": [72, 663.0830078125, 454.41534423828125, 688.4984741210938], "page_size": [612.0, 792.0]}
{"layout": 2452, "type": "text", "text": "If the input dict contains the key “flip”, then the flag will be used, otherwise it will be randomly decided by a ratio specified in the init method. ", "page_idx": 258, "bbox": [96, 693.1204223632812, 540, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 2453, "type": "text", "text": "When random flip is enabled,  flip_ratio / direction  can either be a float/string or tuple of float/string. There are 3 flip modes: ", "page_idx": 259, "bbox": [96, 71.45246887207031, 541, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2454, "type": "text", "text": "•  flip_ratio  is float,  direction  is string: the image will be  direction \\`\\` ly flipped with probability of  \\`\\` flip_ratio . E.g., flip_ratio  $\\scriptstyle=\\!0$  .5 , direction  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' horizontal ' , then image will be horizontally flipped with probability of 0.5. •  flip_ratio  is float,  direction  is list of string: the image will  be direction[i] \\`\\` ly flipped with probability of  \\`\\` flip_ratio/len(direction) . E.g., flip_ratio=0.5 , direction  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ ' horizontal ' ,  ' vertical ' ] , then image will be horizontally flipped with probability of 0.25, vertically with probability of 0.25. •  flip_ratio  is list of float,  direction  is list of string:  given len(flip_ratio)  $==$  len(direction) , the image will be direction[i] \\`\\` ly flipped with probability of  \\`\\` flip_ratio[i] . E.g., flip_ratio  $=$  [0.3, 0.5] , direction  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ ' horizontal ' , ' vertical ' ] , then image will be horizontally flipped with probability of 0.3, vertically with probability of 0.5. ", "page_idx": 259, "bbox": [110, 100.71282958984375, 541, 258.11260986328125], "page_size": [612.0, 792.0]}
{"layout": 2455, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 259, "bbox": [117, 270, 170, 282], "page_size": [612.0, 792.0]}
{"layout": 2456, "type": "text", "text": "•  flip_ratio  ( float | list[float], optional ) – The flipping probability. Default: None. ", "page_idx": 259, "bbox": [145, 286.64556884765625, 518, 311.91058349609375], "page_size": [612.0, 792.0]}
{"layout": 2457, "type": "text", "text": " direction  ( str | list[str], optional ) – The flipping direction. Options are ‘hori- zontal’, ‘vertical’, ‘diagonal’. Default: ‘horizontal’. If input is a list, the length must equal flip_ratio . Each element in  flip_ratio  indicates the flip probability of corresponding direction. ", "page_idx": 259, "bbox": [149.40992736816406, 316.5335693359375, 518, 365.70855712890625], "page_size": [612.0, 792.0]}
{"layout": 2458, "type": "text", "text": "bbox_flip ( bboxes ,  img_shape ,  direction ) Flip bboxes horizontally. ", "page_idx": 259, "bbox": [96, 376.16009521484375, 270.6153869628906, 401.5745544433594], "page_size": [612.0, 792.0]}
{"layout": 2459, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 259, "bbox": [136, 408, 188, 419], "page_size": [612.0, 792.0]}
{"layout": 2460, "type": "text", "text": "•  bboxes  ( numpy.ndarray ) – Bounding boxes, shape   $(\\ldots,4^{*}\\mathbf{k})$  •  img_shape  ( tuple[int] ) – Image shape (height, width) •  direction  ( str ) – Flip direction. Options are ‘horizontal’, ‘vertical’. Returns  Flipped bounding boxes. Return type  numpy.ndarray ", "page_idx": 259, "bbox": [137, 424.1295471191406, 444.7413330078125, 509.76837158203125], "page_size": [612.0, 792.0]}
{"layout": 2461, "type": "text", "text": "class  mmdet.datasets.pipelines. Random Shift ( shift ratio  $\\bullet{=}0.5$  ,  max_shift_  $_{p x=32}$  ,  filter thr px=1 ) Shift the image and box given shift pixels and probability. ", "page_idx": 259, "bbox": [71, 513.6441650390625, 504, 539.0586547851562], "page_size": [612.0, 792.0]}
{"layout": 2462, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 259, "bbox": [117, 545, 169, 557], "page_size": [612.0, 792.0]}
{"layout": 2463, "type": "text", "text": "•  shift ratio  ( float ) – Probability of shifts. Default 0.5. •  max shift px  ( int ) – The max pixels for shifting. Default 32. •  filter thr px  ( int ) – The width and height threshold for filtering. The bbox and the rest of the targets below the width and height threshold will be filtered. Default 1. ", "page_idx": 259, "bbox": [145, 561.6145629882812, 518, 622.74462890625], "page_size": [612.0, 792.0]}
{"layout": 2464, "type": "text", "text": "class  mmdet.datasets.pipelines. Resize ( img_scale  $=$  None ,  multi scale mode  $=$  range' ,  ratio range  $\\mathbf{\\beta}=$  None , keep_ratio  $\\mathbf{\\chi}=$  True ,  b box clip border  $\\scriptstyle\\mathtt{\\sim}$  True ,  backend  $\\scriptstyle{I=c\\nu2}$  ' , override  $=$  False ) ", "page_idx": 259, "bbox": [71, 627.2181396484375, 526.3256225585938, 664.47802734375], "page_size": [612.0, 792.0]}
{"layout": 2465, "type": "text", "text": "Resize images & bbox & mask. ", "page_idx": 259, "bbox": [96, 663.2335815429688, 222.24635314941406, 676.5436401367188], "page_size": [612.0, 792.0]}
{"layout": 2466, "type": "text", "text": "This transform resizes the input image to some scale. Bboxes and masks are then resized with the same scale factor. If the input dict contains the key “scale”, then the scale in the input dict is used, otherwise the specified scale in the init method is used. If the input dict contains the key “scale factor” (if Multi Scale Flip Aug does not give img_scale but scale factor), the actual scale will be computed by image shape and scale factor. ", "page_idx": 259, "bbox": [96, 681.1655883789062, 541, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 2467, "type": "text", "text": "", "page_idx": 260, "bbox": [96, 71.45246887207031, 540, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2468, "type": "text", "text": "img_scale  can either be a tuple (single-scale) or a list of tuple (multi-scale). There are 3 multiscale modes: ", "page_idx": 260, "bbox": [96, 101.19103240966797, 521, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 2469, "type": "text", "text": "•  ratio range is not None : randomly sample a ratio from the ratio range and multiply it with the image scale. •  ratio range is None  and  multi scale mode  $==$   \"range\" : randomly sample a scale from the multi- scale range. •  ratio range is None  and  multi scale mode   $==$   \"value\" : randomly sample a scale from multiple scales. ", "page_idx": 260, "bbox": [110, 119.27247619628906, 540, 204.3135223388672], "page_size": [612.0, 792.0]}
{"layout": 2470, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 260, "bbox": [117, 217, 169, 227], "page_size": [612.0, 792.0]}
{"layout": 2471, "type": "text", "text": "•  img_scale  ( tuple or list[tuple] ) – Images scales for resizing. •  multi scale mode  ( str ) – Either “range” or “value”. •  ratio range  ( tuple[float] ) – (min_ratio, max_ratio) •  keep_ratio  ( bool ) – Whether to keep the aspect ratio when resizing the image. •  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. •  backend  ( str ) – Image resize backend, choices are ‘cv2’ and ‘pillow’. These two backends generates slightly different results. Defaults to ‘cv2’. •  override  ( bool, optional ) – Whether to override  scale  and  scale factor  so as to call resize twice. Default False. If True, after the first resizing, the existed  scale  and  scale factor will be ignored so the second resizing can be allowed. This option is a work-around for multiple times of resize in DETR. Defaults to False. ", "page_idx": 260, "bbox": [145, 232.8465118408203, 521, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 2472, "type": "text", "text": "static random sample(img_scales)", "text_level": 1, "page_idx": 260, "bbox": [96, 426, 256, 436], "page_size": [612.0, 792.0]}
{"layout": 2473, "type": "text", "text": "Randomly sample an img_scale when  multi scale mode  $==$  ' range ' ", "page_idx": 260, "bbox": [118, 436.08447265625, 397.8083190917969, 449.3945007324219], "page_size": [612.0, 792.0]}
{"layout": 2474, "type": "text", "text": "Parameters  img_scales  ( list[tuple] ) – Images scale range for sampling. There must be two tuples in img_scales, which specify the lower and upper bound of image scales. Returns  Returns a tuple  (img_scale, None) , where  img_scale  is sampled scale and None is just a placeholder to be consistent with  random select() . Return type  (tuple, None) ", "page_idx": 260, "bbox": [137, 453.38983154296875, 521, 527.7012329101562], "page_size": [612.0, 792.0]}
{"layout": 2475, "type": "text", "text": "static random sample ratio ( img_scale ,  ratio range ) Randomly sample an img_scale when  ratio range  is specified. ", "page_idx": 260, "bbox": [96, 531.5770263671875, 379.34735107421875, 556.9915161132812], "page_size": [612.0, 792.0]}
{"layout": 2476, "type": "text", "text": "A ratio will be randomly sampled from the range specified by  ratio range . Then it would be multiplied with  img_scale  to generate sampled scale. ", "page_idx": 260, "bbox": [118, 561.6144409179688, 540, 586.8795166015625], "page_size": [612.0, 792.0]}
{"layout": 2477, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 260, "bbox": [136, 593, 188, 604], "page_size": [612.0, 792.0]}
{"layout": 2478, "type": "text", "text": "•  img_scale  ( tuple ) – Images scale base to multiply with ratio. •  ratio range  ( tuple[float] ) – The minimum and maximum ratio to scale the img_scale . Returns  Returns a tuple  (scale, None) , where  scale  is sampled ratio multiplied with img_scale  and None is just a placeholder to be consistent with  random select() . Return type  (tuple, None) ", "page_idx": 260, "bbox": [137, 609.4344482421875, 521, 701.0512084960938], "page_size": [612.0, 792.0]}
{"layout": 2479, "type": "text", "text": "static random select(img_scales)Randomly select an img_scale from given candidates. ", "page_idx": 261, "bbox": [96, 71.30303192138672, 333.21917724609375, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2480, "type": "text", "text": "Parameters  img_scales  ( list[tuple] ) – Images scales for selection. Returns  Returns a tuple  (img_scale, scale_dix) , where  img_scale  is the selected image scale and  scale_idx  is the selected index in the given candidates. Return type  (tuple, int) ", "page_idx": 261, "bbox": [137, 100.71282958984375, 521.36865234375, 163.06826782226562], "page_size": [612.0, 792.0]}
{"layout": 2481, "type": "text", "text": "class  mmdet.datasets.pipelines. Rotate ( level ,  scale  $\\scriptstyle{\\varepsilon=I}$  ,  center  $=$  None ,  img fill va  $l{=}I28$  , seg ignore la be  $l{=}255$  ,  prob  $\\bullet{=}0.5$  ,  max rotate angle  $\\==30$  , random negative pro b $\\backsimeq\\!O.5.$ )", "page_idx": 261, "bbox": [71, 166.9440155029297, 503, 204.2039337158203], "page_size": [612.0, 792.0]}
{"layout": 2482, "type": "text", "text": "Apply Rotate Transformation to image (and its corresponding bbox, mask, segmentation). ", "page_idx": 261, "bbox": [96, 202.95948791503906, 454, 216.26951599121094], "page_size": [612.0, 792.0]}
{"layout": 2483, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 261, "bbox": [117, 222, 169, 234], "page_size": [612.0, 792.0]}
{"layout": 2484, "type": "text", "text": "•  level  ( int | float ) – The level should be in range (0,_MAX_LEVEL]. •  scale  ( int | float ) – Isotropic scale factor. Same in  mmcv.imrotate . •  center  ( int | float | tuple[float] ) – Center point (w, h) of the rotation in the source image. If None, the center of the image will be used. Same in  mmcv.imrotate . •  img fill val  ( int | float | tuple ) – The fill value for image border. If float, the same value will be used for all the three channels of image. If tuple, the should be 3 elements (e.g. equals the number of channels for image). •  seg ignore label  ( int ) – The fill value used for segmentation map. Note this value must equals  ignore label  in  semantic head  of the corresponding config. Default 255. •  prob  ( float ) – The probability for perform transformation and should be in range 0 to 1. •  max rotate angle  ( int | float ) – The maximum angles for rotate transformation. •  random negative pro b  ( float ) – The probability that turns the offset negative. ", "page_idx": 261, "bbox": [145, 238.82447814941406, 518, 425.4845275878906], "page_size": [612.0, 792.0]}
{"layout": 2485, "type": "text", "text": "class  mmdet.datasets.pipelines. SegRescale ( scale factor  ${\\bf\\Xi}_{=I}$  ,  backend='cv2' ) Rescale semantic segmentation maps. ", "page_idx": 261, "bbox": [71, 429.95806884765625, 420, 455.3725280761719], "page_size": [612.0, 792.0]}
{"layout": 2486, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 261, "bbox": [117, 461, 169, 473], "page_size": [612.0, 792.0]}
{"layout": 2487, "type": "text", "text": "•  scale factor  ( float ) – The scale factor of the final output. •  backend    $(s t r)$   – Image rescale backend, choices are ‘cv2’ and ‘pillow’. These two backends generates slightly different results. Defaults to ‘cv2’. ", "page_idx": 261, "bbox": [145, 477.9275207519531, 518, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 2488, "type": "text", "text": "class  mmdet.datasets.pipelines. Shear ( level ,  img fill val  $\\scriptstyle{\\prime=I28}$  ,  seg ignore la be  $l{=}255$  ,  prob  $\\bullet{=}0.5$  , direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  max shear magnitude  $\\mathrm{\\Sigma=}0.3$  , random negative pro b  $\\bullet{=}0.5$  ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'bilinear' ) ", "page_idx": 261, "bbox": [71, 525.5990600585938, 503, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 2489, "type": "text", "text": "Apply Shear Transformation to image (and its corresponding bbox, mask, segmentation). ", "page_idx": 261, "bbox": [96, 561.614501953125, 454, 574.924560546875], "page_size": [612.0, 792.0]}
{"layout": 2490, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 261, "bbox": [117, 581, 168, 593], "page_size": [612.0, 792.0]}
{"layout": 2491, "type": "text", "text": "•  level  ( int | float ) – The level should be in range [0,_MAX_LEVEL]. •  img fill val  ( int | float | tuple ) – The filled values for image border. If float, the same fill value will be used for all the three channels of image. If tuple, the should be 3 elements. •  seg ignore label  ( int ) – The fill value used for segmentation map. Note this value must equals  ignore label  in  semantic head  of the corresponding config. Default 255. •  prob  ( float ) – The probability for performing Shear and should be in range [0, 1]. •  direction  ( str ) – The direction for shear, either “horizontal” or “vertical”. ", "page_idx": 261, "bbox": [145, 597.4794921875, 518, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 2492, "type": "text", "text": "•  max shear magnitude  ( float ) – The maximum magnitude for Shear transformation. •  random negative pro b  ( float ) – The probability that turns the offset negative. Should be in range [0,1] •  interpolation  ( str ) – Same as in  mmcv.imshear() . ", "page_idx": 262, "bbox": [145, 71.45246887207031, 519, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 2493, "type": "text", "text": "class  mmdet.datasets.pipelines. To Data Container ( fields  $=$  ({'key': 'img', 'stack': True}, {'key': 'gt_bboxes'}, {'key': 'gt_labels'}) ) ", "page_idx": 262, "bbox": [71, 137.05601501464844, 538.2506103515625, 162.3609161376953], "page_size": [612.0, 792.0]}
{"layout": 2494, "type": "text", "text": "Parameters  fields  ( Sequence[dict] ) – Each field is a dict like dict  $\\mathbf{(k e y='x x x'}$  , \\*\\*kwargs) . The  key  in result will be converted to  mmcv.Data Container  with \\*\\*kwargs . Default: (dict(key  $\\acute{=}$  ' img ' , stack  $\\circeq$  True), dict(key= ' gt_bboxes ' ), dict(key= ' gt_labels ' )) . ", "page_idx": 262, "bbox": [118, 178.42083740234375, 519, 228.2245330810547], "page_size": [612.0, 792.0]}
{"layout": 2495, "type": "text", "text": "class  mmdet.datasets.pipelines. ToTensor ( keys ) Convert some results to  torch.Tensor  by given keys. ", "page_idx": 262, "bbox": [71, 232.6970672607422, 315.0579528808594, 258.112548828125], "page_size": [612.0, 792.0]}
{"layout": 2496, "type": "text", "text": "Parameters  keys  ( Sequence[str] ) – Keys that need to be converted to Tensor. ", "page_idx": 262, "bbox": [118, 262.10687255859375, 445.2491149902344, 276.6423034667969], "page_size": [612.0, 792.0]}
{"layout": 2497, "type": "text", "text": "class  mmdet.datasets.pipelines. Translate ( level ,  prob  $\\bullet{=}0.5$  ,  img fill va  $l{=}I28$  ,  seg ignore label=255 , direction  $=$  'horizontal' ,  max translate offset=250.0 , random negative pro  $\\mathit{b}\\mathrm{=}0.5$  ,  min_siz  $\\scriptstyle{\\mathcal{S}}=O.$  ) ", "page_idx": 262, "bbox": [71, 280.5180969238281, 519, 317.8875427246094], "page_size": [612.0, 792.0]}
{"layout": 2498, "type": "text", "text": "Translate the images, bboxes, masks and segmentation maps horizontally or vertically. ", "page_idx": 262, "bbox": [96, 316.53350830078125, 439.96881103515625, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 2499, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 262, "bbox": [117, 336, 168, 347], "page_size": [612.0, 792.0]}
{"layout": 2500, "type": "text", "text": "•  level  ( int | float ) – The level for Translate and should be in range [0,_MAX_LEVEL]. •  prob  ( float ) – The probability for performing translation and should be in range [0, 1]. •  img fill val  ( int | float | tuple ) – The filled value for image border. If float, the same fill value will be used for all the three channels of image. If tuple, the should be 3 elements (e.g. equals the number of channels for image). •  seg ignore label  ( int ) – The fill value used for segmentation map. Note this value must equals  ignore label  in  semantic head  of the corresponding config. Default 255. •  direction  ( str ) – The translate direction, either “horizontal” or “vertical”. •  max translate offset  ( int | float ) – The maximum pixel’s offset for Translate. •  random negative pro b  ( float ) – The probability that turns the offset negative. •  min_size  ( int | float ) – The minimum pixel for filtering invalid bboxes after the trans- lation. ", "page_idx": 262, "bbox": [145, 352.3985290527344, 519, 539.05859375], "page_size": [612.0, 792.0]}
{"layout": 2501, "type": "text", "text": "class  mmdet.datasets.pipelines. Transpose ( keys ,  order ) Transpose some results by given keys. ", "page_idx": 262, "bbox": [71, 543.5321655273438, 335, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 2502, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 262, "bbox": [117, 575, 169, 586], "page_size": [612.0, 792.0]}
{"layout": 2503, "type": "text", "text": "•  keys  ( Sequence[str] ) – Keys of results to be transposed. •  order  ( Sequence[int] ) – Order of transpose. ", "page_idx": 262, "bbox": [145, 591.5015258789062, 392.1286315917969, 622.74462890625], "page_size": [612.0, 792.0]}
{"layout": 2504, "type": "text", "text": "mmdet.datasets.pipelines. to_tensor ( data ) Convert objects of various python types to  torch.Tensor . ", "page_idx": 262, "bbox": [71, 627.2181396484375, 335, 652.6326293945312], "page_size": [612.0, 792.0]}
{"layout": 2505, "type": "text", "text": "Supported types are:  numpy.ndarray ,  torch.Tensor ,  Sequence ,  int  and  float . ", "page_idx": 262, "bbox": [96, 657.2555541992188, 433.8124694824219, 670.5656127929688], "page_size": [612.0, 792.0]}
{"layout": 2506, "type": "text", "text": "Parameters  data  ( torch.Tensor | numpy.ndarray | Sequence | int | float ) – Data to be converted. ", "page_idx": 262, "bbox": [118, 674.5609130859375, 519, 700.45361328125], "page_size": [612.0, 792.0]}
{"layout": 2507, "type": "text", "text": "38.3 samplers ", "text_level": 1, "page_idx": 263, "bbox": [71, 72, 172, 88], "page_size": [612.0, 792.0]}
{"layout": 2508, "type": "text", "text": "class  mmdet.datasets.samplers. Distributed Group Sampler ( dataset ,  samples per gpu  ${=}I$  , num replicas  $=$  None ,  rank  $\\mathbf{\\beta}=$  None ,  seed  $\\mathbf{\\chi}{=}0$  ) ", "page_idx": 263, "bbox": [71, 103.16899871826172, 526.2603759765625, 128.58348083496094], "page_size": [612.0, 792.0]}
{"layout": 2509, "type": "text", "text": "Sampler that restricts data loading to a subset of the dataset. ", "page_idx": 263, "bbox": [96, 127.22846984863281, 335.75042724609375, 140.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 2510, "type": "text", "text": "It is especially useful in conjunction with  torch.nn.parallel.Distributed Data Parallel . In such case, each process can pass a Distributed Sampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it. ", "page_idx": 263, "bbox": [96, 145.1614532470703, 540, 182.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 2511, "type": "text", "text": "Note:  Dataset is assumed to be of constant size. ", "page_idx": 263, "bbox": [96, 198.331787109375, 291, 212.86721801757812], "page_size": [612.0, 792.0]}
{"layout": 2512, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 263, "bbox": [117, 236, 169, 247], "page_size": [612.0, 792.0]}
{"layout": 2513, "type": "text", "text": "•  dataset  – Dataset used for sampling. •  num replicas  ( optional ) – Number of processes participating in distributed training. •  rank  ( optional ) – Rank of the current process within num replicas. ", "page_idx": 263, "bbox": [145, 252.75746154785156, 306.8185729980469, 266.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 2514, "type": "text", "text": "", "page_idx": 263, "bbox": [145, 270.69049072265625, 506, 284.0005187988281], "page_size": [612.0, 792.0]}
{"layout": 2515, "type": "text", "text": "", "page_idx": 263, "bbox": [145, 288.6235046386719, 433.0496826171875, 301.93353271484375], "page_size": [612.0, 792.0]}
{"layout": 2516, "type": "text", "text": "•  seed  ( int, optional ) – random seed used to shuffle the sampler if  shuffle  $=$  True . This number should be identical across all processes in the distributed group. Default: 0. ", "page_idx": 263, "bbox": [145, 306.5555114746094, 518, 331.821533203125], "page_size": [612.0, 792.0]}
{"layout": 2517, "type": "text", "text": "class  mmdet.datasets.samplers. Distributed Sampler ( dataset ,  num replicas  $\\mathbf{\\hat{\\Sigma}}$  None ,  rank  $\\mathbf{\\beta}=$  None , shuffle  $\\mathbf{=}$  True ,  seed  $\\scriptstyle\\left=O\\right\\}$  ) ", "page_idx": 263, "bbox": [71, 342.2720642089844, 496.3376159667969, 367.6865234375], "page_size": [612.0, 792.0]}
{"layout": 2518, "type": "text", "text": "class  mmdet.datasets.samplers. Group Sampler ( dataset ,  samples per gpu  $\\scriptstyle{\\varepsilon=I}$  ) ", "page_idx": 263, "bbox": [71, 372.1600646972656, 418, 385.6195373535156], "page_size": [612.0, 792.0]}
{"layout": 2519, "type": "text", "text": "class  mmdet.datasets.samplers. Infinite Batch Sampler ( dataset ,  batch_size  $\\mathsf{\\Pi}_{=I}$  ,  world_size  $=$  None , rank  $\\mathbf{\\beta}=$  None ,  seed  $\\mathbf{\\chi}{=}0$  ,  shuf  ${\\mathcal{S}}{=}$  True ) ", "page_idx": 263, "bbox": [71, 390.09307861328125, 506, 415.5075378417969], "page_size": [612.0, 792.0]}
{"layout": 2520, "type": "text", "text": "Similar to  Batch Sampler  warping a  Distributed Sampler. It is designed iteration-based runners like \\`IterBase- dRunner  and yields a mini-batch indices each time. ", "page_idx": 263, "bbox": [96, 414.0030517578125, 540, 439.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 2521, "type": "text", "text": "The implementation logic is referred to  https://github.com/facebook research/detectron2/blob/main/detectron2/ data/samplers/grouped batch sampler.py ", "page_idx": 263, "bbox": [96, 444.0404968261719, 540, 469.3055114746094], "page_size": [612.0, 792.0]}
{"layout": 2522, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 263, "bbox": [117, 476, 169, 487], "page_size": [612.0, 792.0]}
{"layout": 2523, "type": "text", "text": "•  dataset  ( object ) – The dataset. ", "page_idx": 263, "bbox": [145, 491.86151123046875, 291, 505.1715393066406], "page_size": [612.0, 792.0]}
{"layout": 2524, "type": "text", "text": "•  batch_size  ( int ) – When model is  Distributed Data Parallel , it is the number of training samples on each GPU, When model is  Data Parallel , it is  num_gpus \\* sam- ple s per gpu . Default : 1. •  world_size  ( int, optional ) – Number of processes participating in distributed training. Default: None. •  rank  ( int, optional ) – Rank of current process. Default: None. •  seed  ( int ) – Random seed. Default: 0. •  shuffle  ( bool ) – Whether shuffle the dataset or not. Default: True. ", "page_idx": 263, "bbox": [145, 509.7935485839844, 518, 630.7005004882812], "page_size": [612.0, 792.0]}
{"layout": 2525, "type": "text", "text": "set_epoch ( epoch ) Not supported in  Iteration Based  runner. ", "page_idx": 263, "bbox": [96, 635.174072265625, 278.39105224609375, 660.5885620117188], "page_size": [612.0, 792.0]}
{"layout": 2526, "type": "text", "text": "class  mmdet.datasets.samplers. Infinite Group Batch Sampler ( dataset ,  batch_size  ${=}I$  ,  world_size  $=$  None , rank  $\\mathbf{\\beta}=$  None ,  seed  $\\mathsf{\\chi}\\!=\\!\\!O$  ,  shuffle  $\\mathbf{\\chi}=$  True ) Similar to  Batch Sampler  warping a  Group Sampler. It is designed for iteration-based runners like \\`IterBase- dRunner  and yields a mini-batch indices each time, all indices in a batch should be in the same group. ", "page_idx": 263, "bbox": [71, 665.06103515625, 540, 714.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 2527, "type": "text", "text": "The implementation logic is referred to  https://github.com/facebook research/detectron2/blob/main/detectron2/ data/samplers/grouped batch sampler.py ", "page_idx": 264, "bbox": [96, 71.45246887207031, 539.6348876953125, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2528, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 264, "bbox": [118, 103, 169, 114], "page_size": [612.0, 792.0]}
{"layout": 2529, "type": "text", "text": "•  dataset  ( object ) – The dataset. •  batch_size  ( int ) – When model is  Distributed Data Parallel , it is the number of training samples on each GPU. When model is  Data Parallel , it is  num_gpus \\* sam- ple s per gpu . Default : 1. •  world_size  ( int, optional ) – Number of processes participating in distributed training. Default: None. •  rank  ( int, optional ) – Rank of current process. Default: None. •  seed  ( int ) – Random seed. Default: 0. •  shuffle  ( bool ) – Whether shuffle the indices of a dummy  epoch , it should be noted that shuffle  can not guarantee that you can generate sequential indices because it need to ensure that all indices in a batch is in a group. Default: True. ", "page_idx": 264, "bbox": [145, 119.27247619628906, 518, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 2530, "type": "text", "text": "set_epoch ( epoch ) Not supported in  Iteration Based  runner. ", "page_idx": 264, "bbox": [96, 286.4960632324219, 278.3912048339844, 311.9105224609375], "page_size": [612.0, 792.0]}
{"layout": 2531, "type": "text", "text": "38.4 api wrappers ", "text_level": 1, "page_idx": 264, "bbox": [70, 334, 201, 352], "page_size": [612.0, 792.0]}
{"layout": 2532, "type": "text", "text": "class  mmdet.datasets.api wrappers. COCO ( \\*args: Any ,  \\*\\*kwargs: Any ) This class is almost the same as official py coco tools package. It implements some snake case function aliases. So that the COCO class has the same interface as LVIS class. ", "page_idx": 264, "bbox": [72.00004577636719, 367.9560241699219, 534.4246215820312, 411.3034973144531], "page_size": [612.0, 792.0]}
{"layout": 2533, "type": "text", "text": "MMDET.MODELS ", "text_level": 1, "page_idx": 266, "bbox": [422, 164, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 2534, "type": "text", "text": "39.1 detectors ", "text_level": 1, "page_idx": 266, "bbox": [70, 228, 175, 246], "page_size": [612.0, 792.0]}
{"layout": 2535, "type": "text", "text": "class  mmdet.models.detectors. ATSS ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) Implementation of  ATSS . class  mmdet.models.detectors. AutoAssign ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained=None ) Implementation of  AutoAssign: Differentiable Label Assignment for Dense Object Detection . class  mmdet.models.detectors. Base Detector ( init_cfg  $\\mathbf{\\beta}=$  None ) Base class for detectors. abstract aug_test ( imgs ,  img_metas ,  \\*\\*kwargs ) Test function with test time augmentation. abstract extract feat ( imgs ) Extract features from images. extract feats ( imgs ) Extract features from multiple images. Parameters  imgs  ( list[torch.Tensor] ) – A list of images. The images are augmented from the same image but in different ways. Returns  Features of different images Return type  list[torch.Tensor] forward ( img ,  img_metas ,  return loss  $=$  True ,  \\*\\*kwargs ) Calls either  forward train()  or  forward test()  depending on whether  return loss  is  True . Note this setting will change the expected inputs. When  return loss  $=$  True , img and img_meta are single-nested (i.e. Tensor and List[dict]), and when  res turn loss  $=$  False , img and img_meta should be double nested (i.e. List[Tensor], List[List[dict]]), with the outer list indicating test time augmentations. forward test ( imgs ,  img_metas ,  \\*\\*kwargs ) Parameters •  imgs  ( List[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains all images in the batch. ", "page_idx": 266, "bbox": [71, 260.65301513671875, 540, 674.6104736328125], "page_size": [612.0, 792.0]}
{"layout": 2536, "type": "text", "text": "flip, etc.) and the inner list indicates images in a batch. ", "page_idx": 266, "bbox": [164, 691.1884155273438, 381.847412109375, 704.4984741210938], "page_size": [612.0, 792.0]}
{"layout": 2537, "type": "text", "text": "forward train ( imgs ,  img_metas ,  \\*\\*kwargs ) ", "page_idx": 267, "bbox": [96, 71.30303192138672, 286, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 2538, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 267, "bbox": [137, 103, 187, 114], "page_size": [612.0, 792.0]}
{"layout": 2539, "type": "text", "text": "•  img  ( list[Tensor] ) – List of tensors of shape (1, C, H, W). Typically these should be mean centered and std scaled. •  img_metas  ( list[dict] ) – List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys, see  mmdet.datasets. pipelines.Collect . •  kwargs  ( keyword arguments ) – Specific to concrete implementation. ", "page_idx": 267, "bbox": [155, 119.27247619628906, 521, 216.26951599121094], "page_size": [612.0, 792.0]}
{"layout": 2540, "type": "text", "text": "show result ( img ,  result ,  score_th  $\\scriptstyle r=0.3$  ,  bbox_color=(72, 101, 241) ,  text_color=(72, 101, 241) , mask_colo  $\\leftrightharpoons$  None ,  thickness  $_{;=2}$  ,  font_siz  $_{\\mathit{z}=I3}$  ,  win_name  $\\mathbf{\\lambda}={}^{\\prime\\prime}$  ,  show  $\\mathbf{\\hat{\\rho}}$  False ,  wait_time  $\\mathrm{=}0$  , out_file  $\\leftrightharpoons$  None ) Draw  result  over  img . ", "page_idx": 267, "bbox": [96, 220.74205017089844, 511.8606262207031, 270.06756591796875], "page_size": [612.0, 792.0]}
{"layout": 2541, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 267, "bbox": [136, 276, 187, 287], "page_size": [612.0, 792.0]}
{"layout": 2542, "type": "text", "text": "•  img  ( str or Tensor ) – The image to be displayed. •  result  ( Tensor or tuple ) – The results to draw over  img  b box result or (b box result, seg m result). •  score_thr  ( float, optional ) – Minimum score of bboxes to be shown. Default: 0.3. •  bbox_color  (str or tuple(int) or  Color ) – Color of bbox lines. The tuple of color should be in BGR order. Default: ‘green’ •  text_color  (str or tuple(int) or  Color ) – Color of texts. The tuple of color should be in BGR order. Default: ‘green’ •  mask_color  (None or str or tuple(int) or  Color ) – Color of masks. The tuple of color should be in BGR order. Default: None •  thickness  ( int ) – Thickness of lines. Default: 2 •  font_size  ( int ) – Font size of texts. Default: 13 •  win_name  ( str ) – The window name. Default: ‘’ •  wait_time  ( float ) – Value of waitKey param. Default: 0. •  show  ( bool ) – Whether to show the image. Default: False. •  out_file  ( str or None ) – The filename to write the image. Default: None. ", "page_idx": 267, "bbox": [155, 292.62255859375, 521, 551.013671875], "page_size": [612.0, 792.0]}
{"layout": 2543, "type": "text", "text": "Returns  Only if not  show  or  out_file Return type  img (Tensor) ", "page_idx": 267, "bbox": [137, 555.0089721679688, 286, 587.4774169921875], "page_size": [612.0, 792.0]}
{"layout": 2544, "type": "text", "text": "train_step ( data ,  optimizer ) The iteration step during training. ", "page_idx": 267, "bbox": [96, 591.3532104492188, 253.07989501953125, 616.7676391601562], "page_size": [612.0, 792.0]}
{"layout": 2545, "type": "text", "text": "This method defines an iteration step during training, except for the back propagation and optimizer up- dating, which are done in an optimizer hook. Note that in some complicated cases or models, the whole process including back propagation and optimizer updating is also defined in this method, such as GAN. ", "page_idx": 267, "bbox": [118, 621.3895874023438, 540, 658.6106567382812], "page_size": [612.0, 792.0]}
{"layout": 2546, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 267, "bbox": [137, 664, 188, 676], "page_size": [612.0, 792.0]}
{"layout": 2547, "type": "text", "text": "•  data  ( dict ) – The output of dataloader. ", "page_idx": 267, "bbox": [155, 681.1655883789062, 325.8477783203125, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 2548, "type": "text", "text": "•  optimizer  ( torch.optim.Optimizer  | dict) – The optimizer of runner is passed to train_step() . This argument is unused and reserved. ", "page_idx": 268, "bbox": [154, 71.45246887207031, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2549, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 268, "bbox": [136, 103, 173, 114], "page_size": [612.0, 792.0]}
{"layout": 2550, "type": "text", "text": "It should contain at least 3 keys:  loss ,  log_vars ,  num samples . •  loss  is a tensor for back propagation, which can be a weighted sum of multiple losses. •  log_vars  contains all the variables to be sent to the logger. •  num samples  indicates the batch size (when the model is DDP, it means the batch size on each GPU), which is used for averaging the logs. ", "page_idx": 268, "bbox": [154, 119.27247619628906, 521, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 2551, "type": "text", "text": "Return type  dict ", "text_level": 1, "page_idx": 268, "bbox": [136, 203, 210, 216], "page_size": [612.0, 792.0]}
{"layout": 2552, "type": "text", "text": "val_step ( data ,  optimize  $r{=}$  None ) The iteration step during validation. This method shares the same signature as  train_step() , but used during val epochs. Note that the eval- uation after training epochs is not implemented with this method, but an evaluation hook. ", "page_idx": 268, "bbox": [96, 220.7419891357422, 540.00048828125, 276.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 2553, "type": "text", "text": "property with_bbox whether the detector has a bbox head ", "page_idx": 268, "bbox": [96, 282.3909912109375, 267, 305.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 2554, "type": "text", "text": "Type  bool ", "page_idx": 268, "bbox": [137, 309.9278259277344, 180, 324.4632568359375], "page_size": [612.0, 792.0]}
{"layout": 2555, "type": "text", "text": "property with_mask whether the detector has a mask head ", "page_idx": 268, "bbox": [96, 330.2120056152344, 267, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 2556, "type": "text", "text": "property with_neck whether the detector has a neck ", "page_idx": 268, "bbox": [96, 378.0320129394531, 243, 401.57452392578125], "page_size": [612.0, 792.0]}
{"layout": 2557, "type": "text", "text": "Type  bool ", "page_idx": 268, "bbox": [137, 405.56884765625, 180, 420.1042785644531], "page_size": [612.0, 792.0]}
{"layout": 2558, "type": "text", "text": "property with shared head whether the detector has a shared head in the RoI Head ", "page_idx": 268, "bbox": [96, 425.85302734375, 338.14166259765625, 449.39453125], "page_size": [612.0, 792.0]}
{"layout": 2559, "type": "text", "text": "Type  bool ", "page_idx": 268, "bbox": [137, 453.3898620605469, 180, 467.92529296875], "page_size": [612.0, 792.0]}
{"layout": 2560, "type": "text", "text": "class  mmdet.models.detectors. Cascade R CNN ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  rpn_head=None ,  roi_head  $\\leftrightharpoons$  None , train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  pretrained  $\\leftrightharpoons$  None , init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) Implementation of  Cascade R-CNN: Delving into High Quality Object Detection ", "page_idx": 268, "bbox": [71, 471.80108642578125, 516.2936401367188, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 2561, "type": "text", "text": "show result ( data ,  result ,  \\*\\*kwargs ) Show prediction results of the detector. ", "page_idx": 268, "bbox": [96, 525.5990600585938, 274.5901794433594, 551.0134887695312], "page_size": [612.0, 792.0]}
{"layout": 2562, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 268, "bbox": [136, 557, 188, 569], "page_size": [612.0, 792.0]}
{"layout": 2563, "type": "text", "text": "•  data  ( str or np.ndarray ) – Image filename or loaded image. •  result  ( Tensor or tuple ) – The results to draw over  img  b box result or (b box result, seg m result). Returns  The image with bboxes drawn on it. ", "page_idx": 268, "bbox": [137, 573.5694580078125, 521, 635.2972412109375], "page_size": [612.0, 792.0]}
{"layout": 2564, "type": "text", "text": "Return type  np.ndarray ", "page_idx": 268, "bbox": [137, 638.69482421875, 236, 653.230224609375], "page_size": [612.0, 792.0]}
{"layout": 2565, "type": "text", "text": "class  mmdet.models.detectors. CenterNet ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $=$  None ) ", "page_idx": 268, "bbox": [71, 657.1060180664062, 521, 682.5205078125], "page_size": [612.0, 792.0]}
{"layout": 2566, "type": "text", "text": "Implementation of CenterNet(Objects as Points) < https://arxiv.org/abs/1904.07850 >. ", "page_idx": 268, "bbox": [96, 681.1654663085938, 289.0953674316406, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 2567, "type": "text", "text": "aug_test ( imgs ,  img_metas ,  rescale  $=$  True ) Augment testing of CenterNet. Aug test must have flipped image pair, and unlike CornerNet, it will perform an averaging operation on the feature map instead of detecting bbox. ", "page_idx": 269, "bbox": [96, 71.30303192138672, 540.0028686523438, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 2568, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 269, "bbox": [136, 114, 188, 126], "page_size": [612.0, 792.0]}
{"layout": 2569, "type": "text", "text": "•  imgs  ( list[Tensor] ) – Augmented images. •  img_metas  ( list[list[dict]] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: True. ", "page_idx": 269, "bbox": [154, 131.2284698486328, 521, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 2570, "type": "text", "text": "Note:  imgs  must including flipped image pairs. ", "page_idx": 269, "bbox": [118, 208.308837890625, 312, 222.84426879882812], "page_size": [612.0, 792.0]}
{"layout": 2571, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 269, "bbox": [136, 246, 173, 258], "page_size": [612.0, 792.0]}
{"layout": 2572, "type": "text", "text": "BBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. ", "page_idx": 269, "bbox": [154, 262.1068115234375, 521, 288.00048828125], "page_size": [612.0, 792.0]}
{"layout": 2573, "type": "text", "text": "Return type  list[list[np.ndarray]] ", "page_idx": 269, "bbox": [137, 291.99481201171875, 274, 306.5302429199219], "page_size": [612.0, 792.0]}
{"layout": 2574, "type": "text", "text": "merge aug results ( aug results ,  with_nms ) Merge augmented detection bboxes and score. ", "page_idx": 269, "bbox": [96, 316.384033203125, 302.5941162109375, 341.7984924316406], "page_size": [612.0, 792.0]}
{"layout": 2575, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 269, "bbox": [136, 348, 188, 359], "page_size": [612.0, 792.0]}
{"layout": 2576, "type": "text", "text": "•  aug results  ( list[list[Tensor]] ) – Det_bboxes and det_labels of each image. ", "page_idx": 269, "bbox": [154, 364.3534851074219, 504.1585388183594, 377.66351318359375], "page_size": [612.0, 792.0]}
{"layout": 2577, "type": "text", "text": "•  with_nms  ( bool ) – If True, do nms before return boxes. ", "page_idx": 269, "bbox": [154, 382.2864990234375, 390, 395.5965270996094], "page_size": [612.0, 792.0]}
{"layout": 2578, "type": "text", "text": "Returns  (out_bboxes, out_labels) ", "page_idx": 269, "bbox": [137, 399.59185791015625, 274, 414.1272888183594], "page_size": [612.0, 792.0]}
{"layout": 2579, "type": "text", "text": "Return type  tuple ", "page_idx": 269, "bbox": [137, 417.5248718261719, 213.2501220703125, 432.060302734375], "page_size": [612.0, 792.0]}
{"layout": 2580, "type": "text", "text": "class  mmdet.models.detectors. CornerNet ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 269, "bbox": [71.99992370605469, 435.9350891113281, 521, 461.34954833984375], "page_size": [612.0, 792.0]}
{"layout": 2581, "type": "text", "text": "CornerNet. ", "page_idx": 269, "bbox": [96, 459.9955139160156, 141.49948120117188, 473.3055419921875], "page_size": [612.0, 792.0]}
{"layout": 2582, "type": "text", "text": "This detector is the implementation of the paper  CornerNet: Detecting Objects as Paired Keypoints ", "page_idx": 269, "bbox": [96, 477.9275207519531, 491.9338684082031, 491.237548828125], "page_size": [612.0, 792.0]}
{"layout": 2583, "type": "text", "text": "aug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\dot{\\rho}}=$  False ) Augment testing of CornerNet. ", "page_idx": 269, "bbox": [96, 495.7110900878906, 277.5992736816406, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 2584, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 269, "bbox": [136, 527, 188, 539], "page_size": [612.0, 792.0]}
{"layout": 2585, "type": "text", "text": "•  imgs  ( list[Tensor] ) – Augmented images. •  img_metas  ( list[list[dict]] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. ", "page_idx": 269, "bbox": [154, 543.6814575195312, 521, 604.8115844726562], "page_size": [612.0, 792.0]}
{"layout": 2586, "type": "text", "text": "Note:  imgs  must including flipped image pairs. ", "page_idx": 269, "bbox": [118, 620.7618408203125, 312, 635.2972412109375], "page_size": [612.0, 792.0]}
{"layout": 2587, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 269, "bbox": [136, 658, 172, 670], "page_size": [612.0, 792.0]}
{"layout": 2588, "type": "text", "text": "BBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. ", "page_idx": 269, "bbox": [154, 674.560791015625, 521, 700.4534912109375], "page_size": [612.0, 792.0]}
{"layout": 2589, "type": "text", "text": "Return type  list[list[np.ndarray]] ", "page_idx": 269, "bbox": [137, 704.4487915039062, 274, 718.9841918945312], "page_size": [612.0, 792.0]}
{"layout": 2590, "type": "text", "text": "merge aug results ( aug results ,  img_metas ) Merge augmented detection bboxes and score. ", "page_idx": 270, "bbox": [96, 71.30303192138672, 302.5941162109375, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2591, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 270, "bbox": [136, 103, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 2592, "type": "text", "text": "•  aug results  ( list[list[Tensor]] ) – Det_bboxes and det_labels of each image. •  img_metas  ( list[list[dict]] ) – Meta information of each image, e.g., image size, scaling factor, etc. ", "page_idx": 270, "bbox": [154, 119.27247619628906, 521, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 2593, "type": "text", "text": "Returns  (bboxes, labels) ", "page_idx": 270, "bbox": [137, 166.4658203125, 238, 181.00125122070312], "page_size": [612.0, 792.0]}
{"layout": 2594, "type": "text", "text": "Return type  tuple ", "page_idx": 270, "bbox": [137, 184.3988037109375, 213, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 2595, "type": "text", "text": "class  mmdet.models.detectors. DETR ( backbone ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg  $=$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) Implementation of  DETR: End-to-End Object Detection with Transformers ", "page_idx": 270, "bbox": [71, 202.8099822998047, 470.70458984375, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 2596, "type": "text", "text": "forward dummy ( img ) ", "text_level": 1, "page_idx": 270, "bbox": [95, 246, 191, 257], "page_size": [612.0, 792.0]}
{"layout": 2597, "type": "text", "text": "Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py ", "page_idx": 270, "bbox": [118, 256.7574768066406, 317, 288.0005187988281], "page_size": [612.0, 792.0]}
{"layout": 2598, "type": "text", "text": "on nx export ( img ,  img_metas ) Test function for exporting to ONNX, without test time augmentation. ", "page_idx": 270, "bbox": [96, 292.4730529785156, 397, 317.88751220703125], "page_size": [612.0, 792.0]}
{"layout": 2599, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 270, "bbox": [136, 324, 188, 335], "page_size": [612.0, 792.0]}
{"layout": 2600, "type": "text", "text": "•  img  ( torch.Tensor ) – input images. •  img_metas  ( list[dict] ) – List of image information. ", "page_idx": 270, "bbox": [154, 340.4435119628906, 388, 371.6865539550781], "page_size": [612.0, 792.0]}
{"layout": 2601, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 270, "bbox": [136, 377, 173, 389], "page_size": [612.0, 792.0]}
{"layout": 2602, "type": "text", "text": "dets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. Return type  tuple[Tensor, Tensor] ", "page_idx": 270, "bbox": [137, 393.6138916015625, 439.3216552734375, 426.08233642578125], "page_size": [612.0, 792.0]}
{"layout": 2603, "type": "text", "text": "class  mmdet.models.detectors. De formable DE TR ( \\*args ,  \\*\\*kwargs ) ", "page_idx": 270, "bbox": [71, 429.9581298828125, 373.29437255859375, 443.4176025390625], "page_size": [612.0, 792.0]}
{"layout": 2604, "type": "text", "text": "class  mmdet.models.detectors. FCOS ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) ", "page_idx": 270, "bbox": [71, 447.89013671875, 493, 473.30560302734375], "page_size": [612.0, 792.0]}
{"layout": 2605, "type": "text", "text": "Implementation of  FCOS ", "page_idx": 270, "bbox": [96, 471.9505615234375, 198, 485.2605895996094], "page_size": [612.0, 792.0]}
{"layout": 2606, "type": "text", "text": "class  mmdet.models.detectors. FOVEA ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ) ", "page_idx": 270, "bbox": [71, 489.734130859375, 498.76959228515625, 515.1486206054688], "page_size": [612.0, 792.0]}
{"layout": 2607, "type": "text", "text": "Implementation of  FoveaBox ", "page_idx": 270, "bbox": [96, 513.7935180664062, 213, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 2608, "type": "text", "text": "class  mmdet.models.detectors. FSAF ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\beta}=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 270, "bbox": [71, 531.5770874023438, 493, 556.9915771484375], "page_size": [612.0, 792.0]}
{"layout": 2609, "type": "text", "text": "Implementation of  FSAF ", "page_idx": 270, "bbox": [96, 555.6365356445312, 198, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 2610, "type": "text", "text": "class  mmdet.models.detectors. FastRCNN ( backbone ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 270, "bbox": [71, 573.4201049804688, 479.2625427246094, 598.8345947265625], "page_size": [612.0, 792.0]}
{"layout": 2611, "type": "text", "text": "Implementation of  Fast R-CNN ", "page_idx": 270, "bbox": [96, 597.4795532226562, 222.37582397460938, 610.7896118164062], "page_size": [612.0, 792.0]}
{"layout": 2612, "type": "text", "text": "forward test ( imgs ,  img_metas ,  proposals ,  \\*\\*kwargs ) ", "page_idx": 270, "bbox": [96, 615.2631225585938, 324.3182373046875, 628.7225952148438], "page_size": [612.0, 792.0]}
{"layout": 2613, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 270, "bbox": [136, 646, 188, 658], "page_size": [612.0, 792.0]}
{"layout": 2614, "type": "text", "text": "•  imgs  ( List[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains all images in the batch. •  img_metas  ( List[List[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. ", "page_idx": 270, "bbox": [154, 663.2335205078125, 521, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 2615, "type": "text", "text": "•  proposals  ( List[List[Tensor]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. The Tensor should have a shape  $\\mathrm{Px4}$  , where P is the number of proposals. ", "page_idx": 271, "bbox": [155.88499450683594, 71.45246887207031, 522, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 2616, "type": "text", "text": "class  mmdet.models.detectors. FasterRCNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck  $\\mathbf{\\beta}=$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 271, "bbox": [71, 113.14604949951172, 533.0405883789062, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 2617, "type": "text", "text": "Implementation of  Faster R-CNN ", "page_idx": 271, "bbox": [96, 137.2055206298828, 230.11680603027344, 150.5155487060547], "page_size": [612.0, 792.0]}
{"layout": 2618, "type": "text", "text": "class  mmdet.models.detectors. GFL ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) ", "page_idx": 271, "bbox": [71, 154.9890594482422, 488.3084716796875, 180.40354919433594], "page_size": [612.0, 792.0]}
{"layout": 2619, "type": "text", "text": "class  mmdet.models.detectors. GridRCNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 271, "bbox": [71, 184.87705993652344, 522, 210.2915496826172], "page_size": [612.0, 792.0]}
{"layout": 2620, "type": "text", "text": "Grid R-CNN. ", "page_idx": 271, "bbox": [96, 208.93653869628906, 150.64495849609375, 222.24656677246094], "page_size": [612.0, 792.0]}
{"layout": 2621, "type": "text", "text": "This detector is the implementation of: - Grid R-CNN ( https://arxiv.org/abs/1811.12030 ) - Grid R-CNN Plus: Faster and Better ( https://arxiv.org/abs/1906.05688 ) ", "page_idx": 271, "bbox": [96, 226.86952209472656, 540.0029907226562, 252.1345672607422], "page_size": [612.0, 792.0]}
{"layout": 2622, "type": "text", "text": "class  mmdet.models.detectors. Hybrid Task Cascade ( \\*\\*kwargs ) Implementation of  HTC ", "page_idx": 271, "bbox": [71, 256.60809326171875, 361.40911865234375, 282.0225830078125], "page_size": [612.0, 792.0]}
{"layout": 2623, "type": "text", "text": "property with semantic whether the detector has a semantic head ", "page_idx": 271, "bbox": [96, 288.36907958984375, 282.0520324707031, 311.91058349609375], "page_size": [612.0, 792.0]}
{"layout": 2624, "type": "text", "text": "Type  bool ", "page_idx": 271, "bbox": [137.45474243164062, 315.9059143066406, 180.69223022460938, 330.44134521484375], "page_size": [612.0, 792.0]}
{"layout": 2625, "type": "text", "text": "class  mmdet.models.detectors. Knowledge Distillation Single Stage Detector ( backbone ,  neck , ", "page_idx": 271, "bbox": [71, 334.3161315917969, 501.42840576171875, 347.7756042480469], "page_size": [612.0, 792.0]}
{"layout": 2626, "type": "text", "text": "Implementation of  Distilling the Knowledge in a Neural Network. . ", "page_idx": 271, "bbox": [96.90673828125, 430.10748291015625, 361.35394287109375, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 2627, "type": "text", "text": "bbox_head , teacher config , teacher ck pt=None , e val teach e  $\\asymp$  True , train_cfg  $\\mathbf{\\beta}=$  None , test_cfg  $=$  None , pretrained  $\\leftrightharpoons$  None ) ", "page_idx": 271, "bbox": [438, 346.2721252441406, 518, 431.3529357910156], "page_size": [612.0, 792.0]}
{"layout": 2628, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 271, "bbox": [117, 450, 169, 461], "page_size": [612.0, 792.0]}
{"layout": 2629, "type": "text", "text": "•  teacher config  ( str | dict ) – Config file path or the config object of teacher model. ", "page_idx": 271, "bbox": [145, 465.9725036621094, 512.8255004882812, 479.28253173828125], "page_size": [612.0, 792.0]}
{"layout": 2630, "type": "text", "text": "•  teacher ck pt  ( str, optional ) – Checkpoint path of teacher model. If left as None, the model will not load any weights. ", "page_idx": 271, "bbox": [145, 483.905517578125, 518.0823364257812, 509.1705322265625], "page_size": [612.0, 792.0]}
{"layout": 2631, "type": "text", "text": "cuda ( device=None ) Since teacher model is registered as a plain object, it is necessary to put the teacher model to cuda when calling cuda function. ", "page_idx": 271, "bbox": [96, 513.6441040039062, 540.0035400390625, 551.0135498046875], "page_size": [612.0, 792.0]}
{"layout": 2632, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $:=$  None ) ", "page_idx": 271, "bbox": [96, 555.487060546875, 428.22406005859375, 568.946533203125], "page_size": [612.0, 792.0]}
{"layout": 2633, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 271, "bbox": [136, 587, 188, 598], "page_size": [612.0, 792.0]}
{"layout": 2634, "type": "text", "text": "•  img  ( Tensor ) – Input images of shape (N, C, H, W). Typically these should be mean cen- tered and std scaled. •  img_metas  ( list[dict] ) – A List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. ", "page_idx": 271, "bbox": [154, 603.4574584960938, 521, 712.4085693359375], "page_size": [612.0, 792.0]}
{"layout": 2635, "type": "text", "text": "•  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box ", "page_idx": 272, "bbox": [155.88499450683594, 71.45246887207031, 454.36956787109375, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 2636, "type": "text", "text": " gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 272, "bbox": [159.37188720703125, 89.38447570800781, 522, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 2637, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 272, "bbox": [137, 118.64483642578125, 308, 133.18026733398438], "page_size": [612.0, 792.0]}
{"layout": 2638, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 272, "bbox": [137, 136.57781982421875, 256, 151.11325073242188], "page_size": [612.0, 792.0]}
{"layout": 2639, "type": "text", "text": "train ( mode  $=$  True ) Set the same train mode for teacher and student model. ", "page_idx": 272, "bbox": [96, 154.98899841308594, 337.26397705078125, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 2640, "type": "text", "text": "class  mmdet.models.detectors. MaskRCNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 272, "bbox": [71, 184.8769989013672, 522, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 2641, "type": "text", "text": "Implementation of  Mask R-CNN ", "page_idx": 272, "bbox": [96, 208.9364776611328, 228, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 2642, "type": "text", "text": "class  mmdet.models.detectors. Mask Scoring R CNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg , neck  $\\mathbf{\\beta}=$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 272, "bbox": [71, 226.7200164794922, 509, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 2643, "type": "text", "text": "Mask Scoring RCNN. ", "page_idx": 272, "bbox": [96, 250.7794952392578, 184.78692626953125, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 2644, "type": "text", "text": "https://arxiv.org/abs/1903.00241 ", "page_idx": 272, "bbox": [96, 268.7124938964844, 228, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 2645, "type": "text", "text": "class  mmdet.models.detectors. NASFCOS ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) NAS-FCOS: Fast Neural Architecture Search for Object Detection. ", "page_idx": 272, "bbox": [71, 286.4960632324219, 509, 323.8655090332031], "page_size": [612.0, 792.0]}
{"layout": 2646, "type": "text", "text": "https://arxiv.org/abs/1906.0442 ", "page_idx": 272, "bbox": [96, 328.4884948730469, 222.35581970214844, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 2647, "type": "text", "text": "class  mmdet.models.detectors. PAA ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) ", "page_idx": 272, "bbox": [71, 346.2720642089844, 489, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 2648, "type": "text", "text": "Implementation of  PAA . ", "page_idx": 272, "bbox": [96, 370.33148193359375, 194.79920959472656, 383.6415100097656], "page_size": [612.0, 792.0]}
{"layout": 2649, "type": "text", "text": "class  mmdet.models.detectors. Pan optic FP N ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  rpn_head=None ,  roi_head=None , train_cfg  $=$  None ,  test_cfg  $\\leftrightharpoons$  None ,  pretrained=None , init_cfg  $=$  None ,  semantic head  $\\leftrightharpoons$  None , pan optic fusion head  $\\leftrightharpoons$  None ) ", "page_idx": 272, "bbox": [71, 388.11505126953125, 516.2933959960938, 437.32989501953125], "page_size": [612.0, 792.0]}
{"layout": 2650, "type": "text", "text": "Implementation of  Panoptic feature pyramid networks ", "page_idx": 272, "bbox": [96, 436.0844421386719, 311.8099060058594, 449.39447021484375], "page_size": [612.0, 792.0]}
{"layout": 2651, "type": "text", "text": "class  mmdet.models.detectors. PointRend ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) ", "page_idx": 272, "bbox": [71, 453.8680114746094, 527, 479.282470703125], "page_size": [612.0, 792.0]}
{"layout": 2652, "type": "text", "text": "PointRend: Image Segmentation as Rendering This detector is the implementation of  PointRend . ", "page_idx": 272, "bbox": [96, 477.92742919921875, 295.38153076171875, 509.17047119140625], "page_size": [612.0, 792.0]}
{"layout": 2653, "type": "text", "text": "class  mmdet.models.detectors. QueryInst ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $=$  None ) ", "page_idx": 272, "bbox": [71, 513.6439819335938, 527, 539.0584716796875], "page_size": [612.0, 792.0]}
{"layout": 2654, "type": "text", "text": "Implementation of  Instances as Queries ", "page_idx": 272, "bbox": [96, 537.7034301757812, 256, 551.0134887695312], "page_size": [612.0, 792.0]}
{"layout": 2655, "type": "text", "text": "class  mmdet.models.detectors. RPN ( backbone ,  neck ,  rpn_head ,  train_cfg ,  test_cfg ,  pretrained=None , init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ) Implementation of Region Proposal Network. ", "page_idx": 272, "bbox": [71, 555.4869995117188, 502.066162109375, 592.8565063476562], "page_size": [612.0, 792.0]}
{"layout": 2656, "type": "text", "text": "aug_test ( imgs ,  img_metas ,  rescale  $=$  False ) Test function with test time augmentation. ", "page_idx": 272, "bbox": [96, 597.3300170898438, 286.48504638671875, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 2657, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 272, "bbox": [135, 629, 189, 641], "page_size": [612.0, 792.0]}
{"layout": 2658, "type": "text", "text": "Returns  proposals ", "page_idx": 272, "bbox": [137, 698.4708251953125, 214.63510131835938, 713.0062255859375], "page_size": [612.0, 792.0]}
{"layout": 2659, "type": "text", "text": "Return type  list[np.ndarray] ", "page_idx": 273, "bbox": [137, 70.8248291015625, 256, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 2660, "type": "text", "text": "extract feat ( img ) ", "page_idx": 273, "bbox": [96, 89.23503875732422, 185, 102.58492279052734], "page_size": [612.0, 792.0]}
{"layout": 2661, "type": "text", "text": "Extract features. Parameters  img  ( torch.Tensor ) – Image tensor with shape (n, c, h ,w). ", "page_idx": 273, "bbox": [118, 101.34046936035156, 434.3208312988281, 133.18026733398438], "page_size": [612.0, 792.0]}
{"layout": 2662, "type": "text", "text": "Returns Multi-level features that may have  different resolutions. ", "page_idx": 273, "bbox": [137, 136.57781982421875, 388, 169.04623413085938], "page_size": [612.0, 792.0]}
{"layout": 2663, "type": "text", "text": "Return type  list[torch.Tensor] ", "page_idx": 273, "bbox": [137, 172.44378662109375, 261, 186.97921752929688], "page_size": [612.0, 792.0]}
{"layout": 2664, "type": "text", "text": "forward dummy ( img ) Dummy forward function. ", "page_idx": 273, "bbox": [96, 190.85398864746094, 222.9430694580078, 216.2684783935547], "page_size": [612.0, 792.0]}
{"layout": 2665, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes  $\\leftrightharpoons$  None ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) ", "page_idx": 273, "bbox": [96, 220.7419891357422, 414.21630859375, 234.2014617919922], "page_size": [612.0, 792.0]}
{"layout": 2666, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 273, "bbox": [136, 252, 187, 264], "page_size": [612.0, 792.0]}
{"layout": 2667, "type": "text", "text": "•  img  ( Tensor ) – Input images of shape (N, C, H, W). Typically these should be mean cen- tered and std scaled. •  img_metas  ( list[dict] ) – A List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 273, "bbox": [154, 268.7124328613281, 521, 407.5514221191406], "page_size": [612.0, 792.0]}
{"layout": 2668, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 273, "bbox": [137, 411.5467529296875, 308.6416931152344, 426.0821838378906], "page_size": [612.0, 792.0]}
{"layout": 2669, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 273, "bbox": [137, 429.4797668457031, 256, 444.01519775390625], "page_size": [612.0, 792.0]}
{"layout": 2670, "type": "text", "text": "show result ( data ,  result ,  top_  $k{=}20$  ,  \\*\\*kwargs ) Show RPN proposals on the image. ", "page_idx": 273, "bbox": [96, 447.8899841308594, 299, 473.3054504394531], "page_size": [612.0, 792.0]}
{"layout": 2671, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 273, "bbox": [136, 479, 188, 491], "page_size": [612.0, 792.0]}
{"layout": 2672, "type": "text", "text": "•  data  ( str or np.ndarray ) – Image filename or loaded image. •  result  ( Tensor or tuple ) – The results to draw over  img  b box result or (b box result, seg m result). •  top_k  ( int ) – Plot the first k bboxes only if set positive. Default: 20 Returns  The image with bboxes drawn on it. Return type  np.ndarray ", "page_idx": 273, "bbox": [137, 495.8604431152344, 521, 593.4542236328125], "page_size": [612.0, 792.0]}
{"layout": 2673, "type": "text", "text": "simple test ( img ,  img_metas ,  rescale=False ) Test function without test time augmentation. ", "page_idx": 273, "bbox": [96, 597.3300170898438, 299, 622.7444458007812], "page_size": [612.0, 792.0]}
{"layout": 2674, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 273, "bbox": [136, 628, 188, 640], "page_size": [612.0, 792.0]}
{"layout": 2675, "type": "text", "text": "•  imgs  ( list[torch.Tensor] ) – List of multiple images •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 273, "bbox": [154, 645.3004150390625, 487.12225341796875, 694.4754638671875], "page_size": [612.0, 792.0]}
{"layout": 2676, "type": "text", "text": "Returns  proposals ", "page_idx": 273, "bbox": [137, 698.4707641601562, 214.63555908203125, 713.0061645507812], "page_size": [612.0, 792.0]}
{"layout": 2677, "type": "text", "text": "Return type  list[np.ndarray] ", "page_idx": 274, "bbox": [137, 70.8248291015625, 257, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 2678, "type": "text", "text": "class  mmdet.models.detectors. Rep Points Detector ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None , test_cfg  $\\leftrightharpoons$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) RepPoints: Point Set Representation for Object Detection. ", "page_idx": 274, "bbox": [71, 89.23503875732422, 521, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 2679, "type": "text", "text": "This detector is the implementation of: - RepPoints detector ( https://arxiv.org/pdf/1904.11490 ) ", "page_idx": 274, "bbox": [96, 131.2284698486328, 475.365966796875, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 2680, "type": "text", "text": "class  mmdet.models.detectors. RetinaNet ( backbone ,  neck ,  bbox_head ,  train_cfg=None ,  test_cfg=None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $=$  None ) ", "page_idx": 274, "bbox": [71, 149.0110321044922, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 2681, "type": "text", "text": "class  mmdet.models.detectors. SCNet ( \\*\\*kwargs ) Implementation of  SCNet ", "page_idx": 274, "bbox": [71, 190.8540496826172, 298, 216.26951599121094], "page_size": [612.0, 792.0]}
{"layout": 2682, "type": "text", "text": "class  mmdet.models.detectors. SOLO ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  bbox_head  $\\leftrightharpoons$  None ,  mask_head=None , train_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  pretrained  $\\leftrightharpoons$  None ) SOLO: Segmenting Objects by Locations ", "page_idx": 274, "bbox": [71, 220.74205017089844, 521, 258.112548828125], "page_size": [612.0, 792.0]}
{"layout": 2683, "type": "text", "text": "class  mmdet.models.detectors. Single Stage Detector ( backbone ,  neck  $\\leftrightharpoons$  None ,  bbox_head=None , train_cfg  $=$  None ,  test_cfg  $=$  None ,  pretrained=None , init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) ", "page_idx": 274, "bbox": [71, 262.5850830078125, 531.6554565429688, 299.845947265625], "page_size": [612.0, 792.0]}
{"layout": 2684, "type": "text", "text": "Base class for single-stage detectors. ", "page_idx": 274, "bbox": [96, 298.6004943847656, 244, 311.9105224609375], "page_size": [612.0, 792.0]}
{"layout": 2685, "type": "text", "text": "Single-stage detectors directly and densely predict bounding boxes on the output features of the backbone+neck. ", "page_idx": 274, "bbox": [96, 316.53350830078125, 540.0030517578125, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 2686, "type": "text", "text": "aug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test function with test time augmentation. ", "page_idx": 274, "bbox": [96, 346.2720642089844, 286.4853210449219, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 2687, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 274, "bbox": [136, 378, 188, 389], "page_size": [612.0, 792.0]}
{"layout": 2688, "type": "text", "text": "•  imgs  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 274, "bbox": [154, 394.24151611328125, 521, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 2689, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 274, "bbox": [136, 473, 173, 485], "page_size": [612.0, 792.0]}
{"layout": 2690, "type": "text", "text": "BBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. ", "page_idx": 274, "bbox": [154, 489.2558898925781, 521, 515.1485595703125], "page_size": [612.0, 792.0]}
{"layout": 2691, "type": "text", "text": "Return type  list[list[np.ndarray]] ", "page_idx": 274, "bbox": [137, 519.1439208984375, 273.3443603515625, 533.6793212890625], "page_size": [612.0, 792.0]}
{"layout": 2692, "type": "text", "text": "extract feat ( img ) Directly extract features from the backbone+neck. ", "page_idx": 274, "bbox": [96, 537.5541381835938, 317, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 2693, "type": "text", "text": "forward dummy ( img ) Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py ", "page_idx": 274, "bbox": [96, 567.4420776367188, 317, 610.7896118164062], "page_size": [612.0, 792.0]}
{"layout": 2694, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $:=$  None ) ", "page_idx": 274, "bbox": [96, 615.2631225585938, 428.2241516113281, 628.7225952148438], "page_size": [612.0, 792.0]}
{"layout": 2695, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 274, "bbox": [136, 646, 188, 659], "page_size": [612.0, 792.0]}
{"layout": 2696, "type": "text", "text": "•  img  ( Tensor ) – Input images of shape (N, C, H, W). Typically these should be mean cen- tered and std scaled. •  img_metas  ( list[dict] ) – A List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ", "page_idx": 274, "bbox": [154, 663.2335205078125, 521, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 2697, "type": "text", "text": "‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 275, "bbox": [154, 71.45246887207031, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 2698, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 275, "bbox": [137, 178.42083740234375, 308.6417541503906, 192.95626831054688], "page_size": [612.0, 792.0]}
{"layout": 2699, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 275, "bbox": [137, 196.35382080078125, 257, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 2700, "type": "text", "text": "on nx export ( img ,  img_metas ,  with_nms  $\\mathbf{:=}$  True ) Test function without test time augmentation. ", "page_idx": 275, "bbox": [96, 214.76499938964844, 300, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 2701, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 275, "bbox": [136, 246, 188, 258], "page_size": [612.0, 792.0]}
{"layout": 2702, "type": "text", "text": "•  img  ( torch.Tensor ) – input images. •  img_metas  ( list[dict] ) – List of image information. ", "page_idx": 275, "bbox": [154, 262.7344665527344, 388, 293.9775085449219], "page_size": [612.0, 792.0]}
{"layout": 2703, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 275, "bbox": [136, 300, 173, 311], "page_size": [612.0, 792.0]}
{"layout": 2704, "type": "text", "text": "dets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. Return type  tuple[Tensor, Tensor] ", "page_idx": 275, "bbox": [137, 315.9058532714844, 439.32171630859375, 348.373291015625], "page_size": [612.0, 792.0]}
{"layout": 2705, "type": "text", "text": "simple test ( img ,  img_metas ,  rescale  $=$  False ) Test function without test-time augmentation. ", "page_idx": 275, "bbox": [96, 352.24908447265625, 300, 377.6635437011719], "page_size": [612.0, 792.0]}
{"layout": 2706, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 275, "bbox": [136, 383, 188, 395], "page_size": [612.0, 792.0]}
{"layout": 2707, "type": "text", "text": "•  img  ( torch.Tensor ) – Images with shape (N, C, H, W). •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 275, "bbox": [154, 400.21954345703125, 487.122314453125, 449.39459228515625], "page_size": [612.0, 792.0]}
{"layout": 2708, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 275, "bbox": [137, 456, 173, 466], "page_size": [612.0, 792.0]}
{"layout": 2709, "type": "text", "text": "BBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. Return type  list[list[np.ndarray]] ", "page_idx": 275, "bbox": [137, 471.32293701171875, 521, 515.746337890625], "page_size": [612.0, 792.0]}
{"layout": 2710, "type": "text", "text": "class  mmdet.models.detectors. SparseRCNN ( \\*args ,  \\*\\*kwargs ) Implementation of  Sparse R-CNN: End-to-End Object Detection with Learnable Proposals ", "page_idx": 275, "bbox": [71.99996948242188, 519.6211547851562, 458.80828857421875, 545.03662109375], "page_size": [612.0, 792.0]}
{"layout": 2711, "type": "text", "text": "forward dummy ( img ) ", "text_level": 1, "page_idx": 275, "bbox": [95, 551, 191, 562], "page_size": [612.0, 792.0]}
{"layout": 2712, "type": "text", "text": "Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py ", "page_idx": 275, "bbox": [118, 561.6145629882812, 317, 592.8566284179688], "page_size": [612.0, 792.0]}
{"layout": 2713, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_masks  $\\leftrightharpoons$  None , proposal  $\\overleftarrow{}$  None ,  \\*\\*kwargs ) Forward function of SparseR-CNN and QueryInst in train stage. ", "page_idx": 275, "bbox": [96, 597.3301391601562, 495.05364990234375, 634.6996459960938], "page_size": [612.0, 792.0]}
{"layout": 2714, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 275, "bbox": [136, 640, 188, 652], "page_size": [612.0, 792.0]}
{"layout": 2715, "type": "text", "text": "•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. ", "page_idx": 275, "bbox": [154, 657.2555541992188, 521, 682.5206298828125], "page_size": [612.0, 792.0]}
{"layout": 2716, "type": "text", "text": "•  img_metas  ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor ) – specify which bounding boxes can be ig- nored when computing the loss. •  gt_masks  ( List[Tensor], optional ) – Segmentation masks for each box. This is required to train QueryInst. •  proposals  ( List[Tensor], optional ) – override rpn proposals with custom propos- als. Use when  with_rpn  is False. ", "page_idx": 276, "bbox": [154, 71.45246887207031, 521, 258.112548828125], "page_size": [612.0, 792.0]}
{"layout": 2717, "type": "text", "text": "Returns  a dictionary of loss components ", "page_idx": 276, "bbox": [137, 262.10687255859375, 305, 276.6423034667969], "page_size": [612.0, 792.0]}
{"layout": 2718, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 276, "bbox": [137, 280.0398864746094, 256, 294.5753173828125], "page_size": [612.0, 792.0]}
{"layout": 2719, "type": "text", "text": "simple test ( img ,  img_metas ,  rescale=False ) Test function without test time augmentation. ", "page_idx": 276, "bbox": [96, 298.45111083984375, 299, 323.8655700683594], "page_size": [612.0, 792.0]}
{"layout": 2720, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 276, "bbox": [136, 330, 188, 341], "page_size": [612.0, 792.0]}
{"layout": 2721, "type": "text", "text": "•  imgs  ( list[torch.Tensor] ) – List of multiple images •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool ) – Whether to rescale the results. Defaults to False. ", "page_idx": 276, "bbox": [154, 346.42156982421875, 434, 395.59661865234375], "page_size": [612.0, 792.0]}
{"layout": 2722, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 276, "bbox": [136, 402, 173, 413], "page_size": [612.0, 792.0]}
{"layout": 2723, "type": "text", "text": "BBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. Return type  list[list[np.ndarray]] ", "page_idx": 276, "bbox": [137, 417.52496337890625, 521, 461.9483947753906], "page_size": [612.0, 792.0]}
{"layout": 2724, "type": "text", "text": "class  mmdet.models.detectors. Trident Faster R CNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg , neck  $\\mathbf{\\beta}=$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 276, "bbox": [71, 465.82318115234375, 521, 491.2376403808594], "page_size": [612.0, 792.0]}
{"layout": 2725, "type": "text", "text": "Implementation of  TridentNet ", "page_idx": 276, "bbox": [96, 489.88360595703125, 218, 503.1936340332031], "page_size": [612.0, 792.0]}
{"layout": 2726, "type": "text", "text": "aug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 276, "bbox": [96, 507.66619873046875, 434, 551.0136108398438], "page_size": [612.0, 792.0]}
{"layout": 2727, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  \\*\\*kwargs ) make copies of img and gts to fit multi-branch. ", "page_idx": 276, "bbox": [96, 555.4871826171875, 368.2733459472656, 580.9016723632812], "page_size": [612.0, 792.0]}
{"layout": 2728, "type": "text", "text": "simple test ( img ,  img_metas ,  proposals  $\\mathbf{\\varepsilon}=$  None ,  rescale  $\\mathbf{\\beta}=$  False ) Test without augmentation. ", "page_idx": 276, "bbox": [96, 585.3751831054688, 360.9563293457031, 610.7896118164062], "page_size": [612.0, 792.0]}
{"layout": 2729, "type": "text", "text": "class  mmdet.models.detectors. Two Stage Detector ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  rpn_head  $\\leftrightharpoons$  None , roi_head  $\\leftrightharpoons$  None ,  train_cfg  $=$  None ,  test_cfg  $=$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 276, "bbox": [71, 615.26318359375, 510.3255615234375, 652.6326293945312], "page_size": [612.0, 792.0]}
{"layout": 2730, "type": "text", "text": "Base class for two-stage detectors. ", "page_idx": 276, "bbox": [96, 651.277587890625, 233.26502990722656, 664.587646484375], "page_size": [612.0, 792.0]}
{"layout": 2731, "type": "text", "text": "Two-stage detectors typically consisting of a region proposal network and a task-specific regression head. ", "page_idx": 276, "bbox": [96, 669.2105712890625, 515.4855346679688, 682.5206298828125], "page_size": [612.0, 792.0]}
{"layout": 2732, "type": "text", "text": "async a sync simple test ( img ,  img_meta ,  proposals  $=$  None ,  rescale  $\\mathbf{=}$  False ) Async test without augmentation. ", "page_idx": 276, "bbox": [96, 686.9942016601562, 419.84527587890625, 712.4086303710938], "page_size": [612.0, 792.0]}
{"layout": 2733, "type": "text", "text": "aug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 277, "bbox": [96, 71.30303192138672, 432.90496826171875, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 2734, "type": "text", "text": "extract feat ( img ) Directly extract features from the backbone+neck. ", "page_idx": 277, "bbox": [96, 119.12303924560547, 317, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 2735, "type": "text", "text": "forward dummy ( img ) Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py ", "page_idx": 277, "bbox": [96, 149.0110321044922, 317, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 2736, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_masks  $\\mathbf{=}$  None , proposal  $s{=}$  None ,  \\*\\*kwargs ) ", "page_idx": 277, "bbox": [96, 196.83201599121094, 495, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 2737, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 277, "bbox": [136, 240, 188, 252], "page_size": [612.0, 792.0]}
{"layout": 2738, "type": "text", "text": "•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box ", "page_idx": 277, "bbox": [155, 256.7574768066406, 521, 383.6415100097656], "page_size": [612.0, 792.0]}
{"layout": 2739, "type": "text", "text": " gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 277, "bbox": [159, 388.2644958496094, 521, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 2740, "type": "text", "text": " gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. ", "page_idx": 277, "bbox": [159, 418.1524963378906, 521, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 2741, "type": "text", "text": "Returns  a dictionary of loss components ", "page_idx": 277, "bbox": [137, 465.3448486328125, 305, 479.8802795410156], "page_size": [612.0, 792.0]}
{"layout": 2742, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 277, "bbox": [137, 483.2778625488281, 257, 497.81329345703125], "page_size": [612.0, 792.0]}
{"layout": 2743, "type": "text", "text": "simple test ( img ,  img_metas ,  proposals  $\\mathbf{\\varepsilon}=$  None ,  rescale=False ) Test without augmentation. ", "page_idx": 277, "bbox": [96, 501.6890869140625, 360.9564208984375, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 2744, "type": "text", "text": "property with roi head whether the detector has a RoI head ", "page_idx": 277, "bbox": [96, 533.4500732421875, 261.23046875, 556.9915161132812], "page_size": [612.0, 792.0]}
{"layout": 2745, "type": "text", "text": "Type  bool ", "page_idx": 277, "bbox": [137, 560.9868774414062, 180, 575.5222778320312], "page_size": [612.0, 792.0]}
{"layout": 2746, "type": "text", "text": "property with_rpn whether the detector has RPN ", "page_idx": 277, "bbox": [96, 581.27001953125, 237.79843139648438, 604.8125610351562], "page_size": [612.0, 792.0]}
{"layout": 2747, "type": "text", "text": "Type  bool ", "page_idx": 277, "bbox": [137, 608.806884765625, 180, 623.34228515625], "page_size": [612.0, 792.0]}
{"layout": 2748, "type": "text", "text": "class  mmdet.models.detectors. Two Stage Pan optic Segment or ( backbone ,  neck  $\\leftrightharpoons$  None ,  rpn_head=None , roi_head  $\\leftrightharpoons$  None ,  train_cfg  $\\leftrightharpoons$  None , test_cfg  $=$  None ,  pretrained  $\\mathbf{\\{}=}$  None , init_cfg  $=$  None ,  semantic head  $\\leftrightharpoons$  None , pan optic fusion head=None ) ", "page_idx": 277, "bbox": [72.00005340576172, 627.2180786132812, 521, 688.388916015625], "page_size": [612.0, 792.0]}
{"layout": 2749, "type": "text", "text": "Base class of Two-stage Panoptic Segmentor. ", "page_idx": 277, "bbox": [96, 687.1434936523438, 277, 700.4535522460938], "page_size": [612.0, 792.0]}
{"layout": 2750, "type": "text", "text": "As well as the components in Two Stage Detector, Panoptic Segmentor has extra semantic head and panop- tic fusion head. ", "page_idx": 278, "bbox": [96, 71.45246887207031, 540, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2751, "type": "text", "text": "forward dummy ( ) ", "text_level": 1, "page_idx": 278, "bbox": [95, 103, 191, 109.75], "page_size": [612.0, 792.0]}
{"layout": 2752, "type": "text", "text": "Used for computing network flops. See  mm detection/tools/get_flops.py ", "page_idx": 278, "bbox": [118, 113.29548645019531, 258, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 2753, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_masks  $\\mathbf{\\hat{\\rho}}$  None , gt semantic seg  $=$  None ,  proposals  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) ", "page_idx": 278, "bbox": [96, 149.0110321044922, 498, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 2754, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 278, "bbox": [136, 192, 187, 204], "page_size": [612.0, 792.0]}
{"layout": 2755, "type": "text", "text": "•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. •  proposals  – override rpn proposals with custom proposals. Use when  with_rpn  is False. ", "page_idx": 278, "bbox": [155, 208.9364776611328, 521, 413.5295715332031], "page_size": [612.0, 792.0]}
{"layout": 2756, "type": "text", "text": "Returns  a dictionary of loss components Return type  dict[str, Tensor] ", "page_idx": 278, "bbox": [137, 417.52490234375, 305, 449.9923400878906], "page_size": [612.0, 792.0]}
{"layout": 2757, "type": "text", "text": "simple test ( img ,  img_metas ,  proposals  $\\mathbf{\\hat{\\rho}}$  None ,  rescale  $\\mathbf{\\beta}=$  False ) Test without Augmentation. ", "page_idx": 278, "bbox": [96, 453.8681335449219, 360.95635986328125, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 2758, "type": "text", "text": "simple test mask ( x ,  img_metas ,  det_bboxes ,  det_labels ,  rescale=False ) Simple test for mask head without augmentation. ", "page_idx": 278, "bbox": [96, 483.7561340332031, 400.1893615722656, 509.17059326171875], "page_size": [612.0, 792.0]}
{"layout": 2759, "type": "text", "text": "class  mmdet.models.detectors. VFNet ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\boldsymbol{\\mathrm{\\ell}}=$  None ,  init_cfg  $=$  None ) Implementation of  \\`Var i focal Net (VFNet).<https://arxiv.org/abs/2008.13367>\\`_ class  mmdet.models.detectors. YOLACT ( backbone ,  neck ,  bbox_head ,  segm_head ,  mask_head ,  train_cfg  $\\leftrightharpoons$  None , test_cfg  $=$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) Implementation of  YOLACT aug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test with augmentations. forward dummy ( img ) Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py ", "page_idx": 278, "bbox": [71, 513.6441040039062, 540, 670.5656127929688], "page_size": [612.0, 792.0]}
{"layout": 2760, "type": "text", "text": "forward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_masks=None ) ", "page_idx": 278, "bbox": [96, 675.0391235351562, 498, 688.4985961914062], "page_size": [612.0, 792.0]}
{"layout": 2761, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 278, "bbox": [136, 706, 187, 718], "page_size": [612.0, 792.0]}
{"layout": 2762, "type": "text", "text": "•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. ", "page_idx": 279, "bbox": [154, 71.45246887207031, 521, 258.112548828125], "page_size": [612.0, 792.0]}
{"layout": 2763, "type": "text", "text": "Returns  a dictionary of loss components ", "page_idx": 279, "bbox": [137, 262.10687255859375, 305, 276.6423034667969], "page_size": [612.0, 792.0]}
{"layout": 2764, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 279, "bbox": [137, 280.0398864746094, 256.1192932128906, 294.5753173828125], "page_size": [612.0, 792.0]}
{"layout": 2765, "type": "text", "text": "simple test ( img ,  img_metas ,  rescale=False ) Test function without test-time augmentation. ", "page_idx": 279, "bbox": [96, 298.45111083984375, 300, 323.8655700683594], "page_size": [612.0, 792.0]}
{"layout": 2766, "type": "text", "text": "class  mmdet.models.detectors. YOLOF ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ) Implementation of  You Only Look One-level Feature ", "page_idx": 279, "bbox": [71, 328.339111328125, 498, 365.70855712890625], "page_size": [612.0, 792.0]}
{"layout": 2767, "type": "text", "text": "class  mmdet.models.detectors. YOLOV3 ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\beta}=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 279, "bbox": [71, 370.1820983886719, 503.99957275390625, 395.5965576171875], "page_size": [612.0, 792.0]}
{"layout": 2768, "type": "text", "text": "on nx export ( img ,  img_metas ) Test function for exporting to ONNX, without test time augmentation. ", "page_idx": 279, "bbox": [96, 412.02508544921875, 397, 437.4395446777344], "page_size": [612.0, 792.0]}
{"layout": 2769, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 279, "bbox": [136, 443, 188, 455], "page_size": [612.0, 792.0]}
{"layout": 2770, "type": "text", "text": "•  img  ( torch.Tensor ) – input images. •  img_metas  ( list[dict] ) – List of image information. ", "page_idx": 279, "bbox": [154, 459.99554443359375, 388.45751953125, 491.2375793457031], "page_size": [612.0, 792.0]}
{"layout": 2771, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 279, "bbox": [136, 498, 173, 509], "page_size": [612.0, 792.0]}
{"layout": 2772, "type": "text", "text": "dets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. ", "page_idx": 279, "bbox": [154, 513.1658935546875, 439.32159423828125, 527.7012939453125], "page_size": [612.0, 792.0]}
{"layout": 2773, "type": "text", "text": "Return type  tuple[Tensor, Tensor] ", "page_idx": 279, "bbox": [137, 531.0989379882812, 278.1065979003906, 545.6343383789062], "page_size": [612.0, 792.0]}
{"layout": 2774, "type": "text", "text": "class  mmdet.models.detectors. YOLOX ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) Implementation of  YOLOX: Exceeding YOLO Series in 2021 ", "page_idx": 279, "bbox": [71, 549.5090942382812, 498, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 2775, "type": "text", "text": "39.2 backbones ", "text_level": 1, "page_idx": 280, "bbox": [70, 71, 183, 87], "page_size": [612.0, 792.0]}
{"layout": 2776, "type": "text", "text": "class  mmdet.models.backbones. CSPDarknet ( arch  $\\scriptstyle{\\prime=P5}$  ,  deepen factor  $\\scriptstyle{:=I.O}$  ,  widen facto  ${\\bf\\tau}{=}I.0$  , out_indices=(2, 3, 4) ,  frozen stages=- 1 ,  use depth wise  $\\mathbf{\\beta}=$  False , arch ove write  $\\mathbf{\\beta}=$  None ,  spp kern al sizes=(5, 9, 13) , conv_cfg  $\\mathbf{\\beta}=$  None ,  norm_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} ,  act_cfg  $\\scriptstyle{\\tilde{}}=$  {'type': 'Swish'} ,  norm_eval  $\\leftrightharpoons$  False , init_cfg  $=$  {'a': 2.23606797749979, 'distribution': 'uniform', 'layer': 'Conv2d', 'mode': 'fan_in', 'non linearity': 'leaky_relu', 'type': 'Kaiming'} ) ", "page_idx": 280, "bbox": [72.0, 103.16899871826172, 535.361572265625, 200.20497131347656], "page_size": [612.0, 792.0]}
{"layout": 2777, "type": "text", "text": "CSP-Darknet backbone used in YOLOv5 and YOLOX. ", "page_idx": 280, "bbox": [95, 198.9595489501953, 315.99395751953125, 212.2695770263672], "page_size": [612.0, 792.0]}
{"layout": 2778, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 280, "bbox": [117, 218, 169, 230], "page_size": [612.0, 792.0]}
{"layout": 2779, "type": "text", "text": "•  arch  ( str ) – Architecture of CSP-Darknet, from {P5, P6}. Default: P5. •  deepen factor  ( float ) – Depth multiplier, multiply number of channels in each layer by this amount. Default: 1.0. •  widen factor  ( float ) – Width multiplier, multiply number of blocks in CSP layer by this amount. Default: 1.0. •  out indices  ( Sequence[int] ) – Output from which stages. Default: (2, 3, 4). •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. Default: -1. •  use depth wise  ( bool ) – Whether to use depthwise separable convolution. Default: False. •  arch ove write  ( list ) – Overwrite default arch settings. Default: None. •  spp kern al sizes  – (tuple[int]): Sequential of kernel sizes of SPP layers. Default: (5, 9, 13). •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $=^{,}$  ’BN’, requires grad  $\\scriptstyle\\varepsilon=$  True). •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\r=$  ’LeakyReLU’, nega- tive_slope  $\\mathrm{{=}}0.1$  ). •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. ", "page_idx": 280, "bbox": [145, 234.8245391845703, 518, 558.9696655273438], "page_size": [612.0, 792.0]}
{"layout": 2780, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 280, "bbox": [95, 579, 138, 590], "page_size": [612.0, 792.0]}
{"layout": 2781, "type": "text", "text": " $>>$   from  mmdet.models  import  CSPDarknet  $>>$   import  torch  $>>$   self  $=$   CSPDarknet(depth  $\\mathord{\\mathrm{\\varepsilon}}\\!=\\!53$  )  $>>$   self . eval()  $>>$   inputs  $=$   torch . rand( 1 ,  3 ,  416 ,  416 )  $>>$   level outputs  $=$   self . forward(inputs)  $>>$   for  level_out  in  level outputs: ... print ( tuple (level_out . shape)) ", "page_idx": 280, "bbox": [95, 606.4080200195312, 306.1211853027344, 701.0030517578125], "page_size": [612.0, 792.0]}
{"layout": 2782, "type": "text", "text": "(continues on next page) ", "page_idx": 280, "bbox": [461.9010009765625, 702.8345336914062, 540.0, 713.4826049804688], "page_size": [612.0, 792.0]}
{"layout": 2783, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. ", "page_idx": 281, "bbox": [96, 147, 313.4832763671875, 171.38951110839844], "page_size": [612.0, 792.0]}
{"layout": 2784, "type": "text", "text": "Should be overridden by all subclasses. ", "page_idx": 281, "bbox": [118, 176.01246643066406, 275.4460144042969, 189.32249450683594], "page_size": [612.0, 792.0]}
{"layout": 2785, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 281, "bbox": [118, 205.27178955078125, 540, 243.12046813964844], "page_size": [612.0, 792.0]}
{"layout": 2786, "type": "text", "text": "train ( mode  $=$  True ) Sets the module in training mode. ", "page_idx": 281, "bbox": [96, 259.54901123046875, 253.31906127929688, 284.9634704589844], "page_size": [612.0, 792.0]}
{"layout": 2787, "type": "text", "text": "This has any effect only on certain modules. See documentation s of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g.  Dropout ,  BatchNorm , etc. ", "page_idx": 281, "bbox": [118, 289.5864562988281, 540, 314.8514709472656], "page_size": [612.0, 792.0]}
{"layout": 2788, "type": "text", "text": "Parameters  mode  ( bool ) – whether to set training mode ( True ) or evaluation mode ( False ). Default:  True . ", "page_idx": 281, "bbox": [137, 318.8468017578125, 523, 344.7394714355469], "page_size": [612.0, 792.0]}
{"layout": 2789, "type": "text", "text": "Returns  self ", "page_idx": 281, "bbox": [137, 348.73480224609375, 190.83396911621094, 363.2702331542969], "page_size": [612.0, 792.0]}
{"layout": 2790, "type": "text", "text": "Return type  Module ", "page_idx": 281, "bbox": [137, 366.66680908203125, 224.31858825683594, 381.2022399902344], "page_size": [612.0, 792.0]}
{"layout": 2791, "type": "text", "text": "class  mmdet.models.backbones. Darknet ( depth  $\\mathord{:=}53$  ,  out_indices=(3, 4, 5) ,  frozen stages  $\\scriptstyle{\\prime}=-I$  , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'requires grad': True, 'type': 'BN'} , act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'negative slope': 0.1, 'type': 'LeakyReLU'} , norm_eval  $\\leftrightharpoons$  True ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\leftrightharpoons$  None ) ", "page_idx": 281, "bbox": [71.99995422363281, 385.0780334472656, 523, 434.4034729003906], "page_size": [612.0, 792.0]}
{"layout": 2792, "type": "text", "text": "Darknet backbone. ", "page_idx": 281, "bbox": [96, 433.0484313964844, 171.95510864257812, 446.35845947265625], "page_size": [612.0, 792.0]}
{"layout": 2793, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 281, "bbox": [115, 452, 169, 463], "page_size": [612.0, 792.0]}
{"layout": 2794, "type": "text", "text": "•  depth  ( int ) – Depth of Darknet. Currently only support 53. •  out indices  ( Sequence[int] ) – Output from which stages. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. Default: -1. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $\\L_{:=}^{:}$  ’BN’, requires grad  $\\scriptstyle\\varepsilon=$  True) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $=^{!}$  ’LeakyReLU’, nega- tive_slope  $\\mathrm{{=}}0.1$  ). •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 281, "bbox": [145, 468.9134521484375, 518, 673.5064697265625], "page_size": [612.0, 792.0]}
{"layout": 2795, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 282, "bbox": [96, 73, 139, 85], "page_size": [612.0, 792.0]}
{"layout": 2796, "type": "text", "text": " $>>$   from  mmdet.models  import  Darknet\n\n  $>>$   import  torch\n\n  $>>$   self  $=$   Darknet(depth  $\\mathtt{=}53$  )\n\n  $>>$   self . eval()\n\n  $>>$   inputs  $=$   torch . rand( 1 ,  3 ,  416 ,  416 )\n\n  $>>$   level outputs  $=$   self . forward(inputs)\n\n  $>>$   for  level_out  in  level outputs:\n\n ... print ( tuple (level_out . shape))\n\n ...\n\n (1, 256, 52, 52)\n\n (1, 512, 26, 26)\n\n (1, 1024, 13, 13) ", "page_idx": 282, "bbox": [95, 100.33097076416016, 306.1211853027344, 242.44729614257812], "page_size": [612.0, 792.0]}
{"layout": 2797, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 282, "bbox": [95, 256.7439880371094, 313, 298.2185363769531], "page_size": [612.0, 792.0]}
{"layout": 2798, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 282, "bbox": [118, 314.1688232421875, 540, 352.0164794921875], "page_size": [612.0, 792.0]}
{"layout": 2799, "type": "text", "text": "static make con v res block ( in channels ,  out channels ,  res_repeat ,  conv_cfg  $\\mathbf{\\hat{\\mu}}$  None , norm_cfg  $\\leftleftarrows$  {'requires grad': True, 'type': 'BN'} ,  act_cfg  $\\scriptstyle{\\tilde{}}=$  {'negative slope': 0.1, 'type': 'LeakyReLU'} ) ", "page_idx": 282, "bbox": [95, 368.4450378417969, 540, 405.70489501953125], "page_size": [612.0, 792.0]}
{"layout": 2800, "type": "text", "text": "In Darknet backbone, ConvLayer is usually followed by ResBlock. This function will make that. The Conv layers always have 3x3 filters with stride  $_{:=2}$  . The number of the filters in Conv layer is the same as the out channels of the ResBlock. ", "page_idx": 282, "bbox": [118, 404.46044921875, 540, 441.6804504394531], "page_size": [612.0, 792.0]}
{"layout": 2801, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 282, "bbox": [136, 448, 188, 459], "page_size": [612.0, 792.0]}
{"layout": 2802, "type": "text", "text": "•  in channels  ( int ) – The number of input channels. •  out channels  ( int ) – The number of output channels. •  res_repeat  ( int ) – The number of ResBlocks. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $\\mathbf{\\varepsilon}\\mathbf{=}\\mathbf{'}\\mathbf{B}\\mathbf{N}^{\\ast}$  ’, requires grad  $=$  True) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\scriptstyle{:=}$  ’LeakyReLU’, neg- at ive slope  $\\mathrm{{=}}0.1$  ). ", "page_idx": 282, "bbox": [154, 464.2354431152344, 521, 591.1205444335938], "page_size": [612.0, 792.0]}
{"layout": 2803, "type": "text", "text": "train ( mode  $=$  True ) Sets the module in training mode. ", "page_idx": 282, "bbox": [95, 595.5930786132812, 253.31980895996094, 621.0075073242188], "page_size": [612.0, 792.0]}
{"layout": 2804, "type": "text", "text": "This has any effect only on certain modules. See documentation s of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g.  Dropout ,  BatchNorm , etc. ", "page_idx": 282, "bbox": [118, 625.6304321289062, 540, 650.8955078125], "page_size": [612.0, 792.0]}
{"layout": 2805, "type": "text", "text": "Parameters  mode  ( bool ) – whether to set training mode ( True ) or evaluation mode ( False ). Default:  True . Returns  self Return type  Module ", "page_idx": 282, "bbox": [137, 654.890869140625, 521, 717.2472534179688], "page_size": [612.0, 792.0]}
{"layout": 2806, "type": "text", "text": "class  mmdet.models.backbones. DetectoRS Res NeXt ( group  $\\mathrm{\\Sigma}_{:=I}$  ,  base_width  $\\scriptstyle{=4}$  ,  \\*\\*kwargs ) ResNeXt backbone for DetectoRS. ", "page_idx": 283, "bbox": [71, 71.30303192138672, 467.3013610839844, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2807, "type": "text", "text": "Parameters ", "page_idx": 283, "bbox": [118, 100.71282958984375, 170, 115.24826049804688], "page_size": [612.0, 792.0]}
{"layout": 2808, "type": "text", "text": "•  groups  ( int ) – The number of groups in ResNeXt. •  base_width  ( int ) – The base width of ResNeXt. ", "page_idx": 283, "bbox": [145, 119.27247619628906, 361.16455078125, 150.51548767089844], "page_size": [612.0, 792.0]}
{"layout": 2809, "type": "text", "text": "make res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer  for DetectoRS. ", "page_idx": 283, "bbox": [96, 154.98899841308594, 349, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 2810, "type": "text", "text": "class  mmdet.models.backbones. DetectoRS Res Net ( sac  $=$  None ,  stage with sac=(False, False, False, False) , rfp in planes  $\\leftrightharpoons$  None ,  output_img  $\\mathbf{\\hat{\\Sigma}}$  False , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) ", "page_idx": 283, "bbox": [71, 184.8769989013672, 535.5106811523438, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 2811, "type": "text", "text": "ResNet backbone for DetectoRS. ", "page_idx": 283, "bbox": [96, 220.89149475097656, 228.1642303466797, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 2812, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 283, "bbox": [117, 240, 169, 252], "page_size": [612.0, 792.0]}
{"layout": 2813, "type": "text", "text": "•  sac  ( dict, optional ) – Dictionary to construct SAC (Switchable Atrous Convolution). Default: None. •  stage with sac  ( list ) – Which stage to use sac. Default: (False, False, False, False). •  rfp in planes  ( int, optional ) – The number of channels from RFP. Default: None. If specified, an additional conv layer will be added for  rfp_feat . Otherwise, the structure is the same as base class. •  output_img  ( bool ) – If  True , the input image will be inserted into the starting position of output. Default: False. ", "page_idx": 283, "bbox": [145, 256.7574768066406, 518, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 2814, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 283, "bbox": [96, 378, 190.51478576660156, 401.57452392578125], "page_size": [612.0, 792.0]}
{"layout": 2815, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 283, "bbox": [96, 407.9200134277344, 204.58203125, 431.4625244140625], "page_size": [612.0, 792.0]}
{"layout": 2816, "type": "text", "text": "make res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer  for DetectoRS. ", "page_idx": 283, "bbox": [96, 435.93505859375, 349, 461.35052490234375], "page_size": [612.0, 792.0]}
{"layout": 2817, "type": "text", "text": "rfp forward ( x ,  rfp_feats ) Forward function for RFP. ", "page_idx": 283, "bbox": [96, 465.82305908203125, 223.44114685058594, 491.2375183105469], "page_size": [612.0, 792.0]}
{"layout": 2818, "type": "text", "text": "class  mmdet.models.backbones. HRNet ( extra ,  in channels  $\\scriptstyle{\\prime=3}$  ,  conv_cfg  $=$  None ,  norm_cfg={'type': 'BN'} , norm_eva  $\\leftrightharpoons$  True ,  with_cp  $\\mathbf{\\varepsilon}=$  False ,  zero in it res i du a  $\\leftrightharpoons$  False , multi scale out pu  $\\mathbf{\\dot{\\rho}}=$  True ,  pretrained=None ,  init_cfg  $=$  None ) ", "page_idx": 283, "bbox": [71, 495.7110595703125, 513.2744750976562, 533.0814819335938], "page_size": [612.0, 792.0]}
{"layout": 2819, "type": "text", "text": "HRNet backbone. ", "text_level": 1, "page_idx": 283, "bbox": [95, 533.25, 167, 544], "page_size": [612.0, 792.0]}
{"layout": 2820, "type": "text", "text": "High-Resolution Representations for Labeling Pixels and Regions arXiv: . ", "page_idx": 283, "bbox": [96, 549.658447265625, 389.13970947265625, 562.968505859375], "page_size": [612.0, 792.0]}
{"layout": 2821, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 283, "bbox": [117, 569, 169, 581], "page_size": [612.0, 792.0]}
{"layout": 2822, "type": "text", "text": "•  extra  ( dict ) – Detailed configuration for each stage of HRNet. There must be 4 stages, the configuration for each stage must have 5 keys: –  num modules(int): The number of HRModule in this stage. –  num branches(int): The number of branches in the HRModule. –  block(str): The type of convolution block. – num_blocks(tuple): The number of blocks in each branch.  The length must be equal to num branches. ", "page_idx": 283, "bbox": [145, 585.5244750976562, 518, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 2823, "type": "text", "text": "– num channels(tuple): The number of channels in each branch.  The length must be equal to num branches.\n\n •  in channels  ( int ) – Number of input image channels. Default: 3.\n\n •  conv_cfg  ( dict ) – Dictionary to construct and config conv layer.\n\n •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer.\n\n •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. Default: True.\n\n •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.\n\n •  zero in it residual  ( bool ) – Whether to use zero init for last norm layer in resblocks to let them behave as identity. Default: False.\n\n •  multi scale output  ( bool ) – Whether to output multi-level features produced by multiple branches. If False, only the first level feature will be output. Default: True.\n\n •  pretrained  ( str, optional ) – Model pretrained path. Default: None.\n\n •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. ", "page_idx": 284, "bbox": [145, 70.8248291015625, 518, 317.88751220703125], "page_size": [612.0, 792.0]}
{"layout": 2824, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 284, "bbox": [95, 336, 138, 348], "page_size": [612.0, 792.0]}
{"layout": 2825, "type": "table", "page_idx": 284, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_146.jpg", "bbox": [91, 358, 544, 714], "page_size": [612.0, 792.0], "ocr_text": ">>> from mmdet.models import HRNet\n>>> import torch\n>>> extra = dict(\n\n>>> stagel=dict(\n\n>>> num_modules=1,\n\n>>> num_branches=1,\n\n>>> block='BOTTLENECK\"' ,\n\n>>> num_blocks=(4, ),\n\n>>> num_channels=(64, )),\n>>> stage2=dict(\n\n>>> num_modules=1,\n\n= num_branches=2,\n\n>>> block='BASIC',\n\n>>> num_blocks=(4, 4),\n\n>> num_channels=(32, 64)),\n>>> stage3=dict(\n\n>>> num_modules=4,\n\n>>> num_branches=3,\n\n>>> block='BASIC',\n\n>>> num_blocks=(4, 4, 4),\n>>> num_channels=(32, 64, 128)),\n>>> stage4=dict(\n\n>>> num_modules=3,\n\n>>> num_branches=4,\n\n>>> block='BASIC',\n\n>>> num_blocks=(4, 4, 4, 4),\n>>> num_channels=(32, 64, 128, 256)))\n\n>>> self = HRNet(extra, in_channels=1)\n\n>>> self.evalQ\n\n", "vlm_text": "The table contains Python code configuring an HRNet (High-Resolution Network) model using the MMDetection library. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `from mmdet.models import HRNet`: Imports the HRNet model from the MMDetection library.\n   - `import torch`: Imports the PyTorch library.\n\n2. **Model Configuration** (`extra` dictionary):\n   - `stage1`:\n     - `num_modules`: 1\n     - `num_branches`: 1\n     - `block`: 'BOTTLENECK'\n     - `num_blocks`: (4,)\n     - `num_channels`: (64,)\n   - `stage2`:\n     - `num_modules`: 1\n     - `num_branches`: 2\n     - `block`: 'BASIC'\n     - `num_blocks`: (4, 4)\n     - `num_channels`: (32, 64)\n   - `stage3`:\n     - `num_modules`: 4\n     - `num_branches`: 3\n     - `block`: 'BASIC'\n     - `num_blocks`: (4, 4, 4)\n     - `num_channels`: (32, 64, 128)\n   - `stage4`:\n     - `num_modules`: 3\n     - `num_branches`: 4\n     - `block`: 'BASIC'\n     - `num_blocks`: (4, 4, 4, 4)\n     - `num_channels`: (32, 64, 128, 256)\n\n3. **Model Initialization**:\n   - `self = HRNet(extra, in_channels=1)`: Initializes the HRNet model with the configuration `extra` and 1 input channel.\n\n4. **Model Evaluation**:\n   - `self.eval()`: Sets the model in evaluation mode."}
{"layout": 2826, "type": "table", "page_idx": 285, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_147.jpg", "bbox": [93, 82, 544, 187], "page_size": [612.0, 792.0], "ocr_text": ">>\n>>>\n>>\na1,\na1,\na1,\na1,\n\ninputs torch.rand(1, 1, 32, 32)\n\nlevel_outputs = self.forward(inputs)\n\nfor level_out in level_outputs:\nprint (tuple(level_out.shape))\n\n32, 8, 8)\n\n64, 4, 4)\n\n128, 25 2)\n\n256, 1, 1)\n\n", "vlm_text": "The image shows a block of Python code, likely from a deep learning framework like PyTorch. Here's a breakdown of the code:\n\n1. **Creation of `inputs`:**\n   - `inputs = torch.rand(1, 1, 32, 32)`: This line creates a random tensor with the shape `(1, 1, 32, 32)` using PyTorch. The dimensions likely represent batches, channels, height, and width.\n\n2. **Forward pass:**\n   - `level_outputs = self.forward(inputs)`: This line calls a `forward` method on the inputs, typically part of a model's architecture in a neural network, to get `level_outputs`.\n\n3. **Iterating over output levels:**\n   - A loop iterates over `level_outputs`, printing the shape of each `level_out`.\n\n4. **Printed shapes:**\n   - The shapes are: `(1, 32, 8, 8)`, `(1, 64, 4, 4)`, `(1, 128, 2, 2)`, `(1, 256, 1, 1)`. These shapes suggest a series of operations that reduce the spatial dimensions, possibly through convolutional or pooling layers."}
{"layout": 2827, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 285, "bbox": [96, 195.66799926757812, 190.5148468017578, 219.2095184326172], "page_size": [612.0, 792.0]}
{"layout": 2828, "type": "text", "text": "property norm1 the normalization layer named “norm1” ", "page_idx": 285, "bbox": [96, 225.55599975585938, 277, 249.09751892089844], "page_size": [612.0, 792.0]}
{"layout": 2829, "type": "text", "text": "Type  nn.Module ", "page_idx": 285, "bbox": [137, 253.09283447265625, 206, 267.6282653808594], "page_size": [612.0, 792.0]}
{"layout": 2830, "type": "text", "text": "property norm2 ", "text_level": 1, "page_idx": 285, "bbox": [95, 274, 171, 284], "page_size": [612.0, 792.0]}
{"layout": 2831, "type": "text", "text": "the normalization layer named “norm2” Type  nn.Module ", "page_idx": 285, "bbox": [118, 283.6084899902344, 277, 315.44927978515625], "page_size": [612.0, 792.0]}
{"layout": 2832, "type": "text", "text": "train ( mode  $=$  True ) Convert the model into training mode will keeping the normalization layer freezed. ", "page_idx": 285, "bbox": [96, 319.3250732421875, 451, 344.7395324707031], "page_size": [612.0, 792.0]}
{"layout": 2833, "type": "text", "text": "class  mmdet.models.backbones. Hourglass Net ( down sample times  $\\scriptstyle{\\prime=5}$  ,  num_stack  $s{=}2$  ,  stage channels=(256, 256, 384, 384, 384, 512) ,  stage_blocks=(2, 2, 2, 2, 2, 4) , feat ch anne  $l{=}256$  ,  norm_cfg  $\\mathsf{\\chi}=\\!\\!\\big/$  'requires grad': True, 'type': 'BN'} ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 285, "bbox": [72.00001525878906, 349.2120666503906, 534, 398.5375061035156], "page_size": [612.0, 792.0]}
{"layout": 2834, "type": "text", "text": "Hourglass Net backbone. ", "page_idx": 285, "bbox": [96, 397.1824645996094, 194.35110473632812, 410.49249267578125], "page_size": [612.0, 792.0]}
{"layout": 2835, "type": "text", "text": "Stacked Hourglass Networks for Human Pose Estimation. More details can be found in the  paper  . ", "page_idx": 285, "bbox": [96, 415.115478515625, 487.3711242675781, 428.4255065917969], "page_size": [612.0, 792.0]}
{"layout": 2836, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 285, "bbox": [117, 434, 168, 446], "page_size": [612.0, 792.0]}
{"layout": 2837, "type": "text", "text": "•  down sample times  ( int ) – Downsample times in a Hourglass Module. •  num_stacks  ( int ) – Number of Hourglass Module modules stacked, 1 for Hourglass-52, 2 for Hourglass-104. •  stage channels  ( list[int] ) – Feature channel of each sub-module in a Hourglass Mod- ule. •  stage blocks  ( list[int] ) – Number of sub-modules stacked in a Hourglass Module. •  feat channel  ( int ) – Feature channel of conv after a Hourglass Module. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 285, "bbox": [145, 450.9804992675781, 518, 613.7305297851562], "page_size": [612.0, 792.0]}
{"layout": 2838, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 286, "bbox": [96, 73, 138, 85], "page_size": [612.0, 792.0]}
{"layout": 2839, "type": "table", "page_idx": 286, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_148.jpg", "bbox": [93, 96, 544, 224], "page_size": [612.0, 792.0], "ocr_text": ">>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\na1,\na,\n\nfrom mndet.models import HourglassNet\n\nimport torch\n\nself = HourglassNet()\n\nself.evalQ\n\ninputs = torch.rand(1, 3, 511, 511)\n\nlevel_outputs = self.forward(inputs)\n\nfor level_output in level_outputs:\nprint (tuple(level_output.shape))\n\n256, 128, 128)\n\n256, 128, 128)\n\n", "vlm_text": "The image shows a Python script snippet related to using a neural network model called \"HourglassNet\" with PyTorch. Here's a breakdown:\n\n1. **Import Statements:**\n   - `HourglassNet` is imported from `mmdet.models`.\n   - The `torch` library is imported.\n\n2. **Model Initialization and Evaluation:**\n   - An instance of `HourglassNet` is created with `self = HourglassNet()`.\n   - The model is set to evaluation mode using `self.eval()`.\n\n3. **Input and Forward Pass:**\n   - Random input data of shape (1, 3, 511, 511) is created using `torch.rand`.\n   - The input is forwarded through the model with `self.forward(inputs)` and results are stored in `level_outputs`.\n\n4. **Output Shapes:**\n   - The shapes of the outputs are printed in a loop, resulting in two outputs with shapes `(1, 256, 128, 128)`."}
{"layout": 2840, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 286, "bbox": [96, 232.83395385742188, 190, 256.37548828125], "page_size": [612.0, 792.0]}
{"layout": 2841, "type": "text", "text": "in it weights()Init module weights. ", "page_idx": 286, "bbox": [96, 262.7219543457031, 200.716552734375, 286.2634582519531], "page_size": [612.0, 792.0]}
{"layout": 2842, "type": "text", "text": "class  mmdet.models.backbones. Mobile Ne tV 2 ( widen facto  $\\scriptstyle{=}I.O$  ,  out_indices=(1, 2, 4, 7) ,  frozen stage  $\\scriptstyle{\\varepsilon=-\\ I}$  , conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $=$  {'type': 'BN'} ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'ReLU6'} ,  norm_eval  $\\leftrightharpoons$  False ,  with_cp=False ,  pretrained  $\\leftrightharpoons$  None , init_cfg  $=$  None ) ", "page_idx": 286, "bbox": [71.99998474121094, 290.73699951171875, 539.2674560546875, 339.95184326171875], "page_size": [612.0, 792.0]}
{"layout": 2843, "type": "text", "text": "Mobile Ne tV 2 backbone. ", "page_idx": 286, "bbox": [96, 338.7063903808594, 195.0583038330078, 352.01641845703125], "page_size": [612.0, 792.0]}
{"layout": 2844, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 286, "bbox": [116, 358, 169, 369], "page_size": [612.0, 792.0]}
{"layout": 2845, "type": "text", "text": "•  widen factor  ( float ) – Width multiplier, multiply number of channels in each layer by this amount. Default: 1.0. •  out indices  ( Sequence[int], optional ) – Output from which stages. Default: (1, 2, 4, 7). •  frozen stages  ( int ) – Stages to be frozen (all param fixed). Default: -1, which means not freezing any parameters. •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None, which means using conv2d. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict(type  $=^{:}$  ’BN’). •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\L_{:=}^{:}$  ’ReLU6’). •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. Default: False. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 286, "bbox": [145, 374.5724182128906, 518, 621.0084838867188], "page_size": [612.0, 792.0]}
{"layout": 2846, "type": "text", "text": "forward  $(x)$  Forward function. make_layer ( out channels ,  num_blocks ,  stride ,  expand ratio ) Stack Inverted Residual blocks to build a layer for Mobile Ne tV 2. Parameters •  out channels  ( int ) – out channels of block. ", "page_idx": 286, "bbox": [96, 627, 374.5447998046875, 716.6494750976562], "page_size": [612.0, 792.0]}
{"layout": 2847, "type": "text", "text": "•  num_blocks  ( int ) – number of blocks. •  stride  ( int ) – stride of the first block. Default: 1 •  expand ratio  ( int ) – Expand the number of channels of the hidden layer in Inverte- dResidual by this ratio. Default: 6. ", "page_idx": 287, "bbox": [154, 71.45246887207031, 521, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 2848, "type": "text", "text": "train ( mode=True ) Convert the model into training mode while keep normalization layer frozen. ", "page_idx": 287, "bbox": [96, 137.05601501464844, 424.6358642578125, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 2849, "type": "text", "text": "( pre train img size  $\\scriptstyle{\\bullet=224}$  ,  in_channel  $\\mathfrak{s}{=}3$  , embed_dim  $\\mathord{\\breve{=}}64$  ,  num_stage  $s{=}4$  , num_layers=[3, 4, 6, 3] ,  num_heads=[1, 2, 5, 8] ,  patch_sizes=[4, 2, 2, 2] ,  strides=[4, 2, 2, 2] ,  paddings=[0, 0, 0, 0] ,  sr_ratios=[8, 4, 2, 1] ,  out_indices=(0, 1, 2, 3) ,  mlp_ratios=[8, 8, 4, 4] ,  qkv_bias  $\\mathbf{=}$  True ,  drop_rate  $\\bullet{=}0.0$  , at tn drop rate=0.0 ,  drop path rate=0.1 , use abs pos embed=True , norm after stage  $\\mathbf{\\tilde{=}}$  False , use con v ff n  $:=$  False ,  act_cfg={'type': 'GELU'} ,  norm_cfg={'eps': 1e-06, 'type': 'LN'} ,  pretrained  $\\leftrightharpoons$  None , convert weights  $\\mathbf{=}$  True ,  init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ) ", "page_idx": 287, "bbox": [349.2099609375, 166.9440155029297, 540, 335.820556640625], "page_size": [612.0, 792.0]}
{"layout": 2850, "type": "text", "text": "Pyramid Vision Transformer (PVT) ", "page_idx": 287, "bbox": [96, 334.46551513671875, 239.06298828125, 347.7755432128906], "page_size": [612.0, 792.0]}
{"layout": 2851, "type": "text", "text": "Implementation of  Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolu- tions . ", "page_idx": 287, "bbox": [96, 352.3985290527344, 540, 377.6635437011719], "page_size": [612.0, 792.0]}
{"layout": 2852, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 287, "bbox": [118, 384, 168, 394], "page_size": [612.0, 792.0]}
{"layout": 2853, "type": "text", "text": "•  pre train img size  ( int | tuple[int] ) – The size of input image when pretrain. De- faults: 224. •  in channels  ( int ) – Number of input channels. Default: 3. •  embed_dims  ( int ) – Embedding dimension. Default: 64. •  num_stags  ( int ) – The num of stages. Default: 4. •  num_layers  ( Sequence[int] ) – The layer number of each transformer encode layer. De- fault: [3, 4, 6, 3]. •  num_heads  ( Sequence[int] ) – The attention heads of each transformer encode layer. De- fault: [1, 2, 5, 8]. •  patch sizes  ( Sequence[int] ) – The patch_size of each patch embedding. Default: [4, 2, 2, 2]. •  strides  ( Sequence[int] ) – The stride of each patch embedding. Default: [4, 2, 2, 2]. •  paddings  ( Sequence[int] ) – The padding of each patch embedding. Default: [0, 0, 0, 0]. •  sr_ratios  ( Sequence[int] ) – The spatial reduction rate of each transformer encode layer. Default: [8, 4, 2, 1]. •  out indices  ( Sequence[int] | int ) – Output from which stages. Default: (0, 1, 2, 3). •  mlp_ratios  ( Sequence[int] ) – The ratio of the mlp hidden dim to the embedding dim of each transformer encode layer. Default: [8, 8, 4, 4]. •  qkv_bias  ( bool ) – Enable bias for qkv if True. Default: True. •  drop_rate  ( float ) – Probability of an element to be zeroed. Default 0.0. ", "page_idx": 287, "bbox": [145, 400.21954345703125, 521, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 2854, "type": "text", "text": "•  at tn drop rate  ( float ) – The drop out rate for attention layer. Default 0.0. •  drop path rate  ( float ) – stochastic depth rate. Default 0.1. •  use abs pos embed  ( bool ) – If True, add absolute position embedding to the patch em- bedding. Defaults: True. •  use con v ff n  ( bool ) – If True, use Convolutional FFN to replace FFN. Default: False. •  act_cfg  ( dict ) – The activation config for FFNs. Default: dict(type  $=^{!}$  ’GELU’). •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict(type  $=$  ’LN’). •  pretrained  ( str, optional ) – model pretrained path. Default: None. •  convert weights  ( bool ) – The flag indicates whether the pre-trained model is from the original repo. We may need to convert some keys to make it compatible. Default: True. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. ", "page_idx": 288, "bbox": [145, 71.45246887207031, 518, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 2855, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 288, "bbox": [96, 270, 313.4832763671875, 311.91046142578125], "page_size": [612.0, 792.0]}
{"layout": 2856, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 288, "bbox": [118, 327.8608093261719, 540, 365.7084655761719], "page_size": [612.0, 792.0]}
{"layout": 2857, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 288, "bbox": [96, 384.0099792480469, 204.58209228515625, 407.5514831542969], "page_size": [612.0, 792.0]}
{"layout": 2858, "type": "text", "text": "class  mmdet.models.backbones. Pyramid Vision Transformer V 2 ( \\*\\*kwargs ) Implementation of  PVTv2: Improved Baselines with Pyramid Vision Transformer . ", "page_idx": 288, "bbox": [71.99998474121094, 412.0250244140625, 425.21441650390625, 437.4394836425781], "page_size": [612.0, 792.0]}
{"layout": 2859, "type": "text", "text": "( arch ,  in channels  $\\scriptstyle{:=3}$  ,  stem channels  $\\scriptstyle{\\mathfrak{s}}=32$  ,  base channel  $s{=}32$  , strides=(2, 2, 2, 2) ,  dilations=(1, 1, 1, 1) ,  out_indices=(0, 1, 2, 3) , style  $\\mathbf{=}$  'pytorch' ,  deep_stem  $\\mathbf{\\beta}=$  False ,  avg_down  $=$  False ,  frozen stages=- 1 ,  conv_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}\\mathbf{0}$  None ,  norm_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'requires grad': True, 'type': 'BN'} , norm_eva  $\\leftrightharpoons$  True ,  dcn  $=$  None ,  stage with dc n=(False, False, False, False) ,  plugins  $\\mathbf{\\hat{\\rho}}$  None ,  with_cp=False ,  zero in it res i du a  $\\leftrightharpoons$  True , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 288, "bbox": [255.06300354003906, 441.91302490234375, 536.5075073242188, 527.1034545898438], "page_size": [612.0, 792.0]}
{"layout": 2860, "type": "text", "text": "RegNet backbone. ", "page_idx": 288, "bbox": [96, 525.7483520507812, 170, 539.0584106445312], "page_size": [612.0, 792.0]}
{"layout": 2861, "type": "text", "text": "More details can be found in  paper ", "page_idx": 288, "bbox": [96, 543.6813354492188, 235.39683532714844, 556.9913940429688], "page_size": [612.0, 792.0]}
{"layout": 2862, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 288, "bbox": [118, 564, 169, 574], "page_size": [612.0, 792.0]}
{"layout": 2863, "type": "text", "text": " arch  ( dict ) – The parameter of RegNets. –  w0 (int): initial width –  wa (float): slope of width –  wm (float): quantization parameter to quantize the width –  depth (int): depth of the backbone –  group_w (int): width of group –  bot_mul (float): bottleneck ratio, i.e. expansion of bottleneck.  strides  ( Sequence[int] ) – Strides of the first block of each stage. ", "page_idx": 288, "bbox": [149, 579.5463256835938, 430.8628845214844, 718.3864135742188], "page_size": [612.0, 792.0]}
{"layout": 2864, "type": "text", "text": "•  base channels  ( int ) – Base channels after stem layer. •  in channels  ( int ) – Number of input image channels. Default: 3. •  dilations  ( Sequence[int] ) – Dilation of each stage. •  out indices  ( Sequence[int] ) – Output from which stages. •  style    $(s t r)$   –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  frozen stages  ( int ) – Stages to be frozen (all param fixed). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – whether to use zero init for last norm layer in resblocks to let them behave as identity. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 289, "bbox": [145, 71.45246887207031, 518, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 2865, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 289, "bbox": [95, 361, 139, 374], "page_size": [612.0, 792.0]}
{"layout": 2866, "type": "table", "page_idx": 289, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_149.jpg", "bbox": [92, 384, 545, 619], "page_size": [612.0, 792.0], "ocr_text": ">>>\n>>>\n>>>\n\n>>\n>>>\n>>>\n>>>\n(1,\na1,\n(1,\na1,\n\nfrom mmdet.models import RegNet\nimport torch\nself = RegNet(\narch=dict(\nw0=88,\nwa=26.31,\nwm=2.25,\ngroup_w=48,\ndepth=25,\nbot_mul=1.0))\nself.evalQ\ninputs = torch.rand(1, 3, 32, 32)\nlevel_outputs = self.forward(inputs)\nfor level_out in level_outputs:\nprint (tuple(level_out.shape))\n96, 8, 8)\n\n192, 4, 4)\n432, 2, 2)\n1008, 1, 1)\n\n", "vlm_text": "This image contains a Python code snippet for using the `RegNet` model from `mmdet.models`. Here's a summary of the code:\n\n1. **Imports:**\n   - Import `RegNet` from `mmdet.models`.\n   - Import `torch`.\n\n2. **Initialize RegNet:**\n   - A `RegNet` model named `self` is initialized with specific architecture parameters: \n     - `w0=88`\n     - `wa=26.31`\n     - `wm=2.25`\n     - `group_w=48`\n     - `depth=25`\n     - `bot_mul=1.0`\n\n3. **Evaluate Model:**\n   - Set the model to evaluation mode with `self.eval()`.\n\n4. **Generate Inputs:**\n   - Create random input data with shape `(1, 3, 32, 32)` using `torch.rand`.\n\n5. **Forward Pass:**\n   - Pass the inputs through the model with `self.forward(inputs)` and store the outputs in `level_outputs`.\n\n6. **Print Output Shapes:**\n   - Iterate over each output in `level_outputs` and print its shape:\n     - `(1, 96, 8, 8)`\n     - `(1, 192, 4, 4)`\n     - `(1, 432, 2, 2)`\n     - `(1, 1008, 1, 1)`"}
{"layout": 2867, "type": "text", "text": "adjust width group ( widths ,  bottleneck ratio ,  groups ) Adjusts the compatibility of widths and groups. ", "page_idx": 289, "bbox": [96.90699768066406, 627.4630126953125, 329.2893981933594, 652.8784790039062], "page_size": [612.0, 792.0]}
{"layout": 2868, "type": "text", "text": "Parameters •  widths  ( list[int] ) – Width of each stage. •  bottleneck ratio  ( float ) – Bottleneck ratio. ", "page_idx": 289, "bbox": [137.45501708984375, 656.872802734375, 359.83953857421875, 706.676513671875], "page_size": [612.0, 792.0]}
{"layout": 2869, "type": "text", "text": "•  groups  ( int ) – number of groups in each stage Returns  The adjusted widths and groups of each stage. Return type  tuple(list) ", "page_idx": 290, "bbox": [137, 71.45246887207031, 362, 121.22525024414062], "page_size": [612.0, 792.0]}
{"layout": 2870, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 290, "bbox": [96, 126.9739761352539, 190.5148468017578, 150.51548767089844], "page_size": [612.0, 792.0]}
{"layout": 2871, "type": "text", "text": "generate reg net ( initial width ,  width slope ,  width parameter ,  depth ,  diviso  $r{=}8.$  ) Generates per block width from RegNet parameters. ", "page_idx": 290, "bbox": [96, 154.98899841308594, 434.11639404296875, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 2872, "type": "text", "text": "•  initial width  ( [int] ) – Initial width of the backbone •  width slope  ( [float] ) – Slope of the quantized linear function •  width parameter  ( [int] ) – Parameter used to quantize the width. •  depth  ( [int] ) – Depth of the backbone. •  divisor  ( int, optional ) – The divisor of channels. Defaults to 8. Returns  return a list of widths of each stage and the number of stages Return type  list, int ", "page_idx": 290, "bbox": [137, 202.9594268798828, 441.9471435546875, 324.4632263183594], "page_size": [612.0, 792.0]}
{"layout": 2873, "type": "text", "text": "get stages from blocks ( widths ) Gets widths/stage blocks of network at each stage. Parameters  widths  ( list[int] ) – Width in each stage. Returns  width and depth of each stage Return type  tuple(list) ", "page_idx": 290, "bbox": [96, 328.3390197753906, 370, 408.1492614746094], "page_size": [612.0, 792.0]}
{"layout": 2874, "type": "text", "text": "static quant ize float ( number ,  divisor ) Converts a float to closest non-zero int divisible by divisor. ", "page_idx": 290, "bbox": [96, 412.0250549316406, 362, 437.43951416015625], "page_size": [612.0, 792.0]}
{"layout": 2875, "type": "text", "text": "Parameters •  number  ( int ) – Original number to be quantized. •  divisor  ( int ) – Divisor used to quantize the number. Returns  quantized number that is divisible by devisor. Return type  int ", "page_idx": 290, "bbox": [137, 441.4348449707031, 382.7047424316406, 527.7012939453125], "page_size": [612.0, 792.0]}
{"layout": 2876, "type": "text", "text": "class  mmdet.models.backbones. Res2Net ( scales  $\\scriptstyle{:=4}$  ,  base_width  $\\iota{=}26$  ,  style  $=$  'pytorch' ,  deep_stem  $\\scriptstyle{\\mathcal{S}}$  True , avg_down  $.=$  True ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ,  \\*\\*kwargs ) ", "page_idx": 290, "bbox": [72.00003051757812, 531.5770874023438, 516.1293334960938, 556.9915771484375], "page_size": [612.0, 792.0]}
{"layout": 2877, "type": "text", "text": "Res2Net backbone. ", "page_idx": 290, "bbox": [96, 555.6365356445312, 173.7783966064453, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 2878, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 290, "bbox": [117, 575, 169, 586], "page_size": [612.0, 792.0]}
{"layout": 2879, "type": "text", "text": " scales  ( int ) – Scales used in Res2Net. Default: 4  base_width  ( int ) – Basic width of each scale. Default: 26  depth  ( int ) – Depth of res2net, from {50, 101, 152}.  in channels  ( int ) – Number of input image channels. Default: 3.  num_stages  ( int ) – Res2net stages. Default: 4.  strides  ( Sequence[int] ) – Strides of the first block of each stage.  dilations  ( Sequence[int] ) – Dilation of each stage. ", "page_idx": 290, "bbox": [149, 591.5015258789062, 430, 712.4085693359375], "page_size": [612.0, 792.0]}
{"layout": 2880, "type": "text", "text": "•  out indices  ( Sequence[int] ) – Output from which stages. •  style    $(s t r)$   –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  deep_stem  ( bool ) – Replace 7x7 conv in input stem with 3 3x3 conv •  avg_down  ( bool ) – Use AvgPool instead of stride conv when down sampling in the bot- tle2neck. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  plugins  ( list[dict] ) – List of plugins for stages, each dict contains: –  cfg (dict, required): Cfg dict to build plugin. –  position (str, required): Position inside block to insert plugin, options are ‘after con v 1’, ‘after con v 2’, ‘after con v 3’. –  stages (tuple[bool], optional): Stages to apply plugin, length should be same as ‘num_stages’. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – Whether to use zero init for last norm layer in resblocks to let them behave as identity. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 291, "bbox": [145, 71.45246887207031, 518, 431.4625244140625], "page_size": [612.0, 792.0]}
{"layout": 2881, "type": "text", "text": "Example ", "page_idx": 291, "bbox": [95, 449.3251037597656, 137.83335876464844, 463.591552734375], "page_size": [612.0, 792.0]}
{"layout": 2882, "type": "text", "text": " $>>$   from  mmdet.models  import  Res2Net  $>>$   import  torch  $>>$   self  $=$   Res2Net(depth  $\\scriptstyle{=50}$  , scales  ${=}4$  , base_width  $\\mathbf{\\lambda=}26.$  ) >>>  self . eval()  $>>$   inputs  $=$   torch . rand( 1 ,  3 ,  32 ,  32 )  $>>$   level outputs  $=$   self . forward(inputs)  $>>$   for  level_out  in  level outputs: ... print ( tuple (level_out . shape)) (1, 256, 8, 8) (1, 512, 4, 4) (1, 1024, 2, 2) (1, 2048, 1, 1) ", "page_idx": 291, "bbox": [95, 478.9009704589844, 376, 621.0171508789062], "page_size": [612.0, 792.0]}
{"layout": 2883, "type": "text", "text": "make res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer . ", "page_idx": 291, "bbox": [95, 633.4410400390625, 290, 658.8555297851562], "page_size": [612.0, 792.0]}
{"layout": 2884, "type": "text", "text": "class  mmdet.models.backbones. ResNeSt ( group  $s{=}I$  ,  base_width  $\\scriptstyle{:=4}$  ,  radix  $_{:=2}$  ,  reduction factor  $\\scriptstyle\\Leftarrow4$  , avg down stride  $:=$  True ,  \\*\\*kwargs ) ", "page_idx": 291, "bbox": [71.99996948242188, 663.3290405273438, 489.4035949707031, 688.7434692382812], "page_size": [612.0, 792.0]}
{"layout": 2885, "type": "text", "text": "ResNeSt backbone. ", "page_idx": 291, "bbox": [95, 687.388427734375, 175, 700.698486328125], "page_size": [612.0, 792.0]}
{"layout": 2886, "type": "text", "text": "Parameters ", "page_idx": 291, "bbox": [118, 704.69384765625, 170, 719.229248046875], "page_size": [612.0, 792.0]}
{"layout": 2887, "type": "text", "text": "•  groups  ( int ) – Number of groups of Bottleneck. Default: 1 •  base_width  ( int ) – Base width of Bottleneck. Default: 4 •  radix  ( int ) – Radix of Split Attention Con v 2 d. Default: 2 •  reduction factor  ( int ) – Reduction factor of inter channels in Split Attention Con v 2 d. Default: 4. •  avg down stride  ( bool ) – Whether to use average pool for stride in Bottleneck. Default: True. •  kwargs  ( dict ) – Keyword arguments for ResNet. ", "page_idx": 292, "bbox": [145, 71.45246887207031, 518, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 2888, "type": "text", "text": "make res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer . ", "page_idx": 292, "bbox": [96, 202.8099822998047, 290, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 2889, "type": "text", "text": "class  mmdet.models.backbones. ResNeXt ( group  $s{=}I$  ,  base_width  $\\scriptstyle{:=4}$  ,  \\*\\*kwargs ) ResNeXt backbone. ", "page_idx": 292, "bbox": [72.0, 232.69700622558594, 418, 258.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 2890, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 292, "bbox": [117, 264, 169, 275], "page_size": [612.0, 792.0]}
{"layout": 2891, "type": "text", "text": "•  depth  ( int ) – Depth of resnet, from {18, 34, 50, 101, 152}. •  in channels  ( int ) – Number of input image channels. Default: 3. •  num_stages  ( int ) – Resnet stages. Default: 4. •  groups  ( int ) – Group of resnext. •  base_width  ( int ) – Base width of resnext. •  strides  ( Sequence[int] ) – Strides of the first block of each stage. •  dilations  ( Sequence[int] ) – Dilation of each stage. •  out indices  ( Sequence[int] ) – Output from which stages. •  style  ( str ) –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  frozen stages  ( int ) – Stages to be frozen (all param fixed). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – whether to use zero init for last norm layer in resblocks to let them behave as identity. ", "page_idx": 292, "bbox": [145, 280.66748046875, 518, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 2892, "type": "text", "text": "make res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer ", "page_idx": 292, "bbox": [96, 591.3531494140625, 290, 616.7676391601562], "page_size": [612.0, 792.0]}
{"layout": 2893, "type": "text", "text": "class  mmdet.models.backbones. ResNet ( depth ,  in_channel  $\\mathfrak{s}{=}3$  ,  stem channels  $\\mathbf{:=}$  None ,  base channel  $\\scriptstyle s=64$  , num_stage  $s{=}4$  ,  strides=(1, 2, 2, 2) ,  dilations=(1, 1, 1, 1) , out_indice  $s{=}$  (0, 1, 2, 3) ,  style  $\\mathbf{\\dot{\\rho}}=\\mathbf{\\dot{\\rho}}$  'pytorch' ,  deep_stem  $\\scriptstyle{\\mathcal{S}}$  False , avg_down  $=$  False ,  frozen stages=- 1 ,  conv_cfg  $=$  None , norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} ,  norm_eval  $\\acute{=}$  True , dcn  $\\scriptstyle.\\equiv$  None ,  stage with dc n  $=$  (False, False, False, False) , plugins  $\\leftrightharpoons$  None ,  with_cp=False ,  zero in it residual  $\\leftrightharpoons$  True , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 293, "bbox": [72.0, 71.30303192138672, 522, 168.44859313964844], "page_size": [612.0, 792.0]}
{"layout": 2894, "type": "text", "text": "ResNet backbone. ", "text_level": 1, "page_idx": 293, "bbox": [96, 169.25, 169, 179], "page_size": [612.0, 792.0]}
{"layout": 2895, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 293, "bbox": [116, 187, 169, 197], "page_size": [612.0, 792.0]}
{"layout": 2896, "type": "text", "text": "•  depth  ( int ) – Depth of resnet, from {18, 34, 50, 101, 152}. •  stem channels  ( int | None ) – Number of stem channels. If not specified, it will be the same as  base channels . Default: None. •  base channels  ( int ) – Number of base channels of res layer. Default: 64. •  in channels  ( int ) – Number of input image channels. Default: 3. •  num_stages  ( int ) – Resnet stages. Default: 4. •  strides  ( Sequence[int] ) – Strides of the first block of each stage. •  dilations  ( Sequence[int] ) – Dilation of each stage. •  out indices  ( Sequence[int] ) – Output from which stages. •  style  ( str ) –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  deep_stem  ( bool ) – Replace  $7\\mathrm{x}7$   conv in input stem with 3 3x3 conv •  avg_down  ( bool ) – Use AvgPool instead of stride conv when down sampling in the bottle- neck. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  plugins  ( list[dict] ) – List of plugins for stages, each dict contains: –  cfg (dict, required): Cfg dict to build plugin. –  position (str, required): Position inside block to insert plugin, options are ‘after con v 1’, ‘after con v 2’, ‘after con v 3’. –  stages (tuple[bool], optional): Stages to apply plugin, length should be same as ‘num_stages’. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – Whether to use zero init for last norm layer in resblocks to let them behave as identity. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 293, "bbox": [145, 202.9595489501953, 522, 700.4536743164062], "page_size": [612.0, 792.0]}
{"layout": 2897, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 294, "bbox": [96, 72, 138, 85], "page_size": [612.0, 792.0]}
{"layout": 2898, "type": "table", "page_idx": 294, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_150.jpg", "bbox": [93, 96, 545, 248], "page_size": [612.0, 792.0], "ocr_text": ">>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\na1,\na,\na1,\na,\n\nfrom mndet.models import ResNet\n\nimport torch\n\nself = ResNet(depth=18)\n\nself.evalQ\n\ninputs = torch.rand(1, 3, 32, 32)\n\nlevel_outputs = self. forward(inputs)\n\nfor level_out in level_outputs:\nprint (tuple(level_out.shape))\n\n64, 8, 8)\n\n128, 4, 4)\n2565, 2, 2)\n5125 te 1)\n\n", "vlm_text": "The image shows a Python code snippet, specifically for testing the ResNet model from MMDetection using PyTorch. It performs the following steps:\n\n1. Imports the `ResNet` class from `mmdet.models` and the `torch` library.\n2. Initializes a ResNet model with a depth of 18 layers and sets it to evaluation mode.\n3. Creates a random input tensor with shape `(1, 3, 32, 32)`.\n4. Passes the inputs through the model to get outputs at different levels.\n5. Loops through the level outputs and prints their shapes. The printed shapes are:\n\n   - `(1, 64, 8, 8)`\n   - `(1, 128, 4, 4)`\n   - `(1, 256, 2, 2)`\n   - `(1, 512, 1, 1)`"}
{"layout": 2899, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 294, "bbox": [96, 256.7439880371094, 190.5148468017578, 280.2855224609375], "page_size": [612.0, 792.0]}
{"layout": 2900, "type": "text", "text": "make res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer . ", "page_idx": 294, "bbox": [96, 284.7590637207031, 290.2616271972656, 310.17352294921875], "page_size": [612.0, 792.0]}
{"layout": 2901, "type": "text", "text": "Make plugins for ResNet  stage_idx  th stage. Currently we support to insert  context block ,  empirical attention block ,  non local block  into the backbone like ResNet/ResNeXt. They could be inserted after conv1/conv2/conv3 of Bottleneck. ", "page_idx": 294, "bbox": [118, 326.7514953613281, 539.9998168945312, 369.94952392578125], "page_size": [612.0, 792.0]}
{"layout": 2902, "type": "text", "text": "An example of plugins format could be: ", "page_idx": 294, "bbox": [118, 374.572509765625, 277.55804443359375, 387.8825378417969], "page_size": [612.0, 792.0]}
{"layout": 2903, "type": "text", "text": "Examples ", "text_level": 1, "page_idx": 294, "bbox": [117, 407, 166, 419], "page_size": [612.0, 792.0]}
{"layout": 2904, "type": "table", "page_idx": 294, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_151.jpg", "bbox": [113, 430, 544, 690], "page_size": [612.0, 792.0], "ocr_text": ">>> plugins=[\n\ndict (cfg=dict(type='xxx', argl='xxx'),\nstages=(False, True, True, True),\nposition='after_conv2'),\n\ndict (cfg=dict(type='yyy'),\nstages=(True, True, True, True),\nposition='after_conv3'),\n\ndict (cfg=dict(type='zzz', postfix='1'),\nstages=(True, True, True, True),\nposition='after_conv3'),\n\ndict (cfg=dict(type='zzz', postfix='2'),\nstages=(True, True, True, True),\n\n« position='after_conv3')\n\nsaa |]\n\n>>> self = ResNet(depth=18)\n\n>>> stage_plugins = self.make_stage_plugins(plugins, 0)\n\n>>> assert len(stage_plugins) == 3\n\nSuppose stage_idx=9, the structure of blocks in the stage would be:\n\nconvl-> conv2->conv3->yyy->zzz1->zzz2\n\n", "vlm_text": "The table displays a Python code snippet. It shows a list of dictionaries named `plugins`, where each dictionary represents a configuration for a plugin with attributes such as `cfg`, `stages`, and `position`. These configurations appear to be for a neural network model, specifically using stages in a `ResNet`.\n\nHere’s a summary:\n\n1. Four plugin configurations are defined, each with specific `type`, `stages`, and `position` values.\n2. Two plugins of type `'zzz'` have different postfixes.\n3. The `ResNet` model is instantiated with a depth of 18.\n4. The function `make_stage_plugins` is called with the `plugins` list and a stage index of 0.\n5. An assertion checks that the length of `stage_plugins` is 3.\n\nThe result for `stage_idx=0` shows the structure of blocks in the stage: \n`conv1 -> conv2 -> conv3 -> yyy -> zzz1 -> zzz2`."}
{"layout": 2905, "type": "text", "text": "Suppose ‘stage_idx  $\\scriptstyle=1^{;}$  ’, the structure of blocks in the stage would be: ", "page_idx": 294, "bbox": [118, 695.4154663085938, 392.237548828125, 708.7255249023438], "page_size": [612.0, 792.0]}
{"layout": 2906, "type": "text", "text": "conv1-> conv2->xxx->conv3->yyy->zzz1->zzz2 If stages is missing, the plugin would be applied to all stages. ", "page_idx": 295, "bbox": [118, 79.55177307128906, 338.49945068359375, 89.5641860961914], "page_size": [612.0, 792.0]}
{"layout": 2907, "type": "text", "text": "", "page_idx": 295, "bbox": [118, 102.13746643066406, 362.03094482421875, 115.44750213623047], "page_size": [612.0, 792.0]}
{"layout": 2908, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 295, "bbox": [136, 121, 188, 133], "page_size": [612.0, 792.0]}
{"layout": 2909, "type": "text", "text": " plugins  ( list[dict] ) – List of plugins cfg to build. The postfix is required if multiple same type plugins are inserted. ", "page_idx": 295, "bbox": [159.37191772460938, 138.00245666503906, 521.3687133789062, 163.2675018310547], "page_size": [612.0, 792.0]}
{"layout": 2910, "type": "text", "text": "•  stage_idx  ( int ) – Index of stage to build ", "page_idx": 295, "bbox": [154, 167.8904571533203, 335.4613342285156, 181.2004852294922], "page_size": [612.0, 792.0]}
{"layout": 2911, "type": "text", "text": "Returns  Plugins for current stage ", "page_idx": 295, "bbox": [137, 185.19580078125, 273.5733642578125, 199.73123168945312], "page_size": [612.0, 792.0]}
{"layout": 2912, "type": "text", "text": "Return type  list[dict] ", "page_idx": 295, "bbox": [137, 203.1278076171875, 226.93882751464844, 217.66323852539062], "page_size": [612.0, 792.0]}
{"layout": 2913, "type": "text", "text": "property norm1 the normalization layer named “norm1” ", "page_idx": 295, "bbox": [96, 223.41195678710938, 277.38873291015625, 246.95347595214844], "page_size": [612.0, 792.0]}
{"layout": 2914, "type": "text", "text": "Type  nn.Module ", "page_idx": 295, "bbox": [137, 250.94879150390625, 206.4258575439453, 265.4842224121094], "page_size": [612.0, 792.0]}
{"layout": 2915, "type": "text", "text": "train ( mode  $=$  True ) Convert the model into training mode while keep normalization layer freezed. ", "page_idx": 295, "bbox": [96, 269.3600158691406, 428.50140380859375, 294.77447509765625], "page_size": [612.0, 792.0]}
{"layout": 2916, "type": "text", "text": "class  mmdet.models.backbones. ResNetV1d ( \\*\\*kwargs ) ResNetV1d variant described in  Bag of Tricks . ", "page_idx": 295, "bbox": [71, 299.2480163574219, 319.5763854980469, 324.6624755859375], "page_size": [612.0, 792.0]}
{"layout": 2917, "type": "text", "text": "Compared with default ResNet(ResNetV1b), ResNetV1d replaces the  $7\\mathrm{x}7$   conv in the input stem with three 3x3 convs. And in the down sampling block, a  $2\\mathrm{x}2$   avg_pool with stride 2 is added before conv, whose stride is changed to 1. ", "page_idx": 295, "bbox": [96, 329.28546142578125, 540, 366.5054626464844], "page_size": [612.0, 792.0]}
{"layout": 2918, "type": "text", "text": "class  mmdet.models.backbones. SSDVGG ( depth ,  with last poo  $\\leftrightharpoons$  False ,  ceil_mode  $=$  True ,  out indices=(3, 4) , out feature indices=(22, 34) ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None , input_size  $=$  None ,  l 2 norm scale  $=$  None ) VGG Backbone network for single-shot-detection. ", "page_idx": 295, "bbox": [71, 370.97900390625, 526.753662109375, 420.3034362792969], "page_size": [612.0, 792.0]}
{"layout": 2919, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 295, "bbox": [117, 426, 168, 438], "page_size": [612.0, 792.0]}
{"layout": 2920, "type": "text", "text": "•  with last pool  ( bool ) – Whether to add a pooling layer at the last of the model •  ceil_mode  ( bool ) – When True, will use  ceil  instead of  floor  to compute the output shape. •  out indices  ( Sequence[int] ) – Output from which stages. •  out feature indices  ( Sequence[int] ) – Output from which feature map. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 295, "bbox": [145, 460.7924499511719, 484.41766357421875, 474.10247802734375], "page_size": [612.0, 792.0]}
{"layout": 2921, "type": "text", "text": "", "page_idx": 295, "bbox": [145, 478.57501220703125, 518, 492.03448486328125], "page_size": [612.0, 792.0]}
{"layout": 2922, "type": "text", "text": "", "page_idx": 295, "bbox": [145, 496.657470703125, 405.1003723144531, 509.9674987792969], "page_size": [612.0, 792.0]}
{"layout": 2923, "type": "text", "text": "", "page_idx": 295, "bbox": [145, 514.5904541015625, 469.9469909667969, 527.9005126953125], "page_size": [612.0, 792.0]}
{"layout": 2924, "type": "text", "text": "", "page_idx": 295, "bbox": [145, 532.5234375, 444.9056701660156, 545.83349609375], "page_size": [612.0, 792.0]}
{"layout": 2925, "type": "text", "text": "", "page_idx": 295, "bbox": [145, 550.4554443359375, 518, 563.7655029296875], "page_size": [612.0, 792.0]}
{"layout": 2926, "type": "text", "text": "•  input_size  ( int, optional ) – Deprecated argumment. Width and height of input, from {300, 512}. ", "page_idx": 295, "bbox": [145, 568.388427734375, 518, 593.6535034179688], "page_size": [612.0, 792.0]}
{"layout": 2927, "type": "text", "text": "•  l 2 norm scale  ( float, optional ) – Deprecated argumment. L2 normalization layer init scale. ", "page_idx": 295, "bbox": [145, 598.2764282226562, 518, 623.54150390625], "page_size": [612.0, 792.0]}
{"layout": 2928, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 296, "bbox": [96, 72, 139, 85], "page_size": [612.0, 792.0]}
{"layout": 2929, "type": "table", "page_idx": 296, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_152.jpg", "bbox": [92, 97, 545, 236], "page_size": [612.0, 792.0], "ocr_text": ">>>\n>>>\n>>>\n>>>\n>>>\n(1,\na,\na,\na,\na,\n\nself = SSDVGG(input_size=300, depth=11)\n\nself.evalQ\n\ninputs = torch.rand(1, 3, 300, 300)\n\nlevel_outputs = self. forward(inputs)\n\nfor level_out in level_outputs:\nprint (tuple(level_out.shape))\n\n1024, 19, 19)\n\n512, 10, 10)\n\n256; 54 5)\n\n256, 3, 3)\n\n256, 1, 1)\n\n", "vlm_text": "This image is a display of Python code using the PyTorch library, specifically dealing with a neural network model, likely a variant of VGG used in SSD (Single Shot Multibox Detector) for object detection. The code is set up as follows:\n\n1. `self = SSDVGG(input_size=300, depth=11)`: This line creates an instance of an SSDVGG network, specifying an input size of 300x300 pixels and a depth of 11.\n   \n2. `self.eval()`: This line puts the model into evaluation mode, which affects certain layers like dropout and batchnorm to behave appropriately during evaluation.\n\n3. `inputs = torch.rand(1, 3, 300, 300)`: This line generates a random tensor simulating an input image batch for the model. The shape `(1, 3, 300, 300)` implies a batch size of 1, 3 color channels (probably RGB), and spatial dimensions of 300x300 pixels.\n\n4. `level_outputs = self.forward(inputs)`: This line performs a forward pass through the model using the `inputs`, resulting in multiple outputs (`level_outputs`). These outputs correspond to different feature maps or layers in the network.\n\n5. The `for` loop iterates over each output in `level_outputs`, printing the shape of each feature map.\n  \n- The shapes printed are:\n  - `(1, 1024, 19, 19)`: A feature map with a batch size of 1, 1024 channels, and spatial dimensions of 19x19.\n  - `(1, 512, 10, 10)`: A feature map with 512 channels and 10x10 spatial dimensions.\n  - `(1, 256, 5, 5)`: Two identical feature maps, each with 256 channels and 5x5 spatial dimensions.\n  - `(1, 256, 3, 3)`: A feature map with 256 channels and 3x3 spatial dimensions.\n  - `(1, 256, 1, 1)`: A feature map with 256 channels and a 1x1 spatial dimension.\n\nThese feature maps are characteristic of SSD networks, which use multiple layers of different resolutions for detecting objects of various sizes."}
{"layout": 2930, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 296, "bbox": [96, 244, 190.5148468017578, 268.33050537109375], "page_size": [612.0, 792.0]}
{"layout": 2931, "type": "text", "text": "in it weights ( pretrained  $\\leftrightharpoons$  None ) Initialize the weights. ", "page_idx": 296, "bbox": [96, 272.8040466308594, 239.0033416748047, 298.218505859375], "page_size": [612.0, 792.0]}
{"layout": 2932, "type": "text", "text": "( pre train img s iz  $\\scriptstyle{z e=224}$  ,  in channels=3 ,  embed_dims=96 , patch_size  $\\scriptstyle=4$  ,  window size=7 ,  mlp_ratio=4 ,  depths=(2, 2, 6, 2) ,  num_heads=(3, 6, 12, 24) ,  strides=(4, 2, 2, 2) , out indices  $=$  (0, 1, 2, 3) ,  qkv_bias  $\\mathbf{:=}$  True ,  qk_scale=None , patch_norm  $\\mathbf{\\beta}=$  True ,  drop_rate=0.0 ,  at tn drop rate=0.0 , drop path rate $\\mathord{=}\\!O.I$ , use abs pos embed=False,act_cfg  $=$  {'type': 'GELU'} ,  norm_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'LN'} , with_cp  $\\leftrightharpoons$  False ,  pretrained  $\\leftrightharpoons$  None , convert weights  $\\mathbf{\\varepsilon}=$  False ,  frozen stages=- 1 , init_cfg  $=$  None ) ", "page_idx": 296, "bbox": [302.135986328125, 302.6920471191406, 540, 423.6378173828125], "page_size": [612.0, 792.0]}
{"layout": 2933, "type": "text", "text": "Swin Transformer A PyTorch implement of  $:$   Swin Transformer: Hierarchical Vision Transformer using Shifted Windows  - ", "page_idx": 296, "bbox": [96, 422.242919921875, 540, 447.65838623046875], "page_size": [612.0, 792.0]}
{"layout": 2934, "type": "text", "text": "https://arxiv.org/abs/2103.14030 ", "page_idx": 296, "bbox": [118, 452.2803649902344, 249.25497436523438, 465.59039306640625], "page_size": [612.0, 792.0]}
{"layout": 2935, "type": "text", "text": "Inspiration from  https://github.com/microsoft/Swin-Transformer ", "page_idx": 296, "bbox": [96, 470.21337890625, 355.44598388671875, 483.5234069824219], "page_size": [612.0, 792.0]}
{"layout": 2936, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 296, "bbox": [117, 490, 169, 501], "page_size": [612.0, 792.0]}
{"layout": 2937, "type": "text", "text": "•  pre train img size  ( int | tuple[int] ) – The size of input image when pretrain. De- faults: 224. •  in channels  ( int ) – The num of input channels. Defaults: 3. •  embed_dims  ( int ) – The feature dimension. Default: 96. •  patch_size  ( int | tuple[int] ) – Patch size. Default: 4. •  window size  ( int ) – Window size. Default: 7. •  mlp_ratio  ( int ) – Ratio of mlp hidden dim to embedding dim. Default: 4. •  depths  ( tuple[int] ) – Depths of each Swin Transformer stage. Default: (2, 2, 6, 2). •  num_heads  ( tuple[int] ) – Parallel attention heads of each Swin Transformer stage. De- fault: (3, 6, 12, 24). •  strides  ( tuple[int] ) – The patch merging or patch embedding stride of each Swin Trans- former stage. (In swin, we set kernel size equal to stride.) Default: (4, 2, 2, 2). •  out indices  ( tuple[int] ) – Output from which stages. Default: (0, 1, 2, 3). ", "page_idx": 296, "bbox": [145, 506.0794372558594, 518, 716.6494140625], "page_size": [612.0, 792.0]}
{"layout": 2938, "type": "text", "text": "•  qkv_bias  ( bool, optional ) – If True, add a learnable bias to query, key, value. Default: True •  qk_scale  ( float | None, optional ) – Override default qk scale of head_dim \\*\\* -0.5 if set. Default: None. •  patch_norm  ( bool ) – If add a norm layer for patch embed and patch merging. Default: True. •  drop_rate  ( float ) – Dropout rate. Defaults: 0. •  at tn drop rate  ( float ) – Attention dropout rate. Default: 0. •  drop path rate  ( float ) – Stochastic depth rate. Defaults: 0.1. •  use abs pos embed  ( bool ) – If True, add absolute position embedding to the patch em- bedding. Defaults: False. •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\scriptstyle{:=}$  ’LN’). •  norm_cfg  ( dict ) – Config dict for normalization layer at output of backone. Defaults: dict $\\scriptstyle(\\mathrm{type}=\\mathrm{LN})$ ).•  with_cp  ( bool, optional ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False. •  pretrained  ( str, optional ) – model pretrained path. Default: None. •  convert weights  ( bool ) – The flag indicates whether the pre-trained model is from the original repo. We may need to convert some keys to make it compatible. Default: False. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. •  init_cfg  ( dict, optional ) – The Config for initialization. Defaults to None. ", "page_idx": 297, "bbox": [145, 71.45246887207031, 518, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 2939, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 297, "bbox": [96, 419.8760070800781, 313.48321533203125, 461.35052490234375], "page_size": [612.0, 792.0]}
{"layout": 2940, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 297, "bbox": [118, 477.2998046875, 540, 515.1484985351562], "page_size": [612.0, 792.0]}
{"layout": 2941, "type": "text", "text": "in it weights()", "text_level": 1, "page_idx": 297, "bbox": [96, 532, 171, 544.75], "page_size": [612.0, 792.0]}
{"layout": 2942, "type": "text", "text": "Initialize the weights. ", "page_idx": 297, "bbox": [118, 543.6814575195312, 204.58209228515625, 556.9915161132812], "page_size": [612.0, 792.0]}
{"layout": 2943, "type": "text", "text": "train ( mode  $=$  True ) Convert the model into training mode while keep layers freezed. ", "page_idx": 297, "bbox": [96, 561.4650268554688, 374.3447570800781, 586.8795166015625], "page_size": [612.0, 792.0]}
{"layout": 2944, "type": "text", "text": "class  mmdet.models.backbones. Trident Res Net ( depth ,  num_branch ,  test branch i dx ,  trident dilation s , \\*\\*kwargs ) ", "page_idx": 297, "bbox": [71.99998474121094, 591.3519897460938, 518, 616.6578979492188], "page_size": [612.0, 792.0]}
{"layout": 2945, "type": "text", "text": "The stem layer, stage 1 and stage 2 in Trident ResNet are identical to ResNet, while in stage 3, Trident Bottle Block is utilized to replace the normal Bottle Block to yield trident output. Different branch shares the convolution weight but uses different dilations to achieve multi-scale output. ", "page_idx": 297, "bbox": [96, 615.4124145507812, 540, 652.6325073242188], "page_size": [612.0, 792.0]}
{"layout": 2946, "type": "text", "text": "/ stage3(b0) x - stem - stage1 - stage2 - stage3(b1) - output stage3(b2) / ", "page_idx": 297, "bbox": [118, 657.2554321289062, 400.0083312988281, 670.5654907226562], "page_size": [612.0, 792.0]}
{"layout": 2947, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 297, "bbox": [117, 682, 169, 694], "page_size": [612.0, 792.0]}
{"layout": 2948, "type": "text", "text": "•  depth  ( int ) – Depth of resnet, from {50, 101, 152}. ", "page_idx": 297, "bbox": [145, 699.0984497070312, 365.8375549316406, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 2949, "type": "text", "text": "•  num_branch  ( int ) – Number of branches in TridentNet. •  test branch i dx  ( int ) – In inference, all 3 branches will be used if  test branch id  $\\scriptstyle{\\mathfrak{c}}==-I$  , otherwise only branch with index  test branch i dx  will be used. •  trident dilation s ( tuple[int] ) – Dilations of different trident branch. len(trident dilation s) should be equal to num_branch. ", "page_idx": 298, "bbox": [145, 71.45246887207031, 518, 144.5375213623047], "page_size": [612.0, 792.0]}
{"layout": 2950, "type": "text", "text": "39.3 necks ", "text_level": 1, "page_idx": 298, "bbox": [70, 167, 150, 184], "page_size": [612.0, 792.0]}
{"layout": 2951, "type": "text", "text": "class  mmdet.models.necks. BFP ( Balanced Feature Pyramids ) BFP takes multi-level features as inputs and gather them into a single one, then refine the gathered feature and scatter the refined results to multi-level features. This module is used in Libra R-CNN (CVPR 2019), see the paper  Libra R-CNN: Towards Balanced Learning for Object Detection  for details. ", "page_idx": 298, "bbox": [72, 200.5840301513672, 540, 249.90855407714844], "page_size": [612.0, 792.0]}
{"layout": 2952, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 298, "bbox": [117, 256, 169, 267], "page_size": [612.0, 792.0]}
{"layout": 2953, "type": "text", "text": "•  in channels  ( int ) – Number of input channels (feature maps of all levels should have the same channels). •  num_levels  ( int ) – Number of input feature levels. •  conv_cfg  ( dict ) – The config dict for convolution layers. •  norm_cfg  ( dict ) – The config dict for normalization layers. •  refine level  ( int ) – Index of integration and refine level of BSF in multi-level features from bottom to top. •  refine type  ( str ) – Type of the refine op, currently support [None, ‘conv’, ‘non_local’]. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 298, "bbox": [145, 272.46453857421875, 518, 417.2816162109375], "page_size": [612.0, 792.0]}
{"layout": 2954, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 298, "bbox": [96, 421.7551574707031, 190.51487731933594, 447.16961669921875], "page_size": [612.0, 792.0]}
{"layout": 2955, "type": "text", "text": "class  mmdet.models.necks. C TRes Net Neck ( in_channel ,  num dec on v filters ,  num dec on v kernels , use_dcn  $.=$  True ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) The neck used in  CenterNet  for object classification and box regression. ", "page_idx": 298, "bbox": [72, 451.6431579589844, 489.1646728515625, 489.0126037597656], "page_size": [612.0, 792.0]}
{"layout": 2956, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 298, "bbox": [117, 495, 169, 507], "page_size": [612.0, 792.0]}
{"layout": 2957, "type": "text", "text": "•  in_channel  ( int ) – Number of input channels. •  num dec on v filters  ( tuple[int] ) – Number of filters per stage. •  num dec on v kernels  ( tuple[int] ) – Number of kernels per stage. •  use_dcn  ( bool ) – If True, use DCNv2. Default: True. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 298, "bbox": [145, 511.5675964355469, 462, 596.6085815429688], "page_size": [612.0, 792.0]}
{"layout": 2958, "type": "text", "text": "forward ( inputs ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 298, "bbox": [96, 601.0821533203125, 313.4833068847656, 644.4296264648438], "page_size": [612.0, 792.0]}
{"layout": 2959, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 298, "bbox": [118, 660.3798217773438, 540, 698.2274780273438], "page_size": [612.0, 792.0]}
{"layout": 2960, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 299, "bbox": [95, 73.1760025024414, 204.58209228515625, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 2961, "type": "text", "text": "class  mmdet.models.necks. Channel Mapper ( in channels ,  out channels ,  kernel size  $\\scriptstyle{:=3}$  ,  conv_cfg=None , norm_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  act_cfg  $=$  {'type': 'ReLU'} ,  num_outs=None , init_cfg  $=$  {'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) ", "page_idx": 299, "bbox": [71, 101.19103240966797, 519, 150.4059600830078], "page_size": [612.0, 792.0]}
{"layout": 2962, "type": "text", "text": "Channel Mapper to reduce/increase channels of backbone features. This is used to reduce/increase channels of backbone features. ", "page_idx": 299, "bbox": [95, 149.16053771972656, 363, 162.47056579589844], "page_size": [612.0, 792.0]}
{"layout": 2963, "type": "text", "text": "", "page_idx": 299, "bbox": [95, 167.09352111816406, 343.8197937011719, 180.40354919433594], "page_size": [612.0, 792.0]}
{"layout": 2964, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 299, "bbox": [117, 186, 169, 197], "page_size": [612.0, 792.0]}
{"layout": 2965, "type": "text", "text": "•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale). •  kernel size  ( int, optional ) – kernel size for reducing channels (used at each scale). Default: 3. •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict, optional ) – Config dict for normalization layer. Default: None. •  act_cfg  ( dict, optional ) – Config dict for activation layer in ConvModule. Default: dict(type  $=^{!}$  ’ReLU’). •  num_outs  ( int, optional ) – Number of output feature maps. There would be extra con vs when num_outs larger than the length of in channels. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 299, "bbox": [145, 202.95948791503906, 519, 377.6635437011719], "page_size": [612.0, 792.0]}
{"layout": 2966, "type": "text", "text": "Example ", "page_idx": 299, "bbox": [95, 395.526123046875, 137.83303833007812, 409.7925720214844], "page_size": [612.0, 792.0]}
{"layout": 2967, "type": "text", "text": " $>>$   import  torch  $>>$   in channels  $=$   [ 2 ,  3 ,  5 ,  7 ]  $>>$   scales    $=$   [ 340 ,  170 ,  84 ,  43 ]  $>>$   inputs    $=$   [torch . rand( 1 , c, s, s) ... for  c, s  in  zip (in channels, scales)]  $>>$   self  $=$   Channel Mapper(in channels,  11 ,  3 ) . eval()  $>>$   outputs  $=$   self . forward(inputs)  $>>$   for  i  in  range ( len (outputs)): ... print ( f ' outputs[ { i } ].shape  $=$   { outputs[i] . shape } ' ) outputs[0].shape  $=$   torch.Size([1, 11, 340, 340]) outputs[1].shape  $=$   torch.Size([1, 11, 170, 170]) outputs[2].shape  $=$   torch.Size([1, 11, 84, 84]) outputs[3].shape  $=$   torch.Size([1, 11, 43, 43]) ", "page_idx": 299, "bbox": [95, 425.10198974609375, 395.7353820800781, 579.1741333007812], "page_size": [612.0, 792.0]}
{"layout": 2968, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 299, "bbox": [95, 591.5980224609375, 190.5148468017578, 617.0125122070312], "page_size": [612.0, 792.0]}
{"layout": 2969, "type": "text", "text": "class  mmdet.models.necks. Dilated Encoder ( in channels ,  out channels ,  block mid channels , num residual blocks ) ", "page_idx": 299, "bbox": [71, 621.4860229492188, 475.6556091308594, 646.7908935546875], "page_size": [612.0, 792.0]}
{"layout": 2970, "type": "text", "text": "Dilated Encoder for YOLOF < https://arxiv.org/abs/2103.09460 >\\`. ", "page_idx": 299, "bbox": [95, 645.5454711914062, 363, 658.8555297851562], "page_size": [612.0, 792.0]}
{"layout": 2971, "type": "text", "text": "This module contains two types of components: •  the original FPN lateral convolution layer and fpn convolution layer,  which are 1x1 conv   $^+$   3x3 conv ", "page_idx": 299, "bbox": [95, 662.850830078125, 540.0044555664062, 706.676513671875], "page_size": [612.0, 792.0]}
{"layout": 2972, "type": "text", "text": "• the dilated residual block ", "page_idx": 300, "bbox": [128.98599243164062, 71.45246887207031, 237.6986846923828, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 2973, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 300, "bbox": [117, 97, 169, 108], "page_size": [612.0, 792.0]}
{"layout": 2974, "type": "text", "text": "•  in channels  ( int ) – The number of input channels. •  out channels  ( int ) – The number of output channels. •  block mid channels  ( int ) – The number of middle block output channels •  num residual blocks  ( int ) – The number of residual blocks. ", "page_idx": 300, "bbox": [145, 113.29542541503906, 462.6242370605469, 180.40342712402344], "page_size": [612.0, 792.0]}
{"layout": 2975, "type": "text", "text": "forward ( feature ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 300, "bbox": [96, 190.8539276123047, 313.48321533203125, 234.20140075683594], "page_size": [612.0, 792.0]}
{"layout": 2976, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 300, "bbox": [118, 250.15179443359375, 540, 287.9994812011719], "page_size": [612.0, 792.0]}
{"layout": 2977, "type": "text", "text": "class  mmdet.models.necks. FPG ( in channels ,  out channels ,  num_outs ,  stack times ,  paths , inter channel  $\\leftrightharpoons$  None ,  same down tr an  $:=$  None , same up trans={'kernel size': 3, 'padding': 1, 'stride': 2, 'type': 'conv'} , across lateral trans  $\\mathbf{\\tilde{=}}$  {'kernel size': 1, 'type': 'conv'} , across down trans={'kernel size': 3, 'type': 'conv'} ,  across up trans  $\\mathbf{\\check{\\Sigma}}$  None , across skip trans={'type': 'identity'} ,  output trans={'kernel size': 3, 'type': 'last_conv'} ,  start level  $\\mathbf{\\chi}{=}0$  ,  end_level=- 1 ,  add extra con vs  $=$  False , norm_cfg $\\mathbf{\\dot{\\Sigma}}$ None, skip_inds $\\mathbf{\\varepsilon}=$ None, init_cfg $=$ [{'type': 'Caff e 2 Xavier', 'layer':'Conv2d'}, {'type': 'Constant', 'layer': ['_BatchNorm', 'Instance Norm', 'GroupNorm', 'LayerNorm'], 'val': 1.0}])", "page_idx": 300, "bbox": [72.0, 304.42803955078125, 533.080322265625, 425.37481689453125], "page_size": [612.0, 792.0]}
{"layout": 2978, "type": "text", "text": "FPG. ", "text_level": 1, "page_idx": 300, "bbox": [96, 426, 117, 436], "page_size": [612.0, 792.0]}
{"layout": 2979, "type": "text", "text": "Implementation of  Feature Pyramid Grids (FPG) . This implementation only gives the basic structure stated in the paper. But users can implement different type of transitions to fully explore the the potential power of the structure of FPG. ", "page_idx": 300, "bbox": [96, 442.0623779296875, 540, 479.2823791503906], "page_size": [612.0, 792.0]}
{"layout": 2980, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 300, "bbox": [117, 485, 168, 496], "page_size": [612.0, 792.0]}
{"layout": 2981, "type": "text", "text": "•  in channels  ( int ) – Number of input channels (feature maps of all levels should have the same channels). •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  stack times  ( int ) – The number of times the pyramid architecture will be stacked. •  paths  ( list[str] ) – Specify the path order of each stack level. Each element in the list should be either ‘bu’ (bottom-up) or ‘td’ (top-down). •  inter channels  ( int ) – Number of inter channels. •  same up trans  ( dict ) – Transition that goes down at the same stage. •  same down trans  ( dict ) – Transition that goes up at the same stage. •  across lateral trans  ( dict ) – Across-pathway same-stage •  across down trans  ( dict ) – Across-pathway bottom-up connection. •  across up trans  ( dict ) – Across-pathway top-down connection. ", "page_idx": 300, "bbox": [145, 501.8383483886719, 518, 718.3863525390625], "page_size": [612.0, 792.0]}
{"layout": 2982, "type": "text", "text": "•  across skip trans  ( dict ) – Across-pathway skip connection. •  output trans  ( dict ) – Transition that trans the output of the last stage. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool ) – It decides whether to add conv layers on top of the original fea- ture maps. Default to False. If True, its actual mode is specified by  extra con vs on inputs . •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 301, "bbox": [145, 71.45246887207031, 518, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 2983, "type": "text", "text": "forward ( inputs ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 301, "bbox": [96, 232.69700622558594, 313.4832763671875, 276.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 2984, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 301, "bbox": [118, 291.99481201171875, 540, 329.8434753417969], "page_size": [612.0, 792.0]}
{"layout": 2985, "type": "text", "text": "class  mmdet.models.necks. FPN ( in channels ,  out channels ,  num_outs ,  start_leve  ${\\mathit{l}}{=}{\\mathit{O}}$  ,  end_level=- 1 , add extra con vs  $\\mathbf{=}$  False ,  re lu before extra con vs  $\\mathbf{\\check{\\mathbf{\\Psi}}}$  False , no norm on later a  $\\leftrightharpoons$  False ,  conv_cfg  $\\leftrightharpoons$  None ,  norm_cfg  $\\scriptstyle{\\tilde{\\mathbf{\\alpha}}}=I$  None ,  act_cfg  $\\mathbf{\\beta}=$  None , up sample cf g  $\\mathbf{\\dot{\\Sigma}}$  {'mode': 'nearest'} ,  init_cfg  $=$  {'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) ", "page_idx": 301, "bbox": [72.0, 346.27203369140625, 540, 407.4418640136719], "page_size": [612.0, 792.0]}
{"layout": 2986, "type": "text", "text": "Feature Pyramid Network. ", "page_idx": 301, "bbox": [96, 406.1964111328125, 202.18153381347656, 419.5064392089844], "page_size": [612.0, 792.0]}
{"layout": 2987, "type": "text", "text": "This is an implementation of paper  Feature Pyramid Networks for Object Detection . ", "page_idx": 301, "bbox": [96, 424.1294250488281, 431.7198181152344, 437.439453125], "page_size": [612.0, 792.0]}
{"layout": 2988, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 301, "bbox": [117, 443, 168, 455], "page_size": [612.0, 792.0]}
{"layout": 2989, "type": "text", "text": "•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool | str ) – If bool, it decides whether to add conv layers on top of the original feature maps. Default to False. If True, it is equivalent to add extra con vs  $=$  ’on_input’ . If str, it specifies the source feature map of the extra convs. Only the following options are allowed –  ’on_input  $\\ddots$   Last feat map of neck inputs (i.e. backbone feature). –  ’on_lateral’: Last feature map after lateral convs. –  ’on_output’: The last output feature map after fpn convs. •  re lu before extra con vs  ( bool ) – Whether to apply relu before the extra conv. Default: False. ", "page_idx": 301, "bbox": [145, 459.9954528808594, 518, 706.4305419921875], "page_size": [612.0, 792.0]}
{"layout": 2990, "type": "text", "text": "•  no norm on lateral  ( bool ) – Whether to apply norm on lateral. Default: False. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  act_cfg  ( str ) – Config dict for activation layer in ConvModule. Default: None. •  up sample cf g  ( dict ) – Config dict for interpolate layer. Default:  dict(mode  $\\mathbf{=}$  ’nearest’) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 302, "bbox": [145, 71.45246887207031, 509.16986083984375, 174.4254608154297], "page_size": [612.0, 792.0]}
{"layout": 2991, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 302, "bbox": [95, 194, 138, 206], "page_size": [612.0, 792.0]}
{"layout": 2992, "type": "text", "text": " $>>$   import  torch  $>>$   in channels  $=$   [ 2 ,  3 ,  5 ,  7 ]  $>>$   scales  $=$   [ 340 ,  170 ,  84 ,  43 ]  $>>$   inputs  $=$   [torch . rand( 1 , c, s, s) ... for  c, s  in  zip (in channels, scales)]  $>>$   self  $=$   FPN(in channels,  11 ,  len (in channels)) . eval()  $>>$   outputs  $=$   self . forward(inputs)  $>>$   for  i  in  range ( len (outputs)): ... print ( f ' outputs[ { i } ].shape  $=$   { outputs[i] . shape } ' ) outputs[0].shape  $=$   torch.Size([1, 11, 340, 340]) outputs[1].shape  $=$   torch.Size([1, 11, 170, 170]) outputs[2].shape  $=$   torch.Size([1, 11, 84, 84]) outputs[3].shape  $=$   torch.Size([1, 11, 43, 43]) ", "page_idx": 302, "bbox": [95, 221.86398315429688, 395.7353820800781, 375.9371032714844], "page_size": [612.0, 792.0]}
{"layout": 2993, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 302, "bbox": [95, 388.3600158691406, 190.5148468017578, 413.77447509765625], "page_size": [612.0, 792.0]}
{"layout": 2994, "type": "text", "text": "class  mmdet.models.necks. FPN_CARAFE ( in channels ,  out channels ,  num_outs ,  start_leve  $\\mathbf{\\chi}{=}0$  ,  end_level=- 1 , norm_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  act_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  order=('conv', 'norm', 'act') , up sample cf g $=$ {'encoder dilation': 1, 'encoder kernel': 3, 'type':'carafe', 'up_group': 1, 'up_kernel': 5} ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ) ", "page_idx": 302, "bbox": [71.99998474121094, 418.2480163574219, 530.9976196289062, 467.57244873046875], "page_size": [612.0, 792.0]}
{"layout": 2995, "type": "text", "text": "FPN_CARAFE is a more flexible implementation of FPN. It allows more choice for upsample methods during the top-down pathway. ", "page_idx": 302, "bbox": [95, 466.2184143066406, 540, 491.4834289550781], "page_size": [612.0, 792.0]}
{"layout": 2996, "type": "text", "text": "It can reproduce the performance of ICCV 2019 paper CARAFE: Content-Aware ReAssembly of FEatures Please refer to  https://arxiv.org/abs/1905.02188  for more details. ", "page_idx": 302, "bbox": [95, 496.1064147949219, 540, 521.3713989257812], "page_size": [612.0, 792.0]}
{"layout": 2997, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 302, "bbox": [117, 528, 168, 539], "page_size": [612.0, 792.0]}
{"layout": 2998, "type": "text", "text": "•  in channels  ( list[int] ) – Number of channels for each input feature map. •  out channels  ( int ) – Output channels of feature pyramids. •  num_outs  ( int ) – Number of output stages. •  start level  ( int ) – Start level of feature pyramids. (Default: 0) •  end_level  ( int ) – End level of feature pyramids. (Default: -1 indicates the last level). •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  activate  ( str ) – Type of activation function in ConvModule (Default: None indicates w/o activation). •  order  ( dict ) – Order of components in ConvModule. •  upsample  ( str ) – Type of upsample layer. ", "page_idx": 302, "bbox": [145, 543.9263305664062, 518.0781860351562, 712.6533813476562], "page_size": [612.0, 792.0]}
{"layout": 2999, "type": "text", "text": "•  up sample cf g  ( dict ) – Dictionary to construct and config upsample layer. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 303, "bbox": [145, 71.45246887207031, 518, 102.69451141357422], "page_size": [612.0, 792.0]}
{"layout": 3000, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 303, "bbox": [96, 107.16802215576172, 190, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 3001, "type": "text", "text": "in it weights()Initialize the weights of module. ", "page_idx": 303, "bbox": [96, 138.92898559570312, 247.75001525878906, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 3002, "type": "text", "text": "slice_as(src, dst)Slice  src  as  dst ", "page_idx": 303, "bbox": [96, 166.9440155029297, 185.98207092285156, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 3003, "type": "text", "text": "Note:  src  should have the same or larger size than  dst ", "page_idx": 303, "bbox": [118, 208.308837890625, 343, 222.84426879882812], "page_size": [612.0, 792.0]}
{"layout": 3004, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 303, "bbox": [136, 246, 188, 258], "page_size": [612.0, 792.0]}
{"layout": 3005, "type": "text", "text": "•  src  ( torch.Tensor ) – Tensors to be sliced. •  dst  ( torch.Tensor ) –  src  will be sliced to have the same size as  dst . Returns  Sliced tensor. Return type  torch.Tensor ", "page_idx": 303, "bbox": [137, 262.7344665527344, 451.8556213378906, 330.4412841796875], "page_size": [612.0, 792.0]}
{"layout": 3006, "type": "text", "text": "tensor_add  $(a,b)$  Add tensors  a  and  b  that might have different sizes. ", "page_idx": 303, "bbox": [96, 342, 323.2176513671875, 365.7085266113281], "page_size": [612.0, 792.0]}
{"layout": 3007, "type": "text", "text": "class  mmdet.models.necks. HRFPN ( High Resolution Feature Pyramids ) paper:  High-Resolution Representations for Labeling Pixels and Regions . ", "page_idx": 303, "bbox": [71, 370.18206787109375, 389.6081237792969, 395.5965270996094], "page_size": [612.0, 792.0]}
{"layout": 3008, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 303, "bbox": [117, 401, 168, 413], "page_size": [612.0, 792.0]}
{"layout": 3009, "type": "text", "text": "•  in channels  ( list ) – number of channels for each branch. •  out channels  ( int ) – output channels of feature pyramids. •  num_outs  ( int ) – number of output stages. •  pooling type  ( str ) – pooling for generating feature pyramids from {MAX, AVG}. •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  stride  ( int ) – stride of 3x3 convolutional layers •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 303, "bbox": [145, 418.15252685546875, 518, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 3010, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 303, "bbox": [96, 591.3521118164062, 190, 616.7676391601562], "page_size": [612.0, 792.0]}
{"layout": 3011, "type": "text", "text": "class  mmdet.models.necks. NAS FCO S FP N ( in channels ,  out channels ,  num_outs ,  start level  $\\scriptstyle{\\dot{=}}I$  ,  end_level=- 1 , add extra con v  $\\mathbf{:=}$  False ,  conv_cfg  $\\mathbf{\\beta}=$  None ,  norm_cfg  $=$  None , init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ) ", "page_idx": 303, "bbox": [71, 621.2401733398438, 536.2285766601562, 658.5009765625], "page_size": [612.0, 792.0]}
{"layout": 3012, "type": "text", "text": "FPN structure in NASFPN. ", "page_idx": 303, "bbox": [96, 657.2555541992188, 205.49923706054688, 670.5656127929688], "page_size": [612.0, 792.0]}
{"layout": 3013, "type": "text", "text": "Implementation of paper  NAS-FCOS: Fast Neural Architecture Search for Object Detection ", "page_idx": 303, "bbox": [96, 675.1885375976562, 462, 688.4985961914062], "page_size": [612.0, 792.0]}
{"layout": 3014, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 303, "bbox": [117, 694, 169, 706], "page_size": [612.0, 792.0]}
{"layout": 3015, "type": "text", "text": "•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool ) – It decides whether to add conv layers on top of the original fea- ture maps. Default to False. If True, its actual mode is specified by  extra con vs on inputs . •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 304, "bbox": [145, 71.45246887207031, 518, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 3016, "type": "text", "text": "forward ( inputs ) Forward function. in it weights()Initialize the weights of module. ", "page_idx": 304, "bbox": [96, 268.56298828125, 247.75001525878906, 323.8654479980469], "page_size": [612.0, 792.0]}
{"layout": 3017, "type": "text", "text": "class  mmdet.models.necks. NASFPN ( in channels ,  out channels ,  num_outs ,  stack times ,  start_leve  $\\mathord{\\left/{\\vphantom{\\left(\\frac{\\partial U}{\\partial t}\\right)}}\\right.\\kern-\\nulldelimiterspace}l\\mathrm{=}O$  , end_level  $\\mathbf{\\chi}=\\!\\cdot$  - 1 ,  add extra con vs  $\\mathbf{=}$  False ,  norm_cfg  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'type': 'Caff e 2 Xavier'} ) ", "page_idx": 304, "bbox": [71, 328.3389892578125, 540.00048828125, 365.5988464355469], "page_size": [612.0, 792.0]}
{"layout": 3018, "type": "text", "text": "NAS-FPN. ", "text_level": 1, "page_idx": 304, "bbox": [96, 366.25, 139, 376], "page_size": [612.0, 792.0]}
{"layout": 3019, "type": "text", "text": "Implementation of  NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection ", "page_idx": 304, "bbox": [96, 382.2864074707031, 496, 395.596435546875], "page_size": [612.0, 792.0]}
{"layout": 3020, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 304, "bbox": [117, 402, 169, 413], "page_size": [612.0, 792.0]}
{"layout": 3021, "type": "text", "text": "•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  stack times  ( int ) – The number of times the pyramid architecture will be stacked. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool ) – It decides whether to add conv layers on top of the original fea- ture maps. Default to False. If True, its actual mode is specified by  extra con vs on inputs . •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 304, "bbox": [145, 418.1524353027344, 518, 592.8565063476562], "page_size": [612.0, 792.0]}
{"layout": 3022, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 304, "bbox": [96, 597.3300170898438, 190, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 3023, "type": "text", "text": "class  mmdet.models.necks. PAFPN ( in channels ,  out channels ,  num_outs ,  start_leve  ${\\mathit{l}}{=}{\\mathit{O}}$  ,  end_level=- 1 , add extra con vs  $\\mathbf{:=}$  False ,  re lu before extra con vs  $\\mathbf{\\hat{\\rho}}$  False , no norm on lateral  $'=$  False ,  conv_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}\\mathbf{0}$  None ,  norm_cfg  $\\leftrightharpoons$  None , act_cfg  $\\mathbf{\\dot{\\Sigma}}$  None ,  init_cfg  $=$  {'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) ", "page_idx": 304, "bbox": [71, 627.218017578125, 530.6592407226562, 688.388916015625], "page_size": [612.0, 792.0]}
{"layout": 3024, "type": "text", "text": "Path Aggregation Network for Instance Segmentation. This is an implementation of the  PAFPN in Path Aggregation Network . ", "page_idx": 304, "bbox": [96, 687.1434326171875, 380.3226013183594, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 3025, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 305, "bbox": [118, 73, 169, 84], "page_size": [612.0, 792.0]}
{"layout": 3026, "type": "text", "text": "•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool | str ) – If bool, it decides whether to add conv layers on top of the original feature maps. Default to False. If True, it is equivalent to add extra con vs  $=$  ’on_input’ . If str, it specifies the source feature map of the extra convs. Only the following options are allowed –  ’on_input’: Last feat map of neck inputs (i.e. backbone feature). –  ’on_lateral’: Last feature map after lateral convs. –  ’on_output’: The last output feature map after fpn convs. •  re lu before extra con vs  ( bool ) – Whether to apply relu before the extra conv. Default: False. •  no norm on lateral  ( bool ) – Whether to apply norm on lateral. Default: False. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  act_cfg  ( str ) – Config dict for activation layer in ConvModule. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 305, "bbox": [145, 89.38447570800781, 518, 425.4845886230469], "page_size": [612.0, 792.0]}
{"layout": 3027, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 305, "bbox": [96, 429.9581298828125, 190, 455.3725891113281], "page_size": [612.0, 792.0]}
{"layout": 3028, "type": "text", "text": "class  mmdet.models.necks. RFP ( Recursive Feature Pyramid ) This is an implementation of RFP in  DetectoRS . Different from standard FPN, the input of RFP should be multi level features along with origin input image of backbone. ", "page_idx": 305, "bbox": [72.00001525878906, 459.84613037109375, 540.0034790039062, 497.215576171875], "page_size": [612.0, 792.0]}
{"layout": 3029, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 305, "bbox": [117, 503, 169, 515], "page_size": [612.0, 792.0]}
{"layout": 3030, "type": "text", "text": "•  rfp_steps  ( int ) – Number of unrolled steps of RFP. •  rfp backbone  ( dict ) – Configuration of the backbone for RFP. •  a spp out channels  ( int ) – Number of output channels of ASPP module. •  a spp dilation s  ( tuple[int] ) – Dilation rates of four branches. Default: (1, 3, 6, 1) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 305, "bbox": [145, 519.7715454101562, 518, 604.8126220703125], "page_size": [612.0, 792.0]}
{"layout": 3031, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 305, "bbox": [96, 609.2850952148438, 190, 634.6995849609375], "page_size": [612.0, 792.0]}
{"layout": 3032, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 305, "bbox": [96, 641.046142578125, 204.58209228515625, 664.5875854492188], "page_size": [612.0, 792.0]}
{"layout": 3033, "type": "text", "text": "class  mmdet.models.necks. SSDNeck ( in channels ,  out channels ,  level strides ,  level padding s , l 2 norm scale  $\\it{=}20.0$  ,  last kernel size  $\\scriptstyle{:=3}$  ,  use depth wise  $:=$  False , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $\\leftrightharpoons$  None ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'ReLU'} , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  [{'type': 'Xavier', 'distribution': 'uniform', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': 'Batch Norm 2 d'}] ) ", "page_idx": 306, "bbox": [71, 71.30303192138672, 523.79443359375, 132.4729766845703], "page_size": [612.0, 792.0]}
{"layout": 3034, "type": "text", "text": "Extra layers of SSD backbone to generate multi-scale feature maps. ", "page_idx": 306, "bbox": [96, 131.22853088378906, 364.94036865234375, 144.53855895996094], "page_size": [612.0, 792.0]}
{"layout": 3035, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 306, "bbox": [117, 151, 169, 162], "page_size": [612.0, 792.0]}
{"layout": 3036, "type": "text", "text": "•  in channels  ( Sequence[int] ) – Number of input channels per scale. •  out channels  ( Sequence[int] ) – Number of output channels per scale. •  level strides  ( Sequence[int] ) – Stride of 3x3 conv per level. •  level padding s  ( Sequence[int] ) – Padding size of 3x3 conv per level. •  l 2 norm scale  ( float|None ) – L2 normalization layer init scale. If None, not use L2 normalization on the first input feature. •  last kernel size  ( int ) – Kernel size of the last conv layer. Default: 3. •  use depth wise  ( bool ) – Whether to use Depth wise Separable Con v. Default: False. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: None. •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\fallingdotseq$  ReLU’). •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 306, "bbox": [145, 167.09352111816406, 518, 371.68658447265625], "page_size": [612.0, 792.0]}
{"layout": 3037, "type": "text", "text": "forward ( inputs ) Forward function. ", "page_idx": 306, "bbox": [96, 376.15911865234375, 190, 401.5745849609375], "page_size": [612.0, 792.0]}
{"layout": 3038, "type": "text", "text": "class  mmdet.models.necks. YOLOV3Neck ( num_scales ,  in channels ,  out channels ,  conv_cfg=None , norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} , act_cfg  $=$  {'negative slope': 0.1, 'type': 'LeakyReLU'} ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ) ", "page_idx": 306, "bbox": [71, 406.047119140625, 540, 443.4175720214844], "page_size": [612.0, 792.0]}
{"layout": 3039, "type": "text", "text": "The neck of YOLOV3. ", "page_idx": 306, "bbox": [96, 442.0625305175781, 190, 455.37255859375], "page_size": [612.0, 792.0]}
{"layout": 3040, "type": "text", "text": "It can be treated as a simplified version of FPN. It will take the result from Darknet backbone and do some upsampling and concatenation. It will finally output the detection result. ", "page_idx": 306, "bbox": [96, 459.99554443359375, 540, 485.26055908203125], "page_size": [612.0, 792.0]}
{"layout": 3041, "type": "text", "text": "Note: The input feats should be from top to bottom.  i.e., from high-lvl to low-lvl But YOLOV3Neck will process them in reversed order.  i.e., from bottom (high-lvl) to top (low-lvl) ", "page_idx": 306, "bbox": [96, 501.2108154296875, 503, 551.6112060546875], "page_size": [612.0, 792.0]}
{"layout": 3042, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 306, "bbox": [117, 575, 168, 586], "page_size": [612.0, 792.0]}
{"layout": 3043, "type": "text", "text": "•  num_scales  ( int ) – The number of scales / stages. •  in channels  ( List[int] ) – The number of input channels per scale. •  out channels  ( List[int] ) – The number of output channels per scale. •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict, optional ) – Dictionary to construct and config norm layer. Default: dict(type  $:=$  ’BN’, requires grad  $=$  True) •  act_cfg ( dict, optional ) – Config dict for activation layer. Default: dict(type  $:=$  ’LeakyReLU’, negative slope  $\\mathrm{\\Lambda}{=}0.1$  ). ", "page_idx": 306, "bbox": [145, 591.5014038085938, 518, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 3044, "type": "text", "text": "•  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 307, "bbox": [145, 71.45246887207031, 518, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3045, "type": "text", "text": "forward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 307, "bbox": [96, 95.21300506591797, 313.4832458496094, 138.5604705810547], "page_size": [612.0, 792.0]}
{"layout": 3046, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 307, "bbox": [118, 154.51080322265625, 540, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 3047, "type": "text", "text": "class  mmdet.models.necks. YOLOXPAFPN ( in channels ,  out channels ,  num csp block  $\\wp=3.$  , use depth wise  $\\mathbf{=}$  False ,  up sample cf g  $=$  {'mode': 'nearest', 'scale factor': 2} ,  conv_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ,  norm_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'Swish'} , init_cfg  $=$  {'a': 2.23606797749979, 'distribution': 'uniform', 'layer': 'Conv2d', 'mode': 'fan_in', 'non linearity': 'leaky_relu', 'type': 'Kaiming'} ) ", "page_idx": 307, "bbox": [72.0, 208.7870330810547, 525.3292236328125, 293.86798095703125], "page_size": [612.0, 792.0]}
{"layout": 3048, "type": "text", "text": "Path Aggregation Network used in YOLOX. ", "page_idx": 307, "bbox": [96, 292.6225280761719, 276, 305.93255615234375], "page_size": [612.0, 792.0]}
{"layout": 3049, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 307, "bbox": [117, 312, 169, 323], "page_size": [612.0, 792.0]}
{"layout": 3050, "type": "text", "text": "•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num csp blocks  ( int ) – Number of bottlenecks in CSPLayer. Default: 3 •  use depth wise  ( bool ) – Whether to depthwise separable convolution in blocks. Default: False •  up sample cf g  ( dict ) – Config dict for interpolate layer. Default:  dict(scale factor  ${\\it\\Delta\\phi}=\\!2{\\it\\Delta\\Psi}$  , mode  $\\mathbf{=}$  ’nearest’) •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None, which means using conv2d. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict(type  $=^{:}$  ’BN’) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $='$  Swish’) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. ", "page_idx": 307, "bbox": [145, 328.4885559082031, 518, 533.0806274414062], "page_size": [612.0, 792.0]}
{"layout": 3051, "type": "text", "text": "forward ( inputs ) ", "page_idx": 307, "bbox": [96, 537.5541381835938, 168.40286254882812, 550.9039916992188], "page_size": [612.0, 792.0]}
{"layout": 3052, "type": "text", "text": "Parameters  inputs  ( tuple[Tensor] ) – input features. Returns  YOLOXPAFPN features. Return type  tuple[Tensor] ", "page_idx": 307, "bbox": [137, 566.9639282226562, 367.321533203125, 617.3653564453125], "page_size": [612.0, 792.0]}
{"layout": 3053, "type": "text", "text": "39.4 dense heads ", "text_level": 1, "page_idx": 308, "bbox": [70, 72, 199, 88], "page_size": [612.0, 792.0]}
{"layout": 3054, "type": "text", "text": "class  mmdet.models.dense heads. ATSSHead ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{:=4}$  ,  conv_cfg  $\\mathbf{\\dot{\\Sigma}}$  None , norm_cfg  $=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  reg decoded b box  $\\mathbf{\\beta}=$  True , loss center ness={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'atss_cls', 'std': 0.01, 'type':'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) ", "page_idx": 308, "bbox": [72.0, 102.84502410888672, 536.9354858398438, 188.0355682373047], "page_size": [612.0, 792.0]}
{"layout": 3055, "type": "text", "text": "Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection. ", "page_idx": 308, "bbox": [96, 186.68055725097656, 533.55712890625, 199.99058532714844], "page_size": [612.0, 792.0]}
{"layout": 3056, "type": "text", "text": "ATSS head structure is similar with FCOS, however ATSS use anchor boxes and assign label by Adaptive Training Sample Selection instead max-iou. ", "page_idx": 308, "bbox": [96, 204.61354064941406, 540, 229.8785858154297], "page_size": [612.0, 792.0]}
{"layout": 3057, "type": "text", "text": "https://arxiv.org/abs/1912.02424 ", "page_idx": 308, "bbox": [96, 234.5015411376953, 227, 247.8115692138672], "page_size": [612.0, 792.0]}
{"layout": 3058, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 308, "bbox": [96, 252.2841033935547, 298.6692810058594, 277.6995849609375], "page_size": [612.0, 792.0]}
{"layout": 3059, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 308, "bbox": [137, 281.69390869140625, 522, 307.5865783691406], "page_size": [612.0, 792.0]}
{"layout": 3060, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 308, "bbox": [137, 313, 173, 325], "page_size": [612.0, 792.0]}
{"layout": 3061, "type": "text", "text": "Usually a tuple of classification scores and bbox prediction ", "page_idx": 308, "bbox": [154, 329.5149230957031, 401.67218017578125, 344.05035400390625], "page_size": [612.0, 792.0]}
{"layout": 3062, "type": "text", "text": "cls_scores (list[Tensor]): Classification scores for all scale  levels, each is a 4D-tensor, the channels number is num anchors \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for all scale  levels, each is a 4D- tensor, the channels number is num anchors   $^{*}\\,4$  . ", "page_idx": 308, "bbox": [164, 347.44793701171875, 522, 403.2286071777344], "page_size": [612.0, 792.0]}
{"layout": 3063, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 308, "bbox": [137, 409, 214, 421], "page_size": [612.0, 792.0]}
{"layout": 3064, "type": "text", "text": "forward single ( x ,  scale ) Forward feature of a single scale level. ", "page_idx": 308, "bbox": [96, 425.6341552734375, 272.2784118652344, 451.04962158203125], "page_size": [612.0, 792.0]}
{"layout": 3065, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 308, "bbox": [136, 457, 188, 468], "page_size": [612.0, 792.0]}
{"layout": 3066, "type": "text", "text": "•  x  ( Tensor ) – Features of a single scale level. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. ", "page_idx": 308, "bbox": [154, 473.6046142578125, 522, 504.84765625], "page_size": [612.0, 792.0]}
{"layout": 3067, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 308, "bbox": [136, 511, 172, 522], "page_size": [612.0, 792.0]}
{"layout": 3068, "type": "text", "text": "cls_score (Tensor): Cls scores for a single scale level  the channels number is num anchors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale  level, the channels number is num anchors \\* 4. centerness (Tensor): Centerness for a single scale level, the  channel number is (N, num anchors \\* 1, H, W). ", "page_idx": 308, "bbox": [154, 526.7750244140625, 522, 612.4436645507812], "page_size": [612.0, 792.0]}
{"layout": 3069, "type": "text", "text": "Return type  tuple ", "page_idx": 308, "bbox": [137, 616.43896484375, 213, 630.974365234375], "page_size": [612.0, 792.0]}
{"layout": 3070, "type": "text", "text": "get targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $\\leftleftarrows$  None , gt labels list=None ,  label channels  $\\mathrm{\\Sigma}_{:=I}$  ,  un map outputs  $\\mathbf{\\tilde{=}}$  True ) Get targets for ATSS head. ", "page_idx": 308, "bbox": [96, 634.8502197265625, 500, 672.2196655273438], "page_size": [612.0, 792.0]}
{"layout": 3071, "type": "text", "text": "This method is almost the same as  AnchorHead.get targets() . Besides returning the targets as the parent method does, it also returns the anchors as the first element of the returned tuple. ", "page_idx": 308, "bbox": [118, 676.6931762695312, 540, 702.107666015625], "page_size": [612.0, 792.0]}
{"layout": 3072, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  center ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 309, "bbox": [96, 71.30303192138672, 513.5743408203125, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3073, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 309, "bbox": [136, 102, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 3074, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  center ness es  ( list[Tensor] ) – Centerness for each scale level with shape (N, num anchors \\* 1, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components. ", "page_idx": 309, "bbox": [137, 119.27247619628906, 521, 330.4412841796875], "page_size": [612.0, 792.0]}
{"layout": 3075, "type": "text", "text": "loss single ( anchors ,  cls_score ,  bbox_pred ,  centerness ,  labels ,  label weights ,  b box targets , num total samples ) Compute loss of a single scale level. ", "page_idx": 309, "bbox": [96, 352.24908447265625, 474.8795166015625, 389.6185302734375], "page_size": [612.0, 792.0]}
{"layout": 3076, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 309, "bbox": [136, 395, 188, 407], "page_size": [612.0, 792.0]}
{"layout": 3077, "type": "text", "text": "•  cls_score  ( Tensor ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W). •  bbox_pred  ( Tensor ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W). •  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  num total samples  ( int ) – Number os positive samples that is reduced over all GPUs. ", "page_idx": 309, "bbox": [155, 412.1745300292969, 521, 592.8565673828125], "page_size": [612.0, 792.0]}
{"layout": 3078, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 309, "bbox": [137, 596.8518676757812, 308, 611.3872680664062], "page_size": [612.0, 792.0]}
{"layout": 3079, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 309, "bbox": [137, 614.784912109375, 256, 629.3203125], "page_size": [612.0, 792.0]}
{"layout": 3080, "type": "text", "text": "class  mmdet.models.dense heads. Anchor Free Head ( num classes ,  in channels ,  feat channels=256 , stacked con vs  $\\scriptstyle{\\prime}=4$  ,  strides=(4, 8, 16, 32, 64) , dc n on last con v  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}\\mathbf{0}$  False ,  conv_bias  $=$  'auto' , loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss weight':1.0, 'type': 'FocalLoss', 'use s igm oid': True} , loss_bbox={'loss weight': 1.0, 'type': 'IoULoss'} , bbox_coder={'type': 'Distance Point B Box Code r'} , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $\\leftrightharpoons$  None ,  train_cfg=None , test_cfg  $\\mathbf{\\beta}=$  None ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ) ", "page_idx": 310, "bbox": [72.0, 71.30303192138672, 535.6701049804688, 204.2040557861328], "page_size": [612.0, 792.0]}
{"layout": 3081, "type": "text", "text": "Anchor-free head (FCOS, Fovea, RepPoints, etc.). ", "page_idx": 310, "bbox": [96, 202.95960998535156, 298, 216.26963806152344], "page_size": [612.0, 792.0]}
{"layout": 3082, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 310, "bbox": [117, 223, 169, 233], "page_size": [612.0, 792.0]}
{"layout": 3083, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. Used in child classes. •  stacked con vs  ( int ) – Number of stacking convs of the head. •  strides  ( tuple ) – Downsample factor of each feature map. •  dc n on last con v  ( bool ) – If true, use dcn in the last layer of towers. Default: False. •  conv_bias    $(b o o I\\ \\ I\\ \\ s t r)-]$  If specified as  auto , it will be decided by the norm_cfg. Bias of conv will be set as True if  norm_cfg  is None, otherwise False. Default: “auto”. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  bbox_coder  ( dict ) – Config of bbox coder. Defaults ‘Distance Point B Box Code r’. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 310, "bbox": [145, 238.82460021972656, 518, 515.1487426757812], "page_size": [612.0, 792.0]}
{"layout": 3084, "type": "text", "text": "aug_test ( feats ,  img_metas ,  rescale  $:=$  False ) Test function with test time augmentation. ", "page_idx": 310, "bbox": [96, 519.6212768554688, 286.4849548339844, 545.0368041992188], "page_size": [612.0, 792.0]}
{"layout": 3085, "type": "text", "text": "•  feats  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains features for all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 310, "bbox": [154, 567.5917358398438, 522, 640.6777954101562], "page_size": [612.0, 792.0]}
{"layout": 3086, "type": "text", "text": "Returns  bbox results of each class Return type  list[ndarray] ", "page_idx": 310, "bbox": [137, 644.673095703125, 277, 677.1405029296875], "page_size": [612.0, 792.0]}
{"layout": 3087, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 310, "bbox": [96, 681.0162963867188, 298, 706.4307861328125], "page_size": [612.0, 792.0]}
{"layout": 3088, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 311, "bbox": [137, 70.8248291015625, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3089, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 311, "bbox": [137, 102, 172, 114], "page_size": [612.0, 792.0]}
{"layout": 3090, "type": "text", "text": "Usually contain classification scores and bbox predictions. cls_scores (list[Tensor]): Box scores for each scale level,  each is a 4D-tensor, the chan- nel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for each scale  level, each is a 4D- tensor, the channel number is num_points   $^{*}\\,4$  . ", "page_idx": 311, "bbox": [154, 118.64483642578125, 521, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 3091, "type": "text", "text": "Return type  tuple ", "page_idx": 311, "bbox": [137, 196.35382080078125, 213, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 3092, "type": "text", "text": "forward single  $(x)$  Forward features of a single scale level. ", "page_idx": 311, "bbox": [96, 216.63796997070312, 276.1534729003906, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3093, "type": "text", "text": "Parameters  x  ( Tensor ) – FPN feature maps of the specified stride. ", "page_idx": 311, "bbox": [137, 244.1748046875, 409.5935363769531, 258.7102355957031], "page_size": [612.0, 792.0]}
{"layout": 3094, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 311, "bbox": [137, 264, 172, 275], "page_size": [612.0, 792.0]}
{"layout": 3095, "type": "text", "text": "Scores for each class, bbox predictions, features  after classification and regression conv layers, some models needs these features like FCOS. ", "page_idx": 311, "bbox": [154, 280.0398254394531, 521, 305.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 3096, "type": "text", "text": "Return type  tuple ", "page_idx": 311, "bbox": [137, 309.9278259277344, 213, 324.4632568359375], "page_size": [612.0, 792.0]}
{"layout": 3097, "type": "text", "text": "get_points ( feat map sizes ,  dtype ,  device ,  flatten  $=$  False ) Get points according to feature map sizes. ", "page_idx": 311, "bbox": [96, 328.33905029296875, 330.11236572265625, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 3098, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 311, "bbox": [136, 360, 188, 371], "page_size": [612.0, 792.0]}
{"layout": 3099, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  dtype  ( torch.dtype ) – Type of points. •  device  ( torch.device ) – Device of points. ", "page_idx": 311, "bbox": [154, 376.3085021972656, 427.9340515136719, 425.48455810546875], "page_size": [612.0, 792.0]}
{"layout": 3100, "type": "text", "text": "Returns  points of each image. ", "page_idx": 311, "bbox": [137, 429.4798889160156, 261.26953125, 444.01531982421875], "page_size": [612.0, 792.0]}
{"layout": 3101, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 311, "bbox": [137, 449, 214, 462], "page_size": [612.0, 792.0]}
{"layout": 3102, "type": "text", "text": "abstract get targets ( points ,  gt b boxes list ,  gt labels list ) Compute regression, classification and centerness targets for points in multiple images. ", "page_idx": 311, "bbox": [96, 465.8231201171875, 465.07513427734375, 491.2375793457031], "page_size": [612.0, 792.0]}
{"layout": 3103, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 311, "bbox": [136, 497, 188, 509], "page_size": [612.0, 792.0]}
{"layout": 3104, "type": "text", "text": "•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). ", "page_idx": 311, "bbox": [154, 513.7935180664062, 521, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 3105, "type": "text", "text": "abstract loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute loss of the head. ", "page_idx": 311, "bbox": [96, 591.3521118164062, 505, 616.7676391601562], "page_size": [612.0, 792.0]}
{"layout": 3106, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 311, "bbox": [136, 622, 188, 634], "page_size": [612.0, 792.0]}
{"layout": 3107, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . ", "page_idx": 311, "bbox": [154, 639.3225708007812, 521, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 3108, "type": "text", "text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 312, "bbox": [154, 71.45246887207031, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 3109, "type": "text", "text": "class  mmdet.models.dense heads. AnchorHead ( num classes ,  in channels ,  feat channels  $\\imath{=}256$  , anchor_generator={'ratios': [0.5, 1.0, 2.0], 'scales': [8, 16, 32], 'strides': [4, 8, 16, 32, 64], 'type': 'AnchorGenerator'} , bbox_coder={'clip border': True, 'target means': (0.0, 0.0, 0.0, 0.0), 'target_stds': (1.0, 1.0, 1.0, 1.0), 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box  $\\mathbf{\\beta}=$  False , loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True}, loss_bbox={'beta':0.1111111111111111, 'loss_weight': 1.0, 'type': 'Smooth L 1 Loss'} ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg  $\\leftrightharpoons$  None , init_cfg  $\\scriptstyle{\\prime}=$  {'layer': 'Conv2d', 'std': 0.01, 'type': 'Normal'} ) ", "page_idx": 312, "bbox": [71.9999771118164, 178.89903259277344, 529, 311.8009948730469], "page_size": [612.0, 792.0]}
{"layout": 3110, "type": "text", "text": "Anchor-based head (RPN, RetinaNet, SSD, etc.). ", "page_idx": 312, "bbox": [96, 310.5555419921875, 291.6552734375, 323.8655700683594], "page_size": [612.0, 792.0]}
{"layout": 3111, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 312, "bbox": [117, 330, 168, 341], "page_size": [612.0, 792.0]}
{"layout": 3112, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. Used in child classes. •  anchor generator  ( dict ) – Config dict for anchor generator •  bbox_coder  ( dict ) – Config of bounding box coder. •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 312, "bbox": [145, 346.42156982421875, 521, 574.9246215820312], "page_size": [612.0, 792.0]}
{"layout": 3113, "type": "text", "text": "aug_test ( feats ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Test function with test time augmentation. ", "page_idx": 312, "bbox": [96, 579.3971557617188, 286.48486328125, 604.8126220703125], "page_size": [612.0, 792.0]}
{"layout": 3114, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 312, "bbox": [136, 611, 187, 622], "page_size": [612.0, 792.0]}
{"layout": 3115, "type": "text", "text": "•  feats  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains features for all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 312, "bbox": [154, 627.3675537109375, 521, 700.45361328125], "page_size": [612.0, 792.0]}
{"layout": 3116, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 312, "bbox": [136, 707, 171, 717], "page_size": [612.0, 792.0]}
{"layout": 3117, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is  bboxes  with shape (n, 5), where 5 rep- resent (tl_x, tl_y, br_x, br_y, score). The shape of the second tensor in the tuple is  labels with shape (n,), The length of list should always be 1. ", "page_idx": 313, "bbox": [154, 70.8248291015625, 521, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 3118, "type": "text", "text": "Return type  list[tuple[Tensor, Tensor]] ", "page_idx": 313, "bbox": [137, 112.6678466796875, 298, 127.20327758789062], "page_size": [612.0, 792.0]}
{"layout": 3119, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 313, "bbox": [96, 131.0790252685547, 298, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 3120, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 313, "bbox": [137, 160.48785400390625, 521, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 3121, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 313, "bbox": [136, 192, 172, 204], "page_size": [612.0, 792.0]}
{"layout": 3122, "type": "text", "text": "A tuple of classification scores and bbox prediction. • cls_scores (list[Tensor]): Classification scores for all scale levels, each is a 4D-tensor, the channels number is num base priors \\* num classes. • bbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-tensor, the channels number is num base priors   $^{*}\\,4$  . ", "page_idx": 313, "bbox": [154, 208.9364776611328, 521, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 3123, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 313, "bbox": [137, 288, 214, 300], "page_size": [612.0, 792.0]}
{"layout": 3124, "type": "text", "text": "forward single  $(x)$  Forward feature of a single scale level. ", "page_idx": 313, "bbox": [96, 306, 272.2789001464844, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 3125, "type": "text", "text": "Parameters  x  ( Tensor ) – Features of a single scale level. ", "page_idx": 313, "bbox": [137, 333.8378601074219, 371.17767333984375, 348.373291015625], "page_size": [612.0, 792.0]}
{"layout": 3126, "type": "text", "text": "Returns  cls_score (Tensor): Cls scores for a single scale level the channels number is num base priors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num base priors  $^{*}\\,4$  . ", "page_idx": 313, "bbox": [137, 351.7708740234375, 521, 389.6185302734375], "page_size": [612.0, 792.0]}
{"layout": 3127, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 313, "bbox": [136, 396, 214, 407], "page_size": [612.0, 792.0]}
{"layout": 3128, "type": "text", "text": "get anchors ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get anchors according to feature map sizes. ", "page_idx": 313, "bbox": [96, 412.02508544921875, 329, 437.4395446777344], "page_size": [612.0, 792.0]}
{"layout": 3129, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 313, "bbox": [136, 443, 188, 455], "page_size": [612.0, 792.0]}
{"layout": 3130, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. ", "page_idx": 313, "bbox": [154, 459.99554443359375, 427, 473.3055725097656], "page_size": [612.0, 792.0]}
{"layout": 3131, "type": "text", "text": "•  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – Device for returned tensors ", "page_idx": 313, "bbox": [154, 477.92755126953125, 417.6425476074219, 509.17059326171875], "page_size": [612.0, 792.0]}
{"layout": 3132, "type": "text", "text": "Returns  anchor list (list[Tensor]): Anchors of each image. valid flag list (list[Tensor]): Valid flags of each image. ", "page_idx": 313, "bbox": [137, 513.1658935546875, 521, 539.05859375], "page_size": [612.0, 792.0]}
{"layout": 3133, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 313, "bbox": [137, 545, 214, 557], "page_size": [612.0, 792.0]}
{"layout": 3134, "type": "text", "text": "get targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $t{=}$  None , gt labels list  $=$  None ,  label channels  $\\scriptstyle{\\prime=}I$  ,  un map outputs  $\\mathbf{\\tilde{=}}$  True , return sampling result  $\\mathbf{\\hat{=}}$  False ) Compute regression and classification targets for anchors in multiple images. ", "page_idx": 313, "bbox": [96, 561.4651489257812, 499.29754638671875, 610.7896118164062], "page_size": [612.0, 792.0]}
{"layout": 3135, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 313, "bbox": [136, 616, 188, 628], "page_size": [612.0, 792.0]}
{"layout": 3136, "type": "text", "text": "•  anchor list  ( list[list[Tensor]] ) – Multi level anchors of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, 4). •  valid flag list  ( list[list[Tensor]] ) – Multi level valid flags of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, ) ", "page_idx": 313, "bbox": [154, 633.3455200195312, 521, 712.4086303710938], "page_size": [612.0, 792.0]}
{"layout": 3137, "type": "text", "text": "•  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. ", "page_idx": 314, "bbox": [154, 71.45246887207031, 508.61651611328125, 174.4254608154297], "page_size": [612.0, 792.0]}
{"layout": 3138, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 314, "bbox": [136, 180, 173, 192], "page_size": [612.0, 792.0]}
{"layout": 3139, "type": "text", "text": "Usually returns a tuple containing learning targets. • labels list (list[Tensor]): Labels of each level. • label weights list (list[Tensor]): Label weights of each level. • b box targets list (list[Tensor]): BBox targets of each level. • b box weights list (list[Tensor]): BBox weights of each level. • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. ", "page_idx": 314, "bbox": [154, 196.9813995361328, 418, 317.8874206542969], "page_size": [612.0, 792.0]}
{"layout": 3140, "type": "text", "text": "additional returns: This function enables user-defined returns from self.get targets single . These returns are currently refined to properties at each feature map (i.e. having HxW dimension). The results will be concatenated after the end ", "page_idx": 314, "bbox": [154, 327.8607482910156, 521, 365.7084045410156], "page_size": [612.0, 792.0]}
{"layout": 3141, "type": "text", "text": "Return type  tuple ", "page_idx": 314, "bbox": [137, 375.68072509765625, 213.25010681152344, 390.2161560058594], "page_size": [612.0, 792.0]}
{"layout": 3142, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 314, "bbox": [96, 394.0919494628906, 458.24127197265625, 419.50640869140625], "page_size": [612.0, 792.0]}
{"layout": 3143, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 314, "bbox": [136, 425, 187, 436], "page_size": [612.0, 792.0]}
{"layout": 3144, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None ", "page_idx": 314, "bbox": [154, 442.0624084472656, 521, 604.8114013671875], "page_size": [612.0, 792.0]}
{"layout": 3145, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 314, "bbox": [137, 608.8067626953125, 308, 623.3421630859375], "page_size": [612.0, 792.0]}
{"layout": 3146, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 314, "bbox": [137, 626.73974609375, 256.119140625, 641.275146484375], "page_size": [612.0, 792.0]}
{"layout": 3147, "type": "text", "text": "loss single ( cls_score ,  bbox_pred ,  anchors ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) Compute loss of a single scale level. ", "page_idx": 314, "bbox": [96, 645.1509399414062, 487.0234375, 682.5204467773438], "page_size": [612.0, 792.0]}
{"layout": 3148, "type": "text", "text": "Parameters ", "page_idx": 314, "bbox": [137, 686.5157470703125, 186.39108276367188, 701.0511474609375], "page_size": [612.0, 792.0]}
{"layout": 3149, "type": "text", "text": "•  cls_score  ( Tensor ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W). •  bbox_pred  ( Tensor ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W). •  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  b box weights  ( Tensor ) – BBox regression loss weights of each anchor with shape (N, num total anchors, 4). •  num total samples  ( int ) – If sampling, num total samples equal to the number of total anchors; Otherwise, it is the number of positive anchors. ", "page_idx": 315, "bbox": [154, 71.45246887207031, 521, 293.9775085449219], "page_size": [612.0, 792.0]}
{"layout": 3150, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 315, "bbox": [137, 297.97283935546875, 308.6417541503906, 312.5082702636719], "page_size": [612.0, 792.0]}
{"layout": 3151, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 315, "bbox": [137, 315.9058532714844, 256, 330.4412841796875], "page_size": [612.0, 792.0]}
{"layout": 3152, "type": "text", "text": "class  mmdet.models.dense heads. Auto Assign Head ( \\*args ,  force_topk  $=$  False ,  topk  $z{=}9$  , pos loss weigh  $t{=}0.25$  ,  ne g loss weight=0.75 , center loss weigh  $t{=}0.75$  ,  \\*\\*kwargs ) ", "page_idx": 315, "bbox": [72.00001525878906, 334.3160705566406, 500.0936584472656, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 3153, "type": "text", "text": "Auto Assign Head head used in AutoAssign. ", "page_idx": 315, "bbox": [96, 370.33148193359375, 269.8078918457031, 383.6415100097656], "page_size": [612.0, 792.0]}
{"layout": 3154, "type": "text", "text": "More details can be found in the  paper  . ", "page_idx": 315, "bbox": [96, 388.2644958496094, 256, 401.57452392578125], "page_size": [612.0, 792.0]}
{"layout": 3155, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 315, "bbox": [117, 408, 169, 419], "page_size": [612.0, 792.0]}
{"layout": 3156, "type": "text", "text": "•  force_topk  ( bool ) – Used in center prior initialization to handle extremely small gt. De- fault is False. •  topk  ( int ) – The number of points used to calculate the center prior when no point falls in gt_bbox. Only work when force_topk if True. Defaults to 9. •  pos loss weight  ( float ) – The loss weight of positive loss and with default value 0.25. •  ne g loss weight  ( float ) – The loss weight of negative loss and with default value 0.75. •  center loss weight  ( float ) – The loss weight of center prior loss and with default value 0.75. ", "page_idx": 315, "bbox": [145, 424.1295166015625, 521, 545.0365600585938], "page_size": [612.0, 792.0]}
{"layout": 3157, "type": "text", "text": "forward single ( x ,  scale ,  stride ) Forward features of a single scale level. ", "page_idx": 315, "bbox": [96, 549.5090942382812, 276.1544189453125, 574.924560546875], "page_size": [612.0, 792.0]}
{"layout": 3158, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 315, "bbox": [136, 581, 188, 592], "page_size": [612.0, 792.0]}
{"layout": 3159, "type": "text", "text": "•  x  ( Tensor ) – FPN feature maps of the specified stride. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. •  stride  ( int ) – The corresponding stride for feature maps, only used to normalize the bbox prediction when self.norm on b box is True. ", "page_idx": 315, "bbox": [154, 597.4794921875, 521, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 3160, "type": "text", "text": "Returns  scores for each class, bbox predictions and centerness predictions of input feature maps. ", "page_idx": 315, "bbox": [137, 662.6058959960938, 521, 677.1412963867188], "page_size": [612.0, 792.0]}
{"layout": 3161, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 315, "bbox": [137, 683, 214, 694], "page_size": [612.0, 792.0]}
{"layout": 3162, "type": "text", "text": "get ne g loss single ( cls_score ,  objectness ,  gt_labels ,  ious ,  inside gt b box mask ) Calculate the negative loss of all points in feature map. ", "page_idx": 316, "bbox": [96, 71.30303192138672, 445.1253356933594, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3163, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 316, "bbox": [136, 102, 187, 114], "page_size": [612.0, 792.0]}
{"layout": 3164, "type": "text", "text": "•  cls_score  ( Tensor ) – All category scores for each point on the feature map. The shape is (num_points, num_class). •  objectness  ( Tensor ) – Foreground probability of all points and is shape of (num_points, 1). •  gt_labels  ( Tensor ) – The zeros based label of all gt with shape of (num_gt). •  ious  ( Tensor ) – Float tensor with shape of (num_points, num_gt). Each value represent the iou of pred_bbox and gt_bboxes. •  inside gt b box mask  ( Tensor ) – Tensor of bool type, with shape of (num_points, num_gt), each value is used to mark whether this point falls within a certain gt. ", "page_idx": 316, "bbox": [155, 119.27247619628906, 521, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 3165, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 316, "bbox": [137, 258, 173, 269], "page_size": [612.0, 792.0]}
{"layout": 3166, "type": "text", "text": "• neg_loss (Tensor): The negative loss of all points in the feature map. ", "page_idx": 316, "bbox": [155, 274.69049072265625, 437.3689880371094, 288.0005187988281], "page_size": [612.0, 792.0]}
{"layout": 3167, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 316, "bbox": [137, 291.9948425292969, 246, 306.5302734375], "page_size": [612.0, 792.0]}
{"layout": 3168, "type": "text", "text": "get pos loss single ( cls_score ,  objectness ,  reg_loss ,  gt_labels ,  center prior weights ) Calculate the positive loss of all points in gt_bboxes. ", "page_idx": 316, "bbox": [96, 310.40606689453125, 460.896240234375, 335.8205261230469], "page_size": [612.0, 792.0]}
{"layout": 3169, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 316, "bbox": [136, 342, 187, 353], "page_size": [612.0, 792.0]}
{"layout": 3170, "type": "text", "text": "•  cls_score  ( Tensor ) – All category scores for each point on the feature map. The shape is (num_points, num_class). •  objectness  ( Tensor ) – Foreground probability of all points, has shape (num_points, 1). •  reg_loss  ( Tensor ) – The regression loss of each gt_bbox and each prediction box, has shape of (num_points, num_gt). •  gt_labels  ( Tensor ) – The zeros based gt_labels of all gt with shape of (num_gt,). •  center prior weights  ( Tensor ) – Float tensor with shape of (num_points, num_gt). Each value represents the center weighting coefficient. ", "page_idx": 316, "bbox": [155, 358.37652587890625, 521, 479.2825622558594], "page_size": [612.0, 792.0]}
{"layout": 3171, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 316, "bbox": [137, 485, 172, 497], "page_size": [612.0, 792.0]}
{"layout": 3172, "type": "text", "text": "• pos_loss (Tensor): The positive loss of all points in the gt_bboxes. ", "page_idx": 316, "bbox": [155, 501.8385925292969, 428.17333984375, 515.1486206054688], "page_size": [612.0, 792.0]}
{"layout": 3173, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 316, "bbox": [137, 519.1429443359375, 246, 533.6783447265625], "page_size": [612.0, 792.0]}
{"layout": 3174, "type": "text", "text": "get targets ( points ,  gt b boxes list ) Compute regression targets and each point inside or outside gt_bbox in multiple images. ", "page_idx": 316, "bbox": [96, 537.5541381835938, 470.0451965332031, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 3175, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 316, "bbox": [136, 569, 187, 581], "page_size": [612.0, 792.0]}
{"layout": 3176, "type": "text", "text": "•  points  ( list[Tensor] ) – Points of all fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). ", "page_idx": 316, "bbox": [155, 585.5245361328125, 521, 628.7225952148438], "page_size": [612.0, 792.0]}
{"layout": 3177, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 316, "bbox": [137, 634, 172, 646], "page_size": [612.0, 792.0]}
{"layout": 3178, "type": "text", "text": "• inside gt b box mask list (list[Tensor]): Each Tensor is with bool type and shape of (num_points, num_gt), each value is used to mark whether this point falls within a cer- tain gt. • con cat lv l b box targets (list[Tensor]): BBox targets of each level. Each tensor has shape (num_points, num_gt, 4). ", "page_idx": 316, "bbox": [155, 651.2775268554688, 521, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 3179, "type": "text", "text": "Return type tuple(list[Tensor])", "page_idx": 317, "bbox": [137, 70.8248291015625, 265.37451171875, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 3180, "type": "text", "text": "in it weights()", "text_level": 1, "page_idx": 317, "bbox": [96, 91, 171, 102], "page_size": [612.0, 792.0]}
{"layout": 3181, "type": "text", "text": "Initialize weights of the head. In particular, we have special initialization for classified conv’s and regression conv’s bias ", "page_idx": 317, "bbox": [118, 101.34046936035156, 476.45147705078125, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 3182, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  object ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute loss of the head. ", "page_idx": 317, "bbox": [96, 137.05601501464844, 513.0263061523438, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 3183, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 317, "bbox": [136, 168, 187, 179], "page_size": [612.0, 792.0]}
{"layout": 3184, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  object ness es  ( list[Tensor] ) – objectness for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\,1$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components. Return type  dict[str, Tensor] ", "page_idx": 317, "bbox": [137, 185.0264434814453, 521, 414.1272888183594], "page_size": [612.0, 792.0]}
{"layout": 3185, "type": "text", "text": "class  mmdet.models.dense heads. Cascade RP N Head ( num_stages ,  stages ,  train_cfg ,  test_cfg ,  init_cfg  $=$  None ) The Cascade RP N Head will predict more accurate region proposals, which is required for two-stage detectors (such as Fast/Faster R-CNN). CascadeRPN consists of a sequence of RPNStage to progressively improve the accuracy of the detected proposals. ", "page_idx": 317, "bbox": [71.99990844726562, 418.0030822753906, 540, 467.3275146484375], "page_size": [612.0, 792.0]}
{"layout": 3186, "type": "text", "text": "More details can be found in  https://arxiv.org/abs/1909.06720 . ", "page_idx": 317, "bbox": [96, 471.95050048828125, 383.26251220703125, 485.2605285644531], "page_size": [612.0, 792.0]}
{"layout": 3187, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 317, "bbox": [117, 492, 169, 503], "page_size": [612.0, 792.0]}
{"layout": 3188, "type": "text", "text": "•  num_stages  ( int ) – number of CascadeRPN stages. •  stages  ( list[dict] ) – list of configs to build the stages. •  train_cfg  ( list[dict] ) – list of configs at training time each stage. •  test_cfg  ( dict ) – config at testing time. ", "page_idx": 317, "bbox": [145, 507.8155212402344, 437.05462646484375, 574.924560546875], "page_size": [612.0, 792.0]}
{"layout": 3189, "type": "text", "text": "aug test rp n ( x ,  img_metas ) Augmented forward test function. ", "page_idx": 317, "bbox": [96, 579.3970336914062, 252.95127868652344, 604.8125610351562], "page_size": [612.0, 792.0]}
{"layout": 3190, "type": "text", "text": "forward train ( x ,  img_metas ,  gt_bboxes ,  gt_labels  $\\mathbf{\\hat{\\rho}}$  None ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  proposal cf g=None ) Forward train function. ", "page_idx": 317, "bbox": [96, 609.2850952148438, 530.0321044921875, 634.6995239257812], "page_size": [612.0, 792.0]}
{"layout": 3191, "type": "text", "text": "get_bboxes () get_bboxes() is implemented in Stage Cascade RP N Head. ", "page_idx": 317, "bbox": [96, 641.0460205078125, 344.09912109375, 664.5875244140625], "page_size": [612.0, 792.0]}
{"layout": 3192, "type": "text", "text": "loss()loss() is implemented in Stage Cascade RP N Head. ", "page_idx": 317, "bbox": [96, 670.93408203125, 315.117919921875, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 3193, "type": "text", "text": "simple test rp n ( x ,  img_metas ) Simple forward test function. ", "page_idx": 318, "bbox": [96, 71.30303192138672, 238.34136962890625, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3194, "type": "text", "text": "class  mmdet.models.dense heads. Center Net Head ( in_channel ,  feat channel ,  num classes , loss center heat map={'loss weight': 1.0, 'type': 'Gaussian Focal Loss'} ,  loss_wh={'loss weight': 0.1, 'type': 'L1Loss'} ,  loss_offse  $\\mathbf{\\dot{\\rho}}$  {'loss weight': 1.0, 'type': 'L1Loss'} ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg  $\\mathbf{\\beta}=$  None , init_cfg  $=$  None ) ", "page_idx": 318, "bbox": [72.0, 101.19103240966797, 528.5673217773438, 174.3159942626953], "page_size": [612.0, 792.0]}
{"layout": 3195, "type": "text", "text": "Objects as Points Head. CenterHead use center point to indicate object’s position. Paper link < https://arxiv.org/ abs/1904.07850 > ", "page_idx": 318, "bbox": [96, 173.07154846191406, 539.634521484375, 198.3365936279297], "page_size": [612.0, 792.0]}
{"layout": 3196, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 318, "bbox": [117, 204, 169, 216], "page_size": [612.0, 792.0]}
{"layout": 3197, "type": "text", "text": "•  in_channel  ( int ) – Number of channel in the input feature map. •  feat channel  ( int ) – Number of channel in the intermediate feature map. •  num classes  ( int ) – Number of categories excluding the background category. •  loss center heat map  ( dict | None ) – Config of center heatmap loss. Default: Gaus- s ian Focal Loss. •  loss_wh  ( dict | None ) – Config of wh loss. Default: L1Loss. •  loss offset  ( dict | None ) – Config of offset loss. Default: L1Loss. •  train_cfg  ( dict | None ) – Training config. Useless in CenterNet, but we keep this vari- able for Single Stage Detector. Default: None. •  test_cfg  ( dict | None ) – Testing config of CenterNet. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 318, "bbox": [145, 220.8915557861328, 518, 401.5746154785156], "page_size": [612.0, 792.0]}
{"layout": 3198, "type": "text", "text": "decode heat map ( center heat map p red ,  wh_pred ,  offset p red ,  img_shape ,  $k{=}I O O$  ,  kerne  $l{=}3$  ) Transform outputs into detections raw bbox prediction. ", "page_idx": 318, "bbox": [96, 406.0471496582031, 478, 431.4626159667969], "page_size": [612.0, 792.0]}
{"layout": 3199, "type": "text", "text": "", "text_level": 1, "page_idx": 318, "bbox": [136, 438, 187, 444.75], "page_size": [612.0, 792.0]}
{"layout": 3200, "type": "text", "text": "•  center heat map p red  ( Tensor ) – center predict heatmap, shape (B, num classes, H, W). •  wh_pred  ( Tensor ) – wh predict, shape (B, 2, H, W). •  offset p red  ( Tensor ) – offset predict, shape (B, 2, H, W). •  img_shape  ( list[int] ) – image shape in [h, w] format. •  k  ( int ) – Get top k center keypoints from heatmap. Default 100. •  kernel  ( int ) – Max pooling kernel for extract local maximum pixels. Default 3. ", "page_idx": 318, "bbox": [154, 454.0176086425781, 521.3659057617188, 568.9466552734375], "page_size": [612.0, 792.0]}
{"layout": 3201, "type": "text", "text": "Returns Decoded output of Center Net Head, containing the following Tensors: • batch b boxes (Tensor): Coords of each box with shape (B, k, 5) • batch top k labels (Tensor): Categories of each box with shape   $(\\mathbf{B},\\mathbf{k})$  Return type  tuple[torch.Tensor] ", "page_idx": 318, "bbox": [137, 572.9420166015625, 451, 689.0963745117188], "page_size": [612.0, 792.0]}
{"layout": 3202, "type": "text", "text": "forward ( feats ) Forward features. Notice CenterNet head does not use FPN. ", "page_idx": 318, "bbox": [96, 692.97119140625, 357.5583801269531, 718.3866577148438], "page_size": [612.0, 792.0]}
{"layout": 3203, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 319, "bbox": [137, 70.8248291015625, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3204, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 319, "bbox": [137, 103, 172, 114], "page_size": [612.0, 792.0]}
{"layout": 3205, "type": "text", "text": "center predict heatmaps for  all levels, the channels number is num classes. wh_preds (List[Tensor]): wh predicts for all levels, the channels  number is 2. offset p reds (List[Tensor]): offset predicts for all levels, the  channels number is 2. ", "page_idx": 319, "bbox": [154, 118.64483642578125, 495.9594421386719, 169.04623413085938], "page_size": [612.0, 792.0]}
{"layout": 3206, "type": "text", "text": "", "page_idx": 319, "bbox": [136, 179.25, 342, 186], "page_size": [612.0, 792.0]}
{"layout": 3207, "type": "text", "text": "forward single ( feat ) Forward feature of a single level. ", "page_idx": 319, "bbox": [96, 190.85398864746094, 249.8721160888672, 216.2694549560547], "page_size": [612.0, 792.0]}
{"layout": 3208, "type": "text", "text": "Parameters  feat  ( Tensor ) – Feature of a single level. ", "page_idx": 319, "bbox": [137, 220.2637939453125, 360.5874938964844, 234.79922485351562], "page_size": [612.0, 792.0]}
{"layout": 3209, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 319, "bbox": [136, 240, 173, 252], "page_size": [612.0, 792.0]}
{"layout": 3210, "type": "text", "text": "center predict heatmaps, the  channels number is num classes. wh_pred (Tensor): wh predicts, the channels number is 2. offset p red (Tensor): offset pre- dicts, the channels number is 2. ", "page_idx": 319, "bbox": [154, 256.1297607421875, 521, 299.9554443359375], "page_size": [612.0, 792.0]}
{"layout": 3211, "type": "text", "text": "", "page_idx": 319, "bbox": [137, 311.25, 316, 318], "page_size": [612.0, 792.0]}
{"layout": 3212, "type": "text", "text": "get_bboxes ( center heat map p reds ,  wh_preds ,  offset p reds ,  img_metas ,  rescale  $=$  True ,  with_nms=False ) Transform network output for a batch into bbox predictions. ", "page_idx": 319, "bbox": [96, 322.3609924316406, 524.413330078125, 347.77545166015625], "page_size": [612.0, 792.0]}
{"layout": 3213, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 319, "bbox": [136, 353, 187, 365], "page_size": [612.0, 792.0]}
{"layout": 3214, "type": "text", "text": "•  center heat map p reds  ( list[Tensor] ) – Center predict heatmaps for all levels with shape (B, num classes, H, W). •  wh_preds  ( list[Tensor] ) – WH predicts for all levels with shape (B, 2, H, W). •  offset p reds  ( list[Tensor] ) – Offset predicts for all levels with shape (B, 2, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: True. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: False. ", "page_idx": 319, "bbox": [154, 370.3314514160156, 521, 497.21551513671875], "page_size": [612.0, 792.0]}
{"layout": 3215, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 319, "bbox": [137, 503, 173, 514], "page_size": [612.0, 792.0]}
{"layout": 3216, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is an (n, 5) tensor, where 5 represent (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1. The shape of the second tensor in the tuple is (n,), and each element represents the class label of the corresponding box. ", "page_idx": 319, "bbox": [154, 519.142822265625, 521, 556.9915161132812], "page_size": [612.0, 792.0]}
{"layout": 3217, "type": "text", "text": "Return type  list[tuple[Tensor, Tensor]] ", "page_idx": 319, "bbox": [137, 560.98681640625, 296.7765197753906, 575.522216796875], "page_size": [612.0, 792.0]}
{"layout": 3218, "type": "text", "text": "get targets ( gt_bboxes ,  gt_labels ,  feat_shape ,  img_shape ) Compute regression and classification targets in multiple images. ", "page_idx": 319, "bbox": [96, 579.3970336914062, 377.8424987792969, 604.8115234375], "page_size": [612.0, 792.0]}
{"layout": 3219, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 319, "bbox": [136, 611, 188, 622], "page_size": [612.0, 792.0]}
{"layout": 3220, "type": "text", "text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box. •  feat_shape  ( list[int] ) – feature map shape with value [B, _, H, W] •  img_shape  ( list[int] ) – image shape in [h, w] format. ", "page_idx": 319, "bbox": [154, 627.3674926757812, 521, 706.4305419921875], "page_size": [612.0, 792.0]}
{"layout": 3221, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 320, "bbox": [137, 73, 173, 85], "page_size": [612.0, 792.0]}
{"layout": 3222, "type": "text", "text": "The float value is mean avg_factor, the dict has  components below: - cen- ter heat map target (Tensor): targets of center heatmap, shape (B, num classes, H, W). - wh_target (Tensor): targets of wh predict, shape (B, 2, H, W). - offset target (Tensor): targets of offset predict, shape (B, 2, H, W). - wh offset target weight (Tensor): weights of wh and offset predict, shape (B, 2, H, W). ", "page_idx": 320, "bbox": [154, 88.7568359375, 521, 150.5155487060547], "page_size": [612.0, 792.0]}
{"layout": 3223, "type": "text", "text": "Return type  tuple[dict,float] ", "page_idx": 320, "bbox": [137, 154.5108642578125, 256, 169.04629516601562], "page_size": [612.0, 792.0]}
{"layout": 3224, "type": "text", "text": "in it weights()Initialize weights of the head. ", "page_idx": 320, "bbox": [96, 174.79501342773438, 236.67164611816406, 198.33653259277344], "page_size": [612.0, 792.0]}
{"layout": 3225, "type": "text", "text": "loss ( center heat map p reds ,  wh_preds ,  offset p reds ,  gt_bboxes ,  gt_labels ,  img_metas , gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None ) Compute losses of the head. ", "page_idx": 320, "bbox": [96, 202.81004333496094, 447.441650390625, 240.17955017089844], "page_size": [612.0, 792.0]}
{"layout": 3226, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 320, "bbox": [136, 246, 187, 257], "page_size": [612.0, 792.0]}
{"layout": 3227, "type": "text", "text": "•  center heat map p reds  ( list[Tensor] ) – center predict heatmaps for all levels with shape (B, num classes, H, W). •  wh_preds  ( list[Tensor] ) – wh predicts for all levels with shape (B, 2, H, W). •  offset p reds  ( list[Tensor] ) – offset predicts for all levels with shape (B, 2, H, W). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None ", "page_idx": 320, "bbox": [154, 262.7345275878906, 521, 431.46258544921875], "page_size": [612.0, 792.0]}
{"layout": 3228, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 320, "bbox": [137, 438, 172, 449], "page_size": [612.0, 792.0]}
{"layout": 3229, "type": "text", "text": "which has components below: ", "text_level": 1, "page_idx": 320, "bbox": [153, 456, 280, 467], "page_size": [612.0, 792.0]}
{"layout": 3230, "type": "text", "text": "• loss center heat map (Tensor): loss of center heatmap. • loss_wh (Tensor): loss of hw heatmap • loss offset (Tensor): loss of offset heatmap. ", "page_idx": 320, "bbox": [164, 471.9505920410156, 391.7596435546875, 521.1256713867188], "page_size": [612.0, 792.0]}
{"layout": 3231, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 320, "bbox": [137, 525.1209716796875, 256, 539.6563720703125], "page_size": [612.0, 792.0]}
{"layout": 3232, "type": "text", "text": "class  mmdet.models.dense heads. Centripetal Head ( \\*args ,  centripetal shift channels=2 , ", "page_idx": 320, "bbox": [71.99992370605469, 543.5321655273438, 464.9755859375, 556.9916381835938], "page_size": [612.0, 792.0]}
{"layout": 3233, "type": "text", "text": "guiding shift channels  $_{:=2}$  , feat adaption con v kern e  $l{=}3$  , loss guiding shift={'beta': 1.0, 'loss weight': 0.05,'type': 'Smooth L 1 Loss'} ,  loss centripetal shift={'beta': 1.0, 'loss weight': 1, 'type': 'Smooth L 1 Loss'} , init_cfg  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) ", "page_idx": 320, "bbox": [317, 555.4871826171875, 540, 628.72265625], "page_size": [612.0, 792.0]}
{"layout": 3234, "type": "text", "text": "Head of Centripetal Net: Pursuing High-quality Keypoint Pairs for Object Detection. ", "page_idx": 320, "bbox": [96, 627.3676147460938, 432.5057678222656, 640.6776733398438], "page_size": [612.0, 792.0]}
{"layout": 3235, "type": "text", "text": "Centripetal Head inherits from  CornerHead . It removes the embedding branch and adds guiding shift and cen- tripetal shift branches. More details can be found in the  paper  . ", "page_idx": 320, "bbox": [96, 645.3005981445312, 540, 670.5656127929688], "page_size": [612.0, 792.0]}
{"layout": 3236, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 320, "bbox": [117, 676, 169, 689], "page_size": [612.0, 792.0]}
{"layout": 3237, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. ", "page_idx": 320, "bbox": [145.92274475097656, 693.12158203125, 477.1290588378906, 706.431640625], "page_size": [612.0, 792.0]}
{"layout": 3238, "type": "text", "text": "•  in channels  ( int ) – Number of channels in the input feature map. •  num feat levels  ( int ) – Levels of feature from the previous module. 2 for Hourglass Net- 104 and 1 for Hourglass Net-52. Hourglass Net-104 outputs the final feature and intermediate supervision feature and Hourglass Net-52 only outputs the final feature. Default: 2. •  corner em b channels  ( int ) – Channel of embedding vector. Default: 1. •  train_cfg  ( dict | None ) – Training config. Useless in CornerHead, but we keep this variable for Single Stage Detector. Default: None. •  test_cfg  ( dict | None ) – Testing config of CornerHead. Default: None. •  loss heat map  ( dict | None ) – Config of corner heatmap loss. Default: Gaussian Focal- Loss. •  loss embedding  ( dict | None ) – Config of corner embedding loss. Default: Associa- ti ve Embedding Loss. •  loss offset  ( dict | None ) – Config of corner offset loss. Default: Smooth L 1 Loss. •  loss guiding shift  ( dict ) – Config of guiding shift loss. Default: Smooth L 1 Loss. •  loss centripetal shift  ( dict ) – Config of centripetal shift loss. Default: Smooth L 1 Loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 321, "bbox": [145, 71.45246887207031, 518, 335.8205261230469], "page_size": [612.0, 792.0]}
{"layout": 3239, "type": "text", "text": "forward single  $(x,l\\nu l\\_i n d)$  Forward feature of a single level. ", "page_idx": 321, "bbox": [96, 342, 249, 365.7085266113281], "page_size": [612.0, 792.0]}
{"layout": 3240, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 321, "bbox": [136, 371, 188, 383], "page_size": [612.0, 792.0]}
{"layout": 3241, "type": "text", "text": "•  x  ( Tensor ) – Feature of a single level. •  lvl_ind  ( int ) – Level index of current feature. ", "page_idx": 321, "bbox": [154, 388.2645263671875, 356, 419.5065612792969], "page_size": [612.0, 792.0]}
{"layout": 3242, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 321, "bbox": [136, 426, 173, 437], "page_size": [612.0, 792.0]}
{"layout": 3243, "type": "text", "text": "sors: • tl_heat (Tensor): Predicted top-left corner heatmap. • br_heat (Tensor): Predicted bottom-right corner heatmap. • tl_off (Tensor): Predicted top-left offset heatmap. • br_off (Tensor): Predicted bottom-right offset heatmap. • tl guiding shift (Tensor): Predicted top-left guiding shift heatmap. • br guiding shift (Tensor): Predicted bottom-right guiding shift heatmap. • tl centripetal shift (Tensor): Predicted top-left centripetal shift heatmap. • br centripetal shift (Tensor): Predicted bottom-right centripetal shift heatmap. ", "page_idx": 321, "bbox": [154, 454.0175476074219, 478.9826354980469, 610.7896118164062], "page_size": [612.0, 792.0]}
{"layout": 3244, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 321, "bbox": [137.4549102783203, 614.784912109375, 249, 629.3203125], "page_size": [612.0, 792.0]}
{"layout": 3245, "type": "text", "text": "get_bboxes ( tl_heats ,  br_heats ,  tl_offs ,  br_offs ,  tl guiding shifts ,  br guiding shifts ,  tl centripetal shifts , br centripetal shifts ,  img_metas ,  rescale  $\\mathbf{\\varepsilon}=$  False ,  with_nms  $\\mathbf{\\hat{=}}$  True ) Transform network output for a batch into bbox predictions. ", "page_idx": 321, "bbox": [96, 633.1961059570312, 523, 670.5656127929688], "page_size": [612.0, 792.0]}
{"layout": 3246, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 321, "bbox": [136, 676, 188, 688], "page_size": [612.0, 792.0]}
{"layout": 3247, "type": "text", "text": "•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). ", "page_idx": 321, "bbox": [154, 693.1205444335938, 523, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 3248, "type": "text", "text": "•  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  tl guiding shifts  ( list[Tensor] ) – Top-left guiding shifts for each level with shape (N, guiding shift channels, H, W). Useless in this function, we keep this arg because it’s the raw output from Centripetal Head. •  br guiding shifts  ( list[Tensor] ) – Bottom-right guiding shifts for each level with shape (N, guiding shift channels, H, W). Useless in this function, we keep this arg because it’s the raw output from Centripetal Head. •  tl centripetal shifts  ( list[Tensor] ) – Top-left centripetal shifts for each level with shape (N, centripetal shift channels, H, W). •  br centripetal shifts  ( list[Tensor] ) – Bottom-right centripetal shifts for each level with shape (N, centripetal shift channels, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. ", "page_idx": 322, "bbox": [155, 71.45246887207031, 521, 365.7085876464844], "page_size": [612.0, 792.0]}
{"layout": 3249, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 322, "bbox": [96, 372.0550842285156, 206, 395.5965881347656], "page_size": [612.0, 792.0]}
{"layout": 3250, "type": "text", "text": "loss ( tl_heats ,  br_heats ,  tl_offs ,  br_offs ,  tl guiding shifts ,  br guiding shifts ,  tl centripetal shifts , br centripetal shifts ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{=}$  None ) Compute losses of the head. ", "page_idx": 322, "bbox": [96, 400.07012939453125, 492.0045166015625, 437.4395751953125], "page_size": [612.0, 792.0]}
{"layout": 3251, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 322, "bbox": [136, 443, 187, 455], "page_size": [612.0, 792.0]}
{"layout": 3252, "type": "text", "text": "•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  tl guiding shifts  ( list[Tensor] ) – Top-left guiding shifts for each level with shape (N, guiding shift channels, H, W). •  br guiding shifts  ( list[Tensor] ) – Bottom-right guiding shifts for each level with shape (N, guiding shift channels, H, W). •  tl centripetal shifts  ( list[Tensor] ) – Top-left centripetal shifts for each level with shape (N, centripetal shift channels, H, W). •  br centripetal shifts  ( list[Tensor] ) – Bottom-right centripetal shifts for each level with shape (N, centripetal shift channels, H, W). ", "page_idx": 322, "bbox": [155, 459.9955749511719, 521, 694.4755859375], "page_size": [612.0, 792.0]}
{"layout": 3253, "type": "text", "text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [left, top, right, bottom] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – Specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 323, "bbox": [154, 71.45246887207031, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 3254, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 323, "bbox": [137, 180, 172, 192], "page_size": [612.0, 792.0]}
{"layout": 3255, "type": "text", "text": "A dictionary of loss components. Containing the following losses: • det_loss (list[Tensor]): Corner keypoint losses of all feature levels. • off_loss (list[Tensor]): Corner offset losses of all feature levels. • guiding loss (list[Tensor]): Guiding shift losses of all feature levels. • centripetal loss (list[Tensor]): Centripetal shift losses of all feature levels. ", "page_idx": 323, "bbox": [154, 196.98146057128906, 459, 282.0224609375], "page_size": [612.0, 792.0]}
{"layout": 3256, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 323, "bbox": [137.45497131347656, 286.0177917480469, 256.1192626953125, 300.55322265625], "page_size": [612.0, 792.0]}
{"layout": 3257, "type": "text", "text": "loss single ( tl_hmp ,  br_hmp ,  tl_off ,  br_off ,  tl guiding shift ,  br guiding shift ,  tl centripetal shift , br centripetal shift ,  targets ) ", "page_idx": 323, "bbox": [96.90696716308594, 304.4280090332031, 501.48956298828125, 329.8434753417969], "page_size": [612.0, 792.0]}
{"layout": 3258, "type": "text", "text": "Compute losses for single level. ", "page_idx": 323, "bbox": [118.82490539550781, 328.4884338378906, 247, 341.7984619140625], "page_size": [612.0, 792.0]}
{"layout": 3259, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 323, "bbox": [136, 347, 187, 359], "page_size": [612.0, 792.0]}
{"layout": 3260, "type": "text", "text": "•  tl_hmp  ( Tensor ) – Top-left corner heatmap for current level with shape (N, num classes, H, W). •  br_hmp  ( Tensor ) – Bottom-right corner heatmap for current level with shape (N, num classes, H, W). •  tl_off  ( Tensor ) – Top-left corner offset for current level with shape (N, cor- ner offset channels, H, W). •  br_off  ( Tensor ) – Bottom-right corner offset for current level with shape (N, cor- ner offset channels, H, W). •  tl guiding shift  ( Tensor ) – Top-left guiding shift for current level with shape (N, guiding shift channels, H, W). •  br guiding shift  ( Tensor ) – Bottom-right guiding shift for current level with shape (N, guiding shift channels, H, W). •  tl centripetal shift  ( Tensor ) – Top-left centripetal shift for current level with shape (N, centripetal shift channels, H, W). •  br centripetal shift  ( Tensor ) – Bottom-right centripetal shift for current level with shape (N, centripetal shift channels, H, W). •  targets  ( dict ) – Corner target generated by  get targets . ", "page_idx": 323, "bbox": [154, 364.35345458984375, 521, 616.7675170898438], "page_size": [612.0, 792.0]}
{"layout": 3261, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 323, "bbox": [136, 623, 172, 634], "page_size": [612.0, 792.0]}
{"layout": 3262, "type": "text", "text": "Losses of the head’s different branches containing the following losses: • det_loss (Tensor): Corner keypoint loss. • off_loss (Tensor): Corner offset loss. • guiding loss (Tensor): Guiding shift loss. ", "page_idx": 323, "bbox": [154, 639.3224487304688, 437, 706.4304809570312], "page_size": [612.0, 792.0]}
{"layout": 3263, "type": "text", "text": "• centripetal loss (Tensor): Centripetal shift loss. ", "page_idx": 324, "bbox": [154, 71.45246887207031, 354.28094482421875, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3264, "type": "text", "text": "Return type  tuple[torch.Tensor] ", "page_idx": 324, "bbox": [137.45498657226562, 88.7568359375, 269.5189208984375, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 3265, "type": "text", "text": "class  mmdet.models.dense heads. CornerHead ( num classes ,  in channels ,  num feat levels=2 , corner em b channel  $\\mathord{\\breve{=}}I$  ,  train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None , loss heat map $\\leftrightharpoons$ {'alpha': 2.0, 'gamma': 4.0, 'loss_weight': 1,'type': 'Gaussian Focal Loss'} ,  loss embedding={'pull weight': 0.25, 'push_weight': 0.25, 'type': 'Associative Embedding Loss'} ,  loss offset={'beta': 1.0, 'loss weight': 1, 'type': 'Smooth L 1 Loss'} ,  init_cfg=None ) ", "page_idx": 324, "bbox": [71.99998474121094, 107.16802215576172, 540.00048828125, 192.3585662841797], "page_size": [612.0, 792.0]}
{"layout": 3266, "type": "text", "text": "Head of CornerNet: Detecting Objects as Paired Keypoints. ", "page_idx": 324, "bbox": [96, 191.00355529785156, 335.03271484375, 204.31358337402344], "page_size": [612.0, 792.0]}
{"layout": 3267, "type": "text", "text": "Code is modified from the  official github repo  . ", "page_idx": 324, "bbox": [96, 208.93653869628906, 284.7414855957031, 222.24656677246094], "page_size": [612.0, 792.0]}
{"layout": 3268, "type": "text", "text": "More details can be found in the  paper  . ", "page_idx": 324, "bbox": [96, 226.86952209472656, 255.04298400878906, 240.17955017089844], "page_size": [612.0, 792.0]}
{"layout": 3269, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 324, "bbox": [117, 246, 169, 258], "page_size": [612.0, 792.0]}
{"layout": 3270, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  num feat levels  ( int ) – Levels of feature from the previous module. 2 for Hourglass Net- 104 and 1 for Hourglass Net-52. Because Hourglass Net-104 outputs the final feature and intermediate supervision feature and Hourglass Net-52 only outputs the final feature. Default: 2. •  corner em b channels  ( int ) – Channel of embedding vector. Default: 1. •  train_cfg  ( dict | None ) – Training config. Useless in CornerHead, but we keep this variable for Single Stage Detector. Default: None. •  test_cfg  ( dict | None ) – Testing config of CornerHead. Default: None. •  loss heat map  ( dict | None ) – Config of corner heatmap loss. Default: Gaussian Focal- Loss. •  loss embedding  ( dict | None ) – Config of corner embedding loss. Default: Associa- ti ve Embedding Loss. •  loss offset  ( dict | None ) – Config of corner offset loss. Default: Smooth L 1 Loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 324, "bbox": [145, 262.7345275878906, 518, 509.17059326171875], "page_size": [612.0, 792.0]}
{"layout": 3271, "type": "text", "text": "decode heat map ( tl_heat ,  br_heat ,  tl_off ,  br_off ,  tl_emb  $=$  None ,  br_emb  $\\leftrightharpoons$  None ,  tl centripetal shift  $\\mathbf{\\dot{\\rho}}=$  None , br centripetal sh if  $\\mathbf{\\dot{=}}$  None ,  img_meta  $\\mathbf{\\beta}=$  None ,  $k{=}I O O$  ,  kernel  $\\scriptstyle{I=3}$  ,  distance threshold  $\\mathit{I}\\!\\!=\\!\\!0.5$  , num_dets=1000 ) ", "page_idx": 324, "bbox": [96, 513.6441040039062, 530, 550.9039916992188], "page_size": [612.0, 792.0]}
{"layout": 3272, "type": "text", "text": "Transform outputs for a single batch item into raw bbox predictions. ", "page_idx": 324, "bbox": [118.82463836669922, 549.6585693359375, 389.48846435546875, 562.9686279296875], "page_size": [612.0, 792.0]}
{"layout": 3273, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 324, "bbox": [136, 569, 187, 580], "page_size": [612.0, 792.0]}
{"layout": 3274, "type": "text", "text": "•  tl_heat  ( Tensor ) – Top-left corner heatmap for current level with shape (N, num classes, H, W). •  br_heat  ( Tensor ) – Bottom-right corner heatmap for current level with shape (N, num classes, H, W). •  tl_off  ( Tensor ) – Top-left corner offset for current level with shape (N, cor- ner offset channels, H, W). •  br_off  ( Tensor ) – Bottom-right corner offset for current level with shape (N, cor- ner offset channels, H, W). ", "page_idx": 324, "bbox": [154, 585.5245361328125, 521, 700.45361328125], "page_size": [612.0, 792.0]}
{"layout": 3275, "type": "text", "text": "•  tl_emb  ( Tensor | None ) – Top-left corner embedding for current level with shape (N, corner em b channels, H, W). •  br_emb  ( Tensor | None ) – Bottom-right corner embedding for current level with shape (N, corner em b channels, H, W). •  tl centripetal shift  ( Tensor | None ) – Top-left centripetal shift for current level with shape (N, 2, H, W). •  br centripetal shift  ( Tensor | None ) – Bottom-right centripetal shift for current level with shape (N, 2, H, W). •  img_meta  ( dict ) – Meta information of current image, e.g., image size, scaling factor, etc. •  k  ( int ) – Get top k corner keypoints from heatmap. •  kernel  ( int ) – Max pooling kernel for extract local maximum pixels. •  distance threshold  ( float ) – Distance threshold. Top-left and bottom-right corner keypoints with feature distance less than the threshold will be regarded as keypoints from same object. •  num_dets  ( int ) – Num of raw boxes before doing nms. ", "page_idx": 325, "bbox": [154, 71.45246887207031, 521, 311.9105224609375], "page_size": [612.0, 792.0]}
{"layout": 3276, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 325, "bbox": [137, 318, 172, 329], "page_size": [612.0, 792.0]}
{"layout": 3277, "type": "text", "text": "Decoded output of CornerHead, containing the following Tensors: ", "page_idx": 325, "bbox": [154, 334.46551513671875, 418.0710144042969, 347.7755432128906], "page_size": [612.0, 792.0]}
{"layout": 3278, "type": "text", "text": "• bboxes (Tensor): Coords of each box. • scores (Tensor): Scores of each box. • clses (Tensor): Categories of each box. ", "page_idx": 325, "bbox": [154, 352.3985290527344, 319, 401.5745849609375], "page_size": [612.0, 792.0]}
{"layout": 3279, "type": "text", "text": "Return type  tuple[torch.Tensor] forward ( feats ) ", "page_idx": 325, "bbox": [96.90701293945312, 405.56890869140625, 269.5189514160156, 437.33001708984375], "page_size": [612.0, 792.0]}
{"layout": 3280, "type": "text", "text": "Forward features from the upstream network. ", "page_idx": 325, "bbox": [118.82402038574219, 436.0845642089844, 299, 449.39459228515625], "page_size": [612.0, 792.0]}
{"layout": 3281, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 325, "bbox": [137, 453.3899230957031, 521, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 3282, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 325, "bbox": [137, 485, 172, 497], "page_size": [612.0, 792.0]}
{"layout": 3283, "type": "text", "text": "Usually a tuple of corner heatmaps, offset heatmaps and embedding heatmaps. ", "page_idx": 325, "bbox": [154, 501.8385925292969, 466.8879699707031, 515.1486206054688], "page_size": [612.0, 792.0]}
{"layout": 3284, "type": "text", "text": "• tl_heats (list[Tensor]): Top-left corner heatmaps for all levels, each is a 4D-tensor, the channels number is num classes. • br_heats (list[Tensor]): Bottom-right corner heatmaps for all levels, each is a 4D-tensor, the channels number is num classes. • tl_embs (list[Tensor] | list[None]): Top-left embedding heatmaps for all levels, each is a 4D-tensor or None. If not None, the channels number is corner em b channels. • br_embs (list[Tensor] | list[None]): Bottom-right embedding heatmaps for all levels, each is a 4D-tensor or None. If not None, the channels number is corner em b channels. • tl_offs (list[Tensor]): Top-left offset heatmaps for all levels, each is a 4D-tensor. The chan- nels number is corner offset channels. • br_offs (list[Tensor]): Bottom-right offset heatmaps for all levels, each is a 4D-tensor. The channels number is corner offset channels. ", "page_idx": 325, "bbox": [154, 519.7705688476562, 521, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 3285, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 325, "bbox": [137, 701, 213, 712], "page_size": [612.0, 792.0]}
{"layout": 3286, "type": "text", "text": "forward single ( x ,  lvl_ind ,  return pool  $\\mathbf{\\{}=}$  False ) Forward feature of a single level. ", "page_idx": 326, "bbox": [96, 71.30303192138672, 299, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3287, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 326, "bbox": [136, 102, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 3288, "type": "text", "text": "•  x  ( Tensor ) – Feature of a single level. •  lvl_ind  ( int ) – Level index of current feature. •  return pool  ( bool ) – Return corner pool feature or not. ", "page_idx": 326, "bbox": [154, 119.27247619628906, 395, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 3289, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 326, "bbox": [136, 174, 174, 186], "page_size": [612.0, 792.0]}
{"layout": 3290, "type": "text", "text": "A tuple of CornerHead’s output for current feature level. Containing the following Tensors: • tl_heat (Tensor): Predicted top-left corner heatmap. • br_heat (Tensor): Predicted bottom-right corner heatmap. • tl_emb (Tensor | None): Predicted top-left embedding heatmap. None for self.with corner em b  $\\cdot==F a l s e$  . • br_emb (Tensor | None): Predicted bottom-right embedding heatmap. None for self.with corner em b  $\\cdot==F a l s e$  . • tl_off (Tensor): Predicted top-left offset heatmap. • br_off (Tensor): Predicted bottom-right offset heatmap. • tl_pool (Tensor): Top-left corner pool feature. Not must have. • br_pool (Tensor): Bottom-right corner pool feature. Not must have. ", "page_idx": 326, "bbox": [154, 191.00343322753906, 521, 371.6864929199219], "page_size": [612.0, 792.0]}
{"layout": 3291, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 326, "bbox": [137.45498657226562, 375.6808166503906, 249, 390.21624755859375], "page_size": [612.0, 792.0]}
{"layout": 3292, "type": "text", "text": "get_bboxes ( tl_heats ,  br_heats ,  tl_embs ,  br_embs ,  tl_offs ,  br_offs ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False , with_nms  $\\mathbf{=}$  True ) Transform network output for a batch into bbox predictions. ", "page_idx": 326, "bbox": [96, 394.092041015625, 475.8256530761719, 431.4624938964844], "page_size": [612.0, 792.0]}
{"layout": 3293, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 326, "bbox": [136, 437, 187, 449], "page_size": [612.0, 792.0]}
{"layout": 3294, "type": "text", "text": "•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_embs  ( list[Tensor] ) – Top-left corner embeddings for each level with shape (N, corner em b channels, H, W). •  br_embs  ( list[Tensor] ) – Bottom-right corner embeddings for each level with shape (N, corner em b channels, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. ", "page_idx": 326, "bbox": [154, 454.0174865722656, 521, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 3295, "type": "text", "text": "get targets ( gt_bboxes ,  gt_labels ,  feat_shape ,  img_shape ,  with corner em b  $=$  False , with guiding sh if  $\\leftrightharpoons$  False ,  with centripetal shift  $\\leftrightharpoons$  False ) Generate corner targets. ", "page_idx": 327, "bbox": [96, 71.30303192138672, 443, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 3296, "type": "text", "text": "Including corner heatmap, corner offset. Optional: corner embedding, corner guiding shift, centripetal shift. For CornerNet, we generate corner heatmap, corner offset and corner embedding from this function. ", "page_idx": 327, "bbox": [118, 113.29548645019531, 517.5670776367188, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 3297, "type": "text", "text": "For Centripetal Net, we generate corner heatmap, corner offset, guiding shift and centripetal shift from this function. ", "page_idx": 327, "bbox": [118, 167.0934600830078, 540.0027465820312, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 3298, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 327, "bbox": [136, 198, 187, 210], "page_size": [612.0, 792.0]}
{"layout": 3299, "type": "text", "text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). •  feat_shape  ( list[int] ) – Shape of output feature, [batch, channel, height, width]. •  img_shape  ( list[int] ) – Shape of input image, [height, width, channel]. •  with corner em b  ( bool ) – Generate corner embedding target or not. Default: False. •  with guiding shift  ( bool ) – Generate guiding shift target or not. Default: False. •  with centripetal shift  ( bool ) – Generate centripetal shift target or not. Default: False. ", "page_idx": 327, "bbox": [154, 214.91444396972656, 521, 359.7315368652344], "page_size": [612.0, 792.0]}
{"layout": 3300, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 327, "bbox": [136, 366, 172, 377], "page_size": [612.0, 792.0]}
{"layout": 3301, "type": "text", "text": "Ground truth of corner heatmap, corner offset, corner embedding, guiding shift and cen- tripetal shift. Containing the following keys: ", "page_idx": 327, "bbox": [154, 382.2865295410156, 521, 407.5515441894531], "page_size": [612.0, 792.0]}
{"layout": 3302, "type": "text", "text": "• top left heat map (Tensor): Ground truth top-left corner heatmap. • bottom right heat map (Tensor): Ground truth bottom-right corner heatmap. • top left offset (Tensor): Ground truth top-left corner offset. • bottom right offset (Tensor): Ground truth bottom-right corner offset. • corner embedding (list[list[list[int]]]): Ground truth corner embedding. Not must have. • top left guiding shift (Tensor): Ground truth top-left corner guiding shift. Not must have. • bottom right guiding shift (Tensor): Ground truth bottom-right corner guiding shift. Not must have. • top left centripetal shift (Tensor): Ground truth top-left corner centripetal shift. Not must have. • bottom right centripetal shift (Tensor): Ground truth bottom-right corner centripetal shift. Not must have. ", "page_idx": 327, "bbox": [154, 412.1745300292969, 521, 604.8116455078125], "page_size": [612.0, 792.0]}
{"layout": 3303, "type": "text", "text": "Return type  dict ", "text_level": 1, "page_idx": 327, "bbox": [137, 611, 210, 623], "page_size": [612.0, 792.0]}
{"layout": 3304, "type": "text", "text": "in it weights()", "text_level": 1, "page_idx": 327, "bbox": [96, 629, 172, 640], "page_size": [612.0, 792.0]}
{"layout": 3305, "type": "text", "text": "Initialize the weights. ", "page_idx": 327, "bbox": [118, 639.3225708007812, 206, 652.6326293945312], "page_size": [612.0, 792.0]}
{"layout": 3306, "type": "text", "text": "loss ( tl_heats ,  br_heats ,  tl_embs ,  br_embs ,  tl_offs ,  br_offs ,  gt_bboxes ,  gt_labels ,  img_metas , gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None ) Compute losses of the head. ", "page_idx": 327, "bbox": [96, 657.1061401367188, 469.469482421875, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 3307, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 327, "bbox": [136, 700, 187, 711], "page_size": [612.0, 792.0]}
{"layout": 3308, "type": "text", "text": "•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_embs  ( list[Tensor] ) – Top-left corner embeddings for each level with shape (N, corner em b channels, H, W). •  br_embs  ( list[Tensor] ) – Bottom-right corner embeddings for each level with shape (N, corner em b channels, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [left, top, right, bottom] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – Specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 328, "bbox": [154, 71.45246887207031, 521, 353.7535705566406], "page_size": [612.0, 792.0]}
{"layout": 3309, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 328, "bbox": [137, 360, 172, 372], "page_size": [612.0, 792.0]}
{"layout": 3310, "type": "text", "text": "A dictionary of loss components. Containing the following losses: ", "page_idx": 328, "bbox": [154, 376.3085632324219, 418, 389.61859130859375], "page_size": [612.0, 792.0]}
{"layout": 3311, "type": "text", "text": "• det_loss (list[Tensor]): Corner keypoint losses of all feature levels. • pull_loss (list[Tensor]): Part one of Associative Embedding losses of all feature levels. • push_loss (list[Tensor]): Part two of Associative Embedding losses of all feature levels. • off_loss (list[Tensor]): Corner offset losses of all feature levels. ", "page_idx": 328, "bbox": [154, 394.2415771484375, 509.85693359375, 461.3496398925781], "page_size": [612.0, 792.0]}
{"layout": 3312, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 328, "bbox": [137.4550018310547, 465.344970703125, 256.1192932128906, 479.8804016113281], "page_size": [612.0, 792.0]}
{"layout": 3313, "type": "text", "text": "loss single ( tl_hmp ,  br_hmp ,  tl_emb ,  br_emb ,  tl_off ,  br_off ,  targets ) Compute losses for single level. ", "page_idx": 328, "bbox": [96.90699768066406, 483.7561950683594, 382.88934326171875, 509.170654296875], "page_size": [612.0, 792.0]}
{"layout": 3314, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 328, "bbox": [136, 515, 187, 527], "page_size": [612.0, 792.0]}
{"layout": 3315, "type": "text", "text": "•  tl_hmp  ( Tensor ) – Top-left corner heatmap for current level with shape (N, num classes, H, W). •  br_hmp  ( Tensor ) – Bottom-right corner heatmap for current level with shape (N, num classes, H, W). •  tl_emb  ( Tensor ) – Top-left corner embedding for current level with shape (N, cor- ner em b channels, H, W). •  br_emb  ( Tensor ) – Bottom-right corner embedding for current level with shape (N, cor- ner em b channels, H, W). •  tl_off  ( Tensor ) – Top-left corner offset for current level with shape (N, cor- ner offset channels, H, W). •  br_off  ( Tensor ) – Bottom-right corner offset for current level with shape (N, cor- ner offset channels, H, W). ", "page_idx": 328, "bbox": [154, 531.7266235351562, 521, 706.4306640625], "page_size": [612.0, 792.0]}
{"layout": 3316, "type": "text", "text": "•  targets  ( dict ) – Corner target generated by  get targets . ", "page_idx": 329, "bbox": [154, 71.30303192138672, 395.50665283203125, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3317, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 329, "bbox": [137, 90, 173, 102], "page_size": [612.0, 792.0]}
{"layout": 3318, "type": "text", "text": "Losses of the head’s different branches containing the following losses: ", "page_idx": 329, "bbox": [154, 107.31745910644531, 437.6575012207031, 120.62749481201172], "page_size": [612.0, 792.0]}
{"layout": 3319, "type": "text", "text": "• det_loss (Tensor): Corner keypoint loss. • pull_loss (Tensor): Part one of Associative Embedding loss. • push_loss (Tensor): Part two of Associative Embedding loss. • off_loss (Tensor): Corner offset loss. ", "page_idx": 329, "bbox": [154, 125.25044250488281, 404.7116394042969, 192.3584442138672], "page_size": [612.0, 792.0]}
{"layout": 3320, "type": "text", "text": "Return type  tuple[torch.Tensor] ", "page_idx": 329, "bbox": [137, 196.353759765625, 269.5189208984375, 210.88919067382812], "page_size": [612.0, 792.0]}
{"layout": 3321, "type": "text", "text": "on nx export ( tl_heats ,  br_heats ,  tl_embs ,  br_embs ,  tl_offs ,  br_offs ,  img_metas ,  rescale=False , with_nms  $\\mathbf{\\tilde{=}}$  True ) Transform network output for a batch into bbox predictions. ", "page_idx": 329, "bbox": [96.906982421875, 214.7649383544922, 482, 252.1344451904297], "page_size": [612.0, 792.0]}
{"layout": 3322, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 329, "bbox": [136, 257, 187, 269], "page_size": [612.0, 792.0]}
{"layout": 3323, "type": "text", "text": "•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_embs  ( list[Tensor] ) – Top-left corner embeddings for each level with shape (N, corner em b channels, H, W). •  br_embs  ( list[Tensor] ) – Bottom-right corner embeddings for each level with shape (N, corner em b channels, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. ", "page_idx": 329, "bbox": [154, 274.6904296875, 521, 515.1484985351562], "page_size": [612.0, 792.0]}
{"layout": 3324, "type": "text", "text": "Returns  First tensor bboxes with shape [N, num_det, 5], 5 arrange as (x1, y1, x2, y2, score) and second element is class labels of shape [N, num_det]. ", "page_idx": 329, "bbox": [137, 519.142822265625, 521, 545.0364379882812], "page_size": [612.0, 792.0]}
{"layout": 3325, "type": "text", "text": "Return type  tuple[Tensor, Tensor] ", "page_idx": 329, "bbox": [137, 549.03076171875, 278, 563.566162109375], "page_size": [612.0, 792.0]}
{"layout": 3326, "type": "text", "text": "class  mmdet.models.dense heads. DETRHead ( num classes ,  in channels ,  num_query=100 ,  num reg fcs=2 , transforme  $r{=}$  None ,  sync cls avg factor=False , positional encoding={'normalize': True, 'num_feats': 128, 'type': 'Sine Positional Encoding'} ,  loss_cls={'bg cls weight': 0.1, 'class_weight': 1.0, 'loss_weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': False} , loss_bbox={'loss weight': 5.0, 'type': 'L1Loss'} , loss_iou={'loss weight': 2.0, 'type': 'GIoULoss'} , train_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'assigner': {'cls_cost': {'type': 'Classification Cost', 'weight': 1.0}, 'iou_cost': {'iou_mode': 'giou', 'type': 'IoUCost', 'weight': 2.0}, 'reg_cost': {'type': 'BBoxL1Cost', 'weight': 5.0}, 'type': 'Hungarian As signer'}} ,  test_cfg  $=$  {'max_per_img': 100} , init_cfg  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) ", "page_idx": 330, "bbox": [72.0, 71.30303192138672, 533, 228.2246551513672], "page_size": [612.0, 792.0]}
{"layout": 3327, "type": "text", "text": "Implements the DETR transformer head. ", "page_idx": 330, "bbox": [96, 226.86964416503906, 262, 240.17967224121094], "page_size": [612.0, 792.0]}
{"layout": 3328, "type": "text", "text": "See  paper: End-to-End Object Detection with Transformers  for details. ", "page_idx": 330, "bbox": [96, 244.80262756347656, 379.12652587890625, 258.1126708984375], "page_size": [612.0, 792.0]}
{"layout": 3329, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 330, "bbox": [117, 264, 169, 275], "page_size": [612.0, 792.0]}
{"layout": 3330, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background. •  in channels  ( int ) – Number of channels in the input feature map. •  num_query  ( int ) – Number of query in Transformer. •  num reg fcs  ( int, optional ) – Number of fully-connected layers used in  FFN , which is then used for the regression head. Default 2. •  (obj  ( test_cfg ) –  \\` mmcv.ConfigDict\\`|dict): Config for transformer. Default: None. •  sync cls avg factor  ( bool ) – Whether to sync the avg_factor of all ranks. Default to False. •  (obj  –  \\` mmcv.ConfigDict\\`|dict): Config for position encoding. •  (obj  –  mmcv.ConfigDict\\`|dict): Config of the classification loss. Default \\`Cross Entropy- Loss . •  (obj  –  mmcv.ConfigDict\\`|dict): Config of the regression loss. Default \\`L1Loss . •  (obj  –  mmcv.ConfigDict\\`|dict): Config of the regression iou loss. Default \\`GIoULoss . •  (obj  –  \\` mmcv.ConfigDict\\`|dict): Training config of transformer head. •  (obj  –  \\` mmcv.ConfigDict\\`|dict): Testing config of transformer head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 330, "bbox": [145, 280.66766357421875, 518, 545.0368041992188], "page_size": [612.0, 792.0]}
{"layout": 3331, "type": "text", "text": "forward ( feats ,  img_metas ) Forward function. ", "page_idx": 330, "bbox": [96, 549.5093383789062, 210.5642547607422, 574.9247436523438], "page_size": [612.0, 792.0]}
{"layout": 3332, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 330, "bbox": [136, 581, 188, 592], "page_size": [612.0, 792.0]}
{"layout": 3333, "type": "text", "text": "•  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – List of image information. ", "page_idx": 330, "bbox": [154, 597.479736328125, 505.76116943359375, 628.7227783203125], "page_size": [612.0, 792.0]}
{"layout": 3334, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 330, "bbox": [136, 635, 173, 646], "page_size": [612.0, 792.0]}
{"layout": 3335, "type": "text", "text": "Outputs for all scale levels. • all cls scores list (list[Tensor]): Classification scores for each scale level. Each is a 4D- tensor with shape [nb_dec, bs, num_query, cls out channels]. Note  cls out channels should includes background. ", "page_idx": 330, "bbox": [154, 651.2777099609375, 521, 706.4307861328125], "page_size": [612.0, 792.0]}
{"layout": 3336, "type": "text", "text": "• all b box p reds list (list[Tensor]): Sigmoid regression outputs for each scale level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. ", "page_idx": 331, "bbox": [154, 71.45246887207031, 521, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 3337, "type": "text", "text": "Return type  tuple[list[Tensor], list[Tensor]] ", "page_idx": 331, "bbox": [137, 112.6678466796875, 315, 127.20327758789062], "page_size": [612.0, 792.0]}
{"layout": 3338, "type": "text", "text": "forward on nx ( feats ,  img_metas ) Forward function for exporting to ONNX. Over-write  forward  because:  masks  is directly created with zero (valid position tag) and has the same spatial size as  $x$  . Thus the construction of  masks  is different from that in  forward . ", "page_idx": 331, "bbox": [96, 131.0790252685547, 539.994873046875, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 3339, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 331, "bbox": [136, 192, 188, 204], "page_size": [612.0, 792.0]}
{"layout": 3340, "type": "text", "text": "•  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – List of image information. ", "page_idx": 331, "bbox": [154, 208.9364776611328, 505.76226806640625, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 3341, "type": "text", "text": "", "page_idx": 331, "bbox": [154, 226.8694610595703, 388, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3342, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 331, "bbox": [136, 246, 173, 258], "page_size": [612.0, 792.0]}
{"layout": 3343, "type": "text", "text": "", "page_idx": 331, "bbox": [152, 264, 262, 271.75], "page_size": [612.0, 792.0]}
{"layout": 3344, "type": "text", "text": "• all cls scores list (list[Tensor]): Classification scores for each scale level. Each is a 4D- tensor with shape [nb_dec, bs, num_query, cls out channels]. Note  cls out channels should includes background. • all b box p reds list (list[Tensor]): Sigmoid regression outputs for each scale level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. ", "page_idx": 331, "bbox": [154, 280.66748046875, 521, 359.7314758300781], "page_size": [612.0, 792.0]}
{"layout": 3345, "type": "text", "text": "Return type  tuple[list[Tensor], list[Tensor]] ", "page_idx": 331, "bbox": [137, 363.7257995605469, 315, 378.26123046875], "page_size": [612.0, 792.0]}
{"layout": 3346, "type": "text", "text": "forward single ( x ,  img_metas ) “Forward function for a single feature level. ", "page_idx": 331, "bbox": [96, 382.13702392578125, 292.9114685058594, 407.5514831542969], "page_size": [612.0, 792.0]}
{"layout": 3347, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 331, "bbox": [136, 414, 188, 425], "page_size": [612.0, 792.0]}
{"layout": 3348, "type": "text", "text": "•  x  ( Tensor ) – Input feature from backbone’s single stage, shape [bs, c, h, w]. •  img_metas  ( list[dict] ) – List of image information. ", "page_idx": 331, "bbox": [154, 430.10748291015625, 468, 461.3495178222656], "page_size": [612.0, 792.0]}
{"layout": 3349, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 331, "bbox": [136, 468, 172, 479], "page_size": [612.0, 792.0]}
{"layout": 3350, "type": "text", "text": "Outputs from the classification head,  shape [nb_dec, bs, num_query, cls out channels]. Note cls out channels should includes background. all b box p reds (Tensor): Sigmoid outputs from the regression  head with normalized coordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4]. ", "page_idx": 331, "bbox": [154, 483.2778625488281, 521, 539.0585327148438], "page_size": [612.0, 792.0]}
{"layout": 3351, "type": "text", "text": "Return type  all cls scores (Tensor) ", "page_idx": 331, "bbox": [137, 543.0538330078125, 286, 557.5892333984375], "page_size": [612.0, 792.0]}
{"layout": 3352, "type": "text", "text": "forward single on nx ( x ,  img_metas ) “Forward function for a single feature level with ONNX exportation. ", "page_idx": 331, "bbox": [96, 561.465087890625, 392.1488342285156, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 3353, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 331, "bbox": [136, 592, 188, 604], "page_size": [612.0, 792.0]}
{"layout": 3354, "type": "text", "text": "•  x  ( Tensor ) – Input feature from backbone’s single stage, shape [bs, c, h, w]. •  img_metas  ( list[dict] ) – List of image information. ", "page_idx": 331, "bbox": [154, 609.4345092773438, 468, 640.6775512695312], "page_size": [612.0, 792.0]}
{"layout": 3355, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 331, "bbox": [136, 647, 172, 658], "page_size": [612.0, 792.0]}
{"layout": 3356, "type": "text", "text": "Outputs from the classification head,  shape [nb_dec, bs, num_query, cls out channels]. Note cls out channels should includes background. all b box p reds (Tensor): Sigmoid outputs from the regression  head with normalized coordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4]. ", "page_idx": 331, "bbox": [154, 662.6048583984375, 521, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 3357, "type": "text", "text": "Return type  all cls scores (Tensor) ", "page_idx": 332, "bbox": [137, 70.8248291015625, 285.1602478027344, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 3358, "type": "text", "text": "forward train ( x ,  img_metas ,  gt_bboxes ,  gt_labels  $\\mathbf{\\hat{\\rho}}$  None ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  proposal cf g  $=$  None , \\*\\*kwargs ) Forward function for training mode. ", "page_idx": 332, "bbox": [96, 89.23503875732422, 527.2926635742188, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 3359, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 332, "bbox": [136, 133, 188, 144], "page_size": [612.0, 792.0]}
{"layout": 3360, "type": "text", "text": "•  x  ( list[Tensor] ) – Features from backbone. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt_bboxes  ( Tensor ) – Ground truth bboxes of the image, shape (num_gts, 4). •  gt_labels  ( Tensor ) – Ground truth labels of each box, shape (num_gts,). •  gt b boxes ignore ( Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). •  proposal cf g  ( mmcv.Config ) – Test / post processing configuration, if None, test_cfg would be used. ", "page_idx": 332, "bbox": [154, 149.1604766845703, 521, 288.00048828125], "page_size": [612.0, 792.0]}
{"layout": 3361, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 332, "bbox": [137, 291.99481201171875, 308.6417236328125, 306.5302429199219], "page_size": [612.0, 792.0]}
{"layout": 3362, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 332, "bbox": [137, 309.9278259277344, 256, 324.4632568359375], "page_size": [612.0, 792.0]}
{"layout": 3363, "type": "text", "text": "get_bboxes ( all cls scores list ,  all b box p reds list ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Transform network outputs for a batch into bbox predictions. ", "page_idx": 332, "bbox": [96, 328.33905029296875, 426.45037841796875, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 3364, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 332, "bbox": [136, 360, 187, 371], "page_size": [612.0, 792.0]}
{"layout": 3365, "type": "text", "text": "•  all cls scores list  ( list[Tensor] ) – Classification outputs for each feature level. Each is a 4D-tensor with shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds list  ( list[Tensor] ) – Sigmoid regression outputs for each feature level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  img_metas  ( list[dict] ) – Meta information of each image. •  rescale  ( bool, optional ) – If True, return boxes in original image space. Default False. ", "page_idx": 332, "bbox": [154, 376.3085021972656, 521, 491.2375183105469], "page_size": [612.0, 792.0]}
{"layout": 3366, "type": "text", "text": "Returns  Each item in result list is 2-tuple. The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. ", "page_idx": 332, "bbox": [137, 495.23284912109375, 521, 545.0364990234375], "page_size": [612.0, 792.0]}
{"layout": 3367, "type": "text", "text": "Return type  list[list[Tensor, Tensor]] ", "page_idx": 332, "bbox": [137, 549.0308227539062, 288.88616943359375, 563.5662231445312], "page_size": [612.0, 792.0]}
{"layout": 3368, "type": "text", "text": "get targets ( cls scores list ,  b box p reds list ,  gt b boxes list ,  gt labels list ,  img_metas , gt b boxes ignore lis $\\leftleftarrows$ None)“Compute regression and classification targets for a batch image. ", "page_idx": 332, "bbox": [96, 567.4420166015625, 457.0955505371094, 604.8115234375], "page_size": [612.0, 792.0]}
{"layout": 3369, "type": "text", "text": "Outputs from a single decoder layer of a single feature level are used. ", "page_idx": 332, "bbox": [118, 609.4344482421875, 394.1712646484375, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 3370, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 332, "bbox": [136, 628, 188, 640], "page_size": [612.0, 792.0]}
{"layout": 3371, "type": "text", "text": " cls scores list  ( list[Tensor] ) – Box score logits from a single decoder layer for each image with shape [num_query, cls out channels]. •  b box p reds list  ( list[Tensor] ) – Sigmoid outputs from a single decoder layer for each image, with normalized coordinate (cx, cy, w, h) and shape [num_query, 4]. ", "page_idx": 332, "bbox": [154, 645.3004760742188, 521, 700.4534912109375], "page_size": [612.0, 792.0]}
{"layout": 3372, "type": "text", "text": "•  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore list  ( list[Tensor], optional ) – Bounding boxes which can be ignored for each image. Default None. ", "page_idx": 333, "bbox": [154, 71.45246887207031, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 3373, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 333, "bbox": [136, 180, 173, 193], "page_size": [612.0, 792.0]}
{"layout": 3374, "type": "text", "text": "a tuple containing the following targets. • labels list (list[Tensor]): Labels for all images. • label weights list (list[Tensor]): Label weights for all images. • b box targets list (list[Tensor]): BBox targets for all images. • b box weights list (list[Tensor]): BBox weights for all images. • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. ", "page_idx": 333, "bbox": [154, 196.98146057128906, 418, 317.8874816894531], "page_size": [612.0, 792.0]}
{"layout": 3375, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 333, "bbox": [136, 323, 215, 336], "page_size": [612.0, 792.0]}
{"layout": 3376, "type": "text", "text": "in it weights()Initialize weights of the transformer head. ", "page_idx": 333, "bbox": [96, 342.1669921875, 285.4385681152344, 365.70849609375], "page_size": [612.0, 792.0]}
{"layout": 3377, "type": "text", "text": "loss ( all cls scores list ,  all b box p reds list ,  gt b boxes list ,  gt labels list ,  img_metas , gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) “Loss function. ", "page_idx": 333, "bbox": [96, 370.1820373535156, 451.4866638183594, 407.5514831542969], "page_size": [612.0, 792.0]}
{"layout": 3378, "type": "text", "text": "Only outputs from the last feature level are used for computing losses by default. ", "page_idx": 333, "bbox": [118, 412.1744689941406, 439.16143798828125, 425.4844970703125], "page_size": [612.0, 792.0]}
{"layout": 3379, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 333, "bbox": [136, 432, 187, 443], "page_size": [612.0, 792.0]}
{"layout": 3380, "type": "text", "text": "•  all cls scores list  ( list[Tensor] ) – Classification outputs for each feature level. Each is a 4D-tensor with shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds list  ( list[Tensor] ) – Sigmoid regression outputs for each feature level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore  ( list[Tensor], optional ) – Bounding boxes which can be ig- nored for each image. Default None. ", "page_idx": 333, "bbox": [154, 448.03948974609375, 521, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 3381, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 333, "bbox": [137.45494079589844, 644.6728515625, 256.1192321777344, 659.208251953125], "page_size": [612.0, 792.0]}
{"layout": 3382, "type": "text", "text": "loss single ( cls_scores ,  bbox_preds ,  gt b boxes list ,  gt labels list ,  img_metas , gt b boxes ignore lis $\\leftleftarrows$ None)“Loss function for outputs from a single decoder layer of a single feature level. ", "page_idx": 333, "bbox": [96, 663.0840454101562, 431.6893615722656, 700.4534912109375], "page_size": [612.0, 792.0]}
{"layout": 3383, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 333, "bbox": [136, 706, 186, 718], "page_size": [612.0, 792.0]}
{"layout": 3384, "type": "text", "text": "•  cls_scores  ( Tensor ) – Box score logits from a single decoder layer for all images. Shape [bs, num_query, cls out channels]. •  bbox_preds  ( Tensor ) – Sigmoid outputs from a single decoder layer for all images, with normalized coordinate (cx, cy, w, h) and shape [bs, num_query, 4]. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore list  ( list[Tensor], optional ) – Bounding boxes which can be ignored for each image. Default None. ", "page_idx": 334, "bbox": [154, 71.45246887207031, 521, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 3385, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 334, "bbox": [137, 240, 173, 252], "page_size": [612.0, 792.0]}
{"layout": 3386, "type": "text", "text": "A dictionary of loss components for outputs from  a single decoder layer. ", "page_idx": 334, "bbox": [154, 256.12982177734375, 456.18878173828125, 270.6652526855469], "page_size": [612.0, 792.0]}
{"layout": 3387, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 334, "bbox": [137, 274.0628356933594, 256.1192626953125, 288.5982666015625], "page_size": [612.0, 792.0]}
{"layout": 3388, "type": "text", "text": "on nx export ( all cls scores list ,  all b box p reds list ,  img_metas ) Transform network outputs into bbox predictions, with ONNX exportation. ", "page_idx": 334, "bbox": [96, 292.4730529785156, 417.8412780761719, 317.88751220703125], "page_size": [612.0, 792.0]}
{"layout": 3389, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 334, "bbox": [136, 324, 187, 335], "page_size": [612.0, 792.0]}
{"layout": 3390, "type": "text", "text": "•  all cls scores list  ( list[Tensor] ) – Classification outputs for each feature level. Each is a 4D-tensor with shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds list  ( list[Tensor] ) – Sigmoid regression outputs for each feature level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  img_metas  ( list[dict] ) – Meta information of each image. ", "page_idx": 334, "bbox": [154, 340.4435119628906, 521, 425.4845275878906], "page_size": [612.0, 792.0]}
{"layout": 3391, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 334, "bbox": [136, 431, 173, 443], "page_size": [612.0, 792.0]}
{"layout": 3392, "type": "text", "text": "dets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. Return type  tuple[Tensor, Tensor] ", "page_idx": 334, "bbox": [137, 447.411865234375, 439.3216552734375, 479.88031005859375], "page_size": [612.0, 792.0]}
{"layout": 3393, "type": "text", "text": "simple test b boxes ( feats ,  img_metas ,  rescale=False ) Test det bboxes without test-time augmentation. ", "page_idx": 334, "bbox": [96, 483.756103515625, 329, 509.1705627441406], "page_size": [612.0, 792.0]}
{"layout": 3394, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 334, "bbox": [136, 515, 187, 527], "page_size": [612.0, 792.0]}
{"layout": 3395, "type": "text", "text": "•  feats  ( tuple[torch.Tensor] ) – Multi-level features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 334, "bbox": [154, 531.7265014648438, 521, 592.8565673828125], "page_size": [612.0, 792.0]}
{"layout": 3396, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 334, "bbox": [137, 599, 173, 610], "page_size": [612.0, 792.0]}
{"layout": 3397, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is  bboxes  with shape (n, 5), where 5 rep- resent (tl_x, tl_y, br_x, br_y, score). The shape of the second tensor in the tuple is  labels with shape (n,) ", "page_idx": 334, "bbox": [154, 614.784912109375, 521, 652.632568359375], "page_size": [612.0, 792.0]}
{"layout": 3398, "type": "text", "text": "Return type  list[tuple[Tensor, Tensor]] ", "page_idx": 334, "bbox": [137, 656.6278686523438, 296.7765808105469, 671.1632690429688], "page_size": [612.0, 792.0]}
{"layout": 3399, "type": "text", "text": "class  mmdet.models.dense heads. Decoupled SOLO Head ( \\*args ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  [{'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}, {'type': 'Normal', 'std': 0.01, 'bias_prob':0.01, 'override': {'name': 'con v mask list x'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01,'override': {'name': 'con v mask list y'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override':{'name': 'conv_cls'}}] ,  \\*\\*kwargs ) ", "page_idx": 335, "bbox": [72.0, 71.30303192138672, 540, 156.4935760498047], "page_size": [612.0, 792.0]}
{"layout": 3400, "type": "text", "text": "Decoupled SOLO mask head used in  \\` SOLO: Segmenting Objects by Locations. ", "page_idx": 335, "bbox": [96, 154.51092529296875, 418.420166015625, 169.04635620117188], "page_size": [612.0, 792.0]}
{"layout": 3401, "type": "text", "text": "< https://arxiv.org/abs/1912.04488 ", "page_idx": 335, "bbox": [96, 173.07154846191406, 232.9559326171875, 186.38157653808594], "page_size": [612.0, 792.0]}
{"layout": 3402, "type": "text", "text": "Parameters  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 335, "bbox": [118, 190.37591552734375, 483, 204.91134643554688], "page_size": [612.0, 792.0]}
{"layout": 3403, "type": "text", "text": "forward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 335, "bbox": [96, 208.78709411621094, 313.4840087890625, 252.1345672607422], "page_size": [612.0, 792.0]}
{"layout": 3404, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 335, "bbox": [118, 268.0848388671875, 540, 305.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 3405, "type": "text", "text": "get results ( ml vl mask p reds x ,  ml vl mask p reds y ,  ml vl cls scores ,  img_metas ,  rescale=None , \\*\\*kwargs ) Get multi-image mask results. ", "page_idx": 335, "bbox": [96, 322.36102294921875, 500.7516784667969, 359.7314758300781], "page_size": [612.0, 792.0]}
{"layout": 3406, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 335, "bbox": [136, 366, 187, 377], "page_size": [612.0, 792.0]}
{"layout": 3407, "type": "text", "text": "•  ml vl mask p reds x  ( list[Tensor] ) – Multi-level mask prediction from x branch. Each element in the list has shape (batch_size, num_grids ,h ,w). •  ml vl mask p reds y  ( list[Tensor] ) – Multi-level mask prediction from y branch. Each element in the list has shape (batch_size, num_grids ,h ,w). •  ml vl cls scores  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes ,num_grids ,num_grids). •  img_metas  ( list[dict] ) – Meta information of all images. ", "page_idx": 335, "bbox": [154, 382.2864685058594, 521, 485.260498046875], "page_size": [612.0, 792.0]}
{"layout": 3408, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 335, "bbox": [136, 491, 173, 503], "page_size": [612.0, 792.0]}
{"layout": 3409, "type": "text", "text": "Processed results of multiple images.Each  Instance Data  usually contains following keys. ", "page_idx": 335, "bbox": [154, 507.8155212402344, 517, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 3410, "type": "text", "text": "• scores (Tensor): Classification scores, has shape (num instance,). • labels (Tensor): Has shape (num instances,). • masks (Tensor): Processed mask results, has shape (num instances, h, w). ", "page_idx": 335, "bbox": [154, 525.7484741210938, 459.4163818359375, 574.9244995117188], "page_size": [612.0, 792.0]}
{"layout": 3411, "type": "text", "text": "Return type  list[ Instance Data ] ", "page_idx": 335, "bbox": [137.4539337158203, 578.9188232421875, 275, 593.4542236328125], "page_size": [612.0, 792.0]}
{"layout": 3412, "type": "text", "text": "loss ( ml vl mask p reds x ,  ml vl mask p reds y ,  ml vl cls p reds ,  gt_labels ,  gt_masks ,  img_metas , gt_bboxes  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Calculate the loss of total batch. ", "page_idx": 335, "bbox": [96, 597.3300170898438, 483, 634.6995239257812], "page_size": [612.0, 792.0]}
{"layout": 3413, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 335, "bbox": [136, 640, 188, 652], "page_size": [612.0, 792.0]}
{"layout": 3414, "type": "text", "text": "•  ml vl mask p reds x  ( list[Tensor] ) – Multi-level mask prediction from x branch. Each element in the list has shape (batch_size, num_grids ,h ,w). •  ml vl mask p reds x  – Multi-level mask prediction from y branch. Each element in the list has shape (batch_size, num_grids ,h ,w). ", "page_idx": 335, "bbox": [154, 657.2554321289062, 521, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 3415, "type": "text", "text": "•  ml vl cls p reds  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes, num_grids ,num_grids). •  gt_labels  ( list[Tensor] ) – Labels of multiple images. •  gt_masks  ( list[Tensor] ) – Ground truth masks of multiple images. Each has shape (num instances, h, w). •  img_metas  ( list[dict] ) – Meta information of multiple images. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of multiple images. Default: None. ", "page_idx": 336, "bbox": [154, 71.45246887207031, 521, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 3416, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 336, "bbox": [137, 184.3988037109375, 308.6416931152344, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 3417, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 336, "bbox": [137, 202.331787109375, 256, 216.86721801757812], "page_size": [612.0, 792.0]}
{"layout": 3418, "type": "text", "text": "class  mmdet.models.dense heads. Decoupled SOLO Light Head ( \\*args ,  dcn_cfg  $=$  None ,  init_cfg=[{'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}, {'type': 'Normal', 'std': 0.01, 'bias_prob':0.01, 'override': {'name': 'con v mask list x'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override': {'name': 'con v mask list y'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override': {'name': 'conv_cls'}}] ,  \\*\\*kwargs ) ", "page_idx": 336, "bbox": [71, 220.7419891357422, 532, 329.8434753417969], "page_size": [612.0, 792.0]}
{"layout": 3419, "type": "text", "text": "Decoupled Light SOLO mask head used in  SOLO: Segmenting Objects by Locations ", "page_idx": 336, "bbox": [96, 328.4884338378906, 436.69110107421875, 341.7984619140625], "page_size": [612.0, 792.0]}
{"layout": 3420, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 336, "bbox": [118, 348, 169, 359], "page_size": [612.0, 792.0]}
{"layout": 3421, "type": "text", "text": "•  with_dcn  ( bool ) – Whether use dcn in mask_convs and cls_convs, default: False. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 336, "bbox": [145, 364.35345458984375, 484.9248962402344, 395.59649658203125], "page_size": [612.0, 792.0]}
{"layout": 3422, "type": "text", "text": "forward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 336, "bbox": [96, 400.0700378417969, 313.4840087890625, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 3423, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 336, "bbox": [118, 459.3678283691406, 540, 497.2154846191406], "page_size": [612.0, 792.0]}
{"layout": 3424, "type": "text", "text": "class  mmdet.models.dense heads. De formable DET RHead ( \\*args ,  with box refine  $\\mathbf{\\beta}=$  False , as two stage  $\\mathbf{=}$  False ,  transformer=None , \\*\\*kwargs ) ", "page_idx": 336, "bbox": [71, 513.6439819335938, 497.22467041015625, 550.9038696289062], "page_size": [612.0, 792.0]}
{"layout": 3425, "type": "text", "text": "Head of DeformDETR: Deformable DETR: Deformable Transformers for End-to- End Object Detection. ", "page_idx": 336, "bbox": [96, 549.658447265625, 521, 562.968505859375], "page_size": [612.0, 792.0]}
{"layout": 3426, "type": "text", "text": "Code is modified from the  official github repo . More details can be found in the  paper  . ", "page_idx": 336, "bbox": [96, 567.5914306640625, 282.2511901855469, 598.83447265625], "page_size": [612.0, 792.0]}
{"layout": 3427, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 336, "bbox": [117, 605, 169, 616], "page_size": [612.0, 792.0]}
{"layout": 3428, "type": "text", "text": "•  with box refine  ( bool ) – Whether to refine the reference points in the decoder. Defaults to False. •  as two stage  ( bool ) – Whether to generate the proposal from the outputs of encoder. •  (obj  ( transformer ) –  ConfigDict ): ConfigDict is used for building the Encoder and De- coder. ", "page_idx": 336, "bbox": [145, 621.3894653320312, 521, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 3429, "type": "text", "text": "forward ( mlvl_feats ,  img_metas ) Forward function. ", "page_idx": 337, "bbox": [96, 71.30303192138672, 232.25436401367188, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3430, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 337, "bbox": [136, 103, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 3431, "type": "text", "text": "•  mlvl_feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor with shape (N, C, H, W). ", "page_idx": 337, "bbox": [154, 119.27247619628906, 521, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 3432, "type": "text", "text": "list[dict] ", "page_idx": 337, "bbox": [217.23500061035156, 151.4817657470703, 270.62457275390625, 161.49417114257812], "page_size": [612.0, 792.0]}
{"layout": 3433, "type": "text", "text": "Returns  Outputs from the classification head, shape [nb_dec, bs, num_query, cls out channels]. Note cls out channels should includes background. all b box p reds (Tensor): Sigmoid out- puts from the regression head with normalized coordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4]. enc outputs class (Tensor): The score of each point on en- code feature map, has shape (N,  $\\mathbf{h}^{\\ast}\\mathbf{w}$  , num_class). Only when as two stage is True it would be returned, otherwise  None  would be returned. enc outputs coord (Tensor): The proposal generate from the encode feature map, has shape   $(\\mathsf{N},\\,\\mathsf{h}^{\\ast}\\mathbf{w},\\,4)$  . Only when as two stage is True it would be returned, otherwise  None  would be returned. ", "page_idx": 337, "bbox": [137, 166.4658203125, 521, 264.089599609375], "page_size": [612.0, 792.0]}
{"layout": 3434, "type": "text", "text": "Return type  all cls scores (Tensor) ", "page_idx": 337, "bbox": [137, 268.08489990234375, 285.1602478027344, 282.6203308105469], "page_size": [612.0, 792.0]}
{"layout": 3435, "type": "text", "text": "get_bboxes ( all cls scores ,  all b box p reds ,  enc cls scores ,  enc b box p reds ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Transform network outputs for a batch into bbox predictions. ", "page_idx": 337, "bbox": [96, 286.4961242675781, 526.9932861328125, 311.91058349609375], "page_size": [612.0, 792.0]}
{"layout": 3436, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 337, "bbox": [137, 318, 187, 329], "page_size": [612.0, 792.0]}
{"layout": 3437, "type": "text", "text": "•  all cls scores  ( Tensor ) – Classification score of all decoder layers, has shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds  ( Tensor ) – Sigmoid regression outputs of all decode layers. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  enc cls scores  ( Tensor ) – Classification scores of points on encode feature map , has shape (N,   $\\mathbf{h}^{\\ast}\\mathbf{w}.$  , num classes). Only be passed when as two stage is True, otherwise is None. •  enc b box p reds  ( Tensor ) – Regression results of each points on the encode feature map, has shape (N, h\\*w, 4). Only be passed when as two stage is True, otherwise is None. •  img_metas  ( list[dict] ) – Meta information of each image. •  rescale  ( bool, optional ) – If True, return boxes in original image space. Default False. ", "page_idx": 337, "bbox": [154, 334.465576171875, 521, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 3438, "type": "text", "text": "Returns  Each item in result list is 2-tuple. The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. ", "page_idx": 337, "bbox": [137, 525.1209106445312, 521, 574.924560546875], "page_size": [612.0, 792.0]}
{"layout": 3439, "type": "text", "text": "Return type  list[list[Tensor, Tensor]] ", "page_idx": 337, "bbox": [137, 578.9188842773438, 288.8861083984375, 593.4542846679688], "page_size": [612.0, 792.0]}
{"layout": 3440, "type": "text", "text": "in it weights()Initialize weights of the DeformDETR head. ", "page_idx": 337, "bbox": [96, 599.2030639648438, 295.9589538574219, 622.7445678710938], "page_size": [612.0, 792.0]}
{"layout": 3441, "type": "text", "text": "loss ( all cls scores ,  all b box p reds ,  enc cls scores ,  enc b box p reds ,  gt b boxes list ,  gt labels list , img_metas, gt b boxes ignore $\\mathbf{\\dot{\\rho}}$ None)“Loss function. ", "page_idx": 337, "bbox": [96, 627.2180786132812, 505, 664.5875854492188], "page_size": [612.0, 792.0]}
{"layout": 3442, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 337, "bbox": [136, 670, 187, 682], "page_size": [612.0, 792.0]}
{"layout": 3443, "type": "text", "text": "•  all cls scores  ( Tensor ) – Classification score of all decoder layers, has shape [nb_dec, bs, num_query, cls out channels]. ", "page_idx": 337, "bbox": [154, 687.1434936523438, 521, 712.4085693359375], "page_size": [612.0, 792.0]}
{"layout": 3444, "type": "text", "text": "•  all b box p reds  ( Tensor ) – Sigmoid regression outputs of all decode layers. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  enc cls scores  ( Tensor ) – Classification scores of points on encode feature map , has shape (N, h\\*w, num classes). Only be passed when as two stage is True, otherwise is None. •  enc b box p reds  ( Tensor ) – Regression results of each points on the encode feature map, has shape (N, h\\*w, 4). Only be passed when as two stage is True, otherwise is None. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore  ( list[Tensor], optional ) – Bounding boxes which can be ig- nored for each image. Default None.  A dictionary of loss components. ", "page_idx": 338, "bbox": [154, 71.45246887207031, 521, 305.93255615234375], "page_size": [612.0, 792.0]}
{"layout": 3445, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 338, "bbox": [137.4549102783203, 309.9278869628906, 256, 324.46331787109375], "page_size": [612.0, 792.0]}
{"layout": 3446, "type": "text", "text": "class  mmdet.models.dense heads. Embedding RP N Head ( num proposals  $\\mathbf{\\tilde{=}}$  100 ,  proposal feature ch anne  $!{=}256$  , init_cfg  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) ", "page_idx": 338, "bbox": [71.99990844726562, 328.339111328125, 539, 353.7535705566406], "page_size": [612.0, 792.0]}
{"layout": 3447, "type": "text", "text": "RPNHead in the  Sparse R-CNN ", "page_idx": 338, "bbox": [96, 352.3985290527344, 227, 365.70855712890625], "page_size": [612.0, 792.0]}
{"layout": 3448, "type": "text", "text": "Unlike traditional RPNHead, this module does not need FPN input, but just decode  in it proposal b boxes  and expand the first dimension of  in it proposal b boxes  and  in it proposal features  to the batch_size. ", "page_idx": 338, "bbox": [96, 370.1820983886719, 539, 395.5965576171875], "page_size": [612.0, 792.0]}
{"layout": 3449, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 338, "bbox": [117, 402, 169, 413], "page_size": [612.0, 792.0]}
{"layout": 3450, "type": "text", "text": "•  num proposals  ( int ) – Number of in it proposals. Default 100. •  proposal feature channel  ( int ) – Channel number of in it proposal feature. Defaults to 256. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 338, "bbox": [145, 418.1525573730469, 518, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 3451, "type": "text", "text": "forward dummy ( img ,  img_metas ) Dummy forward function. Used in flops calculation. ", "page_idx": 338, "bbox": [96, 483.7561340332031, 238, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 3452, "type": "text", "text": "forward train ( img ,  img_metas ) Forward function in training stage. ", "page_idx": 338, "bbox": [96, 531.5771484375, 256, 556.9916381835938], "page_size": [612.0, 792.0]}
{"layout": 3453, "type": "text", "text": "in it weights()Initialize the in it proposal b boxes as normalized. [c_x, c_y, w, h], and we initialize it to the size of the entire image. ", "page_idx": 338, "bbox": [96, 563.338134765625, 381.2189025878906, 604.8126220703125], "page_size": [612.0, 792.0]}
{"layout": 3454, "type": "text", "text": "simple test ( img ,  img_metas ) Forward function in testing stage. ", "page_idx": 338, "bbox": [96, 609.2850952148438, 252, 634.6995849609375], "page_size": [612.0, 792.0]}
{"layout": 3455, "type": "text", "text": "simple test rp n ( img ,  img_metas ) Forward function in testing stage. ", "page_idx": 338, "bbox": [96, 639.1731567382812, 252, 664.5875854492188], "page_size": [612.0, 792.0]}
{"layout": 3456, "type": "text", "text": "class  mmdet.models.dense heads. FCOSHead ( num classes ,  in channels ,  regress_ranges=((- 1, 64), (64, 128), (128, 256), (256, 512), (512, 100000000.0)) , center sampling  $\\scriptstyle{\\tilde{}}=$  False ,  center sample radius=1.5 , norm on b box  $\\mathbf{\\beta}=$  False ,  center ness on reg  $\\mathbf{\\hat{\\Sigma}}$  False , loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 1.0, 'type':'FocalLoss', 'use s igm oid': True} ,  loss_bbox={'loss weight': 1.0, 'type': 'IoULoss'} ,  loss center ness={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , norm_cfg  $=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) ", "page_idx": 339, "bbox": [72.0, 71.30303192138672, 540, 216.26963806152344], "page_size": [612.0, 792.0]}
{"layout": 3457, "type": "text", "text": "Anchor-free head used in  FCOS . ", "page_idx": 339, "bbox": [96, 214.9146270751953, 227.07762145996094, 228.2246551513672], "page_size": [612.0, 792.0]}
{"layout": 3458, "type": "text", "text": "The FCOS head does not use anchor boxes. Instead bounding boxes are predicted at each pixel and a centerness measure is used to suppress low-quality predictions. Here norm on b box, center ness on reg, dc n on last con v are training tricks used in official repo, which will bring remarkable mAP gains of up to 4.9. Please see  https: //github.com/tian zhi 0549/FCOS  for more detail. ", "page_idx": 339, "bbox": [96, 232.8466339111328, 540, 282.022705078125], "page_size": [612.0, 792.0]}
{"layout": 3459, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 339, "bbox": [117, 288, 169, 299], "page_size": [612.0, 792.0]}
{"layout": 3460, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  strides  ( list[int] | list[tuple[int, int]] ) – Strides of points in multiple fea- ture levels. Default: (4, 8, 16, 32, 64). •  regress ranges  ( tuple[tuple[int, int]] ) – Regress range of multiple level points. •  center sampling  ( bool ) – If true, use center sampling. Default: False. •  center sample radius  ( float ) – Radius of center sampling. Default: 1.5. •  norm on b box  ( bool ) – If true, normalize the regression targets with FPN strides. Default: False. •  center ness on reg  ( bool ) – If true, position centerness on the regress branch. Please re- fer to  https://github.com/tian zhi 0549/FCOS/issues/89#issue comment-516877042 . Default: False. •  conv_bias  ( bool | str ) – If specified as  auto , it will be decided by the norm_cfg. Bias of conv will be set as True if  norm_cfg  is None, otherwise False. Default: “auto”. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  loss center ness  ( dict ) – Config of centerness loss. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $=$  dict(type  $\\risingdotseq$  ’GN’, num_group  $\\wp{=}32$  , requires grad  $\\scriptstyle\\varepsilon=$  True). •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 339, "bbox": [145, 304.5787048339844, 519, 622.7447509765625], "page_size": [612.0, 792.0]}
{"layout": 3461, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 340, "bbox": [96, 72, 139, 86], "page_size": [612.0, 792.0]}
{"layout": 3462, "type": "table", "page_idx": 340, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_153.jpg", "bbox": [92, 96, 544, 151], "page_size": [612.0, 792.0], "ocr_text": ">>> self = FCOSHead(11, 7)\n\n>>> feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]\n>>> cls_score, bbox_pred, centerness = self.forward(feats)\n\n>>> assert len(cls_score) == len(self.scales)\n", "vlm_text": "The table contains a snippet of Python code related to a neural network model. Here's an explanation of the code:\n\n1. `self = FCOSHead(11, 7)`:\n   - This initializes an object `self` using the `FCOSHead` class with parameters `11` and `7`.\n\n2. `feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]`:\n   - This creates a list of tensors named `feats`. The tensors are randomly generated with dimensions `(1, 7, s, s)`, where `s` is a scale from the list `[4, 8, 16, 32, 64]`.\n\n3. `cls_score, bbox_pred, centerness = self.forward(feats)`:\n   - This line calls the `forward` method on the `self` object using `feats` as input and unpacks the output into `cls_score`, `bbox_pred`, and `centerness`.\n\n4. `assert len(cls_score) == len(self.scales)`:\n   - This assertion checks if the length of `cls_score` is equal to the length of `self.scales`. If not, an error will be raised.\n\nThis code is likely used to process multi-scale features through a network head, such as for object detection in a neural network model."}
{"layout": 3463, "type": "text", "text": "center ness target ( pos b box targets ) Compute centerness targets. ", "page_idx": 340, "bbox": [96, 159.2300262451172, 266.08636474609375, 184.64451599121094], "page_size": [612.0, 792.0]}
{"layout": 3464, "type": "text", "text": "Parameters  pos b box targets  ( Tensor ) – BBox targets of positive bboxes in shape (num_pos, 4) ", "page_idx": 340, "bbox": [137, 188.63983154296875, 521, 214.5325164794922], "page_size": [612.0, 792.0]}
{"layout": 3465, "type": "text", "text": "Returns  Centerness target. ", "page_idx": 340, "bbox": [137, 218.52783203125, 247.6307830810547, 233.06326293945312], "page_size": [612.0, 792.0]}
{"layout": 3466, "type": "text", "text": "Return type  Tensor ", "page_idx": 340, "bbox": [137, 236.4598388671875, 220, 250.99526977539062], "page_size": [612.0, 792.0]}
{"layout": 3467, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 340, "bbox": [96, 254.8710174560547, 298.6688537597656, 280.2855224609375], "page_size": [612.0, 792.0]}
{"layout": 3468, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 340, "bbox": [137, 284.2808532714844, 521, 310.17352294921875], "page_size": [612.0, 792.0]}
{"layout": 3469, "type": "text", "text": "Returns  cls_scores (list[Tensor]): Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box energies  $/$   deltas for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\mathrm{~}4$  . center ness es (list[Tensor]): centerness for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\,1$  . ", "page_idx": 340, "bbox": [137, 314.1688537597656, 521, 375.927490234375], "page_size": [612.0, 792.0]}
{"layout": 3470, "type": "text", "text": "forward single ( x ,  scale ,  stride ) Forward features of a single scale level. ", "page_idx": 340, "bbox": [96, 398.3330383300781, 276.1534423828125, 423.74749755859375], "page_size": [612.0, 792.0]}
{"layout": 3471, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 340, "bbox": [136, 429, 188, 441], "page_size": [612.0, 792.0]}
{"layout": 3472, "type": "text", "text": "•  x  ( Tensor ) – FPN feature maps of the specified stride. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. ", "page_idx": 340, "bbox": [154, 446.3034973144531, 521, 477.5455322265625], "page_size": [612.0, 792.0]}
{"layout": 3473, "type": "text", "text": "•  stride  ( int ) – The corresponding stride for feature maps, only used to normalize the bbox prediction when self.norm on b box is True. ", "page_idx": 340, "bbox": [154, 482.16851806640625, 521, 507.43353271484375], "page_size": [612.0, 792.0]}
{"layout": 3474, "type": "text", "text": "Returns  scores for each class, bbox predictions and centerness predictions of input feature maps. ", "page_idx": 340, "bbox": [137, 511.4288330078125, 521, 525.9642333984375], "page_size": [612.0, 792.0]}
{"layout": 3475, "type": "text", "text": "Return type  tuple ", "page_idx": 340, "bbox": [137, 529.3618774414062, 213, 543.8972778320312], "page_size": [612.0, 792.0]}
{"layout": 3476, "type": "text", "text": "get targets ( points ,  gt b boxes list ,  gt labels list ) Compute regression, classification and centerness targets for points in multiple images. ", "page_idx": 340, "bbox": [96, 547.7730712890625, 465.07415771484375, 573.1875610351562], "page_size": [612.0, 792.0]}
{"layout": 3477, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 340, "bbox": [136, 579, 187, 591], "page_size": [612.0, 792.0]}
{"layout": 3478, "type": "text", "text": "•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). Returns  con cat lv l labels (list[Tensor]): Labels of each level. con cat lv l b box targets (list[Tensor]): BBox targets of each level. ", "page_idx": 340, "bbox": [137, 595.7424926757812, 521, 698.716552734375], "page_size": [612.0, 792.0]}
{"layout": 3479, "type": "text", "text": "Return type  tuple ", "page_idx": 340, "bbox": [137, 702.7118530273438, 213, 717.2472534179688], "page_size": [612.0, 792.0]}
{"layout": 3480, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  center ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute loss of the head. ", "page_idx": 341, "bbox": [95, 71.30303192138672, 513, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3481, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 341, "bbox": [136, 102, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 3482, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  center ness es  ( list[Tensor] ) – centerness for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\,1$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 341, "bbox": [154, 119.27247619628906, 521, 311.9105224609375], "page_size": [612.0, 792.0]}
{"layout": 3483, "type": "text", "text": "", "page_idx": 341, "bbox": [136, 322.25, 309, 330], "page_size": [612.0, 792.0]}
{"layout": 3484, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 341, "bbox": [137.4549102783203, 333.8378601074219, 256, 348.373291015625], "page_size": [612.0, 792.0]}
{"layout": 3485, "type": "text", "text": "class  mmdet.models.dense heads. FSAFHead ( \\*args ,  score threshold  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ,  \\*\\*kwargs ) Anchor-free head used in  FSAF . ", "page_idx": 341, "bbox": [71.99990844726562, 352.24908447265625, 513, 377.6635437011719], "page_size": [612.0, 792.0]}
{"layout": 3486, "type": "text", "text": "The head contains two sub networks. The first classifies anchor boxes and the second regresses deltas for the anchors (num anchors is 1 for anchor- free methods) ", "page_idx": 341, "bbox": [95, 382.2865295410156, 540.0034790039062, 407.5515441894531], "page_size": [612.0, 792.0]}
{"layout": 3487, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 341, "bbox": [117, 414, 169, 425], "page_size": [612.0, 792.0]}
{"layout": 3488, "type": "text", "text": "•  \\*args  – Same as its base class in  RetinaHead •  score threshold  ( float, optional ) – The score threshold to calculate positive recall. If given, prediction scores lower than this value is counted as incorrect prediction. Default to None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None •  \\*\\*kwargs  – Same as its base class in  RetinaHead ", "page_idx": 341, "bbox": [145, 430.1075439453125, 518, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 3489, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 341, "bbox": [95, 541, 138, 552], "page_size": [612.0, 792.0]}
{"layout": 3490, "type": "text", "text": " $>>$   import  torch  $>>$   self  $=$   FSAFHead( 11 ,  7 )  $>>$   x  $=$   torch . rand( 1 ,  7 ,  32 ,  32 )  $>>$   cls_score, bbox_pred  $=$   self . forward single(x)  $>>$   # Each anchor predicts a score for each class except background  $>>$   cls per anchor  $=$   cls_score . shape[ 1 ]  /  self . num anchors  $>>$   box per anchor  $=$   bbox_pred . shape[ 1 ]  /  self . num anchors >>>  assert  cls per anchor  $==$   self . num classes >>>  assert  box per anchor  $==~4$  ", "page_idx": 341, "bbox": [95, 568.5640258789062, 447.34136962890625, 675.114990234375], "page_size": [612.0, 792.0]}
{"layout": 3491, "type": "text", "text": "calculate pos recall ( cls_scores ,  labels list ,  pos_inds ) Calculate positive recall with score threshold. ", "page_idx": 341, "bbox": [95, 687.239013671875, 342, 712.6535034179688], "page_size": [612.0, 792.0]}
{"layout": 3492, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 342, "bbox": [136, 72, 188, 85], "page_size": [612.0, 792.0]}
{"layout": 3493, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Classification scores at all fpn levels. Each tensor is in shape (N, num classes \\* num anchors, H, W) •  labels list  ( list[Tensor] ) – The label that each anchor is assigned to. Shape (N \\* H \\* W \\* num anchors, ) •  pos_inds  ( list[Tensor] ) – List of bool tensors indicating whether the anchor is as- signed to a positive label. Shape   $(\\mathrm{N}*\\mathrm{H}*\\mathrm{W}*$   num anchors, ) ", "page_idx": 342, "bbox": [154, 89.38447570800781, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 3494, "type": "text", "text": "Returns  A single float number indicating the positive recall. ", "page_idx": 342, "bbox": [137, 178.42083740234375, 380.8704528808594, 192.95626831054688], "page_size": [612.0, 792.0]}
{"layout": 3495, "type": "text", "text": "Return type  Tensor ", "page_idx": 342, "bbox": [137, 196.35382080078125, 222, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 3496, "type": "text", "text": "collect loss level single ( cls_loss ,  reg_loss ,  assigned gt in ds ,  labels_seq ) Get the average loss in each FPN level w.r.t. each gt label. ", "page_idx": 342, "bbox": [96, 214.76499938964844, 429.4443054199219, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3497, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 342, "bbox": [136, 246, 188, 258], "page_size": [612.0, 792.0]}
{"layout": 3498, "type": "text", "text": "•  cls_loss  ( Tensor ) – Classification loss of each feature map pixel, shape (num_anchor, num_class) •  reg_loss  ( Tensor ) – Regression loss of each feature map pixel, shape (num_anchor, 4) •  assigned gt in ds  ( Tensor ) – It indicates which gt the prior is assigned to (0-based, -1: no assignment). shape (num_anchor), •  labels_seq  – The rank of labels. shape (num_gt) ", "page_idx": 342, "bbox": [154, 262.7344665527344, 521, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 3499, "type": "text", "text": "Returns  (num_gt), average loss of each gt in this level ", "page_idx": 342, "bbox": [137, 357.74884033203125, 356.59161376953125, 372.2842712402344], "page_size": [612.0, 792.0]}
{"layout": 3500, "type": "text", "text": "Return type  shape ", "page_idx": 342, "bbox": [137, 375.68084716796875, 216.00973510742188, 390.2162780761719], "page_size": [612.0, 792.0]}
{"layout": 3501, "type": "text", "text": "forward single  $(x)$  Forward feature map of a single scale level. ", "page_idx": 342, "bbox": [96, 395.96502685546875, 291.92510986328125, 419.50653076171875], "page_size": [612.0, 792.0]}
{"layout": 3502, "type": "text", "text": "Parameters  x  ( Tensor ) – Feature map of a single scale level. ", "page_idx": 342, "bbox": [137, 423.5018615722656, 386.948486328125, 438.03729248046875], "page_size": [612.0, 792.0]}
{"layout": 3503, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 342, "bbox": [136, 443, 173, 455], "page_size": [612.0, 792.0]}
{"layout": 3504, "type": "text", "text": "cls_score (Tensor): Box scores for each scale level  Has shape (N, num_points \\* num classes, H, W). bbox_pred (Tensor): Box energies / deltas for each scale  level with shape (N, num_points \\* 4, H, W). Return type  tuple (Tensor) ", "page_idx": 342, "bbox": [137, 459.3678894042969, 521, 533.6782836914062], "page_size": [612.0, 792.0]}
{"layout": 3505, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute loss of the head. ", "page_idx": 342, "bbox": [96, 537.5540771484375, 458.24127197265625, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 3506, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 342, "bbox": [136, 568, 187, 581], "page_size": [612.0, 792.0]}
{"layout": 3507, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num_points \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num_points \\* 4, H, W). •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. ", "page_idx": 342, "bbox": [154, 585.5244750976562, 521, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 3508, "type": "text", "text": "•  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 343, "bbox": [155, 71.45246887207031, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3509, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 343, "bbox": [137, 100.71282958984375, 308.6417236328125, 115.24826049804688], "page_size": [612.0, 792.0]}
{"layout": 3510, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 343, "bbox": [137, 118.64483642578125, 256, 133.18026733398438], "page_size": [612.0, 792.0]}
{"layout": 3511, "type": "text", "text": "re weight loss single ( cls_loss ,  reg_loss ,  assigned gt in ds ,  labels ,  level ,  min_levels ) Reweight loss values at each level. ", "page_idx": 343, "bbox": [96, 137.05601501464844, 456, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 3512, "type": "text", "text": "Reassign loss values at each level by masking those where the pre-calculated loss is too large. Then return the reduced losses. ", "page_idx": 343, "bbox": [118, 167.0934600830078, 540, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 3513, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 343, "bbox": [136, 198, 187, 210], "page_size": [612.0, 792.0]}
{"layout": 3514, "type": "text", "text": "•  cls_loss  ( Tensor ) – Element-wise classification loss. Shape: (num anchors, num classes) •  reg_loss  ( Tensor ) – Element-wise regression loss. Shape: (num anchors, 4) •  assigned gt in ds  ( Tensor ) – The gt indices that each anchor bbox is assigned to. -1 denotes a negative anchor, otherwise it is the gt index (0-based). Shape: (num anchors, ), •  labels  ( Tensor ) – Label assigned to anchors. Shape: (num anchors, ). •  level  ( int ) – The current level index in the pyramid (0-4 for RetinaNet) •  min_levels  ( Tensor ) – The best-matching level for each gt. Shape: (num_gts, ), ", "page_idx": 343, "bbox": [155, 214.91444396972656, 521, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 3515, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 343, "bbox": [136, 348, 173, 359], "page_size": [612.0, 792.0]}
{"layout": 3516, "type": "text", "text": "• cls_loss: Reduced corrected classification loss. Scalar. • reg_loss: Reduced corrected regression loss. Scalar. • pos_flags (Tensor): Corrected bool tensor indicating the final positive anchors. Shape: (num anchors, ). ", "page_idx": 343, "bbox": [155, 364.353515625, 521, 425.48455810546875], "page_size": [612.0, 792.0]}
{"layout": 3517, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 343, "bbox": [136, 432, 214, 444], "page_size": [612.0, 792.0]}
{"layout": 3518, "type": "text", "text": "class  mmdet.models.dense heads. Feature Adaption ( in channels ,  out channels ,  kernel size  ${\\scriptstyle\\cdot=}3$  , deform groups  $\\scriptstyle{\\prime}=4$  ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'name': 'con v adaption', 'std': 0.01, 'type': 'Normal'}, 'std': 0.1, 'type': 'Normal'} ) ", "page_idx": 343, "bbox": [71.99990844726562, 447.8901062011719, 535.8193359375, 497.10595703125], "page_size": [612.0, 792.0]}
{"layout": 3519, "type": "text", "text": "Feature Adaption Module. ", "page_idx": 343, "bbox": [96, 495.8605041503906, 201.83277893066406, 509.1705322265625], "page_size": [612.0, 792.0]}
{"layout": 3520, "type": "text", "text": "Feature Adaption Module is implemented based on DCN v1. It uses anchor shape prediction rather than feature map to predict offsets of deform conv layer. ", "page_idx": 343, "bbox": [96, 513.7935180664062, 540, 539.0585327148438], "page_size": [612.0, 792.0]}
{"layout": 3521, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 343, "bbox": [117, 545, 168, 556], "page_size": [612.0, 792.0]}
{"layout": 3522, "type": "text", "text": "•  in channels  ( int ) – Number of channels in the input feature map. •  out channels  ( int ) – Number of channels in the output feature map. •  kernel size  ( int ) – Deformable conv kernel size. •  deform groups  ( int ) – Deformable conv group size. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 343, "bbox": [145, 561.614501953125, 462.2249755859375, 646.655517578125], "page_size": [612.0, 792.0]}
{"layout": 3523, "type": "text", "text": "forward ( x ,  shape ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 343, "bbox": [96, 651.1280517578125, 313.4839782714844, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 3524, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 344, "bbox": [118, 82.77984619140625, 540, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 3525, "type": "text", "text": "class  mmdet.models.dense heads. FoveaHead ( num classes ,  in channels ,  base edge list  $=$  (16, 32, 64, 128, 256) ,  scale_ranges=((8, 32), (16, 64), (32, 128), (64, 256), (128, 512)) ,  sigma=0.4 ,  with deform  $\\leftrightharpoons$  False , deform group  $\\scriptstyle{\\mathfrak{s}}=4$  ,  init_cfg  $\\scriptstyle{\\tilde{}}=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) ", "page_idx": 344, "bbox": [72.0, 137.05601501464844, 525.6073608398438, 210.2915496826172], "page_size": [612.0, 792.0]}
{"layout": 3526, "type": "text", "text": "FoveaBox: Beyond Anchor-based Object Detector  https://arxiv.org/abs/1904.03797 ", "page_idx": 344, "bbox": [96, 208.93653869628906, 429.5367126464844, 222.24656677246094], "page_size": [612.0, 792.0]}
{"layout": 3527, "type": "text", "text": "forward single  $(x)$  Forward features of a single scale level. ", "page_idx": 344, "bbox": [96, 228, 276.153076171875, 252.1345672607422], "page_size": [612.0, 792.0]}
{"layout": 3528, "type": "text", "text": "Parameters  x  ( Tensor ) – FPN feature maps of the specified stride. ", "page_idx": 344, "bbox": [137, 256.1298828125, 409.5931701660156, 270.6653137207031], "page_size": [612.0, 792.0]}
{"layout": 3529, "type": "text", "text": "Returns ", "page_idx": 344, "bbox": [137, 274.0628967285156, 171.46597290039062, 288.59832763671875], "page_size": [612.0, 792.0]}
{"layout": 3530, "type": "text", "text": "Scores for each class, bbox predictions, features  after classification and regression conv layers, some models needs these features like FCOS. ", "page_idx": 344, "bbox": [154, 291.9949035644531, 521, 317.8875732421875], "page_size": [612.0, 792.0]}
{"layout": 3531, "type": "text", "text": "Return type  tuple ", "page_idx": 344, "bbox": [137, 321.8829040527344, 213, 336.4183349609375], "page_size": [612.0, 792.0]}
{"layout": 3532, "type": "text", "text": "get targets ( gt b box list ,  gt label list ,  feat map sizes ,  points ) Compute regression, classification and centerness targets for points in multiple images. ", "page_idx": 344, "bbox": [96, 340.29412841796875, 465, 365.7085876464844], "page_size": [612.0, 792.0]}
{"layout": 3533, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 344, "bbox": [136, 372, 187, 383], "page_size": [612.0, 792.0]}
{"layout": 3534, "type": "text", "text": "•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). ", "page_idx": 344, "bbox": [154, 388.26458740234375, 521, 461.349609375], "page_size": [612.0, 792.0]}
{"layout": 3535, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt b box list ,  gt label list ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute loss of the head. ", "page_idx": 344, "bbox": [96, 465.8231506347656, 480.198974609375, 491.23760986328125], "page_size": [612.0, 792.0]}
{"layout": 3536, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 344, "bbox": [136, 498, 187, 509], "page_size": [612.0, 792.0]}
{"layout": 3537, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 344, "bbox": [154, 513.7935791015625, 521, 676.5436401367188], "page_size": [612.0, 792.0]}
{"layout": 3538, "type": "text", "text": "class  mmdet.models.dense heads. Free Anchor Retina Head ( num classes ,  in channels ,  stacked con v  $\\mathfrak{z}{=}4$  , ", "page_idx": 345, "bbox": [72.0, 71.30303192138672, 523, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3539, "type": "text", "text": "conv_cfg  $=$  None ,  norm_cfg  $\\mathbf{\\hat{\\Sigma}}$  None , pre anchor top  $k{=}50$  ,  bbox_th  $\\it{r=0.6}$  , gamma  $\\it{:=2.0}$  ,  alpha  $\\mathord{=}\\!O.5$  ,  \\*\\*kwargs ) ", "page_idx": 345, "bbox": [343, 83.25804901123047, 490.82867431640625, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 3540, "type": "text", "text": "FreeAnchor RetinaHead used in  https://arxiv.org/abs/1909.02466 . ", "page_idx": 345, "bbox": [96, 119.27253723144531, 360.0191650390625, 132.5825653076172], "page_size": [612.0, 792.0]}
{"layout": 3541, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 345, "bbox": [117, 138, 170, 150], "page_size": [612.0, 792.0]}
{"layout": 3542, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of conv layers in cls and reg tower. Default: 4. •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. Default: None. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $=$  dict(type  $\\risingdotseq$  ’GN’, num_group  $\\wp{=}32$  , requires grad  $\\risingdotseq$  True). •  pre anchor top k  ( int ) – Number of boxes that be token in each bag. •  bbox_thr  ( float ) – The threshold of the saturated linear function. It is usually the same with the IoU threshold used in NMS. •  gamma  ( float ) – Gamma parameter in focal loss. •  alpha  ( float ) – Alpha parameter in focal loss. ", "page_idx": 345, "bbox": [145, 155.1385040283203, 518, 335.8205261230469], "page_size": [612.0, 792.0]}
{"layout": 3543, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 345, "bbox": [96, 340.2940673828125, 458.2413024902344, 365.7085266113281], "page_size": [612.0, 792.0]}
{"layout": 3544, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 345, "bbox": [136, 371, 188, 383], "page_size": [612.0, 792.0]}
{"layout": 3545, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components.  dict[str, Tensor] ", "page_idx": 345, "bbox": [137.4548797607422, 388.2645263671875, 523, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 3546, "type": "text", "text": "negative bag loss ( cls_prob ,  box_prob ) Compute negative bag loss.  $F L((1-P_{a_{j}\\in A_{+}})*(1-P_{j}^{b g})).$   $P_{a_{j}\\in A_{+}}$  : Box probability of matched samples.  $P_{j}^{b g}$  : Classification probability of negative samples. ", "page_idx": 345, "bbox": [96, 591.3521118164062, 324.8592224121094, 675], "page_size": [612.0, 792.0]}
{"layout": 3547, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 345, "bbox": [136, 679, 188, 690], "page_size": [612.0, 792.0]}
{"layout": 3548, "type": "text", "text": "•  cls_prob  ( Tensor ) – Classification probability, in shape (num_img, num anchors, num classes). ", "page_idx": 345, "bbox": [154, 695.25146484375, 523, 720.5165405273438], "page_size": [612.0, 792.0]}
{"layout": 3549, "type": "text", "text": "•  box_prob  ( Tensor ) – Box probability, in shape (num_img, num anchors, num classes). ", "page_idx": 346, "bbox": [155, 71.45246887207031, 521, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3550, "type": "text", "text": "Returns  Negative bag loss in shape (num_img, num anchors, num classes). ", "page_idx": 346, "bbox": [137, 88.7568359375, 445.6673583984375, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 3551, "type": "text", "text": "Return type  Tensor ", "page_idx": 346, "bbox": [137, 106.6898193359375, 220, 121.22525024414062], "page_size": [612.0, 792.0]}
{"layout": 3552, "type": "text", "text": "positive bag loss(matched cls pro b, matched box pro b)Compute positive bag loss. ", "page_idx": 346, "bbox": [96, 125.10100555419922, 350.3953552246094, 150.51548767089844], "page_size": [612.0, 792.0]}
{"layout": 3553, "type": "equation", "text": "\n$$\n-l o g(M e a n-m a x(P_{i j}^{c l s}*P_{i j}^{l o c})).\n$$\n ", "text_format": "latex", "page_idx": 346, "bbox": [117, 154, 262, 170], "page_size": [612.0, 792.0]}
{"layout": 3554, "type": "text", "text": " $P_{i j}^{c l s}$  : matched cls pro b, classification probability of matched samples.  $P_{i j}^{l o c}$  : matched box pro b, box probability of matched samples. ", "page_idx": 346, "bbox": [118, 174.51930236816406, 403.1537170410156, 189], "page_size": [612.0, 792.0]}
{"layout": 3555, "type": "text", "text": "", "page_idx": 346, "bbox": [118, 193.90126037597656, 369.4229431152344, 209], "page_size": [612.0, 792.0]}
{"layout": 3556, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 346, "bbox": [136, 214, 188, 225], "page_size": [612.0, 792.0]}
{"layout": 3557, "type": "text", "text": "•  matched cls pro b  ( Tensor ) – Classification probability of matched samples in shape (num_gt, pre anchor top k). •  matched box pro b  ( Tensor ) – BBox probability of matched samples, in shape (num_gt, pre anchor top k). ", "page_idx": 346, "bbox": [155, 229.76625061035156, 521, 284.9192810058594], "page_size": [612.0, 792.0]}
{"layout": 3558, "type": "text", "text": "Returns  Positive bag loss in shape (num_gt,). ", "page_idx": 346, "bbox": [137, 288.91461181640625, 322.79852294921875, 303.4500427246094], "page_size": [612.0, 792.0]}
{"layout": 3559, "type": "text", "text": "Return type  Tensor ", "page_idx": 346, "bbox": [137, 306.8476257324219, 220, 321.383056640625], "page_size": [612.0, 792.0]}
{"layout": 3560, "type": "text", "text": "class  mmdet.models.dense heads. GARPNHead ( in channels ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_loc', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) ", "page_idx": 346, "bbox": [71, 325.2578430175781, 510.6242980957031, 362.6282958984375], "page_size": [612.0, 792.0]}
{"layout": 3561, "type": "text", "text": "Guided-Anchor-based RPN head. ", "page_idx": 346, "bbox": [96, 361.27325439453125, 230, 374.5832824707031], "page_size": [612.0, 792.0]}
{"layout": 3562, "type": "text", "text": "forward single  $(x)$  Forward feature of a single scale level. ", "page_idx": 346, "bbox": [96, 380.9297790527344, 272.2786560058594, 404.4712829589844], "page_size": [612.0, 792.0]}
{"layout": 3563, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  shape p reds ,  loc_preds ,  gt_bboxes ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 346, "bbox": [96, 408.94482421875, 515.7459716796875, 434.3592834472656], "page_size": [612.0, 792.0]}
{"layout": 3564, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 346, "bbox": [136, 440, 187, 452], "page_size": [612.0, 792.0]}
{"layout": 3565, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None ", "page_idx": 346, "bbox": [155, 456.9142761230469, 521, 619.664306640625], "page_size": [612.0, 792.0]}
{"layout": 3566, "type": "text", "text": "", "page_idx": 346, "bbox": [136, 629.25, 309, 637], "page_size": [612.0, 792.0]}
{"layout": 3567, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 346, "bbox": [137, 641.5916137695312, 256.1188659667969, 656.1270141601562], "page_size": [612.0, 792.0]}
{"layout": 3568, "type": "text", "text": "class  mmdet.models.dense heads. GA Retina Head ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{:=4}$  , conv_cfg  $\\leftrightharpoons$  None ,  norm_cfg  $\\mathbf{\\beta}=$  None ,  init_cfg  $=$  None , \\*\\*kwargs ) ", "page_idx": 346, "bbox": [71, 660.0028076171875, 501.8671569824219, 697.2626953125], "page_size": [612.0, 792.0]}
{"layout": 3569, "type": "text", "text": "Guided-Anchor-based RetinaNet head. ", "page_idx": 346, "bbox": [96, 696.0182495117188, 251.11756896972656, 709.3283081054688], "page_size": [612.0, 792.0]}
{"layout": 3570, "type": "text", "text": "forward single  $(x)$  Forward feature map of a single scale level. ", "page_idx": 347, "bbox": [96, 73, 291.9241943359375, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3571, "type": "text", "text": "class  mmdet.models.dense heads. GFLHead ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  ,  conv_cfg=None , norm_cfg  $=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  loss_dfl={'loss weight': 0.25, 'type': 'Distribution Focal Loss'} ,  bbox_coder={'type': 'Distance Point B Box Code r'} ,  reg_max=16 ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'gfl_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) ", "page_idx": 347, "bbox": [71.99998474121094, 101.19103240966797, 528.979736328125, 186.38157653808594], "page_size": [612.0, 792.0]}
{"layout": 3572, "type": "text", "text": "Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection. ", "page_idx": 347, "bbox": [96, 185.0265655517578, 524, 198.3365936279297], "page_size": [612.0, 792.0]}
{"layout": 3573, "type": "text", "text": "GFL head structure is similar with ATSS, however GFL uses 1) joint representation for classification and local- ization quality, and 2) flexible General distribution for bounding box locations, which are supervised by Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), respectively ", "page_idx": 347, "bbox": [96, 202.9595489501953, 540, 240.1796112060547], "page_size": [612.0, 792.0]}
{"layout": 3574, "type": "text", "text": "https://arxiv.org/abs/2006.04388 ", "page_idx": 347, "bbox": [96, 244.8025665283203, 227.33668518066406, 258.11260986328125], "page_size": [612.0, 792.0]}
{"layout": 3575, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 347, "bbox": [117, 265, 169, 275], "page_size": [612.0, 792.0]}
{"layout": 3576, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of conv layers in cls and reg tower. Default: 4. •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. Default: None. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: dict(type=’GN’, num_groups  $_{:=32}$  , requires grad  $\\scriptstyle\\varepsilon=$  True). •  loss_qfl  ( dict ) – Config of Quality Focal Loss (QFL). •  bbox_coder  ( dict ) – Config of bbox coder. Defaults ‘Distance Point B Box Code r’. •  reg_max  ( int ) – Max value of integral set :math:  {0, ..., reg_max}  in QFL setting. Default: 16. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 347, "bbox": [145, 280.6676025390625, 524, 461.3507080078125], "page_size": [612.0, 792.0]}
{"layout": 3577, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 347, "bbox": [95, 481, 139, 493], "page_size": [612.0, 792.0]}
{"layout": 3578, "type": "image", "page_idx": 347, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_154.jpg", "bbox": [92, 503, 544, 559], "page_size": [612.0, 792.0], "ocr_text": ">>> self = GFLHead(11, 7)\n\n>>> feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]\n>>> cls_quality_score, bbox_pred = self. forward(feats)\n\n>>> assert len(cls_quality_score) == len(self.scales)\n", "vlm_text": "This image shows a snippet of Python code using the PyTorch library. Here’s a breakdown:\n\n1. `self = GFLHead(11, 7)`: An instance of `GFLHead` is being created with parameters 11 and 7.\n2. `feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]`: A list comprehension creating random tensors of shape `(1, 7, s, s)` for each `s` in the list `[4, 8, 16, 32, 64]`.\n3. `cls_quality_score, bbox_pred = self.forward(feats)`: The method `forward` is called on `self` with `feats` as input, and it returns two values: `cls_quality_score` and `bbox_pred`.\n4. `assert len(cls_quality_score) == len(self.scales)`: An assertion checks if the length of `cls_quality_score` is equal to the length of `self.scales`. \n\nThis code is likely a part of a testing script or a model simulation involving object detection or feature map processing in a neural network context."}
{"layout": 3579, "type": "text", "text": "Get anchor centers from anchors. Parameters  anchors  ( Tensor ) – Anchor list with shape (N, 4), “xyxy” format. Returns  Anchor centers with shape (N, 2), “xy” format. Return type  Tensor ", "page_idx": 347, "bbox": [118, 579.7924194335938, 462, 647.4982299804688], "page_size": [612.0, 792.0]}
{"layout": 3580, "type": "text", "text": "forward ( feats ) ", "text_level": 1, "page_idx": 347, "bbox": [96, 654, 163, 663], "page_size": [612.0, 792.0]}
{"layout": 3581, "type": "text", "text": "Forward features from the upstream network. Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 347, "bbox": [118, 663.4784545898438, 524, 706.676513671875], "page_size": [612.0, 792.0]}
{"layout": 3582, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 348, "bbox": [137, 72, 173, 84], "page_size": [612.0, 792.0]}
{"layout": 3583, "type": "text", "text": "Usually a tuple of classification scores and bbox prediction ", "page_idx": 348, "bbox": [154, 88.7568359375, 401.6727294921875, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 3584, "type": "text", "text": "cls_scores (list[Tensor]): Classification and quality (IoU)  joint scores for all scale lev- els, each is a 4D-tensor, the channel number is num classes. bbox_preds (list[Tensor]): Box distribution logits for all  scale levels, each is a 4D- tensor, the channel number is  $4^{*}(\\mathsf{n}\\!+\\!1)$  , n is max value of integral set. ", "page_idx": 348, "bbox": [163, 106.6898193359375, 521, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 3585, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 348, "bbox": [137, 168, 214, 180], "page_size": [612.0, 792.0]}
{"layout": 3586, "type": "text", "text": "forward single ( x ,  scale ) Forward feature of a single scale level. ", "page_idx": 348, "bbox": [96, 184.8769989013672, 272.2779541015625, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 3587, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 348, "bbox": [136, 216, 188, 228], "page_size": [612.0, 792.0]}
{"layout": 3588, "type": "text", "text": "•  x  ( Tensor ) – Features of a single scale level. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. ", "page_idx": 348, "bbox": [154, 232.84645080566406, 521, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 3589, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 348, "bbox": [137, 270, 172, 282], "page_size": [612.0, 792.0]}
{"layout": 3590, "type": "text", "text": "cls_score (Tensor): Cls and quality joint scores for a single  scale level the channel num- ber is num classes. bbox_pred (Tensor): Box distribution logits for a single scale  level, the channel number is  $4^{*}(\\mathsf{n}\\!+\\!1)$  , n is max value of integral set. ", "page_idx": 348, "bbox": [154, 286.0177917480469, 521, 341.7984619140625], "page_size": [612.0, 792.0]}
{"layout": 3591, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 348, "bbox": [137, 347, 215, 360], "page_size": [612.0, 792.0]}
{"layout": 3592, "type": "text", "text": "get targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $t{=}$  None , gt labels list  $=$  None ,  label channels  $\\scriptstyle{\\prime=}I$  ,  un map outputs  $=$  True ) Get targets for GFL head. ", "page_idx": 348, "bbox": [96, 364.2040100097656, 499.29754638671875, 401.574462890625], "page_size": [612.0, 792.0]}
{"layout": 3593, "type": "text", "text": "This method is almost the same as  AnchorHead.get targets() . Besides returning the targets as the parent method does, it also returns the anchors as the first element of the returned tuple. ", "page_idx": 348, "bbox": [118, 406.0469970703125, 540, 431.46246337890625], "page_size": [612.0, 792.0]}
{"layout": 3594, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 348, "bbox": [96, 435.93499755859375, 458.2412109375, 461.3494567871094], "page_size": [612.0, 792.0]}
{"layout": 3595, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 348, "bbox": [136, 467, 187, 479], "page_size": [612.0, 792.0]}
{"layout": 3596, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Cls and quality scores for each scale level has shape (N, num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box distribution logits for each scale level with shape (N,  $4^{*}(\\mathsf{n}\\!+\\!1)$  , H, W), n is max value of integral set. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 348, "bbox": [154, 483.90545654296875, 521, 646.6554565429688], "page_size": [612.0, 792.0]}
{"layout": 3597, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 348, "bbox": [137, 650.6497802734375, 308.64154052734375, 665.1851806640625], "page_size": [612.0, 792.0]}
{"layout": 3598, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 348, "bbox": [137, 668.5828247070312, 256.11907958984375, 683.1182250976562], "page_size": [612.0, 792.0]}
{"layout": 3599, "type": "text", "text": "loss single ( anchors ,  cls_score ,  bbox_pred ,  labels ,  label weights ,  b box targets ,  stride ,  num total samples ) Compute loss of a single scale level. ", "page_idx": 348, "bbox": [96, 686.9940185546875, 540, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 3600, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 349, "bbox": [137, 72, 188, 84], "page_size": [612.0, 792.0]}
{"layout": 3601, "type": "text", "text": "•  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  cls_score  ( Tensor ) – Cls and quality joint scores for each scale level has shape (N, num classes, H, W). •  bbox_pred  ( Tensor ) – Box distribution logits for each scale level with shape (N  ${\\bf\\Phi},4{\\bf\\Phi}^{*}({\\bf n}{+}1)$  , H, W), n is max value of integral set. •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  stride  ( tuple ) – Stride in this scale level. •  num total samples  ( int ) – Number of positive samples that is reduced over all GPUs. ", "page_idx": 349, "bbox": [155, 89.38447570800781, 523, 288.0005187988281], "page_size": [612.0, 792.0]}
{"layout": 3602, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 349, "bbox": [137, 291.9948425292969, 308.6417541503906, 306.5302734375], "page_size": [612.0, 792.0]}
{"layout": 3603, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 349, "bbox": [137, 309.9278564453125, 256, 324.4632873535156], "page_size": [612.0, 792.0]}
{"layout": 3604, "type": "text", "text": "", "page_idx": 349, "bbox": [70, 329, 514, 336.75], "page_size": [612.0, 792.0]}
{"layout": 3605, "type": "text", "text": "approx anchor generator={'octave base scale': 8, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [4, 8, 16, 32, 64], 'type': 'AnchorGenerator'} , square anchor generator={'ratios': [1.0], 'scales': [8], 'strides': [4, 8, 16, 32, 64], 'type': 'Anchor Generator'} ,  anchor code r={'target means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} , bbox_coder={'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box=False , deform groups=4 ,  loc filter thr=0.01 , train_cfg=None ,  test_cfg=None ,  loss_loc={'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True}, loss_shape={'beta':0.2, 'loss weight': 1.0, 'type': 'Bounded I oU Loss'} , loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_bbox={'beta': 1.0, 'loss weight': 1.0, 'type':'Smooth L 1 Loss'} ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_loc', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ) ", "page_idx": 349, "bbox": [323, 340.2940673828125, 540, 616.6577758789062], "page_size": [612.0, 792.0]}
{"layout": 3606, "type": "text", "text": "Guided-Anchor-based head (GA-RPN, GA-RetinaNet, etc.). ", "page_idx": 349, "bbox": [96, 615.4122924804688, 338, 628.7223510742188], "page_size": [612.0, 792.0]}
{"layout": 3607, "type": "text", "text": "This Guided Anchor Head will predict high-quality feature guided anchors and locations where anchors will be kept in inference. There are mainly 3 categories of bounding-boxes. ", "page_idx": 349, "bbox": [96, 633.3452758789062, 540, 658.6103515625], "page_size": [612.0, 792.0]}
{"layout": 3608, "type": "text", "text": "• Sampled 9 pairs for target assignment. (approxes) • The square boxes where the predicted anchors are based on. (squares) • Guided anchors. ", "page_idx": 349, "bbox": [110, 663.2333374023438, 396.6808166503906, 712.4083862304688], "page_size": [612.0, 792.0]}
{"layout": 3609, "type": "text", "text": "Please refer to  https://arxiv.org/abs/1901.03278  for more details. ", "page_idx": 350, "bbox": [96, 71.45246887207031, 354, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3610, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 350, "bbox": [117, 91, 169, 102], "page_size": [612.0, 792.0]}
{"layout": 3611, "type": "text", "text": "•  num classes  ( int ) – Number of classes. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. •  approx anchor generator  ( dict ) – Config dict for approx generator •  square anchor generator  ( dict ) – Config dict for square generator •  anchor code r  ( dict ) – Config dict for anchor coder •  bbox_coder  ( dict ) – Config dict for bbox coder •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  deform groups  – (int): Group number of DCN in Feature Adaption module. •  loc filter thr  ( float ) – Threshold to filter out unconcerned regions. •  loss_loc  ( dict ) – Config of location loss. •  loss_shape  ( dict ) – Config of anchor shape loss. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of bbox regression loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 350, "bbox": [145, 107.31745910644531, 518, 407.5515441894531], "page_size": [612.0, 792.0]}
{"layout": 3612, "type": "text", "text": "forward ( feats ) ", "page_idx": 350, "bbox": [96, 412.02508544921875, 162.56532287597656, 425.3749694824219], "page_size": [612.0, 792.0]}
{"layout": 3613, "type": "text", "text": "Forward features from the upstream network. Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 350, "bbox": [118, 424.1295166015625, 521, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 3614, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 350, "bbox": [136, 474, 173, 485], "page_size": [612.0, 792.0]}
{"layout": 3615, "type": "text", "text": "A tuple of classification scores and bbox prediction. • cls_scores (list[Tensor]): Classification scores for all scale levels, each is a 4D-tensor, the channels number is num base priors \\* num classes. • bbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-tensor, the channels number is num base priors  $^{*}\\,4$  . Return type  tuple ", "page_idx": 350, "bbox": [137, 489.883544921875, 521, 581.499267578125], "page_size": [612.0, 792.0]}
{"layout": 3616, "type": "text", "text": "forward single  $(x)$  ", "text_level": 1, "page_idx": 350, "bbox": [96, 588, 186, 598], "page_size": [612.0, 792.0]}
{"layout": 3617, "type": "text", "text": "Forward feature of a single scale level. ", "page_idx": 350, "bbox": [118, 597.4794921875, 272.2789001464844, 610.78955078125], "page_size": [612.0, 792.0]}
{"layout": 3618, "type": "text", "text": "Parameters  x  ( Tensor ) – Features of a single scale level. Returns  cls_score (Tensor): Cls scores for a single scale level the channels number is num base priors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num base priors  $^{*}\\,4$  . Return type  tuple ", "page_idx": 350, "bbox": [137, 614.784912109375, 521, 689.0963134765625], "page_size": [612.0, 792.0]}
{"layout": 3619, "type": "text", "text": "ga loc targets ( gt b boxes list ,  feat map sizes ) Compute location targets for guided anchoring. ", "page_idx": 351, "bbox": [96, 71.30303192138672, 306.7882995605469, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3620, "type": "text", "text": "Each feature map is divided into positive, negative and ignore regions. - positive regions: target 1, weight 1 - ignore regions: target 0, weight 0 - negative regions: target 0, weight 0.1 ", "page_idx": 351, "bbox": [118, 101.34046936035156, 540, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 3621, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 351, "bbox": [136, 132, 188, 144], "page_size": [612.0, 792.0]}
{"layout": 3622, "type": "text", "text": "•  gt b boxes list  ( list[Tensor] ) – Gt bboxes of each image. •  feat map sizes  ( list[tuple] ) – Multi level sizes of each feature maps. ", "page_idx": 351, "bbox": [154, 149.1604766845703, 463, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 3623, "type": "text", "text": "Returns  tuple ", "page_idx": 351, "bbox": [137, 184.3988037109375, 196, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 3624, "type": "text", "text": "ga shape targets ( approx list ,  inside flag list ,  square list ,  gt b boxes list ,  img_metas , gt b boxes ignore list $=$ None, un map output $\\mathfrak{s}=$ True)Compute guided anchoring targets. ", "page_idx": 351, "bbox": [96, 202.8099822998047, 457.7436218261719, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3625, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 351, "bbox": [136, 246, 188, 257], "page_size": [612.0, 792.0]}
{"layout": 3626, "type": "text", "text": "•  approx list  ( list[list] ) – Multi level approxs of each image. •  inside flag list  ( list[list] ) – Multi level inside flags of each image. •  square list  ( list[list] ) – Multi level squares of each image. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – ignore list of gt bboxes. •  un map outputs  ( bool ) – unmap outputs or not. ", "page_idx": 351, "bbox": [154, 262.7344665527344, 471.9844970703125, 383.6415710449219], "page_size": [612.0, 792.0]}
{"layout": 3627, "type": "text", "text": "Returns  tuple ", "page_idx": 351, "bbox": [137, 387.63690185546875, 196, 402.1723327636719], "page_size": [612.0, 792.0]}
{"layout": 3628, "type": "text", "text": "get anchors ( feat map sizes ,  shape p reds ,  loc_preds ,  img_metas ,  use loc filter=False ,  device  $=$  'cuda' ) Get squares according to feature map sizes and guided anchors. ", "page_idx": 351, "bbox": [96, 406.047119140625, 516, 431.46258544921875], "page_size": [612.0, 792.0]}
{"layout": 3629, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 351, "bbox": [136, 437, 188, 449], "page_size": [612.0, 792.0]}
{"layout": 3630, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  shape p reds  ( list[tensor] ) – Multi-level shape predictions. •  loc_preds  ( list[tensor] ) – Multi-level location predictions. •  img_metas  ( list[dict] ) – Image meta info. •  use loc filter  ( bool ) – Use loc filter or not. •  device  ( torch.device | str ) – device for returned tensors ", "page_idx": 351, "bbox": [154, 454.017578125, 431, 556.9916381835938], "page_size": [612.0, 792.0]}
{"layout": 3631, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 351, "bbox": [136, 563, 172, 574], "page_size": [612.0, 792.0]}
{"layout": 3632, "type": "text", "text": "square approxs of each image, guided anchors of each image,  loc masks of each image Return type  tuple ", "page_idx": 351, "bbox": [137, 578.9189453125, 516, 611.3873291015625], "page_size": [612.0, 792.0]}
{"layout": 3633, "type": "text", "text": "get_bboxes ( cls_scores ,  bbox_preds ,  shape p reds ,  loc_preds ,  img_metas ,  cfg  $\\mathbf{\\beta}=$  None ,  rescale  $=$  False ) Transform network outputs of a batch into bbox results. ", "page_idx": 351, "bbox": [96, 615.2631225585938, 504.9962158203125, 640.6776123046875], "page_size": [612.0, 792.0]}
{"layout": 3634, "type": "text", "text": "Note: When score factors is not None, the cls_scores are usually multiplied by it then obtain the real score used in NMS, such as CenterNess in FCOS, IoU branch in ATSS. ", "page_idx": 351, "bbox": [118, 645.3005981445312, 540, 670.5656127929688], "page_size": [612.0, 792.0]}
{"layout": 3635, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 351, "bbox": [136, 676, 188, 688], "page_size": [612.0, 792.0]}
{"layout": 3636, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). ", "page_idx": 351, "bbox": [154, 693.1205444335938, 521.368408203125, 718.3866577148438], "page_size": [612.0, 792.0]}
{"layout": 3637, "type": "text", "text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  score factors  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, num_priors \\* 1, H, W). Default None. •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. ", "page_idx": 352, "bbox": [154, 71.45246887207031, 521, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 3638, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 352, "bbox": [137, 216, 173, 228], "page_size": [612.0, 792.0]}
{"layout": 3639, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. ", "page_idx": 352, "bbox": [154, 232.21881103515625, 521, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 3640, "type": "text", "text": "Return type  list[list[Tensor, Tensor]] ", "page_idx": 352, "bbox": [137, 286.0178527832031, 288, 300.55328369140625], "page_size": [612.0, 792.0]}
{"layout": 3641, "type": "text", "text": "get sampled approx s ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get sampled approxs and inside flags according to feature map sizes. ", "page_idx": 352, "bbox": [96, 304.4280700683594, 392.8751220703125, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 3642, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 352, "bbox": [136, 336, 188, 347], "page_size": [612.0, 792.0]}
{"layout": 3643, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – device for returned tensors ", "page_idx": 352, "bbox": [154, 352.3985290527344, 427.9339599609375, 401.5745849609375], "page_size": [612.0, 792.0]}
{"layout": 3644, "type": "text", "text": "Returns  approxes of each image, inside flags of each image ", "page_idx": 352, "bbox": [137, 405.56890869140625, 377, 420.1043395996094], "page_size": [612.0, 792.0]}
{"layout": 3645, "type": "text", "text": "Return type  tuple ", "page_idx": 352, "bbox": [137, 423.5019226074219, 213.25006103515625, 438.037353515625], "page_size": [612.0, 792.0]}
{"layout": 3646, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  shape p reds ,  loc_preds ,  gt_bboxes ,  gt_labels ,  img_metas , gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute losses of the head. ", "page_idx": 352, "bbox": [96, 441.91314697265625, 452, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 3647, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 352, "bbox": [136, 485, 187, 497], "page_size": [612.0, 792.0]}
{"layout": 3648, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None ", "page_idx": 352, "bbox": [154, 501.8385925292969, 521, 664.5875854492188], "page_size": [612.0, 792.0]}
{"layout": 3649, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 352, "bbox": [137, 668.5828857421875, 308, 683.1182861328125], "page_size": [612.0, 792.0]}
{"layout": 3650, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 352, "bbox": [137, 686.5159301757812, 256.11907958984375, 701.0513305664062], "page_size": [612.0, 792.0]}
{"layout": 3651, "type": "text", "text": "class  mmdet.models.dense heads. LDHead ( num classes ,  in channels ,  loss_ld={'T': 10, 'loss_weight': 0.25, 'type': 'Localization Distillation Loss'} ,  \\*\\*kwargs ) ", "page_idx": 353, "bbox": [72.0, 71.30303192138672, 526.0162353515625, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3652, "type": "text", "text": "Localization distillation Head. (Short description) ", "page_idx": 353, "bbox": [96, 95.36250305175781, 298, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 3653, "type": "text", "text": "It utilizes the learned bbox distributions to transfer the localization dark knowledge from teacher to student. Original paper:  Localization Distillation for Object Detection. ", "page_idx": 353, "bbox": [96, 113.29548645019531, 540.0032958984375, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 3654, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 353, "bbox": [117, 144, 169, 156], "page_size": [612.0, 792.0]}
{"layout": 3655, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  loss_ld  ( dict ) – Config of Localization Distillation Loss (LD), T is the temperature for distillation. ", "page_idx": 353, "bbox": [145, 161.11549377441406, 518.08251953125, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 3656, "type": "text", "text": "forward train ( x ,  out teacher ,  img_metas ,  gt_bboxes ,  gt_labels  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  None ,  gt b boxes ignore=None , proposal cf g  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ,  \\*\\*kwargs ) ", "page_idx": 353, "bbox": [96, 226.7200164794922, 494.594482421875, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 3657, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 353, "bbox": [136, 270, 188, 282], "page_size": [612.0, 792.0]}
{"layout": 3658, "type": "text", "text": "•  x  ( list[Tensor] ) – Features from FPN. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt_bboxes  ( Tensor ) – Ground truth bboxes of the image, shape (num_gts, 4). •  gt_labels  ( Tensor ) – Ground truth labels of each box, shape (num_gts,). •  gt b boxes ignore ( Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). •  proposal cf g  ( mmcv.Config ) – Test / post processing configuration, if None, test_cfg would be used ", "page_idx": 353, "bbox": [154, 286.6455078125, 521, 425.48455810546875], "page_size": [612.0, 792.0]}
{"layout": 3659, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 353, "bbox": [136, 431, 173, 443], "page_size": [612.0, 792.0]}
{"layout": 3660, "type": "text", "text": "The loss components and proposals of each image. • losses (dict[str, Tensor]): A dictionary of loss components. • proposal list (list[Tensor]): Proposals of each image. Return type  tuple[dict, list] ", "page_idx": 353, "bbox": [137.4547882080078, 448.03955078125, 398.3553161621094, 515.746337890625], "page_size": [612.0, 792.0]}
{"layout": 3661, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  soft target ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute losses of the head. ", "page_idx": 353, "bbox": [96, 519.6211547851562, 505.8031005859375, 545.03662109375], "page_size": [612.0, 792.0]}
{"layout": 3662, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 353, "bbox": [136, 551, 187, 563], "page_size": [612.0, 792.0]}
{"layout": 3663, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Cls and quality scores for each scale level has shape (N, num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box distribution logits for each scale level with shape (N,  $4^{*}(\\mathsf{n}\\!+\\!1)$  , H, W), n is max value of integral set. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. ", "page_idx": 353, "bbox": [154, 567.591552734375, 521, 700.45361328125], "page_size": [612.0, 792.0]}
{"layout": 3664, "type": "text", "text": "•  gt b boxes ignore  ( list[Tensor] | None ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 354, "bbox": [154, 71.45246887207031, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3665, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 354, "bbox": [137, 100.71282958984375, 308.6417236328125, 115.24826049804688], "page_size": [612.0, 792.0]}
{"layout": 3666, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 354, "bbox": [137, 118.64483642578125, 256, 133.18026733398438], "page_size": [612.0, 792.0]}
{"layout": 3667, "type": "text", "text": "loss single ( anchors ,  cls_score ,  bbox_pred ,  labels ,  label weights ,  b box targets ,  stride ,  soft targets , num total samples ) Compute loss of a single scale level. ", "page_idx": 354, "bbox": [96, 137.05601501464844, 506.858642578125, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 3668, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 354, "bbox": [136, 180, 187, 192], "page_size": [612.0, 792.0]}
{"layout": 3669, "type": "text", "text": "•  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  cls_score  ( Tensor ) – Cls and quality joint scores for each scale level has shape (N, num classes, H, W). •  bbox_pred  ( Tensor ) – Box distribution logits for each scale level with shape (N  $\\mathsf{I},4^{*}\\mathsf{(n+1)}$  , H, W), n is max value of integral set. •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  stride  ( tuple ) – Stride in this scale level. •  num total samples  ( int ) – Number of positive samples that is reduced over all GPUs. Returns  Loss components and weight targets. Return type  dict[tuple, Tensor] ", "page_idx": 354, "bbox": [137, 196.98146057128906, 521, 432.0603332519531], "page_size": [612.0, 792.0]}
{"layout": 3670, "type": "text", "text": "class  mmdet.models.dense heads. NAS FCO S Head ( \\*args ,  init_cfg  $=$  None ,  \\*\\*kwargs ) Anchor-free head used in  NASFCOS . ", "page_idx": 354, "bbox": [71, 435.93511962890625, 433, 461.3495788574219], "page_size": [612.0, 792.0]}
{"layout": 3671, "type": "text", "text": "It is quite similar with FCOS head, except for the searched structure of classification branch and bbox regression branch, where a structure of “dconv3x3, conv3x3, dconv3x3, conv1x1” is utilized instead. ", "page_idx": 354, "bbox": [96, 465.9725646972656, 540.0033569335938, 491.2375793457031], "page_size": [612.0, 792.0]}
{"layout": 3672, "type": "text", "text": "class  mmdet.models.dense heads. PAAHead ( \\*args ,  topk  $_{:=9}$  ,  score voting  $=$  True ,  co variance type  $\\mathbf{=}$  diag' , \\*\\*kwargs ) ", "page_idx": 354, "bbox": [71, 495.71112060546875, 514.3505249023438, 521.0159301757812], "page_size": [612.0, 792.0]}
{"layout": 3673, "type": "text", "text": "", "page_idx": 354, "bbox": [79, 527.25, 505, 539], "page_size": [612.0, 792.0]}
{"layout": 3674, "type": "text", "text": "More details can be found in the  paper  . ", "page_idx": 354, "bbox": [96, 555.6365356445312, 256, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 3675, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 354, "bbox": [117, 575, 169, 586], "page_size": [612.0, 792.0]}
{"layout": 3676, "type": "text", "text": "•  topk  ( int ) – Select topk samples with smallest loss in each level. •  score voting  ( bool ) – Whether to use score voting in post-process. •  co variance type  – String describing the type of covariance parameters to be used in sklearn.mixture.Gaussian Mixture . It must be one of: –  ’full’: each component has its own general covariance matrix –  ’tied’: all components share the same general covariance matrix –  ’diag’: each component has its own diagonal covariance matrix ", "page_idx": 354, "bbox": [145, 591.5015258789062, 518.0816650390625, 707.0282592773438], "page_size": [612.0, 792.0]}
{"layout": 3677, "type": "text", "text": "–  ’spherical’: each component has its own single variance Default: ‘diag’. From ‘full’ to ‘spherical’, the gmm fitting process is faster yet the perfor- mance could be influenced. For most cases, ‘diag’ should be a good choice. ", "page_idx": 355, "bbox": [154, 70.8248291015625, 521, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 3678, "type": "text", "text": "get_bboxes ( cls_scores ,  bbox_preds ,  score factors  $\\mathbf{\\hat{\\Sigma}}$  None ,  img_metas  $\\leftrightharpoons$  None ,  cfg  $=$  None ,  rescale  $\\mathbf{\\Pi}=$  False , with_nms  $\\mathbf{=}$  True ,  \\*\\*kwargs ) Transform network outputs of a batch into bbox results. ", "page_idx": 355, "bbox": [96, 119.12303924560547, 521, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 3679, "type": "text", "text": "Note: When score factors is not None, the cls_scores are usually multiplied by it then obtain the real score used in NMS, such as CenterNess in FCOS, IoU branch in ATSS. ", "page_idx": 355, "bbox": [118, 161.11549377441406, 540.002685546875, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 3680, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 355, "bbox": [136, 192, 187, 204], "page_size": [612.0, 792.0]}
{"layout": 3681, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  score factors  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, num_priors \\* 1, H, W). Default None. •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. ", "page_idx": 355, "bbox": [154, 208.9364776611328, 521, 377.6635437011719], "page_size": [612.0, 792.0]}
{"layout": 3682, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 355, "bbox": [136, 384, 173, 395], "page_size": [612.0, 792.0]}
{"layout": 3683, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. ", "page_idx": 355, "bbox": [154, 399.5918884277344, 521, 449.39453125], "page_size": [612.0, 792.0]}
{"layout": 3684, "type": "text", "text": "Return type  list[list[Tensor, Tensor]] ", "page_idx": 355, "bbox": [137, 453.3898620605469, 288, 467.92529296875], "page_size": [612.0, 792.0]}
{"layout": 3685, "type": "text", "text": "get pos loss ( anchors ,  cls_score ,  bbox_pred ,  label ,  label weight ,  b box target ,  b box weight ,  pos_inds ) Calculate loss of all potential positive samples obtained from first match process. ", "page_idx": 355, "bbox": [96, 471.80108642578125, 521, 497.2155456542969], "page_size": [612.0, 792.0]}
{"layout": 3686, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 355, "bbox": [136, 503, 187, 515], "page_size": [612.0, 792.0]}
{"layout": 3687, "type": "text", "text": "•  anchors  ( list[Tensor] ) – Anchors of each scale. •  cls_score  ( Tensor ) – Box scores of single image with shape (num anchors, num classes) •  bbox_pred  ( Tensor ) – Box energies / deltas of single image with shape (num anchors, 4) •  label  ( Tensor ) – classification target of each anchor with shape (num anchors,) •  label weight  ( Tensor ) – Classification loss weight of each anchor with shape (num anchors). •  b box target  ( dict ) – Regression target of each anchor with shape (num anchors, 4). •  b box weight  ( Tensor ) – Bbox weight of each anchor with shape (num anchors, 4). •  pos_inds  ( Tensor ) – Index of all positive samples got from first assign process. ", "page_idx": 355, "bbox": [154, 519.7705078125, 521, 694.4755859375], "page_size": [612.0, 792.0]}
{"layout": 3688, "type": "text", "text": "Returns  Losses of all positive samples in single image. ", "page_idx": 355, "bbox": [137, 698.4708862304688, 360.7071533203125, 713.0062866210938], "page_size": [612.0, 792.0]}
{"layout": 3689, "type": "text", "text": "Return type  Tensor ", "page_idx": 356, "bbox": [137, 70.8248291015625, 220, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 3690, "type": "text", "text": "get targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $t{=}$  None , gt labels list=None ,  label channel  $\\varsigma{=}I$  ,  un map outputs  $:=$  True ) Get targets for PAA head. ", "page_idx": 356, "bbox": [96, 89.23503875732422, 499, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 3691, "type": "text", "text": "This method is almost the same as  AnchorHead.get targets() . We direct return the results from get targets single instead map it to levels by images to levels function. ", "page_idx": 356, "bbox": [118, 131.0790252685547, 540, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 3692, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 356, "bbox": [136, 162, 188, 174], "page_size": [612.0, 792.0]}
{"layout": 3693, "type": "text", "text": "•  anchor list  ( list[list[Tensor]] ) – Multi level anchors of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, 4). •  valid flag list  ( list[list[Tensor]] ) – Multi level valid flags of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, ) •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. ", "page_idx": 356, "bbox": [154, 179.04847717285156, 521, 365.7086181640625], "page_size": [612.0, 792.0]}
{"layout": 3694, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 356, "bbox": [136, 372, 172, 383], "page_size": [612.0, 792.0]}
{"layout": 3695, "type": "text", "text": "Usually returns a tuple containing learning targets. ", "page_idx": 356, "bbox": [154, 388.2646179199219, 355.85455322265625, 401.57464599609375], "page_size": [612.0, 792.0]}
{"layout": 3696, "type": "text", "text": "•  labels (list[Tensor]): Labels of all anchors, each with  shape (num anchors,). •  label weights (list[Tensor]): Label weights of all anchor.  each with shape (num anchors,). •  b box targets (list[Tensor]): BBox targets of all anchors.  each with shape (num anchors, 4). •  b box weights (list[Tensor]): BBox weights of all anchors.  each with shape (num anchors, 4). •  pos_inds (list[Tensor]): Contains all index of positive  sample in all anchor. •  gt_inds (list[Tensor]): Contains all gt_index of positive  sample in all anchor. ", "page_idx": 356, "bbox": [154, 405.5689697265625, 521, 545.6343994140625], "page_size": [612.0, 792.0]}
{"layout": 3697, "type": "text", "text": "Return type  tuple ", "page_idx": 356, "bbox": [137, 549.031005859375, 213.25009155273438, 563.56640625], "page_size": [612.0, 792.0]}
{"layout": 3698, "type": "text", "text": "g mm separation scheme ( g mm assignment ,  scores ,  pos in ds g mm ) A general separation scheme for gmm model. ", "page_idx": 356, "bbox": [96, 567.4421997070312, 382, 592.8566284179688], "page_size": [612.0, 792.0]}
{"layout": 3699, "type": "text", "text": "It separates a GMM distribution of candidate samples into three parts, 0 1 and uncertain areas, and you can implement other separation schemes by rewriting this function. ", "page_idx": 356, "bbox": [118, 597.4796142578125, 540, 622.7446899414062], "page_size": [612.0, 792.0]}
{"layout": 3700, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 356, "bbox": [136, 628, 188, 640], "page_size": [612.0, 792.0]}
{"layout": 3701, "type": "text", "text": "•  g mm assignment  ( Tensor ) – The prediction of GMM which is of shape (num samples,). The 0/1 value indicates the distribution that each sample comes from. •  scores  ( Tensor ) – The probability of sample coming from the fit GMM distribution. The tensor is of shape (num samples,). ", "page_idx": 356, "bbox": [154, 645.3005981445312, 521, 700.4536743164062], "page_size": [612.0, 792.0]}
{"layout": 3702, "type": "text", "text": "•  pos in ds g mm  ( Tensor ) – All the indexes of samples which are used to fit GMM model. The tensor is of shape (num samples,) ", "page_idx": 357, "bbox": [154, 71.45246887207031, 524, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3703, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 357, "bbox": [136, 103, 173, 114], "page_size": [612.0, 792.0]}
{"layout": 3704, "type": "text", "text": "The indices of positive and ignored samples. • pos in ds temp (Tensor): Indices of positive samples. • ignore in ds temp (Tensor): Indices of ignore samples. ", "page_idx": 357, "bbox": [154, 119.27247619628906, 382.92340087890625, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 3705, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 357, "bbox": [137, 172.44378662109375, 246.70458984375, 186.97921752929688], "page_size": [612.0, 792.0]}
{"layout": 3706, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  iou_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. ", "page_idx": 357, "bbox": [96, 190.85398864746094, 504, 216.2694549560547], "page_size": [612.0, 792.0]}
{"layout": 3707, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 357, "bbox": [136, 222, 187, 234], "page_size": [612.0, 792.0]}
{"layout": 3708, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  iou_preds  ( list[Tensor] ) – iou_preds for each scale level with shape (N, num anchors \\* 1, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – Specify which bounding boxes can be ignored when are computing the loss. ", "page_idx": 357, "bbox": [154, 238.8244171142578, 524, 431.46246337890625], "page_size": [612.0, 792.0]}
{"layout": 3709, "type": "text", "text": "Returns  A dictionary of loss g mm assignment. ", "page_idx": 357, "bbox": [137, 435.456787109375, 331, 449.9922180175781], "page_size": [612.0, 792.0]}
{"layout": 3710, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 357, "bbox": [137, 453.3898010253906, 256.1191711425781, 467.92523193359375], "page_size": [612.0, 792.0]}
{"layout": 3711, "type": "text", "text": "paa reassign ( pos_losses ,  label ,  label weight ,  b box weight ,  pos_inds ,  pos gt in ds ,  anchors ) Fit loss to GMM distribution and separate positive, ignore, negative samples again with GMM model. ", "page_idx": 357, "bbox": [96, 471.801025390625, 524, 497.2154846191406], "page_size": [612.0, 792.0]}
{"layout": 3712, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 357, "bbox": [136, 503, 187, 515], "page_size": [612.0, 792.0]}
{"layout": 3713, "type": "text", "text": "•  pos_losses  ( Tensor ) – Losses of all positive samples in single image. •  label  ( Tensor ) – classification target of each anchor with shape (num anchors,) •  label weight  ( Tensor ) – Classification loss weight of each anchor with shape (num anchors). •  b box weight  ( Tensor ) – Bbox weight of each anchor with shape (num anchors, 4). •  pos_inds  ( Tensor ) – Index of all positive samples got from first assign process. •  pos gt in ds  ( Tensor ) – Gt_index of all positive samples got from first assign process. •  anchors  ( list[Tensor] ) – Anchors of each scale. ", "page_idx": 357, "bbox": [154, 519.7704467773438, 524, 652.6325073242188], "page_size": [612.0, 792.0]}
{"layout": 3714, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 357, "bbox": [136, 659, 173, 670], "page_size": [612.0, 792.0]}
{"layout": 3715, "type": "text", "text": "Usually returns a tuple containing learning targets. • label (Tensor): classification target of each anchor after paa assign, with shape (num anchors,) ", "page_idx": 357, "bbox": [154, 675.1884155273438, 524, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 3716, "type": "text", "text": "• label weight (Tensor): Classification loss weight of each anchor after paa assign, with shape (num anchors). • b box weight (Tensor): Bbox weight of each anchor with shape (num anchors, 4). • num_pos (int): The number of positive samples after paa assign. ", "page_idx": 358, "bbox": [154, 71.45246887207031, 521, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 3717, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 358, "bbox": [137, 138, 215, 150], "page_size": [612.0, 792.0]}
{"layout": 3718, "type": "text", "text": "score voting(det_bboxes, det_labels, ml vl b boxes, ml vl nm s scores, score_thr)Implementation of score voting method works on each remaining boxes after NMS procedure. ", "page_idx": 358, "bbox": [96, 154.98899841308594, 493, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 3719, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 358, "bbox": [137, 187, 187, 198], "page_size": [612.0, 792.0]}
{"layout": 3720, "type": "text", "text": "•  det_bboxes  ( Tensor ) – Remaining boxes after NMS procedure, with shape (k, 5), each dimension means (x1, y1, x2, y2, score). •  det_labels  ( Tensor ) – The label of remaining boxes, with shape (k, 1),Labels are 0- based. •  ml vl b boxes  ( Tensor ) – All boxes before the NMS procedure, with shape (num anchors,4). •  ml vl nm s scores  ( Tensor ) – The scores of all boxes which is used in the NMS proce- dure, with shape (num anchors, num_class) •  score_thr  ( float ) – The score threshold of bboxes. ", "page_idx": 358, "bbox": [154, 202.9594268798828, 521, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 3721, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 358, "bbox": [136, 342, 173, 353], "page_size": [612.0, 792.0]}
{"layout": 3722, "type": "text", "text": "Usually returns a tuple containing voting results. •  det b boxes voted (Tensor): Remaining boxes after  score voting procedure, with shape (k, 5), each dimension means (x1, y1, x2, y2, score). •  det labels voted (Tensor): Label of remaining bboxes  after voting, with shape (num anchors,). ", "page_idx": 358, "bbox": [154, 358.3764953613281, 521, 431.4625244140625], "page_size": [612.0, 792.0]}
{"layout": 3723, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 358, "bbox": [137, 437, 213, 450], "page_size": [612.0, 792.0]}
{"layout": 3724, "type": "text", "text": "class  mmdet.models.dense heads. PISA Retina Head ( num classes ,  in channels ,  stacked con vs=4 , ", "page_idx": 358, "bbox": [71.99992370605469, 453.8680725097656, 493, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 3725, "type": "text", "text": "conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $=$  None , anchor generator={'octave base scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} , init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'retina_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) ", "page_idx": 358, "bbox": [312, 465.82305908203125, 530, 551.0134887695312], "page_size": [612.0, 792.0]}
{"layout": 3726, "type": "text", "text": "PISA Retinanet Head. ", "page_idx": 358, "bbox": [96, 549.658447265625, 184.57745361328125, 562.968505859375], "page_size": [612.0, 792.0]}
{"layout": 3727, "type": "text", "text": "The head owns the same structure with Retinanet Head, but differs in two  aspects: 1. Importance-based Sample Re weighting Positive (ISR-P) is applied to ", "page_idx": 358, "bbox": [96, 566.9638061523438, 540.0042114257812, 592.8565063476562], "page_size": [612.0, 792.0]}
{"layout": 3728, "type": "text", "text": "change the positive loss weights. 2. Classification-aware regression loss is adopted as a third loss. ", "page_idx": 358, "bbox": [125.00155639648438, 597.4794311523438, 383, 634.6995239257812], "page_size": [612.0, 792.0]}
{"layout": 3729, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 358, "bbox": [96, 645.1510009765625, 458.2408752441406, 670.5654907226562], "page_size": [612.0, 792.0]}
{"layout": 3730, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 358, "bbox": [136, 677, 188, 688], "page_size": [612.0, 792.0]}
{"layout": 3731, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) ", "page_idx": 358, "bbox": [154, 693.1204223632812, 521, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 3732, "type": "text", "text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image with shape (num_obj, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each image with shape (num_obj, 4). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] ) – Ignored gt bboxes of each image. Default: None. ", "page_idx": 359, "bbox": [154, 71.45246887207031, 521, 204.3135223388672], "page_size": [612.0, 792.0]}
{"layout": 3733, "type": "text", "text": "Loss dict, comprise classification loss, regression loss and  carl loss. Return type  dict ", "page_idx": 359, "bbox": [137, 226.2418212890625, 435, 258.7102355957031], "page_size": [612.0, 792.0]}
{"layout": 3734, "type": "text", "text": "( num classes=80 ,  in_channels=(512, 1024, 512, 256, 256, 256) ,  stacked con v  $\\mathord{:=}O$  ,  feat channels=256 , use depth wise  $\\mathbf{\\dot{\\rho}}=$  False ,  conv_cfg=None ,  norm_cfg=None , act_cfg=None ,  anchor generator={'base size ratio range': (0.1, 0.9), 'input_size': 300, 'ratios': ([2], [2, 3], [2, 3], [2, 3], [2], [2]), 'scale_major': False, 'strides': [8, 16, 32, 64, 100, 300], 'type': 'SSD Anchor Generator'} , bbox_coder={'clip border': True, 'target means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box=False , train_cfg  $=$  None ,  test_cfg=None ,  init_cfg={'bias': 0, 'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) ", "page_idx": 359, "bbox": [291.68597412109375, 262.58502197265625, 535, 407.4417724609375], "page_size": [612.0, 792.0]}
{"layout": 3735, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 359, "bbox": [96.906494140625, 423.9798889160156, 458.2408142089844, 449.39434814453125], "page_size": [612.0, 792.0]}
{"layout": 3736, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 359, "bbox": [136, 455, 187, 466], "page_size": [612.0, 792.0]}
{"layout": 3737, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image with shape (num_obj, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each image with shape (num_obj, 4). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] ) – Ignored gt bboxes of each image. Default: None. ", "page_idx": 359, "bbox": [154, 471.9503479003906, 521, 634.7003784179688], "page_size": [612.0, 792.0]}
{"layout": 3738, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 359, "bbox": [137, 642, 172, 652], "page_size": [612.0, 792.0]}
{"layout": 3739, "type": "text", "text": "Loss dict, comprise classification loss regression loss and  carl loss. Return type  dict ", "page_idx": 359, "bbox": [137, 656.627685546875, 435, 689.0960693359375], "page_size": [612.0, 792.0]}
{"layout": 3740, "type": "text", "text": "class  mmdet.models.dense heads. RPNHead ( in channels ,  init_cfg  $=$  {'layer': 'Conv2d', 'std': 0.01, 'type': 'Normal'} ,  num_convs  $\\mathsf{\\chi}_{=I}$  ,  \\*\\*kwargs ) ", "page_idx": 360, "bbox": [72.0, 71.30303192138672, 510.4154357910156, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3741, "type": "text", "text": "RPN head. ", "page_idx": 360, "bbox": [96, 95.36250305175781, 140.0747833251953, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 3742, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 360, "bbox": [117, 114, 170, 126], "page_size": [612.0, 792.0]}
{"layout": 3743, "type": "text", "text": "•  in channels  ( int ) – Number of channels in the input feature map. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. •  num_convs  ( int ) – Number of convolution layers in the head. Default 1. ", "page_idx": 360, "bbox": [145, 131.2284698486328, 462.22509765625, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 3744, "type": "text", "text": "forward single  $(x)$  Forward feature map of a single scale level. loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. ", "page_idx": 360, "bbox": [96, 186, 416.7271728515625, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3745, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 360, "bbox": [136, 246, 188, 257], "page_size": [612.0, 792.0]}
{"layout": 3746, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components.  dict[str, Tensor] ", "page_idx": 360, "bbox": [137, 262.7344665527344, 521, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 3747, "type": "text", "text": "on nx export ( x ,  img_metas ) Test without augmentation. ", "page_idx": 360, "bbox": [96, 447.8900451660156, 227.13815307617188, 473.3055114746094], "page_size": [612.0, 792.0]}
{"layout": 3748, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 360, "bbox": [136, 479, 188, 491], "page_size": [612.0, 792.0]}
{"layout": 3749, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – Meta info of each image. Returns  dets of shape [N, num_det, 5]. Return type  Tensor ", "page_idx": 360, "bbox": [137, 495.8605041503906, 484.841064453125, 563.5662841796875], "page_size": [612.0, 792.0]}
{"layout": 3750, "type": "text", "text": "class  mmdet.models.dense heads. Rep Points Head ( num classes ,  in channels ,  point feat channel  $'s{=}256$  , num_points  $\\scriptstyle{:=9}$  ,  gradient mu  $l{=}0.1$  ,  point strides=[8, 16, 32, 64, 128] ,  point base scale=4 ,  loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True}, loss b box in it={'beta':0.1111111111111111, 'loss_weight': 0.5, 'type': 'Smooth L 1 Loss'} ,  loss b box refine={'beta': 0.1111111111111111, 'loss_weight': 1.0, 'type': 'Smooth L 1 Loss'} ,  use grid points  $=$  False , center in it  $\\mathbf{=}$  True ,  transform method  $\\leftrightharpoons$  moment' , moment_mul=0.01 ,  init_cfg={'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'rep points cls out', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} , \\*\\*kwargs ) ", "page_idx": 361, "bbox": [72.0, 71.30303192138672, 539, 240.07008361816406], "page_size": [612.0, 792.0]}
{"layout": 3751, "type": "text", "text": "RepPoint head. ", "page_idx": 361, "bbox": [96, 238.8246612548828, 157.2896728515625, 252.1346893310547], "page_size": [612.0, 792.0]}
{"layout": 3752, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 361, "bbox": [113, 259, 168, 268], "page_size": [612.0, 792.0]}
{"layout": 3753, "type": "text", "text": "•  point feat channels  ( int ) – Number of channels of points features. •  gradient mul  ( float ) – The multiplier to gradients from points refinement and recogni- tion. •  point strides  ( Iterable ) – points strides. •  point base scale  ( int ) – bbox scale for assigning labels. •  loss_cls  ( dict ) – Config of classification loss. •  loss b box in it  ( dict ) – Config of initial points loss. •  loss b box refine  ( dict ) – Config of points loss in refinement. •  use grid points  ( bool ) – If we use bounding box representation, the •  is represented as grid points on the bounding box.  ( reppoints ) – •  center in it  ( bool ) – Whether to use center point assignment. •  transform method  ( str ) – The methods to transform RepPoints to bbox. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 361, "bbox": [145, 274.690673828125, 521, 497.2158203125], "page_size": [612.0, 792.0]}
{"layout": 3754, "type": "text", "text": "centers to b boxes ( point_list ) Get bboxes according to center points. Only used in  MaxI oU As signer . ", "page_idx": 361, "bbox": [96, 501.68939208984375, 271.4613037109375, 545.0368041992188], "page_size": [612.0, 792.0]}
{"layout": 3755, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 361, "bbox": [96, 549.5093383789062, 521, 604.8128051757812], "page_size": [612.0, 792.0]}
{"layout": 3756, "type": "text", "text": "Returns Usually contain classification scores and bbox predictions. cls_scores (list[Tensor]): Box scores for each scale level,  each is a 4D-tensor, the chan- nel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for each scale  level, each is a 4D- tensor, the channel number is num_points   $^{*}\\,4$  . Return type  tuple ", "page_idx": 361, "bbox": [137, 608.80712890625, 521, 718.9844970703125], "page_size": [612.0, 792.0]}
{"layout": 3757, "type": "text", "text": "forward single  $(x)$  Forward feature map of a single FPN level. ", "page_idx": 362, "bbox": [96, 73, 290.2803649902344, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3758, "type": "text", "text": "gen grid from reg ( reg ,  previous boxes ) Base on the previous bboxes and regression values, we compute the regressed bboxes and generate the grids on the bboxes. ", "page_idx": 362, "bbox": [96, 101.19103240966797, 540.0029907226562, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 3759, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 362, "bbox": [136, 145, 188, 156], "page_size": [612.0, 792.0]}
{"layout": 3760, "type": "text", "text": "•  reg  – the regression value to previous bboxes. •  previous boxes  – previous bboxes. Returns  generate grids on the regressed bboxes. ", "page_idx": 362, "bbox": [137, 161.11549377441406, 351, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 3761, "type": "text", "text": "get_points ( feat map sizes ,  img_metas ,  device ) Get points according to feature map sizes. ", "page_idx": 362, "bbox": [96, 214.76499938964844, 293.6983642578125, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3762, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 362, "bbox": [136, 246, 188, 257], "page_size": [612.0, 792.0]}
{"layout": 3763, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. Returns  points of each image, valid flags of each image ", "page_idx": 362, "bbox": [137, 262.7344665527344, 427.93402099609375, 312.5082702636719], "page_size": [612.0, 792.0]}
{"layout": 3764, "type": "text", "text": "Return type  tuple ", "page_idx": 362, "bbox": [137, 315.9058532714844, 213, 330.4412841796875], "page_size": [612.0, 792.0]}
{"layout": 3765, "type": "text", "text": "get targets ( proposals list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore lis  $\\leftleftarrows$  None , gt labels list  $=$  None ,  stage  $:=$  'init' ,  label channel  $\\scriptstyle{\\mathfrak{s}}=I$  ,  un map outputs  $\\backsimeq$  True ) Compute corresponding GT box and classification targets for proposals. ", "page_idx": 362, "bbox": [96, 334.3160705566406, 510, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 3766, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 362, "bbox": [136, 377, 187, 389], "page_size": [612.0, 792.0]}
{"layout": 3767, "type": "text", "text": "•  proposals list  ( list[list] ) – Multi level points/bboxes of each image. •  valid flag list  ( list[list] ) – Multi level valid flags of each image. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt b boxes list  – Ground truth labels of each box. •  stage  ( str ) –  init  or  refine . Generate target for init stage or refine stage •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. ", "page_idx": 362, "bbox": [155, 394.24151611328125, 510, 551.0136108398438], "page_size": [612.0, 792.0]}
{"layout": 3768, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 362, "bbox": [136, 557, 172, 568], "page_size": [612.0, 792.0]}
{"layout": 3769, "type": "text", "text": "• labels list (list[Tensor]): Labels of each level. • label weights list (list[Tensor]): Label weights of each level. # noqa: E501 • b box gt list (list[Tensor]): Ground truth bbox of each level. • proposal list (list[Tensor]): Proposals(points/bboxes) of each level. # noqa: E501 • proposal weights list (list[Tensor]): Proposal weights of each level. # noqa: E501 • num total pos (int): Number of positive samples in all images. # noqa: E501 • num total ne g (int): Number of negative samples in all images. # noqa: E501 ", "page_idx": 362, "bbox": [155, 573.569580078125, 492.521728515625, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 3770, "type": "text", "text": "Return type  tuple ", "page_idx": 362, "bbox": [137, 698.470947265625, 213, 713.00634765625], "page_size": [612.0, 792.0]}
{"layout": 3771, "type": "text", "text": "loss ( cls_scores ,  pts p reds in it ,  pts p reds refine ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute loss of the head. ", "page_idx": 363, "bbox": [95, 71.30303192138672, 540, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3772, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 363, "bbox": [136, 102, 187, 114], "page_size": [612.0, 792.0]}
{"layout": 3773, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 363, "bbox": [155, 119.27247619628906, 521, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 3774, "type": "text", "text": "offset to pts ( center list ,  pred_list ) Change from point offset to point coordinate. ", "page_idx": 363, "bbox": [95, 286.4960632324219, 299, 311.9105224609375], "page_size": [612.0, 792.0]}
{"layout": 3775, "type": "text", "text": "points 2 b box ( pts ,  y_first=True ) Converting the points set into bounding box. ", "page_idx": 363, "bbox": [95, 316.3840637207031, 299, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 3776, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 363, "bbox": [136, 348, 188, 359], "page_size": [612.0, 792.0]}
{"layout": 3777, "type": "text", "text": "•  pts  – the input points sets (fields), each points set (fields) is represented as 2n scalar. •  y_first  – if y_firs  $\\mathrel{\\mathop:}=$  True, the point set is represented as [y1, x1, y2, x2 ... yn, xn], otherwise the point set is represented as [x1, y1, x2, y2 ... xn, yn]. Returns  each points set is converting to a bbox [x1, y1, x2, y2]. ", "page_idx": 363, "bbox": [137.4539794921875, 364.353515625, 521, 426.0823059082031], "page_size": [612.0, 792.0]}
{"layout": 3778, "type": "text", "text": "( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  ,  conv_cfg  $\\mathbf{\\beta}=$  None , norm_cfg  $=$  None ,  anchor generator={'octave base scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'retina_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} , \\*\\*kwargs ) ", "page_idx": 363, "bbox": [286.4549865722656, 429.9580993652344, 540, 515.0388793945312], "page_size": [612.0, 792.0]}
{"layout": 3779, "type": "text", "text": "An anchor-based head used in  RetinaNet . ", "page_idx": 363, "bbox": [95, 513.7933959960938, 264, 527.1034545898438], "page_size": [612.0, 792.0]}
{"layout": 3780, "type": "text", "text": "The head contains two sub networks. The first classifies anchor boxes and the second regresses deltas for the anchors.\n\n ", "page_idx": 363, "bbox": [95, 531.7264404296875, 540, 556.9915161132812], "page_size": [612.0, 792.0]}
{"layout": 3781, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 363, "bbox": [96, 577, 138, 588], "page_size": [612.0, 792.0]}
{"layout": 3782, "type": "text", "text": " $>>$   import  torch\n\n  $>>$   self  $=$   RetinaHead( 11 ,  7 )\n\n  $>>$   $\\tt{x\\ =}$  torch.rand(1, 7, 32, 32)\n\n $>>$   cls_score, bbox_pred  $=$   self . forward single(x)\n\n >>>  # Each anchor predicts a score for each class except background\n\n  $>>$   cls per anchor  $=$   cls_score . shape[ 1 ]  /  self . num anchors\n\n  $>>$   box per anchor  $=$   bbox_pred . shape[ 1 ]  /  self . num anchors\n\n  $>>$   assert  cls per anchor  $==$   ( self . num classes)\n\n  $>>$   assert  box per anchor  $==~4$  ", "page_idx": 363, "bbox": [95, 604.4299926757812, 447.34136962890625, 710.9800415039062], "page_size": [612.0, 792.0]}
{"layout": 3783, "type": "text", "text": "forward single  $(x)$  ", "text_level": 1, "page_idx": 364, "bbox": [96, 73, 186, 84], "page_size": [612.0, 792.0]}
{"layout": 3784, "type": "text", "text": "Forward feature of a single scale level. ", "page_idx": 364, "bbox": [118, 83.40748596191406, 272.2779846191406, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3785, "type": "text", "text": "Parameters  x  ( Tensor ) – Features of a single scale level. ", "page_idx": 364, "bbox": [137, 100.71282958984375, 371.177734375, 115.24826049804688], "page_size": [612.0, 792.0]}
{"layout": 3786, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 364, "bbox": [136, 121, 173, 132], "page_size": [612.0, 792.0]}
{"layout": 3787, "type": "text", "text": "cls_score (Tensor): Cls scores for a single scale level  the channels number is num anchors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale  level, the channels number is num anchors  $^{*}\\,4$  . ", "page_idx": 364, "bbox": [154, 136.57781982421875, 521, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 3788, "type": "text", "text": "Return type  tuple ", "page_idx": 364, "bbox": [137, 196.35382080078125, 213.25018310546875, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 3789, "type": "text", "text": "class  mmdet.models.dense heads. RetinaS epB N Head ( num classes ,  num_ins ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  , conv_cfg  $\\mathbf{\\beta}=$  None ,  norm_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  init_cfg  $=$  None , \\*\\*kwargs ) ", "page_idx": 364, "bbox": [71.99998474121094, 214.76499938964844, 535.7996215820312, 252.02491760253906], "page_size": [612.0, 792.0]}
{"layout": 3790, "type": "text", "text": "“RetinaHead with separate BN. ", "page_idx": 364, "bbox": [96, 250.7794952392578, 222.06704711914062, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 3791, "type": "text", "text": "In RetinaHead, conv/norm layers are shared across different FPN levels, while in RetinaS epB N Head, conv layers are shared across different FPN levels, but BN layers are separated. ", "page_idx": 364, "bbox": [96, 268.7124938964844, 540.0034790039062, 293.9775085449219], "page_size": [612.0, 792.0]}
{"layout": 3792, "type": "text", "text": "forward ( feats ) ", "text_level": 1, "page_idx": 364, "bbox": [96, 300, 163, 311], "page_size": [612.0, 792.0]}
{"layout": 3793, "type": "text", "text": "Forward features from the upstream network. ", "page_idx": 364, "bbox": [118, 310.55548095703125, 298.6697692871094, 323.8655090332031], "page_size": [612.0, 792.0]}
{"layout": 3794, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 364, "bbox": [137, 327.86083984375, 521, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 3795, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 364, "bbox": [136, 360, 172, 371], "page_size": [612.0, 792.0]}
{"layout": 3796, "type": "text", "text": "Usually a tuple of classification scores and bbox prediction cls_scores (list[Tensor]): Classification scores for all scale  levels, each is a 4D-tensor, the channels number is num anchors \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for all scale  levels, each is a 4D- tensor, the channels number is num anchors   $^{*}\\,4$  . ", "page_idx": 364, "bbox": [154, 375.68084716796875, 521, 449.39453125], "page_size": [612.0, 792.0]}
{"layout": 3797, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 364, "bbox": [136, 455, 214, 467], "page_size": [612.0, 792.0]}
{"layout": 3798, "type": "text", "text": "in it weights()Initialize weights of the head. ", "page_idx": 364, "bbox": [96, 473.6740417480469, 236, 497.2155456542969], "page_size": [612.0, 792.0]}
{"layout": 3799, "type": "text", "text": "class  mmdet.models.dense heads. S ABL Retina Head ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  , ", "page_idx": 365, "bbox": [72.0, 71.30303192138672, 491.8246765136719, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3800, "type": "text", "text": "approx anchor generator={'octave base scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} , square anchor generator={'ratios': [1.0], 'scales': [4], 'strides': [8, 16, 32, 64, 128], 'type': 'Anchor Generator'} ,  conv_cfg=None ,  norm_cfg=None , bbox_coder={'num buckets': 14, 'scale factor': 3.0, 'type': 'Bucketing B Box Code r'} , reg decoded b box=False ,  train_cfg=None , test_cfg=None ,  loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True} ,  loss b box cls={'loss weight': 1.5, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_bbox_reg={'beta': 0.1111111111111111, 'loss weight': 1.5, 'type': 'Smooth L 1 Loss'} , init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'retina_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ) ", "page_idx": 365, "bbox": [312, 95.21306610107422, 535, 311.8011169433594], "page_size": [612.0, 792.0]}
{"layout": 3801, "type": "text", "text": "Side-Aware Boundary Localization (SABL) for RetinaNet. ", "page_idx": 365, "bbox": [96, 310.5556640625, 330.6983947753906, 323.8656921386719], "page_size": [612.0, 792.0]}
{"layout": 3802, "type": "text", "text": "The anchor generation, assigning and sampling in S ABL Retina Head are the same as Guided Anchor Head for guided anchoring. ", "page_idx": 365, "bbox": [96, 328.4886779785156, 540.002685546875, 353.7536926269531], "page_size": [612.0, 792.0]}
{"layout": 3803, "type": "text", "text": "Please refer to  https://arxiv.org/abs/1912.04260  for more details. ", "page_idx": 365, "bbox": [96, 358.3766784667969, 354, 371.68670654296875], "page_size": [612.0, 792.0]}
{"layout": 3804, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 365, "bbox": [117, 378, 168, 389], "page_size": [612.0, 792.0]}
{"layout": 3805, "type": "text", "text": "•  num classes  ( int ) – Number of classes. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of Convs for classification and regression branches. De- faults to 4. •  feat channels  ( int ) – Number of hidden channels. Defaults to 256. •  approx anchor generator  ( dict ) – Config dict for approx generator. •  square anchor generator  ( dict ) – Config dict for square generator. •  conv_cfg  ( dict ) – Config dict for ConvModule. Defaults to None. •  norm_cfg  ( dict ) – Config dict for Norm Layer. Defaults to None. •  bbox_coder  ( dict ) – Config dict for bbox coder. •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  train_cfg  ( dict ) – Training config of S ABL Retina Head. •  test_cfg  ( dict ) – Testing config of S ABL Retina Head. •  loss_cls  ( dict ) – Config of classification loss. •  loss b box cls  ( dict ) – Config of classification loss for bbox branch. •  loss b box reg  ( dict ) – Config of regression loss for bbox branch. ", "page_idx": 365, "bbox": [145, 394.24169921875, 522, 706.4317626953125], "page_size": [612.0, 792.0]}
{"layout": 3806, "type": "text", "text": "•  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "text_level": 1, "page_idx": 366, "bbox": [145, 73, 462, 85], "page_size": [612.0, 792.0]}
{"layout": 3807, "type": "text", "text": "forward ( feats ) ", "page_idx": 366, "bbox": [96, 89.23503875732422, 162.5653839111328, 102.58492279052734], "page_size": [612.0, 792.0]}
{"layout": 3808, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 366, "bbox": [118, 101.34046936035156, 313.4832763671875, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 3809, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 366, "bbox": [118, 148.5328369140625, 540, 186.3815155029297], "page_size": [612.0, 792.0]}
{"layout": 3810, "type": "text", "text": "get anchors ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get squares according to feature map sizes and guided anchors. ", "page_idx": 366, "bbox": [96, 202.8090057373047, 371.09698486328125, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 3811, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 366, "bbox": [136, 235, 187, 246], "page_size": [612.0, 792.0]}
{"layout": 3812, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – device for returned tensors ", "page_idx": 366, "bbox": [154, 250.77943420410156, 427.93402099609375, 299.9554748535156], "page_size": [612.0, 792.0]}
{"layout": 3813, "type": "text", "text": "Returns  square approxs of each image ", "page_idx": 366, "bbox": [137, 303.9497985839844, 294.0066223144531, 318.4852294921875], "page_size": [612.0, 792.0]}
{"layout": 3814, "type": "text", "text": "Return type  tuple ", "page_idx": 366, "bbox": [137, 321.8828125, 213.2501220703125, 336.4182434082031], "page_size": [612.0, 792.0]}
{"layout": 3815, "type": "text", "text": "get_bboxes ( cls_scores ,  bbox_preds ,  img_metas ,  cfg  $\\mathbf{\\beta}=$  None ,  rescale  $=$  False ) Transform network outputs of a batch into bbox results. ", "page_idx": 366, "bbox": [96, 340.2940368652344, 405.977294921875, 365.70849609375], "page_size": [612.0, 792.0]}
{"layout": 3816, "type": "text", "text": "Note: When score factors is not None, the cls_scores are usually multiplied by it then obtain the real score used in NMS, such as CenterNess in FCOS, IoU branch in ATSS. ", "page_idx": 366, "bbox": [118, 370.33148193359375, 540, 395.59649658203125], "page_size": [612.0, 792.0]}
{"layout": 3817, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 366, "bbox": [136, 401, 187, 413], "page_size": [612.0, 792.0]}
{"layout": 3818, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  score factors  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, num_priors \\* 1, H, W). Default None. •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. ", "page_idx": 366, "bbox": [154, 418.1524963378906, 521, 586.8795166015625], "page_size": [612.0, 792.0]}
{"layout": 3819, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 366, "bbox": [136, 593, 173, 604], "page_size": [612.0, 792.0]}
{"layout": 3820, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. ", "page_idx": 366, "bbox": [154, 608.8068237304688, 521, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 3821, "type": "text", "text": "Return type  list[list[Tensor, Tensor]] ", "page_idx": 366, "bbox": [137, 662.6048583984375, 288, 677.1402587890625], "page_size": [612.0, 792.0]}
{"layout": 3822, "type": "text", "text": "get_target ( approx list ,  inside flag list ,  square list ,  gt b boxes list ,  img_metas , gt b boxes ignore list  $\\leftleftarrows$  None ,  gt labels list  $\\leftrightharpoons$  None ,  label channels  $\\overleftarrow{}$  None ,  sampling  $\\mathbf{\\dot{\\Sigma}}$  True , un map outputs $\\backsimeq$ True)", "page_idx": 367, "bbox": [96, 71.30303192138672, 517, 108.56295013427734], "page_size": [612.0, 792.0]}
{"layout": 3823, "type": "text", "text": "list[list] :param inside flag list: Multi level inside flags of each ", "page_idx": 367, "bbox": [118, 119.27253723144531, 372, 132.5825653076172], "page_size": [612.0, 792.0]}
{"layout": 3824, "type": "text", "text": "image. ", "page_idx": 367, "bbox": [137.4550323486328, 137.2055206298828, 164.0452117919922, 150.5155487060547], "page_size": [612.0, 792.0]}
{"layout": 3825, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 367, "bbox": [137, 163, 188, 174], "page_size": [612.0, 792.0]}
{"layout": 3826, "type": "text", "text": "•  square list  ( list[list] ) – Multi level squares of each image. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – ignore list of gt bboxes. •  gt b boxes list  – Gt bboxes of each image. •  label channels  ( int ) – Channel of label. •  sampling  ( bool ) – Sample Anchors or not. •  un map outputs  ( bool ) – unmap outputs or not. ", "page_idx": 367, "bbox": [154, 179.04847717285156, 463.0372619628906, 317.8874816894531], "page_size": [612.0, 792.0]}
{"layout": 3827, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 367, "bbox": [136, 324, 172, 335], "page_size": [612.0, 792.0]}
{"layout": 3828, "type": "text", "text": "Returns a tuple containing learning targets. • labels list (list[Tensor]): Labels of each level. • label weights list (list[Tensor]): Label weights of each level. • b box cls targets list (list[Tensor]): BBox cls targets of each level. • b box cls weights list (list[Tensor]): BBox cls weights of each level. • b box reg targets list (list[Tensor]): BBox reg targets of each level. • b box reg weights list (list[Tensor]): BBox reg weights of each level. • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. ", "page_idx": 367, "bbox": [154, 340.4434814453125, 441.4635314941406, 497.2156066894531], "page_size": [612.0, 792.0]}
{"layout": 3829, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 367, "bbox": [136, 503, 214, 515], "page_size": [612.0, 792.0]}
{"layout": 3830, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. ", "page_idx": 367, "bbox": [96, 525.5991821289062, 458.2413330078125, 551.0136108398438], "page_size": [612.0, 792.0]}
{"layout": 3831, "type": "text", "text": "", "page_idx": 367, "bbox": [70, 556, 468, 562.75], "page_size": [612.0, 792.0]}
{"layout": 3832, "type": "text", "text": "stacked con vs  $\\scriptstyle{\\;=4}$  ,  strides=(4, 8, 16, 32, 64) ,  scale ranges=((8, 32), (16, 64), (32, 128), (64, 256), (128, 512)) ,  pos_scale=0.2 , num_grids=[40, 36, 24, 16, 12] ,  cls down index=0 , loss_mask  $\\leftrightharpoons$  None ,  loss_cls  $\\leftrightharpoons$  None ,  norm_cfg={'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  train_cfg=None , test_cfg  $=$  None ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  [{'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01,'override': {'name': 'con v mask list'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override': {'name': 'conv_cls'}}] ) ", "page_idx": 367, "bbox": [281, 567.442138671875, 532, 676.4330444335938], "page_size": [612.0, 792.0]}
{"layout": 3833, "type": "text", "text": "SOLO mask head used in  \\` SOLO: Segmenting Objects by Locations. < https://arxiv.org/abs/1912.04488 >\\`_ ", "page_idx": 367, "bbox": [96, 674.5609741210938, 372, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 3834, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 368, "bbox": [117, 73, 169, 84], "page_size": [612.0, 792.0]}
{"layout": 3835, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. Used in child classes. Default: 256. •  stacked con vs  ( int ) – Number of stacking convs of the head. Default: 4. •  strides  ( tuple ) – Downsample factor of each feature map. •  scale ranges  ( tuple[tuple[int, int]] ) – Area range of multiple level masks, in the format [(min1, max1), (min2, max2), ...]. A range of (16, 64) means the area range between (16, 64). •  pos_scale  ( float ) – Constant scale factor to control the center region. •  num_grids  ( list[int] ) – Divided image into a uniform grids, each feature map has a different grid value. The number of output channels is grid   $^{**}2$  . Default: [40, 36, 24, 16, 12]. •  cls down index  ( int ) – The index of downsample operation in classification branch. De- fault: 0. •  loss_mask  ( dict ) – Config of mask loss. •  loss_cls  ( dict ) – Config of classification loss. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $\\mathbf{\\beta}=$  dict(type  $\\scriptstyle{:=}$  ’GN’, num_groups  $\\scriptstyle{:=32}$  , requires grad=True). •  train_cfg  ( dict ) – Training config of head. •  test_cfg  ( dict ) – Testing config of head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 368, "bbox": [145, 89.38447570800781, 518, 443.4175720214844], "page_size": [612.0, 792.0]}
{"layout": 3836, "type": "text", "text": "forward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 368, "bbox": [96, 447.8901062011719, 313.4832458496094, 491.2375793457031], "page_size": [612.0, 792.0]}
{"layout": 3837, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 368, "bbox": [118, 507.18780517578125, 540, 545.0364379882812], "page_size": [612.0, 792.0]}
{"layout": 3838, "type": "text", "text": "get results ( ml vl mask p reds ,  ml vl cls scores ,  img_metas ,  \\*\\*kwargs ) Get multi-image mask results. ", "page_idx": 368, "bbox": [96, 561.4650268554688, 393, 586.8795166015625], "page_size": [612.0, 792.0]}
{"layout": 3839, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 368, "bbox": [136, 593, 188, 605], "page_size": [612.0, 792.0]}
{"layout": 3840, "type": "text", "text": "•  ml vl mask p reds  ( list[Tensor] ) – Multi-level mask prediction. Each element in the list has shape (batch_size, num_grids\\*\\*2 ,h ,w). •  ml vl cls scores  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes, num_grids ,num_grids). •  img_metas  ( list[dict] ) – Meta information of all images. ", "page_idx": 368, "bbox": [154, 609.4344482421875, 521, 682.5205078125], "page_size": [612.0, 792.0]}
{"layout": 3841, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 368, "bbox": [136, 689, 172, 700], "page_size": [612.0, 792.0]}
{"layout": 3842, "type": "text", "text": "Processed results of multiple images.Each  Instance Data  usually contains following keys. ", "page_idx": 368, "bbox": [154, 705.076416015625, 518, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 3843, "type": "text", "text": "• scores (Tensor): Classification scores, has shape (num instance,). • labels (Tensor): Has shape (num instances,). • masks (Tensor): Processed mask results, has shape (num instances, h, w). ", "page_idx": 369, "bbox": [154, 71.45246887207031, 459.41644287109375, 120.62749481201172], "page_size": [612.0, 792.0]}
{"layout": 3844, "type": "text", "text": "Return type  list[ Instance Data ] ", "page_idx": 369, "bbox": [137, 124.622802734375, 274.759521484375, 139.15823364257812], "page_size": [612.0, 792.0]}
{"layout": 3845, "type": "text", "text": "loss ( ml vl mask p reds ,  ml vl cls p reds ,  gt_labels ,  gt_masks ,  img_metas ,  gt_bboxes  $\\mathbf{\\check{\\Sigma}}$  None ,  \\*\\*kwargs ) Calculate the loss of total batch. ", "page_idx": 369, "bbox": [96, 143.0339813232422, 512, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 3846, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 369, "bbox": [136, 175, 187, 186], "page_size": [612.0, 792.0]}
{"layout": 3847, "type": "text", "text": "•  ml vl mask p reds  ( list[Tensor] ) – Multi-level mask prediction. Each element in the list has shape (batch_size, num_grids\\*  $^{*}2$   ,h ,w). •  ml vl cls p reds  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes, num_grids ,num_grids). •  gt_labels  ( list[Tensor] ) – Labels of multiple images. •  gt_masks  ( list[Tensor] ) – Ground truth masks of multiple images. Each has shape (num instances, h, w). •  img_metas  ( list[dict] ) – Meta information of multiple images. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of multiple images. Default: None. ", "page_idx": 369, "bbox": [154, 191.00343322753906, 521, 329.8434753417969], "page_size": [612.0, 792.0]}
{"layout": 3848, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 369, "bbox": [137, 333.8377990722656, 308.6416015625, 348.37322998046875], "page_size": [612.0, 792.0]}
{"layout": 3849, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 369, "bbox": [137, 351.77081298828125, 256, 366.3062438964844], "page_size": [612.0, 792.0]}
{"layout": 3850, "type": "text", "text": "resize feats ( feats ) Downsample the first feat and upsample last feat in feats. ", "page_idx": 369, "bbox": [96, 370.1820373535156, 345, 395.59649658203125], "page_size": [612.0, 792.0]}
{"layout": 3851, "type": "text", "text": "class  mmdet.models.dense heads. SSDHead ( num_classe  $s{=}8O$  ,  in channels  $\\mathbf{:=}$  (512, 1024, 512, 256, 256, 256) , ", "page_idx": 369, "bbox": [71.99986267089844, 400.0700378417969, 531.2765502929688, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 3852, "type": "text", "text": "stacked con vs  $\\mathord{\\breve{=}}0$  ,  feat channels=256 ,  use depth wise=False , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $=$  None ,  act_cfg=None , anchor generator={'base size ratio range': (0.1, 0.9), 'input_size': 300, 'ratios': ([2], [2, 3], [2, 3], [2, 3], [2], [2]), 'scale_major': False, 'strides': [8, 16, 32, 64, 100, 300], 'type': 'SSD Anchor Generator'} ,  bbox_coder={'clip border': True, 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box=False , train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  init_cfg  $=_{l}$  {'bias': 0, 'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) ", "page_idx": 369, "bbox": [275, 412.0250244140625, 539.2863159179688, 532.9717407226562], "page_size": [612.0, 792.0]}
{"layout": 3853, "type": "text", "text": "SSD head used in  https://arxiv.org/abs/1512.02325 . ", "page_idx": 369, "bbox": [96, 531.726318359375, 302.8831481933594, 545.036376953125], "page_size": [612.0, 792.0]}
{"layout": 3854, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 369, "bbox": [117, 551, 168, 562], "page_size": [612.0, 792.0]}
{"layout": 3855, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of conv layers in cls and reg tower. Default: 0. •  feat channels  ( int ) – Number of hidden channels when stacked con vs   $>0$  . Default: 256. •  use depth wise  ( bool ) – Whether to use Depth wise Separable Con v. Default: False. •  conv_cfg  ( dict ) – Dictionary to construct and config conv layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: None. •  act_cfg  ( dict ) – Dictionary to construct and config activation layer. Default: None. ", "page_idx": 369, "bbox": [145, 567.59130859375, 521, 718.3863525390625], "page_size": [612.0, 792.0]}
{"layout": 3856, "type": "text", "text": "•  anchor generator  ( dict ) – Config dict for anchor generator •  bbox_coder  ( dict ) – Config of bounding box coder. •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 370, "bbox": [145, 71.45246887207031, 518, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 3857, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 370, "bbox": [96, 214.76499938964844, 299, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3858, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 370, "bbox": [137, 244.1748046875, 521, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 3859, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 370, "bbox": [137, 276, 172, 288], "page_size": [612.0, 792.0]}
{"layout": 3860, "type": "text", "text": "cls_scores (list[Tensor]): Classification scores for all scale  levels, each is a 4D-tensor, the channels number is num anchors \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for all scale  levels, each is a 4D-tensor, the channels number is num anchors  $^{*}\\,4$  . ", "page_idx": 370, "bbox": [154, 291.9948425292969, 521, 347.7755126953125], "page_size": [612.0, 792.0]}
{"layout": 3861, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 370, "bbox": [136, 354, 214, 365], "page_size": [612.0, 792.0]}
{"layout": 3862, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. ", "page_idx": 370, "bbox": [96, 370.18206787109375, 458.2413330078125, 395.5965270996094], "page_size": [612.0, 792.0]}
{"layout": 3863, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 370, "bbox": [136, 401, 187, 413], "page_size": [612.0, 792.0]}
{"layout": 3864, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components. ", "page_idx": 370, "bbox": [137, 418.15252685546875, 521, 599.4322509765625], "page_size": [612.0, 792.0]}
{"layout": 3865, "type": "text", "text": "", "page_idx": 370, "bbox": [137, 608.25, 257, 616], "page_size": [612.0, 792.0]}
{"layout": 3866, "type": "text", "text": "loss single ( cls_score ,  bbox_pred ,  anchor ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) Compute loss of a single image. ", "page_idx": 370, "bbox": [96, 621.2400512695312, 483.29754638671875, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 3867, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 370, "bbox": [136, 664, 188, 676], "page_size": [612.0, 792.0]}
{"layout": 3868, "type": "text", "text": "•  cls_score  ( Tensor ) – Box scores for eachimage Has shape (num total anchors, num classes). ", "page_idx": 370, "bbox": [154, 681.1654663085938, 521, 706.4305419921875], "page_size": [612.0, 792.0]}
{"layout": 3869, "type": "text", "text": "•  bbox_pred  ( Tensor ) – Box energies / deltas for each image level with shape (num total anchors, 4). •  anchors  ( Tensor ) – Box reference for each scale level with shape (num total anchors, 4). •  labels  ( Tensor ) – Labels of each anchors with shape (num total anchors,). •  label weights ( Tensor ) – Label weights of each anchor with shape (num total anchors,) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (num total anchors, 4). •  b box weights  ( Tensor ) – BBox regression loss weights of each anchor with shape (num total anchors, 4). •  num total samples  ( int ) – If sampling, num total samples equal to the number of total anchors; Otherwise, it is the number of positive anchors. ", "page_idx": 371, "bbox": [154, 71.45246887207031, 521, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 3870, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 371, "bbox": [137, 268.0848388671875, 308.64166259765625, 282.6202697753906], "page_size": [612.0, 792.0]}
{"layout": 3871, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 371, "bbox": [137, 286.0178527832031, 256.11920166015625, 300.55328369140625], "page_size": [612.0, 792.0]}
{"layout": 3872, "type": "text", "text": "property num anchors Returns: list[int]: Number of base anchors on each point of each level. ", "page_idx": 371, "bbox": [96, 306.301025390625, 401.4838562011719, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 3873, "type": "text", "text": "class  mmdet.models.dense heads. Stage Cascade RP N Head ( in channels ,  anchor generator={'ratios': [1.0], 'scales': [8], 'strides': [4, 8, 16, 32, 64], 'type': 'Anchor Generator'} ,  adapt_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'dilation': 3, 'type': 'dilation'} ,  bridged feature  $\\mathbf{=}$  False , with_cls  $\\mathbf{=}$  True ,  sampling  $\\scriptstyle{\\tilde{}}=$  True ,  init_cfg=None , \\*\\*kwargs ) ", "page_idx": 371, "bbox": [71.99992370605469, 334.3160705566406, 530, 407.44189453125], "page_size": [612.0, 792.0]}
{"layout": 3874, "type": "text", "text": "Stage of Cascade RP N Head. ", "page_idx": 371, "bbox": [96, 406.1964416503906, 207.95989990234375, 419.5064697265625], "page_size": [612.0, 792.0]}
{"layout": 3875, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 371, "bbox": [117, 426, 169, 436], "page_size": [612.0, 792.0]}
{"layout": 3876, "type": "text", "text": "•  in channels  ( int ) – Number of channels in the input feature map. •  anchor generator  ( dict ) – anchor generator config. •  adapt_cfg  ( dict ) – adaptation config. •  bridged feature  ( bool, optional ) – whether update rpn feature. Default: False. •  with_cls  ( bool, optional ) – whether use classification branch. Default: True. •  sampling  ( bool, optional ) – whether use sampling. Default: True. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 371, "bbox": [145, 442.0624694824219, 518, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 3877, "type": "text", "text": "anchor offset ( anchor list ,  anchor strides ,  feat map sizes ) Get offset for deformable conv based on anchor shape NOTE: currently support deformable kernel size  $_{:=3}$  and dilation  $^{=1}$  ", "page_idx": 371, "bbox": [96, 567.4420776367188, 540, 604.8115234375], "page_size": [612.0, 792.0]}
{"layout": 3878, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 371, "bbox": [136, 611, 188, 623], "page_size": [612.0, 792.0]}
{"layout": 3879, "type": "text", "text": "•  anchor list  ( list[list[tensor]) ) – [NI, NLVL, NA, 4] list of multi-level anchors •  anchor strides  ( list[int] ) – anchor stride of each level ", "page_idx": 371, "bbox": [154, 627.3674926757812, 518, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 3880, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 371, "bbox": [136, 665, 173, 676], "page_size": [612.0, 792.0]}
{"layout": 3881, "type": "text", "text": "[NLVL, NA, 2, 18]: offset of DeformConv  kernel. ", "page_idx": 371, "bbox": [154, 680.537841796875, 361.324951171875, 695.0732421875], "page_size": [612.0, 792.0]}
{"layout": 3882, "type": "text", "text": "Return type  offset list (list[tensor]) ", "page_idx": 371, "bbox": [137, 698.4708251953125, 284.512451171875, 713.0062255859375], "page_size": [612.0, 792.0]}
{"layout": 3883, "type": "text", "text": "forward ( feats ,  offset_lis  $\\mathbf{\\dot{=}}$  None ) Forward function. ", "page_idx": 372, "bbox": [96, 71.30303192138672, 233, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3884, "type": "text", "text": "forward single ( x ,  offset ) Forward function of single scale. ", "page_idx": 372, "bbox": [96, 101.19103240966797, 250.00155639648438, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 3885, "type": "text", "text": "get_bboxes ( anchor list ,  cls_scores ,  bbox_preds ,  img_metas ,  cfg ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Get proposal predict. ", "page_idx": 372, "bbox": [96, 131.0790252685547, 428.4433288574219, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 3886, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 372, "bbox": [136, 163, 187, 174], "page_size": [612.0, 792.0]}
{"layout": 3887, "type": "text", "text": "•  anchor list  ( list[list] ) – Multi level anchors of each image. •  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. ", "page_idx": 372, "bbox": [154, 179.04847717285156, 521, 317.88751220703125], "page_size": [612.0, 792.0]}
{"layout": 3888, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 372, "bbox": [136, 324, 173, 335], "page_size": [612.0, 792.0]}
{"layout": 3889, "type": "text", "text": "Labeled boxes in shape (n, 5), where the first 4 columns  are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. ", "page_idx": 372, "bbox": [154, 339.81585693359375, 521, 365.7085266113281], "page_size": [612.0, 792.0]}
{"layout": 3890, "type": "text", "text": "Return type  Tensor ", "page_idx": 372, "bbox": [137.4549102783203, 369.703857421875, 220.14422607421875, 384.2392883300781], "page_size": [612.0, 792.0]}
{"layout": 3891, "type": "text", "text": "get targets ( anchor list ,  valid flag list ,  gt_bboxes ,  img_metas ,  feat map sizes ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , label channels  $\\imath\\!=\\!I$  ) Compute regression and classification targets for anchors. ", "page_idx": 372, "bbox": [96, 388.1150817871094, 528, 425.4845275878906], "page_size": [612.0, 792.0]}
{"layout": 3892, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 372, "bbox": [136, 431, 188, 443], "page_size": [612.0, 792.0]}
{"layout": 3893, "type": "text", "text": "•  anchor list  ( list[list] ) – Multi level anchors of each image. •  valid flag list  ( list[list] ) – Multi level valid flags of each image. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  feat map sizes  ( list[Tensor] ) – Feature mapsize each level •  gt b boxes ignore  ( list[Tensor] ) – Ignore bboxes of each images •  label channels  ( int ) – Channel of label. ", "page_idx": 372, "bbox": [154, 448.04052734375, 463, 568.9465942382812], "page_size": [612.0, 792.0]}
{"layout": 3894, "type": "text", "text": "", "page_idx": 372, "bbox": [135, 578.25, 266, 586], "page_size": [612.0, 792.0]}
{"layout": 3895, "type": "text", "text": "loss ( anchor list ,  valid flag list ,  cls_scores ,  bbox_preds ,  gt_bboxes ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. ", "page_idx": 372, "bbox": [96, 591.3531494140625, 528, 616.7676391601562], "page_size": [612.0, 792.0]}
{"layout": 3896, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 372, "bbox": [136, 623, 188, 634], "page_size": [612.0, 792.0]}
{"layout": 3897, "type": "text", "text": "•  anchor list  ( list[list] ) – Multi level anchors of each image. •  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) ", "page_idx": 372, "bbox": [154, 639.3225708007812, 521, 712.4086303710938], "page_size": [612.0, 792.0]}
{"layout": 3898, "type": "text", "text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None ", "page_idx": 373, "bbox": [155, 71.45246887207031, 530, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 3899, "type": "text", "text": "Returns  A dictionary of loss components. Return type  dict[str, Tensor] ", "page_idx": 373, "bbox": [137, 160.48785400390625, 310, 192.95626831054688], "page_size": [612.0, 792.0]}
{"layout": 3900, "type": "text", "text": "loss single ( cls_score ,  bbox_pred ,  anchors ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) Loss function on single scale. ", "page_idx": 373, "bbox": [96, 196.83201599121094, 487, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 3901, "type": "text", "text": "refine b boxes ( anchor list ,  bbox_preds ,  img_metas ) Refine bboxes through stages. ", "page_idx": 373, "bbox": [96, 238.67503356933594, 319.27728271484375, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 3902, "type": "text", "text": "region targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  feat map sizes , gt b boxes ignore list=None ,  gt labels list  $\\leftleftarrows$  None ,  label channels=1 , un map outputs $\\mathbf{=}$ True)See  Stage Cascade RP N Head.get targets() . ", "page_idx": 373, "bbox": [96, 268.56304931640625, 456, 317.8874816894531], "page_size": [612.0, 792.0]}
{"layout": 3903, "type": "text", "text": "class  mmdet.models.dense heads. VFNetHead ( num classes ,  in channels ,  regress_ranges=((- 1, 64), (64, ", "page_idx": 373, "bbox": [71.99989318847656, 322.36102294921875, 530, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 3904, "type": "text", "text": "128), (128, 256), (256, 512), (512, 100000000.0)) , center sampling  $=$  False ,  center sample radius=1.5 , sync num pos  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ,  gradient mu  $l{=}0.1$  , b box norm type  $=$  'reg_denom' ,  loss cls fl={'alpha': 0.25, 'gamma': 2.0, 'loss weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True} ,  use_vfl=True ,  loss_cls={'alpha': 0.75, 'gamma': 2.0, 'i ou weighted': True, 'loss weight': 1.0, 'type': 'Var i focal Loss', 'use s igm oid': True} , loss_bbox={'loss weight': 1.5, 'type': 'GIoULoss'} , loss b box refine={'loss weight': 2.0, 'type': 'GIoULoss'} , norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  use_atss  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ,  reg decoded b box=True , anchor generator={'center offset': 0.0, 'octave base scale': 8, 'ratios': [1.0], 'scales_per_octave': 1, 'strides': [8, 16, 32, 64, 128], 'type': 'Anchor Generator'} ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'vfnet_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} , \\*\\*kwargs ) ", "page_idx": 373, "bbox": [286, 334.3160095214844, 530, 550.9036865234375], "page_size": [612.0, 792.0]}
{"layout": 3905, "type": "text", "text": "Head of \\`Var i focal Net (VFNet): An IoU-aware Dense Object Detec- tor.<https://arxiv.org/abs/2008.13367>\\`_ . ", "page_idx": 373, "bbox": [96, 549.0316162109375, 540, 575.5220336914062], "page_size": [612.0, 792.0]}
{"layout": 3906, "type": "text", "text": "The VFNet predicts IoU-aware classification scores which mix the object presence confidence and object local- ization accuracy as the detection score. It is built on the FCOS architecture and uses ATSS for defining posi- tive/negative training examples. The VFNet is trained with Varifocal Loss and empolys star-shaped deformable convolution to extract features for a bbox. ", "page_idx": 373, "bbox": [96, 579.5462036132812, 540, 628.7222900390625], "page_size": [612.0, 792.0]}
{"layout": 3907, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 373, "bbox": [117, 634, 169, 646], "page_size": [612.0, 792.0]}
{"layout": 3908, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  regress ranges  ( tuple[tuple[int, int]] ) – Regress range of multiple level points. •  center sampling  ( bool ) – If true, use center sampling. Default: False. ", "page_idx": 373, "bbox": [145, 651.2772216796875, 530, 718.3862915039062], "page_size": [612.0, 792.0]}
{"layout": 3909, "type": "text", "text": "•  center sample radius  ( float ) – Radius of center sampling. Default: 1.5. •  sync num pos  ( bool ) – If true, synchronize the number of positive examples across GPUs. Default: True •  gradient mul  ( float ) – The multiplier to gradients from bbox refinement and recognition. Default: 0.1. •  b box norm type  ( str ) – The bbox normalization type, ‘reg_denom’ or ‘stride’. Default: reg_denom •  loss cls fl  ( dict ) – Config of focal loss. •  use_vfl  ( bool ) – If true, use varifocal loss for training. Default: True. •  loss_cls  ( dict ) – Config of varifocal loss. •  loss_bbox  ( dict ) – Config of localization loss, GIoU Loss. •  loss_bbox  – Config of localization refinement loss, GIoU Loss. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $=$  dict(type  $\\risingdotseq$  ’GN’, num_groups  $\\scriptstyle=32$  , requires grad  $\\risingdotseq$  True). •  use_atss  ( bool ) – If true, use ATSS to define positive/negative examples. Default: True. •  anchor generator  ( dict ) – Config of anchor generator for ATSS. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict.\n\n ", "page_idx": 374, "bbox": [145, 71.45246887207031, 518, 347.7754821777344], "page_size": [612.0, 792.0]}
{"layout": 3910, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 374, "bbox": [95, 367, 139, 380], "page_size": [612.0, 792.0]}
{"layout": 3911, "type": "text", "text": ">>>  self  $=$   VFNetHead( 11 ,  7 )\n\n  $>>$   feats  $=$   [torch . rand( 1 ,  7 , s, s)  for  s  in  [ 4 ,  8 ,  16 ,  32 ,  64 ]]\n\n  $>>$   cls_score, bbox_pred, b box p red refine  $=$   self . forward(feats)\n\n  $>>$   assert  len (cls_score)  $==$   len ( self . scales) ", "page_idx": 374, "bbox": [95, 395.2139892578125, 431.69012451171875, 441.989013671875], "page_size": [612.0, 792.0]}
{"layout": 3912, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 374, "bbox": [95, 454.113037109375, 298.6688537597656, 479.52850341796875], "page_size": [612.0, 792.0]}
{"layout": 3913, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 374, "bbox": [137, 483.5228271484375, 521, 509.41650390625], "page_size": [612.0, 792.0]}
{"layout": 3914, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 374, "bbox": [136, 515, 173, 527], "page_size": [612.0, 792.0]}
{"layout": 3915, "type": "text", "text": "cls_scores (list[Tensor]): Box iou-aware scores for each scale  level, each is a 4D-tensor, the channel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box offsets for each  scale level, each is a 4D-tensor, the chan- nel number is num_points  $^{\\ast}\\,4$  . b box p reds refine (list[Tensor]): Refined Box offsets for  each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . Return type  tuple ", "page_idx": 374, "bbox": [137, 531.3438110351562, 521, 635.543212890625], "page_size": [612.0, 792.0]}
{"layout": 3916, "type": "text", "text": "forward single (  $\\acute{x}$  ,  scale ,  scale refine ,  stride ,  reg_denom ) Forward features of a single scale level. ", "page_idx": 374, "bbox": [95, 639.4190063476562, 340.47833251953125, 664.83349609375], "page_size": [612.0, 792.0]}
{"layout": 3917, "type": "text", "text": "Parameters •  x  ( Tensor ) – FPN feature maps of the specified stride. ", "page_idx": 374, "bbox": [137, 668.828857421875, 382.5744934082031, 700.698486328125], "page_size": [612.0, 792.0]}
{"layout": 3918, "type": "text", "text": "•  (  ( scale refine ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. •  (  – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the refined bbox prediction. •  stride  ( int ) – The corresponding stride for feature maps, used to normalize the bbox prediction when b box norm type  $=$   ‘stride’. •  reg_denom  ( int ) – The corresponding regression range for feature maps, only used to normalize the bbox prediction when b box norm type  $=$  ‘reg_denom’. ", "page_idx": 375, "bbox": [154, 71.30303192138672, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 3919, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 375, "bbox": [137, 180, 173, 192], "page_size": [612.0, 792.0]}
{"layout": 3920, "type": "text", "text": "iou-aware cls scores for each box, bbox predictions and  refined bbox predictions of input feature maps. ", "page_idx": 375, "bbox": [154, 196.35382080078125, 521, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 3921, "type": "text", "text": "Return type  tuple ", "page_idx": 375, "bbox": [137, 226.2418212890625, 213, 240.77725219726562], "page_size": [612.0, 792.0]}
{"layout": 3922, "type": "text", "text": "get anchors ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get anchors according to feature map sizes. ", "page_idx": 375, "bbox": [96, 244.6529998779297, 329.8333435058594, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 3923, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 375, "bbox": [136, 276, 188, 287], "page_size": [612.0, 792.0]}
{"layout": 3924, "type": "text", "text": "•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – Device for returned tensors ", "page_idx": 375, "bbox": [154, 292.62249755859375, 427.93402099609375, 341.7985534667969], "page_size": [612.0, 792.0]}
{"layout": 3925, "type": "text", "text": "Returns  anchor list (list[Tensor]): Anchors of each image. valid flag list (list[Tensor]): Valid flags of each image. ", "page_idx": 375, "bbox": [137, 345.79388427734375, 521, 371.6865539550781], "page_size": [612.0, 792.0]}
{"layout": 3926, "type": "text", "text": "Return type  tuple ", "page_idx": 375, "bbox": [137, 375.6808776855469, 213, 390.21630859375], "page_size": [612.0, 792.0]}
{"layout": 3927, "type": "text", "text": "get at s s targets ( cls_scores ,  ml vl points ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{=}$  None ) A wrapper for computing ATSS targets for points in multiple images. ", "page_idx": 375, "bbox": [96, 394.09210205078125, 521, 419.5065612792969], "page_size": [612.0, 792.0]}
{"layout": 3928, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 375, "bbox": [136, 425, 187, 437], "page_size": [612.0, 792.0]}
{"layout": 3929, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box iou-aware scores for each scale level with shape (N, num_points \\* num classes, H, W). •  ml vl points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). Default: None. ", "page_idx": 375, "bbox": [154, 442.06256103515625, 521, 604.8125610351562], "page_size": [612.0, 792.0]}
{"layout": 3930, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 375, "bbox": [137, 611, 173, 622], "page_size": [612.0, 792.0]}
{"layout": 3931, "type": "text", "text": "labels list (list[Tensor]): Labels of each level. label weights (Tensor): Label weights of all levels. b box targets list (list[Tensor]): Regression targets of each level, (l, t, r, b). b box weights (Tensor): Bbox weights of all levels. ", "page_idx": 375, "bbox": [154, 627.3674926757812, 521, 688.4985961914062], "page_size": [612.0, 792.0]}
{"layout": 3932, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 375, "bbox": [137, 697.25, 214, 706], "page_size": [612.0, 792.0]}
{"layout": 3933, "type": "text", "text": "get fco s targets ( points ,  gt b boxes list ,  gt labels list ) Compute FCOS regression and classification targets for points in multiple images. ", "page_idx": 376, "bbox": [96, 71.30303192138672, 445.706787109375, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 3934, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 376, "bbox": [136, 102, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 3935, "type": "text", "text": "•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). Returns  labels (list[Tensor]): Labels of each level. label weights: None, to be compatible with ATSS targets. b box targets (list[Tensor]): BBox targets of each level. b box weights: None, to be compatible with ATSS targets. ", "page_idx": 376, "bbox": [137, 119.27247619628906, 521, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 3936, "type": "text", "text": "Return type  tuple ", "page_idx": 376, "bbox": [137, 238.19683837890625, 213, 252.73226928710938], "page_size": [612.0, 792.0]}
{"layout": 3937, "type": "text", "text": "get targets ( cls_scores ,  ml vl points ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore ) A wrapper for computing ATSS and FCOS targets for points in multiple images. ", "page_idx": 376, "bbox": [96, 256.6080322265625, 467.5063171386719, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 3938, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 376, "bbox": [136, 288, 187, 299], "page_size": [612.0, 792.0]}
{"layout": 3939, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box iou-aware scores for each scale level with shape (N, num_points \\* num classes, H, W). •  ml vl points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). ", "page_idx": 376, "bbox": [154, 304.5775146484375, 521, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 3940, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 376, "bbox": [136, 473, 172, 485], "page_size": [612.0, 792.0]}
{"layout": 3941, "type": "text", "text": "labels list (list[Tensor]): Labels of each level. label weights (Tensor/None): Label weights of all levels. b box targets list (list[Tensor]): Regression targets of each level, (l, t, r, b). b box weights (Tensor/None): Bbox weights of all levels. ", "page_idx": 376, "bbox": [154, 489.883544921875, 521, 551.0135498046875], "page_size": [612.0, 792.0]}
{"layout": 3942, "type": "text", "text": "Return type  tuple ", "page_idx": 376, "bbox": [137, 555.0089111328125, 213, 569.5443115234375], "page_size": [612.0, 792.0]}
{"layout": 3943, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  b box p reds refine ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute loss of the head. ", "page_idx": 376, "bbox": [96, 573.4201049804688, 536.4782104492188, 598.8345336914062], "page_size": [612.0, 792.0]}
{"layout": 3944, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 376, "bbox": [136, 605, 187, 616], "page_size": [612.0, 792.0]}
{"layout": 3945, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box iou-aware scores for each scale level, each is a 4D- tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box offsets for each scale level, each is a 4D-tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  b box p reds refine  ( list[Tensor] ) – Refined Box offsets for each scale level, each is a 4D-tensor, the channel number is num_points  $^{*}\\,4$  . ", "page_idx": 376, "bbox": [154, 621.3894653320312, 521, 706.4305419921875], "page_size": [612.0, 792.0]}
{"layout": 3946, "type": "text", "text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None. ", "page_idx": 377, "bbox": [154, 71.45246887207031, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 3947, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 377, "bbox": [137, 178.42083740234375, 308.6416931152344, 192.95626831054688], "page_size": [612.0, 792.0]}
{"layout": 3948, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 377, "bbox": [137, 196.35382080078125, 256.1192626953125, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 3949, "type": "text", "text": "property num anchors Returns: int: Number of anchors on each point of feature map. ", "page_idx": 377, "bbox": [96, 216.63796997070312, 367.7196044921875, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 3950, "type": "text", "text": "star dc n offset ( bbox_pred ,  gradient mul ,  stride ) Compute the star deformable conv offsets. ", "page_idx": 377, "bbox": [96, 244.6529998779297, 313.63336181640625, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 3951, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 377, "bbox": [136, 276, 188, 287], "page_size": [612.0, 792.0]}
{"layout": 3952, "type": "text", "text": "•  bbox_pred  ( Tensor ) – Predicted bbox distance offsets (l, r, t, b). •  gradient mul  ( float ) – Gradient multiplier. •  stride  ( int ) – The corresponding stride for feature maps, used to project the bbox onto the feature map. Returns  The offsets for deformable convolution. ", "page_idx": 377, "bbox": [137, 292.62249755859375, 521, 372.2843017578125], "page_size": [612.0, 792.0]}
{"layout": 3953, "type": "text", "text": "Return type  dc n offsets (Tensor) ", "page_idx": 377, "bbox": [137, 375.6808776855469, 274.5400695800781, 390.21630859375], "page_size": [612.0, 792.0]}
{"layout": 3954, "type": "text", "text": "transform b box targets ( decoded b boxes ,  ml vl points ,  num_imgs ) Transform b box targets (x1, y1, x2, y2) into (l, t, r, b) format. ", "page_idx": 377, "bbox": [96, 394.09210205078125, 386, 419.5065612792969], "page_size": [612.0, 792.0]}
{"layout": 3955, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 377, "bbox": [136, 425, 187, 437], "page_size": [612.0, 792.0]}
{"layout": 3956, "type": "text", "text": "•  decoded b boxes  ( list[Tensor] ) – Regression targets of each level, in the form of (x1, y1, x2, y2). •  ml vl points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  num_imgs  ( int ) – the number of images in a batch. ", "page_idx": 377, "bbox": [154, 442.06256103515625, 521, 515.1486206054688], "page_size": [612.0, 792.0]}
{"layout": 3957, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 377, "bbox": [137, 521, 172, 532], "page_size": [612.0, 792.0]}
{"layout": 3958, "type": "text", "text": "Regression targets of each level in  the form of (l, t, r, b). Return type  b box targets (list[Tensor]) ", "page_idx": 377, "bbox": [137, 537.075927734375, 386, 569.5443115234375], "page_size": [612.0, 792.0]}
{"layout": 3959, "type": "text", "text": "class  mmdet.models.dense heads. YOLACTHead ( num classes ,  in channels , ", "page_idx": 377, "bbox": [71.99989318847656, 573.4201049804688, 397.3695373535156, 586.8795776367188], "page_size": [612.0, 792.0]}
{"layout": 3960, "type": "text", "text": "anchor generator={'octave base scale': 3, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 1, 'strides': [8, 16, 32, 64, 128], 'type': 'Anchor Generator'} ,  loss_cls={'loss weight': 1.0, 'reduction': 'none', 'type': 'Cross Entropy Loss', 'use s igm oid': False}, loss_bbox $\\mathbf{\\beta}=$ {'beta': 1.0, 'loss_weight': 1.5, 'type':'Smooth L 1 Loss'} ,  num head con vs=1 ,  num_protos=32 , use_ohem  $\\mathbf{\\beta}=$  True ,  conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg=None , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'bias': 0, 'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ,  \\*\\*kwargs ) ", "page_idx": 377, "bbox": [291, 585.3751220703125, 539, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 3961, "type": "text", "text": "YOLACT box head used in  https://arxiv.org/abs/1904.02689 . ", "page_idx": 377, "bbox": [96, 693.1205444335938, 340.8504333496094, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 3962, "type": "text", "text": "Note that YOLACT head is a light version of RetinaNet head. Four differences are described as follows: ", "page_idx": 378, "bbox": [96, 71.45246887207031, 510.29510498046875, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3963, "type": "text", "text": "1. YOLACT box head has three-times fewer anchors. 2. YOLACT box head shares the convs for box and cls branches. 3. YOLACT box head uses OHEM instead of Focal loss. 4. YOLACT box head predicts a set of mask coefficients for each box. ", "page_idx": 378, "bbox": [106, 89.38447570800781, 386.8580627441406, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 3964, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 378, "bbox": [117, 168, 169, 180], "page_size": [612.0, 792.0]}
{"layout": 3965, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  anchor generator  ( dict ) – Config dict for anchor generator •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  num head con vs  ( int ) – Number of the conv layers shared by box and cls branches. •  num_protos  ( int ) – Number of the mask coefficients. •  use_ohem  ( bool ) – If true,  loss single OH EM  will be used for cls loss calculation. If false,  loss single  will be used. •  conv_cfg  ( dict ) – Dictionary to construct and config conv layer. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 378, "bbox": [145, 185.02638244628906, 518.0848388671875, 389.6184387207031], "page_size": [612.0, 792.0]}
{"layout": 3966, "type": "text", "text": "forward single  $(x)$  ", "text_level": 1, "page_idx": 378, "bbox": [96, 402, 185, 413], "page_size": [612.0, 792.0]}
{"layout": 3967, "type": "text", "text": "Forward feature of a single scale level. ", "page_idx": 378, "bbox": [118, 412.1744079589844, 272.2780456542969, 425.48443603515625], "page_size": [612.0, 792.0]}
{"layout": 3968, "type": "text", "text": "Parameters  x  ( Tensor ) – Features of a single scale level. Returns  cls_score (Tensor): Cls scores for a single scale level the channels number is num anchors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num anchors   $^{*}\\,4$  . coeff_pred (Tensor): Mask coefficients for a single scale level, the channels number is num anchors \\* num_protos. Return type  tuple ", "page_idx": 378, "bbox": [137, 429.4797668457031, 521, 515.7461547851562], "page_size": [612.0, 792.0]}
{"layout": 3969, "type": "text", "text": "get_bboxes ( cls_scores ,  bbox_preds ,  co eff p reds ,  img_metas ,  cfg  $=$  None ,  rescale  $:=$  False ) “Similar to func: AnchorHead.get_bboxes , but additionally processes co eff p reds. ", "page_idx": 378, "bbox": [96, 519.6220092773438, 456, 545.0364379882812], "page_size": [612.0, 792.0]}
{"layout": 3970, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 378, "bbox": [136, 551, 187, 563], "page_size": [612.0, 792.0]}
{"layout": 3971, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level with shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  co eff p reds  ( list[Tensor] ) – Mask coefficients for each scale level with shape (N, num anchors \\* num_protos, H, W)•  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  cfg  ( mmcv.Config | None ) – Test / post processing configuration, if None, test_cfg would be used ", "page_idx": 378, "bbox": [154, 567.5913696289062, 521, 712.408447265625], "page_size": [612.0, 792.0]}
{"layout": 3972, "type": "text", "text": "•  rescale  ( bool ) – If True, return boxes in original image space. Default: False. ", "page_idx": 379, "bbox": [154, 71.45246887207031, 482, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 3973, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 379, "bbox": [136, 91, 172, 102], "page_size": [612.0, 792.0]}
{"layout": 3974, "type": "text", "text": "Each item in result list is  a 3-tuple. The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is an (n,) tensor where each item is the predicted class label of the corresponding box. The third item is an (n, num_protos) tensor where each item is the predicted mask coefficients of instance inside the corresponding box. ", "page_idx": 379, "bbox": [154, 106.6898193359375, 521, 168.4485321044922], "page_size": [612.0, 792.0]}
{"layout": 3975, "type": "text", "text": "Return type  list[tuple[Tensor, Tensor, Tensor]] ", "page_idx": 379, "bbox": [137, 172.44384765625, 328.1787414550781, 186.97927856445312], "page_size": [612.0, 792.0]}
{"layout": 3976, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) A combination of the func: AnchorHead.loss  and func: SSDHead.loss . ", "page_idx": 379, "bbox": [96, 190.8540496826172, 458.2413330078125, 216.26951599121094], "page_size": [612.0, 792.0]}
{"layout": 3977, "type": "text", "text": "When  self.use_ohem  $==$   True , it functions like  SSDHead.loss , otherwise, it follows  AnchorHead. loss . Besides, it additionally returns  sampling results . ", "page_idx": 379, "bbox": [118, 220.89149475097656, 539.9999389648438, 246.1565399169922], "page_size": [612.0, 792.0]}
{"layout": 3978, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 379, "bbox": [136, 252, 187, 264], "page_size": [612.0, 792.0]}
{"layout": 3979, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. Default: None Returns  dict[str, Tensor]: A dictionary of loss components. List[:obj: Sampling Result ]: Sam- pler results for each image. ", "page_idx": 379, "bbox": [137, 268.7124938964844, 521, 461.3495178222656], "page_size": [612.0, 792.0]}
{"layout": 3980, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 379, "bbox": [137, 468, 214, 479], "page_size": [612.0, 792.0]}
{"layout": 3981, "type": "text", "text": "loss single OH EM ( cls_score ,  bbox_pred ,  anchors ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) “See func: SSDHead.loss . ", "page_idx": 379, "bbox": [96, 483.7560729980469, 515, 521.1255493164062], "page_size": [612.0, 792.0]}
{"layout": 3982, "type": "text", "text": "class  mmdet.models.dense heads. YO L ACT Proton et ( num classes ,  in channels  $:=$  256 ,  proto channels=(256, 256, 256, None, 256, 32) ,  proto kernel sizes=(3, 3, 3, - 2, 3, 1) ,  include last re lu  $=$  True ,  num_protos  $\\scriptstyle{\\varepsilon=32}$  , loss mask weight=1.0 ,  max masks to train  $\\scriptstyle{\\mathcal{S}}$  100 , init_cfg  $=$  {'distribution': 'uniform', 'override': {'name': 'protonet'}, 'type': 'Xavier'} ) ", "page_idx": 379, "bbox": [71.99990844726562, 525.5990600585938, 534.5643920898438, 598.7249145507812], "page_size": [612.0, 792.0]}
{"layout": 3983, "type": "text", "text": "YOLACT mask head used in  https://arxiv.org/abs/1904.02689 This head outputs the mask prototypes for YOLACT. ", "page_idx": 379, "bbox": [96, 597.4794311523438, 344.796142578125, 610.7894897460938], "page_size": [612.0, 792.0]}
{"layout": 3984, "type": "text", "text": "", "page_idx": 379, "bbox": [96, 615.4124755859375, 308, 628.7225341796875], "page_size": [612.0, 792.0]}
{"layout": 3985, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 379, "bbox": [117, 635, 169, 646], "page_size": [612.0, 792.0]}
{"layout": 3986, "type": "text", "text": "•  in channels  ( int ) – Number of channels in the input feature map. •  proto channels  ( tuple[int] ) – Output channels of protonet convs. •  proto kernel sizes  ( tuple[int] ) – Kernel sizes of protonet convs. •  include last re lu  ( Bool ) – If keep the last relu of protonet. ", "page_idx": 379, "bbox": [145, 651.2774658203125, 444.0434875488281, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 3987, "type": "text", "text": "•  num_protos  ( int ) – Number of prototypes. •  num classes  ( int ) – Number of categories excluding the background category. •  loss mask weight  ( float ) – Reweight the mask loss by this factor. •  max masks to train  ( int ) – Maximum number of masks to train for each image. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 380, "bbox": [145, 71.45246887207031, 490.7091369628906, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 3988, "type": "text", "text": "crop ( masks ,  boxes ,  padding  $\\scriptstyle{:=I}$  ) Crop predicted masks by zeroing out everything not in the predicted bbox. ", "page_idx": 380, "bbox": [96, 160.9659881591797, 414.404296875, 186.38145446777344], "page_size": [612.0, 792.0]}
{"layout": 3989, "type": "text", "text": "Parameters •  masks  ( Tensor ) – shape [H, W, N]. •  boxes  ( Tensor ) – bbox coords in relative point form with shape [N, 4]. Returns  The cropped masks. Return type  Tensor ", "page_idx": 380, "bbox": [137, 190.37579345703125, 450, 276.6421813964844], "page_size": [612.0, 792.0]}
{"layout": 3990, "type": "text", "text": "forward ( x ,  coeff_pred ,  bboxes ,  img_meta ,  sampling results  $\\mathbf{\\beta}=$  None ) Forward feature from the upstream network to get prototypes and linearly combine the prototypes, using masks coefficients, into instance masks. Finally, crop the instance masks with given bboxes. ", "page_idx": 380, "bbox": [96, 280.5179748535156, 540.0038452148438, 317.8874206542969], "page_size": [612.0, 792.0]}
{"layout": 3991, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 380, "bbox": [136, 323, 188, 335], "page_size": [612.0, 792.0]}
{"layout": 3992, "type": "text", "text": "•  x  ( Tensor ) – Feature from the upstream network, which is a 4D-tensor. •  coeff_pred  ( list[Tensor] ) – Mask coefficients for each scale level with shape (N, num anchors \\* num_protos, H, W).•  bboxes  ( list[Tensor] ) – Box used for cropping with shape (N, num anchors \\* 4, H, W). During training, they are ground truth boxes. During testing, they are predicted boxes. •  img_meta  ( list[dict] ) – Meta information of each image, e.g., image size, scaling fac- tor, etc. •  sampling results  (List[:obj: Sampling Result ]) – Sampler results for each image. Returns  Predicted instance segmentation masks. ", "page_idx": 380, "bbox": [137, 340.44342041015625, 521, 479.8802185058594], "page_size": [612.0, 792.0]}
{"layout": 3993, "type": "text", "text": "Return type  list[Tensor] ", "page_idx": 380, "bbox": [137, 483.2778015136719, 238.81427001953125, 497.813232421875], "page_size": [612.0, 792.0]}
{"layout": 3994, "type": "text", "text": "get seg masks ( mask_pred ,  label_pred ,  img_meta ,  rescale ) Resize, binarize, and format the instance mask predictions. ", "page_idx": 380, "bbox": [96, 501.68902587890625, 353.0556945800781, 527.1034545898438], "page_size": [612.0, 792.0]}
{"layout": 3995, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 380, "bbox": [136, 533, 188, 545], "page_size": [612.0, 792.0]}
{"layout": 3996, "type": "text", "text": "•  mask_pred  ( Tensor ) – shape (N, H, W). •  label_pred  ( Tensor ) – shape (N, ). •  img_meta  ( dict ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If rescale is False, then returned masks will fit the scale of imgs[0]. Returns  Mask predictions grouped by their predicted classes. Return type  list[ndarray] ", "page_idx": 380, "bbox": [137, 549.6583862304688, 518.1357421875, 653.2301635742188], "page_size": [612.0, 792.0]}
{"layout": 3997, "type": "text", "text": "get targets ( mask_pred ,  gt_masks ,  pos assigned gt in ds ) Compute instance segmentation targets for each image. ", "page_idx": 380, "bbox": [96, 657.1060180664062, 344, 682.5204467773438], "page_size": [612.0, 792.0]}
{"layout": 3998, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 380, "bbox": [136, 688, 188, 700], "page_size": [612.0, 792.0]}
{"layout": 3999, "type": "text", "text": "•  mask_pred  ( Tensor ) – Predicted prototypes with shape (num classes, H, W). ", "page_idx": 380, "bbox": [155, 705.076416015625, 478, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 4000, "type": "text", "text": "•  gt_masks  ( Tensor ) – Ground truth masks for each image with the same shape of the input image. •  pos assigned gt in ds  ( Tensor ) – GT indices of the corresponding positive samples. ", "page_idx": 381, "bbox": [154, 71.45246887207031, 521, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4001, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 381, "bbox": [136, 121, 173, 132], "page_size": [612.0, 792.0]}
{"layout": 4002, "type": "text", "text": "Instance segmentation targets with shape  (num instances, H, W). Return type  Tensor ", "page_idx": 381, "bbox": [137, 136.57781982421875, 429.0102844238281, 169.04623413085938], "page_size": [612.0, 792.0]}
{"layout": 4003, "type": "text", "text": "loss ( mask_pred ,  gt_masks ,  gt_bboxes ,  img_meta ,  sampling results ) Compute loss of the head. ", "page_idx": 381, "bbox": [96, 172.92198181152344, 375.79534912109375, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 4004, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 381, "bbox": [136, 204, 187, 215], "page_size": [612.0, 792.0]}
{"layout": 4005, "type": "text", "text": "•  mask_pred  ( list[Tensor] ) – Predicted prototypes with shape (num classes, H, W). •  gt_masks  ( list[Tensor] ) – Ground truth masks for each image with the same shape of the input image. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  img_meta  ( list[dict] ) – Meta information of each image, e.g., image size, scaling fac- tor, etc. •  sampling results  (List[:obj: Sampling Result ]) – Sampler results for each image. ", "page_idx": 381, "bbox": [154, 220.8914337158203, 521, 341.7984619140625], "page_size": [612.0, 792.0]}
{"layout": 4006, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 381, "bbox": [137, 345.7937927246094, 308.6416931152344, 360.3292236328125], "page_size": [612.0, 792.0]}
{"layout": 4007, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 381, "bbox": [137, 363.7257995605469, 256.1192321777344, 378.26123046875], "page_size": [612.0, 792.0]}
{"layout": 4008, "type": "text", "text": "sanitize coordinates  $(x l,x2$  ,  img_size ,  padding  $\\scriptstyle{\\prime=0}$  ,  cast=True ) Sanitizes the input coordinates so that   $\\tt X1<x2$  ,   $\\mathrm{x}1\\stackrel{!}{=}\\mathrm{x}2$  ,   $\\mathrm{x}1>=0$  , and   $\\mathbf{x}2<=$  image_size. Also converts from relative to absolute coordinates and casts the results to long tensors. ", "page_idx": 381, "bbox": [96, 382.13702392578125, 540.0027465820312, 419.5064697265625], "page_size": [612.0, 792.0]}
{"layout": 4009, "type": "text", "text": "Warning: this does things in-place behind the scenes so copy if necessary. ", "page_idx": 381, "bbox": [118, 424.12945556640625, 413, 437.4394836425781], "page_size": [612.0, 792.0]}
{"layout": 4010, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 381, "bbox": [136, 443, 188, 455], "page_size": [612.0, 792.0]}
{"layout": 4011, "type": "text", "text": "•  _x1  ( Tensor ) – shape (N, ). •  _x2  ( Tensor ) – shape (N, ). •  img_size  ( int ) – Size of the input image. •  padding    $(i n t)-\\mathrm{x}1>=$   padding,  $\\mathbf{X}2<=$   image_size-padding. •  cast  ( bool ) – If cast is false, the result won’t be cast to longs. ", "page_idx": 381, "bbox": [154, 459.9954833984375, 413, 545.0365600585938], "page_size": [612.0, 792.0]}
{"layout": 4012, "type": "text", "text": "Returns  x1 (Tensor): Sanitized _x1. x2 (Tensor): Sanitized  $\\_{\\mathrm{X}2}$  . ", "page_idx": 381, "bbox": [137, 549.0308837890625, 397.7669677734375, 563.5662841796875], "page_size": [612.0, 792.0]}
{"layout": 4013, "type": "text", "text": "Return type  tuple ", "page_idx": 381, "bbox": [137, 566.9638671875, 213.2500762939453, 581.499267578125], "page_size": [612.0, 792.0]}
{"layout": 4014, "type": "text", "text": "simple test ( feats ,  det_bboxes ,  det_labels ,  det_coeffs ,  img_metas ,  rescale  $\\mathbf{\\dot{\\rho}}=$  False ) Test function without test-time augmentation. ", "page_idx": 381, "bbox": [96, 585.3750610351562, 433.1352233886719, 610.78955078125], "page_size": [612.0, 792.0]}
{"layout": 4015, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 381, "bbox": [136, 617, 188, 628], "page_size": [612.0, 792.0]}
{"layout": 4016, "type": "text", "text": "•  feats  ( tuple[torch.Tensor] ) – Multi-level features from the upstream network, each is a 4D-tensor. •  det_bboxes  ( list[Tensor] ) – BBox results of each image. each element is (n, 5) tensor, where 5 represent (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1. •  det_labels  ( list[Tensor] ) – BBox results of each image. each element is (n, ) tensor, each element represents the class label of the corresponding box. ", "page_idx": 381, "bbox": [154, 633.345458984375, 521, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 4017, "type": "text", "text": "•  det_coeffs  ( list[Tensor] ) – BBox coefficient of each image. each element is (n, m) tensor, m is vector length. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 382, "bbox": [154, 71.45246887207031, 521, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 4018, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 382, "bbox": [136, 151, 173, 162], "page_size": [612.0, 792.0]}
{"layout": 4019, "type": "text", "text": "encoded masks. The c-th item in the outer list  corresponds to the c-th class. Given the c- th outer list, the i-th item in that inner list is the mask for the i-th box with class label c. ", "page_idx": 382, "bbox": [154, 166.4658203125, 521, 204.3135223388672], "page_size": [612.0, 792.0]}
{"layout": 4020, "type": "text", "text": "Return type  list[list] ", "page_idx": 382, "bbox": [137, 208.308837890625, 224.02975463867188, 222.84426879882812], "page_size": [612.0, 792.0]}
{"layout": 4021, "type": "text", "text": "class  mmdet.models.dense heads. YO LAC TSe gm Head ( num classes ,  in channels=256 , loss_segm={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'distribution': 'uniform', 'override': {'name': 'segm_conv'}, 'type': 'Xavier'} ) ", "page_idx": 382, "bbox": [71.99998474121094, 226.7200164794922, 529.0154418945312, 287.8909606933594], "page_size": [612.0, 792.0]}
{"layout": 4022, "type": "text", "text": "YOLACT segmentation head used in  https://arxiv.org/abs/1904.02689 ", "page_idx": 382, "bbox": [96, 286.6455078125, 376.8955993652344, 299.9555358886719], "page_size": [612.0, 792.0]}
{"layout": 4023, "type": "text", "text": "Apply a semantic segmentation loss on feature space using layers that are only evaluated during training to increase performance with no speed penalty. ", "page_idx": 382, "bbox": [96, 304.5775146484375, 540.0033569335938, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 4024, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 382, "bbox": [117, 336, 169, 347], "page_size": [612.0, 792.0]}
{"layout": 4025, "type": "text", "text": "•  in channels  ( int ) – Number of channels in the input feature map. •  num classes  ( int ) – Number of categories excluding the background category. •  loss_segm  ( dict ) – Config of semantic segmentation loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 382, "bbox": [145, 352.3985290527344, 477, 419.506591796875], "page_size": [612.0, 792.0]}
{"layout": 4026, "type": "text", "text": "forward  $(x)$  Forward feature from the upstream network. ", "page_idx": 382, "bbox": [96, 425, 294, 449.39459228515625], "page_size": [612.0, 792.0]}
{"layout": 4027, "type": "text", "text": "Parameters  x  ( Tensor ) – Feature from the upstream network, which is a 4D-tensor. ", "page_idx": 382, "bbox": [137, 453.3899230957031, 477, 467.92535400390625], "page_size": [612.0, 792.0]}
{"layout": 4028, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 382, "bbox": [137, 474, 172, 485], "page_size": [612.0, 792.0]}
{"layout": 4029, "type": "text", "text": "Predicted semantic segmentation map with shape  (N, num classes, H, W). Return type  Tensor ", "page_idx": 382, "bbox": [137, 489.2559509277344, 466.91876220703125, 521.723388671875], "page_size": [612.0, 792.0]}
{"layout": 4030, "type": "text", "text": "get targets ( segm_pred ,  gt_masks ,  gt_labels ) Compute semantic segmentation targets for each image. ", "page_idx": 382, "bbox": [96, 525.5991821289062, 340.921142578125, 551.0136108398438], "page_size": [612.0, 792.0]}
{"layout": 4031, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 382, "bbox": [136, 557, 188, 569], "page_size": [612.0, 792.0]}
{"layout": 4032, "type": "text", "text": "•  segm_pred  ( Tensor ) – Predicted semantic segmentation map with shape (num classes, H, W). •  gt_masks  ( Tensor ) – Ground truth masks for each image with the same shape of the input image. •  gt_labels  ( Tensor ) – Class indices corresponding to each box. ", "page_idx": 382, "bbox": [154, 573.569580078125, 521, 646.6556396484375], "page_size": [612.0, 792.0]}
{"layout": 4033, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 382, "bbox": [136, 653, 173, 663], "page_size": [612.0, 792.0]}
{"layout": 4034, "type": "text", "text": "Semantic segmentation targets with shape  (num classes, H, W). ", "page_idx": 382, "bbox": [154, 668.5829467773438, 427, 683.1183471679688], "page_size": [612.0, 792.0]}
{"layout": 4035, "type": "text", "text": "Return type  Tensor ", "page_idx": 382, "bbox": [137, 686.5159912109375, 220, 701.0513916015625], "page_size": [612.0, 792.0]}
{"layout": 4036, "type": "text", "text": "loss ( segm_pred ,  gt_masks ,  gt_labels ) Compute loss of the head. ", "page_idx": 383, "bbox": [96, 71.30303192138672, 256, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4037, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 383, "bbox": [136, 103, 187, 114], "page_size": [612.0, 792.0]}
{"layout": 4038, "type": "text", "text": "•  segm_pred  ( list[Tensor] ) – Predicted semantic segmentation map with shape (N, num classes, H, W). •  gt_masks  ( list[Tensor] ) – Ground truth masks for each image with the same shape of the input image. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box. ", "page_idx": 383, "bbox": [154, 119.27247619628906, 521, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 4039, "type": "text", "text": "Returns  A dictionary of loss components. Return type  dict[str, Tensor] ", "page_idx": 383, "bbox": [137, 196.35382080078125, 308.6416931152344, 228.82223510742188], "page_size": [612.0, 792.0]}
{"layout": 4040, "type": "text", "text": "simple test ( feats ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test function without test-time augmentation. ", "page_idx": 383, "bbox": [96, 232.69700622558594, 300.0446472167969, 258.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 4041, "type": "text", "text": "class  mmdet.models.dense heads. YOLOFHead ( num classes ,  in channels ,  num cls con vs  $\\scriptstyle{\\prime=2}$  , num reg con vs  $\\scriptstyle{\\prime}=4$  ,  norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} ,  \\*\\*kwargs ) YOLOFHead Paper link:  https://arxiv.org/abs/2103.09460 . ", "page_idx": 383, "bbox": [71.99995422363281, 262.58502197265625, 528.66650390625, 311.91046142578125], "page_size": [612.0, 792.0]}
{"layout": 4042, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 383, "bbox": [117, 318, 169, 329], "page_size": [612.0, 792.0]}
{"layout": 4043, "type": "text", "text": "•  num classes  ( int ) – The number of object classes (w/o background) •  in channels  ( List[int] ) – The number of input channels per scale. •  cls num con vs  ( int ) – The number of convolutions of cls branch. Default 2. •  reg num con vs  ( int ) – The number of convolutions of reg branch. Default 4. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. ", "page_idx": 383, "bbox": [145, 334.4654541015625, 470, 419.50653076171875], "page_size": [612.0, 792.0]}
{"layout": 4044, "type": "text", "text": "forward single ( feature ) Forward feature of a single scale level. Parameters  x  ( Tensor ) – Features of a single scale level. Returns  cls_score (Tensor): Cls scores for a single scale level the channels number is num base priors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num base priors  $^{*}\\,4$  . Return type  tuple ", "page_idx": 383, "bbox": [96, 423.9800720214844, 521, 527.7012939453125], "page_size": [612.0, 792.0]}
{"layout": 4045, "type": "text", "text": "get targets ( cls scores list ,  b box p reds list ,  anchor list ,  valid flag list ,  gt b boxes list ,  img_metas , gt b boxes ignore lis  $\\mathbf{\\dot{\\rho}}=$  None ,  gt labels list  $\\leftrightharpoons$  None ,  label channel  $\\scriptstyle{\\mathfrak{s}}=I$  ,  un map outputs  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ) Compute regression and classification targets for anchors in multiple images. ", "page_idx": 383, "bbox": [96, 531.5770874023438, 534.2862548828125, 568.946533203125], "page_size": [612.0, 792.0]}
{"layout": 4046, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 383, "bbox": [136, 574, 187, 587], "page_size": [612.0, 792.0]}
{"layout": 4047, "type": "text", "text": "•  cls scores list  ( list[Tensor] ) – each image. each is a 4D-tensor, the shape is (h \\* w, num anchors \\* num classes). •  b box p reds list  ( list[Tensor] ) – each is a 4D-tensor, the shape is   $(\\textbf{h}^{*}\\ \\textbf{w},$  , num anchors  $^{*}\\,4$  ). •  anchor list  ( list[Tensor] ) – Anchors of each image. Each element of is a tensor of shape   $(\\mathbf{h}^{\\ast}\\mathbf{\\Sigma}^{\\ast}\\mathbf{w}^{\\ast}$   num anchors, 4). •  valid flag list  ( list[Tensor] ) – Valid flags of each image. Each element of is a tensor of shape   $(\\mathbf{h}^{\\mathbf{\\alpha}*}\\mathbf{w}^{\\mathbf{\\alpha}*}$   num anchors, ) ", "page_idx": 383, "bbox": [154, 591.5025024414062, 521, 706.4305419921875], "page_size": [612.0, 792.0]}
{"layout": 4048, "type": "text", "text": "•  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. ", "page_idx": 384, "bbox": [154, 71.45246887207031, 508.61651611328125, 174.4254608154297], "page_size": [612.0, 792.0]}
{"layout": 4049, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 384, "bbox": [136, 180, 173, 193], "page_size": [612.0, 792.0]}
{"layout": 4050, "type": "text", "text": "• batch labels (Tensor): Label of all images. Each element of is a tensor of shape (batch, h \\* w \\* num anchors) • batch label weights (Tensor): Label weights of all images of is a tensor of shape (batch, h \\* w \\* num anchors) • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. ", "page_idx": 384, "bbox": [154, 214.9143829345703, 521, 305.9324645996094], "page_size": [612.0, 792.0]}
{"layout": 4051, "type": "text", "text": "additional returns: This function enables user-defined returns from self.get targets single . These returns are currently refined to properties at each feature map (i.e. having HxW dimension). The results will be concatenated after the end ", "page_idx": 384, "bbox": [154, 315.9057922363281, 521, 353.7534484863281], "page_size": [612.0, 792.0]}
{"layout": 4052, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 384, "bbox": [137, 365, 215, 377], "page_size": [612.0, 792.0]}
{"layout": 4053, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 384, "bbox": [96, 384.00994873046875, 206, 407.55145263671875], "page_size": [612.0, 792.0]}
{"layout": 4054, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. ", "page_idx": 384, "bbox": [96, 412.0249938964844, 458.2412414550781, 437.439453125], "page_size": [612.0, 792.0]}
{"layout": 4055, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 384, "bbox": [136, 443, 187, 455], "page_size": [612.0, 792.0]}
{"layout": 4056, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (batch, num anchors \\* num classes, h, w) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (batch, num anchors \\* 4, h, w) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None ", "page_idx": 384, "bbox": [154, 459.9954528808594, 521, 622.7444458007812], "page_size": [612.0, 792.0]}
{"layout": 4057, "type": "text", "text": "Returns  A dictionary of loss components. ", "page_idx": 384, "bbox": [137, 626.7398071289062, 308.6415710449219, 641.2752075195312], "page_size": [612.0, 792.0]}
{"layout": 4058, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 384, "bbox": [137, 644.6727905273438, 256.119140625, 659.2081909179688], "page_size": [612.0, 792.0]}
{"layout": 4059, "type": "text", "text": "class  mmdet.models.dense heads. YOLOV3Head ( num classes ,  in channels ,  out_channels=(1024, 512, 256) , anchor_generator={'base_sizes': [[(116, 90), (156, 198), (373, 326)], [(30, 61), (62, 45), (59, 119)], [(10, 13), (16, 30), (33, 23)]], 'strides': [32, 16, 8], 'type': 'YO LO Anchor Generator'} ,  bbox_coder={'type': 'YO LOB Box Code r'} ,  feat map strides=[32, 16, 8] , one hot smoother=0.0 ,  conv_cfg=None , norm_cfg  $\\mathrm{=}$  {'requires grad': True, 'type': 'BN'} , act_cfg  $=$  {'negative slope': 0.1, 'type': 'LeakyReLU'} , loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  loss_conf={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_xy={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  loss_wh={'loss weight': 1.0, 'type': 'MSELoss'} ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'override': {'name': 'convs_pred'}, 'std': 0.01, 'type': 'Normal'} ) ", "page_idx": 385, "bbox": [72.0, 71.30303192138672, 538.7785034179688, 275.9351501464844], "page_size": [612.0, 792.0]}
{"layout": 4060, "type": "text", "text": "YOLOV3Head Paper link:  https://arxiv.org/abs/1804.02767 . ", "page_idx": 385, "bbox": [96, 274.6907043457031, 338.34979248046875, 288.000732421875], "page_size": [612.0, 792.0]}
{"layout": 4061, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 385, "bbox": [117, 294, 168, 305], "page_size": [612.0, 792.0]}
{"layout": 4062, "type": "text", "text": "•  num classes  ( int ) – The number of object classes (w/o background) •  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( List[int] ) – The number of output channels per scale before the final 1x1 layer. Default: (1024, 512, 256). •  anchor generator  ( dict ) – Config dict for anchor generator •  bbox_coder  ( dict ) – Config of bounding box coder. •  feat map strides  ( List[int] ) – The stride of each scale. Should be in descending order. Default: (32, 16, 8). •  one hot smoother  ( float ) – Set a non-zero value to enable label-smooth Default: 0. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $='$  ’BN’, requires grad  $\\scriptstyle\\varepsilon=$  True) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $=^{!}$  ’LeakyReLU’, nega- tive_slope  $\\mathrm{{=}}0.1$  ). •  loss_cls  ( dict ) – Config of classification loss. •  loss_conf  ( dict ) – Config of confidence loss. •  loss_xy  ( dict ) – Config of xy coordinate loss. •  loss_wh  ( dict ) – Config of wh coordinate loss. •  train_cfg  ( dict ) – Training config of YOLOV3 head. Default: None. •  test_cfg  ( dict ) – Testing config of YOLOV3 head. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 385, "bbox": [145, 310.55572509765625, 521, 658.6107788085938], "page_size": [612.0, 792.0]}
{"layout": 4063, "type": "text", "text": "aug_test ( feats ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test function with test time augmentation. Parameters ", "page_idx": 385, "bbox": [96, 663.0843505859375, 286.48468017578125, 707.0285034179688], "page_size": [612.0, 792.0]}
{"layout": 4064, "type": "text", "text": "•  feats  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains features for all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. ", "page_idx": 386, "bbox": [154, 71.45246887207031, 521, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 4065, "type": "text", "text": "Returns  bbox results of each class ", "page_idx": 386, "bbox": [137, 148.5328369140625, 277.7676086425781, 163.06826782226562], "page_size": [612.0, 792.0]}
{"layout": 4066, "type": "text", "text": "Return type  list[ndarray] ", "page_idx": 386, "bbox": [137, 166.4658203125, 242.37088012695312, 181.00125122070312], "page_size": [612.0, 792.0]}
{"layout": 4067, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 386, "bbox": [96, 184.8769989013672, 298, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 4068, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 386, "bbox": [137, 214.28680419921875, 521, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 4069, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 386, "bbox": [137, 246, 172, 258], "page_size": [612.0, 792.0]}
{"layout": 4070, "type": "text", "text": "A tuple of multi-level predication map, each is a  4D-tensor of shape (batch_size,  $^{5+}$  num classes, height, width). ", "page_idx": 386, "bbox": [154, 262.1068115234375, 521, 288.00048828125], "page_size": [612.0, 792.0]}
{"layout": 4071, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 386, "bbox": [137, 291.99481201171875, 246.7045440673828, 306.5302429199219], "page_size": [612.0, 792.0]}
{"layout": 4072, "type": "text", "text": "get_bboxes ( pred_maps ,  img_metas ,  cfg  $=$  None ,  rescale  $=$  False ,  with_nms  $\\mathbf{:=}$  True ) Transform network output for a batch into bbox predictions. It has been accelerated since PR #5991. ", "page_idx": 386, "bbox": [96, 310.4060363769531, 518.0760498046875, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 4073, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 386, "bbox": [136, 342, 187, 353], "page_size": [612.0, 792.0]}
{"layout": 4074, "type": "text", "text": "•  pred_maps  ( list[Tensor] ) – Raw predictions for a batch of images. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  cfg  ( mmcv.Config | None ) – Test / post processing configuration, if None, test_cfg would be used. Default: None. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. ", "page_idx": 386, "bbox": [154, 358.3764953613281, 521, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 4075, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 386, "bbox": [137, 473, 173, 485], "page_size": [612.0, 792.0]}
{"layout": 4076, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is an (n, 5) tensor, where 5 represent (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1. The shape of the second tensor in the tuple is (n,), and each element represents the class label of the corresponding box. ", "page_idx": 386, "bbox": [154, 489.2558898925781, 521, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 4077, "type": "text", "text": "Return type  list[tuple[Tensor, Tensor]] ", "page_idx": 386, "bbox": [137, 531.098876953125, 298, 545.63427734375], "page_size": [612.0, 792.0]}
{"layout": 4078, "type": "text", "text": "get targets ( anchor list ,  responsible flag list ,  gt b boxes list ,  gt labels list ) Compute target maps for anchors in multiple images. ", "page_idx": 386, "bbox": [96, 549.5090942382812, 418.6902770996094, 574.9244995117188], "page_size": [612.0, 792.0]}
{"layout": 4079, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 386, "bbox": [137, 581, 187, 592], "page_size": [612.0, 792.0]}
{"layout": 4080, "type": "text", "text": "•  anchor list  ( list[list[Tensor]] ) – Multi level anchors of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num total anchors, 4). •  responsible flag list  ( list[list[Tensor]] ) – Multi level responsible flags of each image. Each element is a tensor of shape (num total anchors, ) •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. ", "page_idx": 386, "bbox": [154, 597.4794921875, 521, 700.4535522460938], "page_size": [612.0, 792.0]}
{"layout": 4081, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 386, "bbox": [136, 707, 172, 718], "page_size": [612.0, 792.0]}
{"layout": 4082, "type": "text", "text": "Usually returns a tuple containing learning targets. ", "text_level": 1, "page_idx": 387, "bbox": [153, 73, 372, 85], "page_size": [612.0, 792.0]}
{"layout": 4083, "type": "text", "text": "• target map list (list[Tensor]): Target map of each level. • ne g map list (list[Tensor]): Negative map of each level. ", "page_idx": 387, "bbox": [164, 89.38447570800781, 401, 120.62749481201172], "page_size": [612.0, 792.0]}
{"layout": 4084, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 387, "bbox": [136, 126, 216, 138], "page_size": [612.0, 792.0]}
{"layout": 4085, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 387, "bbox": [96, 144.90695190429688, 206, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 4086, "type": "text", "text": "loss ( pred_maps ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore=None ) Compute loss of the head. ", "page_idx": 387, "bbox": [96, 172.92198181152344, 410.1413879394531, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 4087, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 387, "bbox": [136, 204, 188, 215], "page_size": [612.0, 792.0]}
{"layout": 4088, "type": "text", "text": "•  pred_maps  ( list[Tensor] ) – Prediction map for each scale level, shape (N, num anchors \\* num_attrib, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss.  A dictionary of loss components. ", "page_idx": 387, "bbox": [154, 220.8914337158203, 521, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 4089, "type": "text", "text": "loss single ( pred_map ,  target_map ,  neg_map ) Compute loss of a single image from a batch. ", "page_idx": 387, "bbox": [96, 394.0920715332031, 305, 419.50653076171875], "page_size": [612.0, 792.0]}
{"layout": 4090, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 387, "bbox": [136, 425, 188, 437], "page_size": [612.0, 792.0]}
{"layout": 4091, "type": "text", "text": "•  pred_map  ( Tensor ) – Raw predictions for a single level. •  target_map  ( Tensor ) – The Ground-Truth target for a single level. •  neg_map  ( Tensor ) – The negative masks for a single level. Returns  loss_cls (Tensor): Classification loss. loss_conf (Tensor): Confidence loss. loss_xy (Tensor): Regression loss of x, y coordinate. loss_wh (Tensor): Regression loss of w, h coordinate. ", "page_idx": 387, "bbox": [137.4549560546875, 442.0625305175781, 521, 533.08056640625], "page_size": [612.0, 792.0]}
{"layout": 4092, "type": "text", "text": "Return type  tuple ", "text_level": 1, "page_idx": 387, "bbox": [137, 539.25, 215, 551], "page_size": [612.0, 792.0]}
{"layout": 4093, "type": "text", "text": "property num anchors Returns: int: Number of anchors on each point of feature map. ", "page_idx": 387, "bbox": [96, 557.360107421875, 367.7205810546875, 580.9015502929688], "page_size": [612.0, 792.0]}
{"layout": 4094, "type": "text", "text": "property num_attrib number of attributes in pred_map, bboxes (4)  $^+$   objectness (1)  $^+$   num classes ", "page_idx": 387, "bbox": [96, 587.248046875, 426.23089599609375, 610.78955078125], "page_size": [612.0, 792.0]}
{"layout": 4095, "type": "text", "text": "Type  int ", "text_level": 1, "page_idx": 387, "bbox": [137, 617, 175, 629], "page_size": [612.0, 792.0]}
{"layout": 4096, "type": "text", "text": "on nx export ( pred_maps ,  img_metas ,  with_nms  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ) Transform network output for a batch into bbox predictions. ", "page_idx": 387, "bbox": [96, 633.1961059570312, 356.78155517578125, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 4097, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 387, "bbox": [136, 664, 188, 676], "page_size": [612.0, 792.0]}
{"layout": 4098, "type": "text", "text": "•  cls_scores  ( list[Tensor] ) – Box scores for each scale level with shape (N, num_points \\* num classes, H, W). ", "page_idx": 387, "bbox": [154, 681.16552734375, 521, 706.4305419921875], "page_size": [612.0, 792.0]}
{"layout": 4099, "type": "text", "text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num_points \\* 4, H, W). •  score factors  ( list[Tensor] ) – score factors for each s cale level with shape (N, num_points \\* 1, H, W). Default: None. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. Default: None. •  with_nms  ( bool ) – Whether apply nms to the bboxes. Default: True. ", "page_idx": 388, "bbox": [154, 71.45246887207031, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 4100, "type": "text", "text": "Returns  When  with_nms  is True, it is tuple[Tensor, Tensor], first tensor bboxes with shape [N, num_det, 5], 5 arrange as (x1, y1, x2, y2, score) and second element is class labels of shape [N, num_det]. When  with_nms  is False, first tensor is bboxes with shape [N, num_det, 4], second tensor is raw score has shape [N, num_det, num classes]. ", "page_idx": 388, "bbox": [137, 178.42083740234375, 521, 228.2245330810547], "page_size": [612.0, 792.0]}
{"layout": 4101, "type": "text", "text": "Return type  tuple[Tensor, Tensor] | list[tuple] ", "page_idx": 388, "bbox": [137, 232.2188720703125, 323, 246.75430297851562], "page_size": [612.0, 792.0]}
{"layout": 4102, "type": "text", "text": "( num classes ,  in channels ,  feat channel  $\\imath{=}256$  , stacked con vs  $^{=2}$  ,  strides=[8, 16, 32] ,  use depth wise  $\\mathbf{=}$  False , dc n on last con v  $\\mathbf{=}$  False ,  conv_bias  $=$  'auto' ,  conv_cfg  $\\mathbf{\\hat{\\mu}}$  None , norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} , act_cfg  $=$  {'type': 'Swish'} ,  loss_cls={'loss weight': 1.0, 'reduction': 'sum', 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  loss_bbox={'eps': 1e-16, 'loss_weight': 5.0, 'mode': 'square', 'reduction': 'sum', 'type': 'IoULoss'} , loss_obj={'loss weight': 1.0, 'reduction': 'sum', 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_l1={'loss weight': 1.0, 'reduction': 'sum', 'type': 'L1Loss'} ,  train_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  test_cfg  $=$  None ,  init_cfg={'a': 2.23606797749979, 'distribution': 'uniform', 'layer': 'Conv2d', 'mode': 'fan_in', 'non linearity': 'leaky_relu', 'type': 'Kaiming'} ) ", "page_idx": 388, "bbox": [281.2249755859375, 250.6300506591797, 539.2962646484375, 419.3968200683594], "page_size": [612.0, 792.0]}
{"layout": 4103, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 388, "bbox": [117, 438, 168, 448], "page_size": [612.0, 792.0]}
{"layout": 4104, "type": "text", "text": "•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels in stacking convs. Default: 256 •  stacked con vs  ( int ) – Number of stacking convs of the head. Default: 2. •  strides  ( tuple ) – Downsample factor of each feature map. •  use depth wise  ( bool ) – Whether to depthwise separable convolution in blocks. Default: False •  dc n on last con v  ( bool ) – If true, use dcn in the last layer of towers. Default: False. •  conv_bias  ( bool | str ) – If specified as  auto , it will be decided by the norm_cfg. Bias of conv will be set as True if  norm_cfg  is None, otherwise False. Default: “auto”. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  act_cfg  ( dict ) – Config dict for activation layer. Default: None. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. ", "page_idx": 388, "bbox": [145, 454.01739501953125, 521, 706.4314575195312], "page_size": [612.0, 792.0]}
{"layout": 4105, "type": "text", "text": "•  loss_obj  ( dict ) – Config of objectness loss. •  loss_l1  ( dict ) – Config of L1 loss. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 389, "bbox": [145, 71.45246887207031, 463, 156.4934539794922], "page_size": [612.0, 792.0]}
{"layout": 4106, "type": "text", "text": "forward ( feats ) Forward features from the upstream network. ", "page_idx": 389, "bbox": [96, 160.9659881591797, 298.66888427734375, 186.38145446777344], "page_size": [612.0, 792.0]}
{"layout": 4107, "type": "text", "text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. ", "page_idx": 389, "bbox": [137, 190.37579345703125, 521, 216.2684783935547], "page_size": [612.0, 792.0]}
{"layout": 4108, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 389, "bbox": [137, 222, 173, 234], "page_size": [612.0, 792.0]}
{"layout": 4109, "type": "text", "text": "A tuple of multi-level predication map, each is a  4D-tensor of shape (batch_size,  $^{5+}$  num classes, height, width). Return type  tuple[Tensor] ", "page_idx": 389, "bbox": [137, 238.19677734375, 521, 282.6202087402344], "page_size": [612.0, 792.0]}
{"layout": 4110, "type": "text", "text": "forward single ( x ,  cls_convs ,  reg_convs ,  conv_cls ,  conv_reg ,  conv_obj ) Forward feature of a single scale level. ", "page_idx": 389, "bbox": [96, 286.4960021972656, 397.369384765625, 311.91046142578125], "page_size": [612.0, 792.0]}
{"layout": 4111, "type": "text", "text": "get_bboxes ( cls_scores ,  bbox_preds ,  object ness es ,  img_metas  $=$  None ,  cfg  $=$  None ,  rescale=False , with_nms  $\\backsimeq$  True ) Transform network outputs of a batch into bbox results. :param cls_scores: Classification scores for all scale levels, each is a 4D-tensor, has shape (batch_size, num_priors \\* num classes, H, W). ", "page_idx": 389, "bbox": [96, 316.3840026855469, 527.7786865234375, 371.68646240234375], "page_size": [612.0, 792.0]}
{"layout": 4112, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 389, "bbox": [136, 383, 187, 395], "page_size": [612.0, 792.0]}
{"layout": 4113, "type": "text", "text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  object ness es  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, 1, H, W). •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. ", "page_idx": 389, "bbox": [154, 400.2194519042969, 521, 539.0584716796875], "page_size": [612.0, 792.0]}
{"layout": 4114, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 389, "bbox": [136, 545, 173, 556], "page_size": [612.0, 792.0]}
{"layout": 4115, "type": "text", "text": "Each item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. ", "page_idx": 389, "bbox": [154, 560.98681640625, 521, 610.7894897460938], "page_size": [612.0, 792.0]}
{"layout": 4116, "type": "text", "text": "Return type  list[list[Tensor, Tensor]] ", "page_idx": 389, "bbox": [137, 614.7847900390625, 288, 629.3201904296875], "page_size": [612.0, 792.0]}
{"layout": 4117, "type": "text", "text": "in it weights()", "text_level": 1, "page_idx": 389, "bbox": [96, 641, 172, 652], "page_size": [612.0, 792.0]}
{"layout": 4118, "type": "text", "text": "Initialize the weights. ", "page_idx": 389, "bbox": [118, 651.2774047851562, 204.5830841064453, 664.5874633789062], "page_size": [612.0, 792.0]}
{"layout": 4119, "type": "text", "text": "loss ( cls_scores ,  bbox_preds ,  object ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute loss of the head. :param cls_scores: Box scores for each scale level, ", "page_idx": 389, "bbox": [96, 669.06103515625, 513.0263061523438, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 4120, "type": "text", "text": "each is a 4D-tensor, the channel number is num_priors \\* num classes. ", "page_idx": 389, "bbox": [137, 699.0984497070312, 417.5235900878906, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 4121, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 390, "bbox": [136, 73, 188, 85], "page_size": [612.0, 792.0]}
{"layout": 4122, "type": "text", "text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_priors   $^{*}\\,4.$  . •  object ness es  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, 1, H, W). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 390, "bbox": [155, 89.38447570800781, 521, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 4123, "type": "text", "text": "39.5 roi_heads ", "text_level": 1, "page_idx": 390, "bbox": [70, 274, 176, 292], "page_size": [612.0, 792.0]}
{"layout": 4124, "type": "text", "text": "( with avg poo  $\\leftrightharpoons$  False ,  with_cls=True ,  with_reg=True , roi feat size=7 ,  in channels=256 ,  num classes=80 , bbox_coder={'clip border': True, 'target means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [0.1, 0.1, 0.2, 0.2], 'type': 'Delta XY WH B Box Code r'} ,  reg class agnostic=False , reg decoded b box  $\\mathbf{\\beta}=$  False ,  reg predictor cf g={'type': 'Linear'} , cls predictor cf g={'type': 'Linear'} ,  loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': False} , loss_bbox={'beta': 1.0, 'loss weight': 1.0, 'type': 'Smooth L 1 Loss'} init_cfg  $\\scriptstyle\\in$  None ) ", "page_idx": 390, "bbox": [265.53399658203125, 306.5470275878906, 535, 427.4927978515625], "page_size": [612.0, 792.0]}
{"layout": 4125, "type": "text", "text": "Simplest RoI head, with only two fc layers for classification and regression respectively. ", "page_idx": 390, "bbox": [96, 426.2473449707031, 448, 439.557373046875], "page_size": [612.0, 792.0]}
{"layout": 4126, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 390, "bbox": [96, 447, 148, 456], "page_size": [612.0, 792.0]}
{"layout": 4127, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 390, "bbox": [118, 456.1353454589844, 313.48382568359375, 487.3783874511719], "page_size": [612.0, 792.0]}
{"layout": 4128, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 390, "bbox": [118, 503.328857421875, 540, 541.176513671875], "page_size": [612.0, 792.0]}
{"layout": 4129, "type": "text", "text": "get_bboxes ( rois ,  cls_score ,  bbox_pred ,  img_shape ,  scale factor ,  rescale  $\\mathbf{=}$  False ,  cfg=None ) Transform network output for a batch into bbox predictions. ", "page_idx": 390, "bbox": [96, 557.6050415039062, 472.49835205078125, 583.0194702148438], "page_size": [612.0, 792.0]}
{"layout": 4130, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 390, "bbox": [136, 590, 187, 601], "page_size": [612.0, 792.0]}
{"layout": 4131, "type": "text", "text": "•  rois  ( Tensor ) – Boxes to be transformed. Has shape (num_boxes, 5). last dimension 5 arrange as (batch index, x1, y1, x2, y2). •  cls_score  ( Tensor ) – Box scores, has shape (num_boxes, num classes  $+\\;1$  ). •  bbox_pred  ( Tensor, optional ) – Box energies / deltas. has shape (num_boxes, num classes \\* 4). •  img_shape  ( Sequence[int], optional ) – Maximum bounds for boxes, specifies (H, W, C) or (H, W). ", "page_idx": 390, "bbox": [155, 605.575439453125, 521, 708.5494995117188], "page_size": [612.0, 792.0]}
{"layout": 4132, "type": "text", "text": "•  scale factor  ( ndarray ) – Scale factor of the image arrange as (w_scale, h_scale, w_scale, h_scale). •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  (obj  ( cfg ) –  ConfigDict ):  test_cfg  of Bbox Head. Default: None Returns  First tensor is  det_bboxes , has the shape (num_boxes, 5) and last dimension 5 represent (tl_x, tl_y, br_x, br_y, score). Second tensor is the labels with shape (num_boxes, ). ", "page_idx": 391, "bbox": [137, 71.45246887207031, 521, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 4133, "type": "text", "text": "", "page_idx": 391, "bbox": [136, 172.25, 280, 180], "page_size": [612.0, 792.0]}
{"layout": 4134, "type": "text", "text": "get targets ( sampling results ,  gt_bboxes ,  gt_labels ,  r cnn train cf g ,  conca  $t{=}$  True ) Calculate the ground truth for all samples in a batch according to the sampling results. ", "page_idx": 391, "bbox": [96, 184.8769989013672, 464.45635986328125, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 4135, "type": "text", "text": "Almost the same as the implementation in bbox_head, we passed additional parameters pos in ds list and ne g in ds list to  get target single  function. ", "page_idx": 391, "bbox": [118, 214.91444396972656, 540.002685546875, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 4136, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 391, "bbox": [137, 246, 187, 257], "page_size": [612.0, 792.0]}
{"layout": 4137, "type": "text", "text": "•  (List[obj  ( sampling results ) – Sampling Results]): Assign results of all images in a batch after sampling. •  gt_bboxes  ( list[Tensor] ) – Gt_bboxes of all images in a batch, each tensor has shape (num_gt, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  gt_labels  ( list[Tensor] ) – Gt_labels of all images in a batch, each tensor has shape (num_gt,). •  (obj  ( r cnn train cf g ) – ConfigDict):  train_cfg  of RCNN. •  concat  ( bool ) – Whether to concatenate the results of all the images in a single batch. ", "page_idx": 391, "bbox": [154, 262.7344665527344, 521, 383.6415100097656], "page_size": [612.0, 792.0]}
{"layout": 4138, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 391, "bbox": [137, 390, 172, 401], "page_size": [612.0, 792.0]}
{"layout": 4139, "type": "text", "text": "Ground truth for proposals in a single image. Containing the following list of Tensors: ", "page_idx": 391, "bbox": [154, 406.1965026855469, 497.2735595703125, 419.50653076171875], "page_size": [612.0, 792.0]}
{"layout": 4140, "type": "text", "text": "• labels (list[Tensor],Tensor): Gt_labels for all proposals in a batch, each tensor in list has shape (num proposals,) when  conca  $\\leftleftarrows$  False , otherwise just a single tensor has shape (num all proposals,). • label weights (list[Tensor]): Labels weights for all proposals in a batch, each tensor in list has shape (num proposals,) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals,). • b box targets (list[Tensor],Tensor): Regression target for all proposals in a batch, each ten- sor in list has shape (num proposals, 4) when  conca  $\\mathbf{\\dot{\\rho}}=$  False , otherwise just a single tensor has shape (num all proposals, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. • b box weights (list[tensor],Tensor): Regression weights for all proposals in a batch, each tensor in list has shape (num proposals, 4) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals, 4). ", "page_idx": 391, "bbox": [154, 424.1295166015625, 521, 586.8795166015625], "page_size": [612.0, 792.0]}
{"layout": 4141, "type": "text", "text": "Return type  Tuple[Tensor] ", "page_idx": 391, "bbox": [137, 590.873779296875, 251, 605.4091796875], "page_size": [612.0, 792.0]}
{"layout": 4142, "type": "text", "text": "on nx export ( rois ,  cls_score ,  bbox_pred ,  img_shape ,  cfg  $\\mathbf{\\hat{\\mu}}$  None ,  \\*\\*kwargs ) Transform network output for a batch into bbox predictions. ", "page_idx": 391, "bbox": [96, 609.2850341796875, 405.6042175292969, 634.6995239257812], "page_size": [612.0, 792.0]}
{"layout": 4143, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 391, "bbox": [136, 640, 188, 652], "page_size": [612.0, 792.0]}
{"layout": 4144, "type": "text", "text": "•  rois  ( Tensor ) – Boxes to be transformed. Has shape (B, num_boxes, 5) •  cls_score  ( Tensor ) – Box scores. has shape (B, num_boxes, num classes  $+\\;1$  ), 1 repre- sent the background. ", "page_idx": 391, "bbox": [154, 657.2554321289062, 521, 700.4534912109375], "page_size": [612.0, 792.0]}
{"layout": 4145, "type": "text", "text": "•  bbox_pred  ( Tensor, optional ) – Box energies / deltas for, has shape (B, num_boxes, num classes   $^{*}\\,4$  ) when. •  img_shape  ( torch.Tensor ) – Shape of image. •  (obj  ( cfg ) –  ConfigDict ):  test_cfg  of Bbox Head. Default: None ", "page_idx": 392, "bbox": [154, 71.45246887207031, 521, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 4146, "type": "text", "text": "Returns ", "page_idx": 392, "bbox": [137, 136.57781982421875, 171.46730041503906, 151.11325073242188], "page_size": [612.0, 792.0]}
{"layout": 4147, "type": "text", "text": "dets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. ", "page_idx": 392, "bbox": [154, 154.51080322265625, 439.3216552734375, 169.04623413085938], "page_size": [612.0, 792.0]}
{"layout": 4148, "type": "text", "text": "Return type  tuple[Tensor, Tensor] ", "page_idx": 392, "bbox": [137, 172.44378662109375, 278.1066589355469, 186.97921752929688], "page_size": [612.0, 792.0]}
{"layout": 4149, "type": "text", "text": "refine b boxes ( rois ,  labels ,  bbox_preds ,  pos_is_gts ,  img_metas ) Refine bboxes during training. ", "page_idx": 392, "bbox": [96.90695190429688, 190.85398864746094, 365.3742980957031, 216.2694549560547], "page_size": [612.0, 792.0]}
{"layout": 4150, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 392, "bbox": [136, 222, 187, 234], "page_size": [612.0, 792.0]}
{"layout": 4151, "type": "text", "text": "•  rois  ( Tensor ) – Shape   $(\\mathtt{n}^{*}\\mathtt{b s},5)$  , where n is image number per GPU, and bs is the sampled RoIs per image. The first column is the image id and the next 4 columns are x1, y1, x2, y2. •  labels  ( Tensor ) – Shape   $(\\mathsf{n}^{\\ast}\\mathsf{b s}.$  , ). •  bbox_preds  ( Tensor ) – Shape   $(\\mathbf{n}^{*}\\mathbf{b}\\mathbf{s},4)$  ) or (n\\*bs, 4\\*#class). •  pos_is_gts  ( list[Tensor] ) – Flags indicating if each positive bbox is a gt bbox. •  img_metas  ( list[dict] ) – Meta info of each image. ", "page_idx": 392, "bbox": [154, 238.8244171142578, 521, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 4152, "type": "text", "text": "Returns  Refined bboxes of each image in a mini-batch. ", "page_idx": 392, "bbox": [137, 339.8158264160156, 360, 354.35125732421875], "page_size": [612.0, 792.0]}
{"layout": 4153, "type": "text", "text": "Return type  list[Tensor] ", "page_idx": 392, "bbox": [137, 357.74884033203125, 239, 372.2842712402344], "page_size": [612.0, 792.0]}
{"layout": 4154, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 392, "bbox": [117, 391, 161, 403], "page_size": [612.0, 792.0]}
{"layout": 4155, "type": "image", "page_idx": 392, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_155.jpg", "bbox": [114, 413, 544, 711], "page_size": [612.0, 792.0], "ocr_text": ">>>\nwoe\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n\n>>>\n\n# xdoctest: +REQUIRES (module: kwarray)\n\nimport kwarray\n\nimport numpy as np\n\nfrom mmdet.core.bbox.demodata import random_boxes\nself = BBoxHead(reg_class_agnostic=True)\n\nn_roi = 2\nn_img = 4\nscale = 512\n\nrng = np.random.RandomState(0)\nimg_metas = [{'img_shape': (scale, scale)}\nfor _ in range(n_img)]\n\n# Create rois in the expected format\n\nroi_boxes = random_boxes(n_roi, scale=scale, rng=rng)\n\nimg_ids = torch.randint(0, n_img, (n_roi,))\n\nimg_ids = img_ids.float(Q)\n\nrois = torch.cat([img_ids[:, None], roi_boxes], dim=1)\n\n# Create other args\n\nlabels = torch.randint(0, 2, (n_roi,)).longQ\n\nbbox_preds = random_boxes(n_roi, scale=scale, rng=rng)\n\n# For each image, pretend random positive boxes are gts\n\nis_label_pos = (labels.numpy(Q) > 0).astype(np.int)\n\nlbl_per_img = kwarray.group_items(is_label_pos,\nimg_ids.numpy())\n\npos_per_img = [sum(lbl_per_img.get(gid, []))\n\n", "vlm_text": "The image shows a snippet of Python code related to object detection. Here's a brief overview of the code:\n\n1. **Imports**: \n   - `kwarray`\n   - `numpy` as `np`\n   - `random_boxes` from `mmdet.core.bbox.demodata`\n\n2. **Configuration**:\n   - `BBoxHead` is initialized with `reg_class_agnostic=True`.\n   - Number of regions of interest (`n_roi`): 2\n   - Number of images (`n_img`): 4\n   - `scale`: 512\n   - Random number generator is set with a seed of 0.\n\n3. **Image metadata**:\n   - Contains 'img_shape' set to `(scale, scale)`.\n\n4. **ROI Creation**:\n   - `roi_boxes` are generated using `random_boxes`.\n   - `img_ids` are random integers between 0 and `n_img`.\n\n5. **Concatenation**:\n   - `rois` is created by concatenating `img_ids` with `roi_boxes`.\n\n6. **Labels and Predictions**:\n   - Random labels are generated.\n   - `bbox_preds` are also generated using `random_boxes`.\n\n7. **Positive Labels**:\n   - Determines positive boxes and groups items accordingly.\n   - Summarizes the number of positive labels per image in `pos_per_img`.\n\nThe code seems to be part of a test or initialization process for object detection, likely preparing data for training or evaluation purposes."}
{"layout": 4156, "type": "image", "page_idx": 393, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_156.jpg", "bbox": [113, 80, 544, 198], "page_size": [612.0, 792.0], "ocr_text": "SI: rege\n\nwee for gid in range(n_img)]\n>>> pos_is_gts = [\n\n>> torch.randint(9, 2, (npos,)).byteQ.sort(\n\n>> descending=True) [0]\n\n>>> for npos in pos_per_img\n\n>>> ]\n\n>>> bboxes_list = self.refine_bboxes(rois, labels, bbox_preds,\n>>> pos_is_gts, img_metas)\n\n>>> print (bboxes_list)\n\n", "vlm_text": "This image shows a snippet of Python code, likely part of a machine learning or computer vision program. Here's a breakdown:\n\n- It's using the PyTorch library, as indicated by `torch.randint`.\n- The loop iterates over a range given by `n_img`.\n- `pos_is_gts` is a list comprehension where random integers (0 or 1) are generated, sorted in descending order, and selected for the number of positive samples per image (`npos`).\n- `bboxes_list` is assigned by calling a method `self.refine_bboxes` with arguments `rois`, `labels`, `bbox_preds`, `pos_is_gts`, and `img_metas`.\n- Finally, it prints `bboxes_list`.\n\nThe code appears to be part of a function or script related to refining bounding boxes in an object detection task."}
{"layout": 4157, "type": "text", "text": "regress by class ( rois ,  label ,  bbox_pred ,  img_meta ) Regress the bbox for the predicted class. Used in Cascade R-CNN. ", "page_idx": 393, "bbox": [96, 205.74998474121094, 385, 231.1654510498047], "page_size": [612.0, 792.0]}
{"layout": 4158, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 393, "bbox": [136, 237, 188, 248], "page_size": [612.0, 792.0]}
{"layout": 4159, "type": "text", "text": "•  rois  ( Tensor ) – Rois from  rpn_head  or last stage  bbox_head , has shape (num proposals, 4) or (num proposals, 5). •  label  ( Tensor ) – Only used when  self.reg class agnostic  is False, has shape (num proposals, ). •  bbox_pred  ( Tensor ) – Regression prediction of current stage  bbox_head . When self.reg class agnostic  is False, it has shape (n, num classes  $^{*}\\,4$  ), otherwise it has shape (n, 4). •  img_meta  ( dict ) – Image meta info. Returns  Regressed bboxes, the same shape as input rois. ", "page_idx": 393, "bbox": [137, 253.5709686279297, 521, 387.18023681640625], "page_size": [612.0, 792.0]}
{"layout": 4160, "type": "text", "text": "class  mmdet.models.roi_heads. Base RoI Extractor ( roi_layer ,  out channels ,  feat map strides init_cfg=None ) ", "page_idx": 393, "bbox": [71.99995422363281, 408.988037109375, 475.27642822265625, 434.29290771484375], "page_size": [612.0, 792.0]}
{"layout": 4161, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 393, "bbox": [117, 453, 169, 464], "page_size": [612.0, 792.0]}
{"layout": 4162, "type": "text", "text": "•  roi_layer  ( dict ) – Specify RoI layer type and arguments. •  out channels  ( int ) – Output channels of RoI layers. •  feat map strides  ( int ) – Strides of input feature maps. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 393, "bbox": [145, 468.9134826660156, 518.08154296875, 536.0215454101562], "page_size": [612.0, 792.0]}
{"layout": 4163, "type": "text", "text": "build roi layers ( layer_cfg ,  feat map strides ) Build RoI operator to extract feature from each level feature map. ", "page_idx": 393, "bbox": [96, 540.4950561523438, 378, 565.9095458984375], "page_size": [612.0, 792.0]}
{"layout": 4164, "type": "text", "text": "•  layer_cfg  ( dict ) – Dictionary to construct and config RoI layer operation. Options are modules under  mmcv/ops  such as  RoIAlign . •  feat map strides  ( List[int] ) – The stride of input feature map w.r.t to the original image size, which would be used to scale RoI coordinate (original image coordinate system) to feature coordinate system. ", "page_idx": 393, "bbox": [154, 588.4655151367188, 521, 655.5735473632812], "page_size": [612.0, 792.0]}
{"layout": 4165, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 393, "bbox": [136, 662, 173, 672], "page_size": [612.0, 792.0]}
{"layout": 4166, "type": "text", "text": "The RoI extractor modules for each level feature  map. Return type  nn.ModuleList ", "page_idx": 393, "bbox": [137, 677.5018310546875, 385, 709.96923828125], "page_size": [612.0, 792.0]}
{"layout": 4167, "type": "text", "text": "abstract forward ( feats ,  rois ,  roi scale factor  $\\leftrightharpoons$  None ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 394, "bbox": [96, 71.30303192138672, 330, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4168, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 394, "bbox": [118, 130.59979248046875, 540, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 4169, "type": "text", "text": "property num_inputs Number of input feature maps. Type  int ", "page_idx": 394, "bbox": [96, 186.74996948242188, 242, 228.82223510742188], "page_size": [612.0, 792.0]}
{"layout": 4170, "type": "text", "text": "roi re scale ( rois ,  scale factor ) Scale RoI coordinates by scale factor. ", "page_idx": 394, "bbox": [96, 232.69700622558594, 267.93426513671875, 258.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 4171, "type": "text", "text": "Parameters •  rois  ( torch.Tensor ) – RoI (Region of Interest), shape (n, 5) •  scale factor  ( float ) – Scale factor that RoI will be multiplied by. Returns  Scaled RoI. Return type  torch.Tensor ", "page_idx": 394, "bbox": [137, 262.1068115234375, 441.5233459472656, 348.373291015625], "page_size": [612.0, 792.0]}
{"layout": 4172, "type": "text", "text": "class  mmdet.models.roi_heads. Base RoI Head ( b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head  $\\leftrightharpoons$  None , mask roi extractor  $=$  None ,  mask_head=None , shared head  $\\leftrightharpoons$  None ,  train_cfg  $=$  None ,  test_cfg  $\\leftrightharpoons$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) ", "page_idx": 394, "bbox": [72.0, 352.24908447265625, 499.62567138671875, 401.57452392578125], "page_size": [612.0, 792.0]}
{"layout": 4173, "type": "text", "text": "Base class for RoIHeads. ", "page_idx": 394, "bbox": [96, 400.219482421875, 196, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 4174, "type": "text", "text": "async a sync simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\leftrightharpoons$  None ,  rescale  $=$  False ,  \\*\\*kwargs ) A synchronized test function. ", "page_idx": 394, "bbox": [96, 418.0030517578125, 515, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 4175, "type": "text", "text": "aug_test ( x ,  proposal list ,  img_metas ,  rescale  $=$  False ,  \\*\\*kwargs ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 394, "bbox": [96, 447.8900451660156, 432.906005859375, 491.2375183105469], "page_size": [612.0, 792.0]}
{"layout": 4176, "type": "text", "text": "abstract forward train ( x ,  img_meta ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None , gt_masks  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Forward function during training. ", "page_idx": 394, "bbox": [96, 495.7110595703125, 515, 533.0805053710938], "page_size": [612.0, 792.0]}
{"layout": 4177, "type": "text", "text": "abstract in it as signer sampler () Initialize assigner and sampler. ", "page_idx": 394, "bbox": [96, 539.427001953125, 264.27874755859375, 562.968505859375], "page_size": [612.0, 792.0]}
{"layout": 4178, "type": "text", "text": "abstract in it b box head () Initialize  bbox_head ", "page_idx": 394, "bbox": [96, 569.3150024414062, 227, 592.8565063476562], "page_size": [612.0, 792.0]}
{"layout": 4179, "type": "text", "text": "abstract in it mask head () Initialize  mask_head ", "page_idx": 394, "bbox": [96, 599.2030029296875, 227, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 4180, "type": "text", "text": "simple test (  $\\dot{\\boldsymbol{x}}.$  ,  proposal list ,  img_meta ,  proposal  $\\mathfrak{s}{=}$  None ,  rescale=False ,  \\*\\*kwargs ) Test without augmentation. ", "page_idx": 394, "bbox": [96, 627.218017578125, 447.1183776855469, 652.6325073242188], "page_size": [612.0, 792.0]}
{"layout": 4181, "type": "text", "text": "property with_bbox whether the RoI head contains a  bbox_head ", "page_idx": 394, "bbox": [96, 658.97900390625, 292.48297119140625, 682.5205078125], "page_size": [612.0, 792.0]}
{"layout": 4182, "type": "text", "text": "Type  bool ", "page_idx": 394, "bbox": [137, 686.5158081054688, 180.6925048828125, 701.0512084960938], "page_size": [612.0, 792.0]}
{"layout": 4183, "type": "text", "text": "property with_mask whether the RoI head contains a  mask_head ", "page_idx": 395, "bbox": [96, 73.1760025024414, 293.8877258300781, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4184, "type": "text", "text": "Type  bool ", "page_idx": 395, "bbox": [137, 100.71282958984375, 180, 115.24826049804688], "page_size": [612.0, 792.0]}
{"layout": 4185, "type": "text", "text": "property with shared head whether the RoI head contains a  shared head ", "page_idx": 395, "bbox": [96, 120.99600982666016, 300, 144.5384979248047], "page_size": [612.0, 792.0]}
{"layout": 4186, "type": "text", "text": "Type  bool ", "page_idx": 395, "bbox": [137, 148.5328369140625, 180, 163.06826782226562], "page_size": [612.0, 792.0]}
{"layout": 4187, "type": "text", "text": "class  mmdet.models.roi_heads. Cascade RoI Head ( num_stages ,  stage loss weights , b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head=None , mask roi extractor  $\\leftrightharpoons$  None ,  mask_head  $\\mathbf{\\{}=}$  None , shared head  $\\leftrightharpoons$  None ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 395, "bbox": [72.0000228881836, 166.9440155029297, 515.3167114257812, 228.2245330810547], "page_size": [612.0, 792.0]}
{"layout": 4188, "type": "text", "text": "Cascade roi head including one bbox head and one mask head. ", "page_idx": 395, "bbox": [96, 226.86952209472656, 347, 240.17955017089844], "page_size": [612.0, 792.0]}
{"layout": 4189, "type": "text", "text": "https://arxiv.org/abs/1712.00726 ", "page_idx": 395, "bbox": [96, 244.80250549316406, 227, 258.112548828125], "page_size": [612.0, 792.0]}
{"layout": 4190, "type": "text", "text": "aug_test ( features ,  proposal list ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test with augmentations. ", "page_idx": 395, "bbox": [96, 262.5850830078125, 347, 288.00054931640625], "page_size": [612.0, 792.0]}
{"layout": 4191, "type": "text", "text": "If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 395, "bbox": [118, 292.6225280761719, 432, 305.93255615234375], "page_size": [612.0, 792.0]}
{"layout": 4192, "type": "text", "text": "forward dummy ( x ,  proposals ) Dummy forward function. ", "page_idx": 395, "bbox": [96, 310.4060974121094, 223, 335.820556640625], "page_size": [612.0, 792.0]}
{"layout": 4193, "type": "text", "text": "forward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $=$  None , gt_mask  $\\mathfrak{s}{=}$  None ) ", "page_idx": 395, "bbox": [96, 340.2940979003906, 472.18865966796875, 365.5989685058594], "page_size": [612.0, 792.0]}
{"layout": 4194, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 395, "bbox": [137, 384, 188, 395], "page_size": [612.0, 792.0]}
{"layout": 4195, "type": "text", "text": "•  x  ( list[Tensor] ) – list of multi-level img features. ", "page_idx": 395, "bbox": [155, 400.21954345703125, 375.1668701171875, 413.5295715332031], "page_size": [612.0, 792.0]}
{"layout": 4196, "type": "text", "text": "•  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . ", "page_idx": 395, "bbox": [155, 418.1525573730469, 521, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 4197, "type": "text", "text": "•  proposals  ( list[Tensors] ) – list of region proposals. ", "page_idx": 395, "bbox": [155, 471.9505310058594, 393.4234313964844, 485.26055908203125], "page_size": [612.0, 792.0]}
{"layout": 4198, "type": "text", "text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. ", "page_idx": 395, "bbox": [155, 489.883544921875, 521, 515.1485595703125], "page_size": [612.0, 792.0]}
{"layout": 4199, "type": "text", "text": "•  gt_labels  ( list[Tensor] ) – class indices corresponding to each box ", "page_idx": 395, "bbox": [155, 519.7715454101562, 452.1479187011719, 533.0816040039062], "page_size": [612.0, 792.0]}
{"layout": 4200, "type": "text", "text": " gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. ", "page_idx": 395, "bbox": [159, 537.7035522460938, 521, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 4201, "type": "text", "text": " gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. ", "page_idx": 395, "bbox": [159, 567.5914916992188, 521, 592.8565673828125], "page_size": [612.0, 792.0]}
{"layout": 4202, "type": "text", "text": "Returns  a dictionary of loss components ", "page_idx": 395, "bbox": [137, 596.8519287109375, 306, 611.3873291015625], "page_size": [612.0, 792.0]}
{"layout": 4203, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 395, "bbox": [137, 614.784912109375, 256.1192626953125, 629.3203125], "page_size": [612.0, 792.0]}
{"layout": 4204, "type": "text", "text": "in it as signer sampler () Initialize assigner and sampler for each stage. ", "page_idx": 395, "bbox": [96, 635.069091796875, 300, 658.610595703125], "page_size": [612.0, 792.0]}
{"layout": 4205, "type": "text", "text": "in it b box head ( b box roi extractor ,  bbox_head ) Initialize box head and box roi extractor. ", "page_idx": 395, "bbox": [96, 663.0841064453125, 306, 688.4985961914062], "page_size": [612.0, 792.0]}
{"layout": 4206, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 395, "bbox": [136, 694, 187, 706], "page_size": [612.0, 792.0]}
{"layout": 4207, "type": "text", "text": "•  b box roi extractor  ( dict ) – Config of box roi extractor. •  bbox_head  ( dict ) – Config of box in box head. ", "page_idx": 396, "bbox": [154, 71.45246887207031, 406.9632263183594, 102.69451141357422], "page_size": [612.0, 792.0]}
{"layout": 4208, "type": "text", "text": "in it mask head ( mask roi extractor ,  mask_head ) Initialize mask head and mask roi extractor. ", "page_idx": 396, "bbox": [96, 107.16802215576172, 308.94635009765625, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 4209, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 396, "bbox": [136, 138, 188, 150], "page_size": [612.0, 792.0]}
{"layout": 4210, "type": "text", "text": "•  mask roi extractor  ( dict ) – Config of mask roi extractor. •  mask_head  ( dict ) – Config of mask in mask head. ", "page_idx": 396, "bbox": [154, 155.13844299316406, 413.3990783691406, 186.38145446777344], "page_size": [612.0, 792.0]}
{"layout": 4211, "type": "text", "text": "simple test (  $\\cdot x,$  ,  proposal list ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test without augmentation. ", "page_idx": 396, "bbox": [96, 190.85398864746094, 336.11932373046875, 216.2684783935547], "page_size": [612.0, 792.0]}
{"layout": 4212, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 396, "bbox": [137, 222, 188, 234], "page_size": [612.0, 792.0]}
{"layout": 4213, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). •  proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). •  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. ", "page_idx": 396, "bbox": [154, 238.8244171142578, 521, 329.8434753417969], "page_size": [612.0, 792.0]}
{"layout": 4214, "type": "text", "text": "Returns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. ", "page_idx": 396, "bbox": [137.4549560546875, 333.8377990722656, 521, 395.596435546875], "page_size": [612.0, 792.0]}
{"layout": 4215, "type": "text", "text": "class  mmdet.models.roi_heads. Coarse Mask Head ( num_convs  $\\mathrel{\\mathop:}=\\!\\!O$  ,  num_fc  $\\backsimeq\\!2$  ,  fc out channels  $\\mathbf{\\tilde{=}}$  1024 , down sample facto  $_{r=2}$  ,  init_cfg  $=$  {'override': [{'name': 'fcs'}, {'type': 'Constant', 'val': 0.001, 'name': 'fc_logits'}], 'type': 'Xavier'} ,  \\*arg ,  \\*\\*kwarg ) ", "page_idx": 396, "bbox": [71.99995422363281, 418.00299072265625, 534.6939697265625, 467.3274230957031], "page_size": [612.0, 792.0]}
{"layout": 4216, "type": "text", "text": "Coarse mask head used in PointRend. ", "page_idx": 396, "bbox": [96, 465.9723815917969, 249, 479.28240966796875], "page_size": [612.0, 792.0]}
{"layout": 4217, "type": "text", "text": "Compared with standard  FC N Mask Head ,  Coarse Mask Head  will downsample the input feature map instead of upsample it. ", "page_idx": 396, "bbox": [96, 483.9053955078125, 540, 509.17041015625], "page_size": [612.0, 792.0]}
{"layout": 4218, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 396, "bbox": [117, 515, 168, 527], "page_size": [612.0, 792.0]}
{"layout": 4219, "type": "text", "text": "•  num_convs  ( int ) – Number of conv layers in the head. Default: 0. •  num_fcs  ( int ) – Number of fc layers in the head. Default: 2. •  fc out channels  ( int ) – Number of output channels of fc layer. Default: 1024. •  down sample factor  ( int ) – The factor that feature map is down sampled by. Default: 2. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 396, "bbox": [145, 531.7263793945312, 513.30419921875, 616.7673950195312], "page_size": [612.0, 792.0]}
{"layout": 4220, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 396, "bbox": [96, 623, 313.4840393066406, 664.58740234375], "page_size": [612.0, 792.0]}
{"layout": 4221, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while ", "page_idx": 396, "bbox": [118, 680.537841796875, 540, 706.4304809570312], "page_size": [612.0, 792.0]}
{"layout": 4222, "type": "text", "text": "the latter silently ignores them. ", "page_idx": 397, "bbox": [118, 71.45246887207031, 242, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 4223, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 397, "bbox": [96, 103.06400299072266, 204.58209228515625, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 4224, "type": "text", "text": "class  mmdet.models.roi_heads. Con v FCB Box Head ( num shared con vs  $\\mathrm{\\Sigma=}0$  ,  num shared fc  $\\scriptstyle{s=0}$  , num cls con vs  $\\mathord{:=}0$  ,  num_cls_fc  $s{=}0$  ,  num reg con vs  $\\imath{=}0$  , num_reg_fc  $\\scriptstyle{:s=0}$  ,  con v out channel  $s{=}256$  , fc out channels  $\\mathbf{\\hat{=}}$  1024 ,  conv_cfg  $=$  None ,  norm_cfg  $=$  None , init_cfg  $\\mathbf{\\beta}=$  None ,  \\*args ,  \\*\\*kwargs ) ", "page_idx": 397, "bbox": [71, 131.0790252685547, 535, 192.3585662841797], "page_size": [612.0, 792.0]}
{"layout": 4225, "type": "text", "text": "More general bbox head, with shared conv and fc layers and two optional separated branches. ", "page_idx": 397, "bbox": [96, 191.00355529785156, 470, 204.31358337402344], "page_size": [612.0, 792.0]}
{"layout": 4226, "type": "table", "page_idx": 397, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_157.jpg", "bbox": [92, 208, 544, 253], "page_size": [612.0, 792.0], "ocr_text": "shared convs -> shared fcs\n\n/-> cls convs -> cls fcs -> cls\n\n\\-> reg convs -> reg fcs -> reg\n\n", "vlm_text": "The table appears to represent a flow or processing pipeline for a neural network architecture, possibly a region-based object detection framework. Here's a breakdown of the flow illustrated in the table:\n\n1. **Shared Convs -> Shared FCs**: The data or features initially undergo shared convolutional layers (convs), followed by shared fully connected layers (FCs). This suggests a shared processing path for certain features.\n\n2. **Branching Paths**:\n   - **Cls Path**: One branch of the process involves further convolutional layers for classification (cls convs), followed by fully connected layers for classification (cls FCs), and finally producing a classification output (cls).\n   - **Reg Path**: Another branch involves convolutional layers for regression (reg convs), followed by fully connected layers for regression (reg FCs), and concluding with a regression output (reg).\n\nThis structure is indicative of multi-task learning, where both classification and regression tasks share some initial processing steps before diverging into specialized paths for their respective outputs."}
{"layout": 4227, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 397, "bbox": [96, 263, 149, 273], "page_size": [612.0, 792.0]}
{"layout": 4228, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 397, "bbox": [118, 272.49847412109375, 313.4832763671875, 303.7405090332031], "page_size": [612.0, 792.0]}
{"layout": 4229, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 397, "bbox": [118, 319.6908264160156, 540, 357.53948974609375], "page_size": [612.0, 792.0]}
{"layout": 4230, "type": "text", "text": "class  mmdet.models.roi_heads. DIIHead ( num_classe  $\\wp{=}8O$  ,  num ff n fcs  $\\scriptstyle{\\prime=2}$  ,  num_heads  $\\scriptstyle{:=8}$  ,  num_cls_fc  $s{=}I$  , num_reg_fc  $\\cdot_{s=3}$  ,  feed forward channels  $:=$  2048 ,  in channels=256 , dropou  $\\mathopen{}\\mathclose\\bgroup\\left.\\aftergroup\\egroup\\right.$  ,  ff n act cf g  $=$  {'inplace': True, 'type': 'ReLU'} , dynamic con v cf g  $=$  {'act_cfg': {'inplace': True, 'type': 'ReLU'}, 'feat channels': 64, 'in channels': 256, 'input feat shape': 7, 'norm_cfg': {'type': 'LN'}, 'out channels': 256, 'type': 'Dynamic Con v'} ,  loss_iou={'loss weight': 2.0, 'type': 'GIoULoss'} , init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  \\*\\*kwargs ) ", "page_idx": 397, "bbox": [71, 373.968017578125, 535, 471.1134033203125], "page_size": [612.0, 792.0]}
{"layout": 4231, "type": "text", "text": "Dynamic Instance Interactive Head for  Sparse R-CNN: End-to-End Object Detection with Learnable Proposals ", "page_idx": 397, "bbox": [96, 469.75836181640625, 540, 483.0683898925781], "page_size": [612.0, 792.0]}
{"layout": 4232, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 397, "bbox": [117, 489, 168, 501], "page_size": [612.0, 792.0]}
{"layout": 4233, "type": "text", "text": "•  num classes  ( int ) – Number of class in dataset. Defaults to 80. •  num ff n fcs  ( int ) – The number of fully-connected layers in FFNs. Defaults to 2. •  num_heads  ( int ) – The hidden dimension of FFNs. Defaults to 8. •  num cls fcs  ( int ) – The number of fully-connected layers in classification subnet. De- faults to 1. •  num reg fcs  ( int ) – The number of fully-connected layers in regression subnet. Defaults to 3. •  feed forward channels  ( int ) – The hidden dimension of FFNs. Defaults to 2048 •  in channels  ( int ) – Hidden channels of Multi head Attention. Defaults to 256. •  dropout  ( float ) – Probability of drop the channel. Defaults to 0.0 •  ff n act cf g  ( dict ) – The activation config for FFNs. •  dynamic con v cf g  ( dict ) – The convolution config for Dynamic Con v. •  loss_iou  ( dict ) – The config for iou or giou loss. ", "page_idx": 397, "bbox": [145, 505.6243591308594, 524, 722.1714477539062], "page_size": [612.0, 792.0]}
{"layout": 4234, "type": "text", "text": "forward ( roi_feat ,  proposal feat ) Forward function of Dynamic Instance Interactive Head. ", "page_idx": 398, "bbox": [96, 71.30303192138672, 344.1082458496094, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4235, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 398, "bbox": [136, 102, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 4236, "type": "text", "text": "•  roi_feat  ( Tensor ) – Roi-pooling features with shape (batch_size\\*num proposals, fea- ture dimensions, pooling_h , pooling_w). •  proposal feat  – Intermediate feature get from diihead in last stage, has shape (batch_size, num proposals, feature dimensions) ", "page_idx": 398, "bbox": [154, 119.27247619628906, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 4237, "type": "text", "text": "get targets ( sampling results ,  gt_bboxes ,  gt_labels ,  r cnn train cf g ,  conca  $\\leftleftarrows$  True ) Calculate the ground truth for all samples in a batch according to the sampling results. ", "page_idx": 398, "bbox": [96, 178.89903259277344, 464.45635986328125, 204.3135223388672], "page_size": [612.0, 792.0]}
{"layout": 4238, "type": "text", "text": "Almost the same as the implementation in bbox_head, we passed additional parameters pos in ds list and ne g in ds list to  get target single  function. ", "page_idx": 398, "bbox": [118, 208.9364776611328, 540, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 4239, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 398, "bbox": [136, 240, 187, 252], "page_size": [612.0, 792.0]}
{"layout": 4240, "type": "text", "text": "•  (List[obj  ( sampling results ) – Sampling Results]): Assign results of all images in a batch after sampling. •  gt_bboxes  ( list[Tensor] ) – Gt_bboxes of all images in a batch, each tensor has shape (num_gt, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  gt_labels  ( list[Tensor] ) – Gt_labels of all images in a batch, each tensor has shape (num_gt,). •  (obj  ( r cnn train cf g ) –  ConfigDict ):  train_cfg  of RCNN. •  concat  ( bool ) – Whether to concatenate the results of all the images in a single batch. ", "page_idx": 398, "bbox": [154, 256.7574768066406, 521, 377.6635437011719], "page_size": [612.0, 792.0]}
{"layout": 4241, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 398, "bbox": [136, 383, 172, 395], "page_size": [612.0, 792.0]}
{"layout": 4242, "type": "text", "text": "Ground truth for proposals in a single image. Containing the following list of Tensors: • labels (list[Tensor],Tensor): Gt_labels for all proposals in a batch, each tensor in list has shape (num proposals,) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals,). • label weights (list[Tensor]): Labels weights for all proposals in a batch, each tensor in list has shape (num proposals,) when  conca  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals,). • b box targets (list[Tensor],Tensor): Regression target for all proposals in a batch, each ten- sor in list has shape (num proposals, 4) when  conca  $\\fallingdotseq$  False , otherwise just a single tensor has shape (num all proposals, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. • b box weights (list[tensor],Tensor): Regression weights for all proposals in a batch, each tensor in list has shape (num proposals, 4) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals, 4). ", "page_idx": 398, "bbox": [154, 400.21954345703125, 521, 580.9015502929688], "page_size": [612.0, 792.0]}
{"layout": 4243, "type": "text", "text": "Return type  Tuple[Tensor] ", "page_idx": 398, "bbox": [137.4549102783203, 584.8968505859375, 251, 599.4322509765625], "page_size": [612.0, 792.0]}
{"layout": 4244, "type": "text", "text": "in it weights()", "text_level": 1, "page_idx": 398, "bbox": [96, 605, 170, 616], "page_size": [612.0, 792.0]}
{"layout": 4245, "type": "text", "text": "Use xavier initialization for all weight parameter and set classification head bias as a specific value when use focal loss. ", "page_idx": 398, "bbox": [118, 615.4124755859375, 540, 640.6775512695312], "page_size": [612.0, 792.0]}
{"layout": 4246, "type": "text", "text": "loss ( cls_score ,  bbox_pred ,  labels ,  label weights ,  b box targets ,  b box weights ,  imgs_whwh=None , reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) “Loss function of DIIHead, get loss of all images. ", "page_idx": 398, "bbox": [96, 645.1510620117188, 493.3194885253906, 682.5205688476562], "page_size": [612.0, 792.0]}
{"layout": 4247, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 398, "bbox": [136, 688, 187, 700], "page_size": [612.0, 792.0]}
{"layout": 4248, "type": "text", "text": "•  cls_score  ( Tensor ) – Classification prediction results of all class, has shape (batch_size \\* num proposals single image, num classes) •  bbox_pred  ( Tensor ) – Regression prediction results, has shape (batch_size \\* num proposals single image, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  labels ( Tensor ) – Label of each proposals, has shape (batch_size \\* num proposals single image •  label weights  ( Tensor ) – Classification loss weight of each proposals, has shape (batch_size \\* num proposals single image •  b box targets  ( Tensor ) – Regression targets of each proposals, has shape (batch_size \\* num proposals single image, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  b box weights  ( Tensor ) – Regression loss weight of each proposals’s coordinate, has shape (batch_size \\* num proposals single image, 4), •  imgs_whwh  ( Tensor ) – imgs_whwh (Tensor): Tensor with shape (batch_size, num proposals, 4), the last dimension means [img_width,img_height, img_width, img_height]. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. Defaults to None, •  Returns  – dict[str, Tensor]: Dictionary of loss components ", "page_idx": 399, "bbox": [155, 71.45246887207031, 521, 347.7755432128906], "page_size": [612.0, 792.0]}
{"layout": 4249, "type": "text", "text": "class  mmdet.models.roi_heads. Double Con v FCB Box Head ( num_convs  $\\scriptstyle{\\varepsilon=0}$  ,  num_fc  $s{=}0$  , ", "page_idx": 399, "bbox": [71.99990844726562, 352.24908447265625, 442.1415710449219, 365.70855712890625], "page_size": [612.0, 792.0]}
{"layout": 4250, "type": "text", "text": "Bbox head used in Double-Head R-CNN con v out channel  $\\mathbf{:=}$  1024 ,  fc out channels  $\\mathbf{\\beta}=\\mathbf{\\beta}$  1024 , conv_cfg  $=$  None ,  norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'type': 'BN'} , init_cfg={'override': [{'type': 'Normal', 'name':\n\n 'fc_cls', 'std': 0.01}, {'type': 'Normal', 'name':\n\n 'fc_reg', 'std': 0.001}, {'type': 'Xavier', 'name':\n\n 'fc_branch', 'distribution': 'uniform'}], 'type':\n\n 'Normal'} ,  \\*\\*kwargs ) ", "page_idx": 399, "bbox": [96.90643310546875, 448.0404357910156, 259.585693359375, 461.3504638671875], "page_size": [612.0, 792.0]}
{"layout": 4251, "type": "text", "text": "", "page_idx": 399, "bbox": [333, 364.2040710449219, 535.9295654296875, 449.39447021484375], "page_size": [612.0, 792.0]}
{"layout": 4252, "type": "table", "page_idx": 399, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_158.jpg", "bbox": [93, 465, 543, 558], "page_size": [612.0, 792.0], "ocr_text": "/-> cls\n/-> shared convs ->\n\\-> reg\nroi features\n/-> cls\n\\-> shared fc ->\n\\-> reg\n\n", "vlm_text": "The table visually represents a flow of processing steps for \"roi features\" (Region of Interest features) in a computational model. It outlines two main paths:\n\n1. **Shared Convolutions Path:**\n   - Starts with roi features\n   - Goes through shared convolutions\n   - Splits into two outputs: \"cls\" (classification) and \"reg\" (regression)\n\n2. **Shared Fully Connected (fc) Path:**\n   - Starts with roi features\n   - Goes through shared fully connected layers\n   - Splits into two outputs: \"cls\" (classification) and \"reg\" (regression)\n\nThis table seems to depict a dual-path processing architecture, likely part of an object detection or image segmentation model, where both convolutional and fully connected operations are shared and lead to classification and regression tasks."}
{"layout": 4253, "type": "text", "text": "forward  $(x\\_c l s,x\\_r e g)$  Defines the computation performed at every call. ", "page_idx": 399, "bbox": [96, 567, 313.4832763671875, 590.6644897460938], "page_size": [612.0, 792.0]}
{"layout": 4254, "type": "text", "text": "Should be overridden by all subclasses. ", "page_idx": 399, "bbox": [118, 595.2874145507812, 275.446044921875, 608.5974731445312], "page_size": [612.0, 792.0]}
{"layout": 4255, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 399, "bbox": [118, 624.5478515625, 540, 662.396484375], "page_size": [612.0, 792.0]}
{"layout": 4256, "type": "text", "text": "class  mmdet.models.roi_heads. Double Head RoI Head ( reg roi scale factor ,  \\*\\*kwargs ) RoI head for Double Head RCNN. ", "page_idx": 399, "bbox": [72.0, 678.8240356445312, 449.6283874511719, 704.239501953125], "page_size": [612.0, 792.0]}
{"layout": 4257, "type": "text", "text": "https://arxiv.org/abs/1904.06493 ", "page_idx": 399, "bbox": [96, 708.8614501953125, 227.33729553222656, 722.1715087890625], "page_size": [612.0, 792.0]}
{"layout": 4258, "type": "text", "text": "class  mmdet.models.roi_heads. Dynamic RoI Head ( \\*\\*kwargs ) RoI head for  Dynamic R-CNN . ", "page_idx": 400, "bbox": [71, 71.30303192138672, 345.7274169921875, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4259, "type": "text", "text": "forward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None , gt_mask  $\\mathfrak{s}\\mathbf{=}\\mathfrak{s}$  None ) Forward function for training. ", "page_idx": 400, "bbox": [96, 101.19103240966797, 472, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 4260, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 400, "bbox": [136, 145, 188, 156], "page_size": [612.0, 792.0]}
{"layout": 4261, "type": "text", "text": "•  x  ( list[Tensor] ) – list of multi-level img features. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposals  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task.  a dictionary of loss components  dict[str, Tensor] ", "page_idx": 400, "bbox": [155, 161.11549377441406, 521, 389.61859130859375], "page_size": [612.0, 792.0]}
{"layout": 4262, "type": "text", "text": "update hyper parameters () ", "text_level": 1, "page_idx": 400, "bbox": [95, 397, 222, 406], "page_size": [612.0, 792.0]}
{"layout": 4263, "type": "text", "text": "Update hyper parameters like IoU thresholds for assigner and beta for SmoothL1 loss based on the training statistics. ", "page_idx": 400, "bbox": [118, 406.1965637207031, 540, 431.46258544921875], "page_size": [612.0, 792.0]}
{"layout": 4264, "type": "text", "text": "Returns  the updated  iou_thr  and  beta . Return type  tuple[float] ", "page_idx": 400, "bbox": [137, 435.4569091796875, 304, 467.92535400390625], "page_size": [612.0, 792.0]}
{"layout": 4265, "type": "text", "text": "class  mmdet.models.roi_heads. FC N Mask Head ( num_convs  $\\scriptstyle{\\prime}=4$  ,  roi feat size  $\\scriptstyle{\\varepsilon=I4}$  ,  in channels  $=\\!256$  , ", "page_idx": 400, "bbox": [71, 471.8011474609375, 492.27362060546875, 485.2606201171875], "page_size": [612.0, 792.0]}
{"layout": 4266, "type": "text", "text": "con v kernel size  $\\scriptstyle{\\stackrel{\\prime}{=}}3$  ,  con v out ch anne  $I_{S=256}$  , num_classe  $s{=}8O$  ,  class agnostic  $\\mathbf{\\dot{\\rho}}=$  False , up sample cf g  $\\mathbf{\\beta}=$  {'scale factor': 2, 'type': 'deconv'} , conv_cfg  $=$  None ,  norm_cfg  $=$  None ,  predictor cf g={'type': 'Conv'} ,  loss_mask={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use_mask': True} ,  init_cfg=None ) ", "page_idx": 400, "bbox": [286, 483.7561340332031, 514.4603881835938, 556.9915771484375], "page_size": [612.0, 792.0]}
{"layout": 4267, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 400, "bbox": [96, 575, 150, 585], "page_size": [612.0, 792.0]}
{"layout": 4268, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 400, "bbox": [118, 585.5245361328125, 313.4839782714844, 616.767578125], "page_size": [612.0, 792.0]}
{"layout": 4269, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 400, "bbox": [118, 632.7178344726562, 540, 670.5654907226562], "page_size": [612.0, 792.0]}
{"layout": 4270, "type": "text", "text": "get seg masks ( mask_pred ,  det_bboxes ,  det_labels ,  r cnn test cf g ,  ori_shape ,  scale factor ,  rescale ) Get segmentation masks from mask_pred and bboxes. ", "page_idx": 400, "bbox": [96, 686.9940185546875, 504.8163757324219, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 4271, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 401, "bbox": [137, 73, 187, 84], "page_size": [612.0, 792.0]}
{"layout": 4272, "type": "text", "text": "•  mask_pred  ( Tensor or ndarray ) – shape (n, #class, h, w). For single-scale testing, mask_pred is the direct output of model, whose type is Tensor, while for multi-scale testing, it will be converted to numpy array outside of this method. •  det_bboxes  ( Tensor ) – shape (n, 4/5) •  det_labels  ( Tensor ) – shape (n, ) •  r cnn test cf g  ( dict ) – rcnn testing config •  ori_shape  ( Tuple ) – original image height and width, shape (2,) •  scale factor  ( ndarray | Tensor ) – If  rescale is True , box coordinates are di- vided by this scale factor to fit  ori_shape . •  rescale  ( bool ) – If True, the resulting masks will be rescaled to  ori_shape . ", "page_idx": 401, "bbox": [154, 89.38447570800781, 521, 246.15647888183594], "page_size": [612.0, 792.0]}
{"layout": 4273, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 401, "bbox": [137, 252, 172, 264], "page_size": [612.0, 792.0]}
{"layout": 4274, "type": "text", "text": "encoded masks. The c-th item in the outer list  corresponds to the c-th class. Given the c- th outer list, the i-th item in that inner list is the mask for the i-th box with class label c. ", "page_idx": 401, "bbox": [154, 268.08477783203125, 521, 305.93243408203125], "page_size": [612.0, 792.0]}
{"layout": 4275, "type": "text", "text": "Return type  list[list] ", "page_idx": 401, "bbox": [137.4549560546875, 309.9277648925781, 224.02972412109375, 324.46319580078125], "page_size": [612.0, 792.0]}
{"layout": 4276, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 401, "bbox": [118, 343, 160, 355], "page_size": [612.0, 792.0]}
{"layout": 4277, "type": "text", "text": " $>>$   import  mmcv  $>>$   from  mmdet.models.roi_heads.mask_heads.fc n mask head  import  \\* # NOQA >>>  $\\texttt{N=7}$  #   $N\\;=\\;$   number of extracted ROIs  $\\ggg$  , H,  $\\texttt{W}=\\texttt{11}$  ,  32 ,  32  $>>$   # Create example instance of FCN Mask Head.  $>>$   self  $=$   FC N Mask Head(num classes  $\\mathsf{\\Gamma}=\\mathsf{C}$  , num_convs  $\\scriptstyle\\left.=0\\right)$  )  $>>$   inputs  $=$   torch . rand(N,  self . in channels, H, W)  $>>$   mask_pred  $=$   self . forward(inputs)  $>>$   # Each input is associated with some bounding box  $>>$   det_bboxes  $=$   torch . Tensor([[ 1 ,  1 ,  42 ,  42  ]]  \\*  N) >>>  det_labels  $=$   torch . randint( 0 , C, size  $=$  (N,)) >>>  r cnn test cf g  $=$   mmcv . Config({ ' mask thr binary ' :  0 , }) >>>  ori_shape  $=$   (H  \\*  4 , W  \\*  4 ) >>>  scale factor  $=$   torch . Float Tensor(( 1 ,  1 )) >>>  rescale  $=$   False >>>  # Encoded masks are a list for each category. >>>  encoded masks  $=$   self . get seg masks( >>> mask_pred, det_bboxes, det_labels, r cnn test cf g, ori_shape, >>> scale factor, rescale >>>  ) >>>  assert  len (encoded masks)  $\\mathbf{\\mu}=\\textsf{C}$  >>>  assert  sum ( list ( map ( len , encoded masks)))  ==  N ", "page_idx": 401, "bbox": [117, 371.3039855957031, 500.86029052734375, 633.2718505859375], "page_size": [612.0, 792.0]}
{"layout": 4278, "type": "text", "text": "in it weights()", "text_level": 1, "page_idx": 401, "bbox": [96, 646, 170, 658], "page_size": [612.0, 792.0]}
{"layout": 4279, "type": "text", "text": "Initialize the weights. ", "page_idx": 401, "bbox": [117, 657.5004272460938, 204.58209228515625, 670.8104858398438], "page_size": [612.0, 792.0]}
{"layout": 4280, "type": "text", "text": "loss ( mask_pred ,  mask targets ,  labels ) ", "page_idx": 401, "bbox": [96.90699005126953, 675.2839965820312, 257.8583679199219, 688.7434692382812], "page_size": [612.0, 792.0]}
{"layout": 4281, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 402, "bbox": [117, 72, 160, 85], "page_size": [612.0, 792.0]}
{"layout": 4282, "type": "text", "text": " $>>$   from  mmdet.models.roi_heads.mask_heads.fc n mask head  import  \\* # NOQA >>>  $\\texttt{N=7}$  #   $N\\;=\\;$   number of extracted ROIs >>>  C, H,  $\\texttt{W}=\\texttt{11}$  ,  32 ,  32  $>>$   # Create example instance of FCN Mask Head. >>>  # There are lots of variations depending on the configuration  $>>$   self  $=$   FC N Mask Head(num classes  $\\mathsf{\\Lambda}_{=}\\mathsf{C}$  , num_convs  $^{=1}$  )  $>>$   inputs  $=$   torch . rand(N,  self . in channels, H, W) >>>  mask_pred  $=$   self . forward(inputs) >>>  sf  $=$   self . scale factor  $>>$   labels  $=$   torch . randint( 0 , C, size  $=$  (N,)) >>>  # With the default properties the mask targets should indicate >>>  # a (potentially soft) single-class label  $>>$  mask targets $=$  torch.rand(N, H \\* sf, W \\* sf) $>>$   loss  $=$   self . loss(mask_pred, mask targets, labels) >>>  print (  $\\,^{\\dagger}\\,\\bot o s s\\ =\\ \\{\\,I\\,x\\}\\,^{\\dagger}$  . format(loss)) ", "page_idx": 402, "bbox": [117, 100.33097076416016, 500.86029052734375, 279], "page_size": [612.0, 792.0]}
{"layout": 4283, "type": "text", "text": "on nx export ( mask_pred ,  det_bboxes ,  det_labels ,  r cnn test cf g ,  ori_shape ,  \\*\\*kwargs ) Get segmentation masks from mask_pred and bboxes. ", "page_idx": 402, "bbox": [96, 290.7370300292969, 450.3463439941406, 316.1514892578125], "page_size": [612.0, 792.0]}
{"layout": 4284, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 402, "bbox": [136, 322, 188, 333], "page_size": [612.0, 792.0]}
{"layout": 4285, "type": "text", "text": "•  mask_pred  ( Tensor ) – shape (n, #class, h, w). •  det_bboxes  ( Tensor ) – shape (n, 4/5) •  det_labels  ( Tensor ) – shape (n, ) •  r cnn test cf g  ( dict ) – rcnn testing config •  ori_shape  ( Tuple ) – original image height and width, shape (2,) Returns  a mask of shape (N, img_h, img_w). Return type  Tensor ", "page_idx": 402, "bbox": [137, 338.70648193359375, 428.3772277832031, 460.2113342285156], "page_size": [612.0, 792.0]}
{"layout": 4286, "type": "text", "text": "class  mmdet.models.roi_heads. Feature Relay Head ( in channels  $\\mathbf{\\hat{\\rho}}$  1024 ,  out con v channel  $s{=}256.$  , roi feat size  $\\mathbf{\\beta}=\\mathbf{\\dot{\\beta}}$  7 ,  scale factor  ${\\it\\Delta\\phi}=\\!2{\\it\\Delta\\Psi}$  ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Linear', 'type': 'Kaiming'} ) ", "page_idx": 402, "bbox": [71.99996948242188, 464.08612060546875, 514.8284301757812, 501.34698486328125], "page_size": [612.0, 792.0]}
{"layout": 4287, "type": "text", "text": "Feature Relay Head used in  SCNet . ", "page_idx": 402, "bbox": [96, 500.1015319824219, 237.67828369140625, 513.4115600585938], "page_size": [612.0, 792.0]}
{"layout": 4288, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 402, "bbox": [117, 519, 168, 530], "page_size": [612.0, 792.0]}
{"layout": 4289, "type": "text", "text": "•  in channels  ( int, optional ) – number of input channels. Default: 256. •  con v out channels  ( int, optional ) – number of output channels before classification layer. Default: 256. •  roi feat size  ( int, optional ) – roi feat size at box head. Default: 7. •  scale factor  ( int, optional ) – scale factor to match roi feat size at mask head. De- fault: 2. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 402, "bbox": [145, 535.9664916992188, 518, 644.9185791015625], "page_size": [612.0, 792.0]}
{"layout": 4290, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 402, "bbox": [96, 651, 190.51466369628906, 674.8065795898438], "page_size": [612.0, 792.0]}
{"layout": 4291, "type": "table", "page_idx": 403, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_159.jpg", "bbox": [73, 71, 537, 155.75], "page_size": [612.0, 792.0], "ocr_text": "class mmdet.models.roi_heads .FusedSemanticHead (num_ins, fusion_level, num_convs=4,\nin_channels=256, conv_out_channels=256,\nnum_classes=183, conv_cfg=None, norm_cfg=None,\nignore_label=None, loss_weight=None,\nloss_seg={'ignore_index': 255, ‘loss_weight': 0.2,\n‘type’: 'CrossEntropyLoss'}, init_cfg={'override':\n{‘name': 'conv_logits'}, 'type': 'Kaiming'})\n", "vlm_text": "The image contains a code snippet that defines a class named `FusedSemanticHead` from the `mmdet.models.roi_heads` module. It includes several parameters and configurations, such as:\n\n- `num_ins`\n- `fusion_level`\n- `num_convs=4`\n- `in_channels=256`\n- `conv_out_channels=256`\n- `num_classes=183`\n- `conv_cfg=None`\n- `norm_cfg=None`\n- `ignore_label=None`\n- `loss_weight=None`\n- `loss_seg` which includes details: \n  - `'ignore_index': 255`\n  - `'loss_weight': 0.2`\n  - `'type': 'CrossEntropyLoss'`\n- `init_cfg` which includes:\n  - `{'override': {'name': 'conv_logits'}, 'type': 'Kaiming'}`\n\nThis appears to be part of a configuration for a deep learning model, likely related to semantic segmentation."}
{"layout": 4292, "type": "text", "text": "Multi-level fused semantic segmentation head. ", "page_idx": 403, "bbox": [96, 155.13856506347656, 282, 168.44859313964844], "page_size": [612.0, 792.0]}
{"layout": 4293, "type": "image", "page_idx": 403, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_160.jpg", "bbox": [92, 172, 543, 289], "page_size": [612.0, 792.0], "ocr_text": "in_l ->\n\nin_2 ->\nin_3 ->\nin_4 ->\n\n1x\n\n1x\n\n1x\n\n1x\n\n1x\n\nconv\n\nconv\n\nconv\n\nconv\n\nconv\n\n> 3x3 convs (*4)\n\n/-> 1x1 conv (mask prediction)\n\n\\-> 1x1 conv (feature)\n\n", "vlm_text": "The image is a schematic diagram of a convolutional neural network (CNN) architecture, showing how different input layers (in_1 to in_5) are processed through convolutional layers. Here's a breakdown of the diagram:\n\n1. **Inputs (in_1 to in_5)**: Five different inputs labeled in_1, in_2, in_3, in_4, and in_5.\n\n2. **1x1 Convolutional Layers**: Each input is passed through a 1x1 convolutional layer. This layer is typically used for dimensionality reduction or feature transformation.\n\n3. **Aggregation**: The outputs of the 1x1 convolutional layers from in_1 to in_5 are aggregated (appears to be concatenated or added together).\n\n4. **3x3 Convolutional Layer**: The aggregated output undergoes further processing through a series of four 3x3 convolutional layers, which are often used to capture spatial relationships.\n\n5. **Final Layers**:\n   - **Mask Prediction**: The output of the 3x3 convolutions is passed through another 1x1 convolutional layer specifically designated for mask prediction.\n   - **Feature Layer**: A different path from the 1x1 convolution layer outputs a \"feature\" layer, also through a 1x1 convolutional layer.\n\nThis diagram represents a common pattern in CNNs used for tasks like object detection, segmentation, or feature extraction, where different inputs are convolved and combined in a structured manner to produce outputs like masks or features."}
{"layout": 4294, "type": "text", "text": "forward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 403, "bbox": [96, 296.259033203125, 313, 339.60650634765625], "page_size": [612.0, 792.0]}
{"layout": 4295, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 403, "bbox": [118, 355.55682373046875, 540, 393.40447998046875], "page_size": [612.0, 792.0]}
{"layout": 4296, "type": "text", "text": "class  mmdet.models.roi_heads. Generic RoI Extractor ( aggregation  $=$  sum' ,  pre_cfg  $=$  None ,  post_cfg=None , \\*\\*kwargs ) ", "page_idx": 403, "bbox": [72, 409.8330383300781, 534.4056396484375, 435.1379089355469], "page_size": [612.0, 792.0]}
{"layout": 4297, "type": "text", "text": "Extract RoI features from all level feature maps levels. This is the implementation of  A novel Region of Interest Extraction Layer for Instance Segmentation . ", "page_idx": 403, "bbox": [96, 433.8924560546875, 313, 447.2024841308594], "page_size": [612.0, 792.0]}
{"layout": 4298, "type": "text", "text": "", "page_idx": 403, "bbox": [96, 451.8254699707031, 499.98370361328125, 465.135498046875], "page_size": [612.0, 792.0]}
{"layout": 4299, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 403, "bbox": [117, 471, 168, 483], "page_size": [612.0, 792.0]}
{"layout": 4300, "type": "text", "text": "•  aggregation  ( str ) – The method to aggregate multiple feature maps. Options are ‘sum’, ‘concat’. Default: ‘sum’. ", "page_idx": 403, "bbox": [145, 487.6914978027344, 518, 512.9564819335938], "page_size": [612.0, 792.0]}
{"layout": 4301, "type": "text", "text": "•  pre_cfg  ( dict | None ) – Specify pre-processing modules. Default: None. •  post_cfg  ( dict | None ) – Specify post-processing modules. Default: None. •  kwargs  ( keyword arguments ) – Arguments that are the same as  Base RoI Extractor . ", "page_idx": 403, "bbox": [145, 517.5794677734375, 461.0546875, 530.8895263671875], "page_size": [612.0, 792.0]}
{"layout": 4302, "type": "text", "text": "", "page_idx": 403, "bbox": [145, 535.511474609375, 470.02166748046875, 548.821533203125], "page_size": [612.0, 792.0]}
{"layout": 4303, "type": "text", "text": "", "page_idx": 403, "bbox": [145, 553.4444580078125, 507.7206726074219, 566.7545166015625], "page_size": [612.0, 792.0]}
{"layout": 4304, "type": "text", "text": "forward ( feats ,  rois ,  roi scale factor  $\\leftrightharpoons$  None ) Forward function. ", "page_idx": 403, "bbox": [96, 571.2280883789062, 282, 596.6425170898438], "page_size": [612.0, 792.0]}
{"layout": 4305, "type": "text", "text": "class  mmdet.models.roi_heads. Global Context Head ( num_convs  $\\scriptstyle{\\prime}=4$  ,  in_channel  $\\varsigma{=}256$  , con v out channel  $\\mathfrak{s}{=}256$  ,  num classes  $\\scriptstyle{:=80}$  , loss_weigh  $t{=}I.0$  ,  conv_cfg  $=$  None ,  norm_cfg  $=$  None , con v to res  $=$  False ,  init_cfg  $=$  {'override': {'name': 'fc'}, 'std': 0.01, 'type': 'Normal'} ) ", "page_idx": 403, "bbox": [72, 601.1160278320312, 540, 662.2869262695312], "page_size": [612.0, 792.0]}
{"layout": 4306, "type": "text", "text": "Global context head used in  SCNet . ", "page_idx": 403, "bbox": [96, 661.0414428710938, 239.1928253173828, 674.3515014648438], "page_size": [612.0, 792.0]}
{"layout": 4307, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 403, "bbox": [114, 679, 168, 693], "page_size": [612.0, 792.0]}
{"layout": 4308, "type": "text", "text": "•  num_convs  ( int, optional ) – number of convolutional layer in GlbCtxHead. Default: 4. ", "page_idx": 403, "bbox": [145, 696.906494140625, 518, 710.216552734375], "page_size": [612.0, 792.0]}
{"layout": 4309, "type": "text", "text": "•  in channels  ( int, optional ) – number of input channels. Default: 256. •  con v out channels  ( int, optional ) – number of output channels before classification layer. Default: 256. •  num classes  ( int, optional ) – number of classes. Default: 80. •  loss weight  ( float, optional ) – global context loss weight. Default: 1. •  conv_cfg  ( dict, optional ) – config to init conv layer. Default: None. •  norm_cfg  ( dict, optional ) – config to init norm layer. Default: None. •  con v to res  ( bool, optional ) – if True, 2 convs will be grouped into 1  Simplified Ba- sicBlock  using a skip connection. Default: False. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 404, "bbox": [145, 71.45246887207031, 518, 234.2014617919922], "page_size": [612.0, 792.0]}
{"layout": 4310, "type": "text", "text": "forward ( feats ) Forward function. loss ( pred ,  labels ) Loss function. ", "page_idx": 404, "bbox": [96, 238.6749725341797, 191, 293.9774475097656], "page_size": [612.0, 792.0]}
{"layout": 4311, "type": "text", "text": "class  mmdet.models.roi_heads. GridHead ( grid points  $_{;=9}$  ,  num_convs=8 ,  roi feat size=14 ,  in channels=256 , con v kernel size=3 ,  point feat channels=64 , dec on v kernel size=4 ,  class agnostic=False , loss_grid={'loss weight': 15, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  conv_cfg=None ,  norm_cfg={'num_groups': 36, 'type': 'GN'} ,  init_cfg=[{'type': 'Kaiming', 'layer': ['Conv2d', 'Linear']}, {'type': 'Normal', 'layer': 'Con v Transpose 2 d', 'std': 0.001, 'override': {'type': 'Normal', 'name': 'deconv2', 'std': 0.001, 'bias': - 4.59511985013459}}])", "page_idx": 404, "bbox": [72, 298.45098876953125, 540, 407.4417724609375], "page_size": [612.0, 792.0]}
{"layout": 4312, "type": "text", "text": "calc sub regions () Compute point specific representation regions. See Grid R-CNN Plus ( https://arxiv.org/abs/1906.05688 ) for details. ", "page_idx": 404, "bbox": [96, 425.85284423828125, 391.06256103515625, 467.3273620605469], "page_size": [612.0, 792.0]}
{"layout": 4313, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 404, "bbox": [96, 473, 313.4839172363281, 515.1483764648438], "page_size": [612.0, 792.0]}
{"layout": 4314, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 404, "bbox": [118, 531.0988159179688, 540, 568.9464721679688], "page_size": [612.0, 792.0]}
{"layout": 4315, "type": "text", "text": "class  mmdet.models.roi_heads. Grid RoI Head ( grid roi extractor ,  grid_head ,  \\*\\*kwargs ) Grid roi head for Grid R-CNN. ", "page_idx": 404, "bbox": [72, 585.3750610351562, 455, 610.7894897460938], "page_size": [612.0, 792.0]}
{"layout": 4316, "type": "text", "text": "https://arxiv.org/abs/1811.12030 ", "page_idx": 404, "bbox": [96, 615.4124145507812, 227, 628.7224731445312], "page_size": [612.0, 792.0]}
{"layout": 4317, "type": "text", "text": "forward dummy ( x ,  proposals ) Dummy forward function. ", "page_idx": 404, "bbox": [96, 633.196044921875, 223, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 4318, "type": "text", "text": "simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\mathbf{\\check{\\Sigma}}$  None ,  rescale  $\\mathbf{=}$  False ) Test without augmentation. ", "page_idx": 404, "bbox": [96, 663.0830078125, 407.6614074707031, 688.4984741210938], "page_size": [612.0, 792.0]}
{"layout": 4319, "type": "text", "text": "class  mmdet.models.roi_heads. HTC Mask Head ( with con v res  $\\mathbf{=}$  True ,  \\*args ,  \\*\\*kwargs ) ", "page_idx": 404, "bbox": [72, 692.9710083007812, 445.00543212890625, 706.4304809570312], "page_size": [612.0, 792.0]}
{"layout": 4320, "type": "text", "text": "forward ( x ,  res_feat  $=$  None ,  return log its  $\\mathbf{=}$  True ,  return feat  $=$  True ) Defines the computation performed at every call. ", "page_idx": 405, "bbox": [96, 71.30303192138672, 367.9303894042969, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4321, "type": "text", "text": "Should be overridden by all subclasses. ", "page_idx": 405, "bbox": [118, 101.34046936035156, 275.446044921875, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4322, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 405, "bbox": [118, 130.59979248046875, 540, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 4323, "type": "text", "text": "class  mmdet.models.roi_heads. Hybrid Task Cascade RoI Head ( num_stages ,  stage loss weights , semantic roi extractor  $\\bf{=}$  None , semantic head  $\\leftrightharpoons$  None , semantic fusion  $=$  ('bbox', 'mask') , interleaved  $\\leftrightharpoons$  True ,  mask info flow  $\\scriptstyle\\sum=$  True , \\*\\*kwargs ) ", "page_idx": 405, "bbox": [72.0, 184.8769989013672, 517.78759765625, 258.0029602050781], "page_size": [612.0, 792.0]}
{"layout": 4324, "type": "text", "text": "Hybrid task cascade roi head including one bbox head and one mask head. ", "page_idx": 405, "bbox": [96, 256.7575378417969, 393.6330871582031, 270.06756591796875], "page_size": [612.0, 792.0]}
{"layout": 4325, "type": "text", "text": "https://arxiv.org/abs/1901.07518 ", "page_idx": 405, "bbox": [96, 274.6905517578125, 227, 288.0005798339844], "page_size": [612.0, 792.0]}
{"layout": 4326, "type": "text", "text": "aug_test ( img_feats ,  proposal list ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Test with augmentations. ", "page_idx": 405, "bbox": [96, 292.4731140136719, 355, 317.8875732421875], "page_size": [612.0, 792.0]}
{"layout": 4327, "type": "text", "text": "If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 405, "bbox": [118, 322.51055908203125, 432.9049377441406, 335.8205871582031], "page_size": [612.0, 792.0]}
{"layout": 4328, "type": "text", "text": "forward dummy ( x ,  proposals ) Dummy forward function. ", "page_idx": 405, "bbox": [96, 340.29412841796875, 223, 365.7085876464844], "page_size": [612.0, 792.0]}
{"layout": 4329, "type": "text", "text": "forward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , gt_mask  $\\leftrightharpoons$  None ,  gt semantic seg  $=$  None ) ", "page_idx": 405, "bbox": [96, 370.18212890625, 472.1895751953125, 395.5965881347656], "page_size": [612.0, 792.0]}
{"layout": 4330, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 405, "bbox": [136, 414, 187, 425], "page_size": [612.0, 792.0]}
{"layout": 4331, "type": "text", "text": " x  ( list[Tensor] ) – list of multi-level img features. ", "page_idx": 405, "bbox": [159.37281799316406, 430.1075744628906, 375.1677551269531, 443.4176025390625], "page_size": [612.0, 792.0]}
{"layout": 4332, "type": "text", "text": "•  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposal list  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None, list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None, Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. •  gt semantic seg  ( None, list[Tensor] ) – semantic segmentation masks used if the architecture supports semantic segmentation task. Returns  a dictionary of loss components  dict[str, Tensor] ", "page_idx": 405, "bbox": [137.4548797607422, 448.0395812988281, 521, 688.4985961914062], "page_size": [612.0, 792.0]}
{"layout": 4333, "type": "text", "text": "simple test  $\\langle x.$  ,  proposal list ,  img_metas ,  rescale  $=$  False ) Test without augmentation. ", "page_idx": 405, "bbox": [96, 692.9711303710938, 338, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 4334, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 406, "bbox": [136, 73, 188, 85], "page_size": [612.0, 792.0]}
{"layout": 4335, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). ", "page_idx": 406, "bbox": [154, 89.38447570800781, 523, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4336, "type": "text", "text": " proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). ", "page_idx": 406, "bbox": [159, 119.27247619628906, 523, 144.5375213623047], "page_size": [612.0, 792.0]}
{"layout": 4337, "type": "text", "text": "•  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. ", "page_idx": 406, "bbox": [154, 149.1604766845703, 397, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 4338, "type": "text", "text": "", "page_idx": 406, "bbox": [154, 167.0934600830078, 506.923095703125, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 4339, "type": "text", "text": "Returns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. ", "page_idx": 406, "bbox": [137, 184.3988037109375, 523, 246.1565399169922], "page_size": [612.0, 792.0]}
{"layout": 4340, "type": "text", "text": "Return type  list[list[np.ndarray]] or list[tuple] ", "page_idx": 406, "bbox": [137, 250.15185546875, 325.2197570800781, 264.6872863769531], "page_size": [612.0, 792.0]}
{"layout": 4341, "type": "text", "text": "property with semantic whether the head has semantic head ", "page_idx": 406, "bbox": [96, 270.4360046386719, 261.8570556640625, 293.9775085449219], "page_size": [612.0, 792.0]}
{"layout": 4342, "type": "text", "text": "Type  bool ", "text_level": 1, "page_idx": 406, "bbox": [137, 300, 182, 312], "page_size": [612.0, 792.0]}
{"layout": 4343, "type": "text", "text": "class  mmdet.models.roi_heads. Mask I oU Head ( num_convs  $\\scriptstyle{\\prime}=4$  ,  num_fcs=2 ,  roi feat size=14 , ", "page_idx": 406, "bbox": [71.99999237060547, 316.3840637207031, 467.6656799316406, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 4344, "type": "text", "text": "in_channel  $\\backsimeq$  256 ,  con v out channels=256 , fc out channels  $\\mathbf{\\hat{\\rho}}$  1024 ,  num classes=80 , loss_iou  $\\mathbf{\\beta}=$  {'loss weight': 0.5, 'type': 'MSELoss'} , init_cfg=[{'type': 'Kaiming', 'override': {'name': 'convs'}}, {'type': 'Caff e 2 Xavier', 'override': {'name': 'fcs'}}, {'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc mask i ou'}}] ) ", "page_idx": 406, "bbox": [286, 328.33905029296875, 523, 401.4648742675781], "page_size": [612.0, 792.0]}
{"layout": 4345, "type": "text", "text": "Mask IoU Head. ", "page_idx": 406, "bbox": [96, 400.21942138671875, 163.02830505371094, 413.5294494628906], "page_size": [612.0, 792.0]}
{"layout": 4346, "type": "text", "text": "This head predicts the IoU of predicted masks and corresponding gt masks. ", "page_idx": 406, "bbox": [96, 418.1524353027344, 397, 431.46246337890625], "page_size": [612.0, 792.0]}
{"layout": 4347, "type": "text", "text": "forward ( mask_feat ,  mask_pred ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 406, "bbox": [96, 435.93499755859375, 313.48382568359375, 479.282470703125], "page_size": [612.0, 792.0]}
{"layout": 4348, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 406, "bbox": [118, 495.2328186035156, 540, 533.0805053710938], "page_size": [612.0, 792.0]}
{"layout": 4349, "type": "text", "text": "get mask scores ( mask i ou p red ,  det_bboxes ,  det_labels ) Get the mask scores. ", "page_idx": 406, "bbox": [96, 549.509033203125, 342.1513671875, 574.9244995117188], "page_size": [612.0, 792.0]}
{"layout": 4350, "type": "text", "text": "mask_score  $=$   bbox_score \\* mask_iou ", "page_idx": 406, "bbox": [118, 579.5464477539062, 270.7436218261719, 592.8565063476562], "page_size": [612.0, 792.0]}
{"layout": 4351, "type": "text", "text": "get targets ( sampling results ,  gt_masks ,  mask_pred ,  mask targets ,  r cnn train cf g ) Compute target of mask IoU. ", "page_idx": 406, "bbox": [96, 597.3300170898438, 444, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 4352, "type": "text", "text": "Mask IoU target is the IoU of the predicted mask (inside a bbox) and the gt mask of corresponding gt mask (the whole instance). The intersection area is computed inside the bbox, and the gt mask area is computed with two steps, firstly we compute the gt area inside the bbox, then divide it by the area ratio of gt area inside the bbox and the gt area of the whole instance. ", "page_idx": 406, "bbox": [118, 627.367431640625, 540, 676.5425415039062], "page_size": [612.0, 792.0]}
{"layout": 4353, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 406, "bbox": [136, 682, 188, 693], "page_size": [612.0, 792.0]}
{"layout": 4354, "type": "text", "text": " sampling results  (list[ Sampling Result ]) – sampling results. ", "page_idx": 406, "bbox": [159, 699.0984497070312, 426.54913330078125, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 4355, "type": "text", "text": "•  gt_masks  ( BitmapMask | Polygon Mask ) – Gt masks (the whole instance) of each im- age, with the same shape of the input image. •  mask_pred  ( Tensor ) – Predicted masks of each positive proposal, shape (num_pos, h, w). •  mask targets  ( Tensor ) – Gt mask of each positive proposal, binary map of the shape (num_pos, h, w). •  r cnn train cf g  ( dict ) – Training config for R-CNN part. ", "page_idx": 407, "bbox": [154, 71.45246887207031, 521, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 4356, "type": "text", "text": "Returns  mask iou target (length  $==$   num positive). ", "page_idx": 407, "bbox": [137, 166.4658203125, 341, 181.00125122070312], "page_size": [612.0, 792.0]}
{"layout": 4357, "type": "text", "text": "Return type  Tensor ", "page_idx": 407, "bbox": [137, 184.3988037109375, 220.14427185058594, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 4358, "type": "text", "text": "class  mmdet.models.roi_heads. Mask Point Head ( num classes ,  num_fc  $\\mathfrak{s}{=}3$  ,  in channels=256 , fc channels=256 ,  class agnostic=False , coarse p red each layer  $\\mathbf{=}$  True ,  conv_cfg  $=$  {'type': 'Conv1d'} ,  norm_cfg  $=$  None ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'ReLU'} , loss_point={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use_mask': True} ,  init_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'override': {'name': 'fc_logits'}, 'std': 0.001, 'type': 'Normal'} ) ", "page_idx": 407, "bbox": [71.99995422363281, 202.8099822998047, 531.0573120117188, 287.8909606933594], "page_size": [612.0, 792.0]}
{"layout": 4359, "type": "text", "text": "A mask point head use in PointRend. ", "page_idx": 407, "bbox": [96, 286.6455078125, 244.46243286132812, 299.9555358886719], "page_size": [612.0, 792.0]}
{"layout": 4360, "type": "text", "text": "Mask Point Head  use shared multi-layer perceptron (equivalent to nn.Conv1d) to predict the logit of input points. The fine-grained feature and coarse feature will be concatenate together for predication. ", "page_idx": 407, "bbox": [96, 304.5775146484375, 540.0018920898438, 329.8435363769531], "page_size": [612.0, 792.0]}
{"layout": 4361, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 407, "bbox": [118, 336, 169, 347], "page_size": [612.0, 792.0]}
{"layout": 4362, "type": "text", "text": "•  num_fcs  ( int ) – Number of fc layers in the head. Default: 3. •  in channels  ( int ) – Number of input channels. Default: 256. •  fc channels  ( int ) – Number of fc channels. Default: 256. •  num classes  ( int ) – Number of classes for logits. Default: 80. •  class agnostic  ( bool ) – Whether use class agnostic classification. If so, the output chan- nels of logits will be 1. Default: False. •  coarse p red each layer  ( bool ) – Whether concatenate coarse feature with the output of each fc layer. Default: True. •  conv_cfg  ( dict | None ) – Dictionary to construct and config conv layer. Default: dict(type  $:=$  ’Conv1d’)) •  norm_cfg  ( dict | None ) – Dictionary to construct and config norm layer. Default: None. •  loss_point  ( dict ) – Dictionary to construct and config loss layer of point head. Default: dict(type  $\\mathrel{\\mathop:}=^{:}$  ’Cross Entropy Loss’, use_mask  $\\risingdotseq$  True, loss weight  $\\mathrm{\\Sigma=}1.0$  ). •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 407, "bbox": [145, 352.3985290527344, 518, 574.9246215820312], "page_size": [612.0, 792.0]}
{"layout": 4363, "type": "text", "text": "forward ( fine grained feats ,  coarse feats ) Classify each point base on fine grained and coarse feats. ", "page_idx": 407, "bbox": [96, 579.3971557617188, 345.85211181640625, 604.8125610351562], "page_size": [612.0, 792.0]}
{"layout": 4364, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 407, "bbox": [136, 611, 188, 622], "page_size": [612.0, 792.0]}
{"layout": 4365, "type": "text", "text": "•  fine grained feats  ( Tensor ) – Fine grained feature sampled from FPN, shape (num_rois, in channels, num_points). •  coarse feats  ( Tensor ) – Coarse feature sampled from Coarse Mask Head, shape (num_rois, num classes, num_points). ", "page_idx": 407, "bbox": [154, 627.3675537109375, 521, 682.5205688476562], "page_size": [612.0, 792.0]}
{"layout": 4366, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 407, "bbox": [137, 689, 172, 699], "page_size": [612.0, 792.0]}
{"layout": 4367, "type": "text", "text": "Point classification results,  shape (num_rois, num_class, num_points). ", "page_idx": 407, "bbox": [154, 704.4489135742188, 445, 718.9843139648438], "page_size": [612.0, 792.0]}
{"layout": 4368, "type": "text", "text": "Return type  Tensor ", "page_idx": 408, "bbox": [137, 70.8248291015625, 220.14431762695312, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 4369, "type": "text", "text": "get roi rel points test ( mask_pred ,  pred_label ,  cfg ) Get  num_points  most uncertain points during test. ", "page_idx": 408, "bbox": [96, 89.23503875732422, 336, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4370, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 408, "bbox": [136, 121, 188, 132], "page_size": [612.0, 792.0]}
{"layout": 4371, "type": "text", "text": "•  mask_pred  ( Tensor ) – A tensor of shape (num_rois, num classes, mask height, mask_width) for class-specific or class-agnostic prediction. •  pred_label  ( list ) – The predication class for each instance. •  cfg  ( dict ) – Testing config of point head. ", "page_idx": 408, "bbox": [154, 137.20545959472656, 521, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 4372, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 408, "bbox": [136, 204, 173, 216], "page_size": [612.0, 792.0]}
{"layout": 4373, "type": "text", "text": "A tensor of shape (num_rois, num_points)  that contains indices from [0, mask height x mask_width) of the most uncertain points. point coord s (Tensor): A tensor of shape (num_rois, num_points, 2)  that contains [0, 1] x [0, 1] normalized coordinates of the most uncertain points from the [mask height, mask_width] grid . ", "page_idx": 408, "bbox": [154, 220.2637939453125, 521, 287.9994812011719], "page_size": [612.0, 792.0]}
{"layout": 4374, "type": "text", "text": "", "page_idx": 408, "bbox": [136, 299.25, 284, 306], "page_size": [612.0, 792.0]}
{"layout": 4375, "type": "text", "text": "get roi rel points train ( mask_pred ,  labels ,  cfg ) Get  num_points  most uncertain points with random points during train. ", "page_idx": 408, "bbox": [96, 310.4060363769531, 408.59613037109375, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 4376, "type": "text", "text": "Sample points in  $[0,1]\\,\\mathtt{x}\\,[0,1]$   coordinate space based on their uncertainty. The uncertainties are calculated for each point using ‘get uncertainty()’ function that takes point’s logit prediction as input. ", "page_idx": 408, "bbox": [118, 340.4434814453125, 540.002685546875, 365.70849609375], "page_size": [612.0, 792.0]}
{"layout": 4377, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 408, "bbox": [136, 372, 188, 383], "page_size": [612.0, 792.0]}
{"layout": 4378, "type": "text", "text": "•  mask_pred  ( Tensor ) – A tensor of shape (num_rois, num classes, mask height, mask_width) for class-specific or class-agnostic prediction. •  labels  ( list ) – The ground truth class for each instance. •  cfg  ( dict ) – Training config of point head. ", "page_idx": 408, "bbox": [154, 388.2644958496094, 521, 449.39453125], "page_size": [612.0, 792.0]}
{"layout": 4379, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 408, "bbox": [136, 455, 173, 466], "page_size": [612.0, 792.0]}
{"layout": 4380, "type": "text", "text": "A tensor of shape (num_rois, num_points, 2)  that contains the coordinates sampled points. Return type  point coord s (Tensor) ", "page_idx": 408, "bbox": [137, 471.3228759765625, 521, 515.7462768554688], "page_size": [612.0, 792.0]}
{"layout": 4381, "type": "text", "text": "get targets ( rois ,  rel roi points ,  sampling results ,  gt_masks ,  cfg ) Get training targets of Mask Point Head for all images. ", "page_idx": 408, "bbox": [96, 519.62109375, 373.58831787109375, 545.0365600585938], "page_size": [612.0, 792.0]}
{"layout": 4382, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 408, "bbox": [136, 551, 187, 563], "page_size": [612.0, 792.0]}
{"layout": 4383, "type": "text", "text": "•  rois  ( Tensor ) – Region of Interest, shape (num_rois, 5). •  rel roi points  – Points coordinates relative to RoI, shape (num_rois, num_points, 2). •  sampling results  ( Sampling Result ) – Sampling result after sampling and assign- ment. •  gt_masks  ( Tensor ) – Ground truth segmentation masks of corresponding boxes, shape (num_rois, height, width). •  cfg  ( dict ) – Training cfg. Returns  Point target, shape (num_rois, num_points). ", "page_idx": 408, "bbox": [137, 567.5914916992188, 521, 695.0733032226562], "page_size": [612.0, 792.0]}
{"layout": 4384, "type": "text", "text": "Return type  Tensor ", "text_level": 1, "page_idx": 408, "bbox": [136, 700.25, 221, 712], "page_size": [612.0, 792.0]}
{"layout": 4385, "type": "text", "text": "loss ( point_pred ,  point targets ,  labels ) Calculate loss for Mask Point Head. ", "page_idx": 409, "bbox": [96, 71.30303192138672, 257, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4386, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 409, "bbox": [136, 103, 188, 114], "page_size": [612.0, 792.0]}
{"layout": 4387, "type": "text", "text": "•  point_pred  ( Tensor ) – Point predication result, shape (num_rois, num classes, num_points). •  point targets  ( Tensor ) – Point targets, shape (num_roi, num_points). •  labels  ( Tensor ) – Class label of corresponding boxes, shape (num_rois, ) ", "page_idx": 409, "bbox": [155, 119.27247619628906, 521, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 4388, "type": "text", "text": "Returns  a dictionary of point loss components ", "page_idx": 409, "bbox": [137, 184.3988037109375, 326.355224609375, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 4389, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 409, "bbox": [137, 202.331787109375, 257, 216.86721801757812], "page_size": [612.0, 792.0]}
{"layout": 4390, "type": "text", "text": "class  mmdet.models.roi_heads. Mask Scoring RoI Head ( mask i ou head ,  \\*\\*kwargs ) Mask Scoring RoIHead for Mask Scoring RCNN. ", "page_idx": 409, "bbox": [72, 220.7419891357422, 434, 246.15647888183594], "page_size": [612.0, 792.0]}
{"layout": 4391, "type": "text", "text": "https://arxiv.org/abs/1903.00241 ", "page_idx": 409, "bbox": [96, 250.77943420410156, 227.3373260498047, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 4392, "type": "text", "text": "simple test mask ( x ,  img_metas ,  det_bboxes ,  det_labels ,  rescale=False ) Obtain mask prediction without augmentation. ", "page_idx": 409, "bbox": [96, 268.56298828125, 400.1894226074219, 293.9774475097656], "page_size": [612.0, 792.0]}
{"layout": 4393, "type": "text", "text": "class  mmdet.models.roi_heads. PISA RoI Head ( b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head  $\\leftrightharpoons$  None , mask roi extractor=None ,  mask_head=None , shared head  $\\leftrightharpoons$  None ,  train_cfg=None ,  test_cfg=None , pretrained=None ,  init_cfg=None ) ", "page_idx": 409, "bbox": [72, 298.45098876953125, 499.625732421875, 347.7754211425781], "page_size": [612.0, 792.0]}
{"layout": 4394, "type": "text", "text": "The RoI head for  Prime Sample Attention in Object Detection . ", "page_idx": 409, "bbox": [96, 346.42138671875, 346.3008117675781, 359.7314147949219], "page_size": [612.0, 792.0]}
{"layout": 4395, "type": "text", "text": "forward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , gt_mask  $\\mathbf{\\hat{\\rho}}=$  None ) Forward function for training. ", "page_idx": 409, "bbox": [96, 364.2039489746094, 472, 401.57440185546875], "page_size": [612.0, 792.0]}
{"layout": 4396, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 409, "bbox": [136, 408, 188, 419], "page_size": [612.0, 792.0]}
{"layout": 4397, "type": "text", "text": "•  x  ( list[Tensor] ) – List of multi-level img features. ", "page_idx": 409, "bbox": [155, 424.12939453125, 378.48541259765625, 437.4394226074219], "page_size": [612.0, 792.0]}
{"layout": 4398, "type": "text", "text": "•  img_metas ( list[dict] ) – List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposals  ( list[Tensors] ) – List of region proposals. •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box •  gt b boxes ignore  ( list[Tensor], optional ) – Specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – True segmentation masks for each box used if the archi- tecture supports a segmentation task. ", "page_idx": 409, "bbox": [155, 442.0624084472656, 521, 616.7673950195312], "page_size": [612.0, 792.0]}
{"layout": 4399, "type": "text", "text": "", "page_idx": 409, "bbox": [136, 626.25, 304, 634], "page_size": [612.0, 792.0]}
{"layout": 4400, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 409, "bbox": [137, 638.6947631835938, 257, 653.2301635742188], "page_size": [612.0, 792.0]}
{"layout": 4401, "type": "text", "text": "class  mmdet.models.roi_heads. Point Rend RoI Head ( point_head ,  \\*args ,  \\*\\*kwargs ) PointRend . ", "page_idx": 409, "bbox": [72, 657.10595703125, 434, 682.5204467773438], "page_size": [612.0, 792.0]}
{"layout": 4402, "type": "text", "text": "aug test mask ( feats ,  img_metas ,  det_bboxes ,  det_labels ) Test for mask head with test time augmentation. ", "page_idx": 409, "bbox": [96, 686.9939575195312, 337.08135986328125, 712.408447265625], "page_size": [612.0, 792.0]}
{"layout": 4403, "type": "text", "text": "in it point head ( point_head ) Initialize  point_head ", "page_idx": 410, "bbox": [96, 71.30303192138672, 231.2173614501953, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4404, "type": "text", "text": "mask on nx export ( x ,  img_metas ,  det_bboxes ,  det_labels ,  \\*\\*kwargs ) Export mask branch to onnx which supports batch inference. ", "page_idx": 410, "bbox": [96, 101.19103240966797, 381, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 4405, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 410, "bbox": [136, 133, 188, 144], "page_size": [612.0, 792.0]}
{"layout": 4406, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Feature maps of all scale level. •  img_metas  ( list[dict] ) – Image meta info. •  det_bboxes  ( Tensor ) – Bboxes and corresponding scores. has shape [N, num_bboxes, 5]. •  det_labels  ( Tensor ) – class labels of shape [N, num_bboxes]. ", "page_idx": 410, "bbox": [154, 149.1604766845703, 521, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 4407, "type": "text", "text": "Returns ", "page_idx": 410, "bbox": [137, 232.21881103515625, 171.46726989746094, 246.75424194335938], "page_size": [612.0, 792.0]}
{"layout": 4408, "type": "text", "text": "The segmentation results of shape [N, num_bboxes,  image height, image width]. ", "page_idx": 410, "bbox": [154, 250.15179443359375, 493.39935302734375, 264.6872253417969], "page_size": [612.0, 792.0]}
{"layout": 4409, "type": "text", "text": "Return type  Tensor ", "page_idx": 410, "bbox": [137, 268.08477783203125, 220.1442413330078, 282.6202087402344], "page_size": [612.0, 792.0]}
{"layout": 4410, "type": "text", "text": "simple test mask (  $\\cdot_{x}$  ,  img_metas ,  det_bboxes ,  det_labels ,  rescale  $\\mathbf{=}$  False ) Obtain mask prediction without augmentation. ", "page_idx": 410, "bbox": [96, 286.4960021972656, 400.1893005371094, 311.91046142578125], "page_size": [612.0, 792.0]}
{"layout": 4411, "type": "text", "text": "class  mmdet.models.roi_heads. ResLayer ( depth ,  stage  $\\scriptstyle{:=3}$  ,  stride  $_{:=2}$  ,  dilation  ${=}I$  ,  style  $=$  'pytorch' , norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} ,  norm_eval=True , with_cp $\\leftrightharpoons$ False, dcn $=$ None, pretrained $\\leftrightharpoons$ None, init_cfg $\\mathbf{\\hat{\\mu}}$ None)", "page_idx": 410, "bbox": [71.99992370605469, 316.3840026855469, 532.8704223632812, 353.7534484863281], "page_size": [612.0, 792.0]}
{"layout": 4412, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 410, "bbox": [96, 372, 149, 383], "page_size": [612.0, 792.0]}
{"layout": 4413, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 410, "bbox": [118, 382.2864074707031, 313.48394775390625, 413.5294494628906], "page_size": [612.0, 792.0]}
{"layout": 4414, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 410, "bbox": [118, 429.4798278808594, 540, 467.3274841308594], "page_size": [612.0, 792.0]}
{"layout": 4415, "type": "text", "text": "train  $(m o d e{=}T r u e)$  ", "page_idx": 410, "bbox": [96, 485, 178, 496.53802490234375], "page_size": [612.0, 792.0]}
{"layout": 4416, "type": "text", "text": "Sets the module in training mode. ", "page_idx": 410, "bbox": [118, 495.8604431152344, 253.31906127929688, 509.17047119140625], "page_size": [612.0, 792.0]}
{"layout": 4417, "type": "text", "text": "This has any effect only on certain modules. See documentation s of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g.  Dropout ,  BatchNorm , etc. ", "page_idx": 410, "bbox": [118, 513.7933959960938, 540, 539.0584716796875], "page_size": [612.0, 792.0]}
{"layout": 4418, "type": "text", "text": "Parameters  mode  ( bool ) – whether to set training mode ( True ) or evaluation mode ( False ). Default:  True . ", "page_idx": 410, "bbox": [137, 543.0538330078125, 521, 568.9464721679688], "page_size": [612.0, 792.0]}
{"layout": 4419, "type": "text", "text": "Returns  self ", "page_idx": 410, "bbox": [137, 572.9417724609375, 190.83396911621094, 587.4771728515625], "page_size": [612.0, 792.0]}
{"layout": 4420, "type": "text", "text": "Return type  Module ", "page_idx": 410, "bbox": [137, 590.873779296875, 224.31858825683594, 605.4091796875], "page_size": [612.0, 792.0]}
{"layout": 4421, "type": "text", "text": "class  mmdet.models.roi_heads. SABLHead ( num classes ,  cls in channels=256 ,  reg in channel  $s{=}256$  , roi feat size=7 ,  reg feat up ratio=2 ,  reg pre kernel=3 , reg post kernel  $!\\!=\\!\\!3$  ,  reg pre num=2 ,  reg post num  $\\scriptstyle{=I}$  , cls out channels  $:=$  1024 ,  reg offset out channels=256 , reg cls out channels=256 ,  num cls fcs=1 ,  num reg fcs=0 , reg class agnostic=True ,  norm_cfg  $=$  None , bbox_coder={'num buckets': 14, 'scale factor': 1.7, 'type': 'Bucketing B Box Code r'} ,  loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': False} , loss b box cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True}, loss b box reg $=$ {'beta': 0.1, 'loss weight':1.0, 'type': 'Smooth L 1 Loss'} ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) ", "page_idx": 411, "bbox": [72.0, 71.30303192138672, 526.4151611328125, 216.26963806152344], "page_size": [612.0, 792.0]}
{"layout": 4422, "type": "text", "text": "Side-Aware Boundary Localization (SABL) for RoI-Head. ", "page_idx": 411, "bbox": [96, 214.9146270751953, 329.6328125, 228.2246551513672], "page_size": [612.0, 792.0]}
{"layout": 4423, "type": "text", "text": "Side-Aware features are extracted by conv layers with an attention mechanism. Boundary Localization with Bucketing and Bucketing Guided Rescoring are implemented in Bucketing B Box Code r. ", "page_idx": 411, "bbox": [96, 232.8466339111328, 540.0031127929688, 258.1126708984375], "page_size": [612.0, 792.0]}
{"layout": 4424, "type": "text", "text": "Please refer to  https://arxiv.org/abs/1912.04260  for more details. ", "page_idx": 411, "bbox": [96, 262.7346496582031, 354.17071533203125, 276.044677734375], "page_size": [612.0, 792.0]}
{"layout": 4425, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 411, "bbox": [117, 283, 169, 293], "page_size": [612.0, 792.0]}
{"layout": 4426, "type": "text", "text": "•  cls in channels  ( int ) – Input channels of cls RoI feature. Defaults to 256. •  reg in channels  ( int ) – Input channels of reg RoI feature. Defaults to 256. •  roi feat size  ( int ) – Size of RoI features. Defaults to 7. •  reg feat up ratio  ( int ) – Upsample ratio of reg features. Defaults to 2. •  reg pre kernel  ( int ) – Kernel of 2D conv layers before attention pooling. Defaults to 3. •  reg post kernel  ( int ) – Kernel of 1D conv layers after attention pooling. Defaults to 3. •  reg pre num  ( int ) – Number of pre convs. Defaults to 2. •  reg post num  ( int ) – Number of post convs. Defaults to 1. •  num classes  ( int ) – Number of classes in dataset. Defaults to 80. •  cls out channels  ( int ) – Hidden channels in cls fcs. Defaults to 1024. •  reg offset out channels  ( int ) – Hidden and output channel of reg offset branch. De- faults to 256. •  reg cls out channels  ( int ) – Hidden and output channel of reg cls branch. Defaults to 256. •  num cls fcs  ( int ) – Number of fcs for cls branch. Defaults to 1. •  num reg fcs  ( int ) – Number of fcs for reg branch.. Defaults to 0. •  reg class agnostic  ( bool ) – Class agnostic regression or not. Defaults to True. •  norm_cfg  ( dict ) – Config of norm layers. Defaults to None. •  bbox_coder  ( dict ) – Config of bbox coder. Defaults ‘Bucketing B Box Code r’. •  loss_cls  ( dict ) – Config of classification loss. •  loss b box cls  ( dict ) – Config of classification loss for bbox branch. •  loss b box reg  ( dict ) – Config of regression loss for bbox branch. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 411, "bbox": [145, 298.6006774902344, 519, 694.475830078125], "page_size": [612.0, 792.0]}
{"layout": 4427, "type": "text", "text": "attention pool ( reg_x ) Extract direction-specific features fx and fy with attention methanism. ", "page_idx": 412, "bbox": [96, 71.30303192138672, 397.65728759765625, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4428, "type": "text", "text": "b box p red split ( bbox_pred ,  num proposals per img ) Split batch bbox prediction back to each image. ", "page_idx": 412, "bbox": [96, 101.19103240966797, 332.19439697265625, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 4429, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 412, "bbox": [96, 132.95199584960938, 313, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 4430, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 412, "bbox": [118, 190.37579345703125, 540, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 4431, "type": "text", "text": "refine b boxes ( rois ,  labels ,  bbox_preds ,  pos_is_gts ,  img_metas ) Refine bboxes during training. ", "page_idx": 412, "bbox": [96, 244.6529998779297, 366, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 4432, "type": "text", "text": "Parameters •  rois  ( Tensor ) – Shape (n\\*bs, 5), where n is image number per GPU, and bs is the sampled RoIs per image. •  labels  ( Tensor ) – Shape (n\\*bs, ). •  bbox_preds ( list[Tensor] ) – Shape [(n\\*bs, num buckets\\*2), (n\\*bs, num buckets\\*2)]. •  pos_is_gts  ( list[Tensor] ) – Flags indicating if each positive bbox is a gt bbox. •  img_metas  ( list[dict] ) – Meta info of each image. Returns  Refined bboxes of each image in a mini-batch. Return type  list[Tensor] ", "page_idx": 412, "bbox": [137, 274.0628356933594, 521, 438.0373229980469], "page_size": [612.0, 792.0]}
{"layout": 4433, "type": "text", "text": "reg_pred ( x ,  offset_fcs ,  cls_fcs ) Predict bucketing estimation (cls_pred) and fine regression (offset pred) with side-aware features. ", "page_idx": 412, "bbox": [96, 441.9131164550781, 505.8221435546875, 467.32757568359375], "page_size": [612.0, 792.0]}
{"layout": 4434, "type": "text", "text": "regress by class ( rois ,  label ,  bbox_pred ,  img_meta ) Regress the bbox for the predicted class. Used in Cascade R-CNN. Parameters •  rois  ( Tensor ) – shape (n, 4) or (n, 5) •  label  ( Tensor ) – shape (n, ) •  bbox_pred  ( list[Tensor] ) – shape [(n, num buckets  $^{*}2$  ), (n, num buckets    $^{*}2$  )] •  img_meta  ( dict ) – Image meta info. Returns  Regressed bboxes, the same shape as input rois. Return type  Tensor ", "page_idx": 412, "bbox": [96, 471.8011169433594, 494.779296875, 623.34228515625], "page_size": [612.0, 792.0]}
{"layout": 4435, "type": "text", "text": "side aware feature extractor ( reg_x ) Refine and extract side-aware features without split them. ", "page_idx": 412, "bbox": [96, 627.2180786132812, 346.97845458984375, 652.632568359375], "page_size": [612.0, 792.0]}
{"layout": 4436, "type": "text", "text": "side aware split ( feat ) Split side-aware features aligned with orders of bucketing targets. ", "page_idx": 412, "bbox": [96, 657.1061401367188, 379.1476745605469, 682.5205688476562], "page_size": [612.0, 792.0]}
{"layout": 4437, "type": "text", "text": "class  mmdet.models.roi_heads. S CNet B Box Head ( num shared con vs  $\\mathrel{\\mathop:}=\\!\\!O$  ,  num shared fc  $\\cdot_{s=0}$  , num cls con vs  $\\scriptstyle{\\prime=0}$  ,  num_cls_fc  $s{=}0.$  ,  num reg con vs  $\\imath\\!=\\!\\!O$  , num_reg_fc  $\\cdot_{S=0}$  ,  con v out channel  $\\mathfrak{s}{=}256$  , fc out channels  $=$  1024 ,  conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $=$  None , init_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  \\*args ,  \\*\\*kwargs ) ", "page_idx": 413, "bbox": [71, 71.30303192138672, 532, 132.5825653076172], "page_size": [612.0, 792.0]}
{"layout": 4438, "type": "text", "text": "BBox head for  SCNet . ", "page_idx": 413, "bbox": [96, 131.22853088378906, 186, 144.53855895996094], "page_size": [612.0, 792.0]}
{"layout": 4439, "type": "text", "text": "This inherits  Con v FCB Box Head  with modified forward() function, allow us to get intermediate shared feature. ", "page_idx": 413, "bbox": [96, 149.16053771972656, 532, 162.47056579589844], "page_size": [612.0, 792.0]}
{"layout": 4440, "type": "text", "text": "forward (  $\\dot{x}_{i}$  ,  return shared feat  $\\fallingdotseq$  False ) Forward function. ", "page_idx": 413, "bbox": [96, 166.94407653808594, 257.5442810058594, 192.3585662841797], "page_size": [612.0, 792.0]}
{"layout": 4441, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 413, "bbox": [136, 198, 188, 210], "page_size": [612.0, 792.0]}
{"layout": 4442, "type": "text", "text": "•  x  ( Tensor ) – input features •  return shared feat  ( bool ) – If True, return cls-reg-shared feature. ", "page_idx": 413, "bbox": [154, 214.9145050048828, 446.7090759277344, 246.1565399169922], "page_size": [612.0, 792.0]}
{"layout": 4443, "type": "text", "text": "Returns ", "page_idx": 413, "bbox": [137, 250.15185546875, 171.46722412109375, 264.6872863769531], "page_size": [612.0, 792.0]}
{"layout": 4444, "type": "text", "text": "contain  cls_score  and  bbox_pred ,  if  return shared feat  is True, append  x_shared to the returned tuple. Return type  out (tuple[Tensor]) ", "page_idx": 413, "bbox": [137, 268.0848388671875, 521, 312.5082702636719], "page_size": [612.0, 792.0]}
{"layout": 4445, "type": "text", "text": "class  mmdet.models.roi_heads. S CNet Mask Head ( con v to res  $=$  True ,  \\*\\*kwargs ) Mask head for  SCNet . ", "page_idx": 413, "bbox": [71, 316.3840637207031, 418.57427978515625, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 4446, "type": "text", "text": "Parameters  con v to res ( bool, optional ) – if True, change the conv layers to Simplified Basic Block . ", "page_idx": 413, "bbox": [118, 345.7938537597656, 518, 371.6865234375], "page_size": [612.0, 792.0]}
{"layout": 4447, "type": "text", "text": "class  mmdet.models.roi_heads. S CNet RoI Head ( num_stages ,  stage loss weights , semantic roi extractor=None ,  semantic head  $\\leftrightharpoons$  None , feat relay head  $\\leftrightharpoons$  None ,  gl bc tx head=None ,  \\*\\*kwargs ) ", "page_idx": 413, "bbox": [71, 376.1590576171875, 512.6422119140625, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 4448, "type": "text", "text": "RoIHead for  SCNet . ", "page_idx": 413, "bbox": [96, 412.1744689941406, 177.6337890625, 425.4844970703125], "page_size": [612.0, 792.0]}
{"layout": 4449, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 413, "bbox": [116, 431, 170, 443], "page_size": [612.0, 792.0]}
{"layout": 4450, "type": "text", "text": "•  num_stages  ( int ) – number of cascade stages. •  stage loss weights  ( list ) – loss weight of cascade stages. •  semantic roi extractor  ( dict ) – config to init semantic roi extractor. •  semantic head  ( dict ) – config to init semantic head. •  feat relay head  ( dict ) – config to init feature relay head. •  gl bc tx head  ( dict ) – config to init global context head. ", "page_idx": 413, "bbox": [145, 448.0404968261719, 451.7550964355469, 551.0136108398438], "page_size": [612.0, 792.0]}
{"layout": 4451, "type": "text", "text": "aug_test ( img_feats ,  proposal list ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 413, "bbox": [96, 555.4871215820312, 432, 598.8345947265625], "page_size": [612.0, 792.0]}
{"layout": 4452, "type": "text", "text": "forward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , gt_mask  $\\mathfrak{s}{=}$  None ,  gt semantic seg  $=$  None ) ", "page_idx": 413, "bbox": [96, 603.30810546875, 472.1894836425781, 628.7225952148438], "page_size": [612.0, 792.0]}
{"layout": 4453, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 413, "bbox": [136, 646, 188, 658], "page_size": [612.0, 792.0]}
{"layout": 4454, "type": "text", "text": "•  x  ( list[Tensor] ) – list of multi-level img features. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ", "page_idx": 413, "bbox": [154, 663.2335205078125, 521, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 4455, "type": "text", "text": "‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . ", "page_idx": 414, "bbox": [164, 71.45246887207031, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4456, "type": "text", "text": "•  proposal list  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None, list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None, Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. •  gt semantic seg  ( None, list[Tensor] ) – semantic segmentation masks used if the architecture supports semantic segmentation task. ", "page_idx": 414, "bbox": [154, 101.34046936035156, 521, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 4457, "type": "text", "text": "Returns  a dictionary of loss components ", "page_idx": 414, "bbox": [137, 256.12982177734375, 305, 270.6652526855469], "page_size": [612.0, 792.0]}
{"layout": 4458, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 414, "bbox": [137, 274.0628356933594, 256.1192932128906, 288.5982666015625], "page_size": [612.0, 792.0]}
{"layout": 4459, "type": "text", "text": "in it mask head ( mask roi extractor ,  mask_head ) Initialize  mask_head ", "page_idx": 414, "bbox": [96, 292.4730529785156, 308.94635009765625, 317.88751220703125], "page_size": [612.0, 792.0]}
{"layout": 4460, "type": "text", "text": "simple test ( x ,  proposal list ,  img_metas ,  rescale=False ) Test without augmentation. ", "page_idx": 414, "bbox": [96, 322.3610534667969, 336.1193542480469, 347.7755126953125], "page_size": [612.0, 792.0]}
{"layout": 4461, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 414, "bbox": [136, 353, 188, 365], "page_size": [612.0, 792.0]}
{"layout": 4462, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). •  proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). •  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. ", "page_idx": 414, "bbox": [154, 370.3315124511719, 521, 461.34954833984375], "page_size": [612.0, 792.0]}
{"layout": 4463, "type": "text", "text": "Returns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. ", "page_idx": 414, "bbox": [137, 465.3448791503906, 521, 527.103515625], "page_size": [612.0, 792.0]}
{"layout": 4464, "type": "text", "text": "Return type  list[list[np.ndarray]] or list[tuple] ", "page_idx": 414, "bbox": [137, 531.0988159179688, 325.2197265625, 545.6342163085938], "page_size": [612.0, 792.0]}
{"layout": 4465, "type": "text", "text": "property with feat relay whether the head has feature relay head ", "page_idx": 414, "bbox": [96, 551.3820190429688, 275.8554992675781, 574.9244995117188], "page_size": [612.0, 792.0]}
{"layout": 4466, "type": "text", "text": "property with gl bc tx whether the head has global context head ", "page_idx": 414, "bbox": [96, 599.2030029296875, 282.8193054199219, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 4467, "type": "text", "text": "Type  bool ", "page_idx": 414, "bbox": [137, 626.7398071289062, 180, 641.2752075195312], "page_size": [612.0, 792.0]}
{"layout": 4468, "type": "text", "text": "property with semantic whether the head has semantic head ", "page_idx": 414, "bbox": [96, 647.0240478515625, 261.8580322265625, 670.5654907226562], "page_size": [612.0, 792.0]}
{"layout": 4469, "type": "text", "text": "Type  bool ", "page_idx": 414, "bbox": [137, 674.560791015625, 180, 689.09619140625], "page_size": [612.0, 792.0]}
{"layout": 4470, "type": "text", "text": "class  mmdet.models.roi_heads. S CNet Semantic Head ( con v to res  $\\mathbf{=}$  True ,  \\*\\*kwargs ) Mask head for  SCNet . ", "page_idx": 414, "bbox": [71.99996185302734, 692.9710083007812, 439.496337890625, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 4471, "type": "text", "text": "Parameters  con v to res ( bool, optional ) – if True, change the conv layers to Simplified Basic Block . ", "page_idx": 415, "bbox": [118, 70.8248291015625, 519, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4472, "type": "text", "text": "class  mmdet.models.roi_heads. Shared 2 FCB Box Head ( fc out channels  $=$  1024 ,  \\*args ,  \\*\\*kwargs ) ", "page_idx": 415, "bbox": [71, 101.19103240966797, 488, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4473, "type": "text", "text": "class  mmdet.models.roi_heads. Shared 4 Con v 1 FCB Box Head ( fc out channel  $\\mathbf{:=}_{1}$  1024 ,  \\*args ,  \\*\\*kwargs ) ", "page_idx": 415, "bbox": [71, 119.12303924560547, 513, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 4474, "type": "text", "text": "class  mmdet.models.roi_heads. Single RoI Extractor ( roi_layer ,  out channels ,  feat map strides , finest scale=56 ,  init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 415, "bbox": [71, 137.05601501464844, 488, 162.4705047607422], "page_size": [612.0, 792.0]}
{"layout": 4475, "type": "text", "text": "Extract RoI features from a single level feature map. ", "page_idx": 415, "bbox": [96, 161.11549377441406, 304.6969299316406, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 4476, "type": "text", "text": "If there are multiple input feature levels, each RoI is mapped to a level according to its scale. The mapping rule is proposed in  FPN . ", "page_idx": 415, "bbox": [96, 179.04847717285156, 540, 204.3135223388672], "page_size": [612.0, 792.0]}
{"layout": 4477, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 415, "bbox": [117, 211, 169, 221], "page_size": [612.0, 792.0]}
{"layout": 4478, "type": "text", "text": "•  roi_layer  ( dict ) – Specify RoI layer type and arguments. •  out channels  ( int ) – Output channels of RoI layers. •  feat map strides  ( List[int] ) – Strides of input feature maps. •  finest scale  ( int ) – Scale threshold of mapping to level 0. Default: 56. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 415, "bbox": [145, 226.8694610595703, 519, 311.9105224609375], "page_size": [612.0, 792.0]}
{"layout": 4479, "type": "text", "text": "forward ( feats ,  rois ,  roi scale factor  $\\leftrightharpoons$  None ) Forward function. ", "page_idx": 415, "bbox": [96, 316.3840637207031, 280.4983215332031, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 4480, "type": "text", "text": "map roi levels ( rois ,  num_levels ) Map rois to corresponding feature levels by scales. • scale  $<$   finest scale  $^{*}\\,2$  : level 0 • finest scale  $^*\\;2<=$  scale  $<$   finest scale \\* 4: level 1 • finest scale  $^*4<=$  scale  $<$   finest scale  $^\\ast\\,8$  : level 2 • scale  $>=$   finest scale  $^{*}\\mathrm{~}8$  : level 3 ", "page_idx": 415, "bbox": [96, 346.2720642089844, 339, 443.4175720214844], "page_size": [612.0, 792.0]}
{"layout": 4481, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 415, "bbox": [135, 455, 188, 466], "page_size": [612.0, 792.0]}
{"layout": 4482, "type": "text", "text": "•  rois  ( Tensor ) – Input RoIs, shape (k, 5). •  num_levels  ( int ) – Total level number. Returns  Level index (0-based) of each RoI, shape (k, ) Return type  Tensor ", "page_idx": 415, "bbox": [137, 471.9505615234375, 360, 539.6563110351562], "page_size": [612.0, 792.0]}
{"layout": 4483, "type": "text", "text": "class  mmdet.models.roi_heads. Sparse RoI Head ( num_stages  $\\scriptstyle{\\prime=6}$  ,  stage_loss_weights=(1, 1, 1, 1, 1, 1) , ", "page_idx": 415, "bbox": [71, 549.5091552734375, 506.838623046875, 562.9686279296875], "page_size": [612.0, 792.0]}
{"layout": 4484, "type": "text", "text": "proposal feature ch anne  $l{=}256$  , b box roi extractor={'feat map strides': [4, 8, 16, 32], 'out channels': 256, 'roi_layer': {'output size': 7, 'sampling ratio': 2, 'type': 'RoIAlign'}, 'type': 'Single RoI Extractor'} ,  mask roi extractor=None , bbox_head={'dropout': 0.0, 'feed forward channels': 2048, 'ff n act cf g': {'inplace': True, 'type': 'ReLU'}, 'hidden channels': 256, 'num classes': 80, 'num cls fcs': 1, 'num_fcs': 2, 'num_heads': 8, 'num_reg_fcs': 3, 'roi feat size': 7, 'type': 'DIIHead'} ,  mask_head  $\\leftrightharpoons$  None , train_cfg  $=$  None ,  test_cfg  $=$  None ,  pretrained  $\\fallingdotseq$  None , init_cfg=None ) ", "page_idx": 415, "bbox": [296, 561.4651489257812, 540, 706.3209838867188], "page_size": [612.0, 792.0]}
{"layout": 4485, "type": "text", "text": "The RoIHead for  Sparse R-CNN: End-to-End Object Detection with Learnable Proposals  and  Instances as Queries ", "page_idx": 415, "bbox": [96, 705.0765380859375, 540, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 4486, "type": "text", "text": "", "page_idx": 416, "bbox": [96, 71.45246887207031, 128.08993530273438, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 4487, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 416, "bbox": [117, 90, 169, 102], "page_size": [612.0, 792.0]}
{"layout": 4488, "type": "text", "text": "•  num_stages  ( int ) – Number of stage whole iterative process. Defaults to 6. •  stage loss weights  ( Tuple[float] ) – The loss weight of each stage. By default all stages have the same weight 1. •  b box roi extractor  ( dict ) – Config of box roi extractor. •  mask roi extractor  ( dict ) – Config of mask roi extractor. •  bbox_head  ( dict ) – Config of box head. •  mask_head  ( dict ) – Config of mask head. •  train_cfg  ( dict, optional ) – Configuration information in train stage. Defaults to None. •  test_cfg  ( dict, optional ) – Configuration information in test stage. Defaults to None. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 416, "bbox": [145, 107.31745910644531, 518, 305.9324645996094], "page_size": [612.0, 792.0]}
{"layout": 4489, "type": "text", "text": "aug_test ( features ,  proposal list ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test with augmentations. ", "page_idx": 416, "bbox": [96, 310.406005859375, 347.50634765625, 335.8204650878906], "page_size": [612.0, 792.0]}
{"layout": 4490, "type": "text", "text": "If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 416, "bbox": [118, 340.4434509277344, 432.90496826171875, 353.75347900390625], "page_size": [612.0, 792.0]}
{"layout": 4491, "type": "text", "text": "forward dummy ( x ,  proposal boxes ,  proposal features ,  img_metas ) Dummy forward function when do the flops computing. ", "page_idx": 416, "bbox": [96, 358.2270202636719, 372.00933837890625, 383.6414794921875], "page_size": [612.0, 792.0]}
{"layout": 4492, "type": "text", "text": "forward train ( x ,  proposal boxes ,  proposal features ,  img_metas ,  gt_bboxes ,  gt_labels , gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None ,  imgs_whwh  $=$  None ,  gt_mask  $\\leftrightharpoons$  None ) Forward function in training stage. ", "page_idx": 416, "bbox": [96, 388.1150207519531, 455.8506164550781, 425.4844665527344], "page_size": [612.0, 792.0]}
{"layout": 4493, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 416, "bbox": [136, 432, 188, 443], "page_size": [612.0, 792.0]}
{"layout": 4494, "type": "text", "text": "•  x  ( list[Tensor] ) – list of multi-level img features. •  proposals  ( Tensor ) – Decoded proposal bboxes, has shape (batch_size, num proposals, 4) •  proposal features  ( Tensor ) – Expanded proposal features, has shape (batch_size, num proposals, proposal feature channel) •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  imgs_whwh  ( Tensor ) – Tensor with shape (batch_size, 4), the dimension means [img_width,img_height, img_width, img_height]. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. ", "page_idx": 416, "bbox": [154, 448.0394592285156, 521, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 4495, "type": "text", "text": "Returns  a dictionary of loss components of all stage. ", "page_idx": 417, "bbox": [137, 70.8248291015625, 351, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 4496, "type": "text", "text": "Return type  dict[str, Tensor] ", "page_idx": 417, "bbox": [137, 88.7568359375, 256.1192932128906, 103.29226684570312], "page_size": [612.0, 792.0]}
{"layout": 4497, "type": "text", "text": "simple test (  $\\dot{\\boldsymbol{x}}.$  ,  proposal boxes ,  proposal features ,  img_metas ,  imgs_whwh ,  rescale=False ) Test without augmentation. ", "page_idx": 417, "bbox": [96, 107.16802215576172, 475.09832763671875, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 4498, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 417, "bbox": [137, 139, 187, 150], "page_size": [612.0, 792.0]}
{"layout": 4499, "type": "text", "text": "•  x  ( list[Tensor] ) – list of multi-level img features. •  proposal boxes  ( Tensor ) – Decoded proposal bboxes, has shape (batch_size, num proposals, 4) •  proposal features  ( Tensor ) – Expanded proposal features, has shape (batch_size, num proposals, proposal feature channel) •  img_metas  ( dict ) – meta information of images. •  imgs_whwh  ( Tensor ) – Tensor with shape (batch_size, 4), the dimension means [img_width,img_height, img_width, img_height]. •  rescale  ( bool ) – If True, return boxes in original image space. Defaults to False. ", "page_idx": 417, "bbox": [154, 155.13844299316406, 521, 293.9775085449219], "page_size": [612.0, 792.0]}
{"layout": 4500, "type": "text", "text": "Returns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has a mask branch, it is a list[tuple] that contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. ", "page_idx": 417, "bbox": [137, 297.97283935546875, 521, 359.7314758300781], "page_size": [612.0, 792.0]}
{"layout": 4501, "type": "text", "text": "Return type  list[list[np.ndarray]] or list[tuple] ", "page_idx": 417, "bbox": [137, 363.7257995605469, 325.2197265625, 378.26123046875], "page_size": [612.0, 792.0]}
{"layout": 4502, "type": "text", "text": "class  mmdet.models.roi_heads. Standard RoI Head ( b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head  $\\leftrightharpoons$  None , mask roi extractor  $=$  None ,  mask_head  $\\leftrightharpoons$  None , shared head  $\\leftrightharpoons$  None ,  train_cfg  $\\mathbf{\\beta}=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) ", "page_idx": 417, "bbox": [71.99995422363281, 382.13702392578125, 521, 431.46246337890625], "page_size": [612.0, 792.0]}
{"layout": 4503, "type": "text", "text": "Simplest base roi head including one bbox head and one mask head. ", "page_idx": 417, "bbox": [96, 430.107421875, 368.12872314453125, 443.4174499511719], "page_size": [612.0, 792.0]}
{"layout": 4504, "type": "text", "text": "async a sync simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\leftrightharpoons$  None ,  rescale=False ) Async test without augmentation. ", "page_idx": 417, "bbox": [96, 447.8899841308594, 470.42626953125, 473.3054504394531], "page_size": [612.0, 792.0]}
{"layout": 4505, "type": "text", "text": "aug_test (  $\\dot{\\boldsymbol{x}}.$  ,  proposal list ,  img_metas ,  rescale  $=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. ", "page_idx": 417, "bbox": [96, 477.7779846191406, 432.9058837890625, 521.1254272460938], "page_size": [612.0, 792.0]}
{"layout": 4506, "type": "text", "text": "b box on nx export (  $\\cdot_{x}$  ,  img_metas ,  proposals ,  r cnn test cf g ,  \\*\\*kwargs ) Export bbox branch to onnx which supports batch inference. ", "page_idx": 417, "bbox": [96, 525.5989990234375, 389.56427001953125, 551.0134887695312], "page_size": [612.0, 792.0]}
{"layout": 4507, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 417, "bbox": [136, 557, 188, 569], "page_size": [612.0, 792.0]}
{"layout": 4508, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Feature maps of all scale level. •  img_metas  ( list[dict] ) – Image meta info. •  proposals  ( Tensor ) – Region proposals with batch dimension, has shape [N, num_bboxes, 5]. •  (obj  ( r cnn test cf g ) –  ConfigDict ):  test_cfg  of R-CNN. ", "page_idx": 417, "bbox": [154, 573.5693969726562, 521, 652.6324462890625], "page_size": [612.0, 792.0]}
{"layout": 4509, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 417, "bbox": [136, 659, 173, 670], "page_size": [612.0, 792.0]}
{"layout": 4510, "type": "text", "text": "bboxes of shape [N, num_bboxes, 5]  and class labels of shape [N, num_bboxes]. Return type  tuple[Tensor, Tensor] ", "page_idx": 417, "bbox": [137, 674.560791015625, 481.97174072265625, 707.0281982421875], "page_size": [612.0, 792.0]}
{"layout": 4511, "type": "text", "text": "forward dummy ( x ,  proposals ) Dummy forward function. ", "page_idx": 418, "bbox": [96, 71.30303192138672, 223, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4512, "type": "text", "text": "forward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None , gt_mask  $\\mathfrak{s}{=}$  None ,  \\*\\*kwargs ) ", "page_idx": 418, "bbox": [96, 101.19103240966797, 472.1886291503906, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 4513, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 418, "bbox": [136, 144, 188, 156], "page_size": [612.0, 792.0]}
{"layout": 4514, "type": "text", "text": "•  x  ( list[Tensor] ) – list of multi-level img features. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposals  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. Returns  a dictionary of loss components Return type  dict[str, Tensor] ", "page_idx": 418, "bbox": [137, 161.11549377441406, 521, 390.2163391113281], "page_size": [612.0, 792.0]}
{"layout": 4515, "type": "text", "text": "in it as signer sampler () Initialize assigner and sampler. ", "page_idx": 418, "bbox": [96, 395.965087890625, 242.28050231933594, 419.506591796875], "page_size": [612.0, 792.0]}
{"layout": 4516, "type": "text", "text": "in it b box head ( b box roi extractor ,  bbox_head ) Initialize  bbox_head ", "page_idx": 418, "bbox": [96, 423.9801330566406, 308, 449.39459228515625], "page_size": [612.0, 792.0]}
{"layout": 4517, "type": "text", "text": "in it mask head ( mask roi extractor ,  mask_head ) Initialize  mask_head ", "page_idx": 418, "bbox": [96, 453.8681335449219, 308, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 4518, "type": "text", "text": "mask on nx export ( x ,  img_metas ,  det_bboxes ,  det_labels ,  \\*\\*kwargs ) Export mask branch to onnx which supports batch inference. ", "page_idx": 418, "bbox": [96, 483.7561340332031, 381, 509.17059326171875], "page_size": [612.0, 792.0]}
{"layout": 4519, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 418, "bbox": [136, 515, 188, 527], "page_size": [612.0, 792.0]}
{"layout": 4520, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Feature maps of all scale level. •  img_metas  ( list[dict] ) – Image meta info. •  det_bboxes  ( Tensor ) – Bboxes and corresponding scores. has shape [N, num_bboxes, 5]. •  det_labels  ( Tensor ) – class labels of shape [N, num_bboxes]. ", "page_idx": 418, "bbox": [155, 531.7265625, 521, 610.7896118164062], "page_size": [612.0, 792.0]}
{"layout": 4521, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 418, "bbox": [136, 617, 173, 628], "page_size": [612.0, 792.0]}
{"layout": 4522, "type": "text", "text": "on nx export (  $\\cdot x,$  ,  proposals ,  img_metas ,  rescale  $\\mathbf{\\dot{\\rho}}=$  False ) Test without augmentation. ", "page_idx": 418, "bbox": [96, 669.0610961914062, 322.9292297363281, 694.4755859375], "page_size": [612.0, 792.0]}
{"layout": 4523, "type": "text", "text": "simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\mathbf{\\check{\\Sigma}}$  None ,  rescale  $\\mathbf{=}$  False ) Test without augmentation. ", "page_idx": 419, "bbox": [96, 71.30303192138672, 407, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4524, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 419, "bbox": [136, 102, 187, 114], "page_size": [612.0, 792.0]}
{"layout": 4525, "type": "text", "text": "•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). •  proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). •  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. ", "page_idx": 419, "bbox": [154, 119.27247619628906, 521, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 4526, "type": "text", "text": "Returns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. ", "page_idx": 419, "bbox": [137, 214.28680419921875, 521, 276.0445556640625], "page_size": [612.0, 792.0]}
{"layout": 4527, "type": "text", "text": "", "page_idx": 419, "bbox": [137, 286.25, 326, 294], "page_size": [612.0, 792.0]}
{"layout": 4528, "type": "text", "text": "class  mmdet.models.roi_heads. Trident RoI Head ( num_branch ,  test branch i dx ,  \\*\\*kwargs ) Trident roi head. ", "page_idx": 419, "bbox": [71, 298.45111083984375, 468.9063720703125, 323.8655700683594], "page_size": [612.0, 792.0]}
{"layout": 4529, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 419, "bbox": [117, 330, 169, 342], "page_size": [612.0, 792.0]}
{"layout": 4530, "type": "text", "text": "•  num_branch  ( int ) – Number of branches in TridentNet. •  test branch i dx  ( int ) – In inference, all 3 branches will be used if  test branch i dx  $==$  -1 , otherwise only branch with index  test branch i dx  will be used. ", "page_idx": 419, "bbox": [145, 346.42156982421875, 518.0826416015625, 389.61859130859375], "page_size": [612.0, 792.0]}
{"layout": 4531, "type": "text", "text": "aug test b boxes ( feats ,  img_metas ,  proposal list ,  r cnn test cf g ) Test det bboxes with test time augmentation. ", "page_idx": 419, "bbox": [96, 394.0921325683594, 368, 419.506591796875], "page_size": [612.0, 792.0]}
{"layout": 4532, "type": "text", "text": "merge trident b boxes ( trident det b boxes ,  trident det labels ) Merge bbox predictions of each branch. ", "page_idx": 419, "bbox": [96, 423.9801330566406, 368, 449.39459228515625], "page_size": [612.0, 792.0]}
{"layout": 4533, "type": "text", "text": "simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\mathbf{\\check{\\Sigma}}$  None ,  rescale=False ) Test without augmentation as follows: ", "page_idx": 419, "bbox": [96, 453.8681335449219, 407, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 4534, "type": "text", "text": "1. Compute prediction bbox and label per branch. 2. Merge predictions of each branch according to scores of bboxes, i.e., bboxes with higher score are kept to give top-k prediction. ", "page_idx": 419, "bbox": [125, 483.90557861328125, 540.0029296875, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 4535, "type": "text", "text": "39.6 losses ", "text_level": 1, "page_idx": 419, "bbox": [71, 551, 152, 566], "page_size": [612.0, 792.0]}
{"layout": 4536, "type": "text", "text": "class  mmdet.models.losses. Accuracy ( topk=(1) ,  thresh  $=$  None ) ", "page_idx": 419, "bbox": [71, 583.1491088867188, 353, 596.6085815429688], "page_size": [612.0, 792.0]}
{"layout": 4537, "type": "text", "text": "forward ( pred ,  target ) Forward function to calculate accuracy. ", "page_idx": 419, "bbox": [96, 613.0371704101562, 276, 638.4515991210938], "page_size": [612.0, 792.0]}
{"layout": 4538, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 419, "bbox": [136, 645, 188, 656], "page_size": [612.0, 792.0]}
{"layout": 4539, "type": "text", "text": "•  pred  ( torch.Tensor ) – Prediction of models. •  target  ( torch.Tensor ) – Target for each prediction. ", "page_idx": 419, "bbox": [154, 661.007568359375, 382, 692.2506103515625], "page_size": [612.0, 792.0]}
{"layout": 4540, "type": "text", "text": "Returns  The accuracies under different topk criterions. ", "page_idx": 419, "bbox": [137, 696.2449340820312, 360.4382019042969, 710.7803344726562], "page_size": [612.0, 792.0]}
{"layout": 4541, "type": "text", "text": "Return type  tuple[float] ", "page_idx": 420, "bbox": [137, 70.8248291015625, 237.5988006591797, 85.36026000976562], "page_size": [612.0, 792.0]}
{"layout": 4542, "type": "text", "text": "class  mmdet.models.losses. Associative Embedding Loss ( pull_weigh  $t{=}0.25$  ,  push weight=0.25 ) Associative Embedding Loss. ", "page_idx": 420, "bbox": [72, 89.23503875732422, 500, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4543, "type": "text", "text": "More details can be found in  Associative Embedding  and  CornerNet  . Code is modified from  kp_utils.py  # noqa: E501 ", "page_idx": 420, "bbox": [96, 119.27247619628906, 540.0035400390625, 144.5375213623047], "page_size": [612.0, 792.0]}
{"layout": 4544, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 420, "bbox": [117, 151, 169, 162], "page_size": [612.0, 792.0]}
{"layout": 4545, "type": "text", "text": "•  pull weight  ( float ) – Loss weight for corners from same object. •  push weight  ( float ) – Loss weight for corners from different object. ", "page_idx": 420, "bbox": [145, 167.0934600830078, 438.6839904785156, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 4546, "type": "text", "text": "forward ( pred ,  target ,  match ) Forward function. ", "page_idx": 420, "bbox": [96, 202.8090057373047, 219, 228.22447204589844], "page_size": [612.0, 792.0]}
{"layout": 4547, "type": "text", "text": "class  mmdet.models.losses. Balanced L 1 Loss ( alpha  $\\mathbf{\\varepsilon}_{:=0.5}$  ,  gamma  $\\mathrm{=}I.5$  ,  beta  ${=}l.0$  ,  reduction  $.=$  'mean' , loss_weigh $t{=}I.0$ )", "page_idx": 420, "bbox": [72, 232.69700622558594, 500, 258.0028991699219], "page_size": [612.0, 792.0]}
{"layout": 4548, "type": "text", "text": "Balanced L1 Loss. ", "page_idx": 420, "bbox": [96, 256.7574768066406, 171.3375701904297, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 4549, "type": "text", "text": "arXiv:  https://arxiv.org/pdf/1904.02701.pdf  (CVPR 2019) ", "page_idx": 420, "bbox": [96, 274.69049072265625, 330.02178955078125, 288.0005187988281], "page_size": [612.0, 792.0]}
{"layout": 4550, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 420, "bbox": [117, 294, 169, 305], "page_size": [612.0, 792.0]}
{"layout": 4551, "type": "text", "text": "•  alpha  ( float ) – The denominator  alpha  in the balanced L1 loss. Defaults to 0.5. •  gamma  ( float ) – The  gamma  in the balanced L1 loss. Defaults to 1.5. •  beta  ( float, optional ) – The loss is a piecewise function of prediction and target.  beta serves as a threshold for the difference between the prediction and target. Defaults to 1.0. •  reduction  ( str, optional ) – The method that reduces the loss to a scalar. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of the loss. Defaults to 1.0 ", "page_idx": 420, "bbox": [145, 310.5555114746094, 518, 419.5065612792969], "page_size": [612.0, 792.0]}
{"layout": 4552, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) Forward function of loss. ", "page_idx": 420, "bbox": [96, 423.9801025390625, 476, 449.3945617675781], "page_size": [612.0, 792.0]}
{"layout": 4553, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 420, "bbox": [136, 455, 188, 467], "page_size": [612.0, 792.0]}
{"layout": 4554, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction with shape (N, 4). •  target  ( torch.Tensor ) – The learning target of the prediction with shape (N, 4). •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight with shape (N, ). •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”.  The calculated loss ", "page_idx": 420, "bbox": [154, 471.9505615234375, 521, 598.8345947265625], "page_size": [612.0, 792.0]}
{"layout": 4555, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 420, "bbox": [137, 602.8299560546875, 242.9586944580078, 617.3653564453125], "page_size": [612.0, 792.0]}
{"layout": 4556, "type": "text", "text": "class  mmdet.models.losses. Bounded I oU Loss ( beta  $\\a=\\!0.2$  ,  eps=0.001 ,  reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0.$  ) ", "page_idx": 420, "bbox": [72, 621.2401733398438, 521, 634.6996459960938], "page_size": [612.0, 792.0]}
{"layout": 4557, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 420, "bbox": [96, 651.1281127929688, 476, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 4558, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 421, "bbox": [118, 82.77984619140625, 540, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 4559, "type": "text", "text": "class  mmdet.models.losses. CIoULoss ( eps=1e-06 ,  reduction  $\\mathbf{\\beta}=$  'mean' ,  loss_weigh  $\\scriptstyle{t=l.O.}$  ) ", "page_idx": 421, "bbox": [71, 137.05601501464844, 449, 150.51548767089844], "page_size": [612.0, 792.0]}
{"layout": 4560, "type": "text", "text": "forward ( pred ,  target ,  weigh  $\\leftleftarrows$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. ", "page_idx": 421, "bbox": [96, 166.9440155029297, 475, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 4561, "type": "text", "text": "Should be overridden by all subclasses. ", "page_idx": 421, "bbox": [118, 196.98146057128906, 275, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 4562, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 421, "bbox": [118, 226.2418212890625, 540, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 4563, "type": "text", "text": "class  mmdet.models.losses. Cross Entropy Loss ( use s igm oid  $\\leftrightharpoons$  False ,  use_mask  $\\mathbf{\\beta}=$  False ,  reduction  $.=$  'mean' , class weight  $\\leftrightharpoons$  None ,  ignore index  $=$  None ,  loss_weigh  $t{=}I.0.$  ) ", "page_idx": 421, "bbox": [71, 280.5180358886719, 536.7513427734375, 305.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 4564, "type": "text", "text": "forward ( cls_score ,  label ,  weight  $\\leftrightharpoons$  None ,  avg_factor=None ,  reduction override  $=$  None ,  ignore index  $\\leftrightharpoons$  None , \\*\\*kwargs ) Forward function. ", "page_idx": 421, "bbox": [96, 322.36102294921875, 530.5806274414062, 359.73046875], "page_size": [612.0, 792.0]}
{"layout": 4565, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 421, "bbox": [136, 366, 188, 377], "page_size": [612.0, 792.0]}
{"layout": 4566, "type": "text", "text": " cls_score torch.Tensor •  label  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. •  ignore index  ( int | None ) – The label index to be ignored. If not None, it will override the default value. Default: None. Returns  The calculated loss. Return type  torch.Tensor ", "page_idx": 421, "bbox": [137, 384.0099792480469, 521, 557.5892333984375], "page_size": [612.0, 792.0]}
{"layout": 4567, "type": "text", "text": "class  mmdet.models.losses. DIoULoss (  $.e p s{=}I e{\\mathrm{-}}$  06 ,  reduction  $=$  'mean' ,  loss weight  $\\scriptstyle{:=I.O}$  ) ", "page_idx": 421, "bbox": [71, 561.4640502929688, 449, 574.9235229492188], "page_size": [612.0, 792.0]}
{"layout": 4568, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. ", "page_idx": 421, "bbox": [96, 591.35205078125, 475, 616.7675170898438], "page_size": [612.0, 792.0]}
{"layout": 4569, "type": "text", "text": "Should be overridden by all subclasses. ", "page_idx": 421, "bbox": [118, 621.3894653320312, 275, 634.6995239257812], "page_size": [612.0, 792.0]}
{"layout": 4570, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 421, "bbox": [118, 650.6497802734375, 540, 688.4984741210938], "page_size": [612.0, 792.0]}
{"layout": 4571, "type": "text", "text": "class  mmdet.models.losses. DiceLoss ( use s igm oid  $\\leftrightharpoons$  True ,  activate  $=$  True ,  reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0$  ,  $e p s{=}0.00I)$  ) ", "page_idx": 422, "bbox": [71, 71.30303192138672, 540, 96], "page_size": [612.0, 792.0]}
{"layout": 4572, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  avg_factor=None ) Forward function. ", "page_idx": 422, "bbox": [96, 113.14604949951172, 432, 138.56053161621094], "page_size": [612.0, 792.0]}
{"layout": 4573, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 422, "bbox": [136, 144, 188, 156], "page_size": [612.0, 792.0]}
{"layout": 4574, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction, has a shape   $(\\mathbf{n},\\mathbf{\\Omega}^{*})$  . •  target  ( torch.Tensor ) – The label of the prediction, shape (n,  \\* ), same shape of pred. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction, has a shape (n,). Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. ", "page_idx": 422, "bbox": [155, 161.11549377441406, 521, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 4575, "type": "text", "text": "Returns  The calculated loss ", "page_idx": 422, "bbox": [137, 286.0178527832031, 252, 300.55328369140625], "page_size": [612.0, 792.0]}
{"layout": 4576, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 422, "bbox": [137, 303.9498596191406, 242.9585723876953, 318.48529052734375], "page_size": [612.0, 792.0]}
{"layout": 4577, "type": "text", "text": "class  mmdet.models.losses. Distribution Focal Loss ( reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ,  loss weight  $\\scriptstyle{t=I.O}$  ) Distribution Focal Loss (DFL) is a variant of  Generalized Focal Loss: Learning Qualified and Distributed Bound- ing Boxes for Dense Object Detection . ", "page_idx": 422, "bbox": [71, 322.361083984375, 540, 359.7315368652344], "page_size": [612.0, 792.0]}
{"layout": 4578, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 422, "bbox": [117, 366, 169, 377], "page_size": [612.0, 792.0]}
{"layout": 4579, "type": "text", "text": "•  reduction  ( str ) – Options are  ‘none’ ,  ‘mean’  and  ‘sum’ . •  loss weight  ( float ) – Loss weight of current loss. ", "page_idx": 422, "bbox": [145, 382.1370849609375, 388, 413.5295715332031], "page_size": [612.0, 792.0]}
{"layout": 4580, "type": "text", "text": "forward ( pred ,  target ,  weigh  $\\leftleftarrows$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. ", "page_idx": 422, "bbox": [96, 418.00311279296875, 432, 443.4175720214844], "page_size": [612.0, 792.0]}
{"layout": 4581, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 422, "bbox": [136, 450, 187, 461], "page_size": [612.0, 792.0]}
{"layout": 4582, "type": "text", "text": "•  pred  ( torch.Tensor ) – Predicted general distribution of bounding boxes (before soft- max) with shape (N,  $_{\\mathrm{n+1}}$  ), n is the max value of the integral set  $\\langle O,\\,...\\,,\\,n\\rangle$   in paper. •  target  ( torch.Tensor ) – Target distance label for bounding boxes with shape   $\\mathbf{(N,})$  . •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 422, "bbox": [155, 465.9725646972656, 521, 598.8345947265625], "page_size": [612.0, 792.0]}
{"layout": 4583, "type": "text", "text": "class  mmdet.models.losses. FocalLoss ( use s igm oid  $\\leftrightharpoons$  True ,  gamma  $\\it{=2.0}$  ,  alpha=0.25 ,  reduction  $.=$  'mean' , loss weight  $\\scriptstyle{:=}I.O$  ) ", "page_idx": 422, "bbox": [71, 603.3081665039062, 521, 628.6129760742188], "page_size": [612.0, 792.0]}
{"layout": 4584, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. ", "page_idx": 422, "bbox": [96, 645.151123046875, 432, 670.5656127929688], "page_size": [612.0, 792.0]}
{"layout": 4585, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 422, "bbox": [136, 676, 189, 688], "page_size": [612.0, 792.0]}
{"layout": 4586, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction. ", "page_idx": 422, "bbox": [155, 693.1205444335938, 328.40850830078125, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 4587, "type": "text", "text": "•  target  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. ", "page_idx": 423, "bbox": [155, 71.45246887207031, 521, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 4588, "type": "text", "text": "Returns  The calculated loss ", "page_idx": 423, "bbox": [137, 178.42083740234375, 252, 192.95626831054688], "page_size": [612.0, 792.0]}
{"layout": 4589, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 423, "bbox": [137, 196.35382080078125, 242.9586639404297, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 4590, "type": "text", "text": "class  mmdet.models.losses. GHMC (  $.b i n s{=}I O$  ,  momentum  $\\mathbf{\\chi}_{=0}$  ,  use s igm oid  $\\leftrightharpoons$  True ,  loss weight=1.0 , reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ) ", "page_idx": 423, "bbox": [71, 214.76499938964844, 480.83660888671875, 240.0699005126953], "page_size": [612.0, 792.0]}
{"layout": 4591, "type": "text", "text": "GHM Classification Loss. ", "page_idx": 423, "bbox": [96, 238.82447814941406, 200, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 4592, "type": "text", "text": "Details of the theorem can be viewed in the paper  Gradient Harmonized Single-stage Detector . ", "page_idx": 423, "bbox": [96, 256.7574768066406, 476, 270.0675048828125], "page_size": [612.0, 792.0]}
{"layout": 4593, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 423, "bbox": [117, 276, 169, 287], "page_size": [612.0, 792.0]}
{"layout": 4594, "type": "text", "text": "•  bins  ( int ) – Number of the unit regions for distribution calculation. •  momentum  ( float ) – The parameter for moving average. •  use s igm oid  ( bool ) – Can only be true for BCE based loss now. •  loss weight  ( float ) – The weight of the total GHM-C loss. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. Defaults to “mean” ", "page_idx": 423, "bbox": [145, 292.62249755859375, 476, 377.66357421875], "page_size": [612.0, 792.0]}
{"layout": 4595, "type": "text", "text": "forward ( pred ,  target ,  label weight ,  reduction override  $=$  None ,  \\*\\*kwargs ) Calculate the GHM-C loss. ", "page_idx": 423, "bbox": [96, 382.1371154785156, 398.0423278808594, 407.55157470703125], "page_size": [612.0, 792.0]}
{"layout": 4596, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 423, "bbox": [136, 413, 187, 425], "page_size": [612.0, 792.0]}
{"layout": 4597, "type": "text", "text": "•  pred  ( float tensor of size [batch_num, class_num] ) – The direct prediction of classification fc layer. •  target  ( float tensor of size [batch_num, class_num] ) – Binary class target for each sample. •  label weight  ( float tensor of size [batch_num, class_num] ) – the value is 1 if the sample is valid and 0 if ignored. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. Returns  The gradient harmonized loss. ", "page_idx": 423, "bbox": [137, 430.1075744628906, 521, 563.5662841796875], "page_size": [612.0, 792.0]}
{"layout": 4598, "type": "text", "text": "class  mmdet.models.losses. GHMR (  $\\cdot_{m u=0.02}$  ,  bins  $\\mathrm{=}I O$  ,  momentum  $\\mathbf{\\chi}_{=0}$  ,  loss weight=1.0 ,  reduction  $.=$  mean' ) GHM Regression Loss. ", "page_idx": 423, "bbox": [71, 567.442138671875, 521, 592.8566284179688], "page_size": [612.0, 792.0]}
{"layout": 4599, "type": "text", "text": "Details of the theorem can be viewed in the paper  Gradient Harmonized Single-stage Detector . ", "page_idx": 423, "bbox": [96, 597.4795532226562, 476, 610.7896118164062], "page_size": [612.0, 792.0]}
{"layout": 4600, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 423, "bbox": [117, 617, 169, 628], "page_size": [612.0, 792.0]}
{"layout": 4601, "type": "text", "text": "•  mu  ( float ) – The parameter for the Authentic Smooth L1 loss. •  bins  ( int ) – Number of the unit regions for distribution calculation. •  momentum  ( float ) – The parameter for moving average. •  loss weight  ( float ) – The weight of the total GHM-R loss. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. Defaults to “mean” ", "page_idx": 423, "bbox": [145, 633.3455200195312, 476, 718.3865966796875], "page_size": [612.0, 792.0]}
{"layout": 4602, "type": "text", "text": "forward ( pred ,  target ,  label weight ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Calculate the GHM-R loss. ", "page_idx": 424, "bbox": [96, 71.30303192138672, 432, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4603, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 424, "bbox": [136, 102, 187, 114], "page_size": [612.0, 792.0]}
{"layout": 4604, "type": "text", "text": "•  pred  ( float tensor of size [batch_num, 4 (\\* class_num)] ) – The prediction of box regression layer. Channel number can be 4 or  $^{4\\ast}$   class_num depending on whether it is class-agnostic. •  target  ( float tensor of size [batch_num, 4 (\\* class_num)] ) – The target regression values with the same size of pred. •  label weight  ( float tensor of size [batch_num, 4 (\\* class_num)] ) – The weight of each sample, 0 if ignored. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 424, "bbox": [155, 119.27247619628906, 521, 246.1565399169922], "page_size": [612.0, 792.0]}
{"layout": 4605, "type": "text", "text": "Returns  The gradient harmonized loss. ", "page_idx": 424, "bbox": [137.4549102783203, 250.15185546875, 297.3839111328125, 264.6872863769531], "page_size": [612.0, 792.0]}
{"layout": 4606, "type": "text", "text": "class  mmdet.models.losses. GIoULoss ( eps=1e-06 ,  reduction  $=$  'mean' ,  loss weight=1.0 ) ", "page_idx": 424, "bbox": [71, 268.56304931640625, 449.5882263183594, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 4607, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 424, "bbox": [96, 298.4510498046875, 475, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 4608, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 424, "bbox": [118, 357.7488098144531, 540, 395.5964660644531], "page_size": [612.0, 792.0]}
{"layout": 4609, "type": "text", "text": "class  mmdet.models.losses. Gaussian Focal Loss ( alpha  $=\\!2.0$  ,  gamma  $\\mathopen{:=}4.0$  ,  reduction  $=$  'mean' , loss_weigh $t{=}I.0$ )", "page_idx": 424, "bbox": [71, 412.0250244140625, 475, 437.32989501953125], "page_size": [612.0, 792.0]}
{"layout": 4610, "type": "text", "text": "Gaussian Focal Loss is a variant of focal loss. ", "page_idx": 424, "bbox": [96, 436.0844421386719, 275, 449.39447021484375], "page_size": [612.0, 792.0]}
{"layout": 4611, "type": "text", "text": "More details can be found in the  paper  Code is modified from  kp_utils.py  # noqa: E501 Please notice that the target in Gaussian Focal Loss is a gaussian heatmap, not 0/1 binary target. ", "page_idx": 424, "bbox": [96, 454.0174560546875, 540, 479.282470703125], "page_size": [612.0, 792.0]}
{"layout": 4612, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 424, "bbox": [117, 485, 169, 497], "page_size": [612.0, 792.0]}
{"layout": 4613, "type": "text", "text": "•  alpha  ( float ) – Power of prediction. •  gamma  ( float ) – Power of target for negative samples. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. •  loss weight  ( float ) – Loss weight of current loss. ", "page_idx": 424, "bbox": [145, 501.8384704589844, 396.3130187988281, 568.9464721679688], "page_size": [612.0, 792.0]}
{"layout": 4614, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ) Forward function. ", "page_idx": 424, "bbox": [96, 573.4200439453125, 432, 598.8345336914062], "page_size": [612.0, 792.0]}
{"layout": 4615, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 424, "bbox": [136, 605, 188, 616], "page_size": [612.0, 792.0]}
{"layout": 4616, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction in gaussian distribution. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. ", "page_idx": 424, "bbox": [155, 621.3894653320312, 521, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 4617, "type": "text", "text": "•  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 425, "bbox": [155, 71.45246887207031, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4618, "type": "text", "text": "class  mmdet.models.losses. IoULoss ( linear=False ,  eps=1e-06 ,  reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ,  loss_weigh  $t{=}I.0$  , mode  $=$  'log' ) ", "page_idx": 425, "bbox": [71, 101.19103240966797, 498.2606201171875, 126.49593353271484], "page_size": [612.0, 792.0]}
{"layout": 4619, "type": "text", "text": "Computing the IoU loss between a set of predicted bboxes and target bboxes. ", "page_idx": 425, "bbox": [96, 143.18348693847656, 402.8782653808594, 156.49351501464844], "page_size": [612.0, 792.0]}
{"layout": 4620, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 425, "bbox": [117, 162, 169, 174], "page_size": [612.0, 792.0]}
{"layout": 4621, "type": "text", "text": "•  linear  ( bool ) – If True, use linear scale of loss else determined by mode. Default: False. •  eps  ( float ) – Eps to avoid log(0). •  reduction  ( str ) – Options are “none”, “mean” and “sum”. •  loss weight  ( float ) – Weight of loss. •  mode  ( str ) – Loss scaling mode, including “linear”, “square”, and “log”. Default: ‘log’ ", "page_idx": 425, "bbox": [145, 179.04847717285156, 515, 264.0894775390625], "page_size": [612.0, 792.0]}
{"layout": 4622, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Forward function. ", "page_idx": 425, "bbox": [96, 268.56298828125, 475.6013488769531, 293.9774475097656], "page_size": [612.0, 792.0]}
{"layout": 4623, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 425, "bbox": [136, 300, 188, 311], "page_size": [612.0, 792.0]}
{"layout": 4624, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. Options are “none”, “mean” and “sum”. ", "page_idx": 425, "bbox": [155, 316.533447265625, 521, 449.39447021484375], "page_size": [612.0, 792.0]}
{"layout": 4625, "type": "text", "text": "class  mmdet.models.losses. Knowledge Distillation KL Div Loss ( reduction  $=$  mean' ,  loss_weigh  $t{=}I.0$  ,  $\\scriptstyle{T=I O}$  ) Loss function for knowledge distilling using KL divergence. ", "page_idx": 425, "bbox": [71, 453.8680114746094, 515, 491.2374572753906], "page_size": [612.0, 792.0]}
{"layout": 4626, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 425, "bbox": [117, 497, 169, 509], "page_size": [612.0, 792.0]}
{"layout": 4627, "type": "text", "text": "•  reduction  ( str ) – Options are  ‘none’ ,  ‘mean’  and  ‘sum’ . •  loss weight  ( float ) – Loss weight of current loss. •  T  ( int ) – Temperature for distillation. ", "page_idx": 425, "bbox": [145, 513.6439819335938, 388, 562.9684448242188], "page_size": [612.0, 792.0]}
{"layout": 4628, "type": "text", "text": "forward ( pred ,  soft_label ,  weigh  $\\leftleftarrows$  None ,  avg_factor=None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. ", "page_idx": 425, "bbox": [96, 567.4420166015625, 448.7673034667969, 592.8565063476562], "page_size": [612.0, 792.0]}
{"layout": 4629, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 425, "bbox": [136, 599, 188, 610], "page_size": [612.0, 792.0]}
{"layout": 4630, "type": "text", "text": "•  pred  ( Tensor ) – Predicted logits with shape   $(\\mathsf{N},\\mathsf{n}+1)$  . •  soft_label  ( Tensor ) – Target logits with shape   $(\\mathbf{N},\\mathbf{N}+1)$  . •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. ", "page_idx": 425, "bbox": [155, 615.4124145507812, 521, 706.4304809570312], "page_size": [612.0, 792.0]}
{"layout": 4631, "type": "text", "text": "•  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 426, "bbox": [154, 71.45246887207031, 521, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4632, "type": "text", "text": "class  mmdet.models.losses. L1Loss ( reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ,  loss_weigh  $t{=}l.0$  ) L1 loss. ", "page_idx": 426, "bbox": [71, 101.19103240966797, 391.4563293457031, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 4633, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 426, "bbox": [117, 133, 169, 144], "page_size": [612.0, 792.0]}
{"layout": 4634, "type": "text", "text": "•  reduction  ( str, optional ) – The method to reduce the loss. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of loss. ", "page_idx": 426, "bbox": [145, 149.1604766845703, 518, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 4635, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. ", "page_idx": 426, "bbox": [96, 196.83201599121094, 432, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 4636, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 426, "bbox": [136, 228, 188, 239], "page_size": [612.0, 792.0]}
{"layout": 4637, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 426, "bbox": [154, 244.8024444580078, 521, 365.70849609375], "page_size": [612.0, 792.0]}
{"layout": 4638, "type": "text", "text": "class  mmdet.models.losses. MSELoss ( reduction  $\\mathbf{\\beta}=$  'mean' ,  loss_weigh  $t{=}I.0$  ) MSELoss. ", "page_idx": 426, "bbox": [71, 370.1820373535156, 396.68634033203125, 395.59649658203125], "page_size": [612.0, 792.0]}
{"layout": 4639, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 426, "bbox": [117, 402, 169, 413], "page_size": [612.0, 792.0]}
{"layout": 4640, "type": "text", "text": "•  reduction  ( str, optional ) – The method that reduces the loss to a scalar. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of the loss. Defaults to 1.0 ", "page_idx": 426, "bbox": [145, 418.1524963378906, 518, 461.3495178222656], "page_size": [612.0, 792.0]}
{"layout": 4641, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function of loss. ", "page_idx": 426, "bbox": [96, 465.82305908203125, 432, 491.2375183105469], "page_size": [612.0, 792.0]}
{"layout": 4642, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 426, "bbox": [136, 498, 187, 509], "page_size": [612.0, 792.0]}
{"layout": 4643, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – Weight of the loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 426, "bbox": [154, 513.7935180664062, 521, 634.6995239257812], "page_size": [612.0, 792.0]}
{"layout": 4644, "type": "text", "text": "Returns  The calculated loss ", "page_idx": 426, "bbox": [137, 638.69482421875, 252.81130981445312, 653.230224609375], "page_size": [612.0, 792.0]}
{"layout": 4645, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 426, "bbox": [137, 656.6278686523438, 242.9586181640625, 671.1632690429688], "page_size": [612.0, 792.0]}
{"layout": 4646, "type": "text", "text": "class  mmdet.models.losses. Quality Focal Loss ( use s igm oid  $\\leftrightharpoons$  True ,  beta  $=\\!2.0$  ,  reduction  $=$  'mean' , loss_weigh $t{=}I.0$ )Quality Focal Loss (QFL) is a variant of  Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection . ", "page_idx": 427, "bbox": [71, 71.30303192138672, 540.0036010742188, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 4647, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 427, "bbox": [117, 126, 169, 138], "page_size": [612.0, 792.0]}
{"layout": 4648, "type": "text", "text": "•  use s igm oid  ( bool ) – Whether sigmoid operation is conducted in QFL. Defaults to True. •  beta  ( float ) – The beta parameter for calculating the modulating factor. Defaults to 2.0. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. •  loss weight  ( float ) – Loss weight of current loss. ", "page_idx": 427, "bbox": [145, 143.18348693847656, 518, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 4649, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\beta}=$  None ) Forward function. ", "page_idx": 427, "bbox": [96, 214.76499938964844, 432.2693176269531, 240.1794891357422], "page_size": [612.0, 792.0]}
{"layout": 4650, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 427, "bbox": [136, 246, 188, 257], "page_size": [612.0, 792.0]}
{"layout": 4651, "type": "text", "text": "•  pred  ( torch.Tensor ) – Predicted joint representation of classification and quality (IoU) estimation with shape (N, C), C is the number of classes. •  target  ( tuple([torch.Tensor]) ) – Target category label with shape (N,) and target quality label with shape (N,). •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 427, "bbox": [154, 262.7344665527344, 521, 407.5514831542969], "page_size": [612.0, 792.0]}
{"layout": 4652, "type": "text", "text": "class  mmdet.models.losses. SeesawLoss ( use s igm oid  $\\leftrightharpoons$  False ,  $p{=}0.8$  ,  $q{=}2.0$  ,  num_classe  $s{=}1203$  ,  eps  $\\it{=0.01}$  , reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0$  ,  return dic t  $\\mathbf{\\dot{\\rho}}=$  True ) Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021) arXiv:  https://arxiv.org/abs/2008.10032 ", "page_idx": 427, "bbox": [71, 412.0250244140625, 528.6375122070312, 449.39447021484375], "page_size": [612.0, 792.0]}
{"layout": 4653, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 427, "bbox": [117, 455, 169, 467], "page_size": [612.0, 792.0]}
{"layout": 4654, "type": "text", "text": "•  use s igm oid  ( bool, optional ) – Whether the prediction uses sigmoid of softmax. Only False is supported. •  p  ( float, optional ) – The  p  in the mitigation factor. Defaults to 0.8. •  q  ( float, optional ) – The  q  in the com pen station factor. Defaults to 2.0. •  num classes  ( int, optional ) – The number of classes. Default to 1203 for LVIS v1 dataset. •  eps  ( float, optional ) – The minimal value of divisor to smooth the computation of compensation factor •  reduction  ( str, optional ) – The method that reduces the loss to a scalar. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of the loss. Defaults to 1.0 •  return dic t  ( bool, optional ) – Whether return the losses as a dict. Default to True. ", "page_idx": 427, "bbox": [145, 471.9504699707031, 518, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 4655, "type": "text", "text": "forward ( cls_score ,  labels ,  label weights  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override=None ) Forward function. ", "page_idx": 427, "bbox": [96, 663.0830078125, 481.2151184082031, 688.4984741210938], "page_size": [612.0, 792.0]}
{"layout": 4656, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 427, "bbox": [136, 694, 187, 706], "page_size": [612.0, 792.0]}
{"layout": 4657, "type": "text", "text": "•  cls_score  ( torch.Tensor ) – The prediction with shape   $(\\mathbf{N},\\mathbf{C}+2)$  . •  labels  ( torch.Tensor ) – The learning label of the prediction. •  label weights  ( torch.Tensor, optional ) – Sample-wise loss weight. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. ", "page_idx": 428, "bbox": [154, 71.45246887207031, 521, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 4658, "type": "text", "text": "lated losses for objectness and classes, respectively. Return type  torch.Tensor | Dict [str, torch.Tensor] ", "page_idx": 428, "bbox": [154, 196.98146057128906, 359.0527648925781, 210.29148864746094], "page_size": [612.0, 792.0]}
{"layout": 4659, "type": "text", "text": "", "page_idx": 428, "bbox": [137, 214.28680419921875, 340.2433776855469, 228.82223510742188], "page_size": [612.0, 792.0]}
{"layout": 4660, "type": "text", "text": "get accuracy ( cls_score ,  labels ) Get custom accuracy w.r.t. cls_score and labels. ", "page_idx": 428, "bbox": [96, 232.69700622558594, 309.7572326660156, 258.11248779296875], "page_size": [612.0, 792.0]}
{"layout": 4661, "type": "text", "text": "Parameters •  cls_score  ( torch.Tensor ) – The prediction with shape   $(\\mathbf{N},\\mathbf{C}+2)$  . •  labels  ( torch.Tensor ) – The learning label of the prediction. Returns The accuracy for objectness and classes,  respectively. Return type  Dict [str, torch.Tensor] ", "page_idx": 428, "bbox": [137, 262.1068115234375, 443, 366.3063049316406], "page_size": [612.0, 792.0]}
{"layout": 4662, "type": "text", "text": "get activation ( cls_score ) Get custom activation of cls_score. Parameters  cls_score  ( torch.Tensor ) – The prediction with shape   $(\\mathbf{N},\\mathbf{C}+2)$  . Returns The custom activation of cls_score with shape    $(\\mathbf{N},\\mathbf{C}+1)$  . Return type  torch.Tensor ", "page_idx": 428, "bbox": [96, 370.1820983886719, 470, 467.92535400390625], "page_size": [612.0, 792.0]}
{"layout": 4663, "type": "text", "text": "get cls channels ( num classes ) Get custom classification channels. Parameters  num classes  ( int ) – The number of classes. Returns  The custom classification channels. Return type  int ", "page_idx": 428, "bbox": [96, 471.8011474609375, 378, 551.611328125], "page_size": [612.0, 792.0]}
{"layout": 4664, "type": "text", "text": "class  mmdet.models.losses. Smooth L 1 Loss ( beta  ${=}l.0$  ,  reduction  $\\mathbf{\\beta}=$  'mean' ,  loss_weigh  $t{=}I.0$  ) Smooth L1 loss. ", "page_idx": 428, "bbox": [71.99990844726562, 555.4871215820312, 464.0042419433594, 580.901611328125], "page_size": [612.0, 792.0]}
{"layout": 4665, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 428, "bbox": [117, 586, 170, 599], "page_size": [612.0, 792.0]}
{"layout": 4666, "type": "text", "text": "•  beta  ( float, optional ) – The threshold in the piecewise function. Defaults to 1.0. •  reduction  ( str, optional ) – The method to reduce the loss. Options are “none”, “mean” and “sum”. Defaults to “mean”. •  loss weight  ( float, optional ) – The weight of loss. ", "page_idx": 428, "bbox": [145, 603.4575805664062, 518.0802001953125, 664.5875854492188], "page_size": [612.0, 792.0]}
{"layout": 4667, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) Forward function. ", "page_idx": 428, "bbox": [96, 669.0611572265625, 475.6012268066406, 694.4756469726562], "page_size": [612.0, 792.0]}
{"layout": 4668, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 428, "bbox": [136, 700, 187, 712], "page_size": [612.0, 792.0]}
{"layout": 4669, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. ", "page_idx": 429, "bbox": [154, 71.45246887207031, 521, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 4670, "type": "text", "text": "class  mmdet.models.losses. Var i focal Loss ( use s igm oid  $\\leftrightharpoons$  True ,  alpha  $\\it{\\Lambda}_{i=0.75}$  ,  gamma  $\\it{=}2.0$  , i ou weighted  $\\leftrightharpoons$  True ,  reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0$  ) ", "page_idx": 429, "bbox": [71, 196.83201599121094, 511.0873107910156, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 4671, "type": "text", "text": "forward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ) Forward function. ", "page_idx": 429, "bbox": [96, 238.67503356933594, 432.2682800292969, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 4672, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 429, "bbox": [136, 270, 188, 281], "page_size": [612.0, 792.0]}
{"layout": 4673, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. ", "page_idx": 429, "bbox": [154, 286.6455078125, 521, 407.5515441894531], "page_size": [612.0, 792.0]}
{"layout": 4674, "type": "text", "text": "Returns  The calculated loss ", "page_idx": 429, "bbox": [137, 411.546875, 254, 426.0823059082031], "page_size": [612.0, 792.0]}
{"layout": 4675, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 429, "bbox": [137, 429.4798889160156, 242.9585723876953, 444.01531982421875], "page_size": [612.0, 792.0]}
{"layout": 4676, "type": "text", "text": "mmdet.models.losses. binary cross entropy ( pred ,  label ,  weigh  $\\leftrightharpoons$  None ,  reduction  $.=$  'mean' , avg_factor  $\\leftrightharpoons$  None ,  class weigh  $t{=}t$  None ,  ignore index  $:=:$  - 100 ) Calculate the binary Cross Entropy loss. ", "page_idx": 429, "bbox": [71, 447.8901062011719, 530.30517578125, 485.26055908203125], "page_size": [612.0, 792.0]}
{"layout": 4677, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 429, "bbox": [117, 491, 169, 503], "page_size": [612.0, 792.0]}
{"layout": 4678, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction with shape (N, 1). •  label  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  reduction  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  class weight  ( list[float], optional ) – The weight for each class. •  ignore index  ( int | None ) – The label index to be ignored. If None, it will be set to default value. Default: -100. ", "page_idx": 429, "bbox": [145, 507.8155212402344, 518, 664.5875854492188], "page_size": [612.0, 792.0]}
{"layout": 4679, "type": "text", "text": "Returns  The calculated loss. ", "text_level": 1, "page_idx": 429, "bbox": [117, 670, 237, 682], "page_size": [612.0, 792.0]}
{"layout": 4680, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 429, "bbox": [118, 686.515869140625, 224.32850646972656, 701.05126953125], "page_size": [612.0, 792.0]}
{"layout": 4681, "type": "text", "text": "mmdet.models.losses. cross entropy ( pred ,  label ,  weight  $=$  None ,  reduction  $=$  'mean' ,  avg_factor  $\\leftrightharpoons$  None , class weigh  $\\leftrightharpoons$  None ,  ignore index  $\\mathbf{=}\\mathbf{\\cdot}$  - 100 ) ", "page_idx": 430, "bbox": [71, 71.30303192138672, 504.3475341796875, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4682, "type": "text", "text": "Calculate the Cross Entropy loss. ", "page_idx": 430, "bbox": [96, 95.36250305175781, 226, 108.67253875732422], "page_size": [612.0, 792.0]}
{"layout": 4683, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 430, "bbox": [117, 115, 170, 126], "page_size": [612.0, 792.0]}
{"layout": 4684, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction with shape (N, C), C is the number of classes. •  label  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  reduction  ( str, optional ) – The method used to reduce the loss. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  class weight  ( list[float], optional ) – The weight for each class. •  ignore index  ( int | None ) – The label index to be ignored. If None, it will be set to default value. Default: -100. ", "page_idx": 430, "bbox": [145, 131.2284698486328, 518, 276.04449462890625], "page_size": [612.0, 792.0]}
{"layout": 4685, "type": "text", "text": "Returns  The calculated loss ", "page_idx": 430, "bbox": [118, 280.0398254394531, 234, 294.57525634765625], "page_size": [612.0, 792.0]}
{"layout": 4686, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 430, "bbox": [118, 297.97283935546875, 226, 312.5082702636719], "page_size": [612.0, 792.0]}
{"layout": 4687, "type": "text", "text": "mmdet.models.losses. mask cross entropy ( pred ,  target ,  label ,  reduction  $=$  'mean' ,  avg_factor=None , class weight  $=$  None ,  ignore index  $\\mathbf{\\beta}=$  None ) ", "page_idx": 430, "bbox": [71, 316.3840637207031, 500, 341.79852294921875], "page_size": [612.0, 792.0]}
{"layout": 4688, "type": "text", "text": "Calculate the Cross Entropy loss for masks. ", "page_idx": 430, "bbox": [96, 340.4434814453125, 267, 353.7535095214844], "page_size": [612.0, 792.0]}
{"layout": 4689, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 430, "bbox": [117, 360, 169, 371], "page_size": [612.0, 792.0]}
{"layout": 4690, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction with shape (N, C,  \\* ), C is the number of classes. The trailing \\* indicates arbitrary shape. •  target  ( torch.Tensor ) – The learning label of the prediction. •  label  ( torch.Tensor ) –  label  indicates the class label of the mask corresponding object. This will be used to select the mask in the of the class which the object belongs to when the mask prediction if not class-agnostic. •  reduction  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  class weight  ( list[float], optional ) – The weight for each class. •  ignore index  ( None ) – Placeholder, to be consistent with other loss. Default: None. ", "page_idx": 430, "bbox": [145, 375.68084716796875, 518, 556.9915161132812], "page_size": [612.0, 792.0]}
{"layout": 4691, "type": "text", "text": "Returns  The calculated loss ", "page_idx": 430, "bbox": [118, 560.98681640625, 234, 575.522216796875], "page_size": [612.0, 792.0]}
{"layout": 4692, "type": "text", "text": "Return type  torch.Tensor ", "page_idx": 430, "bbox": [118, 578.9188232421875, 226, 593.4542236328125], "page_size": [612.0, 792.0]}
{"layout": 4693, "type": "text", "text": "Example ", "page_idx": 431, "bbox": [95, 70.75508880615234, 137.83335876464844, 85.02153015136719], "page_size": [612.0, 792.0]}
{"layout": 4694, "type": "text", "text": " $>>\\texttt{N}$  ,   $\\textsf{C}=\\ 3$  ,  11  $>>\\texttt{H}$  ,   $\\texttt{\\^{W}}=\\texttt{\\^{2}}$  ,  2  $>>$   pred  $=$   torch . randn(N, C, H, W)  \\*  1000  $>>$   target  $=$   torch . rand(N, H, W)  $>>$   label  $=$   torch . randint( 0 , C, size  $\\scriptstyle{\\varepsilon}$  (N,))  $>>$   reduction  $=$   ' mean '  $>>$   avg_factor  $=$   None  $>>$   class weights  $=$   None  $>>$   loss  $=$   mask cross entropy(pred, target, label, reduction,  $>>$  avg_factor, class weights)  $>>$   assert  loss . shape  ==  ( 1 ,) ", "page_idx": 431, "bbox": [95, 100, 415.9590759277344, 230.79116821289062], "page_size": [612.0, 792.0]}
{"layout": 4695, "type": "text", "text": "mmdet.models.losses. mse_loss ( pred ,  target ) Warpper of mse loss. ", "page_idx": 431, "bbox": [71, 242.91600036621094, 275.3173828125, 268.33050537109375], "page_size": [612.0, 792.0]}
{"layout": 4696, "type": "text", "text": "mmdet.models.losses. reduce loss ( loss ,  reduction ) Reduce loss as specified. ", "page_idx": 431, "bbox": [71, 272.8040466308594, 302.973388671875, 298.218505859375], "page_size": [612.0, 792.0]}
{"layout": 4697, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 431, "bbox": [117, 304, 169, 315], "page_size": [612.0, 792.0]}
{"layout": 4698, "type": "text", "text": "•  loss  ( Tensor ) – Element wise loss tensor. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. ", "page_idx": 431, "bbox": [145, 320.77349853515625, 396.31304931640625, 352.01654052734375], "page_size": [612.0, 792.0]}
{"layout": 4699, "type": "text", "text": "Returns  Reduced loss tensor. ", "page_idx": 431, "bbox": [118, 356.0118713378906, 239.25233459472656, 370.54730224609375], "page_size": [612.0, 792.0]}
{"layout": 4700, "type": "text", "text": "Return type  Tensor ", "page_idx": 431, "bbox": [118, 373.94488525390625, 201.5143585205078, 388.4803161621094], "page_size": [612.0, 792.0]}
{"layout": 4701, "type": "text", "text": "mmdet.models.losses. s igm oid focal loss ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  gamma=2.0 ,  alpha=0.25 , reduction  $.=$  'mean' ,  avg_factor  $\\leftrightharpoons$  None ) ", "page_idx": 431, "bbox": [71, 392.3551025390625, 488.1086730957031, 417.77056884765625], "page_size": [612.0, 792.0]}
{"layout": 4702, "type": "text", "text": "A warpper of cuda version  Focal Loss . ", "page_idx": 431, "bbox": [95, 416.41552734375, 251, 429.7255554199219], "page_size": [612.0, 792.0]}
{"layout": 4703, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 431, "bbox": [117, 436, 169, 447], "page_size": [612.0, 792.0]}
{"layout": 4704, "type": "text", "text": "•  pred  ( torch.Tensor ) – The prediction with shape (N, C), C is the number of classes. •  target  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  gamma  ( float, optional ) – The gamma for calculating the modulating factor. Defaults to 2.0. •  alpha  ( float, optional ) – A balanced form for Focal Loss. Defaults to 0.25. •  reduction  ( str, optional ) – The method used to reduce the loss into a scalar. Defaults to ‘mean’. Options are “none”, “mean” and “sum”. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. ", "page_idx": 431, "bbox": [145, 452.2805480957031, 518, 609.0526123046875], "page_size": [612.0, 792.0]}
{"layout": 4705, "type": "text", "text": "mmdet.models.losses. weighted loss ( loss_func ) Create a weighted version of a given loss function. ", "page_idx": 431, "bbox": [71, 613.526123046875, 298.1713562011719, 638.9406127929688], "page_size": [612.0, 792.0]}
{"layout": 4706, "type": "text", "text": "To use this decorator, the loss function must have the signature like  loss_func(pred, target, \\*\\*kwargs) . The function only needs to compute element-wise loss without any reduction. This decorator will add weight and reduction arguments to the function. The decorated function will have the signature like  loss_func(pred, target, weight  $\\leftrightharpoons$  None, reduction  $\\scriptstyle{\\mathcal{S}}$  ’mean’, avg_factor  $\\leftrightharpoons$  None, \\*\\*kwargs) . ", "page_idx": 431, "bbox": [95, 643.4141235351562, 540, 692.7396240234375], "page_size": [612.0, 792.0]}
{"layout": 4707, "type": "text", "text": "Example ", "text_level": 1, "page_idx": 431, "bbox": [118, 699, 156, 710], "page_size": [612.0, 792.0]}
{"layout": 4708, "type": "table", "page_idx": 432, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_161.jpg", "bbox": [91, 74, 545, 294], "page_size": [612.0, 792.0], "ocr_text": ">>> import torch\n\n>>> @weighted_loss\n\n>>> def 11_loss(pred, target):\n\n>>> return (pred - target) .abs()\n>>> pred = torch.Tensor([0, 2, 3])\n>>> target = torch.Tensor([1, 1, 1])\n>>> weight = torch.Tensor([1, 9, 1])\n\n>>> 11_loss(pred,\ntensor (1.3333)\n\ntarget)\n\n>>> 11_loss(pred,\ntensor(1.)\n\n>>> 11_loss(pred,\ntensor([1., 1., 2\n>>> 11_loss(pred,\ntensor (1.5000)\n\ntarget, weight)\n\ntarget, reduction='none')\n\n-D\n\ntarget, weight, avg_factor=2)\n\n", "vlm_text": "The image shows Python code using PyTorch to demonstrate the application of a custom L1 loss function with weights and different computation modes. Here’s a breakdown of the code:\n\n1. **Importing and Defining L1 Loss:**\n   - The `torch` library is imported.\n   - A custom decorator `@weighted_loss` (assumed to be defined elsewhere) is applied to the function `l1_loss`, indicating that it may modify or enhance the behavior of the `l1_loss` function.\n   - The `l1_loss` function is defined, which calculates the absolute difference between `pred` (predictions) and `target`.\n\n2. **Tensor Definitions:**\n   - A prediction tensor `pred` is created with values `[0, 2, 3]`.\n   - A target tensor `target` is created with values `[1, 1, 1]`.\n   - A weight tensor `weight` is created with values `[1, 0, 1]`, indicating the importance of different elements in the loss calculation.\n\n3. **Examples of L1 Loss Calculation:**\n   - `l1_loss(pred, target)` calculates the average L1 loss between `pred` and `target`. The result is `tensor(1.3333)`.\n   - `l1_loss(pred, target, weight)` calculates the weighted L1 loss, considering the `weight`. The result is `tensor(1.)`.\n   - `l1_loss(pred, target, reduction='none')` computes the element-wise L1 loss without reduction, resulting in `tensor([1., 1., 2.])`.\n   - `l1_loss(pred, target, weight, avg_factor=2)` computes the weighted L1 loss with a specified averaging factor (`avg_factor`), resulting in `tensor(1.5000)`.\n\nThe table effectively illustrates how L1 loss can be customized with weights and different reduction modes using PyTorch, allowing users to apply the specifics of their problem contexts in loss calculations."}
{"layout": 4709, "type": "text", "text": "39.7 utils ", "text_level": 1, "page_idx": 432, "bbox": [71, 319, 138, 335], "page_size": [612.0, 792.0]}
{"layout": 4710, "type": "text", "text": "class  mmdet.models.utils. Adaptive Avg Pool 2 d ( output size: Union[int, None, Tuple[Optional[int], ...]] ) Handle empty batch dimension to Adaptive Avg Pool 2 d. ", "page_idx": 432, "bbox": [72, 352.4150390625, 523.6103515625, 377.8294982910156], "page_size": [612.0, 792.0]}
{"layout": 4711, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 432, "bbox": [96, 384, 316, 425.6495056152344], "page_size": [612.0, 792.0]}
{"layout": 4712, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 432, "bbox": [118, 441.5998229980469, 540, 479.448486328125], "page_size": [612.0, 792.0]}
{"layout": 4713, "type": "text", "text": "class  mmdet.models.utils. CSPLayer ( in channels ,  out channels ,  expand ratio  $\\backsimeq\\!O.5$  ,  num_blocks  $\\scriptstyle{\\mathfrak{s}}=I$  , add identity  $\\scriptstyle\\gamma=$  True ,  use depth wise  $\\mathbf{=}$  False ,  conv_cfg  $\\leftrightharpoons$  None , norm_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} , act_cfg  $=$  {'type': 'Swish'} ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) ", "page_idx": 432, "bbox": [72, 495.87701416015625, 499.00762939453125, 545.2014770507812], "page_size": [612.0, 792.0]}
{"layout": 4714, "type": "text", "text": "Cross Stage Partial Layer. ", "page_idx": 432, "bbox": [96, 543.8463745117188, 199.40200805664062, 557.1564331054688], "page_size": [612.0, 792.0]}
{"layout": 4715, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 432, "bbox": [117, 563, 169, 575], "page_size": [612.0, 792.0]}
{"layout": 4716, "type": "text", "text": "•  in channels  ( int ) – The input channels of the CSP layer. •  out channels  ( int ) – The output channels of the CSP layer. •  expand ratio  ( float ) – Ratio to adjust the number of channels of the hidden layer. De- fault: 0.5 •  num_blocks  ( int ) – Number of blocks. Default: 1 •  add identity  ( bool ) – Whether to add identity in blocks. Default: True •  use depth wise  ( bool ) – Whether to depthwise separable convolution in blocks. Default: False ", "page_idx": 432, "bbox": [145, 579.71240234375, 518, 706.596435546875], "page_size": [612.0, 792.0]}
{"layout": 4717, "type": "text", "text": "•  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None, which means using conv2d. ", "page_idx": 433, "bbox": [145, 71.45246887207031, 518, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4718, "type": "text", "text": "•  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict  $\\scriptstyle(\\mathrm{type}={\\mathrm{BN}}^{\\prime})$  ) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $=^{,}$  ’Swish’) ", "page_idx": 433, "bbox": [145, 101.34046936035156, 480, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4719, "type": "text", "text": "", "page_idx": 433, "bbox": [145, 119.27247619628906, 471, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 4720, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 433, "bbox": [95, 139, 149, 149.75], "page_size": [612.0, 792.0]}
{"layout": 4721, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 433, "bbox": [118, 149.1604766845703, 313, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 4722, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 433, "bbox": [118, 196.35382080078125, 540, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 4723, "type": "text", "text": "class  mmdet.models.utils. Con v Up sample ( in channels ,  inner channels ,  num_layers  $\\mathrm{\\Sigma}_{:=I}$  , num up sample  $\\mathbf{\\dot{\\rho}}$  None ,  conv_cfg  $=$  None ,  norm_cfg=None , init_cfg  $=$  None ,  \\*\\*kwargs ) ", "page_idx": 433, "bbox": [72, 250.62998962402344, 500.35260009765625, 287.9994812011719], "page_size": [612.0, 792.0]}
{"layout": 4724, "type": "text", "text": "Con v Up sample performs   $2\\mathrm{x}$   upsampling after Conv. ", "page_idx": 433, "bbox": [96, 286.64544677734375, 304.457763671875, 299.9554748535156], "page_size": [612.0, 792.0]}
{"layout": 4725, "type": "text", "text": "There are several  ConvModule  layers. In the first few layers, upsampling will be applied after each layer of convolution. The number of upsampling must be no more than the number of ConvModule layers. ", "page_idx": 433, "bbox": [96, 304.4280090332031, 540, 329.8434753417969], "page_size": [612.0, 792.0]}
{"layout": 4726, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 433, "bbox": [117, 336, 169, 347], "page_size": [612.0, 792.0]}
{"layout": 4727, "type": "text", "text": "•  in channels  ( int ) – Number of channels in the input feature map. •  inner channels  ( int ) – Number of channels produced by the convolution. •  num_layers  ( int ) – Number of convolution layers. ", "page_idx": 433, "bbox": [145, 352.3984680175781, 427.10711669921875, 365.70849609375], "page_size": [612.0, 792.0]}
{"layout": 4728, "type": "text", "text": "", "page_idx": 433, "bbox": [145, 370.33148193359375, 461.44805908203125, 383.6415100097656], "page_size": [612.0, 792.0]}
{"layout": 4729, "type": "text", "text": "", "page_idx": 433, "bbox": [145, 388.2644958496094, 362.5796813964844, 401.57452392578125], "page_size": [612.0, 792.0]}
{"layout": 4730, "type": "text", "text": "•  num up sample  ( int | optional ) – Number of upsampling layer. Must be no more than num_layers. Upsampling will be applied after the first  num up sample  layers of convolution. Default:  num_layers . ", "page_idx": 433, "bbox": [145, 406.1965026855469, 518, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 4731, "type": "text", "text": "•  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None, which means using conv2d. ", "page_idx": 433, "bbox": [145, 448.03948974609375, 518, 473.3055114746094], "page_size": [612.0, 792.0]}
{"layout": 4732, "type": "text", "text": "•  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  init_cfg  ( dict ) – Config dict for initialization. Default: None. •  kwargs  ( key word augments ) – Other augments used in ConvModule. ", "page_idx": 433, "bbox": [145, 477.927490234375, 439.5500793457031, 491.2375183105469], "page_size": [612.0, 792.0]}
{"layout": 4733, "type": "text", "text": "", "page_idx": 433, "bbox": [145, 495.8605041503906, 413.1192932128906, 509.1705322265625], "page_size": [612.0, 792.0]}
{"layout": 4734, "type": "text", "text": "", "page_idx": 433, "bbox": [145, 513.7935180664062, 445.6429138183594, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 4735, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 433, "bbox": [96, 534, 149, 544], "page_size": [612.0, 792.0]}
{"layout": 4736, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 433, "bbox": [118, 543.6814575195312, 313, 574.924560546875], "page_size": [612.0, 792.0]}
{"layout": 4737, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 433, "bbox": [118, 590.873779296875, 540, 628.7224731445312], "page_size": [612.0, 792.0]}
{"layout": 4738, "type": "text", "text": "class  mmdet.models.utils. De tr Transformer Decoder ( \\*args ,  post norm cf g  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'LN'} , return intermediate  $=$  False ,  \\*\\*kwargs ) ", "page_idx": 433, "bbox": [72, 645.1510620117188, 480, 670.5654907226562], "page_size": [612.0, 792.0]}
{"layout": 4739, "type": "text", "text": "Implements the decoder in DETR transformer. ", "page_idx": 433, "bbox": [96, 669.21044921875, 282.1415100097656, 682.5205078125], "page_size": [612.0, 792.0]}
{"layout": 4740, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 433, "bbox": [117, 688, 169, 700], "page_size": [612.0, 792.0]}
{"layout": 4741, "type": "text", "text": "•  return intermediate  ( bool ) – Whether to return intermediate outputs. ", "page_idx": 433, "bbox": [145, 705.0764770507812, 451, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 4742, "type": "text", "text": "•  post norm cf g  ( dict ) – Config of last normalization layer. Default  LN . ", "page_idx": 434, "bbox": [145, 71.30303192138672, 448.7366638183594, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 4743, "type": "text", "text": "forward ( query ,  \\*args ,  \\*\\*kwargs ) Forward function for  Transformer Decoder . ", "page_idx": 434, "bbox": [96, 89.23503875732422, 290, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4744, "type": "text", "text": " query Tensor ", "page_idx": 434, "bbox": [186.39134216308594, 120.99600982666016, 254.71421813964844, 131.90505981445312], "page_size": [612.0, 792.0]}
{"layout": 4745, "type": "text", "text": "Returns ", "page_idx": 434, "bbox": [137, 136.57781982421875, 171.46737670898438, 151.11325073242188], "page_size": [612.0, 792.0]}
{"layout": 4746, "type": "text", "text": "Results with shape [1, num_query, bs, embed_dims] when  return intermediate is  False , otherwise it has shape [num_layers, num_query, bs, embed_dims]. ", "page_idx": 434, "bbox": [154, 154.51080322265625, 521, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 4747, "type": "text", "text": "Return type  Tensor ", "page_idx": 434, "bbox": [137, 184.3988037109375, 220.1444091796875, 198.93423461914062], "page_size": [612.0, 792.0]}
{"layout": 4748, "type": "text", "text": "class  mmdet.models.utils. De tr Transformer Decoder Layer ( attn_cfgs ,  feed forward channels , ffn_dropou  $\\because0.0$  ,  operation order=None , act_cfg  $\\mathbf{\\hat{\\rho}}$  {'inplace': True, 'type': 'ReLU'} , norm_cfg  $=$  {'type': 'LN'} ,  ffn_num_fc  $\\cdot_{s=2}$  , \\*\\*kwargs ) ", "page_idx": 434, "bbox": [71, 202.8099822998047, 521, 263.9799499511719], "page_size": [612.0, 792.0]}
{"layout": 4749, "type": "text", "text": "Implements decoder layer in DETR transformer. ", "page_idx": 434, "bbox": [96, 262.7345275878906, 290, 276.0445556640625], "page_size": [612.0, 792.0]}
{"layout": 4750, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 434, "bbox": [117, 282, 169, 294], "page_size": [612.0, 792.0]}
{"layout": 4751, "type": "text", "text": "•  attn_cfgs  (list[ mmcv.ConfigDict ] | list[dict] | dict )) – Configs for self attention or cross attention, the order should be consistent with it in  operation order . If it is a dict, it would be expand to the number of attention in  operation order . •  feed forward channels  ( int ) – The hidden dimension for FFNs. •  ff n dropout  ( float ) – Probability of an element to be zeroed in ffn. Default 0.0. •  operation order  ( tuple[str] ) – The execution order of operation in transformer. Such as (‘self_attn’, ‘norm’, ‘ffn’, ‘norm’). Default None •  act_cfg  ( dict ) – The activation config for FFNs. Default:  LN •  norm_cfg  ( dict ) – Config dict for normalization layer. Default:  LN . •  ff n num fcs  ( int ) – The number of fully-connected layers in FFNs. Default2. ", "page_idx": 434, "bbox": [145, 298.45111083984375, 521, 455.37261962890625], "page_size": [612.0, 792.0]}
{"layout": 4752, "type": "text", "text": "class  mmdet.models.utils. Dynamic Con v ( in_channel  $\\imath{=}256$  ,  feat channel  $\\scriptstyle s=64$  ,  out channel  $\\scriptstyle\\sum=$  None , input feat shape  $\\mathrm{\\Sigma=}7$  ,  with_proj  $\\leftleftarrows$  True ,  act_cfg  $=$  {'inplace': True, 'type': 'ReLU'} ,  norm_cfg  $=$  {'type': 'LN'} ,  init_cfg  $=$  None ) ", "page_idx": 434, "bbox": [71, 459.8461608886719, 521, 497.2156066894531], "page_size": [612.0, 792.0]}
{"layout": 4753, "type": "text", "text": "Implements Dynamic Convolution. ", "page_idx": 434, "bbox": [96, 495.8605651855469, 237, 509.17059326171875], "page_size": [612.0, 792.0]}
{"layout": 4754, "type": "text", "text": "This module generate parameters for each sample and use bmm to implement   $1^{*}1$   convolution. Code is modified from the  official github repo  . ", "page_idx": 434, "bbox": [96, 513.7935180664062, 540.0032348632812, 539.05859375], "page_size": [612.0, 792.0]}
{"layout": 4755, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 434, "bbox": [117, 545, 168, 556], "page_size": [612.0, 792.0]}
{"layout": 4756, "type": "text", "text": "•  in channels  ( int ) – The input feature channel. Defaults to 256. •  feat channels  ( int ) – The inner feature channel. Defaults to 64. •  out channels  ( int, optional ) – The output feature channel. When not specified, it will be set to  in channels  by default •  input feat shape  ( int ) – The shape of input feature. Defaults to 7. •  with_proj  ( bool ) – Project two-di mention al feature to one-di mention al feature. Default to True. •  act_cfg  ( dict ) – The activation config for Dynamic Con v. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default layer normalization. ", "page_idx": 434, "bbox": [145, 561.6145629882812, 521, 706.4306030273438], "page_size": [612.0, 792.0]}
{"layout": 4757, "type": "text", "text": "•  (obj  ( init_cfg ) –  mmcv.ConfigDict ): The Config for initialization. Default: None. ", "page_idx": 435, "bbox": [145, 71.30303192138672, 492, 84.76250457763672], "page_size": [612.0, 792.0]}
{"layout": 4758, "type": "text", "text": "forward ( param feature ,  input feature ) Forward function for  Dynamic Con v . ", "page_idx": 435, "bbox": [96, 89.23503875732422, 263.710693359375, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4759, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 435, "bbox": [136, 120, 188, 132], "page_size": [612.0, 792.0]}
{"layout": 4760, "type": "text", "text": "•  param feature  ( Tensor ) – The feature can be used to generate the parameter, has shape (num all proposals, in channels). •  input feature  ( Tensor ) – Feature that interact with parameters, has shape (num all proposals, in channels, H, W). ", "page_idx": 435, "bbox": [154, 137.20545959472656, 521, 192.35850524902344], "page_size": [612.0, 792.0]}
{"layout": 4761, "type": "text", "text": "Returns  The output feature has shape (num all proposals, out channels). ", "page_idx": 435, "bbox": [137, 196.35382080078125, 435.4855041503906, 210.88925170898438], "page_size": [612.0, 792.0]}
{"layout": 4762, "type": "text", "text": "Return type  Tensor ", "page_idx": 435, "bbox": [137, 214.28680419921875, 220.14431762695312, 228.82223510742188], "page_size": [612.0, 792.0]}
{"layout": 4763, "type": "text", "text": "class  mmdet.models.utils. Inverted Residual ( in channels ,  out channels ,  mid channels ,  kernel_siz  $\\scriptstyle{\\stackrel{\\prime}{=}}3$  , stride  $\\scriptstyle{:=I}$  ,  se_cfg  $\\mathbf{\\beta}=$  None ,  with expand con v  $\\mathbf{\\tilde{\\Sigma}}$  True , conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $=$  {'type': 'BN'} ,  act_cfg  $\\mathbf{\\beta}=$  {'type': 'ReLU'} ,  with_cp=False ,  init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ) ", "page_idx": 435, "bbox": [72.0, 232.69700622558594, 526.8533935546875, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 4764, "type": "text", "text": "Inverted Residual Block. ", "page_idx": 435, "bbox": [96, 280.66748046875, 195.39703369140625, 293.9775085449219], "page_size": [612.0, 792.0]}
{"layout": 4765, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 435, "bbox": [117, 300, 169, 311], "page_size": [612.0, 792.0]}
{"layout": 4766, "type": "text", "text": "•  in channels  ( int ) – The input channels of this Module. •  out channels  ( int ) – The output channels of this Module. •  mid channels  ( int ) – The input channels of the depthwise convolution. •  kernel size  ( int ) – The kernel size of the depthwise convolution. Default: 3. •  stride  ( int ) – The stride of the depthwise convolution. Default: 1. •  se_cfg  ( dict ) – Config dict for se layer. Default: None, which means no se layer. •  with expand con v  ( bool ) – Use expand conv or not. If set False, mid channels must be the same with in channels. Default: True. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None, which means using conv2d. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict  $\\scriptstyle(\\mathrm{type}={\\mathrm{BN}}^{\\prime})$  ). •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\fallingdotseq$  ReLU’). •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 435, "bbox": [145, 316.53350830078125, 518, 562.9686279296875], "page_size": [612.0, 792.0]}
{"layout": 4767, "type": "text", "text": "Returns  The output tensor. ", "page_idx": 435, "bbox": [118, 566.9639282226562, 230.09645080566406, 581.4993286132812], "page_size": [612.0, 792.0]}
{"layout": 4768, "type": "text", "text": "Return type  Tensor ", "text_level": 1, "page_idx": 435, "bbox": [117, 587, 203, 599], "page_size": [612.0, 792.0]}
{"layout": 4769, "type": "text", "text": "", "text_level": 1, "page_idx": 435, "bbox": [96, 605, 149, 610.75], "page_size": [612.0, 792.0]}
{"layout": 4770, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 435, "bbox": [118, 615.4125366210938, 313.48406982421875, 646.6556396484375], "page_size": [612.0, 792.0]}
{"layout": 4771, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 435, "bbox": [118, 662.6047973632812, 540, 700.4534912109375], "page_size": [612.0, 792.0]}
{"layout": 4772, "type": "text", "text": "class  mmdet.models.utils. Learned Positional Encoding ( num_feats ,  row num embed  $\\iota{=}50$  , col num embed  $l{=}50$  ,  init_cfg  $=_{i}$  {'layer': 'Embedding', 'type': 'Uniform'} ) ", "page_idx": 436, "bbox": [71, 71.30303192138672, 494.7235107421875, 108.56295013427734], "page_size": [612.0, 792.0]}
{"layout": 4773, "type": "text", "text": "Position embedding with learnable embedding weights. ", "page_idx": 436, "bbox": [96, 107.31752014160156, 317, 120.62755584716797], "page_size": [612.0, 792.0]}
{"layout": 4774, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 436, "bbox": [117, 126, 170, 139], "page_size": [612.0, 792.0]}
{"layout": 4775, "type": "text", "text": "•  num_feats  ( int ) – The feature dimension for each position along x-axis or y-axis. The final returned dimension for each position is 2 times of this value. •  row num embed  ( int, optional ) – The dictionary size of row embeddings. Default 50. •  col num embed  ( int, optional ) – The dictionary size of col embeddings. Default 50. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. ", "page_idx": 436, "bbox": [145, 143.18348693847656, 518, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 4776, "type": "text", "text": "forward ( mask ) Forward function for  Learned Positional Encoding ", "page_idx": 436, "bbox": [96, 226.7200164794922, 317, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 4777, "type": "text", "text": "Parameters  mask  ( Tensor ) – ByteTensor mask. Non-zero values representing ignored posi- tions, while zero values means valid positions for this image. Shape [bs, h, w]. ", "page_idx": 436, "bbox": [137, 256.12982177734375, 521.3714599609375, 282.02252197265625], "page_size": [612.0, 792.0]}
{"layout": 4778, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 436, "bbox": [136, 288, 173, 299], "page_size": [612.0, 792.0]}
{"layout": 4779, "type": "text", "text": "Returned position embedding with shape  [bs, num_feats\\*2, h, w]. Return type  pos (Tensor) ", "page_idx": 436, "bbox": [137, 303.9498596191406, 429.6587219238281, 336.4183044433594], "page_size": [612.0, 792.0]}
{"layout": 4780, "type": "text", "text": "class  mmdet.models.utils. Norm edC on v 2 d ( \\*args ,  tempe artur e=20 ,  power  $=$  1.0 ,  eps=1e-06 , norm over kernel  $\\leftrightharpoons$  False ,  \\*\\*kwargs ) ", "page_idx": 436, "bbox": [71, 340.2940979003906, 462, 365.70855712890625], "page_size": [612.0, 792.0]}
{"layout": 4781, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 436, "bbox": [117, 383, 170, 395], "page_size": [612.0, 792.0]}
{"layout": 4782, "type": "text", "text": "•  tempeature  ( float, optional ) – Tempeature term. Default to 20. •  power  ( int, optional ) – Power term. Default to 1.0. •  eps  ( float, optional ) – The minimal value of divisor to keep numerical stability. Default to 1e-6. •  norm over kernel  ( bool, optional ) – Normalize over kernel. Default to False. ", "page_idx": 436, "bbox": [145, 400.21954345703125, 518, 479.2825927734375], "page_size": [612.0, 792.0]}
{"layout": 4783, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 436, "bbox": [96, 485, 149, 496], "page_size": [612.0, 792.0]}
{"layout": 4784, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 436, "bbox": [118, 495.8605651855469, 317, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 4785, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 436, "bbox": [118, 543.0538330078125, 540, 580.9014892578125], "page_size": [612.0, 792.0]}
{"layout": 4786, "type": "text", "text": "class  mmdet.models.utils. Norm ed Linear ( \\*args ,  tempe artur e  $\\mathrm{=}20$  ,  power=1.0 ,  eps=1e-06 ,  \\*\\*kwargs ) Normalized Linear Layer. ", "page_idx": 436, "bbox": [71, 597.3300170898438, 508.11932373046875, 622.7445068359375], "page_size": [612.0, 792.0]}
{"layout": 4787, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 436, "bbox": [117, 629, 170, 640], "page_size": [612.0, 792.0]}
{"layout": 4788, "type": "text", "text": "•  tempeature  ( float, optional ) – Tempeature term. Default to 20. •  power  ( int, optional ) – Power term. Default to 1.0. •  eps  ( float, optional ) – The minimal value of divisor to keep numerical stability. Default to 1e-6. ", "page_idx": 436, "bbox": [145, 645.3004760742188, 518, 706.4304809570312], "page_size": [612.0, 792.0]}
{"layout": 4789, "type": "text", "text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 437, "bbox": [96, 73, 313.4832763671875, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4790, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 437, "bbox": [118, 130.59979248046875, 540, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 4791, "type": "text", "text": "class  mmdet.models.utils. PatchEmbed ( in_channel  $\\mathfrak{s}{=}3$  ,  embed_dim  $\\imath{=}768$  ,  conv_t  $\\scriptstyle{y e=C o n\\nu2d^{\\prime}}$  , kernel_siz  $\\scriptstyle{\\mathit{\\Sigma}}=I6$  ,  stride  $\\mathrm{\\Sigma=}l6$  ,  padding  $\\mathbf{\\dot{\\Sigma}}$  'corner' ,  dilation  $\\mathrm{\\Sigma}_{:=I}$  ,  bias  $\\mathbf{=}$  True , norm_cfg  $\\leftrightharpoons$  None ,  input_size  $=$  None ,  init_cfg  $\\leftrightharpoons$  None ) ", "page_idx": 437, "bbox": [71, 184.8769989013672, 540, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 4792, "type": "text", "text": "Image to Patch Embedding. ", "page_idx": 437, "bbox": [96, 220.89149475097656, 206.9935302734375, 234.20152282714844], "page_size": [612.0, 792.0]}
{"layout": 4793, "type": "text", "text": "We use a conv layer to implement PatchEmbed. ", "page_idx": 437, "bbox": [96, 238.82447814941406, 286.38555908203125, 252.13450622558594], "page_size": [612.0, 792.0]}
{"layout": 4794, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 437, "bbox": [117, 258, 169, 269], "page_size": [612.0, 792.0]}
{"layout": 4795, "type": "text", "text": "•  in channels  ( int ) – The num of input channels. Default: 3 •  embed_dims  ( int ) – The dimensions of embedding. Default: 768 •  conv_type  ( str ) – The config dict for embedding conv layer type selection. Default: “Conv2d. •  kernel size  ( int ) – The kernel size of embedding conv. Default: 16. •  stride  ( int ) – The slide stride of embedding conv. Default: None (Would be set as  ker- nel_size ). •  padding  ( int | tuple | string ) – The padding length of embedding conv. When it is a string, it means the mode of adaptive padding, support “same” and “corner” now. Default: “corner”. •  dilation  ( int ) – The dilation rate of embedding conv. Default: 1. •  bias  ( bool ) – Bias of embed conv. Default: True. •  norm_cfg  ( dict, optional ) – Config dict for normalization layer. Default: None. •  input_size  ( int | tuple | None ) – The size of input, which will be used to calculate the out size. Only work when  dynamic size  is False. Default: None. •  init_cfg  ( mmcv.ConfigDict , optional) – The Config for initialization. Default: None. ", "page_idx": 437, "bbox": [145, 274.69049072265625, 518, 527.1035766601562], "page_size": [612.0, 792.0]}
{"layout": 4796, "type": "text", "text": "forward ( x ) ", "text_level": 1, "page_idx": 437, "bbox": [96, 533, 149, 545], "page_size": [612.0, 792.0]}
{"layout": 4797, "type": "text", "text": "Parameters  x  ( Tensor ) – Has shape (B, C, H, W). In most case, C is 3. ", "page_idx": 437, "bbox": [137.4547882080078, 560.9868774414062, 427, 575.5222778320312], "page_size": [612.0, 792.0]}
{"layout": 4798, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 437, "bbox": [136, 581, 173, 592], "page_size": [612.0, 792.0]}
{"layout": 4799, "type": "text", "text": "Contains merged results and its spatial shape. • x (Tensor): Has shape (B, out_h \\* out_w, embed_dims) •  out_size (tuple[int]): Spatial shape of x, arrange as  (out_h, out_w). ", "page_idx": 437, "bbox": [154, 597.4794921875, 445.8580627441406, 647.2532958984375], "page_size": [612.0, 792.0]}
{"layout": 4800, "type": "text", "text": "class  mmdet.models.utils. ResLayer ( block ,  inplanes ,  planes ,  num_blocks ,  stride  $\\scriptstyle{\\varepsilon=I}$  ,  avg_down  $=$  False , conv_cfg  $=$  None ,  norm_cfg  $=$  {'type': 'BN'} ,  down sample first=True , \\*\\*kwargs ) ", "page_idx": 437, "bbox": [71, 669.0610961914062, 518, 706.3209228515625], "page_size": [612.0, 792.0]}
{"layout": 4801, "type": "text", "text": "ResLayer to build ResNet style backbone. ", "page_idx": 437, "bbox": [96, 705.0764770507812, 262.6444091796875, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 4802, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 438, "bbox": [117, 73, 169, 84], "page_size": [612.0, 792.0]}
{"layout": 4803, "type": "text", "text": "•  block  ( nn.Module ) – block used to build ResLayer. •  inplanes  ( int ) – inplanes of block. •  planes  ( int ) – planes of block. •  num_blocks  ( int ) – number of blocks. •  stride  ( int ) – stride of the first block. Default: 1 •  avg_down  ( bool ) – Use AvgPool instead of stride conv when down sampling in the bottle- neck. Default: False •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. Default: None •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: dict(type  $=^{!}$  ’BN’) •  down sample first  ( bool ) – Downsample at the first block or last block. False for Hour- glass, True for ResNet. Default: True ", "page_idx": 438, "bbox": [145, 89.38447570800781, 520, 270.06744384765625], "page_size": [612.0, 792.0]}
{"layout": 4804, "type": "text", "text": "class  mmdet.models.utils. SELayer ( channels ,  ratio  $\\scriptstyle\\prime=$  16 ,  conv_cfg  $=$  None ,  act_cfg  $=$  ({'type': 'ReLU'}, {'type': 'Sigmoid'}) ,  init_cfg  $=$  None ) ", "page_idx": 438, "bbox": [71, 274.53997802734375, 529, 299.9554443359375], "page_size": [612.0, 792.0]}
{"layout": 4805, "type": "text", "text": "Squeeze-and-Excitation Module. ", "page_idx": 438, "bbox": [96, 298.60040283203125, 227, 311.9104309082031], "page_size": [612.0, 792.0]}
{"layout": 4806, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 438, "bbox": [117, 318, 169, 329], "page_size": [612.0, 792.0]}
{"layout": 4807, "type": "text", "text": "•  channels  ( int ) – The input (and output) channels of the SE layer. •  ratio  ( int ) – Squeeze ratio in SELayer, the intermediate channel will be  int(channels/ ratio) . Default: 16. •  conv_cfg  ( None or dict ) – Config dict for convolution layer. Default: None, which means using conv2d. •  act_cfg  ( dict or Sequence[dict] ) – Config dict for activation layer. If act_cfg is a dict, two activation layers will be config u rated by this dict. If act_cfg is a sequence of dicts, the first activation layer will be config u rated by the first dict and the second activation layer will be config u rated by the second dict. Default: (dict(type  $='$  ’ReLU’), dict(type  $\\fallingdotseq$  Sigmoid’)) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 438, "bbox": [145, 334.4654235839844, 520, 479.2824401855469], "page_size": [612.0, 792.0]}
{"layout": 4808, "type": "text", "text": "forward  $(x)$  ", "text_level": 1, "page_idx": 438, "bbox": [95, 485, 149, 496], "page_size": [612.0, 792.0]}
{"layout": 4809, "type": "text", "text": "Defines the computation performed at every call. Should be overridden by all subclasses. ", "page_idx": 438, "bbox": [118, 495.86041259765625, 313, 527.1034545898438], "page_size": [612.0, 792.0]}
{"layout": 4810, "type": "text", "text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. ", "page_idx": 438, "bbox": [118, 543.0538330078125, 540, 580.9014892578125], "page_size": [612.0, 792.0]}
{"layout": 4811, "type": "text", "text": "class  mmdet.models.utils. Simplified Basic Block ( inplanes ,  planes ,  stride  $\\scriptstyle{:=I}$  ,  dilation  $\\scriptstyle{=I}$  , downsample  $\\mathbf{\\Pi}^{*}=$  None ,  style  $=$  'pytorch' ,  with_cp  $\\leftrightharpoons$  False , conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'BN'} ,  dcn  $=$  None , plugins  $\\mathbf{\\hat{\\rho}}$  None ,  init_fg  $=$  None ) ", "page_idx": 438, "bbox": [71, 597.3300170898438, 529, 646.655517578125], "page_size": [612.0, 792.0]}
{"layout": 4812, "type": "text", "text": "Simplified version of original basic residual block. This is used in  SCNet . ", "page_idx": 438, "bbox": [96, 645.3004760742188, 390.8533020019531, 658.6105346679688], "page_size": [612.0, 792.0]}
{"layout": 4813, "type": "text", "text": "• Norm layer is now optional • Last ReLU in forward function is removed ", "page_idx": 438, "bbox": [110, 663.232421875, 287.47064208984375, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 4814, "type": "text", "text": "forward  $(x)$  Forward function. ", "page_idx": 439, "bbox": [96, 73, 190, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4815, "type": "text", "text": "property norm1 normalization layer after the first convolution layer ", "page_idx": 439, "bbox": [96, 103.06400299072266, 320.5765380859375, 126.60552215576172], "page_size": [612.0, 792.0]}
{"layout": 4816, "type": "text", "text": "Type  nn.Module ", "page_idx": 439, "bbox": [137, 130.600830078125, 209, 145.13626098632812], "page_size": [612.0, 792.0]}
{"layout": 4817, "type": "text", "text": "", "text_level": 1, "page_idx": 439, "bbox": [95, 152, 172, 157.75], "page_size": [612.0, 792.0]}
{"layout": 4818, "type": "text", "text": "normalization layer after the second convolution layer ", "page_idx": 439, "bbox": [118, 161.11549377441406, 335, 174.42552185058594], "page_size": [612.0, 792.0]}
{"layout": 4819, "type": "text", "text": "Type  nn.Module ", "page_idx": 439, "bbox": [137, 178.42083740234375, 209, 192.95626831054688], "page_size": [612.0, 792.0]}
{"layout": 4820, "type": "text", "text": "class  mmdet.models.utils. Sine Positional Encoding ( num_feats ,  temperature  $\\mathbf{\\beta}=$  10000 ,  normalize  $=$  False , scale=6.283185307179586 ,  eps=1e-06 ,  offset=0.0 , init_cfg  $\\mathbf{\\beta}=$  None ) ", "page_idx": 439, "bbox": [71, 196.83201599121094, 527.9796142578125, 234.09193420410156], "page_size": [612.0, 792.0]}
{"layout": 4821, "type": "text", "text": "Position encoding with sine and cosine functions. ", "page_idx": 439, "bbox": [96, 232.8465118408203, 293.96710205078125, 246.1565399169922], "page_size": [612.0, 792.0]}
{"layout": 4822, "type": "text", "text": "See  End-to-End Object Detection with Transformers  for details. ", "page_idx": 439, "bbox": [96, 250.7794952392578, 350.9133605957031, 264.08953857421875], "page_size": [612.0, 792.0]}
{"layout": 4823, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 439, "bbox": [117, 270, 170, 282], "page_size": [612.0, 792.0]}
{"layout": 4824, "type": "text", "text": "•  num_feats  ( int ) – The feature dimension for each position along x-axis or y-axis. Note the final returned dimension for each position is 2 times of this value. •  temperature  ( int, optional ) – The temperature used for scaling the position embed- ding. Defaults to 10000. •  normalize  ( bool, optional ) – Whether to normalize the position embedding. Defaults to False. •  scale  ( float, optional ) – A scale factor that scales the position embedding. The scale will be used only when  normalize  is True. Defaults to   $2^{*}\\mathrm{\\textmu}$  . •  eps  ( float, optional ) – A value added to the denominator for numerical stability. De- faults to 1e-6. •  offset  ( float ) – offset add to embed when do the normalization. Defaults to 0. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None ", "page_idx": 439, "bbox": [145, 286.6455078125, 518, 467.3275451660156], "page_size": [612.0, 792.0]}
{"layout": 4825, "type": "text", "text": "forward ( mask ) ", "text_level": 1, "page_idx": 439, "bbox": [95, 473, 166, 484.75], "page_size": [612.0, 792.0]}
{"layout": 4826, "type": "text", "text": "", "page_idx": 439, "bbox": [115, 485.25, 302, 491.75], "page_size": [612.0, 792.0]}
{"layout": 4827, "type": "text", "text": "Parameters  mask  ( Tensor ) – ByteTensor mask. Non-zero values representing ignored posi- tions, while zero values means valid positions for this image. Shape [bs, h, w]. Returns Returned position embedding with shape  [bs, num_feats\\*2, h, w]. Return type  pos (Tensor) ", "page_idx": 439, "bbox": [137, 501.21087646484375, 523, 581.499267578125], "page_size": [612.0, 792.0]}
{"layout": 4828, "type": "text", "text": "class  mmdet.models.utils. Transformer ( encoder  $=$  None ,  decoder  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ) Implements the DETR transformer. ", "page_idx": 439, "bbox": [71, 585.3750610351562, 457.8623046875, 610.78955078125], "page_size": [612.0, 792.0]}
{"layout": 4829, "type": "text", "text": "Following the official DETR implementation, this module copy-paste from torch.nn.Transformer with modifica- tions: ", "page_idx": 439, "bbox": [96, 615.4124755859375, 540.0032958984375, 640.6775512695312], "page_size": [612.0, 792.0]}
{"layout": 4830, "type": "text", "text": "• positional encodings are passed in Multi head Attention • extra LN at the end of encoder is removed • decoder returns a stack of activation s from all decoding layers ", "page_idx": 439, "bbox": [110, 645.3004760742188, 365.3982849121094, 694.4755249023438], "page_size": [612.0, 792.0]}
{"layout": 4831, "type": "text", "text": "See  paper: End-to-End Object Detection with Transformers  for details. ", "page_idx": 439, "bbox": [96, 699.0984497070312, 379.12744140625, 712.4085083007812], "page_size": [612.0, 792.0]}
{"layout": 4832, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 440, "bbox": [117, 72, 169, 85], "page_size": [612.0, 792.0]}
{"layout": 4833, "type": "text", "text": "•  encoder  ( mmcv.ConfigDict  | Dict) – Config of Transformer Encoder. Defaults to None. •  decoder  (( mmcv.ConfigDict  | Dict)) – Config of Transformer Decoder. Defaults to None •  (obj  ( init_cfg ) –  mmcv.ConfigDict ): The Config for initialization. Defaults to None. ", "page_idx": 440, "bbox": [145, 89.23503875732422, 506.43994140625, 138.5604705810547], "page_size": [612.0, 792.0]}
{"layout": 4834, "type": "text", "text": "forward ( x ,  mask ,  query embed ,  pos_embed ) Forward function for  Transformer . ", "page_idx": 440, "bbox": [96, 143.0339813232422, 283.0733947753906, 168.44847106933594], "page_size": [612.0, 792.0]}
{"layout": 4835, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 440, "bbox": [136, 174, 188, 186], "page_size": [612.0, 792.0]}
{"layout": 4836, "type": "text", "text": "•  x  ( Tensor ) – Input query with shape [bs, c, h, w] where  $\\mathbf{c}=$   embed_dims. •  mask  ( Tensor ) – The key padding mask used for encoder and decoder, with shape [bs, h, w].•  query embed  ( Tensor ) – The query embedding for decoder, with shape [num_query, c]. •  pos_embed  ( Tensor ) – The positional encoding for encoder and decoder, with the same shape as  $x$  . ", "page_idx": 440, "bbox": [154, 191.00343322753906, 521, 282.0224609375], "page_size": [612.0, 792.0]}
{"layout": 4837, "type": "text", "text": "Returns ", "text_level": 1, "page_idx": 440, "bbox": [136, 288, 173, 299], "page_size": [612.0, 792.0]}
{"layout": 4838, "type": "text", "text": "results of decoder containing the following tensor. •  out_dec: Output from decoder. If return intermediate dec is True output has shape [num dec layers, bs, num_query, embed_dims], else has shape [1, bs, num_query, embed_dims]. • memory: Output results from encoder, with shape [bs, embed_dims, h, w]. ", "page_idx": 440, "bbox": [154, 304.57745361328125, 612.0, 365.70849609375], "page_size": [612.0, 792.0]}
{"layout": 4839, "type": "text", "text": "Return type  tuple[Tensor] ", "page_idx": 440, "bbox": [137.45504760742188, 369.7038269042969, 246.7046661376953, 384.2392578125], "page_size": [612.0, 792.0]}
{"layout": 4840, "type": "text", "text": "in it weights()Initialize the weights. ", "page_idx": 440, "bbox": [96, 389.9880065917969, 207, 413.5295104980469], "page_size": [612.0, 792.0]}
{"layout": 4841, "type": "text", "text": "mmdet.models.utils.adaptive avg pool 2 d(input, output size)Handle empty batch dimension to adaptive avg pool 2 d. ", "page_idx": 440, "bbox": [72, 418.0030517578125, 353, 443.4175109863281], "page_size": [612.0, 792.0]}
{"layout": 4842, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 440, "bbox": [117, 449, 169, 461], "page_size": [612.0, 792.0]}
{"layout": 4843, "type": "text", "text": "•  input  ( tensor ) – 4D tensor. •  output size  ( int, tuple[int,int] ) – the target output size. ", "page_idx": 440, "bbox": [145, 465.9725036621094, 417.40362548828125, 497.2155456542969], "page_size": [612.0, 792.0]}
{"layout": 4844, "type": "text", "text": "mmdet.models.utils. build linear layer ( cfg ,  \\*args ,  \\*\\*kwargs ) Build linear layer. :param cfg: The linear layer config, which should contain: • type (str): Layer type. • layer args: Args needed to instantiate an linear layer. ", "page_idx": 440, "bbox": [72, 501.6890869140625, 402.918212890625, 562.9685668945312], "page_size": [612.0, 792.0]}
{"layout": 4845, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 440, "bbox": [117, 575, 169, 587], "page_size": [612.0, 792.0]}
{"layout": 4846, "type": "text", "text": "•  args  ( argument list ) – Arguments passed to the  __init__  method of the corresponding linear layer. •  kwargs  ( keyword arguments ) – Keyword arguments passed to the  __init__  method of the corresponding linear layer. ", "page_idx": 440, "bbox": [145, 591.3530883789062, 518, 646.655517578125], "page_size": [612.0, 792.0]}
{"layout": 4847, "type": "text", "text": "Returns  Created linear layer. ", "page_idx": 440, "bbox": [118, 650.6498413085938, 237.72808837890625, 665.1852416992188], "page_size": [612.0, 792.0]}
{"layout": 4848, "type": "text", "text": "Return type  nn.Module ", "text_level": 1, "page_idx": 440, "bbox": [117, 671, 220, 682], "page_size": [612.0, 792.0]}
{"layout": 4849, "type": "text", "text": "mmdet.models.utils. build transformer ( cfg ,  default arg s  $\\mathbf{\\hat{\\rho}}$  None ) Builder for Transformer. ", "page_idx": 440, "bbox": [72, 692.9710693359375, 365.3393859863281, 718.3865356445312], "page_size": [612.0, 792.0]}
{"layout": 4850, "type": "text", "text": "mmdet.models.utils. gaussian radius ( det_size ,  min overlap ) Generate 2D gaussian radius. ", "page_idx": 441, "bbox": [72.0, 71.30303192138672, 346.88836669921875, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4851, "type": "text", "text": "This function is modified from the  official github repo . Given  min overlap , radius could computed by a quadratic equation according to Vieta’s formulas. There are 3 cases for computing gaussian radius, details are following: ", "page_idx": 441, "bbox": [96, 101.34046936035156, 314.62957763671875, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4852, "type": "text", "text": "", "page_idx": 441, "bbox": [96, 119.27247619628906, 494.46435546875, 132.58250427246094], "page_size": [612.0, 792.0]}
{"layout": 4853, "type": "text", "text": "", "page_idx": 441, "bbox": [96, 137.20545959472656, 376.9456481933594, 150.51548767089844], "page_size": [612.0, 792.0]}
{"layout": 4854, "type": "text", "text": "• Explanation of figure:  lt  and  br  indicates the left-top and bottom-right corner of ground truth box.  x indicates the generated corner at the limited position when  radius  $\\tt\\lrcorner\\mathrm{\\bfr}$  . ", "page_idx": 441, "bbox": [110, 155.13844299316406, 540.0003662109375, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 4855, "type": "text", "text": "• Case1: one corner is inside the gt box and the other is outside. ", "page_idx": 441, "bbox": [110, 185.0264434814453, 366.6048278808594, 198.3364715576172], "page_size": [612.0, 792.0]}
{"layout": 4856, "type": "image", "page_idx": 441, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_162.jpg", "bbox": [92, 203, 545, 368], "page_size": [612.0, 792.0], "ocr_text": "< width >\n\nLtstes5--brs +\n+--X---------- +--+\n|\n|\noverlap |\n|\n|\nPos poso asst br=-+\n|\n\n", "vlm_text": "The image contains an ASCII diagram representing a rectangle, labeled with dimensions and symbols indicating various aspects of the shape. Here's a breakdown of the elements:\n\n1. **Width**: Indicated by a horizontal line at the top of the rectangle, with the label `width` and arrows pointing left (`<|`) and right (`|>`).\n\n2. **Height**: Denoted by a vertical line next to the rectangle, with a label `height` and arrows pointing up (`^`) and down (`v`).\n\n3. **Points and Symbols**:\n   - `lt` and `br`: These labels likely stand for the top-left and bottom-right corners of the rectangle.\n   - `x`: Occurs twice, suggesting it marks specific reference points along the top and bottom of the rectangle.\n   - `overlap`: Positioned in the center of the rectangle, possibly indicating shared or intersecting space.\n\n4. **Different Line Styles**: The diagram uses vertical (`|`), horizontal (`-`), and corner (`+`) lines to shape the rectangle and indicate structure.\n\nOverall, the image is a visual representation of a rectangle, highlighting its width, height, and particular points or features within it."}
{"layout": 4857, "type": "text", "text": "To ensure IoU of generated box and gt box is larger than  min overlap : ", "page_idx": 441, "bbox": [96, 374.1174621582031, 383, 387.427490234375], "page_size": [612.0, 792.0]}
{"layout": 4858, "type": "equation", "text": "\n$$\n\\frac{(w-r)*(h-r)}{w*h+(w+h)r-r^{2}}\\geq i o u\\quad\\Rightarrow\\quad r^{2}-(w+h)r+\\frac{1-i o u}{1+i o u}*w*h\\geq0\n$$\n ", "text_format": "latex", "page_idx": 441, "bbox": [161, 395, 475, 423], "page_size": [612.0, 792.0]}
{"layout": 4859, "type": "equation", "text": "\n$$\na=1,\\quad b=-(w+h),\\quad c=\\frac{1-i o u}{1+i o u}*w*h r\\leq\\frac{-\\;b-\\sqrt{b^{2}-4*a*c}}{2*a}\n$$\n ", "text_format": "latex", "page_idx": 441, "bbox": [168, 426, 475, 453], "page_size": [612.0, 792.0]}
{"layout": 4860, "type": "text", "text": "• Case2: both two corners are inside the gt box. ", "page_idx": 441, "bbox": [110, 458.32647705078125, 301.4982604980469, 471.6365051269531], "page_size": [612.0, 792.0]}
{"layout": 4861, "type": "image", "page_idx": 441, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_163.jpg", "bbox": [94, 476, 543, 616], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is an ASCII diagram illustrating two overlapping rectangles. The key elements are:\n\n- The outer rectangle is defined by \"lt\" (left top) and \"br\" (bottom right) corners.\n- There is a smaller rectangle inside indicated by \"+\" and \"x\" symbols.\n- The text \"overlap\" is within the overlapping section of the inner rectangle.\n- Dimensions labeled as \"width\" and \"height\" with arrows indicate the size or direction."}
{"layout": 4862, "type": "text", "text": "To ensure IoU of generated box and gt box is larger than  min overlap : ", "page_idx": 441, "bbox": [96, 623.5074462890625, 383, 636.8175048828125], "page_size": [612.0, 792.0]}
{"layout": 4863, "type": "equation", "text": "\n$$\n\\begin{array}{r}{\\displaystyle\\frac{(w-2*r)*(h-2*r)}{w*h}\\geq i o u\\quad\\Rightarrow\\quad4r^{2}-2(w+h)r+(1-i o u)*w*h\\geq0}\\\\ {\\displaystyle a=4,\\quad b=-2(w+h),\\quad c=(1-i o u)*w*h r\\leq\\frac{-\\displaystyle b-\\sqrt{b^{2}-4*a*c}}{2*a}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 441, "bbox": [149, 644, 488, 700], "page_size": [612.0, 792.0]}
{"layout": 4864, "type": "text", "text": "• Case3: both two corners are outside the gt box. ", "page_idx": 441, "bbox": [110, 704.3964233398438, 306.4795227050781, 717.7064819335938], "page_size": [612.0, 792.0]}
{"layout": 4865, "type": "image", "page_idx": 442, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_164.jpg", "bbox": [92, 73, 544, 240], "page_size": [612.0, 792.0], "ocr_text": "< width >|\nX--+---------------- +\n+-1t------------- +\n|\n|\noverlap |\n|\n|\ntoon ---n nn br--+\n|\n\n", "vlm_text": "The image is a diagram illustrating two overlapping rectangles with labels and dimensions. The outer rectangle has a width and height marked, while there's an overlapping inner rectangle labeled \"overlap\" with points \"lt\" and \"br\". The diagram includes directional arrows and measurements, with \"x\" indicating endpoints on the horizontal axis. The labels and symbols are likely used for indicating geometric properties or layout constraints."}
{"layout": 4866, "type": "text", "text": "To ensure IoU of generated box and gt box is larger than  min overlap : ", "page_idx": 442, "bbox": [96, 245.5994415283203, 383.631591796875, 258.90948486328125], "page_size": [612.0, 792.0]}
{"layout": 4867, "type": "equation", "text": "\n$$\n\\begin{array}{r l r}{\\displaystyle\\frac{w*h}{w+2*r)*(h+2*r)}\\geq i o u}&{{}\\Rightarrow}&{4*i o u*r^{2}+2*i o u*(w+h)r+(i o u-1)*w*h\\leq0}\\\\ {\\displaystyle}&{{}}&{a=4*i o u,\\quad b=2*i o u*(w+h),\\quad c=(i o u-1)*w*h}\\\\ {\\displaystyle}&{{}}&{r\\leq\\displaystyle\\frac{-b+\\sqrt{b^{2}-4*a*c}}{2*a}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 442, "bbox": [124, 266, 522, 342], "page_size": [612.0, 792.0]}
{"layout": 4868, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 442, "bbox": [117, 346, 169, 357], "page_size": [612.0, 792.0]}
{"layout": 4869, "type": "text", "text": "•  det_size  ( list[int] ) – Shape of object. •  min overlap  ( float ) – Min IoU with ground truth for boxes generated by keypoints inside the gaussian kernel. ", "page_idx": 442, "bbox": [145, 362.78948974609375, 518, 405.9875183105469], "page_size": [612.0, 792.0]}
{"layout": 4870, "type": "text", "text": "Returns  Radius of gaussian kernel. ", "page_idx": 442, "bbox": [118, 409.98284912109375, 262, 424.5182800292969], "page_size": [612.0, 792.0]}
{"layout": 4871, "type": "text", "text": "Return type  radius (int) ", "page_idx": 442, "bbox": [118, 427.9158630371094, 218.6898956298828, 442.4512939453125], "page_size": [612.0, 792.0]}
{"layout": 4872, "type": "text", "text": "mmdet.models.utils. gen gaussian target ( heatmap ,  center ,  radius ,  $k{=}I$  ) Generate 2D gaussian heatmap. ", "page_idx": 442, "bbox": [72, 446.3260803222656, 396.5624084472656, 471.74053955078125], "page_size": [612.0, 792.0]}
{"layout": 4873, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 442, "bbox": [118, 477, 169, 489], "page_size": [612.0, 792.0]}
{"layout": 4874, "type": "text", "text": "•  heatmap  ( Tensor ) – Input heatmap, the gaussian kernel will cover on it and maintain the max value. •  center  ( list[int] ) – Coord of gaussian kernel’s center. •  radius  ( int ) – Radius of gaussian kernel. •  k  ( int ) – Coefficient of gaussian kernel. Default: 1. ", "page_idx": 442, "bbox": [145, 494.2965393066406, 518, 573.3595581054688], "page_size": [612.0, 792.0]}
{"layout": 4875, "type": "text", "text": "Returns  Updated heatmap covered by gaussian kernel. ", "page_idx": 442, "bbox": [118, 577.3548583984375, 339.4761047363281, 591.8902587890625], "page_size": [612.0, 792.0]}
{"layout": 4876, "type": "text", "text": "Return type  out heat map (Tensor) ", "page_idx": 442, "bbox": [118, 595.287841796875, 262, 609.8232421875], "page_size": [612.0, 792.0]}
{"layout": 4877, "type": "text", "text": "mmdet.models.utils. interpolate as ( source ,  target ,  mode  $\\mathbf{=}$  'bilinear' ,  align corners  $\\mathbf{\\hat{=}}$  False ) Interpolate the  source  to the shape of the  target . ", "page_idx": 442, "bbox": [72, 613.6990966796875, 467.4563293457031, 639.1135864257812], "page_size": [612.0, 792.0]}
{"layout": 4878, "type": "text", "text": "The  source  must be a Tensor, but the  target  can be a Tensor or a np.ndarray with the shape (..., target_h, target_w). ", "page_idx": 442, "bbox": [96, 643.5870971679688, 540.0010986328125, 657.0465698242188], "page_size": [612.0, 792.0]}
{"layout": 4879, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 442, "bbox": [117, 662, 168, 675], "page_size": [612.0, 792.0]}
{"layout": 4880, "type": "text", "text": "•  source  ( Tensor ) – A 3D/4D Tensor with the shape (N, H, W) or (N, C, H, W). •  target  ( Tensor | np.ndarray ) – The interpolation target with the shape (..., target_h, target_w). ", "page_idx": 442, "bbox": [145, 679.6015014648438, 518, 722.799560546875], "page_size": [612.0, 792.0]}
{"layout": 4881, "type": "text", "text": "•  mode  ( str ) – Algorithm used for interpolation. The options are the same as those in F.interpolate(). Default: 'bilinear'.", "page_idx": 443, "bbox": [145, 71.45246887207031, 518, 96.71752166748047], "page_size": [612.0, 792.0]}
{"layout": 4882, "type": "text", "text": "•  align corners  ( bool ) – The same as the argument in F.interpolate(). ", "page_idx": 443, "bbox": [145, 101.34046936035156, 438.808349609375, 114.65050506591797], "page_size": [612.0, 792.0]}
{"layout": 4883, "type": "text", "text": "Returns  The interpolated source Tensor. ", "page_idx": 443, "bbox": [118, 118.64483642578125, 283.4364013671875, 133.18026733398438], "page_size": [612.0, 792.0]}
{"layout": 4884, "type": "text", "text": "Return type  Tensor ", "page_idx": 443, "bbox": [118, 136.57781982421875, 201, 151.11325073242188], "page_size": [612.0, 792.0]}
{"layout": 4885, "type": "text", "text": "mmdet.models.utils. make divisible ( value ,  divisor ,  min_value  $=$  None ,  min_ratio=0.9 ) Make divisible function. ", "page_idx": 443, "bbox": [71, 154.98899841308594, 446.2013244628906, 180.4034881591797], "page_size": [612.0, 792.0]}
{"layout": 4886, "type": "text", "text": "This function rounds the channel number to the nearest value that can be divisible by the divisor. It is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by divisor. It can be seen here:  https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py  # noqa ", "page_idx": 443, "bbox": [96, 185.0264434814453, 540, 222.2465057373047], "page_size": [612.0, 792.0]}
{"layout": 4887, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 443, "bbox": [117, 228, 170, 239], "page_size": [612.0, 792.0]}
{"layout": 4888, "type": "text", "text": "•  value  ( int ) – The original channel number. •  divisor  ( int ) – The divisor to fully divide the channel number. •  min_value  ( int ) – The minimum value of the output channel. Default: None, means that the minimum value equal to the divisor. •  min_ratio  ( float ) – The minimum ratio of the rounded channel number to the original channel number. Default: 0.9. ", "page_idx": 443, "bbox": [145, 244.8024444580078, 518, 335.82049560546875], "page_size": [612.0, 792.0]}
{"layout": 4889, "type": "text", "text": "", "page_idx": 443, "bbox": [117, 346.25, 307, 354], "page_size": [612.0, 792.0]}
{"layout": 4890, "type": "text", "text": "Return type  int ", "text_level": 1, "page_idx": 443, "bbox": [117, 360, 187, 371], "page_size": [612.0, 792.0]}
{"layout": 4891, "type": "text", "text": "mmdet.models.utils. nch w to nl c ( x ) Flatten [N, C, H, W] shape tensor to [N, L, C] shape tensor. ", "page_idx": 443, "bbox": [71, 376.1590576171875, 334, 401.57452392578125], "page_size": [612.0, 792.0]}
{"layout": 4892, "type": "text", "text": "Parameters  x  ( Tensor ) – The input tensor of shape [N, C, H, W] before conversion. Returns  The output tensor of shape [N, L, C] after conversion. Return type  Tensor ", "page_idx": 443, "bbox": [118, 405.56884765625, 459, 455.9703063964844], "page_size": [612.0, 792.0]}
{"layout": 4893, "type": "text", "text": "mmdet.models.utils. nl c to nch w ( x ,  hw_shape ) Convert [N, L, C] shape tensor to [N, C, H, W] shape tensor. ", "page_idx": 443, "bbox": [71, 459.8460998535156, 338.09149169921875, 485.26055908203125], "page_size": [612.0, 792.0]}
{"layout": 4894, "type": "text", "text": "Parameters ", "text_level": 1, "page_idx": 443, "bbox": [117, 491, 170, 503], "page_size": [612.0, 792.0]}
{"layout": 4895, "type": "text", "text": "•  x  ( Tensor ) – The input tensor of shape [N, L, C] before conversion. •  hw_shape  ( Sequence[int] ) – The height and width of output feature map. ", "page_idx": 443, "bbox": [145, 507.8155212402344, 459, 539.05859375], "page_size": [612.0, 792.0]}
{"layout": 4896, "type": "text", "text": "Returns  The output tensor of shape [N, C, H, W] after conversion. ", "page_idx": 443, "bbox": [118, 543.0538940429688, 387.21673583984375, 557.5892944335938], "page_size": [612.0, 792.0]}
{"layout": 4897, "type": "text", "text": "Return type  Tensor ", "page_idx": 443, "bbox": [118, 560.9868774414062, 201, 575.5222778320312], "page_size": [612.0, 792.0]}
{"layout": 4898, "type": "text", "text": "MMDET.UTILS ", "page_idx": 444, "bbox": [442.9049987792969, 162.4604949951172, 540.0000610351562, 183.00425720214844], "page_size": [612.0, 792.0]}
{"layout": 4899, "type": "text", "text": "INDICES AND TABLES ", "text_level": 1, "page_idx": 446, "bbox": [385, 164, 542, 181], "page_size": [612.0, 792.0]}
{"layout": 4900, "type": "text", "text": "• genindex\n\n • search ", "page_idx": 446, "bbox": [88, 225.87342834472656, 132.8321533203125, 257.115478515625], "page_size": [612.0, 792.0]}
{"layout": 4901, "type": "text", "text": "m mmdet.apis ,  181 mmdet.core.anchor ,  183 mmdet.core.bbox ,  191 mmdet.core.evaluation ,  219 mmdet.core.export ,  208 mmdet.core.mask ,  211 mmdet.core.post processing ,  221 mmdet.core.utils, 224mmdet.datasets ,  227 mmdet.datasets.api wrappers ,  257 mmdet.datasets.pipelines ,  239 mmdet.datasets.samplers ,  256 mmdet.models.backbones ,  273 mmdet.models.dense heads ,  301 mmdet.models.detectors ,  259 mmdet.models.losses ,  412 mmdet.models.necks ,  291 mmdet.models.roi_heads ,  383 mmdet.models.utils ,  425 ", "page_idx": 448, "bbox": [71, 205.44456481933594, 233.1451416015625, 454.69635009765625], "page_size": [612.0, 792.0]}
{"layout": 4902, "type": "text", "text": "", "text_level": 1, "page_idx": 450, "bbox": [71, 210, 82, 220.75], "page_size": [612.0, 792.0]}
{"layout": 4903, "type": "text", "text": "Accuracy  ( class in mmdet.models.losses ),  412 adaptive avg pool 2 d() ( in module mmdet.models.utils ),  433 Adaptive Avg Pool 2 d  ( class in mmdet.models.utils ),  425 add dummy nm s for on nx() ( in module mmdet.core.export ),  208 add_gt_()  ( mmdet.core.bbox.Assign Result method ),  192 adjust width group() ( mmdet.models.backbones.RegNet method ), 282 Albu  ( class in mmdet.datasets.pipelines ),  239 alb u builder() ( mmdet.datasets.pipelines.Albu method ),  240 all reduce dic t()  ( in module mmdet.core.utils ),  224 all reduce grads()  ( in module mmdet.core.utils ),  224 anchor center()  ( mmdet.models.dense heads.GFLHead method ),  340 anchor inside flags() ( in module mmdet.core.anchor ),  191 anchor offset()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  364 Anchor Free Head  ( class in mmdet.models.dense heads ), 302 Anchor Generator  ( class in mmdet.core.anchor ),  183 AnchorHead  ( class in mmdet.models.dense heads ),  305 areas  ( mmdet.core.mask.Base Instance Masks property ), 211 areas  ( mmdet.core.mask.Bitmap Masks property ),  214 areas  ( mmdet.core.mask.Polygon Masks property ),  216 assign()  ( mmdet.core.bbox.Base As signer method ),  193 assign() ( mmdet.core.bbox.Center Region As signer method ),  194 assign()  ( mmdet.core.bbox.MaxI oU As signer method ), 198 assign()  ( mmdet.core.bbox.Region As signer method ), 200 assign one hot gt indices() ( mmdet.core.bbox.Center Region As signer method ),  195 assign w rt overlaps() ( mmdet.core.bbox.MaxI oU As signer method ), ", "page_idx": 450, "bbox": [71, 226.69700622558594, 305, 718.36328125], "page_size": [612.0, 792.0]}
{"layout": 4904, "type": "text", "text": "199 Assign Result  ( class in mmdet.core.bbox ),  191 Associative Embedding Loss ( class in mmdet.models.losses ),  413 a sync inference detector() ( in module mmdet.apis ),  181 a sync simple test() ( mmdet.models.detectors.Two Stage Detector method ),  269 a sync simple test() ( mmdet.models.roi_heads.Base RoI Head method ),  387 a sync simple test() ( mmdet.models.roi_heads.Standard RoI Head method ),  410 ATSS  ( class in mmdet.models.detectors ),  259 ATSSHead  ( class in mmdet.models.dense heads ),  301 attention pool()  ( mmdet.models.roi_heads.SABLHead method ),  404 aug_test()  ( mmdet.models.dense heads.Anchor Free Head method ),  303 aug_test() ( mmdet.models.dense heads.AnchorHead method ),  305 aug_test()  ( mmdet.models.dense heads.YOLOV3Head method ),  378 aug_test() ( mmdet.models.detectors.Base Detector method ),  259 aug_test() ( mmdet.models.detectors.CenterNet method ),  261 aug_test() ( mmdet.models.detectors.CornerNet method ),  262 aug_test()  ( mmdet.models.detectors.RPN method ),  265 aug_test()  ( mmdet.models.detectors.Single Stage Detector method ),  267 aug_test()  ( mmdet.models.detectors.Trident Faster R CNN method ),  269 aug_test()  ( mmdet.models.detectors.Two Stage Detector method ),  269 aug_test()  ( mmdet.models.detectors.YOLACT method ), 271 aug_test() ( mmdet.models.roi_heads.Base RoI Head method ),  387 ", "page_idx": 450, "bbox": [310, 211.40321350097656, 549.9039306640625, 714.8750610351562], "page_size": [612.0, 792.0]}
{"layout": 4905, "type": "text", "text": "aug_test() (mmdet.models.roi_heads.Cascade RoI Headmethod ),  388 aug_test()  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head method ),  398 aug_test() ( mmdet.models.roi_heads.S CNet RoI Head method ),  406 aug_test() ( mmdet.models.roi_heads.Sparse RoI Head method ),  409 aug_test()  ( mmdet.models.roi_heads.Standard RoI Head method ),  410 aug test b boxes()  ( mmdet.models.roi_heads.Trident RoI Head method ),  412 aug test mask()  ( mmdet.models.roi_heads.Point Rend RoI Head method ),  402 aug test rp n()  ( mmdet.models.dense heads.Cascade RP N Head method ),  310 AutoAssign  ( class in mmdet.models.detectors ),  259 Auto Assign Head  ( class in mmdet.models.dense heads ), 308 Auto Augment  ( class in mmdet.datasets.pipelines ),  240 average precision() ( in module mmdet.core.evaluation ),  219 ", "page_idx": 451, "bbox": [71, 71.30303192138672, 347, 335.8206787109375], "page_size": [612.0, 792.0]}
{"layout": 4906, "type": "text", "text": "B box Overlaps 2 D  ( class in mmdet.core.bbox ),  194 BFP  ( class in mmdet.models.necks ),  291 binary cross entropy() ( in module mmdet.models.losses ),  422 Bitmap Masks (class in mmdet.core.mask), 213Bounded I oU Loss  ( class in mmdet.models.losses ),  413 Brightness Transform ( class in mmdet.datasets.pipelines ),  241 build as signer()  ( in module mmdet.core.bbox ),  207 build b box code r()  ( in module mmdet.core.bbox ), 207 build data loader()  ( in module mmdet.datasets ),  237 build linear layer() ( in module mmdet.models.utils ),  433 build model from cf g() ( in module mmdet.core.export ),  209 build roi layers()  ( mmdet.models.roi_heads.Base RoI Extractor method ),  386 build sampler()  ( in module mmdet.core.bbox ),  207 build transformer()  ( in module mmdet.models.utils ), 433 ", "page_idx": 451, "bbox": [310, 71.30303192138672, 583.2590942382812, 323.86566162109375], "page_size": [612.0, 792.0]}
{"layout": 4907, "type": "text", "text": "", "text_level": 1, "page_idx": 451, "bbox": [310, 331, 321, 342.75], "page_size": [612.0, 792.0]}
{"layout": 4908, "type": "text", "text": "calc region()  ( in module mmdet.core.anchor ),  191 calc sub regions()  ( mmdet.models.roi_heads.GridHead method ),  397 calculate pos recall() ( mmdet.models.dense heads.FSAFHead method ),  334 Cascade R CNN  ( class in mmdet.models.detectors ),  261 Cascade RoI Head  ( class in mmdet.models.roi_heads ), 388 Cascade RP N Head  ( class in mmdet.models.dense heads ), 310 center of mass()  ( in module mmdet.core.utils ),  224 center ness target() ( mmdet.models.dense heads.FCOSHead method ),  333 CenterNet  ( class in mmdet.models.detectors ),  261 Center Net Head  ( class in mmdet.models.dense heads ), 311 Center Region As signer  ( class in mmdet.core.bbox ), 194 centers to b boxes() ( mmdet.models.dense heads.Rep Points Head method ),  354 Centripetal Head ( class in mmdet.models.dense heads ),  313 Channel Mapper  ( class in mmdet.models.necks ),  292 CIoULoss  ( class in mmdet.models.losses ),  414 Cityscape s Data set  ( class in mmdet.datasets ),  227 Class Balanced Data set  ( class in mmdet.datasets ),  228 Coarse Mask Head  ( class in mmdet.models.roi_heads ), 389 ", "page_idx": 451, "bbox": [310, 347.11419677734375, 552.2155151367188, 719.228515625], "page_size": [612.0, 792.0]}
{"layout": 4909, "type": "text", "text": "", "text_level": 1, "page_idx": 451, "bbox": [71, 344, 81, 357], "page_size": [612.0, 792.0]}
{"layout": 4910, "type": "text", "text": "Balanced L 1 Loss  ( class in mmdet.models.losses ),  413 Base As signer  ( class in mmdet.core.bbox ),  193 Base B Box Code r (class in mmdet.core.bbox), 193Base Detector  ( class in mmdet.models.detectors ),  259 Base Instance Masks  ( class in mmdet.core.mask ),  211 Base RoI Extractor  ( class in mmdet.models.roi_heads ), 386 Base RoI Head  ( class in mmdet.models.roi_heads ),  387 Base Sampler (class in mmdet.core.bbox), 193b box 2 distance()  ( in module mmdet.core.bbox ),  204 b box 2 result()  ( in module mmdet.core.bbox ),  204 bbox2roi()  ( in module mmdet.core.bbox ),  204 b box cx cy wh to xy xy() ( in module mmdet.core.bbox ),  204 bbox_flip()  ( in module mmdet.core.bbox ),  204 bbox_flip() ( mmdet.datasets.pipelines.RandomFlip method ),  252 b box mapping()  ( in module mmdet.core.bbox ),  205 b box mapping back()  ( in module mmdet.core.bbox ), 205 b box on nx export()  ( mmdet.models.roi_heads.Standard RoI Head method ),  410 b box overlaps()  ( in module mmdet.core.bbox ),  205 b box p red split()  ( mmdet.models.roi_heads.SABLHead method ),  405 b box re scale()  ( in module mmdet.core.bbox ),  207 b box xy xy to cx cy wh() ( in module mmdet.core.bbox ),  207 bboxes  ( mmdet.core.bbox.Sampling Result property ),  201 BBoxHead  ( class in mmdet.models.roi_heads ),  383 ", "page_idx": 451, "bbox": [71, 360.4902038574219, 347, 720.6495361328125], "page_size": [612.0, 792.0]}
{"layout": 4911, "type": "text", "text": "COCO  ( class in mmdet.datasets.api wrappers ),  257 Coco Data set  ( class in mmdet.datasets ),  228 Coco Pan optic Data set  ( class in mmdet.datasets ),  230 Collect  ( class in mmdet.datasets.pipelines ),  241 collect loss level single() ( mmdet.models.dense heads.FSAFHead method ),  335 Color Transform  ( class in mmdet.datasets.pipelines ), 242 Combined Sampler (class in mmdet.core.bbox), 195Compose  ( class in mmdet.datasets.pipelines ),  242 Con cat Data set  ( class in mmdet.datasets ),  231 Contrast Transform ( class in mmdet.datasets.pipelines ),  242 Con v FCB Box Head  ( class in mmdet.models.roi_heads ), 390 Con v Up sample  ( class in mmdet.models.utils ),  426 CornerHead  ( class in mmdet.models.dense heads ),  317 CornerNet  ( class in mmdet.models.detectors ),  262 crop()  ( mmdet.core.mask.Base Instance Masks method ), 211 crop()  ( mmdet.core.mask.Bitmap Masks method ),  214 crop()  ( mmdet.core.mask.Polygon Masks method ),  216 crop() ( mmdet.models.dense heads.YO L ACT Proton et method ),  373 crop and resize()  ( mmdet.core.mask.Base Instance Masks method ),  211 crop and resize() ( mmdet.core.mask.Bitmap Masks method ),  214 crop and resize()  ( mmdet.core.mask.Polygon Masks method ),  216 cross entropy()  ( in module mmdet.models.losses ), 422 Cross Entropy Loss  ( class in mmdet.models.losses ),  414 CSPDarknet  ( class in mmdet.models.backbones ),  273 CSPLayer  ( class in mmdet.models.utils ),  425 C TRes Net Neck  ( class in mmdet.models.necks ),  291 cuda()  ( mmdet.models.detectors.Knowledge Distillation Single Stage Detector method ),  264 Custom Data set  ( class in mmdet.datasets ),  232 CutOut  ( class in mmdet.datasets.pipelines ),  242 ", "page_idx": 452, "bbox": [71, 71.30303192138672, 305, 562.969482421875], "page_size": [612.0, 792.0]}
{"layout": 4912, "type": "text", "text": "D ", "text_level": 1, "page_idx": 452, "bbox": [70, 571, 83, 585], "page_size": [612.0, 792.0]}
{"layout": 4913, "type": "text", "text": "Darknet  ( class in mmdet.models.backbones ),  274 decode()  ( mmdet.core.bbox.Base B Box Code r method ), 193 decode() ( mmdet.core.bbox.Delta XY WH B Box Code r method ),  196 decode() ( mmdet.core.bbox.Distance Point B Box Code r method ),  196 decode() ( mmdet.core.bbox.Pseudo B Box Code r method ),  199 decode()  ( mmdet.core.bbox.T BL RB Box Code r method ), 203 ", "page_idx": 452, "bbox": [71, 587.9219970703125, 301, 720.9334716796875], "page_size": [612.0, 792.0]}
{"layout": 4914, "type": "text", "text": "decode heat map()  ( mmdet.models.dense heads.Center Net Head method ),  311 decode heat map()  ( mmdet.models.dense heads.CornerHead method ),  317 Decoupled SOLO Head ( class in mmdet.models.dense heads ),  327 Decoupled SOLO Light Head ( class in mmdet.models.dense heads ),  329 Deep Fashion Data set  ( class in mmdet.datasets ),  234 Default Format Bundle ( class in mmdet.datasets.pipelines ),  243 De formable DE TR  ( class in mmdet.models.detectors ),  263 De formable DET RHead ( class in mmdet.models.dense heads ),  329 Delta XY WH B Box Code r (class in mmdet.core.bbox), 195DetectoRS Res Net  ( class in mmdet.models.backbones ), 276 DetectoRS Res NeXt ( class in mmdet.models.backbones ),  275 DETR  ( class in mmdet.models.detectors ),  263 DETRHead  ( class in mmdet.models.dense heads ),  322 De tr Transformer Decoder ( class in mmdet.models.utils ),  426 De tr Transformer Decoder Layer ( class in mmdet.models.utils ),  427 DiceLoss  ( class in mmdet.models.losses ),  414 DIIHead  ( class in mmdet.models.roi_heads ),  390 Dilated Encoder  ( class in mmdet.models.necks ),  292 DIoULoss  ( class in mmdet.models.losses ),  414 distance 2 b box()  ( in module mmdet.core.bbox ),  207 Distance Point B Box Code r  ( class in mmdet.core.bbox ), 196 Di stEv al Hook  ( class in mmdet.core.evaluation ),  219 Dist Optimizer Hook  ( class in mmdet.core.utils ),  224 Distributed Group Sampler  ( class in mmdet.datasets ), 234 Distributed Group Sampler ( class in mmdet.datasets.samplers ),  256 Distributed Sampler  ( class in mmdet.datasets ),  234 Distributed Sampler ( class in mmdet.datasets.samplers ),  256 Distribution Focal Loss ( class in mmdet.models.losses ),  415 Double Con v FCB Box Head ( class in mmdet.models.roi_heads ),  392 Double Head RoI Head ( class in mmdet.models.roi_heads ),  392 dynamic clip for on nx() ( in module mmdet.core.export ),  209 Dynamic Con v  ( class in mmdet.models.utils ),  427 Dynamic RoI Head  ( class in mmdet.models.roi_heads ), 393 ", "page_idx": 452, "bbox": [310, 71.30297088623047, 564, 694.4763793945312], "page_size": [612.0, 792.0]}
{"layout": 4915, "type": "text", "text": "E ", "text_level": 1, "page_idx": 453, "bbox": [71, 70, 82, 83.75], "page_size": [612.0, 792.0]}
{"layout": 4916, "type": "text", "text": "Embedding RP N Head ( class in mmdet.models.dense heads ),  331 encode()  ( mmdet.core.bbox.Base B Box Code r method ), 193 encode() ( mmdet.core.bbox.Delta XY WH B Box Code r method ),  196 encode() ( mmdet.core.bbox.Distance Point B Box Code r method ),  197 encode() ( mmdet.core.bbox.Pseudo B Box Code r method ),  199 encode()  ( mmdet.core.bbox.T BL RB Box Code r method ), 204 encode mask results() ( in module mmdet.core.mask), 217Equalize Transform ( class in mmdet.datasets.pipelines ),  243 eval_map()  ( in module mmdet.core.evaluation ),  219 e val recalls()  ( in module mmdet.core.evaluation ), 220 EvalHook  ( class in mmdet.core.evaluation ),  219 evaluate() ( mmdet.datasets.Cityscape s Data set method ),  227 evaluate()  ( mmdet.datasets.Coco Data set method ),  228 evaluate() ( mmdet.datasets.Coco Pan optic Data set method ),  231 evaluate()  ( mmdet.datasets.Con cat Data set method ), 232 evaluate()  ( mmdet.datasets.Custom Data set method ), 233 evaluate()  ( mmdet.datasets.LV IS V 05 Data set method ), 235 evaluate()  ( mmdet.datasets.VOCDataset method ),  236 evaluate pan json() ( mmdet.datasets.Coco Pan optic Data set method ),  231 Expand  ( class in mmdet.datasets.pipelines ),  243 expand() ( mmdet.core.mask.Base Instance Masks method ),  211 expand()  ( mmdet.core.mask.Bitmap Masks method ),  214 expand() ( mmdet.core.mask.Polygon Masks method ), 216 extract feat()  ( mmdet.models.detectors.Base Detector method ),  259 extract feat() ( mmdet.models.detectors.RPN method ),  266 extract feat()  ( mmdet.models.detectors.Single Stage Detector method ),  267 extract feat()  ( mmdet.models.detectors.Two Stage Detector method ),  270 extract feats()  ( mmdet.models.detectors.Base Detector method ),  259 ", "page_idx": 453, "bbox": [71, 86.74602508544922, 307, 697.9644165039062], "page_size": [612.0, 792.0]}
{"layout": 4917, "type": "text", "text": "F ", "text_level": 1, "page_idx": 453, "bbox": [309, 69, 320, 81.75], "page_size": [612.0, 792.0]}
{"layout": 4918, "type": "text", "text": "fast_nms()  ( in module mmdet.core.post processing ), 221 FasterRCNN  ( class in mmdet.models.detectors ),  264 FastRCNN  ( class in mmdet.models.detectors ),  263 FC N Mask Head  ( class in mmdet.models.roi_heads ),  393 FCOS  ( class in mmdet.models.detectors ),  263 FCOSHead  ( class in mmdet.models.dense heads ),  331 Feature Adaption ( class in mmdet.models.dense heads ),  336 Feature Relay Head  ( class in mmdet.models.roi_heads ), 395 filter scores and top k() ( in module mmdet.core.utils ),  225 flip()  ( mmdet.core.mask.Base Instance Masks method ), 211 flip()  ( mmdet.core.mask.Bitmap Masks method ),  214 flip()  ( mmdet.core.mask.Polygon Masks method ),  216 flip tensor()  ( in module mmdet.core.utils ),  225 FocalLoss  ( class in mmdet.models.losses ),  415 format results()  ( mmdet.datasets.Cityscape s Data set method ),  227 format results() ( mmdet.datasets.Coco Data set method ),  229 format results() ( mmdet.datasets.Custom Data set method ),  233 forward() ( mmdet.models.backbones.CSPDarknet method ),  274 forward()  ( mmdet.models.backbones.Darknet method ), 275 forward()  ( mmdet.models.backbones.DetectoRS Res Net method ),  276 forward() ( mmdet.models.backbones.Hourglass Net method ),  279 forward()  ( mmdet.models.backbones.HRNet method ), 278 forward() ( mmdet.models.backbones.Mobile Ne tV 2 method ),  279 forward()  ( mmdet.models.backbones.Pyramid Vision Transformer method ),  281 forward()  ( mmdet.models.backbones.RegNet method ), 283 forward()  ( mmdet.models.backbones.ResNet method ), 287 forward() ( mmdet.models.backbones.SSDVGG method ),  289 forward()  ( mmdet.models.backbones.S win Transformer method ),  290 forward()  ( mmdet.models.dense heads.Anchor Free Head method ),  303 forward() ( mmdet.models.dense heads.AnchorHead method ),  306 forward() ( mmdet.models.dense heads.ATSSHead method ),  301 ", "page_idx": 453, "bbox": [310, 86.74590301513672, 570, 721.8743286132812], "page_size": [612.0, 792.0]}
{"layout": 4919, "type": "text", "text": "forward()  ( mmdet.models.dense heads.Center Net Head method ),  414 method ),  311 forward()  ( mmdet.models.losses.DiceLoss method ),  415 forward() ( mmdet.models.dense heads.CornerHead forward() ( mmdet.models.losses.DIoULoss method ), method ),  318 414 forward()  ( mmdet.models.dense heads.Decoupled SOLO Head forward()  ( mmdet.models.losses.Distribution Focal Loss method ),  328 method ),  415 forward()  ( mmdet.models.dense heads.Decoupled SOLO Light Head ( mmdet.models.losses.FocalLoss method ), method ),  329 415 forward()  ( mmdet.models.dense heads.De formable DET RHead forward() ( mmdet.models.losses.Gaussian Focal Loss method ),  329 method ),  417 forward() ( mmdet.models.dense heads.DETRHead forward()  ( mmdet.models.losses.GHMC method ),  416 method ),  323 forward()  ( mmdet.models.losses.GHMR method ),  416 forward() ( mmdet.models.dense heads.FCOSHead forward() ( mmdet.models.losses.GIoULoss method ), method ),  333 417 forward()  ( mmdet.models.dense heads.Feature Adaption forward()  ( mmdet.models.losses.IoULoss method ),  418 method ),  336 forward()  ( mmdet.models.losses.Knowledge Distillation KL Div Loss forward() ( mmdet.models.dense heads.GFLHead method ),  418 method ),  340 forward()  ( mmdet.models.losses.L1Loss method ),  419 forward()  ( mmdet.models.dense heads.Guided Anchor Head forward() ( mmdet.models.losses.MSELoss method ), method ),  343 419 forward()  ( mmdet.models.dense heads.Rep Points Head forward() ( mmdet.models.losses.Quality Focal Loss method ),  354 method ),  420 forward()  ( mmdet.models.dense heads.RetinaS epB N Head forward()  ( mmdet.models.losses.SeesawLoss method ), method ),  357 420 forward()  ( mmdet.models.dense heads.S ABL Retina Head forward() ( mmdet.models.losses.Smooth L 1 Loss method ),  359 method ),  421 forward() ( mmdet.models.dense heads.SOLOHead forward() ( mmdet.models.losses.Var i focal Loss method ),  361 method ),  422 forward() ( mmdet.models.dense heads.SSDHead forward()  ( mmdet.models.necks.BFP method ),  291 method ),  363 forward() ( mmdet.models.necks.Channel Mapper forward()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  292 method ),  364 forward() ( mmdet.models.necks.C TRes Net Neck forward() ( mmdet.models.dense heads.VFNetHead method ),  291 method ),  367 forward() ( mmdet.models.necks.Dilated Encoder forward()  ( mmdet.models.dense heads.YO L ACT Proton et method ),  293 method ),  373 forward()  ( mmdet.models.necks.FPG method ),  294 forward()  ( mmdet.models.dense heads.YO LAC TSe gm Head forward()  ( mmdet.models.necks.FPN method ),  295 method ),  375 forward() ( mmdet.models.necks.FPN_CARAFE forward() ( mmdet.models.dense heads.YOLOV3Head method ),  296 method ),  379 forward()  ( mmdet.models.necks.HRFPN method ),  296 forward() ( mmdet.models.dense heads.YOLOXHead forward() ( mmdet.models.necks.NAS FCO S FP N method ),  382 method ),  297 forward() ( mmdet.models.detectors.Base Detector forward()  ( mmdet.models.necks.NASFPN method ),  297 method ),  259 forward()  ( mmdet.models.necks.PAFPN method ),  298 forward()  ( mmdet.models.losses.Accuracy method ),  412 forward()  ( mmdet.models.necks.RFP method ),  298 forward()  ( mmdet.models.losses.Associative Embedding Loss forward()  ( mmdet.models.necks.SSDNeck method ),  299 method ),  413 forward()  ( mmdet.models.necks.YOLOV3Neck method ), forward() ( mmdet.models.losses.Balanced L 1 Loss 300 method ),  413 forward() ( mmdet.models.necks.YOLOXPAFPN forward() ( mmdet.models.losses.Bounded I oU Loss method ),  300 method ),  413 forward()  ( mmdet.models.roi_heads.Base RoI Extractor forward() ( mmdet.models.losses.CIoULoss method ), method ),  386 414 forward() ( mmdet.models.roi_heads.BBoxHead forward() ( mmdet.models.losses.Cross Entropy Loss method ),  383 ", "page_idx": 454, "bbox": [71, 71.30297088623047, 583.0199584960938, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 4920, "type": "table", "page_idx": 455, "img_path": "layout_images/mmdetection-readthedocs-io-en-v2.18.0_165.jpg", "bbox": [68, 70, 603, 723], "page_size": [612.0, 792.0], "ocr_text": "forward() (mmdet.models.roi_heads.CoarseMaskHead\nmethod), 389\n\nforward () (mmdet.models.roi_heads.ConvF CBBoxHead\nmethod), 390.\n\nforward() (mmdet.models.roi_heads.DIHead method),\n391\n\nforwardQ() = (mmdet.models.utils.SimplifiedBasicBlock\nmethod), 431\n\nforward() (mmdet.models.utils.SinePositionalEncoding\nmethod), 432\n\nforward() (mmdet.models.utils.Transformer method),\n433\n\nforward (Q (mmdet.models.roi_heads. DoubleConvF CBBoxHendard_dummy () (mmdet.models.dense_heads.EmbeddingRPNHead\n\nmethod), 392\n\nforward() (mmdet.models.roi_heads.F CNMaskHead\nmethod), 393\n\nforward() (mmdet.models.roi_heads.FeatureRelayHead\nmethod), 395\n\nmethod), 331\nforward_dummy ()\n\nmethod), 263\nforward_dummy ()\n\nmethod), 266\n\n(mmdet.models.detectors. DETR\n\n(mmdet.models.detectors.RPN\n\nforward () (mmdet.models.roi_heads.FusedSemanticHead forward_dummy () (mmdet.models.detectors.SingleStageDetector\n\nmethod), 396\n\nmethod), 267\n\nforward () (mmdet.models.roi_heads.GenericRolExtractor forward_dummy () (mmdet.models.detectors.SparseRCNN\n\nmethod), 396\n\nmethod), 268\n\nforward () (mmdet.models.roi_heads.GlobalContextHead forward_dummy () (mmdet.models.detectors. TwoStageDetector\n\nmethod), 397\n\nforward() (mmdet.models.roi_heads.GridHead\nmethod), 397\n\nforward() (mmdet.models.roi_heads.HTCMaskHead\nmethod), 397\n\nforward() (mmdet.models.roi_heads.MaskloUHead\nmethod), 399\n\nforward() = (mmdet.models.roi_heads.MaskPointHead\nmethod), 400\n\nforward () (mmdet.models.roi_heads.ResLayer method),\n\n403\n\nforward() (mmdet.models.roi_heads.SABLHead\n\nmethod), 405\n\nforward() (mmdet.models.roi_heads.SCNetBBoxHead\n\nmethod), 406\n\nforward () (mmdet.models.roi_heads.SingleRolExtractor\n\nmethod), 408\n\nforward() (mmdet.models.utils.AdaptiveAvgPool2d\n\nmethod), 425\n\nforward() (mmdet.models.utils. ConvUpsample\n\nmethod), 426\n\nforward() (mmdet.models.utils.CSPLayer method), 426\n\nforward (Q (mmdet.models.utils.DetrTransformerDecoder\nmethod), 427\n\nforward() (mmdet.models.utils. DynamicConv method),\n428\n\nforward() (mmdet.models.utils.InvertedResidual\nmethod), 428\n\nforward (Q (mmdet.models.utils.LearnedPositionalEncoding\n\nmethod), 429\n\nforward() (mmdet.models.utils.NormedConv2d\nmethod), 429\n\nforward() (mmdet.models.utils.NormedLinear method),\n429\n\nforward() (mmdet.models.utils.PatchEmbed method),\n430\n\nforward() (mmdet.models.utils.SELayer method), 431\n\nmethod), 270\nforward_dummy () (mmdet.models.detectors. TwoStagePanopticSegmento\nmethod), 271\nforward_dummy ()\nmethod), 271\nforward_dummy () (mmdet.models.roi_heads.CascadeRolHead\nmethod), 388\nforward_dummy () (mmdet.models.roi_heads.GridRolHead\nmethod), 397\nforward_dummy () (mmdet.models.roi_heads.HybridTaskCascadeRolHe\nmethod), 398\nforward_dummy () (mmdet.models.roi_heads.SparseRolHead\nmethod), 409\nforward_dummy () (mmdet.models.roi_heads.StandardRolHead\nmethod), 410\nforward_onnx() (mmdet.models.dense_heads. DETRHead\nmethod), 324\nforward_single() (mmdet.models.dense_heads.AnchorFreeHead\nmethod), 304\nforward_single() (mmdet.models.dense_heads.AnchorHead\nmethod), 306\nforward_single() (mmdet.models.dense_heads.ATSSHead\nmethod), 301\nforward_single() (mmdet.models.dense_heads.AutoAssignHead\nmethod), 308\nforward_single() (mmdet.models.dense_heads.CenterNetHead\nmethod), 312\nforward_single() (mmdet.models.dense_heads.CentripetalHead\nmethod), 314\nforward_single() (mmdet.models.dense_heads.CornerHead\nmethod), 318\nforward_single() (mmdet.models.dense_heads. DETRHead\nmethod), 324\nforward_single() (mmdet.models.dense_heads.F COSHead\nmethod), 333\nforward_single() (mmdet.models.dense_heads.FoveaHead\nmethod), 337\n\n(mmdet.models.detectors. YOLACT\n", "vlm_text": "The table appears to list various function calls from the MMDetection framework, focusing on methods related to different model components. Each row includes a `forward()` function call with specific model components and methods, often related to region of interest (ROI) heads, utilities, and dense heads in object detection models. The methods are often followed by a page or section number (e.g., 389, 391) indicating their location or reference point within a document or codebase."}
{"layout": 4921, "type": "text", "text": "forward single()  ( mmdet.models.dense heads.FSAFHead method ),  270 method ),  335 forward train()  ( mmdet.models.detectors.Two Stage Pan optic Segment or forward single()  ( mmdet.models.dense heads.GA Retina Head method ),  271 method ),  339 forward train() ( mmdet.models.detectors.YOLACT forward single()  ( mmdet.models.dense heads.GARPNHead method ),  271 method ),  339 forward train()  ( mmdet.models.roi_heads.Base RoI Head forward single()  ( mmdet.models.dense heads.GFLHead method ),  387 method ),  341 forward train()  ( mmdet.models.roi_heads.Cascade RoI Head forward single()  ( mmdet.models.dense heads.Guided Anchor Head method ),  388 method ),  343 forward train()  ( mmdet.models.roi_heads.Dynamic RoI Head forward single()  ( mmdet.models.dense heads.Rep Points Head method ),  393 method ),  354 forward train()  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head forward single()  ( mmdet.models.dense heads.RetinaHead method ),  398 method ),  356 forward train()  ( mmdet.models.roi_heads.PISA RoI Head forward single()  ( mmdet.models.dense heads.RPNHead method ),  402 method ),  353 forward train()  ( mmdet.models.roi_heads.S CNet RoI Head forward single()  ( mmdet.models.dense heads.Stage Cascade RP N Head ),  406 method ),  365 forward train()  ( mmdet.models.roi_heads.Sparse RoI Head forward single()  ( mmdet.models.dense heads.VFNetHead method ),  409 method ),  367 forward train()  ( mmdet.models.roi_heads.Standard RoI Head forward single()  ( mmdet.models.dense heads.YOLACTHead method ),  411 method ),  371 FOVEA  ( class in mmdet.models.detectors ),  263 forward single()  ( mmdet.models.dense heads.YOLOFHead FoveaHead  ( class in mmdet.models.dense heads ),  337 method ),  376 FPG  ( class in mmdet.models.necks ),  293 forward single()  ( mmdet.models.dense heads.YOLOXHead  ( class in mmdet.models.necks ),  294 method ),  382 FPN_CARAFE  ( class in mmdet.models.necks ),  295 forward single on nx() Free Anchor Retina Head ( class in ( mmdet.models.dense heads.DETRHead mmdet.models.dense heads ),  337 method ),  324 FSAF  ( class in mmdet.models.detectors ),  263 forward test()  ( mmdet.models.detectors.Base Detector FSAFHead  ( class in mmdet.models.dense heads ),  334 method ),  259 Fused Semantic Head ( class in forward test() ( mmdet.models.detectors.FastRCNN mmdet.models.roi_heads ),  395 method ),  263 forward train()  ( mmdet.models.dense heads.Cascade RP N Head G method ),  310 ga loc targets()  ( mmdet.models.dense heads.Guided Anchor Head forward train()  ( mmdet.models.dense heads.DETRHead method ),  343 method ),  325 ga shape targets()  ( mmdet.models.dense heads.Guided Anchor Head forward train()  ( mmdet.models.dense heads.Embedding RP N Head method ),  344 method ),  331 GA Retina Head  ( class in mmdet.models.dense heads ), forward train()  ( mmdet.models.dense heads.LDHead 339 method ),  346 GARPNHead  ( class in mmdet.models.dense heads ),  339 forward train()  ( mmdet.models.detectors.Base Detector  ( in module mmdet.models.utils ), gaussian radius() method ),  259 433 forward train()  ( mmdet.models.detectors.Knowledge Distillation Single Stage Detector class in mmdet.models.losses ), method ),  264 417 forward train() ( mmdet.models.detectors.RPN  ( mmdet.core.anchor.Anchor Generator gen base anchors() method ),  266 method ),  184 forward train()  ( mmdet.models.detectors.Single Stage Detector  ( mmdet.core.anchor.YO LO Anchor Generator gen base anchors() method ),  267 method ),  190 forward train()  ( mmdet.models.detectors.SparseRCNN ( in module gen gaussian target() method ),  268 mmdet.models.utils ),  435 forward train()  ( mmdet.models.detectors.Trident Faster R CNN gen grid from reg() method ),  269 ( mmdet.models.dense heads.Rep Points Head forward train()  ( mmdet.models.detectors.Two Stage Detector method ),  355 ", "page_idx": 456, "bbox": [71, 71.30297088623047, 612.0, 720.6494140625], "page_size": [612.0, 792.0]}
{"layout": 4922, "type": "text", "text": "gen single level base anchors() method ),  348 ( mmdet.core.anchor.Anchor Generator method ), get_bboxes()  ( mmdet.models.dense heads.S ABL Retina Head 184 method ),  359 gen single level base anchors() get_bboxes()  ( mmdet.models.dense heads.Stage Cascade RP N Head ( mmdet.core.anchor.Legacy Anchor Generator method ),  365 method ),  187 get_bboxes()  ( mmdet.models.dense heads.YOLACTHead gen single level base anchors() method ),  371 ( mmdet.core.anchor.YO LO Anchor Generator get_bboxes()  ( mmdet.models.dense heads.YOLOV3Head method ),  190 method ),  379 generate coordinate()  ( in module mmdet.core.utils ), get_bboxes()  ( mmdet.models.dense heads.YOLOXHead 225 method ),  382 generate inputs and wrap model() ( in module get_bboxes() ( mmdet.models.roi_heads.BBoxHead mmdet.core.export ),  209 method ),  383 generate reg net()  ( mmdet.models.backbones.RegNet get cat ids()  ( mmdet.datasets.Coco Data set method ), method ),  283 229 Generic RoI Extractor ( class in get cat ids() ( mmdet.datasets.Con cat Data set mmdet.models.roi_heads ),  396 method ),  232 get accuracy() ( mmdet.models.losses.SeesawLoss get cat ids() ( mmdet.datasets.Custom Data set method ),  421 method ),  233 get activation() ( mmdet.models.losses.SeesawLoss get cat ids() ( mmdet.datasets.Repeat Data set method ),  421 method ),  236 get anchors()  ( mmdet.models.dense heads.AnchorHead get cat ids()  ( mmdet.datasets.XMLDataset method ), method ),  306 237 get anchors()  ( mmdet.models.dense heads.Guided Anchor Head get classes()  ( in module mmdet.core.evaluation ),  220 method ),  344 get classes()  ( mmdet.datasets.Custom Data set class get anchors()  ( mmdet.models.dense heads.S ABL Retina Head method ),  233 method ),  359 get cls channels()  ( mmdet.models.losses.SeesawLoss get anchors()  ( mmdet.models.dense heads.VFNetHead method ),  421 method ),  368 get extra property() get ann info() ( mmdet.datasets.Coco Data set ( mmdet.core.bbox.Assign Result method ), method ),  229 192 get ann info()  ( mmdet.datasets.Coco Pan optic Data set get fco s targets()  ( mmdet.models.dense heads.VFNetHead method ),  231 method ),  368 get ann info() ( mmdet.datasets.Custom Data set get gt priorities() method ),  233 ( mmdet.core.bbox.Center Region As signer get ann info() ( mmdet.datasets.XMLDataset method ),  195 method ),  237 get indexes() ( mmdet.datasets.pipelines.MixUp get at s s targets()  ( mmdet.models.dense heads.VFNetHead method ),  246 method ),  368 get indexes() ( mmdet.datasets.pipelines.Mosaic get_bboxes()  ( mmdet.models.dense heads.Cascade RP N Head method ),  247 method ),  310 get k for top k()  ( in module mmdet.core.export ),  210 get_bboxes()  ( mmdet.models.dense heads.Center Net Head get loading pipeline()  ( in module mmdet.datasets ), method ),  312 238 get_bboxes()  ( mmdet.models.dense heads.Centripetal Head get mask scores()  ( mmdet.models.roi_heads.Mask I oU Head method ),  314 method ),  399 get_bboxes()  ( mmdet.models.dense heads.CornerHead get ne g loss single() method ),  319 ( mmdet.models.dense heads.Auto Assign Head get_bboxes()  ( mmdet.models.dense heads.De formable DET RHead method ),  308 method ),  330 get_points()  ( mmdet.models.dense heads.Anchor Free Head get_bboxes()  ( mmdet.models.dense heads.DETRHead method ),  304 method ),  325 get_points()  ( mmdet.models.dense heads.Rep Points Head get_bboxes()  ( mmdet.models.dense heads.Guided Anchor Head method ),  355 method ),  344 get pos loss()  ( mmdet.models.dense heads.PAAHead get_bboxes() ( mmdet.models.dense heads.PAAHead method ),  348 ", "page_idx": 457, "bbox": [71, 71.30297088623047, 587.9913940429688, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 4923, "type": "text", "text": "get pos loss single() get targets()  ( mmdet.models.dense heads.YO L ACT Proton et ( mmdet.models.dense heads.Auto Assign Head method ),  373 method ),  309 get targets()  ( mmdet.models.dense heads.YO LAC TSe gm Head get results()  ( mmdet.models.dense heads.Decoupled SOLO Head method ),  375 method ),  328 get targets()  ( mmdet.models.dense heads.YOLOFHead get results()  ( mmdet.models.dense heads.SOLOHead method ),  376 method ),  361 get targets()  ( mmdet.models.dense heads.YOLOV3Head get roi rel points test() method ),  379 ( mmdet.models.roi_heads.Mask Point Head get targets() ( mmdet.models.roi_heads.BBoxHead method ),  401 method ),  384 get roi rel points train() get targets() ( mmdet.models.roi_heads.DIIHead ( mmdet.models.roi_heads.Mask Point Head method ),  391 method ),  401 get targets()  ( mmdet.models.roi_heads.Mask I oU Head get root logger()  ( in module mmdet.apis ),  181 method ),  399 get sampled approx s() get targets()  ( mmdet.models.roi_heads.Mask Point Head ( mmdet.models.dense heads.Guided Anchor Head method ),  401 method ),  345 GFL  ( class in mmdet.models.detectors ),  264 get seg masks()  ( mmdet.models.dense heads.YO L ACT Proton et  ( class in mmdet.models.dense heads ),  340 method ),  373 GHMC  ( class in mmdet.models.losses ),  416 get seg masks()  ( mmdet.models.roi_heads.FC N Mask Head GHMR  ( class in mmdet.models.losses ),  416 method ),  393 GIoULoss  ( class in mmdet.models.losses ),  417 get stages from blocks() Global Context Head ( class in ( mmdet.models.backbones.RegNet method ), mmdet.models.roi_heads ),  396 283 g mm separation scheme() get_target()  ( mmdet.models.dense heads.S ABL Retina Head ( mmdet.models.dense heads.PAAHead method ),  359 method ),  349 get targets()  ( mmdet.models.dense heads.Anchor Free Head grid anchors()  ( mmdet.core.anchor.Anchor Generator method ),  304 method ),  184 get targets()  ( mmdet.models.dense heads.AnchorHead grid priors() ( mmdet.core.anchor.Anchor Generator method ),  306 method ),  184 get targets()  ( mmdet.models.dense heads.ATSSHead grid priors()  ( mmdet.core.anchor.Ml vl Point Generator method ),  301 method ),  188 get targets()  ( mmdet.models.dense heads.Auto Assign Head GridHead  ( class in mmdet.models.roi_heads ),  397 method ),  309 GridRCNN  ( class in mmdet.models.detectors ),  264 get targets()  ( mmdet.models.dense heads.Center Net Head Grid RoI Head  ( class in mmdet.models.roi_heads ),  397 method ),  312 Group Sampler  ( class in mmdet.datasets ),  234 get targets()  ( mmdet.models.dense heads.CornerHead Group Sampler  ( class in mmdet.datasets.samplers ),  256 method ),  319 gt_inds  ( mmdet.core.bbox.Assign Result attribute ),  191 get targets()  ( mmdet.models.dense heads.DETRHead Guided Anchor Head ( class in method ),  325 mmdet.models.dense heads ),  342 get targets()  ( mmdet.models.dense heads.FCOSHead method ),  333 H get targets()  ( mmdet.models.dense heads.FoveaHead  ( class in mmdet.models.backbones ),  278 Hourglass Net method ),  337 HRFPN  ( class in mmdet.models.necks ),  296 get targets()  ( mmdet.models.dense heads.GFLHead  ( class in mmdet.models.backbones ),  276 HRNet method ),  341 HTC Mask Head  ( class in mmdet.models.roi_heads ),  397 get targets()  ( mmdet.models.dense heads.PAAHead  ( class in mmdet.models.detectors ), Hybrid Task Cascade method ),  349 264 get targets()  ( mmdet.models.dense heads.Rep Points Head ( class in Hybrid Task Cascade RoI Head method ),  355 mmdet.models.roi_heads ),  398 get targets()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  365 I get targets()  ( mmdet.models.dense heads.VFNetHead images to levels()  ( in module mmdet.core.anchor ), method ),  369 191 ", "page_idx": 458, "bbox": [71, 71.30297088623047, 576.1255493164062, 722.8004150390625], "page_size": [612.0, 792.0]}
{"layout": 4924, "type": "text", "text": "Image To Tensor  ( class in mmdet.datasets.pipelines ),  243 in it weights()  ( mmdet.models.dense heads.DETRHead inference detector()  ( in module mmdet.apis ),  181 method ),  326 Infinite Batch Sampler ( class in in it weights()  ( mmdet.models.dense heads.Embedding RP N Head mmdet.datasets.samplers ),  256 method ),  331 Infinite Group Batch Sampler ( class in in it weights()  ( mmdet.models.dense heads.RetinaS epB N Head mmdet.datasets.samplers ),  256 method ),  357 info  ( mmdet.core.bbox.Assign Result property ),  192 in it weights()  ( mmdet.models.dense heads.YOLOFHead info  ( mmdet.core.bbox.Sampling Result property ),  201 method ),  377 in it as signer sampler() in it weights()  ( mmdet.models.dense heads.YOLOV3Head ( mmdet.models.roi_heads.Base RoI Head method ),  380 method ),  387 in it weights()  ( mmdet.models.dense heads.YOLOXHead in it as signer sampler() method ),  382 ( mmdet.models.roi_heads.Cascade RoI Head in it weights() ( mmdet.models.necks.C TRes Net Neck method ),  388 method ),  291 in it as signer sampler() in it weights()  ( mmdet.models.necks.FPN_CARAFE ( mmdet.models.roi_heads.Standard RoI Head method ),  296 method ),  411 in it weights()  ( mmdet.models.necks.NAS FCO S FP N in it b box head()  ( mmdet.models.roi_heads.Base RoI Head method ),  297 method ),  387 in it weights()  ( mmdet.models.necks.RFP method ), in it b box head() (mmdet.models.roi_heads.Cascade RoI Head298method ),  388 in it weights()  ( mmdet.models.roi_heads.Coarse Mask Head in it b box head()  ( mmdet.models.roi_heads.Standard RoI Head method ),  390 method ),  411 in it weights() ( mmdet.models.roi_heads.DIIHead in it detector()  ( in module mmdet.apis ),  181 method ),  391 in it mask head()  ( mmdet.models.roi_heads.Base RoI Head in it weights()  ( mmdet.models.roi_heads.FC N Mask Head method ),  387 method ),  394 in it mask head() (mmdet.models.roi_heads.Cascade RoI Headin it weights()(mmdet.models.utils.Transformermethod ),  389 method ),  433 in it mask head()  ( mmdet.models.roi_heads.S CNet RoI Head InstaBoost  ( class in mmdet.datasets.pipelines ),  243 method ),  407 Instance Balanced Pos Sampler ( class in in it mask head() (mmdet.models.roi_heads.Standard RoI Headmmdet.core.bbox), 197method ),  411 interpolate as()  ( in module mmdet.models.utils ),  435 in it point head()  ( mmdet.models.roi_heads.Point Rend RoI Head Inverted Residual  ( class in mmdet.models.utils ),  428 method ),  402 I oU Balanced Ne g Sampler  ( class in mmdet.core.bbox ), in it weights()  ( mmdet.models.backbones.DetectoRS Res Net 197 method ),  276 IoULoss  ( class in mmdet.models.losses ),  418 in it weights()  ( mmdet.models.backbones.Hourglass Net method ),  279 K in it weights()  ( mmdet.models.backbones.Pyramid Vision Transformer ( class in Knowledge Distillation KL Div Loss method ),  281 mmdet.models.losses ),  418 in it weights() ( mmdet.models.backbones.SSDVGG Knowledge Distillation Single Stage Detector method ),  289 ( class in mmdet.models.detectors ),  264 in it weights()  ( mmdet.models.backbones.S win Transformer method ),  290 L in it weights()  ( mmdet.models.dense heads.Auto Assign Head L1Loss  ( class in mmdet.models.losses ),  419 method ),  310 labels  ( mmdet.core.bbox.Assign Result attribute ),  191 in it weights()  ( mmdet.models.dense heads.Center Net Head LDHead  ( class in mmdet.models.dense heads ),  345 method ),  313 Learned Positional Encoding ( class in in it weights()  ( mmdet.models.dense heads.Centripetal Head mmdet.models.utils ),  428 method ),  315 Legacy Anchor Generator  ( class in mmdet.core.anchor ), in it weights()  ( mmdet.models.dense heads.CornerHead 186 method ),  320 load annotations() ( mmdet.datasets.Coco Data set in it weights()  ( mmdet.models.dense heads.De formable DET RHead method ),  229 method ),  330 ", "page_idx": 459, "bbox": [71, 71.30297088623047, 587.9818115234375, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 4925, "type": "text", "text": "load annotations()  ( mmdet.datasets.Coco Pan optic Data set method ),  231 load annotations()  ( mmdet.datasets.Custom Data set method ),  233 load annotations()  ( mmdet.datasets.LV IS V 05 Data set method ),  235 load annotations()  ( mmdet.datasets.LV IS V 1 Data set method ),  235 load annotations()  ( mmdet.datasets.WIDER Face Data set method ),  237 load annotations() ( mmdet.datasets.XMLDataset method ),  237 load proposals() ( mmdet.datasets.Custom Data set method ),  234 Load Annotations  ( class in mmdet.datasets.pipelines ), 244 Load Image From File ( class in mmdet.datasets.pipelines ),  244 Load Image From Webcam ( class in mmdet.datasets.pipelines ),  245 Load MultiChannel Image From Files ( class in mmdet.datasets.pipelines ),  245 Load Proposals  ( class in mmdet.datasets.pipelines ),  245 loss() ( mmdet.models.dense heads.Anchor Free Head method ),  304 loss() ( mmdet.models.dense heads.AnchorHead method ),  307 loss() ( mmdet.models.dense heads.ATSSHead method ),  301 loss() ( mmdet.models.dense heads.Auto Assign Head method ),  310 loss()  ( mmdet.models.dense heads.Cascade RP N Head method ),  310 loss() ( mmdet.models.dense heads.Center Net Head method ),  313 loss() ( mmdet.models.dense heads.Centripetal Head method ),  315 loss() ( mmdet.models.dense heads.CornerHead method ),  320 loss()  ( mmdet.models.dense heads.Decoupled SOLO Head method ),  328 loss()  ( mmdet.models.dense heads.De formable DET RHead method ),  330 loss() ( mmdet.models.dense heads.DETRHead method ),  326 loss() ( mmdet.models.dense heads.FCOSHead method ),  333 loss() ( mmdet.models.dense heads.FoveaHead method ),  337 loss()  ( mmdet.models.dense heads.Free Anchor Retina Head method ),  338 loss() ( mmdet.models.dense heads.FSAFHead method ),  335 loss() ( mmdet.models.dense heads.GARPNHead ", "page_idx": 460, "bbox": [71, 71.30303192138672, 306, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 4926, "type": "text", "text": "method ),  339 loss()  ( mmdet.models.dense heads.GFLHead method ), 341 loss()  ( mmdet.models.dense heads.Guided Anchor Head method ),  345 loss()  ( mmdet.models.dense heads.LDHead method ), 346 loss()  ( mmdet.models.dense heads.PAAHead method ), 350 loss() ( mmdet.models.dense heads.PISA Retina Head method ),  351 loss() ( mmdet.models.dense heads.PISA SSD Head method ),  352 loss() ( mmdet.models.dense heads.Rep Points Head method ),  355 loss()  ( mmdet.models.dense heads.RPNHead method ), 353 loss() ( mmdet.models.dense heads.S ABL Retina Head method ),  360 loss() ( mmdet.models.dense heads.SOLOHead method ),  362 loss()  ( mmdet.models.dense heads.SSDHead method ), 363 loss()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  365 loss() ( mmdet.models.dense heads.VFNetHead method ),  369 loss() ( mmdet.models.dense heads.YOLACTHead method ),  372 loss() ( mmdet.models.dense heads.YO L ACT Proton et method ),  374 loss()  ( mmdet.models.dense heads.YO LAC TSe gm Head method ),  375 loss() ( mmdet.models.dense heads.YOLOFHead method ),  377 loss() ( mmdet.models.dense heads.YOLOV3Head method ),  380 loss() ( mmdet.models.dense heads.YOLOXHead method ),  382 loss()  ( mmdet.models.roi_heads.DIIHead method ),  391 loss() ( mmdet.models.roi_heads.FC N Mask Head method ),  394 loss() ( mmdet.models.roi_heads.Global Context Head method ),  397 loss() ( mmdet.models.roi_heads.Mask Point Head method ),  401 loss single()  ( mmdet.models.dense heads.AnchorHead method ),  307 loss single()  ( mmdet.models.dense heads.ATSSHead method ),  302 loss single()  ( mmdet.models.dense heads.Centripetal Head method ),  316 loss single()  ( mmdet.models.dense heads.CornerHead method ),  321 ", "page_idx": 460, "bbox": [311, 71.30297088623047, 561, 718.3864135742188], "page_size": [612.0, 792.0]}
{"layout": 4927, "type": "text", "text": "loss single()  ( mmdet.models.dense heads.DETRHead Mask I oU Head  ( class in mmdet.models.roi_heads ),  399 method ),  326 Mask Point Head  ( class in mmdet.models.roi_heads ),  400 loss single()  ( mmdet.models.dense heads.GFLHead MaskRCNN  ( class in mmdet.models.detectors ),  265 method ),  341 Mask Scoring R CNN  ( class in mmdet.models.detectors ), loss single() ( mmdet.models.dense heads.LDHead 265 method ),  347 Mask Scoring RoI Head ( class in loss single()  ( mmdet.models.dense heads.SSDHead mmdet.models.roi_heads ),  402 method ),  363 max overlaps ( mmdet.core.bbox.Assign Result at- loss single()  ( mmdet.models.dense heads.Stage Cascade RP N Head tribute ),  191 method ),  366 MaxI oU As signer  ( class in mmdet.core.bbox ),  197 loss single()  ( mmdet.models.dense heads.YOLOV3Head merge aug b boxes() ( in module method ),  380 mmdet.core.post processing ),  222 loss single OH EM()  ( mmdet.models.dense heads.YOLACTHead merge aug masks() ( in module method ),  372 mmdet.core.post processing ),  223 LV IS Data set  ( in module mmdet.datasets ),  234 merge aug proposals() ( in module LV IS V 05 Data set  ( class in mmdet.datasets ),  234 mmdet.core.post processing ),  223 LV IS V 1 Data set  ( class in mmdet.datasets ),  235 merge aug results() ( mmdet.models.detectors.CenterNet method ), M 262 merge aug results() make con v res block() ( mmdet.models.backbones.Darknet static ( mmdet.models.detectors.CornerNet method ), method ),  275 263  ( in module mmdet.models.utils ),  436 merge aug scores() ( in module make divisible() make_layer()  ( mmdet.models.backbones.Mobile Ne tV 2 mmdet.core.post processing ),  223 method ),  279 merge trident b boxes() make res layer()  ( mmdet.models.backbones.DetectoRS Res Net ( mmdet.models.roi_heads.Trident RoI Head method ),  276 method ),  412  ( mmdet.models.backbones.DetectoRS Res NeXt MinI oU Random Crop  ( class in mmdet.datasets.pipelines ), make res layer() method ),  276 245  ( mmdet.models.backbones.Res2Net MixUp  ( class in mmdet.datasets.pipelines ),  245 make res layer() method ),  284 Ml vl Point Generator  ( class in mmdet.core.anchor ), make res layer()  ( mmdet.models.backbones.ResNeSt 188 method ),  285 mmdet.apis ( mmdet.models.backbones.ResNet module ,  181 make res layer() method ),  287 mmdet.core.anchor  ( mmdet.models.backbones.ResNeXt module ,  183 make res layer() method ),  285 mmdet.core.bbox module ,  191 make stage plugins() ( mmdet.models.backbones.ResNet method ), mmdet.core.evaluation 287 module ,  219  ( mmdet.models.roi_heads.Single RoI Extractor mmdet.core.export map roi levels() method ),  408 module ,  208  ( mmdet.datasets.pipelines.Albu static method ), mmdet.core.mask mapper() 240module, 211 ( in module mmdet.core.utils ),  225 mmdet.core.post processing mask 2 nd array()( in module module ,  221 mask cross entropy() mmdet.models.losses ),  423 mmdet.core.utils ( in module module ,  224 mask matrix nm s() mmdet.core.post processing ),  222 mmdet.datasets  ( mmdet.models.roi_heads.Point Rend RoI Head module ,  227 mask on nx export()method ),  403 mmdet.datasets.api wrappers  ( mmdet.models.roi_heads.Standard RoI Head module ,  257 mask on nx export()method ),  411 mmdet.datasets.pipelines  ( in module mmdet.core.mask ),  218 module ,  239 mask target() ", "page_idx": 461, "bbox": [71, 71.30303192138672, 540, 720.6495361328125], "page_size": [612.0, 792.0]}
{"layout": 4928, "type": "text", "text": "mmdet.datasets.samplers module ,  256 mmdet.models.backbones module ,  273 mmdet.models.dense heads module ,  301 mmdet.models.detectors module ,  259 mmdet.models.losses module ,  412 mmdet.models.necks module ,  291 mmdet.models.roi_heads module ,  383 mmdet.models.utils module ,  425 Mobile Ne tV 2  ( class in mmdet.models.backbones ),  279 module mmdet.apis ,  181 mmdet.core.anchor ,  183 mmdet.core.bbox ,  191 mmdet.core.evaluation ,  219 mmdet.core.export ,  208 mmdet.core.mask ,  211 mmdet.core.post processing ,  221 mmdet.core.utils, 224mmdet.datasets ,  227 mmdet.datasets.api wrappers ,  257 mmdet.datasets.pipelines ,  239 mmdet.datasets.samplers ,  256 mmdet.models.backbones ,  273 mmdet.models.dense heads ,  301 mmdet.models.detectors ,  259 mmdet.models.losses ,  412 mmdet.models.necks ,  291 mmdet.models.roi_heads ,  383 mmdet.models.utils ,  425 Mosaic  ( class in mmdet.datasets.pipelines ),  246 mse_loss()  ( in module mmdet.models.losses ),  424 MSELoss  ( class in mmdet.models.losses ),  419 multi apply()  ( in module mmdet.core.utils ),  226 multi gpu test()  ( in module mmdet.apis ),  181 multi class nm s() ( in module mmdet.core.post processing ),  223 Multi Image Mix Data set  ( class in mmdet.datasets ),  235 Multi Scale Flip Aug ( class in mmdet.datasets.pipelines ),  247 ", "page_idx": 462, "bbox": [71, 73.77375793457031, 301, 634.7005004882812], "page_size": [612.0, 792.0]}
{"layout": 4929, "type": "text", "text": "N ", "text_level": 1, "page_idx": 462, "bbox": [71, 643, 83, 656], "page_size": [612.0, 792.0]}
{"layout": 4930, "type": "text", "text": "NASFCOS  ( class in mmdet.models.detectors ),  265 NAS FCO S FP N  ( class in mmdet.models.necks ),  296 NAS FCO S Head  ( class in mmdet.models.dense heads ),  347 NASFPN  ( class in mmdet.models.necks ),  297 nch w to nl c()  ( in module mmdet.models.utils ),  436 ", "page_idx": 462, "bbox": [71, 659.6530151367188, 301, 720.9334716796875], "page_size": [612.0, 792.0]}
{"layout": 4931, "type": "text", "text": "negative bag loss() ( mmdet.models.dense heads.Free Anchor Retina Head method ),  338 nl c to nch w()  ( in module mmdet.models.utils ),  436 norm1  ( mmdet.models.backbones.HRNet property ),  278 norm1  ( mmdet.models.backbones.ResNet property ),  288 norm1  ( mmdet.models.utils.Simplified Basic Block prop- erty ),  432 norm2  ( mmdet.models.backbones.HRNet property ),  278 norm2  ( mmdet.models.utils.Simplified Basic Block prop- erty ),  432 Normalize  ( class in mmdet.datasets.pipelines ),  248 Norm edC on v 2 d  ( class in mmdet.models.utils ),  429 Norm ed Linear  ( class in mmdet.models.utils ),  429 num anchors ( mmdet.models.dense heads.SSDHead property ),  364 num anchors ( mmdet.models.dense heads.VFNetHead property ),  370 num anchors  ( mmdet.models.dense heads.YOLOV3Head property ),  380 num_attrib  ( mmdet.models.dense heads.YOLOV3Head property ),  380 num base anchors  ( mmdet.core.anchor.Anchor Generator property ),  185 num base priors  ( mmdet.core.anchor.Anchor Generator property ),  185 num base priors  ( mmdet.core.anchor.Ml vl Point Generator property ),  188 num_gts  ( mmdet.core.bbox.Assign Result attribute ),  191 num_inputs  ( mmdet.models.roi_heads.Base RoI Extractor property ),  387 num_levels ( mmdet.core.anchor.Anchor Generator property ),  185 num_levels ( mmdet.core.anchor.Ml vl Point Generator property ),  188 num_levels  ( mmdet.core.anchor.YO LO Anchor Generator property ),  190 num_preds ( mmdet.core.bbox.Assign Result property ), 192 ", "page_idx": 462, "bbox": [310, 73.77369689941406, 561.5006713867188, 539.0593872070312], "page_size": [612.0, 792.0]}
{"layout": 4932, "type": "text", "text": "", "text_level": 1, "page_idx": 462, "bbox": [310, 546, 322, 558], "page_size": [612.0, 792.0]}
{"layout": 4933, "type": "text", "text": "offset to pts()  ( mmdet.models.dense heads.Rep Points Head method ),  356 OH EM Sampler (class in mmdet.core.bbox), 199on nx export()  ( mmdet.models.dense heads.CornerHead method ),  322 on nx export()  ( mmdet.models.dense heads.DETRHead method ),  327 on nx export()  ( mmdet.models.dense heads.RPNHead method ),  353 on nx export()  ( mmdet.models.dense heads.YOLOV3Head method ),  380 on nx export() ( mmdet.models.detectors.DETR method ),  263 ", "page_idx": 462, "bbox": [310, 562.306884765625, 569.1218872070312, 719.2283935546875], "page_size": [612.0, 792.0]}
{"layout": 4934, "type": "text", "text": "on nx export()  ( mmdet.models.detectors.Single Stage Detector print recall summary() ( in module method ),  268 mmdet.core.evaluation ),  221 on nx export() ( mmdet.models.detectors.YOLOV3 process polygons()  ( mmdet.datasets.pipelines.Load Annotations method ),  272 method ),  244 on nx export() ( mmdet.models.roi_heads.BBoxHead Pseudo B Box Code r  ( class in mmdet.core.bbox ),  199 method ),  384 Pseudo Sampler  ( class in mmdet.core.bbox ),  199 on nx export()  ( mmdet.models.roi_heads.FC N Mask Head Pyramid Vision Transformer ( class in method ),  395 mmdet.models.backbones ),  280 on nx export()  ( mmdet.models.roi_heads.Standard RoI Head Pyramid Vision Transformer V 2 ( class in method ),  411 mmdet.models.backbones ),  281 ", "page_idx": 463, "bbox": [71, 71.30290985107422, 580.9675903320312, 192.35948181152344], "page_size": [612.0, 792.0]}
{"layout": 4935, "type": "text", "text": "Q ", "text_level": 1, "page_idx": 463, "bbox": [310, 200, 323, 214], "page_size": [612.0, 792.0]}
{"layout": 4936, "type": "text", "text": "", "text_level": 1, "page_idx": 463, "bbox": [71, 200, 81, 213], "page_size": [612.0, 792.0]}
{"layout": 4937, "type": "text", "text": "PAA  ( class in mmdet.models.detectors ),  265 paa reassign()  ( mmdet.models.dense heads.PAAHead method ),  350 PAAHead  ( class in mmdet.models.dense heads ),  347 Pad  ( class in mmdet.datasets.pipelines ),  248 pad()  ( mmdet.core.mask.Base Instance Masks method ), 211 pad()  ( mmdet.core.mask.Bitmap Masks method ),  214 pad()  ( mmdet.core.mask.Polygon Masks method ),  216 PAFPN  ( class in mmdet.models.necks ),  297 Pan optic FP N  ( class in mmdet.models.detectors ),  265 PatchEmbed  ( class in mmdet.models.utils ),  430 PhotoMetric Distortion ( class in mmdet.datasets.pipelines ),  248 PISA Retina Head  ( class in mmdet.models.dense heads ), 351 PISA RoI Head  ( class in mmdet.models.roi_heads ),  402 PISA SSD Head  ( class in mmdet.models.dense heads ),  352 plot i ou recall()(inmodulemmdet.core.evaluation ),  220 plot num recall() ( in module mmdet.core.evaluation ),  221 PointRend  ( class in mmdet.models.detectors ),  265 Point Rend RoI Head  ( class in mmdet.models.roi_heads ), 402 points 2 b box()  ( mmdet.models.dense heads.Rep Points Head method ),  356 Polygon Masks  ( class in mmdet.core.mask ),  215 positive bag loss() ( mmdet.models.dense heads.Free Anchor Retina Head method ),  339 pre pipeline() ( mmdet.datasets.Custom Data set method ),  234 prepare test img()  ( mmdet.datasets.Custom Data set method ),  234 prepare train img() ( mmdet.datasets.Custom Data set method ), 234 pre process example input() ( in module mmdet.core.export ),  210 print map summary() ( in module mmdet.core.evaluation ),  221 ", "page_idx": 463, "bbox": [71, 217.0271453857422, 322, 720.6494140625], "page_size": [612.0, 792.0]}
{"layout": 4938, "type": "text", "text": "Quality Focal Loss  ( class in mmdet.models.losses ),  419 quant ize float() ( mmdet.models.backbones.RegNet static method ),  283 QueryInst  ( class in mmdet.models.detectors ),  265 ", "page_idx": 463, "bbox": [310, 216.9719696044922, 540, 266.2965087890625], "page_size": [612.0, 792.0]}
{"layout": 4939, "type": "text", "text": "", "text_level": 1, "page_idx": 463, "bbox": [310, 274, 322, 285.75], "page_size": [612.0, 792.0]}
{"layout": 4940, "type": "text", "text": "random()  ( mmdet.core.bbox.Assign Result class method ), 192 random() ( mmdet.core.bbox.Sampling Result class method ),  201 random() ( mmdet.core.mask.Bitmap Masks class method ),  214 random() ( mmdet.core.mask.Polygon Masks class method ),  217 random choice() ( mmdet.core.bbox.Random Sampler method ),  200 random choice()  ( mmdet.core.bbox.Score HL R Sampler static method ),  202 random sample() ( mmdet.datasets.pipelines.Resize static method ),  253 random sample ratio()( mmdet.datasets.pipelines.Resize static method ),  253 random select() ( mmdet.datasets.pipelines.Resize static method ),  253 Random Affine  ( class in mmdet.datasets.pipelines ),  249 Random Center Crop Pad ( class in mmdet.datasets.pipelines ),  249 RandomCrop  ( class in mmdet.datasets.pipelines ),  251 RandomFlip  ( class in mmdet.datasets.pipelines ),  251 Random Sampler (class in mmdet.core.bbox), 199Random Shift  ( class in mmdet.datasets.pipelines ),  252 reduce loss()  ( in module mmdet.models.losses ),  424 reduce mean()  ( in module mmdet.core.utils ),  226 refine b boxes()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  366 refine b boxes()  ( mmdet.models.roi_heads.BBoxHead method ),  385 refine b boxes()  ( mmdet.models.roi_heads.SABLHead method ),  405 reg_pred() ( mmdet.models.roi_heads.SABLHead method ),  405 ", "page_idx": 463, "bbox": [310, 290.9100341796875, 599, 722.80029296875], "page_size": [612.0, 792.0]}
{"layout": 4941, "type": "text", "text": "region targets()  ( method ),  366 Region As signer  ( class in mmdet.core.bbox ),  200 RegNet  ( class in mmdet.models.backbones ),  281 regress by class()  ( mmdet.models.roi_heads.BBoxHead method ),  386 regress by class()  ( mmdet.models.roi_heads.SABLHead method ),  405 Repeat Data set  ( class in mmdet.datasets ),  236 replace Image To Tensor() ( in module mmdet.datasets ),  238 Rep Points Detector  ( class in mmdet.models.detectors ), 267 Rep Points Head  ( class in mmdet.models.dense heads ), 353 Res2Net  ( class in mmdet.models.backbones ),  283 rescale() ( mmdet.core.mask.Base Instance Masks method ),  212 rescale() ( mmdet.core.mask.Bitmap Masks method ), 214 rescale()  ( mmdet.core.mask.Polygon Masks method ), 217 Resize  ( class in mmdet.datasets.pipelines ),  252 resize() ( mmdet.core.mask.Base Instance Masks method ),  212 resize()  ( mmdet.core.mask.Bitmap Masks method ),  214 resize() ( mmdet.core.mask.Polygon Masks method ), 217 resize feats()  ( mmdet.models.dense heads.SOLOHead method ),  362 ResLayer  ( class in mmdet.models.roi_heads ),  403 ResLayer  ( class in mmdet.models.utils ),  430 ResNeSt  ( class in mmdet.models.backbones ),  284 ResNet  ( class in mmdet.models.backbones ),  285 ResNetV1d  ( class in mmdet.models.backbones ),  288 ResNeXt  ( class in mmdet.models.backbones ),  285 responsible flags() ( mmdet.core.anchor.YO LO Anchor Generator method ),  190 results 2 json() ( mmdet.datasets.Coco Data set method ),  230 results 2 json()  ( mmdet.datasets.Coco Pan optic Data set method ),  231 results 2 txt() ( mmdet.datasets.Cityscape s Data set method ),  228 RetinaHead  ( class in mmdet.models.dense heads ),  356 RetinaNet  ( class in mmdet.models.detectors ),  267 RetinaS epB N Head ( class in mmdet.models.dense heads ),  357 re weight loss single() ( mmdet.models.dense heads.FSAFHead method ),  336 RFP  ( class in mmdet.models.necks ),  298 rfp forward()  ( mmdet.models.backbones.DetectoRS Res Net roi2bbox()  ( in module mmdet.core.bbox ),  208 roi re scale()  ( mmdet.models.roi_heads.Base RoI Extractor method ),  387 Rotate  ( class in mmdet.datasets.pipelines ),  254 rotate() ( mmdet.core.mask.Base Instance Masks method ),  212 rotate()  ( mmdet.core.mask.Bitmap Masks method ),  214 rotate() ( mmdet.core.mask.Polygon Masks method ), 217 RPN  ( class in mmdet.models.detectors ),  265 RPNHead  ( class in mmdet.models.dense heads ),  352 ", "page_idx": 464, "bbox": [71, 71.45246887207031, 308, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 4942, "type": "text", "text": "", "page_idx": 464, "bbox": [310, 83.25798797607422, 559, 216.2695770263672], "page_size": [612.0, 792.0]}
{"layout": 4943, "type": "text", "text": "SABLHead  ( class in mmdet.models.roi_heads ),  403 S ABL Retina Head  ( class in mmdet.models.dense heads ), 357 sample()  ( mmdet.core.bbox.Base Sampler method ),  193 sample()  ( mmdet.core.bbox.Pseudo Sampler method ), 199 sample()  ( mmdet.core.bbox.Score HL R Sampler method ), 203 sample via interval() ( mmdet.core.bbox.I oU Balanced Ne g Sampler method ),  197 Sampling Result  ( class in mmdet.core.bbox ),  201 sanitize coordinates() ( mmdet.models.dense heads.YO L ACT Proton et method ),  374 SCNet  ( class in mmdet.models.detectors ),  267 S CNet B Box Head  ( class in mmdet.models.roi_heads ),  405 S CNet Mask Head  ( class in mmdet.models.roi_heads ),  406 S CNet RoI Head  ( class in mmdet.models.roi_heads ),  406 S CNet Semantic Head ( class in mmdet.models.roi_heads ),  407 score voting()  ( mmdet.models.dense heads.PAAHead method ),  351 Score HL R Sampler  ( class in mmdet.core.bbox ),  202 SeesawLoss  ( class in mmdet.models.losses ),  420 SegRescale  ( class in mmdet.datasets.pipelines ),  254 SELayer  ( class in mmdet.models.utils ),  431 select single ml vl()  ( in module mmdet.core.utils ), 226 set_epoch()  ( mmdet.datasets.samplers.Infinite Batch Sampler method ),  256 set_epoch() (mmdet.datasets.samplers.Infinite Group Batch Samplermethod ),  257 set extra property()( mmdet.core.bbox.Assign Result method ), 193 set random seed() (in module mmdet.apis), 182Shared 2 FCB Box Head ( class in mmdet.models.roi_heads ),  408 ", "page_idx": 464, "bbox": [310, 240.93809509277344, 584, 708.6943969726562], "page_size": [612.0, 792.0]}
{"layout": 4944, "type": "text", "text": "Shared 4 Con v 1 FCB Box Head ( class in method ),  411 mmdet.models.roi_heads ),  408 simple test()  ( mmdet.models.roi_heads.Trident RoI Head Shear  ( class in mmdet.datasets.pipelines ),  254 method ),  412 shear() ( mmdet.core.mask.Base Instance Masks simple test b boxes() method ),  212 ( mmdet.models.dense heads.DETRHead shear()  ( mmdet.core.mask.Bitmap Masks method ),  214 method ),  327 shear()  ( mmdet.core.mask.Polygon Masks method ),  217 simple test mask()  ( mmdet.models.detectors.Two Stage Pan opticS eg men show result()  ( mmdet.models.detectors.Base Detector method ),  271 method ),  260 simple test mask()  ( mmdet.models.roi_heads.Mask Scoring RoI Head show result()  ( mmdet.models.detectors.Cascade R CNN method ),  402 method ),  261 simple test mask()  ( mmdet.models.roi_heads.Point Rend RoI Head show result()  ( mmdet.models.detectors.RPN method ), method ),  403 266 simple test rp n()  ( mmdet.models.dense heads.Cascade RP N Head show result py plot()  ( in module mmdet.apis ),  182 method ),  310 side aware feature extractor() simple test rp n()  ( mmdet.models.dense heads.Embedding RP N Head ( mmdet.models.roi_heads.SABLHead method ), method ),  331 405 Simplified Basic Block  ( class in mmdet.models.utils ), side aware split()  ( mmdet.models.roi_heads.SABLHead 431 method ),  405 Sine Positional Encoding ( class in s igm oid focal loss() ( in module mmdet.models.utils ),  432 mmdet.models.losses ),  424 single level grid anchors() simple test()  ( mmdet.models.dense heads.Embedding RP N Head ( mmdet.core.anchor.Anchor Generator method ), method ),  331 185 simple test()  ( mmdet.models.dense heads.YO L ACT Proton et single level grid priors() method ),  374 ( mmdet.core.anchor.Anchor Generator method ), simple test()  ( mmdet.models.dense heads.YO LAC TSe gm Head 185 method ),  376 single level grid priors() simple test()  ( mmdet.models.detectors.RPN method ), ( mmdet.core.anchor.Ml vl Point Generator 266 method ),  188 simple test()  ( mmdet.models.detectors.Single Stage Detector single level responsible flags() method ),  268 ( mmdet.core.anchor.YO LO Anchor Generator simple test()  ( mmdet.models.detectors.SparseRCNN method ),  190 method ),  269 single level valid flags() simple test()  ( mmdet.models.detectors.Trident Faster R CNN ( mmdet.core.anchor.Anchor Generator method ), method ),  269 186 simple test()  ( mmdet.models.detectors.Two Stage Detector single level valid flags() method ),  270 ( mmdet.core.anchor.Ml vl Point Generator simple test()  ( mmdet.models.detectors.Two Stage Pan optic Segment or method ),  189 method ),  271 Single RoI Extractor ( class in simple test() ( mmdet.models.detectors.YOLACT mmdet.models.roi_heads ),  408 method ),  272 Single Stage Detector ( class in simple test()  ( mmdet.models.roi_heads.Base RoI Head mmdet.models.detectors ),  267 method ),  387 slice_as() ( mmdet.models.necks.FPN_CARAFE simple test()  ( mmdet.models.roi_heads.Cascade RoI Head method ),  296 method ),  389 Smooth L 1 Loss  ( class in mmdet.models.losses ),  421 simple test()  ( mmdet.models.roi_heads.Grid RoI Head SOLO  ( class in mmdet.models.detectors ),  267 method ),  397 SOLOHead  ( class in mmdet.models.dense heads ),  360 simple test()  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head  ( mmdet.core.anchor.Anchor Generator method ),  398 method ),  186 simple test()  ( mmdet.models.roi_heads.S CNet RoI Head sparse priors()  ( mmdet.core.anchor.Ml vl Point Generator method ),  407 method ),  189 simple test()  ( mmdet.models.roi_heads.Sparse RoI Head SparseRCNN  ( class in mmdet.models.detectors ),  268 method ),  410 Sparse RoI Head  ( class in mmdet.models.roi_heads ),  408 simple test()  ( mmdet.models.roi_heads.Standard RoI Head split combined poly s() ( in module ", "page_idx": 465, "bbox": [71, 71.30297088623047, 611.96142578125, 718.386474609375], "page_size": [612.0, 792.0]}
{"layout": 4945, "type": "text", "text": "", "page_idx": 466, "bbox": [109, 73, 206, 79.75], "page_size": [612.0, 792.0]}
{"layout": 4946, "type": "text", "text": "SSDHead  ( class in mmdet.models.dense heads ),  362 SSDNeck  ( class in mmdet.models.necks ),  298 SSDVGG  ( class in mmdet.models.backbones ),  288 Stage Cascade RP N Head ( class in mmdet.models.dense heads ),  364 Standard RoI Head  ( class in mmdet.models.roi_heads ), 410 star dc n offset()  ( mmdet.models.dense heads.VFNetHead method ),  370 S win Transformer  ( class in mmdet.models.backbones ), 289 ", "page_idx": 466, "bbox": [70, 80.25, 303, 216], "page_size": [612.0, 792.0]}
{"layout": 4947, "type": "text", "text": "", "text_level": 1, "page_idx": 466, "bbox": [71, 223, 80, 235.75], "page_size": [612.0, 792.0]}
{"layout": 4948, "type": "text", "text": "T BL RB Box Code r  ( class in mmdet.core.bbox ),  203 tensor_add() ( mmdet.models.necks.FPN_CARAFE method ),  296 to()  ( mmdet.core.bbox.Sampling Result method ),  202 to_bitmap() ( mmdet.core.mask.Polygon Masks method ),  217 to_ndarray() ( mmdet.core.mask.Base Instance Masks method ),  212 to_ndarray() ( mmdet.core.mask.Bitmap Masks method ),  215 to_ndarray() ( mmdet.core.mask.Polygon Masks method ),  217 to_tensor()  ( in module mmdet.datasets.pipelines ),  255 to_tensor() ( mmdet.core.mask.Base Instance Masks method ),  213 to_tensor()  ( mmdet.core.mask.Bitmap Masks method ), 215 to_tensor() ( mmdet.core.mask.Polygon Masks method ),  217 To Data Container  ( class in mmdet.datasets.pipelines ), 255 ToTensor  ( class in mmdet.datasets.pipelines ),  255 train() ( mmdet.models.backbones.CSPDarknet method ),  274 train() ( mmdet.models.backbones.Darknet method ), 275 train()  ( mmdet.models.backbones.HRNet method ),  278 train() ( mmdet.models.backbones.Mobile Ne tV 2 method ),  280 train()  ( mmdet.models.backbones.ResNet method ),  288 train() ( mmdet.models.backbones.S win Transformer method ),  290 train()  ( mmdet.models.detectors.Knowledge Distillation Single Stage Detector method ),  265 train()  ( mmdet.models.roi_heads.ResLayer method ), 403 train_step() ( mmdet.models.detectors.Base Detector method ),  260 transform b box targets() ( mmdet.models.dense heads.VFNetHead ", "page_idx": 466, "bbox": [71, 236.25, 304, 721], "page_size": [612.0, 792.0]}
{"layout": 4949, "type": "text", "text": "method ),  370 Transformer  ( class in mmdet.models.utils ),  432 Translate  ( class in mmdet.datasets.pipelines ),  255 translate() ( mmdet.core.mask.Base Instance Masks method ),  213 translate()  ( mmdet.core.mask.Bitmap Masks method ), 215 translate() ( mmdet.core.mask.Polygon Masks method ),  217 Transpose  ( class in mmdet.datasets.pipelines ),  255 Trident Faster R CNN  ( class in mmdet.models.detectors ), 269 Trident Res Net  ( class in mmdet.models.backbones ),  290 Trident RoI Head  ( class in mmdet.models.roi_heads ), 412 Two Stage Detector  ( class in mmdet.models.detectors ), 269 Two Stage Pan optic Segment or ( class in mmdet.models.detectors ),  270 ", "page_idx": 466, "bbox": [310, 71.30290985107422, 542, 299.95556640625], "page_size": [612.0, 792.0]}
{"layout": 4950, "type": "text", "text": "", "text_level": 1, "page_idx": 466, "bbox": [311, 308, 321, 320], "page_size": [612.0, 792.0]}
{"layout": 4951, "type": "text", "text": "unmap()  ( in module mmdet.core.utils ),  226 update dynamic scale() ( mmdet.datasets.Multi Image Mix Data set method ),  236 update hyper parameters() ( mmdet.models.roi_heads.Dynamic RoI Head method ),  393 update skip type keys()( mmdet.datasets.Multi Image Mix Data set method ),  236 ", "page_idx": 466, "bbox": [310, 323.8330993652344, 530, 444.8884582519531], "page_size": [612.0, 792.0]}
{"layout": 4952, "type": "text", "text": "", "text_level": 1, "page_idx": 466, "bbox": [310, 453, 320, 465], "page_size": [612.0, 792.0]}
{"layout": 4953, "type": "text", "text": "val_step() ( mmdet.models.detectors.Base Detector method ),  261 valid flags() ( mmdet.core.anchor.Anchor Generator method ),  186 valid flags()  ( mmdet.core.anchor.Ml vl Point Generator method ),  189 Var i focal Loss  ( class in mmdet.models.losses ),  422 VFNet  ( class in mmdet.models.detectors ),  271 VFNetHead  ( class in mmdet.models.dense heads ),  366 VOCDataset  ( class in mmdet.datasets ),  236 ", "page_idx": 466, "bbox": [310, 468.7659912109375, 542, 589.8214111328125], "page_size": [612.0, 792.0]}
{"layout": 4954, "type": "text", "text": "weighted loss()  ( in module mmdet.models.losses ), WIDER Face Data set  ( class in mmdet.datasets ),  237 with_bbox  ( mmdet.models.detectors.Base Detector prop- erty ),  261 with_bbox ( mmdet.models.roi_heads.Base RoI Head property ),  387 with feat relay  ( mmdet.models.roi_heads.S CNet RoI Head property ),  407 ", "page_idx": 466, "bbox": [310, 613.6989135742188, 556.9577026367188, 722.8004150390625], "page_size": [612.0, 792.0]}
{"layout": 4955, "type": "text", "text": "with gl bc tx  ( mmdet.models.roi_heads.S CNet RoI Head property ),  407 with_mask  ( mmdet.models.detectors.Base Detector prop- erty ),  261 with_mask ( mmdet.models.roi_heads.Base RoI Head property ),  387 with_neck  ( mmdet.models.detectors.Base Detector prop- erty ),  261 with roi head  ( mmdet.models.detectors.Two Stage Detector property ),  270 with_rpn ( mmdet.models.detectors.Two Stage Detector property ),  270 with semantic  ( mmdet.models.detectors.Hybrid Task Cascade property ),  264 with semantic  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head property ),  399 with semantic  ( mmdet.models.roi_heads.S CNet RoI Head property ),  407 with shared head  ( mmdet.models.detectors.Base Detector property ),  261 with shared head  ( mmdet.models.roi_heads.Base RoI Head property ),  388 ", "page_idx": 467, "bbox": [71, 71.30303192138672, 363.0572204589844, 335.8206787109375], "page_size": [612.0, 792.0]}
{"layout": 4956, "type": "text", "text": "X ", "text_level": 1, "page_idx": 467, "bbox": [71, 344, 82, 357], "page_size": [612.0, 792.0]}
{"layout": 4957, "type": "text", "text": "XMLDataset  ( class in mmdet.datasets ),  237 xyxy2xywh() ( mmdet.datasets.Coco Data set method ), 230 ", "page_idx": 467, "bbox": [71, 360.7742004394531, 301, 398.1436462402344], "page_size": [612.0, 792.0]}
{"layout": 4958, "type": "text", "text": "", "text_level": 1, "page_idx": 467, "bbox": [72, 405, 82, 415.75], "page_size": [612.0, 792.0]}
{"layout": 4959, "type": "text", "text": "YOLACT  ( class in mmdet.models.detectors ),  271 YOLACTHead  ( class in mmdet.models.dense heads ),  370 YO L ACT Proton et  ( class in mmdet.models.dense heads ), 372 YO LAC TSe gm Head  ( class in mmdet.models.dense heads ), 375 YO LO Anchor Generator  ( class in mmdet.core.anchor ), 189 YOLOF  ( class in mmdet.models.detectors ),  272 YOLOFHead  ( class in mmdet.models.dense heads ),  376 YOLOV3  ( class in mmdet.models.detectors ),  272 YOLOV3Head  ( class in mmdet.models.dense heads ),  377 YOLOV3Neck  ( class in mmdet.models.necks ),  299 YOLOX  ( class in mmdet.models.detectors ),  272 YOLOXHead  ( class in mmdet.models.dense heads ),  381 YOLOXPAFPN  ( class in mmdet.models.necks ),  300 ", "page_idx": 467, "bbox": [71, 421.39117431640625, 301, 614.1785278320312], "page_size": [612.0, 792.0]}
