{"page": 0, "image_path": "doc_images/N18-1012_0.jpg", "ocr_text": "Dear Sir or Madam, May I Introduce the GYAFC Dataset:\nCorpus, Benchmarks and Metrics for Formality Style Transfer\n\nSudha Rao\nUniversity of Maryland, College Park*\nraosudha@cs.umd.edu\n\nAbstract\n\nStyle transfer is the task of automatically trans-\nforming a piece of text in one particular style\ninto another. A major barrier to progress in\nthis field has been a lack of training and eval-\nuation datasets, as well as benchmarks and au-\ntomatic metrics. In this work, we create the\nlargest corpus for a particular stylistic trans-\nfer (formality) and show that techniques from\nthe machine translation community can serve\nas strong baselines for future work. We also\ndiscuss challenges of using automatic metrics.\n\n1 Introduction\n\nOne key aspect of effective communication is the\naccurate expression of the style or tone of some\ncontent. For example, writing a more persuasive\nemail in a marketing position could lead to in-\ncreased sales; writing a more formal email when\napplying for a job could lead to an offer; and writ-\ning a more polite note to your future spouse’s par-\nents, may put you in a good light. Hovy (1987)\nargues that by varying the style of a text, people\nconvey more information than is present in the lit-\neral meaning of the words. One particularly im-\nportant dimension of style is formality (Heylighen\nand Dewaele, 1999). Automatically changing the\nstyle of a given content to make it more formal can\nbe a useful addition to any writing assistance tool.\n\nIn the field of style transfer, to date, the only\navailable dataset has been for the transformation\nof modern English to Shakespeare, and it led to\nthe application of phrase-based machine transla-\ntion (PBMT) (Xu et al., 2012) and neural machine\ntranslation (NMT) (Jhamtani et al., 2017) models\nto the task. The lack of an equivalent or larger\ndataset for any other form of style transfer has\nblocked progress in this field. Moreover, prior\n\nThis research was performed when the first author was at\nGrammarly.\n\nJoel Tetreault\nGrammarly\njoel.tetreault@grammarly.com\n\nwork has mainly borrowed metrics from machine\ntranslation (MT) and paraphrase communities for\nevaluating style transfer. However, it is not clear if\nthose metrics are the best ones to use for this task.\nIn this work, we address these issues through the\nfollowing three contributions:\n\ne Corpus: We present Grammarly’s Yahoo\nAnswers Formality Corpus (GYAFC), the\nlargest dataset for any style containing a to-\ntal of 110K informal / formal sentence pairs.\nTable 1 shows sample sentence pairs.\n\ne Benchmarks: We introduce a set of learning\nmodels for the task of formality style trans-\nfer. Inspired by work in low resource MT, we\nadapt existing PBMT and NMT approaches\nfor our task and show that they can serve as\nstrong benchmarks for future work.\n\ne Metrics: In addition to MT and paraphrase\nmetrics, we evaluate our models along three\naxes: formality, fluency and meaning preser-\nvation using existing automatic metrics. We\ncompare these metrics with their human\njudgments and show there is much room for\nfurther improvement.\n\nInformal: /’d say it is punk though.\n\nFormal: However, I do believe it to be punk.\nInformal: Gotta see both sides of the story.\n\nFormal: You have to consider both sides of the story.\n\nTable 1: Informal sentences with formal rewrites.\n\nIn this paper, we primarily focus on the informal\nto formal direction since we collect our dataset for\nthis direction. However, we evaluate our models\non the formal to informal direction as well.! All\ndata, model outputs, and evaluation results have\nbeen made public? in the hope that they will en-\ncourage more research into style transfer.\n\n‘Results are in the supplementary material.\n*https://github.com/raosudha89/\nGYAFC-corpus\n\n129\n\nProceedings of NAACL-HLT 2018, pages 129-140\nNew Orleans, Louisiana, June 1 - 6, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer \nSudha Rao Joel Tetreault \n\nUniversity of Maryland, College Park ∗ Grammarly raosudha@cs.umd.edu joel.tetreault@grammarly.com \nAbstract \nStyle transfer is the task of automatically trans- forming a piece of text in one particular style into another. A major barrier to progress in this ﬁeld has been a lack of training and eval- uation datasets, as well as benchmarks and au- tomatic metrics. In this work, we create the largest corpus for a particular stylistic trans- fer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics. \n1 Introduction \nOne key aspect of  effective communication  is the accurate expression of the style or tone of some content. For example, writing a more  persuasive email  in a marketing position could lead to in- creased sales; writing a more  formal email  when applying for a job could lead to an offer; and writ- ing a more  polite note  to your future spouse’s par- ents, may put you in a good light. Hovy ( 1987 ) argues that by varying the style of a text, people convey more information than is present in the lit- eral meaning of the words. One particularly im- portant dimension of style is formality ( Heylighen and Dewaele ,  1999 ). Automatically changing the style of a given content to make it more formal can be a useful addition to any writing assistance tool. \nIn the ﬁeld of style transfer, to date, the only available dataset has been for the transformation of modern English to Shakespeare, and it led to the application of phrase-based machine transla- tion (PBMT) ( Xu et al. ,  2012 ) and neural machine translation (NMT) ( Jhamtani et al. ,  2017 ) models to the task. The lack of an equivalent or larger dataset for any other form of style transfer has blocked progress in this ﬁeld. Moreover, prior work has mainly borrowed metrics from machine translation (MT) and paraphrase communities for evaluating style transfer. However, it is not clear if those metrics are the best ones to use for this task. In this work, we address these issues through the following three contributions: \n\n•  Corpus: We present Grammarly’s Yahoo Answers Formality Corpus (GYAFC), the largest dataset for any style containing a to- tal of 110K informal / formal sentence pairs. Table  1  shows sample sentence pairs. •  Benchmarks:  We introduce a set of learning models for the task of formality style trans- fer. Inspired by work in low resource MT, we adapt existing PBMT and NMT approaches for our task and show that they can serve as strong benchmarks for future work. •  Metrics:  In addition to MT and paraphrase metrics, we evaluate our models along three axes:  formality ,  ﬂuency  and  meaning preser- vation  using existing automatic metrics. We compare these metrics with their human judgments and show there is much room for further improvement. \nTable 1: Informal sentences with formal rewrites. \nIn this paper, we primarily focus on the  informal to  formal  direction since we collect our dataset for this direction. However, we evaluate our models on the  formal  to  informal  direction as well.   All data, model outputs, and evaluation results have been made public 2   in the hope that they will en- courage more research into style transfer. "}
{"page": 1, "image_path": "doc_images/N18-1012_1.jpg", "ocr_text": "In the following two sections we discuss related\nwork and the GYAFC dataset. In §4, we detail our\nrule-based and MT-based approaches. In §5, we\ndescribe our human and automatic metric based\nevaluation. In $6, we describe the results of our\nmodels using both human and automatic evalua-\ntion and discuss how well the automatic metrics\ncorrelate with human judgments.\n\n2 Related Work\n\nStyle Transfer with Parallel Data: Sheikha and\nInkpen (2011) collect pairs of formal and informal\nwords and phrases from different sources and use\na natural language generation system to generate\ninformal and formal texts by replacing lexical\nitems based on user preferences. Xu et al. (2012)\n(henceforth XU12) was one of the first works\nto treat style transfer as a sequence to sequence\ntask. They generate a parallel corpus of 30K\nsentence pairs by scraping the modern translations\nof Shakespeare plays and train a PBMT system to\ntranslate from modern English to Shakespearean\nEnglish.? More recently, Jhamtani et al. (2017)\nshow that a copy-mechanism enriched sequence-\nto-sequence neural model outperforms XU12 on\nthe same set. In text simplification, the availability\nof parallel data extracted from English Wikipedia\nand Simple Wikipedia (Zhu et al., 2010) led to the\napplication of PBMT (Wubben et al., 2012a) and\nmore recently NMT (Wang et al., 2016) models.\nWe take inspiration from both the PBMT and\nNMT models and apply several modifications to\nthese approaches for our task of transforming the\nformality style of the text.\n\nStyle Transfer without Parallel Data: An-\nother direction of research directly controls\ncertain attributes of the generated text without\nusing parallel data. Hu et al. (2017) control the\nsentiment and the tense of the generated text by\nlearning a disentangled latent representation in\na neural generative model. Ficler and Goldberg\n(2017) control several linguistic style aspects\nsimultaneously by conditioning a recurrent neural\nnetwork language model on specific style (pro-\nfessional, personal, length) and content (theme,\nsentiment) parameters. Under NMT models,\nSennrich et al. (2016a) control the politeness of\nthe translated text via side constraints, Niu et al.\n(2017) control the level of formality of MT output\n\n3https://github.com/cocoxu/Shakespeare\n\n130\n\nby selecting phrases of a requisite formality level\nfrom the k-best list during decoding. In the field\nof text simplification, more recently, Xu et al.\n(2016) learn large-scale paraphrase rules using\nbilingual texts whereas Kajiwara and Komachi\n(2016) build a monolingual parallel corpus using\nsentence similarity based on alignment between\nword embeddings. Our work differs from these\nmethods in that we mainly address the question of\nhow much leverage we can derive by collecting\na large amount of informal-formal sentence pairs\nand build models that learn to transfer style\ndirectly using this parallel corpus.\n\nIdentifying Formality: There has been pre-\nvious work on detecting formality of a given text\nat the lexical level (Brooke et al., 2010; Lahiri\net al., 2011; Brooke and Hirst, 2014; Pavlick and\nNenkova, 2015), at the sentence level (Pavlick\nand Tetreault, 2016) and at the document level\n(Sheikha and Inkpen, 2010; Peterson et al., 2011;\nMosquera and Moreda, 2012). In our work, we\nreproduce the sentence-level formality classifier\nintroduced in Pavlick and Tetreault (2016) (PT 16)\nto extract informal sentences for GYAFC creation\nand to automatically evaluate system outputs.\n\nEvaluating Style Transfer: The problem of\nstyle transfer falls under the category of natu-\nral language generation tasks such as machine\ntranslation, paraphrasing, etc. Previous work on\nstyle transfer (Xu et al., 2012; Jhamtani et\n2017; Niu et al., 2017; Sennrich et al., 2016a)\nre-purposed the MT metric BLEU (Papineni et\n2002) and the paraphrase metric PINC (Chen\nand Dolan, 2011) for evaluation. Additionally,\nXU12 introduce three new automatic style metrics\nbased on cosine similarity, language model and\nlogistic regression that measure the degree to\nwhich the output matches the target style. Under\nhuman based evaluation, on the other hand, there\nhas been work on a more fine grained evaluation\nwhere human judgments were separately collected\nfor adequacy, fluency and style (Xu et al., 2012;\nNiu et al., 2017). In our work, we conduct a more\nhorough evaluation where we evaluate model\noutputs on the three criteria of formality, fluency\nand meaning using both automatic metrics and\nhuman judgments.\n\n", "vlm_text": "In the following two sections we discuss related work and the GYAFC dataset. In    $\\S4$  , we detail our rule-based and MT-based approaches. In    $\\S5$  , we describe our human and automatic metric based evaluation. In    $\\S6$  , we describe the results of our models using both human and automatic evalua- tion and discuss how well the automatic metrics correlate with human judgments. \n2 Related Work \nStyle Transfer with Parallel Data:  Sheikha and Inkpen ( 2011 ) collect pairs of formal and informal words and phrases from different sources and use a natural language generation system to generate informal and formal texts by replacing lexical items based on user preferences. Xu et al. ( 2012 ) (henceforth X U 12) was one of the ﬁrst works to treat style transfer as a sequence to sequence task. They generate a parallel corpus of 30K sentence pairs by scraping the modern translations of Shakespeare plays and train a PBMT system to translate from modern English to Shakespearean English.   More recently, Jhamtani et al. ( 2017 ) show that a copy-mechanism enriched sequence- to-sequence neural model outperforms X U 12 on the same set. In text simpliﬁcation, the availability of parallel data extracted from English Wikipedia and Simple Wikipedia ( Zhu et al. ,  2010 ) led to the application of PBMT ( Wubben et al. ,  2012a ) and more recently NMT ( Wang et al. ,  2016 ) models. We take inspiration from both the PBMT and NMT models and apply several modiﬁcations to these approaches for our task of transforming the formality style of the text. \nStyle Transfer without Parallel Data: An- other direction of research directly controls certain attributes of the generated text  without using parallel data. Hu et al. ( 2017 ) control the sentiment and the tense of the generated text by learning a disentangled latent representation in a neural generative model. Ficler and Goldberg ( 2017 ) control several linguistic style aspects simultaneously by conditioning a recurrent neural network language model on speciﬁc style (pro- fessional, personal, length) and content (theme, sentiment) parameters. Under NMT models, Sennrich et al. ( 2016a ) control the politeness of the translated text via side constraints, Niu et al. ( 2017 ) control the level of formality of MT output by selecting phrases of a requisite formality level from the  $\\mathbf{k}$  -best list during decoding. In the ﬁeld of text simpliﬁcation, more recently,  Xu et al. ( 2016 ) learn large-scale paraphrase rules using bilingual texts whereas  Kajiwara and Komachi ( 2016 ) build a monolingual parallel corpus using sentence similarity based on alignment between word embeddings. Our work differs from these methods in that we mainly address the question of how much leverage we can derive by collecting a large amount of informal-formal sentence pairs and build models that learn to transfer style directly using this parallel corpus. \n\nIdentifying Formality: There has been pre- vious work on detecting formality of a given text at the lexical level ( Brooke et al. ,  2010 ;  Lahiri et al. ,  2011 ;  Brooke and Hirst ,  2014 ;  Pavlick and Nenkova ,  2015 ), at the sentence level ( Pavlick and Tetreault ,  2016 ) and at the document level ( Sheikha and Inkpen ,  2010 ;  Peterson et al. ,  2011 ; Mosquera and Moreda ,  2012 ). In our work, we reproduce the sentence-level formality classiﬁer introduced in Pavlick and Tetreault ( 2016 ) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. \nEvaluating Style Transfer: The problem of style transfer falls under the category of natu- ral language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer ( Xu et al. ,  2012 ;  Jhamtani et al. , 2017 ;  Niu et al. ,  2017 ;  Sennrich et al. ,  2016a ) has re-purposed the MT metric BLEU ( Papineni et al. , 2002 ) and the paraphrase metric PINC ( Chen and Dolan ,  2011 ) for evaluation. Additionally, X U 12 introduce three new automatic style metrics based on cosine similarity, language model and logistic regression that measure the degree to which the output matches the target style. Under human based evaluation, on the other hand, there has been work on a more ﬁne grained evaluation where human judgments were separately collected for adequacy, ﬂuency and style ( Xu et al. ,  2012 ; Niu et al. ,  2017 ). In our work, we conduct a more thorough evaluation where we evaluate model outputs on the three criteria of  formality ,  ﬂuency and  meaning  using both automatic metrics and human judgments. "}
{"page": 2, "image_path": "doc_images/N18-1012_2.jpg", "ocr_text": "Domain Total Informal Formal Informal to Formal | Formal to Informal\nAll Yahoo Answers 40M. 24M 16M Train | Tune Test Tune Test\nEntertainment & Music 3.8M. 2.7M 700K E&M 52,595 | 2,877 1,416 2,356 1,082\nFamily & Relationships 7.8M. 5.6M 1.8M F&R 51,967 | 2,788 1,332 2,247 1,019\n\nTable 2: Yahoo Answers corpus statistics\n\n3 GYAFC Dataset\n\n3.1. Creation Process\n\nYahoo Answers,* a question answering forum,\n\ncontains a large number of informal sentences and\nallows redistribution of data. Hence, we use the\nYahoo Answers L6 corpus? to create our GYAFC\ndataset of informal and formal sentence pairs. In\norder to ensure a uniform distribution of data,\nwe remove sentences that are questions, contain\nURLs, and are shorter than 5 words or longer\nthan 25. After these preprocessing steps, 40 mil-\nlion sentences remain. The Yahoo Answers corpus\nconsists of several different domains like Business,\nEntertainment & Music, Travel, Food, etc. PT16\nshow that the formality level varies significantly\nacross different genres. In order to control for\nthis variation, we work with two specific domains\nthat contain the most informal sentences and show\nresults on training and testing within those cate-\ngories. We use the formality classifier from PT 16\nto identify informal sentences. We train this clas-\nsifier on the Answers genre of the PT16 corpus\nwhich consists of nearly 5,000 randomly selected\nsentences from Yahoo Answers manually anno-\ntated on a scale of -3 (very informal) to 3 (very for-\nmal).° We find that the domains of Entertainment\n& Music and Family & Relationships contain the\nmost informal sentences and create our GYAFC\ndataset using these domains. Table 2 shows the\nnumber of formal and informal sentences in all of\nYahoo Answers corpus and within the two selected\ndomains. Sentences with a score less than 0 are\nconsidered as informal and sentences with a score\ngreater than 0 are considered as formal.\n\nNext, we randomly sample a subset of 53,000\ninformal sentences each from the Entertainment &\nMusic (E&M) and Family & Relationships (F&R)\ncategories and collect one formal rewrite per sen-\ntence using Amazon Mechanical Turk. The work-\ners are presented with detailed instructions, as well\n\n‘https: //answers.yahoo.com/answer\n\nShttps://webscope. sandbox. yahoo.com/\ncatalog.php?datatype=1\n\n°nttp://www.seas.upenn.edu/~nlp/\nresources/formality-corpus.tgz\n\nTable 3: GYAFC dataset statistics\n\nas examples. To ensure quality control, four ex-\nperts, two of which are the authors of this paper,\nreviewed the rewrites of the workers and rejected\nhose that they felt did not meet the required stan-\ndards. They also provided the workers with rea-\nsons for rejection so that they would not repeat the\nsame mistakes. Any worker who repeatedly per-\n‘ormed poorly was eventually blocked from doing\nhe task. We use this train set to train our models\nor the style transfer tasks in both directions.\n\nSince we want our tune and test sets to be of\nhigher quality compared to the train set, we re-\ncruit a set of 85 expert workers for this anno-\nation who had a 100% acceptance rate for our\nask and who had previously done more than 100\nrewrites. Further, we collect multiple references\nor the tune/test set to adapt PBMT tuning and\nevaluation techniques to our task. We collect four\ndifferent rewrites per sentence using our expert\nworkers by randomly assigning sentences to the\nexperts until four rewrites for each sentence are\nobtained.” To create our tune and test sets for the\ninformal to formal direction, we sample an addi-\ntional 3,000 informal sentences for our tune set\nand 1,500 sentences for our test set from each of\nthe two domains.\n\nTo create our tune and test sets for the formal\nto informal direction, we start with the same tune\nand test split as the first direction. For each formal\nrewrite® from the first direction, we collect three\ndifferent informal rewrites using our expert work-\ners as before. These three informal rewrites along\nwith the original informal sentence become our set\nof four references for this direction of the task. Ta-\nble 3 shows the exact number of sentences in our\ntrain, tune and test sets.\n\n3.2 Analysis\n\nThe following quantitative and qualitative analy-\nses are aimed at characterizing the changes be-\ntween the original informal sentence and its formal\n\n\"Thus, note that the four rewrites are not from the same\nfour workers for each sentence\n\nOut of four, we pick the one with the most edit distance\nwith the original informal. Rationale explained in Section 3.2\n", "vlm_text": "This table presents data related to Yahoo Answers, particularly focusing on different domains and their respective format (informal or formal). \n\nThe columns are labeled as:\n- \"Domain\": Referring to categories within Yahoo Answers.\n- \"Total\": Indicating the total number of entries or data points in millions (M) or thousands (K).\n- \"Informal\": Showing the quantity of informal entries within each domain.\n- \"Formal\": Showing the quantity of formal entries within each domain.\n\nThe rows display the specific data for:\n1. \"All Yahoo Answers\": With a total of 40 million entries, out of which 24 million are informal and 16 million are formal.\n2. \"Entertainment & Music\": With a total of 3.8 million entries, including 2.7 million informal and 0.7 million (700K) formal entries.\n3. \"Family & Relationships\": Comprising 7.8 million entries, with 5.6 million being informal and 1.8 million formal.\n3 GYAFC  Dataset \n3.1 Creation Process \nYahoo Answers,   a question answering forum, contains a large number of informal sentences and allows redistribution of data. Hence, we use the Yahoo Answers L6 corpus 5   to create our GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, we remove sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps,   $40~\\mathrm{mil}.$  - lion sentences remain. The Yahoo Answers corpus consists of several different domains like  Business, Entertainment & Music, Travel, Food,  etc. PT16 show that the formality level varies signiﬁcantly across different genres. In order to control for this variation, we work with two speciﬁc domains that contain the most informal sentences and show results on training and testing within those cate- gories. We use the formality classiﬁer from PT16 to identify informal sentences. We train this clas- siﬁer on the  Answers  genre of the PT16 corpus which consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually anno- tated on a scale of -3 (very informal) to 3 (very for- mal).   We ﬁnd that the domains of  Entertainment & Music  and  Family & Relationships  contain the most informal sentences and create our GYAFC dataset using these domains. Table  2  shows the number of formal and informal sentences in all of Yahoo Answers corpus and within the two selected domains. Sentences with a score less than 0 are considered as informal and sentences with a score greater than 0 are considered as formal. \nNext, we randomly sample a subset of 53,000 informal sentences each from the  Entertainment & Music  (E&M) and  Family & Relationships  (F&R) categories and collect one formal rewrite per sen- tence using Amazon Mechanical Turk. The work- ers are presented with detailed instructions, as well \nThe table contains data related to datasets used in two different writing style transformation tasks: Informal to Formal and Formal to Informal. The data is organized into columns and rows.\n\nColumns:\n1. Task type (“Informal to Formal” and “Formal to Informal”) divided into “Tune” and “Test”.\n2. Each task type has associated sets labeled “Train”.\n\nRows:\n1. E&M: It has 52,595 instances for training, 2,877 for tuning, and 1,416 for testing in the Informal to Formal transformation task. For the Formal to Informal task, there’s 2,356 for tuning and 1,082 for testing.\n2. F&R: It has 51,967 instances for training, 2,788 for tuning, and 1,332 for testing in the Informal to Formal transformation task. For the Formal to Informal task, there’s 2,247 for tuning and 1,019 for testing.\nas examples. To ensure quality control, four ex- perts, two of which are the authors of this paper, reviewed the rewrites of the workers and rejected those that they felt did not meet the required stan- dards. They also provided the workers with rea- sons for rejection so that they would not repeat the same mistakes. Any worker who repeatedly per- formed poorly was eventually blocked from doing the task. We use this train set to train our models for the style transfer tasks in both directions. \nSince we want our tune and test sets to be of higher quality compared to the train set, we re- cruit a set of 85 expert workers for this anno- tation who had a   $100\\%$   acceptance rate for our task and who had previously done more than 100 rewrites. Further, we collect multiple references for the tune/test set to adapt PBMT tuning and evaluation techniques to our task. We collect four different rewrites per sentence using our expert workers by randomly assigning sentences to the experts until four rewrites for each sentence are obtained.   To create our tune and test sets for the informal  to  formal  direction, we sample an addi- tional 3,000 informal sentences for our tune set and 1,500 sentences for our test set from each of the two domains. \nTo create our tune and test sets for the  formal to  informal  direction, we start with the same tune and test split as the ﬁrst direction. For each formal rewrite 8   from the ﬁrst direction, we collect three different informal rewrites using our expert work- ers as before. These three informal rewrites along with the original informal sentence become our set of four references for this direction of the task. Ta- ble  3  shows the exact number of sentences in our train, tune and test sets. \n3.2 Analysis \nThe following quantitative and qualitative analy- ses are aimed at characterizing the changes be- tween the original informal sentence and its formal rewrite in the GYAFC train split.   We present our analysis here on only the E&M domain data since we observe similar patterns in F&R. "}
{"page": 3, "image_path": "doc_images/N18-1012_3.jpg", "ocr_text": "rewrite in the GYAFC train split.? We present our\nanalysis here on only the E&M domain data since\nwe observe similar patterns in F&R.\n\n40%\n@ Formal Rewrite\n\n30%\n\n20%\n\n0% = 8. is.\n\n[0,10) [10,20) (20,30) (30,40) (40,50) [50,60) (60,70) (70,80)\nEdit Distance\n\n°% of Sentences\n\nWi Formal Rewrite (= 28.85, O = 19.39)\n\nFigure 1: Percentage of sentences binned according\nto formality score in train set of E&M.\n\n20% * Original Informal\nFormal Rewrite\n\n15% +\n\n10%\n\n°% of Sentences\n\n5%\n\n0% — /\n[-3.0,-2.75) [-1.75,-1.5) [-0.5,-0.25) _[0.75,1.0)\n\nFormality Score\n@ Original Informal (= -1.06, o = 0.82)\no = 0.64)\n\n[2.0,2.25)\n\nFormal Rewrite ([= 0.12,\n\nFigure 2: Percentage of sentences binned according\nto formality score in train set of E&M\n\nQuantitative Analysis: While rewriting sen-\ntences more formally, humans tend to make a wide\nrange of lexical/character-level edits. In Figure 1,\nwe plot the distribution of the character-level Lev-\nenshtein edit distance between the original infor-\nmal and the formal rewrites in the train set and\nobserve a standard deviation of o = 19.39 with a\nmean jt = 28.85. Next, we look at the difference\nin the formality level of the original informal and\nthe formal rewrites in GYAFC. We find that the\nclassifier trained on the Answers genre of PT16\ndataset correlates poorly (Spearman p = 0.38) with\nhuman judgments when tested on our domain spe-\ncific datasets. Hence, we collect formality judg-\nments on a scale of -3 to +1, similar to PT 16, for\nan additional 5000 sentences each from both do-\nmains and obtain a formality classifier with higher\ncorrelation (Spearman p = 0.56). We use this re-\ntrained classifier for our evaluation in §5 as well.\n\nIn Figure 2, we plot the distribution of the\n\n°We observe similar patterns on the tune and test set.\n\nformality scores on the original informal sen-\ntence and their formal rewrites in the train set\nand observe an increase in the mean formality\nscore as we go from informal (—1.06) to formal\nrewrites (0.12). As compared to edit distance and\nformality, we observe a much lower variation in\nsentence lengths with the mean slightly increasing\nfrom informal (11.93) to their formal rewrites\n(12.56) in the train set.\n\nQualitative Analysis: To understand what\nstylistic choices differentiate formal from infor-\nmal text, we perform an analysis similar to PT 16\nand look at 50 rewrites from both domains and\nrecord the frequency of the types of edits that\nworkers made when creating a more formal sen-\ntence.!° In contrast to PT16, we observe a higher\npercentage of phrasal paraphrases (47%), edits to\npunctuations (40%) and expansion of contractions\n(12%). This is reflective of our sentences coming\nfrom very informal domains of Yahoo Answers.\nSimilar to PT16, we also observe capitalization\n(46%) and normalization (10%).\n\n4 Models\n\nWe experiment with three main classes of ap-\nproaches: a rule-based approach, PBMT and\nNMT. Inspired by work in low resource machine\ntranslation, we apply several modifications to the\nstandard PBMT and NMT models and create a set\nof strong benchmarks for the style transfer com-\nmunity. We apply these models to both directions\nof style transfer: informal to formal and formal\nto informal. In our description, we refer to the\ntwo styles as source and target. We summarize\nthe models below and direct the reader to supple-\nmentary material for further detail.\n\n4.1 Rule-based Approach\n\nCorresponding to the category of edits described\nin §3.2, we develop a set of rules to automatically\nmake an informal sentence more formal where we\ncapitalize first word and proper nouns, remove re-\npeated punctuations, handcraft a list of expansion\nfor contractions etc. For the formal to informal\ndirection, we design a similar set of rules in the\nopposite direction.\n\n'OExamples of edits in supplementary material.\n\n132\n", "vlm_text": "\nThe image is a bar graph showing the percentage of sentences binned according to formality scores in the training set of E&M. The x-axis represents the edit distance ranges, while the y-axis indicates the percentage of sentences. The edit distance ranges are divided into bins of 10 units (e.g., [0,10], [10,20], etc.). Each bin shows the percentage of sentences with that edit distance, labeled as \"Formal Rewrite.\"\n\nThe distribution of sentences across different edit distances forms a decreasing pattern, with the highest percentages in the [10,20] and [20,30] ranges, gradually decreasing towards higher edit distances.\n\nAdditionally, the graph provides statistical information about the formal rewrite data with a mean (μ) of 28.85 and a standard deviation (σ) of 19.39.\nThe image is a graph depicting the distribution of sentences according to their formality scores in a training set labeled E&M. It includes two distributions: one for \"Original Informal\" sentences, represented by black circles, and one for \"Formal Rewrite\" sentences, shown by gray squares. The x-axis displays formality scores ranging from -3.0 to 2.25, while the y-axis represents the percentage of sentences. \n\nThe plot reveals that the informal sentences, which have a mean formality score (μ) of -1.06 and a standard deviation (σ) of 0.82, are centered left of zero on the formality scale, indicating less formality. Conversely, the formal rewrites have a higher mean score (μ) of 0.12 with a standard deviation (σ) of 0.64, showing that they are on average more formal compared to the original informal sentences. \n\nThe black line with circles peaks left of the origin, whereas the gray line with squares has a peak slightly right of zero, illustrating the shift in distribution towards increased formality through rewriting.\nQuantitative Analysis: While rewriting sen- tences more formally, humans tend to make a wide range of lexical/character-level edits. In Figure  1 , we plot the distribution of the character-level Lev- enshtein edit distance between the original infor- mal and the formal rewrites in the train set and observe a standard deviation of    $\\sigma=19.39$   with a mean  $\\mu=28.85$  . Next, we look at the difference in the formality level of the original informal and the formal rewrites in GYAFC. We ﬁnd that the classiﬁer trained on the  Answers  genre of PT16 dataset correlates poorly (Spearman  $\\rho\\,{=}\\,0.38)$  ) with human judgments when tested on our domain spe- ciﬁc datasets. Hence, we collect formality judg- ments on a scale of  $^{-3}$   to  $+1$  , similar to PT16, for an additional 5000 sentences each from both do- mains and obtain a formality classiﬁer with higher correlation (Spearman    $\\rho=0.56)$  ). We use this re- trained classiﬁer for our evaluation in    $\\S5$   as well. \nformality scores on the original informal sen- tence and their formal rewrites in the train set and observe an increase in the mean formality score as we go from informal   $(-1.06)$   to formal rewrites ( 0 . 12 ). As compared to edit distance and formality, we observe a much lower variation in sentence lengths with the mean slightly increasing from informal ( 11 . 93 ) to their formal rewrites ( 12 . 56 ) in the train set. \nQualitative Analysis: To understand what stylistic choices differentiate formal from infor- mal text, we perform an analysis similar to PT16 and look at 50 rewrites from both domains and record the frequency of the types of edits that workers made when creating a more formal sen- tence.   In contrast to PT16, we observe a higher percentage of phrasal paraphrases   $(47\\%)$  , edits to punctuations   $(40\\%)$   and expansion of contractions  $(12\\%)$  . This is reﬂective of our sentences coming from very informal domains of Yahoo Answers. Similar to PT16, we also observe capitalization  $(46\\%)$   and normalization   $(10\\%)$  . \n4 Models \nWe experiment with three main classes of ap- proaches: a rule-based approach, PBMT and NMT. Inspired by work in low resource machine translation, we apply several modiﬁcations to the standard PBMT and NMT models and create a set of strong benchmarks for the style transfer com- munity. We apply these models to both directions of style transfer:  informal  to  formal  and  formal to  informal . In our description, we refer to the two styles as  source  and  target . We summarize the models below and direct the reader to supple- mentary material for further detail. \n4.1 Rule-based Approach \nCorresponding to the category of edits described in  $\\S3.2$  , we develop a set of rules to automatically make an informal sentence more formal where we capitalize ﬁrst word and proper nouns, remove re- peated punctuations, handcraft a list of expansion for contractions etc. For the  formal  to  informal direction, we design a similar set of rules in the opposite direction. \nIn Figure  2 , we plot the distribution of the "}
{"page": 4, "image_path": "doc_images/N18-1012_4.jpg", "ocr_text": "4.2 Phrase-based Machine Translation\n\nPhrased-based machine translation models have\nhad success in the fields of machine transla-\ntion, style transfer (XU12) and text simplification\n(Wubben et al., 2012b; Xu et al., 2016). Inspired\nby work in low resource machine translation, we\nuse a combination of training regimes to develop\nour model. We train on the output of the rule-\nbased approach when applied to GYAFC. This is\nmeant to force the PBMT model to learn gener-\nalizations outside the rules. To increase the data\nsize, we use self-training (Ueffing, 2006), where\nwe use the PBMT model to translate the large\nnumber of in-domain sentences from GYAFC be-\nlonging to the the source style and use the resul-\ntant output to retrain the PBMT model. Using sub-\nselection, we only select rewrites that have an Lev-\nenshtein edit distance of over 10 characters when\ncompared to the source to encourage the model\nto be less conservative. Finally, we upweight the\nrule-based GYAFC data via duplication (Sennrich\net al., 2016b). For our experiments, we use Moses\n(Koehn et al., 2007). We train a 5-gram language\nmodel using KenLM (Heafield et al., 2013), and\nuse target style sentences from GYAFC and the\nsub-sampled target style sentences from out-of-\ndomain Yahoo Answers, as in Moore and Lewis\n(2010), to create a large language model.\n\n4.3 Neural Machine Translation\n\nWhile encoder-decoder based neural network\nmodels have become quite successful for\nMT(Sutskever et al., 2014; Bahdanau et al., 2014;\nCho et al., 2014), the field of style transfer, has\nnot yet been able to fully take advantage of these\nadvances owing to the lack of availability of large\nparallel data. With GYAFC we can now show\nhow well NMT techniques fare for style transfer.\nWe experiment with three NMT models:\n\nNMT baseline: Our baseline model is a bi-\ndirectional LSTM (Hochreiter and Schmidhuber,\n1997) encoder-decoder model with attention\n(Bahdanau et al., 2014).!' We pretrain the\ninput word embeddings on Yahoo Answers using\nGIoVE (Pennington et al., 2014). As in our PBMT\nbased approach, we train our NMT baseline model\non the output of the rule-based approach when\napplied to GYAFC.\n\n\"Details are in the supplementary material.\n\nNMT Copy: Jhamtani et al., (2017) intro-\nduce a copy-enriched NMT model for style\ntransfer to better handle stretches of text which\nshould not be changed. We incorporate this\nmechanism into our NMT Baseline.\n\nNMT Combined: The size of our parallel\ndata is smaller than the size typically used to train\nNMT models. Motivated by this fact, we propose\nseveral variants to the baseline models that we\nfind helps minimize this issue. We augment the\ndata used to train NMT Copy via two techniques:\n1) we run the PBMT model on additional source\ndata, and 2) we use back-translation (Sennrich\net al., 2016c) of the PBMT model to translate the\nlarge number of in-domain target style sentences\nfrom GYAFC. To balance the over one million\nartificially generated pairs from the respective\ntechniques, we upweight the rule-based GYAFC\ndata via duplication. !”\n\n5 Evaluation\n\nAs discussed earlier, there has been very little re-\nsearch into best practices for style transfer evalu-\nation. Only a few works have included a human\nevaluation (Xu et al., 2012; Jhamtani et al., 2017),\nand automatic evaluations have employed BLEU\nor PINC (Xu et al., 2012; Chen and Dolan, 2011),\nwhich have been borrowed from other fields and\nnot vetted for this task. In our work, we con-\nduct a more thorough and detailed evaluation us-\ning both humans and automatic metrics to assess\ntransformations. Inspired by work in the para-\nphrase community (Callison-Burch, 2008), we so-\nlicit ratings on how formal, how fluent and how\nmeaning-preserving a rewrite is. Additionally, we\nlook at the correlation between the human judg-\nments and the automatic metrics.\n\n5.1. Human-based Evaluation\n\nWe perform human-based evaluation to assess\nmodel outputs on the four criteria: formality,\nfluency, meaning and overall. For a subset of 500\nsentences from the test sets of both Entertainment\n& Music and Family & Relationship domains,\nwe collect five human judgments per sentence\nper criteria using Amazon Mechanical Turk as\nfollows:\n\nTraining data sizes for different methods are summarized\nin the supplementary material.\n\n133\n", "vlm_text": "4.2 Phrase-based Machine Translation \nPhrased-based machine translation models have had success in the ﬁelds of machine transla- tion, style transfer (X U 12) and text simpliﬁcation ( Wubben et al. ,  2012b ;  Xu et al. ,  2016 ). Inspired by work in low resource machine translation, we use a combination of training regimes to develop our model. We train on the output of the rule- based approach when applied to GYAFC. This is meant to force the PBMT model to learn gener- alizations  outside  the rules. To increase the data size, we use self-training ( Uefﬁng ,  2006 ), where we use the PBMT model to translate the large number of in-domain sentences from GYAFC be- longing to the the source style and use the resul- tant output to retrain the PBMT model. Using sub- selection, we only select rewrites that have an Lev- enshtein edit distance of over 10 characters when compared to the source to encourage the model to be less conservative. Finally, we upweight the rule-based GYAFC data via duplication ( Sennrich et al. ,  2016b ). For our experiments, we use Moses ( Koehn et al. ,  2007 ). We train a 5-gram language model using KenLM ( Heaﬁeld et al. ,  2013 ), and use target style sentences from GYAFC and the sub-sampled target style sentences from out-of- domain Yahoo Answers, as in Moore and Lewis ( 2010 ), to create a large language model. \n4.3 Neural Machine Translation \nWhile encoder-decoder based neural network models have become quite successful for MT( Sutskever et al. ,  2014 ;  Bahdanau et al. ,  2014 ; Cho et al. ,  2014 ), the ﬁeld of style transfer, has not yet been able to fully take advantage of these advances owing to the lack of availability of large parallel data. With GYAFC we can now show how well NMT techniques fare for style transfer. We experiment with three NMT models: \nNMT baseline: Our baseline model is a bi- directional LSTM ( Hochreiter and Schmidhuber , 1997 ) encoder-decoder model with attention ( Bahdanau et al. ,  2014 ). We pretrain the input word embeddings on Yahoo Answers using GloVE ( Pennington et al. ,  2014 ). As in our PBMT based approach, we train our NMT baseline model on the output of the rule-based approach when applied to GYAFC. \nNMT Copy: Jhamtani et al., ( 2017 ) intro- duce a copy-enriched NMT model for style transfer to better handle stretches of text which should not be changed. We incorporate this mechanism into our NMT Baseline. \nNMT Combined: The size of our parallel data is smaller than the size typically used to train NMT models. Motivated by this fact, we propose several variants to the baseline models that we ﬁnd helps minimize this issue. We augment the data used to train NMT Copy via two techniques: 1) we run the PBMT model on additional source data, and 2) we use back-translation ( Sennrich et al. ,  2016c ) of the PBMT model to translate the large number of in-domain target style sentences from GYAFC. To balance the over one million artiﬁcially generated pairs from the respective techniques, we upweight the rule-based GYAFC data via duplication. \n5 Evaluation \nAs discussed earlier, there has been very little re- search into best practices for style transfer evalu- ation. Only a few works have included a human evaluation ( Xu et al. ,  2012 ;  Jhamtani et al. ,  2017 ), and automatic evaluations have employed BLEU or PINC ( Xu et al. ,  2012 ;  Chen and Dolan ,  2011 ), which have been borrowed from other ﬁelds and not vetted for this task. In our work, we con- duct a more thorough and detailed evaluation us- ing both humans and automatic metrics to assess transformations. Inspired by work in the para- phrase community ( Callison-Burch ,  2008 ), we so- licit ratings on how formal, how ﬂuent and how meaning-preserving a rewrite is. Additionally, we look at the correlation between the human judg- ments and the automatic metrics. \n5.1 Human-based Evaluation \nWe perform human-based evaluation to assess model outputs on the four criteria: formality , ﬂuency ,  meaning  and  overall . For a subset of 500 sentences from the test sets of both  Entertainment & Music  and  Family & Relationship  domains, we collect ﬁve human judgments per sentence per criteria using Amazon Mechanical Turk as follows: "}
{"page": 5, "image_path": "doc_images/N18-1012_5.jpg", "ocr_text": "Formality: Following PT16, workers rate\nthe formality of the source style sentence, the\ntarget style reference rewrite and the target style\nmodel outputs on a discrete scale of -3 to +3\ndescribed as: -3: Very Informal, -2: Informal, -1:\nSomewhat Informal, 0: Neutral, 1: Somewhat\nFormal, 2: Formal and 3: Very Formal.\n\nFluency: Following Heilman et al. (2014),\nworkers rate the fluency of the source style\nsentence, the target style reference rewrite and the\ntarget style model outputs on a discrete scale of 1\nto 5 described as: 5: Perfect, 4: Comprehensible,\n3: Somewhat Comprehensible, 2: Incomprehen-\nsible. We additionally provide an option of /:\nOther for sentences that are incomplete or just a\nfragment.\n\nMeaning Preservation: Following the an-\nnotation scheme developed for the Semantic\nTextual Similarity (STS) dataset (Agirre et al.,\n2016), given two sentences i.e. the source style\nsentence and the target style reference rewrite or\nthe target style model output, workers rate the\nmeaning similarity of the two sentences on a scale\nof 1 to 6 described as: 6: Completely equivalent,\n5: Mostly equivalent, 4: Roughly equivalent, 3:\nNot equivalent but share some details, 2: Not\nequivalent but on same topic, 1: Completely\ndissimilar.\n\nOverall Ranking: In addition to the fine-\ngrained human judgments, we collect judgments\nto assess the overall ranking of the systems.\nGiven the original source style sentence, the target\nstyle reference rewrite and the target style model\noutputs, we ask workers to rank the rewrites in\nthe order of their overall formality, taking into\naccount both fluency and meaning preservation.\nWe then rank the model using the equation below:\n\n1\nIS|\n\nrank(model) =\n\n1 :\nS JI S rank(Smodel; j)\njet\n\nses '*\n\nqd)\nwhere, model is the one of our models, S is a sub-\nset of 500 test set sentences, J is the set of five\njudgments, sodei is the model rewrite for sen-\ntence s, and rank(Smodet,j) is the rank of 8 model\n\nin judgment 7.\nThe two authors of the paper reviewed these hu-\nman judgments and found that in majority of the\n\n134\n\ncases the annotations looked correct. But as is\ncommon in any such crowdsourced data collection\nprocess, there were some errors, especially in the\noverall ranking of the systems.\n\n5.2 Automatic Metrics\n\nWe cover each of the human evaluations with a\ncorresponding automatic metric:\n\nFormality: We use the formality classifier\ndescribed in PT16. We find that the classifier\ntrained on the answers genre of PT 16 dataset does\nnot perform well when tested on our datasets.\nHence, we collect formality judgments for an\nadditional 5000 sentences and use the formality\nclassifier re-trained on this in-domain data.\n\nFluency: We use the reimplementation!? of\nHeilman et al. (2014) (H14 in Table 4) which is a\nstatistical model for predicting the grammaticality\nof a sentence on a scale of 0 to 4 previously\nshown to be effective for other generation tasks\nlike grammatical error correction (Napoles et al.,\n2016).\n\nMeaning Preservation: Modeling semantic\nsimilarity at a sentence level is a fundamental\nlanguage processing task, and one that is a wide\nopen field of research. Recently, He et al., (2015)\n(HEI5 in Table 4) developed a convolutional\nneural network based sentence similarity measure.\nWe use their off-the-shelf implementation!* to\ntrain a model on the STS and use it to measure the\nmeaning similarity between the original source\nstyle sentence and its target style rewrite (both\nreference and model outputs).\n\nOverall Ranking: We experiment with BLEU\n(Papineni et al., 2002) and PINC (Chen and\nDolan, 2011) as both were used in prior style\nevaluations, as well as TERp (Snover et al., 2009).\n\n6 Results\n\nIn this section, we discuss how well the five mod-\nels perform in the informal to formal style transfer\ntask using human judgments (§6.1) and automatic\nmetrics (86.2), the correlation of the automatic\nmetrics and human judgments to determine the ef-\n\nShttps://github.com/cnap/grammaticality-\nmetrics/tree/master/heilman-et-al\nhttps://github.com/castorini/MP-CNN-Torch\n", "vlm_text": "Formality: Following PT16, workers rate the formality of the source style sentence, the target style reference rewrite and the target style model outputs on a discrete scale of -3 to   $^{+3}$  described as:  -3: Very Informal, -2: Informal, -1: Somewhat Informal, 0: Neutral, 1: Somewhat Formal, 2: Formal and 3: Very Formal . \nFluency: Following Heilman et al. ( 2014 ), workers rate the ﬂuency of the source style sentence, the target style reference rewrite and the target style model outputs on a discrete scale of 1 to 5 described as:  5: Perfect, 4: Comprehensible, 3: Somewhat Comprehensible, 2: Incomprehen- sible . We additionally provide an option of    $I$  : Other  for sentences that are incomplete or just a fragment. \nMeaning Preservation: Following the an- notation scheme developed for the Semantic Textual Similarity (STS) dataset ( Agirre et al. , 2016 ), given two sentences i.e. the source style sentence and the target style reference rewrite or the target style model output, workers rate the meaning similarity of the two sentences on a scale of 1 to 6 described as:  6: Completely equivalent, 5: Mostly equivalent, 4: Roughly equivalent, 3: Not equivalent but share some details, 2: Not equivalent but on same topic, 1: Completely dissimilar . \nOverall Ranking: In addition to the ﬁne- grained human judgments, we collect judgments to assess the overall ranking of the systems. Given the original source style sentence, the target style reference rewrite and the target style model outputs, we ask workers to rank the rewrites in the order of their overall formality, taking into account both ﬂuency and meaning preservation. We then rank the model using the equation below: \n\n$$\nr a n k(m o d e l)=\\frac{1}{|S|}\\sum_{s\\in S}\\frac{1}{|J|}\\sum_{j\\in J}r a n k(s_{m o d e l},j)\n$$\n \nwhere,  model  is the one of our models,  $S$   is a sub- set of 500 test set sentences,    $J$   is the set of ﬁve judgments,    $s_{m o d e l}$   is the model rewrite for sen- tence  $s$  , and    $r a n k\\big(s_{m o d e l},j\\big)$   is the rank of    $s_{m o d e l}$  in judgment    $j$  . \ncases the annotations looked correct. But as is common in any such crowdsourced data collection process, there were some errors, especially in the overall ranking of the systems. \n5.2 Automatic Metrics \nWe cover each of the human evaluations with a corresponding automatic metric: \nFormality: We use the formality classiﬁer described in PT16. We ﬁnd that the classiﬁer trained on the  answers  genre of PT16 dataset does not perform well when tested on our datasets. Hence, we collect formality judgments for an additional 5000 sentences and use the formality classiﬁer re-trained on this in-domain data. \nFluency: We use the re implementation 13   of Heilman et al. ( 2014 ) (H14 in Table  4 ) which is a statistical model for predicting the grammaticality of a sentence on a scale of 0 to 4 previously shown to be effective for other generation tasks like grammatical error correction ( Napoles et al. , 2016 ). \nMeaning Preservation: Modeling semantic similarity at a sentence level is a fundamental language processing task, and one that is a wide open ﬁeld of research. Recently, He et al., ( 2015 ) (H E 15 in Table  4 ) developed a convolutional neural network based sentence similarity measure. We use their off-the-shelf implementation 14   to train a model on the STS and use it to measure the meaning similarity between the original source style sentence and its target style rewrite (both reference and model outputs). \nOverall Ranking:  We experiment with BLEU ( Papineni et al. ,  2002 ) and PINC ( Chen and Dolan ,  2011 ) as both were used in prior style evaluations, as well as TERp ( Snover et al. ,  2009 ). \n6 Results \nIn this section, we discuss how well the ﬁve mod- els perform in the  informal  to  formal  style transfer task using human judgments ( § 6.1 ) and automatic metrics ( § 6.2 ), the correlation of the automatic metrics and human judgments to determine the ef- \nThe two authors of the paper reviewed these hu- man judgments and found that in majority of the "}
{"page": 6, "image_path": "doc_images/N18-1012_6.jpg", "ocr_text": "Formality Fluency Meaning Combined Overall\nModel Human PTI6 | Human H14 | Human HEI5 | Human Auto | BLEU TERp_ PINC\nOriginal Informal | -1.23 — -1.00 3.90 2.89 - - - - 50.69 0.35 0.00\nFormal Reference 0.38 0.17 4.45 3.32 4.57 3.64 5.68 4.67 | 100.0 0.37 69.79\nRule-based -0.59 — -0.34 4.00 3.09 4.85 4.41 5.24 4.69 | 61.38 0.27 26.05\nPBMT -0.19* — 0.00* 3.96 3.28* | 4.64* 4.19% 5.27 4.82* | 67.26* 0.26 44.94*\nNMT Baseline 0.05* —0.07* 4.05 3.52* | 3.55* 3.89* | 4.96* 4.84* | 56.61 0.38* 56.92*\nNMT Copy 0.02* 0.10* 4.07 3.45* | 3.48* 3.87* | 4.93* 4.81* | 58.01  0.38* 56.39*\nNMT Combined -0.16* 0.00* | 4.09* = 3.27* | 4.46*  4.20* | 5.32* 4.82* | 67.67* 0.26 —43.54*\nTable 4: Results of models on 500 test sentences from E&M for informal to formal task evaluated using human\njudgments and automatic metrics for three criteria of evaluation: formality, fluency and meaning preservation.\nScores marked with * are significantly different from the rule-based scores with p < 0.001.\n\n0.675 4 Formal Reference\nRule Based\nPBMT Combined\n@ NMT Combined\n© Original Informal\n\nFormality Score\n\n2\n(5.7) (9,11) 13,15) 117,19) (21,23)\n\nOriginal Informal Sentence Length\n\n+ Rule Based\nPBMT Combined\n\n NMT Combined\n\n4 Formal Reference\n\nMeaning Score\n\n4\n15,7) [9,41) (13,15) (17,19)\n\nOriginal Informal Sentence Length\n\n(21,23)\n\nFigure 3: For varying sentence lengths of the original\ninformal sentence the formality and the meaning scores\nfrom human judgments on different model outputs and\non the original informal and the formal reference sen-\ntences.\n\nficacy of the metrics (§6.3) and present a manual\nanalysis (86.4). We randomly select 500 sentences\nfrom each test set and run all five models. We use\nthe entire train and tune split for training and tun-\ning. We discuss results only on the E&M domain\nand list results on the F&R domain in the supple-\nmentary material.\n\nTable 4 shows the results for human §6.1 and\nautomatic $6.2 evaluation of model rewrites. For\nall metrics except TERp, a higher score is better.\nFor each of the automatic metrics, we evaluate\nagainst four human references. The row ‘Original\nInformal’ contains the scores when the original in-\n\n135\n\nformal sentence is compared with the four formal\nreference rewrites. Comparing the model scores\nto this score helps us understand how closer are\nthe model outputs to the formal reference rewrites\ncompared to initial distance between the informal\nand the formal reference rewrite.\n\n6.1 Results using Human Judgments\n\nThe columns marked ‘Human’ in Table 4 show\nhe human judgments for the models on the three\nseparate criteria of formality, fluency and mean-\ning collected using the process described in Sec-\nion 5.1.!° The NMT Baseline and Copy models\nbeat others on the formality axis by a significant\nmargin. Only the NMT Combined model achieves\na statistically higher fluency score when compared\n0 the rule-based baseline model. As expected, the\nrule-based model is the most meaning preserving\nsince it is the most conservative. Figure 3 shows\nhe trend in the four leading models along formal-\nity and meaning for varying lengths of the source\nsentence. NMT Combined beats PBMT on for-\nmality for shorter lengths whereas the trend re-\nverses as the length increases. PBMT generally\npreserves meaning more than the NMT Combined.\nWe find that the fluency scores for all models de-\ncreases as the sentence length increases which is\nsimilar to the trend generally observed with ma-\nchine translation based approaches.\n\nSince a good style transfer model is the one that\nattains a balanced score across all the three axes,\nwe evaluate the models on a combination of these\nmetrics!® shown under the column ‘Combined’ in\nTable 4. NMT Combined is the only model having\nacombined score statistically greater than the rule-\nbased approach.\n\n'5 Out of the four reference rewrites, we pick one at random\nto show to Turkers.\n\n‘We recalibrate the scores to normalize for different\nranges.\n", "vlm_text": "The table presents a comparison of different models used for formality, fluency, meaning, and combined attributes, scored both by human and automatic evaluation methods. Additionally, the overall performance metrics include BLEU, TERp, and PINC scores. Here are the key components of the table:\n\n1. **Models**:\n   - Original Informal\n   - Formal Reference\n   - Rule-based\n   - PBMT (Phrase-Based Machine Translation)\n   - NMT Baseline (Neural Machine Translation)\n   - NMT Copy\n   - NMT Combined\n\n2. **Evaluation Criteria**:\n   - Formality with scores from Human and PT16 assessments.\n   - Fluency with scores from Human evaluators and H14.\n   - Meaning with scores from Human evaluators and HE15.\n   - Combined scores from Human evaluators and Automatic (Auto) evaluation.\n\n3. **Overall Scores**:\n   - BLEU, which measures the match of n-grams in the predicted text against a reference text.\n   - TERp, a metric for evaluating translation error rate.\n   - PINC, which measures the level of paraphrasing.\n\nEach model is scored across these dimensions, with scores given as numerical values. The Original Informal and Formal Reference serve as baselines for comparison. Asterisks (*) denote that the score is statistically significant within the context of the table's evaluation criteria.\nThe image shows two graphs depicting the relationship between the original informal sentence length and two different scores: formality and meaning. Each graph includes multiple lines representing different models or references.\n\n**Top Graph (Formality Score):**\n- Y-axis: Formality Score\n- X-axis: Original Informal Sentence Length (grouped ranges)\n- Lines represent: \n  - Formal Reference\n  - Rule Based\n  - PBMT Combined\n  - NMT Combined\n  - Original Informal\n\n**Bottom Graph (Meaning Score):**\n- Y-axis: Meaning Score\n- X-axis: Original Informal Sentence Length (grouped ranges)\n- Lines represent:\n  - Rule Based\n  - PBMT Combined\n  - NMT Combined\n  - Formal Reference\n\nThe graphs illustrate how formality and meaning scores, as judged by humans, vary for different models at various sentence lengths.\nﬁcacy of the metrics ( § 6.3 ) and present a manual analysis ( § 6.4 ). We randomly select 500 sentences from each test set and run all ﬁve models. We use the entire train and tune split for training and tun- ing. We discuss results only on the E&M domain and list results on the F&R domain in the supple- mentary material. \nTable  4  shows the results for human    $\\S6.1$   and automatic    $\\S6.2$   evaluation of model rewrites. For all metrics except    $T E R p$  , a higher score is better. For each of the automatic metrics, we evaluate against four human references. The row  ‘Original Informal’  contains the scores when the original in- formal sentence is compared with the four formal reference rewrites. Comparing the model scores to this score helps us understand how closer are the model outputs to the formal reference rewrites compared to initial distance between the informal and the formal reference rewrite. \n\n6.1 Results using Human Judgments \nThe columns marked  ‘Human’  in Table  4  show the human judgments for the models on the three separate criteria of  formality ,  ﬂuency  and  mean- ing  collected using the process described in Sec- tion  5.1 .   The NMT Baseline and Copy models beat others on the formality axis by a signiﬁcant margin. Only the NMT Combined model achieves a statistically higher ﬂuency score when compared to the rule-based baseline model. As expected, the rule-based model is the most meaning preserving since it is the most conservative. Figure  3  shows the trend in the four leading models along  formal- ity  and  meaning  for varying lengths of the source sentence. NMT Combined beats PBMT on  for- mality  for shorter lengths whereas the trend re- verses as the length increases. PBMT generally preserves meaning more than the NMT Combined. We ﬁnd that the ﬂuency scores for all models de- creases as the sentence length increases which is similar to the trend generally observed with ma- chine translation based approaches. \nSince a good style transfer model is the one that attains a balanced score across all the three axes, we evaluate the models on a combination of these metrics 16   shown under the column  ‘Combined’  in Table  4 . NMT Combined is the only model having a combined score statistically greater than the rule- based approach. "}
{"page": 7, "image_path": "doc_images/N18-1012_7.jpg", "ocr_text": "Finally, Table 5 shows the overall rankings\nof the models from best to worst in both do-\nmains. PBMT and NMT Combined models beat\nthe rule-based model although not significantly in\nthe E&M domain but significantly in the F&R do-\nmain. Interestingly, the rule-based approach at-\ntains third place with a score significantly higher\nthan NMT Copy and NMT Baseline models. It is\nimportant to note here that while such a rule-based\napproach is relatively easy to craft for the formal-\nity style transfer task, the same may not be true for\nother styles like politeness or persuasiveness.\n\nE&M F&R\n\n(2.03*) Reference (2.13*) Reference\n(2.47) PBMT (2.38*) PBMT\n\n(2.48) NMT Combined | (2.38*) NMT Combined\n(2.54) Rule-based (2.56) Rule-based\n(3.03*) NMT Copy (2.72*) NMT Copy\n(3.03*) NMT Baseline | (2.79*) NMT Baseline\n\nTable 5: Ranking of different models on the informal\nto formal style transfer task. Rankings marked with *\nare significantly different from the rule-based ranking\nwith p < 0.001.\n\nAutomatic | Human | E&M | F&R\nFormality Formality | 0.47 0.45\nFluency Fluency 0.48 0.46\nMeaning Meaning 0.33 0.30\nBLEU Overall -0.48 | -0.43\nTERp Overall 0.31 0.30\nPINC Overall 0.11 0.08\n\nTable 6: Spearman rank correlation between automatic\nmetrics and human judgments. The first three metrics\nare correlated with their respective human judgments\nand the last three metrics are correlated with the overall\nranking human judgments. All correlations are statisti-\ncally significant with p < 0.001.\n\n6.2 Results with Automatic Metrics\n\nUnder automatic metrics, the formality and mean-\ning scores align with the human judgments with\nthe NMT Baseline and NMT Copy winning on for-\nmality and rule-based winning on meaning. The\nfluency score of the NMT Baseline is the highest\nin contrast to human judgments where the NMT\nCombined wins. This discrepancy could be due to\nH14 being trained on essays which contains sen-\ntences of a more formal genre compared to Ya-\nhoo Answers. In fact, the fluency classifier scores\nthe formal reference quite low as well. Under\noverall metrics, PBMT and NMT Combined mod-\nels beat other models as per BLEU (significantly)\nand TERp (not significantly). NMT Baseline and\nNMT copy win over other models as per PINC\n\n136\n\nwhich can be explained by the fact that PINC\nmeasures lexical dissimilarity with the source and\nNMT models tend towards making more changes.\nAlthough such an analysis is useful, for a more\nthorough understanding of these metrics, we next\nlook at their correlation with human judgments.\n\n6.3 Metric Correlation\n\nWe report the spearman rank correlation co-\nefficient between automatic metrics and human\njudgments in Table 6. For formality, fluency and\nmeaning, the correlation is with their respective\nhuman judgments whereas for BLEU, TERp and\nPINC, the correlation is with the overall ranking.\n\nWe see that the formality and the fluency met-\nrics correlate moderately well while the mean-\ning metric correlates comparatively poorly. To\nbe fair, the HEI5 classifier was trained on the\nSTS dataset which contains more formal writ-\ning than informal. BLEU correlates moderately\nwell (better than what XU12 observed for the\nShakespeare task) whereas the correlation drops\nfor TERp. PINC, on the other hand, correlates\nvery poorly with a positive correlation with rank\nwhen it should have a negative correlation with\nrank, just like BLEU. This sheds light on the fact\nthat PINC, on its own, is not a good metric for\nstyle transfer since it prefers lexical edits at the\ncost of meaning changes. In the Shakespeare task,\nXU12 did observe a higher correlation with PINC\n(0.41) although the correlation was not with over-\nall system ranking but rather only on the style met-\nric. Moreover, in the Shakespeare task, changing\nthe text is more favorable than in formality.\n\n6.4 Manual Analysis\n\nThe prior evaluations reveal the relative perfor-\nmance differences between approaches. Here, we\nidentify trends per and between approaches. We\nsample 50 informal sentences total from both do-\nmains and then analyze the outputs from each\nmodel. We present sample sentences in Table 7.\nThe NMT Baseline and NMT Copy tend to\nhave the most variance in their performance. This\nis likely due to the fact that they are trained on\nonly 50K sentence pairs, whereas the other mod-\nels are trained on much more data. For shorter sen-\nences, these models make some nice formal trans-\normations like from ‘very dumb’ to ‘very fool-\nish’. However, for longer sentences, these models\nmake drastic meaning changes and drop some con-\nent altogether (see examples in Table 7). On the\n\n", "vlm_text": "Finally, Table  5  shows the overall rankings of the models from best to worst in both do- mains. PBMT and NMT Combined models beat the rule-based model although not signiﬁcantly in the E&M domain but signiﬁcantly in the F&R do- main. Interestingly, the rule-based approach at- tains third place with a score signiﬁcantly higher than NMT Copy and NMT Baseline models. It is important to note here that while such a rule-based approach is relatively easy to craft for the formal- ity style transfer task, the same may not be true for other styles like politeness or persuasiveness. \nThe table presents results related to two conditions or groups labeled as \"E&M\" and \"F&R.\" Each column includes a list of methods or systems and an associated numerical value in parentheses, followed by an asterisk (*). Here’s the breakdown:\n\n**E&M Column:**\n- Reference: (2.03*)\n- PBMT: (2.47)\n- NMT Combined: (2.48)\n- Rule-based: (2.54)\n- NMT Copy: (3.03*)\n- NMT Baseline: (3.03*)\n\n**F&R Column:**\n- Reference: (2.13*)\n- PBMT: (2.38*)\n- NMT Combined: (2.38*)\n- Rule-based: (2.56)\n- NMT Copy: (2.72*)\n- NMT Baseline: (2.79*)\n\nThe values in parentheses likely represent scores or metrics related to each specific method or system, and the asterisks may indicate statistical significance or noteworthy results, although the specific meaning is not provided.\nTable 5: Ranking of different models on the  informal to  formal  style transfer task. Rankings marked with \\* are signiﬁcantly different from the rule-based ranking with    $p<0.001$  . \nThis table contains data comparing the performance of automatic and human evaluations of text along several dimensions, specifically focusing on formal respectability. It presents correlation values between automatic metrics and human judgments relating to different aspects of text quality. The table is structured as follows:\n\n- The first column lists the type of evaluation metric used, which are either “Automatic” or standard evaluation metrics for language processing: \n  - Formality\n  - Fluency\n  - Meaning\n  - BLEU\n  - TERp\n  - PINC\n\n- The second column labeled \"Human\" describes the corresponding human-rated categories of evaluation, denoted in italics:\n  - Formality\n  - Fluency\n  - Meaning\n  - Overall (for BLEU, TERp, and PINC)\n\n- The \"E&M\" (presumably “Edit and Model”) column shows correlation values between automatic metrics and human judgment for this specific evaluation setting.\n  - Formality: 0.47\n  - Fluency: 0.48\n  - Meaning: 0.33\n  - BLEU: -0.48\n  - TERp: 0.31\n  - PINC: 0.11\n\n- The “F&R” (presumably “Formality and Respect”) column also shows correlation values between automatic metrics and human judgment for a different evaluation setting.\n  - Formality: 0.45\n  - Fluency: 0.46\n  - Meaning: 0.30\n  - BLEU: -0.43\n  - TERp: 0.30\n  - PINC: 0.08\n\nOverall, the table demonstrates the varying degrees of correlation between automated scoring and human evaluation across different aspects and methods of evaluation of text quality. Positive values indicate positive correlation, and negative values indicate a negative correlation.\nTable 6: Spearman rank correlation between automatic metrics and human judgments. The ﬁrst three metrics are correlated with their respective human judgments and the last three metrics are correlated with the  overall ranking  human judgments. All correlations are statisti- cally signiﬁcant with    $p<0.001$  . \n6.2 Results with Automatic Metrics \nUnder automatic metrics, the formality and mean- ing scores align with the human judgments with the NMT Baseline and NMT Copy winning on for- mality and rule-based winning on meaning. The ﬂuency score of the NMT Baseline is the highest in contrast to human judgments where the NMT Combined wins. This discrepancy could be due to H14 being trained on  essays  which contains sen- tences of a more formal genre compared to Ya- hoo Answers. In fact, the ﬂuency classiﬁer scores the formal reference quite low as well. Under overall metrics, PBMT and NMT Combined mod- els beat other models as per BLEU (signiﬁcantly) and TERp (not signiﬁcantly). NMT Baseline and NMT copy win over other models as per PINC which can be explained by the fact that PINC measures lexical dissimilarity with the source and NMT models tend towards making more changes. Although such an analysis is useful, for a more thorough understanding of these metrics, we next look at their correlation with human judgments. \n\n6.3 Metric Correlation \nWe report the spearman rank correlation co- efﬁcient between automatic metrics and human judgments in Table  6 . For  formality ,  ﬂuency  and meaning , the correlation is with their respective human judgments whereas for BLEU, TERp and PINC, the correlation is with the overall ranking. \nWe see that the formality and the ﬂuency met- rics correlate moderately well while the mean- ing metric correlates comparatively poorly. To be fair, the H E 15 classiﬁer was trained on the STS dataset which contains more formal writ- ing than informal. BLEU correlates moderately well (better than what X U 12 observed for the Shakespeare task) whereas the correlation drops for TERp. PINC, on the other hand, correlates very poorly with a positive correlation with rank when it should have a negative correlation with rank, just like BLEU. This sheds light on the fact that PINC, on its own, is not a good metric for style transfer since it prefers lexical edits at the cost of meaning changes. In the Shakespeare task, X U 12 did observe a higher correlation with PINC (0.41) although the correlation was not with over- all system ranking but rather only on the style met- ric. Moreover, in the Shakespeare task, changing the text is more favorable than in formality. \n6.4 Manual Analysis \nThe prior evaluations reveal the relative perfor- mance differences between approaches. Here, we identify trends per and between approaches. We sample 50 informal sentences total from both do- mains and then analyze the outputs from each model. We present sample sentences in Table  7 . \nThe NMT Baseline and NMT Copy tend to have the most variance in their performance. This is likely due to the fact that they are trained on only 50K sentence pairs, whereas the other mod- els are trained on much more data. For shorter sen- tences, these models make some nice formal trans- formations like from ‘ very dumb ’ to ‘ very fool- ish ’. However, for longer sentences, these models make drastic meaning changes and drop some con- tent altogether (see examples in Table  7 ). On the "}
{"page": 8, "image_path": "doc_images/N18-1012_8.jpg", "ocr_text": "Entertainment & Music\n\nOriginal Informal\nReference Formal\n\nWow , [am very dumb in my observation skills ......\nIdo not have good observation skills .\n\nRule-based\nPBMT\n\nNMT Baseline\nNMT Copy\nNMT Combined\n\nWow , am very dumb in my observation skills .\nWow , Iam very dumb in my observation skills .\nTam very foolish in my observation skills .\n\nWow , Iam very foolish in my observation skills .\nTam very unintelligent in my observation skills .\n\nFamily & Relationship\n\nOriginal Informal\nReference Formal\n\ni hardly everrr see him in school either usually i see hima t my brothers basketball games .\nThardly ever see him in school . I usually see him with my brothers playing basketball .\n\nRule-based\n\nThardly everrr see him in school either usually I see hima t my brothers basketball games .\n\nPBMT\n\nNMT\n\nNMT Copy\nNMT Combined\n\nThardly see him in school as well, but my brothers basketball games .\n\nIrarely see him in school , either I see him at my brother ’s basketball games .\nThardly see him in school either , usually I see him at my brother ’s basketball games .\nIrarely see him in school either usually I see him at my brothers basketball games .\n\nTable 7: Sample model outputs with references from both E&M and F&R domains on the informal to formal task\n\nother hand, the PBMT and NMT Combined mod-\nels have lower variance in their performance. They\nmake changes more conservatively but when they\ndo, they are usually correct. Thus, most of the out-\nputs from these two models are usually meaning\npreserving but at the expense of a lower formality\nscore improvement.\n\nIn most examples, all models are good at re-\nmoving very informal words like ‘stupid’, ‘idiot’\nand ‘hell’, with PBMT and NMT Combined mod-\nels doing slightly better. All models struggle when\nthe original sentence is very informal or disfluent.\nThey all also struggle with sentence completions\nthat humans seem to be very good at. This might\nbe because humans assume a context when absent,\nwhereas the models do not. Unknown tokens, ei-\nther real words or misspelled words, tend to wreak\nhavoc on all approaches. In most cases, the models\nsimply did not transform that section of the sen-\ntence, or remove the unknown tokens. Most mod-\nels are effective at low-level changes such as writ-\ning out numbers, inserting commas, and removing\ncommon informal phrases.\n\n7 Conclusions and Future Work\n\nThe goal of this paper was to move the field of\nstyle transfer forward by creating a large training\nand evaluation corpus to be made public, showing\nthat adapting MT techniques to this task can serve\nas strong baselines for future work, and analyzing\nthe usefulness of existing metrics for overall style\ntransfer as well as three specific criteria of auto-\nmatic style transfer evaluation. We view this work\nas rigorously expanding on the foundation set by\nXu12 five years earlier. It is our hope that with a\ncommon test set, the field can finally benchmark\n\n137\n\napproaches which do not require parallel data.\n\nWe found that while the NMT systems perform\nwell given automatic metrics, humans had a slight\npreference for the PBMT approach. That being\nsaid, two of the neural approaches (NMT Base-\nline and Copy) often made successful changes\nand larger rewrites that the other models could\nnot. However, this often came at the expense of\na meaning change.\n\nWe also introduced new metrics and vetted all\nmetrics using comparison with human judgments.\nWe found that previously-used metrics did not cor-\nrelate well with human judgments, and thus should\nbe avoided in system development or final eval-\nuation. The formality and fluency metrics corre-\nlated best and we believe that some combination\nof these metrics with others would be the best next\nstep in the development of style transfer metrics.\nSuch a metric could then in turn be used to opti-\nmize MT models. Finally, in this work we focused\non one particular style, formality. The long term\ngoal is to generalize the methods and metrics to\nany style.\n\nAcknowledgments\n\nThe authors would like to thank Yahoo Research\nfor making their data available. The authors would\nalso like to thank Junchao Zheng and Claudia\nLeacock for their help in the data creation pro-\ncess, Courtney Napoles for providing the fluency\nscores, Marcin Junczys-Dowmunt, Rico Sennrich,\nEllie Pavlick, Maksym Bezva, Dimitrios Alikan-\niotis and Kyunghyun Cho for helpful discussion\nand the three anonymous reviewers for their use-\nful comments and suggestions.\n", "vlm_text": "The table appears to present various methods or systems for transforming an informal sentence into a more formal one. It is categorized under \"Entertainment & Music,\" possibly indicating the context or theme of the sentences.\n\nHere is a breakdown of the content:\n\n- **Original Informal**: \"Wow, I am very dumb in my observation skills......\"\n- **Reference Formal**: \"I do not have good observation skills.\"\n\nThe table then provides different techniques or models and their formalization attempts:\n\n- **Rule-based**: \"Wow, I am very dumb in my observation skills.\"\n- **PBMT (Phrase-Based Machine Translation)**: \"Wow, I am very dumb in my observation skills.\"\n- **NMT Baseline (Neural Machine Translation Baseline)**: \"I am very foolish in my observation skills.\"\n- **NMT Copy**: \"Wow, I am very foolish in my observation skills.\"\n- **NMT Combined**: \"I am very unintelligent in my observation skills.\"\n\nThe table demonstrates various automated or algorithmic approaches to altering the informal text to be more formal, showcasing different linguistic transformations.\nThe table is displaying different methods of transforming a sentence from informal to formal. It provides several outputs from different approaches:\n\n1. **Original Informal**: The initial informal sentence is \"i hardly everrr see him in school either usually i see hima t my brothers basketball games.\"\n\n2. **Reference Formal**: The target formal version of the sentence is \"I hardly ever see him in school. I usually see him with my brothers playing basketball.\"\n\n3. **Rule-based**: This method gives the output \"I hardly everrr see him in school either usually I see hima t my brothers basketball games.\"\n\n4. **PBMT (Phrase-Based Machine Translation)**: Produces \"I hardly see him in school as well, but my brothers basketball games.\"\n\n5. **NMT (Neural Machine Translation)**: Outputs \"I rarely see him in school, either I see him at my brother’s basketball games.\"\n\n6. **NMT Copy**: Gives the result \"I hardly see him in school either, usually I see him at my brother’s basketball games.\"\n\n7. **NMT Combined**: Produces \"I rarely see him in school either usually I see him at my brothers basketball games.\"\n\nEach row shows how different formalization methods approach the translation of the informal sentence into a formal version.\nother hand, the PBMT and NMT Combined mod- els have lower variance in their performance. They make changes more conservatively but when they do, they are usually correct. Thus, most of the out- puts from these two models are usually meaning preserving but at the expense of a lower formality score improvement. \nIn most examples, all models are good at re- moving very informal words like ‘ stupid ’, ‘ idiot ’ and ‘ hell ’, with PBMT and NMT Combined mod- els doing slightly better. All models struggle when the original sentence is very informal or disﬂuent. They all also struggle with sentence completions that humans seem to be very good at. This might be because humans assume a context when absent, whereas the models do not. Unknown tokens, ei- ther real words or misspelled words, tend to wreak havoc on all approaches. In most cases, the models simply did not transform that section of the sen- tence, or remove the unknown tokens. Most mod- els are effective at low-level changes such as writ- ing out numbers, inserting commas, and removing common informal phrases. \n7 Conclusions and Future Work \nThe goal of this paper was to move the ﬁeld of style transfer forward by creating a large training and evaluation corpus to be made public, showing that adapting MT techniques to this task can serve as strong baselines for future work, and analyzing the usefulness of existing metrics for overall style transfer as well as three speciﬁc criteria of auto- matic style transfer evaluation. We view this work as rigorously expanding on the foundation set by X U 12 ﬁve years earlier. It is our hope that with a common test set, the ﬁeld can ﬁnally benchmark approaches which do not require parallel data. \n\nWe found that while the NMT systems perform well given automatic metrics, humans had a slight preference for the PBMT approach. That being said, two of the neural approaches (NMT Base- line and Copy) often made successful changes and larger rewrites that the other models could not. However, this often came at the expense of a meaning change. \nWe also introduced new metrics and vetted all metrics using comparison with human judgments. We found that previously-used metrics did not cor- relate well with human judgments, and thus should be avoided in system development or ﬁnal eval- uation. The formality and ﬂuency metrics corre- lated best and we believe that some combination of these metrics with others would be the best next step in the development of style transfer metrics. Such a metric could then in turn be used to opti- mize MT models. Finally, in this work we focused on one particular style, formality. The long term goal is to generalize the methods and metrics to any style. \nAcknowledgments \nThe authors would like to thank Yahoo Research for making their data available. The authors would also like to thank Junchao Zheng and Claudia Leacock for their help in the data creation pro- cess, Courtney Napoles for providing the ﬂuency scores, Marcin Junczys-Dowmunt, Rico Sennrich, Ellie Pavlick, Maksym Bezva, Dimitrios Alikan- iotis and Kyunghyun Cho for helpful discussion and the three anonymous reviewers for their use- ful comments and suggestions. "}
{"page": 9, "image_path": "doc_images/N18-1012_9.jpg", "ocr_text": "References\n\nEneko Agirre, Carmen Banea, Daniel M Cer, Mona T\nDiab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-\nman Rigau, and Janyce Wiebe. 2016. Semeval-\n2016 task 1: Semantic textual similarity, monolin-\ngual and cross-lingual evaluation. In SemEval@\nNAACL-HLT. pages 497-511.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv: 1409.0473 .\n\nJulian Brooke and Graeme Hirst. 2014. Supervised\nranking of co-occurrence profiles for acquisition of\ncontinuous lexical attributes. In COLING. pages\n2172-2183.\n\nJulian Brooke, Tong Wang, and Graeme Hirst. 2010.\nAutomatic acquisition of lexical formality. In Pro-\nceedings of the 23rd International Conference on\nComputational Linguistics: Posters. Association for\nComputational Linguistics, pages 90-98.\n\nChris Callison-Burch. 2008. Syntactic constraints on\nparaphrases extracted from parallel corpora. In Pro-\nceedings of the 2008 Conference on Empirical Meth-\nods in Natural Language Processing. Association\nfor Computational Linguistics, Honolulu, Hawaii,\npages 196-205. http://www.aclweb.org/\nanthology/D08-1021.\n\nDavid L Chen and William B Dolan. 2011. Collect-\ning highly parallel data for paraphrase evaluation. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies-Volume 1. Association for Com-\nputational Linguistics, pages 190-200.\n\nKyunghyun Cho, Bart van Merriénboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder—decoder ap-\nproaches. Syntax, Semantics and Structure in Statis-\ntical Translation page 103.\n\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language genera-\ntion. Proceedings of the Workshop on Stylistic Vari-\nation, EMNLP 2017 .\n\nHua He, Kevin Gimpel, and Jimmy J Lin. 2015.\nMulti-perspective sentence similarity modeling with\nconvolutional neural networks. In EMNLP. pages\n1576-1586.\n\nKenneth Heafield, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable\nmodified Kneser-Ney language model  esti-\nmation. In Proceedings of the 51st Annual\nMeeting of the Association for Computational\nLinguistics. Sofia, Bulgaria, pages 690-696.\nhttps://kheafield.com/papers/\nedinburgh/estimate_paper.pdf.\n\nMichael Heilman, Aoife Cahill, Nitin Madnani,\nMelissa Lopez, Matthew Mulholland, and Joel\nTetreault. 2014. Predicting grammaticality on an\nordinal scale. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers). Association for\nComputational Linguistics, Baltimore, Maryland,\npages 174-180. http://www.aclweb.org/\nanthology/P14-2029.\n\nFrancis Heylighen and Jean-Marc Dewaele. 1999. For-\nmality of language: definition, measurement and be-\nhavioral determinants. Interner Bericht, Center Leo\nApostel, Vrije Universiteit Briissel .\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997.\nLong short-term memory. Neural computation\n9(8):1735-1780.\n\nEduard Hovy. 1987. Generating natural language un-\nder pragmatic constraints. Journal of Pragmatics\n11(6):689-719.\n\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward con-\ntrolled generation of text. In International Confer-\nence on Machine Learning. pages 1587-1596.\n\nHarsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric\nNyberg. 2017. Shakespearizing modern language\nusing copy-enriched sequence-to-sequence models.\nProceedings of the Workshop on Stylistic Variation,\nEMNLP 2017 pages 10-19.\n\nTomoyuki Kajiwara and Mamoru Komachi. 2016.\nBuilding a monolingual parallel corpus for text sim-\nplification using sentence similarity based on align-\nment between word embeddings. In Proceedings\nof COLING 2016, the 26th International Confer-\nence on Computational Linguistics: Technical Pa-\npers. pages 1147-1158.\n\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\ntoolkit for statistical machine translation. In Pro-\nceedings of the 45th annual meeting of the ACL on\ninteractive poster and demonstration sessions. As-\nsociation for Computational Linguistics, pages 177—\n180.\n\nShibamouli Lahiri, Prasenjit Mitra, and Xiaofei Lu.\n2011. Informality judgment at sentence level and\nexperiments with formality score. In International\nConference on Intelligent Text Processing and Com-\nputational Linguistics. Springer, pages 446-457.\n\nRobert C Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 conference short papers.\nAssociation for Computational Linguistics, pages\n220-224.\n", "vlm_text": "References \nEneko Agirre, Carmen Banea, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger- man Rigau, and Janyce Wiebe. 2016. Semeval- 2016 task 1: Semantic textual similarity, monolin- gual and cross-lingual evaluation. In  SemEval@ NAACL-HLT . pages 497–511. \nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473  . \nJulian Brooke and Graeme Hirst. 2014. Supervised ranking of co-occurrence proﬁles for acquisition of continuous lexical attributes. In  COLING . pages 2172–2183. \nJulian Brooke, Tong Wang, and Graeme Hirst. 2010. Automatic acquisition of lexical formality. In  Pro- ceedings of the   $23r d$   International Conference on Computational Linguistics: Posters . Association for Computational Linguistics, pages 90–98. \nChris Callison-Burch. 2008.  Syntactic constraints on paraphrases extracted from parallel corpora . In  Pro- ceedings of the 2008 Conference on Empirical Meth- ods in Natural Language Processing . Association for Computational Linguistics, Honolulu, Hawaii, pages 196–205. http://www.aclweb.org/ anthology/D08-1021 . \nDavid L Chen and William B Dolan. 2011. Collect- ing highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies-Volume 1 . Association for Com- putational Linguistics, pages 190–200. \nKyunghyun Cho, Bart van Merri¨ enboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder–decoder ap- proaches.  Syntax, Semantics and Structure in Statis- tical Translation  page 103. \nJessica Ficler and Yoav Goldberg. 2017. Controlling linguistic style aspects in neural language genera- tion.  Proceedings of the Workshop on Stylistic Vari- ation, EMNLP 2017  . \nHua He, Kevin Gimpel, and Jimmy J Lin. 2015. Multi-perspective sentence similarity modeling with convolutional neural networks. In  EMNLP . pages 1576–1586. \nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modiﬁed Kneser-Ney language model esti- mation . In  Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics . Soﬁa, Bulgaria, pages 690–696. https://kheafield.com/papers/ edinburgh/estimate_paper.pdf . \nMelissa Lopez, Matthew Mulholland, and Joel Tetreault. 2014. Predicting grammaticality on an ordinal scale . In  Proceedings of the 52nd Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers) . Association for Computational Linguistics, Baltimore, Maryland, pages 174–180. http://www.aclweb.org/ anthology/P14-2029 . Francis Heylighen and Jean-Marc Dewaele. 1999. For- mality of language: deﬁnition, measurement and be- havioral determinants.  Interner Bericht, Center Leo Apostel, Vrije Universiteit Br¨ ussel  . Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735–1780. Eduard Hovy. 1987. Generating natural language un- der pragmatic constraints. Journal of Pragmatics 11(6):689–719. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017. Toward con- trolled generation of text. In  International Confer- ence on Machine Learning . pages 1587–1596. Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric Nyberg. 2017. Shakespearizing modern language using copy-enriched sequence-to-sequence models. Proceedings of the Workshop on Stylistic Variation, EMNLP 2017  pages 10–19. Tomoyuki Kajiwara and Mamoru Komachi. 2016. Building a monolingual parallel corpus for text sim- pliﬁcation using sentence similarity based on align- ment between word embeddings. In  Proceedings of COLING 2016, the 26th International Confer- ence on Computational Linguistics: Technical Pa- pers . pages 1147–1158. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In  Pro- ceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions . As- sociation for Computational Linguistics, pages 177– 180. Shibamouli Lahiri, Prasenjit Mitra, and Xiaofei Lu. 2011. Informality judgment at sentence level and experiments with formality score. In  International Conference on Intelligent Text Processing and Com- putational Linguistics . Springer, pages 446–457. Robert C Moore and William Lewis. 2010. Intelligent selection of language model training data. In  Pro- ceedings of the ACL 2010 conference short papers . Association for Computational Linguistics, pages 220–224. "}
{"page": 10, "image_path": "doc_images/N18-1012_10.jpg", "ocr_text": "Alejandro Mosquera and Paloma Moreda. 2012.\nSmile: An informality classification tool for help-\ning to assess quality and credibility in web 2.0 texts.\nIn Proceedings of the ICWSM workshop: Real-Time\nAnalysis and Mining of Social Streams (RAMSS).\n\nCourtney Napoles, Keisuke Sakaguchi, and Joel\nTetreault. 2016. There’s no comparison: Reference-\nless evaluation metrics in grammatical error correc-\ntion. In Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, Austin,\nTexas, pages 2109-2115. https://aclweb.\norg/anthology/D16-1228.\n\nXing Niu, Marianna Martindale, and Marine Carpuat.\n2017. A study of style in machine translation: Con-\ntrolling the formality of machine translation output.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing. pages\n2804-2809.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics. Association for Computational\nLinguistics, pages 311-318.\n\nEllie Pavlick and Ani Nenkova. 2015. Inducing lexical\nstyle properties for paraphrase and genre differenti-\nation. In HLT-NAACL. pages 218-224.\n\nEllie Pavlick and Joel Tetreault. 2016. An empiri-\ncal analysis of formality in online communication.\nTransactions of the Association for Computational\nLinguistics 4:61-74.\n\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP). pages 1532-1543.\n\nKelly Peterson, Matt Hohensee, and Fei Xia. 2011.\nEmail formality in the workplace: A case study\non the enron corpus. In Proceedings of the Work-\nshop on Languages in Social Media. Association for\nComputational Linguistics, pages 86-95.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Controlling politeness in neural machine\ntranslation via side constraints. In HLT-NAACL.\npages 35-40.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Edinburgh neural machine translation sys-\ntems for wmt 16. In Proceedings of the First\nConference on Machine Translation. Association\nfor Computational Linguistics, Berlin, Germany,\npages 371-376. http://www.aclweb.org/\nanthology/W16-2323.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016c. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n\n139\n\n54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers).\nAssociation for Computational Linguistics, pages\n86-96. https://doi.org/10.18653/v1/\nP16-1009.\n\nFadi Abu Sheikha and Diana Inkpen. 2010. Automatic\nclassification of documents by formality. In Natu-\nral Language Processing and Knowledge Engineer-\ning (NLP-KE), 2010 International Conference on.\nIEEE, pages 1-S.\n\nFadi Abu Sheikha and Diana Inkpen. 2011. Generation\nof formal and informal sentences. In Proceedings of\nthe 13th European Workshop on Natural Language\nGeneration. Association for Computational Linguis-\ntics, pages 187-193.\n\nMatthew Snover, Nitin Madnani, Bonnie J Dorr, and\nRichard Schwartz. 2009. Fluency, adequacy, or\nhter?: exploring different human judgments with a\ntunable mt metric. In Proceedings of the Fourth\nWorkshop on Statistical Machine Translation. Asso-\nciation for Computational Linguistics, pages 259-\n268.\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems. pages 3104-3112.\n\nNicola Ueffing. 2006. Self-training for machine trans-\nlation. In NIPS workshop on Machine Learning for\nMultilingual Information Access.\n\nTong Wang, Ping Chen, John Rochford, and Jipeng\nQiang. 2016. Text simplification using neural ma-\nchine translation. In AAAI. pages 4270-4271.\n\nSander Wubben, Antal Van Den Bosch, and Emiel\n\nKrahmer. 2012a. Sentence simplification by mono-\ningual machine translation. In Proceedings of the\n50th Annual Meeting of the Association for Compu-\ntational Linguistics: Long Papers-Volume 1. Asso-\nciation for Computational Linguistics, pages 1015—\n024.\n\nSander Wubben, Antal van den Bosch, and Emiel\nKrahmer. 2012b. Sentence simplification by mono-\ningual machine translation. In Proceedings of the\n50th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers).\nAssociation for Computational Linguistics, Jeju Is-\nland, Korea, pages 1015-1024. http://www.\naclweb.org/anthology/P12-1107.\n\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze\nChen, and Chris Callison-Burch. 2016. Optimizing\nstatistical machine translation for text simplification.\nTransactions of the Association for Computational\nLinguistics 4:401-415.\n\nWei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and\nColin Cherry. 2012. Paraphrasing for style. Pro-\nceedings of COLING 2012 pages 2899-2914.\n", "vlm_text": "Smile: An informality classiﬁcation tool for help- ing to assess quality and credibility in web 2.0 texts. In  Proceedings of the ICWSM workshop: Real-Time Analysis and Mining of Social Streams (RAMSS) . Courtney Napoles, Keisuke Sakaguchi, and Joel Tetreault. 2016.  There’s no comparison: Reference- less evaluation metrics in grammatical error correc- tion . In  Proceedings of the 2016 Conference on Em- pirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin, Texas, pages 2109–2115. https://aclweb. org/anthology/D16-1228 . Xing Niu, Marianna Martindale, and Marine Carpuat. 2017. A study of style in machine translation: Con- trolling the formality of machine translation output. In  Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing . pages 2804–2809. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In  Proceedings of the 40th annual meeting on association for compu- tational linguistics . Association for Computational Linguistics, pages 311–318. Ellie Pavlick and Ani Nenkova. 2015. Inducing lexical style properties for paraphrase and genre differenti- ation. In  HLT-NAACL . pages 218–224. Ellie Pavlick and Joel Tetreault. 2016. An empiri- cal analysis of formality in online communication. Transactions of the Association for Computational Linguistics 4:61–74.Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In  Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP) . pages 1532–1543. Kelly Peterson, Matt Hohensee, and Fei Xia. 2011. Email formality in the workplace: A case study on the enron corpus. In  Proceedings of the Work- shop on Languages in Social Media . Association for Computational Linguistics, pages 86–95. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Controlling politeness in neural machine translation via side constraints. In  HLT-NAACL . pages 35–40. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b.  Edinburgh neural machine translation sys- tems for wmt 16 . In  Proceedings of the First Conference on Machine Translation . Association for Computational Linguistics, Berlin, Germany, pages 371–376. http://www.aclweb.org/ anthology/W16-2323 . Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016c.  Improving neural machine translation mod- els with monolingual data . In  Proceedings of the \n54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, pages 86–96. https://doi.org/10.18653/v1/ P16-1009 . Fadi Abu Sheikha and Diana Inkpen. 2010. Automatic classiﬁcation of documents by formality. In  Natu- ral Language Processing and Knowledge Engineer- ing (NLP-KE), 2010 International Conference on . IEEE, pages 1–5. Fadi Abu Sheikha and Diana Inkpen. 2011. Generation of formal and informal sentences. In  Proceedings of the 13th European Workshop on Natural Language Generation . Association for Computational Linguis- tics, pages 187–193. Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric. In  Proceedings of the Fourth Workshop on Statistical Machine Translation . Asso- ciation for Computational Linguistics, pages 259– 268. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In  Advances in neural information process- ing systems . pages 3104–3112. Nicola Uefﬁng. 2006. Self-training for machine trans- lation. In  NIPS workshop on Machine Learning for Multilingual Information Access . Tong Wang, Ping Chen, John Rochford, and Jipeng Qiang. 2016. Text simpliﬁcation using neural ma- chine translation. In  AAAI . pages 4270–4271. Sander Wubben, Antal Van Den Bosch, and Emiel Krahmer. 2012a. Sentence simpliﬁcation by mono- lingual machine translation. In  Proceedings of the 50th Annual Meeting of the Association for Compu- tational Linguistics: Long Papers-Volume 1 . Asso- ciation for Computational Linguistics, pages 1015– 1024. Sander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012b.  Sentence simpliﬁcation by mono- lingual machine translation . In  Proceedings of the 50th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Jeju Is- land, Korea, pages 1015–1024. http://www. aclweb.org/anthology/P12-1107 . Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Optimizing statistical machine translation for text simpliﬁcation. Transactions of the Association for Computational Linguistics  4:401–415. Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. 2012. Paraphrasing for style.  Pro- ceedings of COLING 2012  pages 2899–2914. "}
{"page": 11, "image_path": "doc_images/N18-1012_11.jpg", "ocr_text": "Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.\n2010. A monolingual tree-based translation model\nfor sentence simplification. In Proceedings of the\n23rd international conference on computational lin-\nguistics. Association for Computational Linguistics,\npages 1353-1361.\n\n140\n", "vlm_text": "Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simpliﬁcation. In  Proceedings of the 23rd international conference on computational lin- guistics . Association for Computational Linguistics, pages 1353–1361. "}
