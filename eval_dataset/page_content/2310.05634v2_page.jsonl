{"page": 0, "image_path": "doc_images/2310.05634v2_0.jpg", "ocr_text": "Towards Verifiable Generation: A Benchmark for Knowledge-aware\nLanguage Model Attribution\n\nXinze Li’, Yixin Cao’', Liangming Pan*, Yubo Ma’, Aixin Sun'*\n\n1 §-Lab, Nanyang Technological University\n? Singapore Management University * University of California, Santa Barbara\n\n{xinze002,\nyxcao@smu.edu.sg\n\nAbstract\n\nAlthough achieving great success, Large Lan-\nguage Models (LLMs) usually suffer from un-\nreliable hallucinations. Although language at-\ntribution can be a potential solution, there are\nno suitable benchmarks and evaluation metrics\nto attribute LLMs to structured knowledge. In\nthis paper, we define a new task of Knowledge-\naware Language Model Attribution (KaLMA)\nthat improves upon three core concerns with\nconventional attributed LMs. First, we extend\nattribution source from unstructured texts to\nKnowledge Graph (KG), whose rich structures\nbenefit both the attribution performance and\nworking scenarios. Second, we propose a new\n“Conscious Incompetence\" setting considering\nthe incomplete knowledge repository, where\nthe model identifies the need for supporting\nknowledge beyond the provided KG. Third, we\npropose a comprehensive automatic evaluation\nmetric encompassing text quality, citation qual-\nity, and text citation alignment. To implement\nthe above innovations, we build a dataset in\nbiography domain BiokKaLMA via evolution-\nary question generation strategy, to control the\nquestion complexity and necessary knowledge\nto the answer. For evaluation, we develop a\nbaseline solution and demonstrate the room for\nimprovement in LLMs’ citation generation, em-\nphasizing the importance of incorporating the\n\"Conscious Incompetence\" setting, and the crit-\nical role of retrieval accuracy.\n\n1 Introduction\n\nRecently, Large Language Models (Brown et al.,\n2020) (LLMs) have exhibited great capability in\nopen-ended question answering (Yang et al., 2019).\nHowever, the generated answers may include fac-\ntual errors and are not always reliable, and is com-\nmonly known as the “hallucination” (Shuster et al.,\n2021; Ji et al., 2023) problem. For instance, LLMs\nmay give wrong diagnosis to patient’s symptoms.\nHallucination has severe harms especially on indus-\n\nyubo001}@e.ntu.edu.sg\n\naxsun@ntu.edu.sg\nliangmingpan@ucsb.edu\n\nQuestion: How did Orazio Gentileschi's influence on Artemisia's life\nand career shape her development as a Baroque painter?\n\n=> Retrieve\nKnowledge O\nGraph\n\n1028181\nPainter 037853\n\nBaroque\n\nOrazio Artemisia\nGentileschi Gentileschi\n0367560 T C 0212657\nItaly\n, bd) oe\n\nAttribute\n\n(Output: Artemisia Gentileschi was an Italian painter born on July 8, )\n\n1596 [NA] in Rome [Q212657, citizen: Italy, occupation: painter,\nplace of birth: Rome]. She was a member of ...\n\nXx\nFigure 1: A demonstration of our task set up. Given a\nquestion, the system generates answers attributed from\naretrieved knowledge graph. The underlines in question\nare the retrieved entities, and the underlines in outputs\nare the citations. [NA] is the “Not Applicable Citation”.\n\ntries that require precision and factual knowledge\nlike finance, law, and medical treatment.\n\nTo minimize the negative impacts, researchers\nhave proposed the task of language attribu-\ntion (Bohnet et al., 2023), which not only enables\nusers to verify the generated text flexibly but also\ncontributes to many important applications, such\nas situation reports (Reddy et al., 2023), academic\npapers (Salvagno et al., 2023), medical diagno-\nsis (Zuccon and Koopman, 2023).Existing works\nmainly attribute generated outputs to unstructured\ndocuments like web pages (Nakano et al., 2021;\nMenick et al., 2022) or passages (Gao et al., 2023).\nTo verify the answer quality, they typically compare\nwith a human annotated reference answer for auto-\nmatic evaluation or conduct human evaluation. We\nargue that there are several concerns on such task\ndefinition. Firstly, are documents the only source\n", "vlm_text": "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution \nXinze  $\\mathbf{Li^{1}}$  , Yixin  $\\mathbf{C}\\mathbf{a}\\mathbf{o}^{2\\dagger}$  , Liangming  $\\mathbf{P}\\mathbf{a}\\mathbf{n}^{3}$  , Yubo  $\\mathbf{M}\\mathbf{a}^{1}$  , Aixin Sun 1 † \n1  S-Lab, Nanyang Technological University 2  Singapore Management University  3  University of California, Santa Barbara {xinze002, yubo001}@e.ntu.edu.sg axsun@ntu.edu.sg yxcao@smu.edu.sg liang ming pan@ucsb.edu \nAbstract \nAlthough achieving great success, Large Lan- guage Models (LLMs) usually suffer from un- reliable hallucinations. Although language at- tribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge- aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new “Conscious Incompetence\" setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation qual- ity, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolution- ary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs’ citation generation, em- phasizing the importance of incorporating the \"Conscious Incompetence\" setting, and the crit- ical role of retrieval accuracy. \n1 Introduction \nRecently, Large Language Models ( Brown et al. , 2020 ) (LLMs) have exhibited great capability in open-ended question answering ( Yang et al. ,  2019 ). However, the generated answers may include fac- tual errors and are not always reliable, and is com- monly known as the “hallucination” ( Shuster et al. , 2021 ;  Ji et al. ,  2023 ) problem. For instance, LLMs may give wrong diagnosis to patient’s symptoms. Hallucination has severe harms especially on indus- \nThe image is a diagram illustrating the influence of Orazio Gentileschi on Artemisia Gentileschi's life and career in developing her as a Baroque painter. It uses a knowledge graph to show relationships between:\n\n- Orazio Gentileschi, a painter and the father of Artemisia.\n- Artemisia Gentileschi, a painter associated with the Baroque style.\n- Their citizenship in Italy.\n\nThe knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment. \nTo minimize the negative impacts, researchers have proposed the task of language attribu- tion ( Bohnet et al. ,  2023 ), which not only enables users to verify the generated text flexibly but also contributes to many important applications, such as situation reports ( Reddy et al. ,  2023 ), academic papers ( Salvagno et al. ,  2023 ), medical diagno- sis ( Zuccon and Koopman ,  2023 ).Existing works mainly attribute generated outputs to unstructured documents like web pages ( Nakano et al. ,  2021 ; Menick et al. ,  2022 ) or passages ( Gao et al. ,  2023 ). To verify the answer quality, they typically compare with a human annotated reference answer for auto- matic evaluation or conduct human evaluation. We argue that there are several concerns on such task definition.  Firstly , are documents the only source for attribution? Many real-world applications have their own knowledge bases or semi-structured re- ports.  Secondly , does the attribution source always include all the required knowledge? We consider the coverage issue since no perfect repository can contain all the information in this world.  Thirdly , how to systematically evaluate the attributed con- tent without references? For open-ended questions, there are unlimited number of answers and it is difficult to define a single ground truth. "}
{"page": 1, "image_path": "doc_images/2310.05634v2_1.jpg", "ocr_text": "for attribution? Many real-world applications have\ntheir own knowledge bases or semi-structured re-\nports. Secondly, does the attribution source always\ninclude all the required knowledge? We consider\nthe coverage issue since no perfect repository can\ncontain all the information in this world. Thirdly,\nhow to systematically evaluate the attributed con-\ntent without references? For open-ended questions,\nthere are unlimited number of answers and it is\ndifficult to define a single ground truth.\n\nTo address the first challenge, we utilize knowl-\nedge graph (KG) as a reliable source for attribution,\nnamely Knowledge-aware Language Model Attri-\nbution (KaLMA). We show a demonstration of task\nin Figure 1. KGs efficiently organize world knowl-\nedge in a structured manner and has the poten-\ntial to unify various formats of data. For example,\ndatabases can be easily converted into KGs, or, pas-\nsages and web pages can be represented as a node\nin KG like Wikipedia. KaLMA differs from entity\nlinking (Sevgili et al., 2022) since the sentences or\nphrases are attributed to a knowledge triplet rather\nthan a single entity. For the second challenge, we\ntackle the coverage problem by making the model\naware of its limitations. We introduce a new setting\n“Conscious Incompetence” (Curtiss and Warren,\n1974), which is the psychological stage that one is\naware of the knowledge gap. During generation,\nLLMs identify sentences that require supporting\nknowledge absent in the knowledge graph. Our\nsetting enables an attributed LM to recognize the\nknowledge gaps and allows users to verify uncer-\ntain claims, which enhances trustworthiness. For\nthe third challenge, we propose a comprehensive\nautomatic evaluation metric including text quality,\ncitation quality, and text citation alignment. The\nentire evaluation process does not require human\nannotated ground truth.\n\nTo implement the above innovations, we first\ndesign an automatic dataset construction pipeline.\nUsing this pipeline, we construct a dataset! in the\nbiographical domain, namely BioKaLMA, for a\nbenchmark with all-rounded automatic measure-\nments. Biography forms a good test-set for attri-\nbution due to its practical application and conve-\nnient evaluation. The availability of high-quality\nknowledge graph like WikiData also benefits our\ndataset construction. Derived from the biograph-\n\n'The codes and dataset BioKaLMA are publicly\navailable in https://github.com/lixinze777/\n\nical database” (Plum et al., 2022) and WikiData,\nBioKaLMA contains 1,085 data entries. Each data\nentry includes question and knowledge required to\nanswer the question. For evaluation, we separately\nevaluate the generated text, the generated citations,\nand the alignment between texts and citations. We\nuse G-Eval (Liu et al., 2023b) to automatically eval-\nuate the text quality. We also design measurement\nfor correctness, precision, and recall for citations.\nLastly, we determine the alignment between texts\nand citations employing NLI (Dagan et al., 2005)\n\nWe summarize our contributions as follows: 1)\nWe define the task of Knowledge-aware Language\nModel Attribution (KaLMA) that attributes lan-\nguage models to structured knowledge. 2) We de-\nsign a complete benchmarking pipeline, including\ndataset, baseline, and evaluation metrics. 3) We\nconduct extensive experiments and show room for\nimprovement of the LLMs’ ability to generate ac-\ncurate and thorough citations based on provided\nknowledge graphs. Our experiments on “Conscious\nIncompetence” investigate the capability of current\nLLMs to identify if there are required knowledge\nnot in knowledge graph. We highlight the necessity\nof incorporating this setting in future language at-\ntribution works. Furthermore, our ablation studies\ndemonstrate the crucial role of retrieval accuracy\nin achieving desirable generation results.\n\n2 Task and Dataset\n\n2.1 Task Formulation\n\nWe hereby define the task Knowledge-aware Lan-\nguage Model Attribution (KaLMA): Given a ques-\ntion g and the knowledge graph G, the system gen-\nerates an output text ¢ that answers the question.\nThe output text consists of a list of m sentences s1,\n.-5 Sm grounded with a list of n grounded knowl-\nedge ky .. kn where {k1..kn} € G. Each knowl-\nedge k is a sub-graph of G'. Each sentence s may\nbe grounded by zero up to multiple knowledge.\n\nSetting of Conscious Incompetence We extend\nthis task setting to include conscious incompetence.\nGiven the same input, each sentence s in the output\ntext t can map to a Not Applicable Citation (we use\n[NA] to represent it) if it includes some knowledge\nto be verified, but the knowledge is absent in the\nknowledge graph G. A sentence can map to both\n[NA] and a list of sub-graph knowledge if it can\n\n*https://plumaj.github.io/\n\nKnowledge-aware-Language-Model-Attribution biographical/\n", "vlm_text": "\nTo address the first challenge, we utilize knowl- edge graph (KG) as a reliable source for attribution, namely Knowledge-aware Language Model Attri- bution ( KaLMA ). We show a demonstration of task in Figure  1 . KGs efficiently organize world knowl- edge in a structured manner and has the poten- tial to unify various formats of data. For example, databases can be easily converted into KGs, or, pas- sages and web pages can be represented as a node in KG like Wikipedia. KaLMA differs from entity linking ( Sevgili et al. ,  2022 ) since the sentences or phrases are attributed to a knowledge triplet rather than a single entity. For the second challenge, we tackle the coverage problem by making the model aware of its limitations. We introduce a new setting “ Conscious Incompetence ” ( Curtiss and Warren , 1974 ), which is the psychological stage that one is aware of the knowledge gap. During generation, LLMs identify sentences that require supporting knowledge absent in the knowledge graph. Our setting enables an attributed LM to recognize the knowledge gaps and allows users to verify uncer- tain claims, which enhances trustworthiness. For the third challenge, we propose a comprehensive automatic evaluation metric including text quality, citation quality, and text citation alignment. The entire evaluation process does not require human annotated ground truth. \nTo implement the above innovations, we first design an automatic dataset construction pipeline. Using this pipeline, we construct a dataset 1   in the biographical domain, namely  BioKaLMA , for a benchmark with all-rounded automatic measure- ments. Biography forms a good test-set for attri- bution due to its practical application and conve- nient evaluation. The availability of high-quality knowledge graph like WikiData also benefits our dataset construction. Derived from the biograph- ical database 2   ( Plum et al. ,  2022 ) and WikiData, BioKaLMA contains 1,085 data entries. Each data entry includes question and knowledge required to answer the question. For evaluation, we separately evaluate the generated text, the generated citations, and the alignment between texts and citations. We use G-Eval ( Liu et al. ,  2023b ) to automatically eval- uate the text quality. We also design measurement for correctness, precision, and recall for citations. Lastly, we determine the alignment between texts and citations employing NLI ( Dagan et al. ,  2005 ) \n\nWe summarize our contributions as follows: 1) We define the task of Knowledge-aware Language Model Attribution (KaLMA) that attributes lan- guage models to structured knowledge. 2) We de- sign a complete benchmarking pipeline, including dataset, baseline, and evaluation metrics. 3) We conduct extensive experiments and show room for improvement of the LLMs’ ability to generate ac- curate and thorough citations based on provided knowledge graphs. Our experiments on “Conscious Incompetence” investigate the capability of current LLMs to identify if there are required knowledge not in knowledge graph. We highlight the necessity of incorporating this setting in future language at- tribution works. Furthermore, our ablation studies demonstrate the crucial role of retrieval accuracy in achieving desirable generation results. \n2 Task and Dataset \n2.1 Task Formulation \nWe hereby define the task Knowledge-aware Lan- guage Model Attribution  (KaLMA) : Given a ques- tion  $q$   and the knowledge graph  $G$  , the system gen- erates an output text    $t$   that answers the question. The output text consists of a list of    $m$   sentences    $s_{1}$  , ...,    $s_{m}$   grounded with a list of  $n$   grounded knowl- edge  $k_{1}$   ..    $k_{n}$   where    $\\{k_{1}..k_{n}\\}\\in G$  . Each  nowl- edge  k  is a sub-graph of  G . Each sentence  s  may be grounded by zero up to multiple knowledge. \nSetting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap. "}
{"page": 2, "image_path": "doc_images/2310.05634v2_2.jpg", "ocr_text": "be partially verified by the knowledge graph G.\n[NA] is not a citation on conventional means, but a\nindicator of knowledge gap.\n\n2.2 Dataset Construction\n\nEach entry of dataset biokKaLMA includes two\nquestions and a minimum knowledge set. The two\nquestions enquire about the same people on similar\naspects of their life stories. The minimum knowl-\nedge set is the smallest set of knowledge that is\nrequired to answer each question. One question\nis a general version and the other is specific. The\ngeneral questions are more concise and natural for\nhuman readers, and the specific version questions\nhave a tighter bond to the minimum knowledge set,\nand is hence more accurate for evaluating LLMs.\nAn example data piece is shown in Table 1.\n\nWe construct the dataset using an automatic\npipeline consisting of three steps: Person Selection,\nName Disambiguation, and Evolutionary Question\nGeneration. In the first two steps, we use SPARQL\nqueries to select related people from human written\nsentences and identify their identity in WikiData.\nIn the third step, we iteratively construct paragraph\nand question about the selected people. The first\niteration starts with a human written sentence about\nthe selected people. In each next iteration, we apply\na data selection algorithm to select an appropriate\nknowledge from WikiData based on the existing\nparagraph, and extend the paragraph to include\nthe additional knowledge using LLM. Then, LLM\nconstructs the questions using the final paragraph\nas an answer. The general and specific questions\nare generated with different prompts and demon-\nstrations. All the selected knowledge from each\niteration form the “minimum knowledge set” for\nthe question. While we use the human biography\ndomain as an example, this method is applicable\nto all domains. We present the details of the data\nconstruction in Appendix A.\n\n2.3. Dataset Analysis\n\nStatistics There are 1,085 data entries in\nBioKalMA. On average, there are 6.8 pieces of\nknowledge in each “minimum knowledge set”.\nBioKaLMA demonstrates a good demographic\nvariation. It includes a wide range of geograph-\nical distribution of people from 196 countries and\n949 cities, taking 279 kinds of different occupa-\ntions. The eras of people span from 1950 B.C. to\n2001 A.D.\n\nGeneral Question:\nWho were Oscar and Richard Hertwig,\n\nand what were their contributions to\nthe fields of anatomy and biology?\nSpecific Question:\n\nWhat were the career paths and\nsignificant contributions of Oscar and\nRichard Hertwig in the fields of\nanatomy and biology, and who were\ntheir notable mentors and students?\n\nMinimum Knowledge Set:\n\n[7Q85907’, \"occupation’, ‘’biologist’]\n[’Q85907’, ‘doctoral student’,\n’Stanislaus von Prowazek’ ]\n\n[’0Q68753’, ‘doctoral advisor’, ‘Ernst\nHaeckel’ J\n\n[’Q68753’, ‘student of’, ’Ernst\nHaeckel’ J\n\n[’Q68753’, ‘nominated for’, 'Nobel\n\nPrize in Physiology or Medicine’ ]\nTable 1: An example for generated data entry in\nBioKaLMA. Q85907 and Q68753 are Richard Hertwig\nand Oscar Hertwig’s QIDs in WikiData\n\nMetric (full score) | General Specific\nAuthenticity (1) 1.00 1.00\nRelevance (1) 0.73 0.84\nNaturalness (5) 4.38 3.52\nSignificance (5) 3.94 3.68\n\nTable 2: Human Evaluation on BioKaLMA dataset.\n\nEvaluation of Dataset Quality We evaluate the\nBioKaLMA dataset on the following four metrics\nto ensure the quality of the dataset: 1) Authen-\nticity: The generated questions should accurately\nreflect the objective facts. 2) Relevance: Each min-\nimum knowledge set should provide support to the\ncorresponding question. Each piece of knowledge\nfrom the minimum knowledge set is not redundant.\n3) Naturalness: The generated question should be\nconcise and understandable by human readers. 4)\nSignificance: The generated question should be\nmeaningful and helpful to users.\n\nTo our best knowledge, there is no perfect au-\ntomatic evaluation for these metrics. Naturalness\nand significance are subjective. Hence, we apply\nhuman evaluation to ensure the dataset quality.\n\nWe randomly sample 50 data entries from\nBioKaLMA and ask human annotators to evalu-\nate the data entries based on the four metrics. The\ngeneral and specific questions are evaluated sepa-\nrately. More details are given in Appendix C.\n\nThe final result for each metric is taken average\nand reported in Table 2. For both general and spe-\ncific settings, the questions from sample achieve a\n100% authenticity, which indicates that the overall\nauthenticity of BiokaLMA dataset is high. The rel-\n", "vlm_text": "\n2.2 Dataset Construction \nEach entry of dataset bioKaLMA includes two questions and a minimum knowledge set. The two questions enquire about the same people on similar aspects of their life stories. The minimum knowl- edge set is the smallest set of knowledge that is required to answer each question. One question is a general version and the other is specific. The general questions are more concise and natural for human readers, and the specific version questions have a tighter bond to the minimum knowledge set, and is hence more accurate for evaluating LLMs. An example data piece is shown in Table  1 . \nWe construct the dataset using an automatic pipeline consisting of three steps: Person Selection, Name Disambiguation, and Evolutionary Question Generation. In the first two steps, we use SPARQL queries to select related people from human written sentences and identify their identity in WikiData. In the third step, we iterative ly construct paragraph and question about the selected people. The first iteration starts with a human written sentence about the selected people. In each next iteration, we apply a data selection algorithm to select an appropriate knowledge from WikiData based on the existing paragraph, and extend the paragraph to include the additional knowledge using LLM. Then, LLM constructs the questions using the final paragraph as an answer. The general and specific questions are generated with different prompts and demon- strations. All the selected knowledge from each iteration form the “minimum knowledge set” for the question. While we use the human biography domain as an example, this method is applicable to all domains. We present the details of the data construction in Appendix  A . \n2.3 Dataset Analysis \nStatistics There are 1,085 data entries in BioKalMA. On average, there are 6.8 pieces of knowledge in each “minimum knowledge set”. BioKaLMA demonstrates a good demographic variation. It includes a wide range of geograph- ical distribution of people from 196 countries and 949 cities, taking 279 kinds of different occupa- tions. The eras of people span from 1950 B.C. to 2001 A.D. \nTable 1: An example for generated data entry in BioKaLMA. Q85907 and Q68753 are Richard Hertwig and Oscar Hertwig’s QIDs in WikiData \nThe table presents data on four different metrics: Authenticity, Relevance, Naturalness, and Significance. Each metric has a specified full score indicated in parentheses next to the metric name. The scores are compared across two categories: General and Specific.\n\n- **Authenticity** (full score of 1):\n  - General: 1.00\n  - Specific: 1.00\n\n- **Relevance** (full score of 1):\n  - General: 0.73\n  - Specific: 0.84\n\n- **Naturalness** (full score of 5):\n  - General: 4.38\n  - Specific: 3.52\n\n- **Significance** (full score of 5):\n  - General: 3.94\n  - Specific: 3.68\n\nEach metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C . \nThe final result for each metric is taken average and reported in Table  2 . For both general and spe- cific settings, the questions from sample achieve a  $100\\%$   authenticity, which indicates that the overall authenticity of BioKaLMA dataset is high. The rel- evance on general and specific settings are  $73\\%$   and  $84\\%$   respectively. The specific question normally consists of more parts and include more details than its general version, and hence some knowl- edge are necessary to the specific version but not to the general version. However, the general ver- sion questions sacrifice relevance to achieve better naturalness and significance. "}
{"page": 3, "image_path": "doc_images/2310.05634v2_3.jpg", "ocr_text": "evance on general and specific settings are 73% and\n84% respectively. The specific question normally\nconsists of more parts and include more details\nthan its general version, and hence some knowl-\nedge are necessary to the specific version but not\nto the general version. However, the general ver-\nsion questions sacrifice relevance to achieve better\nnaturalness and significance.\n\nIn practice, it is difficult to define a precise “min-\nimum knowledge set” for a question unless it is\nvery specific. However, a very specific question\ntends to be artificial. The relevance and naturalness\nof a question have a trade-off relationship. It is yet\nchallenging to generate questions that have both\nhigh relevance and high naturalness, but our gener-\nation method allows for a control on the granularity\nof a question on whether it tends to be more natural\nor more relevant.\n\n3 Method\n\nWe build a baseline to enable LLMs to generate\nknowledge-aware attributed answers. Following\nthe approach of many retrieval augmented gener-\nation works (Lee et al., 2022; Izacard and Grave,\n2021), we utilize a pipeline consisting of three com-\nponents: retrieval, re-ranking, and generation.\n\n3.1 Retrieval\n\nOur baseline retrieval process consists of two parts:\nnamed entity recognition and graph retrieval. We\nutilize spaCy? to identify the named entities men-\ntioned in the question. Using these entities, we\nretrieve entity-centered sub-graphs using SPARQL.\nFor each retrieved entity, we search for nodes in\nthe graph that match the entity’s name. We use\nthe named entity recognition (NER) entity type\nas a simple filter (e.g., the NER category “person”\nmatches the “human” entity type in WikiData). Tak-\ning each selected node as the center, we retrieve\none-hop sub-graphs that contain properties associ-\nated with the entity.\n\n3.2 Re-ranking\n\nThe re-ranking component plays a crucial role in\ndisambiguating retrieved entities, as multiple en-\ntities may share the same name in the WikiData\ngraph. Two common scenarios are different indi-\nviduals with the same name (e.g., Anne Hathaway\nthe American actress and Anne Hathaway the wife\nof William Shakespeare) and different references\n\n*https://spacy.io/api/entityrecognizer\n\nto the same word (e.g., “Chinese” the language\nand “Chinese” the ethnic group). When multiple\nentities are retrieved from the graph for a given\nentity name, we rank the graphs based on the Exact\nMatch (EM) between the neighboring nodes and\nthe question. We select the entity with the highest\nnumber of matched neighboring nodes.\n\n3.3. Generation\n\nThe generation component effectively prompt the\nLLMs with the retrieved knowledge graphs (KGs)\nto generate answers that attribute the KG. To adapt\nto the input format of the LLMs, we transform the\nstructured KGs into flat texts. We preserve the\ninformation of the retrieved sub-graphs by map-\nping each sub-graph to a set of triples. Each triple\nconsists of two nodes and one edge, where one\nnode is the centered entity, the other node is its\nneighbor, and the edge represents the relationship\nbetween them. For example, [Q212657 - place of\nbirth - Q220] can be translated to [Artemisia Gen-\ntileschi - place of birth - Rome]. In this translation,\nwe use the names of the entities for better com-\nprehension by both the models and humans, since\nWikiData utilizes QIDs (e.g., Q220) to represent\nunique entities. We construct a prompt (Table 13\nin appendix D) which includes 1) instruction to\nthe models to generate attributed answers. 2) re-\ntrieved knowledge graph, and 3) the question. We\nemploy one-shot in-context learning (Brown et al.,\n2020) by prepending one human written demon-\nstration. In the one-shot demonstration, we use\nthe special token [NA] to represent the “Not Appli-\ncable Citations” for conscious incompetence. We\ndeliberately omit some knowledge in the demon-\nstration example knowledge graph, and we insert\n[NA] tokens in the corresponding sentences that\nuse these knowledge within the example answer.\n\n4 Evaluation Metrics\n\nOur benchmark includes evaluation metrics for\nboth the generated text and citations. We also evalu-\nate the alignment between the text and correspond-\ning citations. We provide more discussions on the\ndesign of evaluation metrics in subsection 4.5.\n\n4.1 Text Evaluation\n\nSince our test-set has no human-written gold an-\nswers as references, we do not utilize comparison-\nbased metrics such as BERTScore (Zhang et al.,\n2019a) or MAUVE (Pillutla et al., 2021). In-\nstead, we employ reference-free NLG evaluator G-\n", "vlm_text": "\nIn practice, it is difficult to define a precise “min- imum knowledge set” for a question unless it is very specific. However, a very specific question tends to be artificial. The relevance and naturalness of a question have a trade-off relationship. It is yet challenging to generate questions that have both high relevance and high naturalness, but our gener- ation method allows for a control on the granularity of a question on whether it tends to be more natural or more relevant. \n3 Method \nWe build a baseline to enable LLMs to generate knowledge-aware attributed answers. Following the approach of many retrieval augmented gener- ation works ( Lee et al. ,  2022 ;  Izacard and Grave , 2021 ), we utilize a pipeline consisting of three com- ponents: retrieval, re-ranking, and generation. \n3.1 Retrieval \nOur baseline retrieval process consists of two parts: named entity recognition and graph retrieval. We utilize spaCy 3   to identify the named entities men- tioned in the question. Using these entities, we retrieve entity-centered sub-graphs using SPARQL. For each retrieved entity, we search for nodes in the graph that match the entity’s name. We use the named entity recognition (NER) entity type as a simple filter (e.g., the NER category “person” matches the “human” entity type in WikiData). Tak- ing each selected node as the center, we retrieve one-hop sub-graphs that contain properties associ- ated with the entity. \n3.2 Re-ranking \nThe re-ranking component plays a crucial role in disambiguating retrieved entities, as multiple en- tities may share the same name in the WikiData graph. Two common scenarios are different indi- viduals with the same name (e.g., Anne Hathaway the American actress and Anne Hathaway the wife of William Shakespeare) and different references to the same word (e.g., “Chinese” the language and “Chinese” the ethnic group). When multiple entities are retrieved from the graph for a given entity name, we rank the graphs based on the Exact Match (EM) between the neighboring nodes and the question. We select the entity with the highest number of matched neighboring nodes. \n\n3.3 Generation \nThe generation component effectively prompt the LLMs with the retrieved knowledge graphs (KGs) to generate answers that attribute the KG. To adapt to the input format of the LLMs, we transform the structured KGs into flat texts. We preserve the information of the retrieved sub-graphs by map- ping each sub-graph to a set of triples. Each triple consists of two nodes and one edge, where one node is the centered entity, the other node is its neighbor, and the edge represents the relationship between them. For example, [Q212657 - place of birth - Q220] can be translated to [Artemisia Gen- tileschi - place of birth - Rome]. In this translation, we use the names of the entities for better com- prehension by both the models and humans, since WikiData utilizes QIDs (e.g., Q220) to represent unique entities. We construct a prompt (Table  13 in appendix  D ) which includes 1) instruction to the models to generate attributed answers. 2) re- trieved knowledge graph, and 3) the question. We employ one-shot in-context learning ( Brown et al. , 2020 ) by prepending one human written demon- stration. In the one-shot demonstration, we use the special token [NA] to represent the “Not Appli- cable Citations” for conscious incompetence. We deliberately omit some knowledge in the demon- stration example knowledge graph, and we insert [NA] tokens in the corresponding sentences that use these knowledge within the example answer. \n4 Evaluation Metrics \nOur benchmark includes evaluation metrics for both the generated text and citations. We also evalu- ate the alignment between the text and correspond- ing citations. We provide more discussions on the design of evaluation metrics in subsection  4.5 . \n4.1 Text Evaluation \nSince our test-set has no human-written gold an- swers as references, we do not utilize comparison- based metrics such as BERTScore ( Zhang et al. , 2019a ) or MAUVE ( Pillutla et al. ,  2021 ). In- stead, we employ reference-free NLG evaluator G- "}
{"page": 4, "image_path": "doc_images/2310.05634v2_4.jpg", "ocr_text": "Model Output: Mininum\nSentence’ [k1][k2]. Knowledge Set:\nSentence2 [k2][k6][NA]. | [k1] [k2]\n\nSentence3 [k6][k9]. [k3] [k4] [k5]\n\nModel Output:\nSentence [k1][NA].\nSentence2 [k3][k6][NA].\nSentence3 [k6][k9].\n\nAbsent\nKnowledge Set:\n[k2] [k4] [k5]\n\nCitation Precision Citation Recall\n\nSentence1 [k1][k2].\nSentence2 [k2][k6][NA]. <—> _ [k1] [k2]\nSentence [k6][k9]. [k3] [k4] [k5]\n\ny Y\n\ncorrect = 3, all=6 hit = 2, all =5\nprecision = 3/6 = 0.5 recall = 2/5 = 0.4\n\nKnowlege:\n\nNLI\nP:Sentence1 H:[k2]A\nP:Sentence1 H:[k4]®)\nP:Sentence1 H:[k5]@)\n\nP:Sentence2 H:[k2] ®)\nP:Sentence2 H:[k4] ®)\nP:Sentence2 H:[k5] ©\n\n1 [NA] Recall\n\nhit=1, all=3\nrecall = 1/3 = 0.33\n\n[NA] Precision\ncorrect=1, all=2\nprecision = 1/2 = 0.5\n\nFigure 2: An illustration of how we evaluate the preci-\nsion and recall for generated citations.\n\nEval (Liu et al., 2023b), which defines the follow-\ning four metrics: 1) Coherence: whether the gen-\nerated text is well-structured and well-organized.\n2) Consistency: whether the generated text is con-\nsistent with the knowledge provided. 3) Fluency:\nwhether the generated text is well-written and gram-\nmatical. 4) Relevance: how well is the generated\ntext relevant to the question.\n\nWe use the model text-davinci-003 for evalua-\ntion, which assigns an integer score of 1 to 5 for\neach metric. We follow the prompt provided in G-\nEval (Liu et al., 2023b) and customize it based on\nour task. The full prompts are given in appendix D.\n\n4.2 Citation Evaluation\n\nWe evaluate the citation qualities from three as-\npects: 1) Correctness, which measures whether the\ngenerated knowledge matches the given knowledge\nfrom the knowledge graph, 2) Precision, which\ndetermines how much of the generated citations\nare helpful to answer the question, and 3) Recall,\nwhich measures how much of the minimum knowl-\nedge set are covered by the generated citations. We\nalso calculate the Fl-Score based on the Precision\nand Recall to reflect the overall quality of citations.\n\nCorrectness We calculate the citation correct-\nness for each citation (0 or 1) and average over all\ncitations. Each citation comprises a triplet of 1)\ncenter entity QID, 2) relation 3) neighbour entity\nvalue. If the generated citation is complete with all\nthree parts, and exactly matches a triplet from the\nquestion’s retrieved KG, correctness = 1.\n\nPrecision We calculate citation precision for\neach citation (0 or 1) and average over all citations\nto get micro precision. Precision = | for a citation\nif and only if 1) it is correct, and 2) it matches one\n\nFigure 3: An illustration of how we evaluate the preci-\nsion and recall for conscious incompetence ({NA])\n\nknowledge triplet from minimum knowledge set of\nthe question. (See Figure 2.)\n\nRecall We calculate citation recall for each\nknowledge (0 or 1) in minimum knowledge set,\nand average over all knowledge to get micro recall.\nRecall = 1 if and only if the knowledge if hit by a\ncorrect citation. (See Figure 2.)\n\nWe average over all citations/knowledge in an an-\nswer, and average all answer-level precision/recall\nto get macro precision and recall. we calculate\nmicro and macro F1-Score from corresponding pre-\ncision and recall.\n\n4.3 Text-Citation Alignment\n\nOther than the text quality and citation quality, we\nmeasure whether the generated citations provide\nsupport for the corresponding sentences. A piece\nof useful knowledge is not an ideal citation if it\nis irrelevant to the sentence it links to. Therefore,\nwe propose the metric “Alignment” which deter-\nmines whether the generated citations are aligned\nto the sentences to which they belong. We use a\nstate-of-the-art natural language inference (NLI)\nmodel TRUE (Honovich et al., 2022), which is\na fine-tuned T5-11B (Raffel et al., 2020) model,\nto check whether the generated sentence entails\nthe generated citation. Since one sentence could\nhave multiple citations, we run NLI on all sentence-\ncitation pairs and report the percentage of entail-\nment. Additionally, we conduct human evaluation\nin § 5.4 to showcase if the automatic evaluation is\ncorrelated with human judgments.\n\n4.4 Conscious Incompetence Evaluation\n\nTheoretically, each [NA] mark should map to\na piece of knowledge absent from the retrieved\n", "vlm_text": "The image shows a diagram explaining how to evaluate precision and recall for generated citations.\n\n- **Model Output**:\n  - Sentence1: [k1][k2]\n  - Sentence2: [k2][k6][NA]\n  - Sentence3: [k6][k9]\n\n- **Minimum Knowledge Set**:\n  - [k1] [k2]\n  - [k3] [k4] [k5]\n\n- **Citation Precision**:\n  - Sentence1: [k1][k2]\n  - Sentence2: [k2][k6][NA]\n  - Sentence3: [k6][k9]\n  - Correct = 3, All = 6\n  - Precision = 3/6 = 0.5\n\n- **Citation Recall**:\n  - Knowledge: [k1][k2][k3][k4][k5]\n  - Hit = 2, All = 5\n  - Recall = 2/5 = 0.4\n\nThe illustration highlights the calculation of precision and recall based on the model output and a set of minimum knowledge.\nEval ( Liu et al. ,  2023b ), which defines the follow- ing four metrics: 1)  Coherence : whether the gen- erated text is well-structured and well-organized. 2)  Consistency : whether the generated text is con- sistent with the knowledge provided. 3)  Fluency : whether the generated text is well-written and gram- matical. 4)  Relevance : how well is the generated text relevant to the question. \nWe use the model text-davinci-003 for evalua- tion, which assigns an integer score of 1 to 5 for each metric. We follow the prompt provided in G- Eval ( Liu et al. ,  2023b ) and customize it based on our task. The full prompts are given in appendix  D . \n4.2 Citation Evaluation \nWe evaluate the citation qualities from three as- pects: 1)  Correctness , which measures whether the generated knowledge matches the given knowledge from the knowledge graph, 2)  Precision , which determines how much of the generated citations are helpful to answer the question, and 3)  Recall , which measures how much of the minimum knowl- edge set are covered by the generated citations. We also calculate the F1-Score based on the Precision and Recall to reflect the overall quality of citations. \nCorrectness We calculate the citation correct- ness for each citation (0 or 1) and average over all citations. Each citation comprises a triplet of 1) center entity QID, 2) relation 3) neighbour entity value. If the generated citation is complete with all three parts, and exactly matches a triplet from the question’s retrieved KG, correctness  $=1$  . \nPrecision We calculate citation precision for each citation (0 or 1) and average over all citations to get micro precision. Precision  $=1$   for a citation if and only if 1) it is correct, and 2) it matches one \nThe image depicts a method for evaluating precision and recall specifically for the concept of \"conscious incompetence\" as indicated by the tag [NA]. \n\n- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as \"conscious incompetence.\"\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1's correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2's hit of k2 against an all of 3 absence marks results in a recall of 0.33.\n\nThe image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment \nOther than the text quality and citation quality, we measure whether the generated citations provide support for the corresponding sentences. A piece of useful knowledge is not an ideal citation if it is irrelevant to the sentence it links to. Therefore, we propose the metric “Alignment” which deter- mines whether the generated citations are aligned to the sentences to which they belong. We use a state-of-the-art natural language inference (NLI) model TRUE ( Honovich et al. ,  2022 ), which is a fine-tuned T5-11B ( Raffel et al. ,  2020 ) model, to check whether the generated sentence entails the generated citation. Since one sentence could have multiple citations, we run NLI on all sentence- citation pairs and report the percentage of entail- ment. Additionally, we conduct human evaluation in  $\\S\\ 5.4$   to showcase if the automatic evaluation is correlated with human judgments. \n4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem. "}
{"page": 5, "image_path": "doc_images/2310.05634v2_5.jpg", "ocr_text": "knowledge graph. However, it is difficult to identify\nif sentence requires any absent knowledge since\nthere is no ground truth. Therefore, we conduct a\nthree-round experiment to manually create ground\ntruth for absent knowledge. In round 1, we select\none knowledge from the minimum knowledge set,\nand remove it from the ground-truth knowledge\ngraph. We let the LLMs attribute to this incomplete\nknowledge graph to generate answers, whereby the\nremoved knowledge forms the “absent knowledge\nground truth”. In subsequent rounds, we each re-\nmove one additional knowledge from the minimum\nknowledge set, simulating a knowledge graph with\nmore serious coverage problem.\n\nWe employ the NLI model TRUE (Honovich\net al., 2022) to measure the alignment between\nsentences and knowledge. A sentence with [NA]\nshould be aligned to an absent knowledge. We\ncalculate precision and recall for [NA].\n\n[NA] precision We calculate [NA] precision for\neach sentence with [NA] (0 or 1) and average over\nall sentences with [NA]. Precision = | for a sen-\ntence if and only if it entails one knowledge triplet\nfrom absent knowledge set of the question. (See\nFigure 3.)\n\n[NA] Recall We calculate [NA] recall for each\nknowledge (0 or 1) in absent knowledge set and\naverage over all absent knowledge. Recall = 1 if\nand only if the knowledge if entailed by a sentence\nwith [NA]. (See Figure 3.)\n\n4.5 Discussion on Evaluation metrics\n\nIn this section, we discuss on the evaluation metrics\nof benchmark BioBaLMA. We design the evalua-\ntion metrics from multiple dimensions to incorpo-\nrate different understandings on what makes a high\nquality citation.\n\n* One understanding argues when the answer\ncontains mistakes, even if the citation is cor-\nrectly answering the questions, it cannot rep-\nresent good LLM attribution performance. In\nthis case, citation quality is considered as a\nmeasure of overall attribution performance,\nincluding the answer quality.\n\nThe other understanding argues for a complete\ndecoupling of answer and citation quality. In\nthis scenario, even if the answer is wrong,\nthe citation is valuable as long as it provides\nreasonable support for the question. In such\n\ncases, citations do not give advice on the cor-\nrectness of the answer.\n\nBoth understandings are plausible, and hence we\nhave considered both of them when we design met-\nrics. The alignment score is designed based on\nhe first understanding, which measures whether\nhe citations are closely linked to the answer. The\nprecision and recall are designed for the second\nunderstanding, where the citations are completely\ndecoupled from the answer, and are correct if they\nprovide support for the question.\n\nIn addition, we also incorporate an edge case\nor design of the [NA] precision calculation. If an\nNA-marked sentence does not answer the question\nat all, it is considered correct in the [NA] precision\ncalculation. In this case, the LLM correctly iden-\nifies a sentence that requires further verification.\n\n5 Experiments\n\nWe run through the method pipeline described in\n§ 3 on different LLMs and present the results in\nthis section. Since we aim to obtain a more accu-\nrate evaluation, we conduct our main experiments\non the specific questions setting, since the mini-\nmum knowledge set has a higher relevance on the\nspecific questions. However, we will also provide\nevaluation results for the general questions in § 5.5\nas ablation studies. The implementation details\nare reported in appendix B. We report five model\nbaselines from both open and closed source model\nfamilies:\n\nOpenAI Models We use GPT4 (gpt-4-0314) and\nChatGPT (gpt-3.5-turbo-0301) for our experiments.\nFor ChatGPT, we experiment on temperature of 0.1,\n0.5, and 0.9 to obtain different levels of randomness\nand creativity in generation.\n\nLLaMA_ We conduct experiments with LLaMA-\n7B (Touvron et al., 2023) and LLaMA-13B since\nthey are powerful open-source models that are\nwidely accessible. We have also conducted hu-\nman instruction tuned LLaMA models, includ-\ning Alpaca-7B (Taori et al., 2023) and Vicuna-\n13B (Chiang et al., 2023).\n\n5.1 Main Results\n\nCitation Quality Evaluation We present the\nmain results in Table 3. For correctness, we report\non a micro scale. For precision, recall, and Fl-\nScore, we report on both micro and macro scales.\n", "vlm_text": "\nWe employ the NLI model TRUE ( Honovich et al. ,  2022 ) to measure the alignment between sentences and knowledge. A sentence with [NA] should be aligned to an absent knowledge. We calculate precision and recall for [NA]. \n[NA] precision We calculate [NA] precision for each sentence with [NA] (0 or 1) and average over all sentences with [NA]. Precision  $=1$   for a sen- tence if and only if it entails one knowledge triplet from absent knowledge set of the question. (See Figure  3 .) \n[NA] Recall We calculate [NA] recall for each knowledge (0 or 1) in absent knowledge set and average over all absent knowledge. Recall  $=1$   if and only if the knowledge if entailed by a sentence with [NA]. (See Figure  3 .) \n4.5 Discussion on Evaluation metrics \nIn this section, we discuss on the evaluation metrics of benchmark BioBaLMA. We design the evalua- tion metrics from multiple dimensions to incorpo- rate different understandings on what makes a high quality citation. \n•  One understanding argues when the answer contains mistakes, even if the citation is cor- rectly answering the questions, it cannot rep- resent good LLM attribution performance. In this case, citation quality is considered as a measure of overall attribution performance, including the answer quality. \n•  The other understanding argues for a complete decoupling of answer and citation quality. In this scenario, even if the answer is wrong, the citation is valuable as long as it provides reasonable support for the question. In such Both understandings are plausible, and hence we have considered both of them when we design met- rics. The alignment score is designed based on the first understanding, which measures whether the citations are closely linked to the answer. The precision and recall are designed for the second understanding, where the citations are completely decoupled from the answer, and are correct if they provide support for the question. \n\nIn addition, we also incorporate an edge case for design of the [NA] precision calculation. If an NA-marked sentence does not answer the question at all, it is considered correct in the [NA] precision calculation. In this case, the LLM correctly iden- tifies a sentence that requires further verification. \n5 Experiments \nWe run through the method pipeline described in  $\\S~3$   on different LLMs and present the results in this section. Since we aim to obtain a more accu- rate evaluation, we conduct our main experiments on the specific questions setting, since the mini- mum knowledge set has a higher relevance on the specific questions. However, we will also provide evaluation results for the general questions in   $\\S\\ 5.5$  as ablation studies. The implementation details are reported in appendix  B . We report five model baselines from both open and closed source model families: \nOpenAI Models We use GPT4 (gpt-4-0314) and ChatGPT (gpt-3.5-turbo-0301) for our experiments. For ChatGPT, we experiment on temperature of 0.1, 0.5, and 0.9 to obtain different levels of randomness and creativity in generation. \nLLaMA We conduct experiments with LLaMA- 7B ( Touvron et al. ,  2023 ) and LLaMA-13B since they are powerful open-source models that are widely accessible. We have also conducted hu- man instruction tuned LLaMA models, includ- ing Alpaca-7B ( Taori et al. ,  2023 ) and Vicuna- 13B ( Chiang et al. ,  2023 ). \n5.1 Main Results \nCitation Quality Evaluation We present the main results in Table  3 . For correctness, we report on a micro scale. For precision, recall, and F1- Score, we report on both micro and macro scales. "}
{"page": 6, "image_path": "doc_images/2310.05634v2_6.jpg", "ocr_text": "Micro Macro\nModel Align. Corr. Prec Rec. FI. Prec Rec. F1.\nGPT-4 (0.5) 92.0(1.5) | 97-6(0.1) 36.0(0.6) 43-6(1.9) 39.4 | 40.7(1.1)  43.9(1.0) 42.3\nChatGPT (0.1) | 85.9(2.5) | 96.1(0.4) 29-0(0.0) 50.8(0.3) 36.9 | 32.7(9.4) 5.20.3) 39.9\nChatGPT (0.5) | 84.5(1.1) | 94.8.2) 29.9.2) 49.0(0.8) 37.2 | 34.1(0.5) 4940.9) 40.4\nChatGPT (0.9) | 84.1(0.5) | 94.2.4) 28-7(0.2) 49-0(0.3) 36.2 | 32.5(0.2) 49.4(0.3) 39.2\nLLaMA-7B 47.8(0.3) | 70.2(0.2) 7-7(2.4)  41-1(0.7) 13.0 | 11.0¢..9)  41-4¢0.7) 17.4\nLLaMA-13B 62.1(0.4) | 71-719) 10.5(3.3)  43-7(1.0) 16.9 | 13.8(2.2) 43.5(1.0) 20.9\n\nTable 3: Citation Quality OpenAI models and LLaMA family models. The first five metrics are reported in Micro,\nand the last three metrics are reported in Macro. We also report text citation alignment.\n\nThe experimental results are the mean of three runs,\nand the standard deviation is reported in brackets.\n\nIn general, there is a room of improvement for\nall models since no model can achieve a micro Fl\nScore of higher than 40. The OpenAI models out-\nperform the LLaMA family models in almost all\nmetrics. The correctness is above 94 for OpenAI\nmodels, but around 70 for LLaMA based models.\nFor ChatGPT, temperature does not play a signifi-\ncant role since it effect on Fl Score is at most 1.2.\nThe GPT-4 model achieves the best performance\nacross almost all metrics, except for recall, since\nGPT-4 models tend to generate shorter answers\nwith fewer citations, resulting in higher precision.\nWhile LLaMA is better at Recall by generating\nlong answers with many citations. The Fl-Score of\nmodels from the same family are close to one an-\nother, showing that our automatic evaluation metric\ndesigned is reliable.\n\nText-Citation Alignment From Table 3, similar\nto citation quality, the OpenAI models also out-\nperform the LLaMA based models on text-citation\nalignment. In addition, models with 7B, 13B, 175B\n(ChatGPT), and trillion level (GPT4) parameters\nhave an alignment score of 40+, 60+, 80+, and 92\nrespectively. LLaMA-13B model has an improve-\nment of 14.3 compared to LLaMA-7B model. This\nshows that parameter size may play an important\nrole in generating sentences and citations with good\nalignment.\n\nText Quality Evaluation We present the evalu-\nation of generated text quality in Table 4. From\nthe results, we find that OpenAI models, in general,\nhave better text quality in all metrics compared\nto LLaMA family models, which corresponds to\nthe citation evaluation results. All models exhibit\nrather high consistency, indicating that the LLMs\n\nModel Coh. Con. Flu. Rel.\nGPT-4 (0.5) 448 489 464 4.72\nChatGPT (0.1) | 4.57 4.94 469 4.83\nChatGPT (0.5) | 4.57 4.94 4.71 4.81\nChatGPT (0.9) | 4.52 4.91 4.67 4.79\nAlpaca-7B 4.10 446 4.23 3.76\nLLaMa-7B 3.06 3.79 3.62 2.96\nLLaMa-13B 3.60 4.23 3.94 3.56\nVicuna-13B 3.67 450 3.96 3.64\n\nTable 4: Evaluation on generated text quality.\n\nRemoved | Corr. Prec. Rec. F1.\n0 (gold) 95.5 30.1 57.1 39.4\n1 941 26.1 42.5 32.3\n2 940 210 31.4 25.2\n3 93.9 163 20.4 18.1\n\nTable 5: Citation quality evaluation for generated texts\nusing a KG with N pieces of knowledge removed.\n\nare capable of generating answers that are not\ncontradictory to the provided knowledge or self-\ncontradictory. However, the relevance is relatively\nlow for smaller models, indicating the difficulty\nthese models face in generating answers that are\nrelevant to the questions.\n\n5.2 Conscious Incompetence\n\nWe first evaluate citation quality of the generated\ntext with knowledge removed using method de-\nscribed in § 4.4. From Table 5, the removal of\nrequired knowledge has a minimal impact on cor-\nrectness, but significantly affects citation precision\nand recall. With more knowledge absent from pro-\nvided knowledge graph, both precision and recall\ndrops drastically, demonstrating that the coverage\nissue poses a considerable challenge to generating\nanswers with high quality citations.\n\nNext, we evaluate [NA] precision and recall.\n", "vlm_text": "The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both \"Micro\" and \"Macro\" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.\n\nThe numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets. \nIn general, there is a room of improvement for all models since no model can achieve a micro F1 Score of higher than 40. The OpenAI models out- perform the LLaMA family models in almost all metrics. The correctness is above 94 for OpenAI models, but around 70 for LLaMA based models. For ChatGPT, temperature does not play a signifi- cant role since it effect on F1 Score is at most 1.2. The GPT-4 model achieves the best performance across almost all metrics, except for recall, since GPT-4 models tend to generate shorter answers with fewer citations, resulting in higher precision. While LLaMA is better at Recall by generating long answers with many citations. The F1-Score of models from the same family are close to one an- other, showing that our automatic evaluation metric designed is reliable. \nText-Citation Alignment From Table  3 , similar to citation quality, the OpenAI models also out- perform the LLaMA based models on text-citation alignment. In addition, models with 7B, 13B, 175B (ChatGPT), and trillion level (GPT4) parameters have an alignment score of  $40+,60+,80+$  , and 92 respectively. LLaMA-13B model has an improve- ment of 14.3 compared to LLaMA-7B model. This shows that parameter size may play an important role in generating sentences and citations with good alignment. \nText Quality Evaluation We present the evalu- ation of generated text quality in Table  4 . From the results, we find that OpenAI models, in general, have better text quality in all metrics compared to LLaMA family models, which corresponds to the citation evaluation results. All models exhibit rather high consistency, indicating that the LLMs \nThe table presents a comparison of different language models across four evaluation metrics: Coherence (Coh.), Consistency (Con.), Fluency (Flu.), and Relevance (Rel.). The models compared include various versions of GPT-4 and ChatGPT, as well as Alpaca-7B, LLaMa-7B, LLaMa-13B, and Vicuna-13B. The numerical values in the table represent the scores for each metric, likely on a scale from 1 to 5, with higher scores indicating better performance. Here's a breakdown of the scores for each model:\n\n1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: \"Removed\", \"Corr.\", \"Prec.\", \"Rec.\", and \"F1.\"\n\n- The row labeled \"0 (gold)\" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled \"1\" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled \"2\" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled \"3\" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).\n\nThese columns likely represent metrics used to evaluate the performance of a model or process.\nare capable of generating answers that are not contradictory to the provided knowledge or self- contradictory. However, the relevance is relatively low for smaller models, indicating the difficulty these models face in generating answers that are relevant to the questions. \n5.2 Conscious Incompetence \nWe first evaluate  citation quality  of the generated text with knowledge removed using method de- scribed in   $\\S~4.4$  . From Table  5 , the removal of required knowledge has a minimal impact on cor- rectness, but significantly affects citation precision and recall. With more knowledge absent from pro- vided knowledge graph, both precision and recall drops drastically, demonstrating that the coverage issue poses a considerable challenge to generating answers with high quality citations. \nNext, we evaluate  [NA] precision  and  recall . "}
{"page": 7, "image_path": "doc_images/2310.05634v2_7.jpg", "ocr_text": "Experiment Result on Conscious Incompetence\n\n-@ precision\n24] —— recall\nFe FI_Score\n\n22\n\n20\n\nScore\n\n18\n\n16\n\n14\n\none two three\nnumber of knowledge removed\n\nFigure 4: Precision, Recall, and F1-Score for [NA].\n\nFrom Figure 4, The recall is stable at about 15\nregardless of the number of absent knowledge.\nThis indicates that the current LLMs have ability\nto identify absent knowledge to a limited extent.\nWhile precision and F1-Score exhibit a clear up-\nward trend, which shows that with more absent\nknowledge in KG, [NA] enables generated out-\nputs to locate absent knowledge more accurately.\nTherefore, the “Conscious Incompetence” setting\nplays an increasingly crucial role when the cover-\nage problem of knowledge graph is more serious.\n\n5.3 Retrieval Analysis\n\nWe conduct an ablation study to examine the impact\nof retrieval accuracy on the model’s output. The ex-\nperiment simulates retrieval accuracy from 100 to\n20 at intervals of 20. We start with the ground truth\nknowledge graphs that we used for question con-\nstruction. In each subsequent rounds, we randomly\nreplace additional 20% knowledge graphs with ir-\nrelevant knowledge graphs to simulate retrieving\nwrong graphs. The results for citation quality are\nin Figure 5. Answers are generated using ChatGPT\nwith a temperature of 0.5.\n\nThe results show clear downward trends in\nall metrics as expected when retrieval accuracy\ndropped. Among precision and recall, the impact\nof poor retrieval quality on recall (green) is much\nmore significant than on precision (yellow). This\nindicates that the model has the ability to filter out\nincorrect knowledge to a certain extent, resulting\nin less noticeable impact on precision compared\nto recall. The reduction in recall was nearly linear\nas retrieval accuracy decreased, which is under-\nstandable since a knowledge cannot be cited if it is\nnot provided. The greatest drop in recall occurred\nbetween the ground truth (57.1) and 80 accuracy\n\nRetrieval Analysis\n\nae\n\n60 —® precision\n= recall\n\n— FIScore\nte correctness\n\nScore\n\n40\n\n20\n\ngold 80 60 40 20\nRetrieval Accuracy\n\nFigure 5: Citation evaluation (Micro) of generated texts\nusing knowledge graphs with retrieval accuracy 100\n(gold), 80, 60,40, and 20.\n\nAlignment Human Avg.\nChatGPT(0.5) 84.5 82.0\nLLaMA-7B 47.8 45.5\nVicuna-13B 66.9 64.5\n\nTable 6: Result of Human Evaluation on text-citation\nalignment\n\n(42.5), demonstrating the potential of the model\nto generate high-quality citations under perfect re-\ntrieval conditions. In practice, a retrieval accuracy\nof 80 is closest to the actual scenario of our exper-\niment (our retrieval accuracy is 75.9). Therefore,\nwhen retrieval accuracy is reasonably high, the cor-\nrectness of citations is not the most significant con-\ncern compared to recall.\n\n5.4 Human Evaluation\n\nWe conduct human evaluation to verify the correla-\ntion between automatic evaluation and human judg-\nment. We randomly sample 100 sentence-citation\npairs from each of the three baselines: ChatGPT\n(temperature 0.5), LLaMA-7B, and Vicuna-13B.\nWe request two proficient English annotators for\neach baseline to determine if the citation aligns to\nthe sentence and provides support for it. The reason\nwe choose metric alignment here is in appendix C,\nwith instruction to annotators and IAA.\n\nThe comparison between automatically calcu-\nlated Alignment and human evaluation results is\nshown in Table 6. For all three baselines, the auto-\nmatic and human scores are close with a gap within\n2.5, despite the significant differences among the\nbaselines. This indicates a strong correlation be-\ntween the automatically calculated alignment and\nhuman judgments. The experiment results demon-\nstrate that the automatic evaluation serves as a reli-\n", "vlm_text": "The image is a line graph titled \"Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.\n\nThe x-axis represents the number of knowledge elements removed (labeled as \"one,\" \"two,\" and \"three\"), and the y-axis represents the score values for each metric.\nFrom Figure  4 , The recall is stable at about 15 regardless of the number of absent knowledge. This indicates that the current LLMs have ability to identify absent knowledge to a limited extent. While precision and F1-Score exhibit a clear up- ward trend, which shows that with more absent knowledge in KG, [NA] enables generated out- puts to locate absent knowledge more accurately. Therefore, the “Conscious Incompetence” setting plays an increasingly crucial role when the cover- age problem of knowledge graph is more serious. \n5.3 Retrieval Analysis \nWe conduct an ablation study to examine the impact of retrieval accuracy on the model’s output. The ex- periment simulates retrieval accuracy from 100 to 20 at intervals of 20. We start with the ground truth knowledge graphs that we used for question con- struction. In each subsequent rounds, we randomly replace additional   $20\\%$   knowledge graphs with ir- relevant knowledge graphs to simulate retrieving wrong graphs. The results for citation quality are in Figure  5 . Answers are generated using ChatGPT with a temperature of 0.5. \nThe results show clear downward trends in all metrics as expected when retrieval accuracy dropped. Among precision and recall, the impact of poor retrieval quality on recall (green) is much more significant than on precision (yellow). This indicates that the model has the ability to filter out incorrect knowledge to a certain extent, resulting in less noticeable impact on precision compared to recall. The reduction in recall was nearly linear as retrieval accuracy decreased, which is under- standable since a knowledge cannot be cited if it is not provided. The greatest drop in recall occurred between the ground truth (57.1) and 80 accuracy \nThe image is a line graph titled \"Retrieval Analysis,\" showing the evaluation of generated texts using knowledge graphs. The x-axis represents \"Retrieval Accuracy\" with values at gold (100), 80, 60, 40, and 20. The y-axis represents the \"Score.\"\n\nThere are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the \"Alignment\" and \"Human Avg.\" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5. \n\nThese scores may represent evaluations of model performance on alignment and human-average ratings, but the specific context or criteria for these evaluations are not provided in the table.\n(42.5), demonstrating the potential of the model to generate high-quality citations under perfect re- trieval conditions. In practice, a retrieval accuracy of 80 is closest to the actual scenario of our exper- iment (our retrieval accuracy is 75.9). Therefore, when retrieval accuracy is reasonably high, the cor- rectness of citations is not the most significant con- cern compared to recall. \n5.4 Human Evaluation \nWe conduct human evaluation to verify the correla- tion between automatic evaluation and human judg- ment. We randomly sample 100 sentence-citation pairs from each of the three baselines: ChatGPT (temperature 0.5), LLaMA-7B, and Vicuna-13B. We request two proficient English annotators for each baseline to determine if the citation aligns to the sentence and provides support for it. The reason we choose metric alignment here is in appendix  C , with instruction to annotators and IAA. \nThe comparison between automatically calcu- lated Alignment and human evaluation results is shown in Table  6 . For all three baselines, the auto- matic and human scores are close with a gap within 2.5, despite the significant differences among the baselines. This indicates a strong correlation be- tween the automatically calculated alignment and human judgments. The experiment results demon- strate that the automatic evaluation serves as a reli- "}
{"page": 8, "image_path": "doc_images/2310.05634v2_8.jpg", "ocr_text": "Citation Eval. Text Eval.\nSetting | Model Align. | Corr. Prec. Rec. Fl. | Coh. Con. Flu. Rel.\nGeneral | GPT-4 (0.5) 90.9 97.6 30.8 42.1 35.6] 438 4.77 448 4.48\nChatGPT (0.5) | 82.7 945 25.2 474 32.9 | 464 489 445 4.70\nSpecific | GPT-4 (0.5) 92.0 97.6 36.0 43.6 39.4| 448 489 464 4.72\nChatGPT (0.5) | 84.5 94.8 29.9 49.0 37.2| 457 4.94 4.71 4.81\n\nTable 7: Comparison of evaluation results on General and Specific question setting\n\nable measurement of the alignment between gener-\nated texts and citations.\n\n5.5 General and Specific Questions\n\nWe compare experiments results of text, citation\n(micro), and alignment between the general and\nspecific questions in Table 7. The results show that\nthe same model’s answers on specific questions\noutperform those on general questions in almost all\nmetrics. The finding is not surprising because the\nspecific questions provide clearer instructions to\nthe models on which knowledge to use. In addition,\nthe general questions in the dataset are inherently\nloosely bonded to the minimum knowledge set, and\nhence have impacts on the evaluation results. This\nexperiment shows a trade-off between how explic-\nitly the question context mentions the knowledge,\nand how irreplaceably the knowledge is required\nby the question. The specific questions target the\nknowledge more explicitly in the question context,\nand hence cover the scope of the paragraph better.\nIt stands for an upper bound for knowledge cover-\nage and a lower bound for question naturalness. The\ngeneral questions implicitly target the knowledge\nin the question context, and there loosely cover\nthe scope of the paragraph. It stands for an upper\nbound for question naturalness and a lower bound\nfor knowledge coverage.\n\n6 Related Work\n\nRetrieval-augmented LLMs_KiC (Pan et al.,\n2022) empower models with external memory of\nmultiple formats including knowledge graph but\ndoes not explore attribution. WebGPT (Nakano\net al., 2021) outsources document retrieval to Mi-\ncrosoft Bing and fine-tunes GPT3 to answer ques-\ntions. GopherCite (Menick et al., 2022) fine-tunes\na Gopher (Rae et al., 2021) model to generate text\nalongside quotes extracted from Google search.\nALCE (Gao et al., 2023) retrieves top-k passages\nfrom Wikipedia and asks LLMs to generate outputs\nwith citations to corresponding supporting docu-\nments. These works attribute LLMs to unstructured\n\ndocuments but not knowledge graph.\n\nEvaluation (Rashkin et al., 2021) define the “At-\ntributable to Identified Sources” (AIS) to mea-\nsure whether model-generated statements are sup-\nported by underlying sources. (Bohnet et al., 2022)\nstudy an automatic metric (AutoAIS) that formu-\nlates evaluation of automated question answer-\ning as a NLI task. (Yue et al., 2023) investigate\nhe automatic evaluation of attribution by prompt-\ning LLMs and fine-tuning smaller LMs. (Liu\net al., 2023a) conduct human evaluation to audit\ngenerative search engines for their citation quali-\nies. ALCE (Gao et al., 2023) evaluates generated\nanswers by comparing with gold answers using\nMAUVE, and calculates precision and recall for\ncitations using NLI. To the best of our knowledge,\nour evaluation methods are the first framework that\nrequires no human annotated data.\n\n7 Conclusion\n\nWe propose KaLMA that comprises a new dataset\nBioKaLMA, a pipeline for generating attributed\nanswers by retrieving from KGs, and a set of au-\ntomatic evaluation metrics to assess text quality,\ncitation quality, and text-citation alignment. We\nintroduce the “Conscious Incompetence” setting,\nenabling LLMs to identify the knowledge required\nto support the answers but is absent from the KG.\nThrough this benchmark, we address three chal-\nlenges: incorporating diverse attribution sources,\nlimited attribution source coverage, and the ab-\nsence of human annotated ground truth for auto-\nmatic evaluation.\n\nOur extensive experimental results demonstrate\nthat current LLMs still have room for improve-\nment when utilizing KGs as attribution sources.\nWe also highlight the increasing effectiveness of\n“Conscious Incompetence” setting as the coverage\nof attribution source becomes worse. Lastly, we\nprove the crucial role of retrieval accuracy in gen-\nerating high-quality attributed texts.\n", "vlm_text": "The table presents a comparison of performance metrics between two models, GPT-4 (0.5) and ChatGPT (0.5), under two different \"Settings\": General and Specific. The metrics are divided into two main categories: \"Citation Eval.\" and \"Text Eval.\"\n\n1. **Citation Eval.:** This section evaluates the models based on Alignment (Align.), Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1 Score (F1.). \n   - In the General setting:\n     - GPT-4 (0.5) scores 90.9 (Align.), 97.6 (Corr.), 30.8 (Prec.), 42.1 (Rec.), and 35.6 (F1.).\n     - ChatGPT (0.5) scores 82.7 (Align.), 94.5 (Corr.), 25.2 (Prec.), 47.4 (Rec.), and 32.9 (F1.).\n   - In the Specific setting:\n     - GPT-4 (0.5) scores 92.0 (Align.), 97.6 (Corr.), 36.0 (Prec.), 43.6 (Rec.), and 39.4 (F1.).\n     - ChatGPT (0.5) scores 84.5 (Align.), 94.8 (Corr.), 29.9 (Prec.), 49.0 (Rec.), and 37.2 (F1.).\n\n2. **Text Eval.:** This section assesses the models based on Coherence (Coh.), Conciseness (Con.), Fluency (Flu.), and Relevance (Rel.).\n   - In the General setting:\n     - GPT-4 (0.5) scores 4.38 (Coh.), 4.77 (Con.), 4.48 (Flu.), and 4.48 (Rel.).\n     - ChatGPT (0.5) scores 4.64 (Coh.), 4.89 (Con.), 4.45 (Flu.), and 4.70 (Rel.).\n   - In the Specific setting:\n     - GPT-4 (0.5) scores 4.48 (Coh.), 4.89 (Con.), 4.64 (Flu.), and 4.72 (Rel.).\n     - ChatGPT (0.5) scores 4.57 (Coh.), 4.94 (Con.), 4.71 (Flu.), and 4.81 (Rel.).\n\nOverall, the table indicates quantitative assessments of the two models across several performance metrics in different evaluation contexts (General vs. Specific). It seems that the \"Specific\" setting generally yields better performance on several evaluation metrics compared to the \"General\" setting, and ChatGPT tends to perform better in Text Evaluation, particularly in Conciseness, Fluency, and Relevance.\nable measurement of the alignment between gener- ated texts and citations. \n5.5 General and Specific Questions \nWe compare experiments results of text, citation (micro), and alignment between the general and specific questions in Table  7 . The results show that the same model’s answers on specific questions outperform those on general questions in almost all metrics. The finding is not surprising because the specific questions provide clearer instructions to the models on which knowledge to use. In addition, the general questions in the dataset are inherently loosely bonded to the minimum knowledge set, and hence have impacts on the evaluation results. This experiment shows a trade-off between how explic- itly the question context mentions the knowledge, and how ir replace ably the knowledge is required by the question. The specific questions target the knowledge more explicitly in the question context, and hence cover the scope of the paragraph better. It stands for an upper bound for knowledge cover- age and a lower bound for question naturalness.The general questions implicitly target the knowledge in the question context, and there loosely cover the scope of the paragraph. It stands for an upper bound for question naturalness and a lower bound for knowledge coverage. \n6 Related Work \nRetrieval-augmented LLMs KiC ( Pan et al. , 2022 ) empower models with external memory of multiple formats including knowledge graph but does not explore attribution. WebGPT ( Nakano et al. ,  2021 ) outsources document retrieval to Mi- crosoft Bing and fine-tunes GPT3 to answer ques- tions. GopherCite ( Menick et al. ,  2022 ) fine-tunes a Gopher ( Rae et al. ,  2021 ) model to generate text alongside quotes extracted from Google search. ALCE ( Gao et al. ,  2023 ) retrieves top-k passages from Wikipedia and asks LLMs to generate outputs with citations to corresponding supporting docu- ments. These works attribute LLMs to unstructured Evaluation ( Rashkin et al. ,  2021 ) define the “At- tributable to Identified Sources” (AIS) to mea- sure whether model-generated statements are sup- ported by underlying sources. ( Bohnet et al. ,  2022 ) study an automatic metric (AutoAIS) that formu- lates evaluation of automated question answer- ing as a NLI task. ( Yue et al. ,  2023 ) investigate the automatic evaluation of attribution by prompt- ing LLMs and fine-tuning smaller LMs. ( Liu et al. ,  2023a ) conduct human evaluation to audit generative search engines for their citation quali- ties. ALCE ( Gao et al. ,  2023 ) evaluates generated answers by comparing with gold answers using MAUVE, and calculates precision and recall for citations using NLI. To the best of our knowledge, our evaluation methods are the first framework that requires no human annotated data. \n\n7 Conclusion \nWe propose KaLMA that comprises a new dataset BioKaLMA, a pipeline for generating attributed answers by retrieving from KGs, and a set of au- tomatic evaluation metrics to assess text quality, citation quality, and text-citation alignment. We introduce the “Conscious Incompetence” setting, enabling LLMs to identify the knowledge required to support the answers but is absent from the KG. Through this benchmark, we address three chal- lenges: incorporating diverse attribution sources, limited attribution source coverage, and the ab- sence of human annotated ground truth for auto- matic evaluation. \nOur extensive experimental results demonstrate that current LLMs still have room for improve- ment when utilizing KGs as attribution sources. We also highlight the increasing effectiveness of “Conscious Incompetence” setting as the coverage of attribution source becomes worse. Lastly, we prove the crucial role of retrieval accuracy in gen- erating high-quality attributed texts. "}
{"page": 9, "image_path": "doc_images/2310.05634v2_9.jpg", "ocr_text": "Limitations\n\nOne limitation is that our work only investigates a\nsimple form of knowledge graph, where each node\nis an entity, and each sub-graph is a knowledge\ntriple. There are more complicated forms of knowl-\nedge graph, where each node is a document. We\nwill explore this setting in future works.\n\nAnother limitation lies within the text quality\nevaluation. We uses ChatGPT as the model to eval-\nuate texts, which could potentially have a bias if\nthe model prefers the text style generated by itself.\nSuch bias can be observed from the abnormal phe-\nnomenon that the scores of ChatGPT generated\nanswers are higher than that of the GPT4 gener-\nated answers for all four dimensions. Due to cost\nconsiderations, we do not repeat the text quality\nevaluation with GPT-4.\n\nEthical Considerations\n\nThe potential risk is when users leverage the au-\ntomatic dataset construction pipeline to generate\nmassive hazardous datasets. This can only happen\nwhen a structured knowledge of harmful content is\navailable. Otherwise there is no risk as long as the\nbenchmark is used correctly. All data are collected\nfrom WikiData which is publicly available. Hence\nthere is no privacy issue. We also conduct human\ncheck to ensure there is no offensive content.\n\nReferences\n\nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha-\nroni, Daniel Andor, Livio Baldini Soares, Massimil-\niano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,\nJonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma,\nJianmo Ni, Lierni Sestorain Saralegui, Tal Schus-\nter, William W. Cohen, Michael Collins, Dipanjan\nDas, Donald Metzler, Slav Petrov, and Kellie Webster.\n2023. Attributed question answering: Evaluation an\nmodeling for attributed large language models.\n\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\n2022. Attributed question answering: Evaluation an\nmodeling for attributed large language models. arXiv\npreprint arXiv:2212.08037.\n\nSamuel Bowman, Gabor Angeli, Christopher Potts, an\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvin\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\n\nlearners. Advances in neural information processing\nsystems, 33:1877-1901.\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\n\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282-7296, Online.\nAssociation for Computational Linguistics.\n\nPaul R Curtiss and Phillip W Warren. 1974. The dy-\nnamics of life skills coaching. life skills series.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine learning challenges workshop,\npages 177-190. Springer.\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\n\nOr Honovich, Roee Aharoni, Jonathan Herzig, Ha-\ngai Taitelbaum, Doron Kukliansy, Vered Cohen,\nThomas Scialom, Idan Szpektor, Avinatan Has-\nsidim, and Yossi Matias. 2022. True: Re-evaluating\nfactual consistency evaluation. arXiv preprint\narXiv:2204.04991.\n\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874-880, Online. Association for Computa-\ntional Linguistics.\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1-38.\n\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. volume 32.\n\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\njape, Christopher D. Manning, and Kyoung-Gu Woo.\n2022. You only need one model for open-domain\nquestion answering.\n\nNelson F Liu, Tianyi Zhang, and Percy Liang. 2023a.\nEvaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848.\n", "vlm_text": "Limitations \nOne limitation is that our work only investigates a simple form of knowledge graph, where each node is an entity, and each sub-graph is a knowledge triple. There are more complicated forms of knowl- edge graph, where each node is a document. We will explore this setting in future works. \nAnother limitation lies within the text quality evaluation. We uses ChatGPT as the model to eval- uate texts, which could potentially have a bias if the model prefers the text style generated by itself. Such bias can be observed from the abnormal phe- nomenon that the scores of ChatGPT generated answers are higher than that of the GPT4 gener- ated answers for all four dimensions. Due to cost considerations, we do not repeat the text quality evaluation with GPT-4. \nEthical Considerations \nThe potential risk is when users leverage the au- tomatic dataset construction pipeline to generate massive hazardous datasets. This can only happen when a structured knowledge of harmful content is available. Otherwise there is no risk as long as the benchmark is used correctly. All data are collected from WikiData which is publicly available. Hence there is no privacy issue. We also conduct human check to ensure there is no offensive content. \nReferences \nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha- roni, Daniel Andor, Livio Baldini Soares, Massimil- iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schus- ter, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2023.  Attributed question answering: Evaluation and modeling for attributed large language models . \nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022.  Attributed question answering: Evaluation and modeling for attributed large language models .  arXiv preprint arXiv:2212.08037 . \nSamuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot \nlearners.  Advances in neural information processing systems , 33:1877–1901. \nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.  Vicuna: An open- source chatbot impressing gpt-4 with   $90\\%^{*}$   chatgpt quality . \nElizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021.  All that’s ‘human’ is not gold: Evaluating human evaluation of generated text . In  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 7282–7296, Online. Association for Computational Linguistics. \nPaul R Curtiss and Phillip W Warren. 1974. The dy- namics of life skills coaching. life skills series. \nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment chal- lenge. In  Machine learning challenges workshop , pages 177–190. Springer. \nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. \nOr Honovich, Roee Aharoni, Jonathan Herzig, Ha- gai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Has- sidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991 . \nGautier Izacard and Edouard Grave. 2021.  Leveraging passage retrieval with generative models for open do- main question answering . In  Proceedings of the 16th Conference of the European Chapter of the Associ- ation for Computational Linguistics: Main Volume , pages 874–880, Online. Association for Computa- tional Linguistics. \nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation.  ACM Comput- ing Surveys , 55(12):1–38. \nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering . volume 32. \nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran- jape, Christopher D. Manning, and Kyoung-Gu Woo. 2022.  You only need one model for open-domain question answering . \nNelson F Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search engines. arXiv preprint arXiv:2304.09848 . "}
{"page": 10, "image_path": "doc_images/2310.05634v2_10.jpg", "ocr_text": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. arXiv preprint arXiv:2203.11147.\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\n\nXiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu,\nDong Yu, and Jianshu Chen. 2022. Knowledge-in-\ncontext: Towards knowledgeable semi-parametric\nlanguage models. arXiv preprint arXiv:2210.16433.\n\nJorge Pérez, Marcelo Arenas, and Claudio Gutierrez.\n2009. Semantics and complexity of sparql. ACM\nTransactions on Database Systems (TODS), 34(3):1-\n45.\n\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems, 34:48 16-4828.\n\nAlistair Plum, Tharindu Ranasinghe, Spencer Jones,\nConstantin Orasan, and Ruslan Mitkov. 2022. Bio-\ngraphical semi-supervised relation extraction dataset.\nIn Proceedings of the 45th International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval, pages 3121-3130.\n\nHongjin Qian, Zhicheng Dou, Jiejun Tan, Haonan Chen,\nHaogqi Gu, Ruofei Lai, Xinyu Zhang, Zhao Cao, and\nJi-Rong Wen. 2023. Optimizing factual accuracy in\ntext generation through dynamic knowledge selec-\ntion.\n\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n\n21(1):5485-5551.\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\n\nReitter. 2021. | Measuring attribution in natu-\nral language generation models. arXiv preprint\narXiv:2112.12870.\n\nRevanth Gangi Reddy, Yi R Fung, Qi Zeng, Manling Li,\nZiqi Wang, Paul Sullivan, et al. 2023. Smartbook: Ai-\nassisted situation report generation. arXiv preprint\narXiv:2303.14337.\n\nMichele Salvagno, Fabio Silvio Taccone, Alberto Gio-\nvanni Gerli, et al. 2023. Can artificial intelligence\nhelp for scientific writing? Critical care, 27(1):1-5.\n\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624-643, Online. As-\nsociation for Computational Linguistics.\n\nOzge Sevgili, Artem Shelmanov, Mikhail Arkhipov,\nAlexander Panchenko, and Chris Biemann. 2022.\nNeural entity linking: A survey of models based on\ndeep learning. Semantic Web, (Preprint):1-44.\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. arXiv preprint\narXiv:2104.07567.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Alpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nhttps://erfm. stanford. edu/2023/03/13/alpaca. html,\n3(6):7.\n\nJames Thorne, Andreas  Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume I (Long\nPapers), pages 809-819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Roziére, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\n\nDenny Vrande¢ié and Markus Krétzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78-85.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference.\n", "vlm_text": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b.  G-eval: Nlg evaluation using gpt-4 with better human align- ment . \nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell- Gillingham, Geoffrey Irving, et al. 2022. Teaching language models to support answers with verified quotes.  arXiv preprint arXiv:2203.11147 . \nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question- answering with human feedback. arXiv preprint arXiv:2112.09332 . \nXiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. 2022. Knowledge-in- context: Towards knowledgeable semi-parametric language models.  arXiv preprint arXiv:2210.16433 . \nJorge Pérez, Marcelo Arenas, and Claudio Gutierrez. 2009. Semantics and complexity of sparql.  ACM Transactions on Database Systems (TODS) , 34(3):1– 45. \nKrishna Pillutla, Swabha S way am dip ta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap be- tween neural text and human text using divergence frontiers.  Advances in Neural Information Process- ing Systems , 34:4816–4828. \nAlistair Plum, Tharindu Ranasinghe, Spencer Jones, Constantin Orasan, and Ruslan Mitkov. 2022. Bio- graphical semi-supervised relation extraction dataset. In  Proceedings of the 45th International ACM SI- GIR Conference on Research and Development in Information Retrieval , pages 3121–3130. \nHongjin Qian, Zhicheng Dou, Jiejun Tan, Haonan Chen, Haoqi Gu, Ruofei Lai, Xinyu Zhang, Zhao Cao, and Ji-Rong Wen. 2023.  Optimizing factual accuracy in text generation through dynamic knowledge selec- tion . \nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 . \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former.  The Journal of Machine Learning Research , 21(1):5485–5551. \nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David \nReitter. 2021. Measuring attribution in natu- ral language generation models. arXiv preprint arXiv:2112.12870 . \nRevanth Gangi Reddy, Yi R Fung, Qi Zeng, Manling Li, Ziqi Wang, Paul Sullivan, et al. 2023. Smartbook: Ai- assisted situation report generation.  arXiv preprint arXiv:2303.14337 . \nMichele Salvagno, Fabio Silvio Taccone, Alberto Gio- vanni Gerli, et al. 2023. Can artificial intelligence help for scientific writing?  Critical care , 27(1):1–5. \nTal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence . In  Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 624–643, Online. As- sociation for Computational Linguistics. \nÖzge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko, and Chris Biemann. 2022. Neural entity linking: A survey of models based on deep learning.  Semantic Web , (Preprint):1–44. \nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation.  arXiv preprint arXiv:2104.07567 . \nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong, replicable instruction-following model.  Stan- ford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 3(6):7.\nJames Thorne, Andreas Vlachos, Christos Christo dou lo poul os, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics. \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models . \nDenny Vrandeˇ ci´ c and Markus Krötzsch. 2014. Wiki- data: a free collaborative knowledge base.  Communi- cations of the ACM, 57(10):78–85.\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018.  A broad-coverage challenge corpus for sen- tence understanding through inference . "}
{"page": 11, "image_path": "doc_images/2310.05634v2_11.jpg", "ocr_text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions.\n\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nbertserini. arXiv preprint arXiv: 1902.01718.\n\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,\nand Huan Sun. 2023. Automatic evaluation of at-\ntribution by large language models. arXiv preprint\narXiv:2305.06311.\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019a. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv: 1904.09675.\n\nYuan Zhang, Jason Baldridge, and Luheng He. 2019b.\nPAWS: Paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume I (Long and Short Papers), pages 1298-1308,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\n\nGuido Zuccon and Bevan Koopman. 2023. Dr chatgpt,\ntell me what i want to hear: How prompt knowledge\nimpacts health answer correctness. arXiv preprint\narXiv:2302.13793.\n\nA _ Dataset Construction\n\nIn this section, we will explain the detailed process\nand algorithms for the automatic dataset construc-\ntion pipeline. Using this pipeline, we are able to\nconstruct datasets with a greater scale or in other\ndomains.\n\nA.1_ Person Selection\n\nTo improve the complexity of the questions and dif-\nficulty to LLMs, we involve more than one person\nin each question. In addition, we need high qual-\nity paragraphs for subsequent dataset generation\nsteps. Therefore, we utilize name pairs and para-\ngraphs from the biographical database, which is\na database specifically designed for the relation ex-\ntraction (RE) task. Each piece of data from the bio-\ngraphical database includes a short paragraph, and\na relation triple extracted from the paragraph. The\nrelation triple consists of two people and their re-\nlationship such as <William Shakespeare, Spouse,\nAnne Hathaway>. The biographical database in-\ncludes an automatically extracted set and a human\nannotated set. We specifically choose the human\nannotated set from the database to ensure high-\nquality name pairs. To avoid potential ambiguities,\nwe filter out data if any name in the triple is incom-\nplete. In practice, we consider a name complete if\nit has at least a family name and a surname.\n\nA.2 Name Disambiguation\n\nDue to the presence of duplicate names (e.g., Anne\nHathaway: the actress, or the wife of William\nShakespeare), we perform name disambiguation to\nmap each name in the triple to a unique entity from\nthe knowledge graph. We utilize WikiData* (Vran-\ndeéié and Krétzsch, 2014) as the knowledge base\nand employ SPARQL (Pérez et al., 2009) queries to\nretrieve all entities associated with the name. Wiki-\nData assigns a unique QID to each entity which\ndistinguishes between entities with the same name.\nIn WikiData, each entity represents a node in the\nknowledge graph. Since each triple consists of two\nnames and one relation, we select the two entities\nobtained from the query if they are connected to\neach other on WikiData. Additionally, the connect-\ning edge should align with the relation specified\nin the triple. Subsequently, we extract the one-\nhop sub-graph centered around each person node,\nwhich provides properties related to the person,\n\n‘https: //www.wikidata.org/wiki/\nWikidata:Main_Page\n", "vlm_text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.  Wizardlm: Empowering large language models to follow complex instructions . Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with bertserini.  arXiv preprint arXiv:1902.01718 . Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of at- tribution by large language models.  arXiv preprint arXiv:2305.06311 . Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein- berger, and Yoav Artzi. 2019a. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675 . Yuan Zhang, Jason Baldridge, and Luheng He. 2019b. PAWS: Paraphrase adversaries from word scrambling . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 1298–1308, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Guido Zuccon and Bevan Koopman. 2023. Dr chatgpt, tell me what i want to hear: How prompt knowledge impacts health answer correctness.  arXiv preprint arXiv:2302.13793 . \nA Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection \nTo improve the complexity of the questions and dif- ficulty to LLMs, we involve more than one person in each question. In addition, we need high qual- ity paragraphs for subsequent dataset generation steps. Therefore, we utilize name pairs and para- graphs from the  biographical database , which is a database specifically designed for the relation ex- traction (RE) task. Each piece of data from the bio- graphical database includes a short paragraph, and a relation triple extracted from the paragraph. The relation triple consists of two people and their re- lationship such as <William Shakespeare, Spouse, Anne Hathaway>. The biographical database in- cludes an automatically extracted set and a human annotated set. We specifically choose the human annotated set from the database to ensure high- quality name pairs. To avoid potential ambiguities, we filter out data if any name in the triple is incom- plete. In practice, we consider a name complete if it has at least a family name and a surname. \nA.2 Name Disambiguation \nDue to the presence of duplicate names (e.g., Anne Hathaway: the actress, or the wife of William Shakespeare), we perform name disambiguation to map each name in the triple to a unique entity from the knowledge graph. We utilize WikiData 4   ( Vran- deˇ ci´ c and Krötzsch ,  2014 ) as the knowledge base and employ SPARQL ( Pérez et al. ,  2009 ) queries to retrieve all entities associated with the name. Wiki- Data assigns a unique QID to each entity which distinguishes between entities with the same name. In WikiData, each entity represents a node in the knowledge graph. Since each triple consists of two names and one relation, we select the two entities obtained from the query if they are connected to each other on WikiData. Additionally, the connect- ing edge should align with the relation specified in the triple. Subsequently, we extract the one- hop sub-graph centered around each person node, which provides properties related to the person, such as gender, birth date, occupation, and more. We convert ambiguous person names from previous steps to unique QID from WikiData. The extracted sub-graphs contain all knowledge from WikiData about the selected people. We call the extracted graphs “knowledge pool”. "}
{"page": 12, "image_path": "doc_images/2310.05634v2_12.jpg", "ocr_text": "such as gender, birth date, occupation, and more.\nWe convert ambiguous person names from previous\nsteps to unique QID from WikiData. The extracted\nsub-graphs contain all knowledge from WikiData\nabout the selected people. We call the extracted\ngraphs “knowledge pool”.\n\nA.3 Evolutionary Question Generation\n\n°\n\nWe employ an “evolutionary question generation’\napproach inspired by WizardLM (Xu et al., 2023)\nand DKGen (Qian et al., 2023), where we gradually\nincrease the set of knowledge required by injecting\nknowledge through iterations. In each iteration,\nLLMs extend the paragraph with one sentence by\nincorporating the additional knowledge. After the\nlast iteration, LLMs propose two questions accord-\ning to the extended paragraph, one is a general\nversion, and the other is a specific version. The\ngeneral question is more concise, and the specific\nquestion is more detailed. Both questions target\nthe same set of knowledge. All injected knowl-\nedge form a “minimum knowledge set”, which\nincludes the least knowledge required to answer\nthe proposed question (Table 1). We do not throw\nall knowledge to LLM at once to form a paragraph\nbecause extending the paragraph and knowledge\nset incrementally allow us to select the appropriate\nknowledge after each iteration.\n\nIn the first iteration, LLMs annotate the orig-\ninal paragraph from Biographical Database with\nthe knowledge from the “knowledge pool”. For in-\nstance, the sentence “Artemisia was born in Rome.”\nis annotated with knowledge [Artemisia , place of\nbirth, Rome]. In each subsequent iteration, we se-\nlect a piece of appropriate knowledge according to\nthe existing paragraph. A sentence with appropriate\nknowledge should have good specificity and coher-\nence. Specificity refers to the significance of the\nknowledge, such that it is not too general or trivial.\nCoherence refers to the naturalness of the addi-\ntional knowledge. The added knowledge should\nnot deviate from the existing paragraph and should\nbe coherent when reading. During knowledge se-\nlection, each piece of knowledge is assigned a score\nby adding the specificity score and coherence score.\nThe specificity score measures the uniqueness of\nthe knowledge. We discourage the system from\nselecting too frequent relation types like “gender”\nor \"date of birth\" which may be less informative.\nA less frequent relation tend to provide a knowl-\nedge specific to the person. Derived from IDF, we\ncalculate the number of occurrences Count, for\n\neach relation r in the dataset with size NV. The\ncoherence score is calculated through perplexity.\nWe convert each piece of knowledge to a simple\nsentence by applying a template. For instance, the\nknowledge [Artemisia, place of birth, Rome] is\nconverted to “Artemisia’s place of birth is Rome”.\nThere are three templates depending on the POS of\nthe relation. We append each sentence to the orig-\ninal paragraph and calculate normalized inverse\nperplexity to obtain coherence score. The overall\nscore is a weighted sum of specificity score and\ncoherence score:\n\nScore, =a: log(2- N/Count,) 1\n\n+ (1 — a) - softmax(1/perp,) a)\nIn each iteration, we leverage the “text-davinci-\n003” model for annotation or generation with in-\ncontext learning. We provide separate instructions\nand demonstrations for general and specific ques-\ntions. The detailed prompt templates used is pro-\nvided in the appendix D. We provide one human\nwritten demonstration. Some examples of full ques-\ntion evolution process are provided in appendix E.\nIn practice, we employ five iterations to ensure suf-\nficient complexity in the questions without making\nthem overly tedious.\n\nB_ Experiment Details\n\nB.1 Main Experiment\n\nFor the main experiments, we run reach model with\ndifferent seeds for three times. The OpenAI fam-\nily models are implemented using OpenAI APIs.\nRunning one round of experiment with ChatGPT\nmodel takes approximately 1 hour, and costs about\n3 USD. Running one round of experiment with\nGPT4 model takes approximately 1.5 to 2 hours,\nand costs about 60 USD. Each LLaMA family\nmodel is run on one TESLA V100 GPU, where\neach run takes about 6 to 8 hours for Alpaca-7B and\nVicuna-13B, and about 12-16 hours for LLaMA-7B\nand LLaMA-13B.\n\nB.2 Text Quality Evaluation\n\nFor text quality evaluation, we use the model “text-\ndavinci-003” with temperature 0 to ensure stability\nand reproducibility of the results. We randomly\nsample 100 outputs from each baseline and take\nthree runs to report mean. We do not report stan-\ndard deviation since most of them are mostly in-\nsignificantly small (below 0.1).\n", "vlm_text": "\nA.3 Evolutionary Question Generation \nWe employ an “evolutionary question generation” approach inspired by WizardLM ( Xu et al. ,  2023 ) and DKGen ( Qian et al. ,  2023 ), where we gradually increase the set of knowledge required by injecting knowledge through iterations. In each iteration, LLMs extend the paragraph with one sentence by incorporating the additional knowledge. After the last iteration, LLMs propose two questions accord- ing to the extended paragraph, one is a general version, and the other is a specific version. The general question is more concise, and the specific question is more detailed. Both questions target the same set of knowledge. All injected knowl- edge form a “ minimum knowledge set ”, which includes the least knowledge required to answer the proposed question (Table  1 ). We do not throw all knowledge to LLM at once to form a paragraph because extending the paragraph and knowledge set increment ally allow us to select the appropriate knowledge after each iteration. \nIn the first iteration, LLMs annotate the orig- inal paragraph from Biographical Database with the knowledge from the “knowledge pool”. For in- stance, the sentence “Artemisia was born in Rome.” is annotated with knowledge [Artemisia , place of birth, Rome]. In each subsequent iteration, we se- lect a piece of appropriate knowledge according to the existing paragraph. A sentence with appropriate knowledge should have good  specificity  and  coher- ence . Specificity refers to the significance of the knowledge, such that it is not too general or trivial. Coherence refers to the naturalness of the addi- tional knowledge. The added knowledge should not deviate from the existing paragraph and should be coherent when reading. During knowledge se- lection, each piece of knowledge is assigned a score by adding the specificity score and coherence score. The specificity score measures the uniqueness of the knowledge. We discourage the system from selecting too frequent relation types like “gender” or \"date of birth\" which may be less informative. A less frequent relation tend to provide a knowl- edge specific to the person. Derived from IDF, we calculate the  number of occurrences  $C o u n t_{r}$   for each  relation  $r$   in the  dataset with size    $N$  . The coherence score is calculated through perplexity. We convert each piece of knowledge to a simple sentence by applying a template. For instance, the knowledge [Artemisia, place of birth, Rome] is converted to “Artemisia’s place of birth is Rome”. There are three templates depending on the POS of the relation. We append each sentence to the orig- inal paragraph and calculate normalized inverse perplexity to obtain coherence score. The overall score is a weighted sum of specificity score and coherence score: \n\n\n$$\n\\begin{array}{r l}{S c o r e_{r}=\\alpha\\cdot\\log(2\\cdot N/C o u n t_{r})}&{{}}\\\\ {+\\left(1-\\alpha\\right)\\cdot\\mathrm{softmax}(1/p e r p_{r})}&{{}}\\end{array}\n$$\n \nIn each iteration, we leverage the “text-davinci-  $_{003}{}^{\\circ}$   model for annotation or generation with in- context learning. We provide separate instructions and demonstrations for general and specific ques- tions. The detailed prompt templates used is pro- vided in the appendix  D . We provide one human written demonstration. Some examples of full ques- tion evolution process are provided in appendix  E . In practice, we employ five iterations to ensure suf- ficient complexity in the questions without making them overly tedious. \nB Experiment Details \nB.1 Main Experiment \nFor the main experiments, we run reach model with different seeds for three times. The OpenAI fam- ily models are implemented using OpenAI APIs. Running one round of experiment with ChatGPT model takes approximately 1 hour, and costs about 3 USD. Running one round of experiment with GPT4 model takes approximately 1.5 to 2 hours, and costs about 60 USD. Each LLaMA family model is run on one TESLA V100 GPU, where each run takes about 6 to 8 hours for Alpaca-7B and Vicuna-13B, and about 12-16 hours for LLaMA-7B and LLaMA-13B. \nB.2 Text Quality Evaluation \nFor text quality evaluation, we use the model “text- davinci-  $.003^{\\circ}$   with temperature 0 to ensure stability and reproducibility of the results. We randomly sample 100 outputs from each baseline and take three runs to report mean. We do not report stan- dard deviation since most of them are mostly in- significantly small (below 0.1). "}
{"page": 13, "image_path": "doc_images/2310.05634v2_13.jpg", "ocr_text": "B.3 NLI\n\nFor the automatic evaluation of text citation align-\nment and evaluation of the known unknown cita-\ntions, we implement the TRUE model from Hug-\ngingFace®, which was trained on SNLI (Bow-\nman et al., 2015), MNLI (Williams et al., 2018),\nFever (Thorne et al., 2018), Scitail (Khot et al.,\n2018), PAWS (Zhang et al., 2019b), and Vita-\nminC (Schuster et al., 2021). The model uses\nthe prompt of “premise: {PREMISE} hypothesis:\n{HYPOTHESIS }”. For each sentence citation pair,\nwe place the sentence in the “PREMISE”, and the\ncitation to the “HYPOTHESIS”, like the following:\n“premise: {Hertwig served as a professor at the Uni-\nversity of Jena for the last 40 years of his career. }\nhypothesis: {employer: University of Jena}”\n\nC Human Evaluation\n\nC.1 Dataset Evaluation\n\nTo evaluate the dataset quality, we have two indi-\nvidual annotators who are proficient in the English\nlanguage. Below are the exact method for evaluat-\ning each metric:\n\n¢ Authenticity. We ask the annotators to check\nfrom WikiPedia and understand the back-\nground stories of the mentioned people, and\ndecide if the generated question matches the\nbackground story. Each question is assigned\nscore | if it matches the background story, and\nscore 0 if there is contradiction.\n\nRelevance. After understanding the back-\nground stories, we ask the annotators to label\neach piece of knowledge from the minimum\nknowledge set. A piece of knowledge is la-\nbeled 1 if the annotator thinks it is necessary\nto answer the question, and 0 if it is redun-\ndant. The relevance score of a question is the\nratio of number of necessary knowledge to the\nnumber of knowledge in the minimum set.\n\nNaturalness. We ask the annotators to give\nan integer score | to 5 to label each question.\n5 means the question can be easily understand-\nable, and is concise. 1 means the question is\nnot written in natural English language or is\nextremely tedious.\n\nSignificance. We ask the annotators to give\nan integer score | to 5 to label each question.\n\nShttps://huggingface.co/google/t5_xxl_\ntrue_nli_mixture\n\n5 means the annotator feels that he or she\nmay be interested in this question under some\ncircumstances, and | means the opposite.\n\nThe agreement between the two annotators are as\nfollow: the agreement between them is 100% for\nauthenticity and 86% for relevance. Since the eval-\nuation for naturalness and significance are score\nbased, in 92% and 90% of the evaluated datasets\nrespectively, the score difference between the two\nannotators is no larger than 1.\n\nC.2. Generated Text Evaluation\n\nAmong text quality evaluation, citation quality eval-\nuation, and text-citation alignment, we conduct hu-\nman evaluation on text-citation alignment. Text\nquality evaluation is conducted using G-Eval. We\nacknowledge this is not a perfect metric, but the hu-\nman evaluation is conducted in (Liu et al., 2023b).\nThe focus is this paper is not to improve G-Eval.\nCitation quality evaluation is conducted with look-\ning for exact match between generated citations\nand minimum knowledge set, which is an objective\nevaluation. The text-citation alignment evaluation\nis conducted using NLI, which we are not certain\nif entailment means providing support. In addition,\nwhether a knowledge supports a sentence can be\nsubjective. Therefore, we conduct human evalua-\ntion on alignment.\n\nWe present the Human Evaluation Instructions\nprovided to the annotators in Table 8. We follow\nthe implementation from (Clark et al., 2021), and\nprovide detailed instructions and examples to im-\nprove evaluation accuracy. For this human evalu-\nation, there are four individual annotators in total.\nWe arrange different annotators for different base-\nlines, and each baseline has two annotators. The\nInter-Annotator Agreement for ChatGPT, LLaMA-\n7B, and Vicuna-13B are reported as follows: 90%,\n97%, and 89% respectively.\n\nD_ Prompts\n\nWe present the prompts and instructions we used\nin this section. We present the prompts for the\nevolutionary question construction in Table 9, 10,\n11, and 12. We present the prompt for the answer\ngeneration in Table 13. We present the prompts we\nuse for text evaluation with G-Eval in Table 14, 15,\n16, and 17.\n", "vlm_text": "B.3 NLI \nFor the automatic evaluation of text citation align- ment and evaluation of the known unknown cita- tions, we implement the TRUE model from Hug- gingFace 5 , which was trained on SNLI ( Bow- man et al. ,  2015 ), MNLI ( Williams et al. ,  2018 ), Fever ( Thorne et al. ,  2018 ), Scitail ( Khot et al. , 2018 ), PAWS ( Zhang et al. ,  2019b ), and Vita- minC ( Schuster et al. ,  2021 ). The model uses the prompt of “premise: {PREMISE} hypothesis: {HYPOTHESIS}”. For each sentence citation pair, we place the sentence in the “PREMISE”, and the citation to the “HYPOTHESIS”, like the following: “premise: {Hertwig served as a professor at the Uni- versity of Jena for the last 40 years of his career.} hypothesis: {employer: University of Jena}” \nC Human Evaluation \nC.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set. \n•  Naturalness . We ask the annotators to give an integer score 1 to 5 to label each question. 5 means the question can be easily understand- able, and is concise. 1 means the question is not written in natural English language or is extremely tedious. \n•  Significance . We ask the annotators to give an integer score 1 to 5 to label each question. \n5 means the annotator feels that he or she may be interested in this question under some circumstances, and 1 means the opposite. \nThe agreement between the two annotators are as follow: the agreement between them is   $100\\%$   for authenticity and   $86\\%$   for relevance. Since the eval- uation for naturalness and significance are score based, in   $92\\%$   and  $90\\%$   of the evaluated datasets respectively, the score difference between the two annotators is no larger than 1. \nC.2 Generated Text Evaluation \nAmong text quality evaluation, citation quality eval- uation, and text-citation alignment, we conduct hu- man evaluation on text-citation alignment. Text quality evaluation is conducted using G-Eval. We acknowledge this is not a perfect metric, but the hu- man evaluation is conducted in ( Liu et al. ,  2023b ). The focus is this paper is not to improve G-Eval. Citation quality evaluation is conducted with look- ing for exact match between generated citations and minimum knowledge set, which is an objective evaluation. The text-citation alignment evaluation is conducted using NLI, which we are not certain if entailment means providing support. In addition, whether a knowledge supports a sentence can be subjective. Therefore, we conduct human evalua- tion on alignment. \nWe present the Human Evaluation Instructions provided to the annotators in Table  8 . We follow the implementation from ( Clark et al. ,  2021 ), and provide detailed instructions and examples to im- prove evaluation accuracy. For this human evalu- ation, there are four individual annotators in total. We arrange different annotators for different base- lines, and each baseline has two annotators. The Inter-Annotator Agreement for ChatGPT, LLaMA- 7B, and Vicuna-13B are reported as follows:   $90\\%$  ,  $97\\%$  , and   $89\\%$   respectively. \nD Prompts \nWe present the prompts and instructions we used in this section. We present the prompts for the evolutionary question construction in Table  9 ,  10 , 11 , and  12 . We present the prompt for the answer generation in Table  13 . We present the prompts we use for text evaluation with G-Eval in Table  14 ,  15 , 16 , and  17 . "}
{"page": 14, "image_path": "doc_images/2310.05634v2_14.jpg", "ocr_text": "Annotation Method:\n\nEach evaluation content includes a\nsentence and a piece of knowledge.\nOur task is to determine whether this\nsentence contains the given knowledge,\ni.e., whether this knowledge provides\nsupport for the sentence. If the\nsentence does not mention the given\nknowledge or if the content of the\nsentence does not align with the\nknowledge, it is considered\nunsupported. We use 1 to indicate\nsupport and 0 to indicate lack of\nsupport.\n\nHere are some examples:\n\nSentence: Stephen Crane was an\nAmerican writer born on November 1,\n1871, in Newark, and died on June 5,\n1900, in Badenweiler.\n\nKnowledge: date of birth:\n\n1871-11-01\n\nResult: 1, because the sentence’s\ndate of birth matches the knowledge’s\ndate of birth.\n\nSentence: Merton died on December 10,\n1968, in Bangkok, Thailand.\nKnowledge: country of citizenship:\nUnited States of America\nResult: 0, because the sentence does\nnot mention Merton’s nationality.\n\nTable 8: Instruction we provide to the human annotators.\n\nE_ Evolutionary Question Generation\n\nWe provide an example of evolutionary question\ngeneration in Table 18.\n\nF Examples\n\nWe show examples of the attributed answers gener-\nated by the LLMs in Table 19 and 20.\n", "vlm_text": "Each evaluation content includes a sentence and a piece of knowledge. Our task is to determine whether this sentence contains the given knowledge, i.e., whether this knowledge provides support for the sentence. If the sentence does not mention the given knowledge or if the content of the sentence does not align with the knowledge, it is considered unsupported. We use 1 to indicate support and 0 to indicate lack of support. \nHere are some examples: \nSentence: Stephen Crane was an American writer born on November 1, 1871, in Newark, and died on June 5, 1900, in Baden weil er. Knowledge: date of birth: 1871-11-01 Result: 1, because the sentence’s date of birth matches the knowledge’s date of birth. \nSentence: Merton died on December 10, 1968, in Bangkok, Thailand. Knowledge: country of citizenship: United States of America Result: 0, because the sentence does not mention Merton’s nationality. \nTable 8: Instruction we provide to the human annotators. \nE Evolutionary Question Generation \nWe provide an example of evolutionary question generation in Table  18 . \nF Examples \nWe show examples of the attributed answers gener- ated by the LLMs in Table  19  and  20 . "}
{"page": 15, "image_path": "doc_images/2310.05634v2_15.jpg", "ocr_text": "nstruction: Your objective is to select relevant knowledge to\nlabel the sentence and generate a question\n\nsentence: Artemisia Gentileschi was born Artemisia Gentileschi\nLomi in Rome on July 8 1593 although her birth certificate from the\nArchivio di Stato indicated she was born in 1590 the eldest child\nof the Tuscan painter Orazio Gentileschi and Prudenzia di Ottaviano\nMontoni.\n\nknowledge: {qid: 0367360, name: Orazio Gentileschi, sex or\ngender: male, place of birth: Pisa, place of death: London,\ninstance of: human, occupation: painter, child: Artemisia\nGentileschi, described by source: The Great Theatre of Dutch\n\nPainters, notable works: Diana the Huntress, given name: Orazio,\ntopic’s main category: Category:Orazio Gentileschi, surname:\nGentileschi, genre: portrait, languages spoken: Italian,\nmovement: mannerism, work location: Rome, ethnic group:\n\nItalians, date of birth: 1563-07-19, date of death: 1639-02-07}\n{qid: Q212657, name: Artemisia Gentileschi, sex or gender:\nfemale, place of birth: Rome, place of death: Naples, instance\nof: human, occupation: painter, member of: Accademia delle Arti\ndel Disegno, father: Orazio Gentileschi, described by source: The\nGreat Theatre of Dutch Painters, notable works: Judith Slaying\nHolofernes, topic’s main category: Category:Artemisia Gentileschi,\nmovement: Caravaggisti, ethnic group: Italians, work location:\nFlorence, depicted by: Artemisia, field of work: painting,\nsurname: Gentileschi, genre: portrait, languages spoken:\n\nItalian, position held: court painter, student of: Orazio\nGentileschi, spouse: Pierantonio Stiattesi, given name:\n\nArtemisia, mother: Prudenzia di Ottaviano Montoni, date of birth:\n1596-07-08, date of death: 1654-01-01}\n\nGenerated Answer: Artemisia Gentileschi [qid: 0212657, name:\nArtemisia Gentileschi] was born Artemisia Gentileschi Lomi in Rome\n[qid: Q212657, place of birth: Rome] on July 8 1593 [qid:\nQ212657, date of birth: 1596-07-08] although her birth certificate\nfrom the Archivio di Stato indicated she was born in 1590 the\neldest child of the Tuscan painter Orazio Gentileschi [qid:\nQ212657, father: Orazio Gentileschi] [qid: 0367360, name: Orazio\nGentileschi, occupation: painter] and Prudenzia di Ottaviano\nMontoni.\n\nTable 9: Instruction and demonstration for initial round of evolutionary question construction. We use brown color\nfor the knowledge, and blue color for the demonstration of expected output.\n", "vlm_text": "Instruction: Your objective is to select relevant knowledge to label the sentence and generate a question \nsentence: Artemisia Gentiles chi was born Artemisia Gentiles chi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi and Prudenzia di Ottaviano Montoni. \nknowledge: {qid: Q367360, name: Orazio Gentiles chi, sex or gender: male, place of birth: Pisa, place of death: London, instance of: human, occupation: painter, child: Artemisia Gentiles chi, described by source: The Great Theatre of Dutch Painters, notable works: Diana the Huntress, given name: Orazio, topic’s main category: Category:Orazio Gentiles chi, surname: Gentiles chi, genre: portrait, languages spoken: Italian, movement: mannerism, work location: Rome, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07} {qid: Q212657, name: Artemisia Gentiles chi, sex or gender: female, place of birth: Rome, place of death: Naples, instance of: human, occupation: painter, member of: Accademia delle Arti del Disegno, father: Orazio Gentiles chi, described by source: The Great Theatre of Dutch Painters, notable works: Judith Slaying Holofernes, topic’s main category: Category:Artemisia Gentiles chi, movement: Car ava gg is ti, ethnic group: Italians, work location: Florence, depicted by: Artemisia, field of work: painting, surname: Gentiles chi, genre: portrait, languages spoken: Italian, position held: court painter, student of: Orazio Gentiles chi, spouse: Pier antonio Stiattesi, given name: Artemisia, mother: Prudenzia di Ottaviano Montoni, date of birth: 1596-07-08, date of death: 1654-01-01} \nGenerated Answer: Artemisia Gentiles chi [qid: Q212657, name: Artemisia Gentiles chi] was born Artemisia Gentiles chi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi [qid: Q212657, father: Orazio Gentiles chi] [qid: Q367360, name: Orazio Gentiles chi, occupation: painter] and Prudenzia di Ottaviano Montoni. "}
{"page": 16, "image_path": "doc_images/2310.05634v2_16.jpg", "ocr_text": "Instruction: Your objective is to extend the original paragraph by\nadding one sentence that includes the given knowledge\n\nanswer: Artemisia Gentileschi [qid: 9212657, name: Artemisia\nGentileschi] was born Artemisia Gentileschi Lomi in Rome [qid:\nQ212657, place of birth: Rome] on July 8 1593 [qid: 0212657, date\nof birth: 1596-07-08] although her birth certificate from the\nArchivio di Stato indicated she was born in 1590 the eldest child\nof the Tuscan painter Orazio Gentileschi [qid: 0212657, father:\nOrazio Gentileschi] [qid: 0367360, name: Orazio Gentileschi,\noccupation: painter] and Prudenzia di Ottaviano Montoni.\nknowledge: {qid: 0212657, name: Artemisia Gentileschi, notable\nworks: Judith Slaying Holofernes}\n\nGenerated Answer: Artemisia Gentileschi [qid: 0212657, name:\nArtemisia Gentileschi] was born Artemisia Gentileschi Lomi in Rome\n[qid: Q212657, place of birth: Rome] on July 8 1593 [qid:\nQ212657, date of birth: 1596-07-08] although her birth certificate\nfrom the Archivio di Stato indicated she was born in 1590 the\neldest child of the Tuscan painter Orazio Gentileschi [qid:\nQ212657, father: Orazio Gentileschi] [qid: 0367360, name: Orazio\nGentileschi, occupation: painter] and Prudenzia di Ottaviano\nMontoni. Under the influence of her father, Artemisia Gentileschi\ncreated her iconic painting Judith Slaying Holofernes [qid:\nQ212657, notable works: Judith Slaying Holofernes] when she was\naround twenty years old.\n\nTable 10: Instruction and demonstration for evolutionary question construction for round 2 to 5. We use brown\n\ncolor for the knowledge, and blue color for the demonstration of expected output.\n\n", "vlm_text": "Instruction: Your objective is to extend the original paragraph by adding one sentence that includes the given knowledge \nanswer: Artemisia Gentiles chi [qid: Q212657, name: Artemisia Gentiles chi] was born Artemisia Gentiles chi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi [qid: Q212657, father: Orazio Gentiles chi] [qid: Q367360, name: Orazio Gentiles chi, occupation: painter] and Prudenzia di Ottaviano Montoni. knowledge: {qid: Q212657, name: Artemisia Gentiles chi, notable works: Judith Slaying Holofernes} \nGenerated Answer: Artemisia Gentiles chi [qid: Q212657, name: Artemisia Gentiles chi] was born Artemisia Gentiles chi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi [qid: Q212657, father: Orazio Gentiles chi] [qid: Q367360, name: Orazio Gentiles chi, occupation: painter] and Prudenzia di Ottaviano Montoni. Under the influence of her father, Artemisia Gentiles chi created her iconic painting Judith Slaying Holofernes [qid: Q212657, notable works: Judith Slaying Holofernes] when she was around twenty years old. \nTable 10: Instruction and demonstration for evolutionary question construction for round 2 to 5. We use brown color for the knowledge, and blue color for the demonstration of expected output. "}
{"page": 17, "image_path": "doc_images/2310.05634v2_17.jpg", "ocr_text": "Instruction: Your objective is to ask a question whose answer is\nthe given paragraph. There should be only one question when\npossible, if not, make sure the question is as concise as possible.\n\nParagraph: Artemisia Gentileschi was born Artemisia Gentileschi\nLomi in Rome on July 8 1593 although her birth certificate from the\nArchivio di Stato indicated she was born in 1590 the eldest child\nof the Tuscan painter Orazio Gentileschi and Prudenzia di Ottaviano\nMontoni. Her life and work were later depicted in the film\nArtemisia; which brought her story to a wider audience. Her\nfather, Orazio, was a prominent figure in the Mannerism art\nmovement, which likely influenced Artemisia’s own artistic style.\nHowever, Artemisia herself was a part of the Caravaggisti movement,\na group of artists who followed the style of Caravaggio. She was\nalso a student of her father, Orazio Gentileschi, which further\nshaped her artistic development. Orazio’s influence on Artemisia’s\ndevelopment as a prominent Baroque painter can be seen in her\nhighly naturalistic portrayal of figures, dramatic scenes and the\nuse of chiaroscuro technique\n\nGenerated Question: Who was Artemisia Gentileschi and what\ninfluences shaped her artistic style?\nTable 11: Instruction and demonstration for general question generation. We use blue color for the demonstration of\nexpected output.\n\nInstruction: Your objective is to ask a question whose answer is\nthe given paragraph. The question should not be too tedious.\n\nParagraph: Artemisia Gentileschi was born Artemisia Gentileschi\nLomi in Rome on July 8 1593 although her birth certificate from the\nArchivio di Stato indicated she was born in 1590 the eldest child\nof the Tuscan painter Orazio Gentileschi and Prudenzia di Ottaviano\nMontoni. Her life and work were later depicted in the film\nArtemisia; which brought her story to a wider audience. Her\nfather, Orazio, was a prominent figure in the Mannerism art\nmovement, which likely influenced Artemisia’s own artistic style.\nHowever, Artemisia herself was a part of the Caravaggisti movement,\na group of artists who followed the style of Caravaggio. She was\nalso a student of her father, Orazio Gentileschi, which further\nshaped her artistic development. Orazio’s influence on Artemisia’s\ndevelopment as a prominent Baroque painter can be seen in her\nhighly naturalistic portrayal of figures, dramatic scenes and the\nuse of chiaroscuro technique\n\nGenerated Question: What were the key artistic influences and\ncharacteristics that shaped Artemisia Gentileschi’s unique Baroque\nstyle, and how did her relationship with her father, Orazio\nGentileschi, impact her artistic development?\nTable 12: Instruction and demonstration for specific question generation. We use blue color for the demonstration of\nexpected output.\n\n", "vlm_text": "Instruction: Your objective is to ask a question whose answer is the given paragraph. There should be only one question when possible, if not, make sure the question is as concise as possible. \nParagraph: Artemisia Gentiles chi was born Artemisia Gentiles chi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi and Prudenzia di Ottaviano Montoni. Her life and work were later depicted in the film Ärtemisia¨ , which brought her story to a wider audience. Her father, Orazio, was a prominent figure in the Mannerism art movement, which likely influenced Artemisia’s own artistic style. However, Artemisia herself was a part of the Car ava gg is ti movement, a group of artists who followed the style of Caravaggio. She was also a student of her father, Orazio Gentiles chi, which further shaped her artistic development. Orazio’s influence on Artemisia’s development as a prominent Baroque painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique \nGenerated Question: Who was Artemisia Gentiles chi and what influences shaped her artistic style? \nTable 11: Instruction and demonstration for general question generation. We use blue color for the demonstration of expected output. \nInstruction: Your objective is to ask a question whose answer is the given paragraph. The question should not be too tedious. \nParagraph: Artemisia Gentiles chi was born Artemisia Gentiles chi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi and Prudenzia di Ottaviano Montoni. Her life and work were later depicted in the film Ärtemisia¨ , which brought her story to a wider audience. Her father, Orazio, was a prominent figure in the Mannerism art movement, which likely influenced Artemisia’s own artistic style. However, Artemisia herself was a part of the Car ava gg is ti movement, a group of artists who followed the style of Caravaggio. She was also a student of her father, Orazio Gentiles chi, which further shaped her artistic development. Orazio’s influence on Artemisia’s development as a prominent Baroque painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique \nGenerated Question: What were the key artistic influences and characteristics that shaped Artemisia Gentiles chi’s unique Baroque style, and how did her relationship with her father, Orazio Gentiles chi, impact her artistic development? "}
{"page": 18, "image_path": "doc_images/2310.05634v2_18.jpg", "ocr_text": "Instruction: You answer the question based on your knowledge, with\nthe given information for annotation, following the given format.\nUse [NA] for claims that need annotation but is unprovided.\n\nQuestion: Considering the information:\n\n{name: Orazio Gentileschi, place of death: London, child:\nArtemisia Gentileschi, notable works: Diana the Huntress, given\nname: Orazio, surname: Gentileschi, languages spoken: Italian,\nmovement: mannerism, work location: Rome, ethnic group:\n\nItalians, date of birth: 1563-07-19, date of death: 1639-02-07,\ngid: 0367360}\n\n{name: Artemisia Gentileschi, place of birth: Rome, place of\ndeath: Naples, occupation: painter, member of: Accademia delle\nArti del Disegno, father: Orazio Gentileschi, notable works:\nJudith Slaying Holofernes, movement: Caravaggisti, ethnic group:\nItalians, work location: Florence, depicted by: Artemisia,\nsurname: Gentileschi, languages spoken: Italian, student of:\nOrazio Gentileschi, given name: Artemisia, mother: Prudenzia di\nOttaviano Montoni, date of death: 1654-01-01, qid: Q212657}\n\nHow did Orazio Gentileschi’s influence on Artemisia’s life and\ncareer shape her development as a prominent Baroque painter,\ndespite facing significant challenges as a female artist ina\nmale-dominated field?\n\nAnswer: Artemisia Gentileschi was an Italian painter born on July\n8, 1596 [NA] in Rome [Q212657, ethnic group: Italians, occupation:\npainter, place of birth: Rome]. She was a member of the Accademia\ndelle Arti del Disegno and is best known for her work Judith\nSlaying Holofernes [0Q212657, member of: Accademia delle Arti del\nDisegno, notable works: Judith Slaying Holofernes]. She was the\neldest child of the Tuscan painter Orazio Gentileschi [Q212657,\nfather: Orazio Gentileschi]. Orazio Gentileschi was an Italian\npainter [NA] born in 1563 and died in 1639 [Q367360, ethnic group:\nItalians, date of birth: 1563-07-19, date of death: 1639-02-07].\nHe was born in Pisa and died in London [Q367360, place of death:\nLondon]. Orazio’s influence on Artemisia’s development as a\nprominent Baroque [NA] painter can be seen in her highly\nnaturalistic portrayal of figures, dramatic scenes and the use of\nchiaroscuro technique [NA]. He also provided her with the\nopportunity to study with him and learn from his experience and\nexpertise. She became an important second-generation proponent of\nCaravaggio’s dramatic realism [Q212657, movement: Caravaggisti].\nTable 13: Full instruction and demonstration for answer generation with citaion. We use brown color for the\nknowledge pool, and blue color for the expected outcome provided by the demonstration.\n\n", "vlm_text": "Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided. \nQuestion: Considering the information: \n{name: Orazio Gentiles chi, place of death: London, child: Artemisia Gentiles chi, notable works: Diana the Huntress, given name: Orazio, surname: Gentiles chi, languages spoken: Italian, movement: mannerism, work location: Rome, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07, qid: Q367360} {name: Artemisia Gentiles chi, place of birth: Rome, place of death: Naples, occupation: painter, member of: Accademia delle Arti del Disegno, father: Orazio Gentiles chi, notable works: Judith Slaying Holofernes, movement: Car ava gg is ti, ethnic group: Italians, work location: Florence, depicted by: Artemisia, surname: Gentiles chi, languages spoken: Italian, student of: Orazio Gentiles chi, given name: Artemisia, mother: Prudenzia di Ottaviano Montoni, date of death: 1654-01-01, qid: Q212657} How did Orazio Gentiles chi’s influence on Artemisia’s life and career shape her development as a prominent Baroque painter, despite facing significant challenges as a female artist in a male-dominated field? \nAnswer: Artemisia Gentiles chi was an Italian painter born on July 8, 1596 [NA] in Rome [Q212657, ethnic group: Italians, occupation: painter, place of birth: Rome]. She was a member of the Accademia delle Arti del Disegno and is best known for her work Judith Slaying Holofernes [Q212657, member of: Accademia delle Arti del Disegno, notable works: Judith Slaying Holofernes]. She was the eldest child of the Tuscan painter Orazio Gentiles chi [Q212657, father: Orazio Gentiles chi]. Orazio Gentiles chi was an Italian painter [NA] born in 1563 and died in 1639 [Q367360, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07]. He was born in Pisa and died in London [Q367360, place of death: London]. Orazio’s influence on Artemisia’s development as a prominent Baroque [NA] painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique [NA]. He also provided her with the opportunity to study with him and learn from his experience and expertise. She became an important second-generation proponent of Caravaggio’s dramatic realism [Q212657, movement: Car ava gg is ti]. \nTable 13: Full instruction and demonstration for answer generation with citaion. We use brown color for the knowledge pool, and blue color for the expected outcome provided by the demonstration. "}
{"page": 19, "image_path": "doc_images/2310.05634v2_19.jpg", "ocr_text": "Instruction: You will be given one question and answer. Your task\nis to rate the answer on one metric. Please make sure you read and\nunderstand these instructions carefully. Please keep this document\nopen while reviewing, and refer to it as needed.\n\nEvaluation Criteria:\n\nCoherence (1-5) - the collective quality of all sentences. We\nalign this dimension with the DUC quality question of structure and\ncoherence whereby the answer should be well-structured and\nwell-organized. The answer should not just be a heap of related\ninformation, but should build from sentence to sentence to a\ncoherent body of information about a topic.\n\nEvaluation Steps:\n\n1. Read the questions carefully and identify the main topic and\nkey points.\n\n2. Read the answer and compare it to the question. Check if the\nanswer covers the main topic and key points of the question, and if\nit presents them in a clear and logical order.\n\n3. Assign a score for coherence on a scale of 1 to 5, where 1 is\nthe lowest and 5 is the highest based on the Evaluation Criteria.\n\nTable 14: Instruction for text evaluation with GPT-EVAL - Coherence\n\nInstruction: You will be given one question and answer. Your task\nis to rate the answer on one metric. Please make sure you read and\nunderstand these instructions carefully. Please keep this document\nopen while reviewing, and refer to it as needed.\n\nEvaluation Criteria:\n\nConsistency (1-5) - the answer should be consistent with the given\nknowledge. The answer should also be self-consistent, without any\ncontradiction to itself.\n\nEvaluation Steps:\n\n1. Read the question and knowledge carefully.\n\n2. Read the answer and compare it to the knowledge. Check if the\nanswer is consistent with the give knowledge.\n\n3. Assign a score for consistency on a scale of 1 to 5, where 1 is\nthe lowest and 5 is the highest based on the Evaluation Criteria.\n\nTable 15: Instruction for text evaluation with GPT-EVAL - Consistency\n", "vlm_text": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. \nEvaluation Criteria: \nCoherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby the answer should be well-structured and well-organized. The answer should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic. \nEvaluation Steps: \n1. Read the questions carefully and identify the main topic and key points. 2. Read the answer and compare it to the question. Check if the answer covers the main topic and key points of the question, and if it presents them in a clear and logical order. 3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. \nInstruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. \nEvaluation Criteria: Consistency (1-5) - the answer should be consistent with the given knowledge. The answer should also be self-consistent, without any contradiction to itself.\n\n \nEvaluation Steps: \n1. Read the question and knowledge carefully.\n\n 2. Read the answer and compare it to the knowledge. Check if the answer is consistent with the give knowledge. 3. Assign a score for consistency on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. "}
{"page": 20, "image_path": "doc_images/2310.05634v2_20.jpg", "ocr_text": "Instruction: You will be given one question and answer. Your task\nis to rate the answer on one metric. Please make sure you read and\nunderstand these instructions carefully. Please keep this document\nopen while reviewing, and refer to it as needed.\n\nEvaluation Criteria:\n\nFluency (1-5) - the answer should be written in fluent language.\nThe answer should use appropriate vocabulary, grammar, and sentence\nstructures that enable readers or listeners to comprehend the\ncontent effortlessly.\n\nEvaluation Steps:\n\n1. Read the question carefully.\n\n2. Read the answer and check if the language in the answer is\nfluent.\n\n3. Assign a score for fluency on a scale of 1 to 5, where 1 is the\nlowest and 5 is the highest based on the Evaluation Criteria.\n\nTable 16: Instruction for text evaluation with GPT-EVAL - Fluency\n\nInstruction: You will be given one question and answer. Your task\nis to rate the answer on one metric. Please make sure you read and\nunderstand these instructions carefully. Please keep this document\nopen while reviewing, and refer to it as needed.\n\nEvaluation Criteria:\n\nRelevance (1-5) - the answer should be relevant to the question.\nThe answer should directly answers the question, without providing\nany irrelevant information.\n\nEvaluation Steps:\n\n1. Read the question carefully.\n\n2. Read the answer and compare with the question to check if it\n\nfully answers the question and have no redundancies.\n\n3. Assign a score for relevance on a scale of 1 to 5, where 1 is\nthe lowest and 5 is the highest based on the Evaluation Criteria.\n\nTable 17: Instruction for text evaluation with GPT-EVAL - Relevance\n", "vlm_text": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. \n\nFluency (1-5) - the answer should be written in fluent language. The answer should use appropriate vocabulary, grammar, and sentence structures that enable readers or listeners to comprehend the content effortlessly. \nEvaluation Steps:\n\n \n1. Read the question carefully.\n\n 2. Read the answer and check if the language in the answer is fluent. \n3. Assign a score for fluency on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. \nTable 16: Instruction for text evaluation with GPT-EVAL - Fluency \nInstruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. \nEvaluation Criteria: Relevance (1-5) - the answer should be relevant to the question. The answer should directly answers the question, without providing any irrelevant information.\n\n \nEvaluation Steps: \n1. Read the question carefully.\n\n 2. Read the answer and compare with the question to check if it fully answers the question and have no redundancies. 3. Assign a score for relevance on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. "}
{"page": 21, "image_path": "doc_images/2310.05634v2_21.jpg", "ocr_text": "Round 1:\nAnnotated Knowledge:\n\n[qid: 9258115, name: Diego Simeone, occupation: association football player\n[qid: 96439494, name: Giovanni Simeone, father: Diego Simeone]\n[qi 96439494, name: Giovanni Simeone, place of birth: Madrid]\n[qid: 09258115, name: Diego Simeone, member of sports team: Atletico Madrid]\n\nParagraph: The son of former Argentine international footballer Diego Simeone\nand Carolina Baldini Giovanni Simeone was born in Madrid while his father was\nplaying for Atletico Madrid.\n\nRound 2:\n\nInput Knowledge: ([qgid: 0258115, name: Diego Simeone, position played on team:\nmidfielder]\n\nParagraph: The son of former Argentine international footballer Diego Simeone\nand Carolina Baldini Giovanni Simeone was born in Madrid while his father was\nplaying as a midfielder for Atletico Madrid.\n\nRound 3:\n\nInput Knowledge: [gid: 0258115, Diego Simeone, member of sports team: S.S.\nLazio]\n\nParagraph: The son of former Argentine international footballer Diego Simeone\nand Carolina Baldini Giovanni Simeone was born in Madrid while his father was\nplaying as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid,\nDiego Simeone had also played for S.S. Lazio.\n\nRound 4:\n\nInput Knowledge: [qid: 9258115, Diego Simeone, medical condition: COVID-19]\nParagraph: The son of former Argentine international footballer Diego Simeone\nand Carolina Baldini Giovanni Simeone was born in Madrid while his father was\nplaying as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid,\nDiego Simeone had also played for S.S. Lazio. Later in life,\n\nDiego Simeone contracted the COVID-19 virus, but managed to recover.\n\nRound 5:\n\nInput Knowledge: ([qgid: 06439494, Giovanni Simeone, position played on team:\nforward]\n\nParagraph: The son of former Argentine international footballer Diego Simeone\nand Carolina Baldini Giovanni Simeone was born in Madrid while his father was\nplaying as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid,\nDiego Simeone had also played for S.S. Lazio. Later in life, Diego Simeone\ncontracted the COVID-19 virus, but managed to recover. Following in his father’s\nfootsteps but with a different role on the pitch,\n\nGiovanni Simeone grew up to be a forward in the game of football.\n\nGeneral Question:\nHow did the career of Diego Simeone influence his son’s path in football?\n\nSpecific Question:\nWho are Diego Simeone and Giovanni Simeone, and how are there career path in\n\nfootball different from each other?\nTable 18: An example of the generated questions after each round in evolutionary question construction. We use\nbrown color for the knowledge, and blue color for the generated questions. In each round we underline the texts to\nthe newly injected knowledge.\n\n", "vlm_text": "Round 1: \nAnnotated Knowledge \n[qid: Q258115, name: Diego Simeone, occupation: association football player] [qid: Q6439494, name: Giovanni Simeone, father: Diego Simeone] [qid: Q6439494, name: Giovanni Simeone, place of birth: Madrid] [qid: Q258115, name: Diego Simeone, member of sports team: Atletico Madrid] \nParagraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing for Atletico Madrid. \nRound 2: \nInput Knowledge : [qid: Q258115, name: Diego Simeone, position played on team: midfielder] \nParagraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. \nRound 3: \nInput Knowledge : [qid: Q258115, Diego Simeone, member of sports team: S.S. Lazio] \nParagraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. \nRound 4: \nInput Knowledge : [qid: Q258115, Diego Simeone, medical condition: COVID-19] Paragraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. Later in life, Diego Simeone contracted the COVID-19 virus, but managed to recover. \nRound 5: \nInput Knowledge : [qid: Q6439494, Giovanni Simeone, position played on team: forward] \nParagraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. Later in life, Diego Simeone contracted the COVID-19 virus, but managed to recover. Following in his father’s footsteps but with a different role on the pitch, Giovanni Simeone grew up to be a forward in the game of football. \nGeneral Question : \nHow did the career of Diego Simeone influence his son’s path in football? Specific Question : \nWho are Diego Simeone and Giovanni Simeone, and how are there career path in football different from each other? \nTable 18: An example of the generated questions after each round in evolutionary question construction. We use brown color for the knowledge, and blue color for the generated questions. In each round we underline the texts to the newly injected knowledge. "}
{"page": 22, "image_path": "doc_images/2310.05634v2_22.jpg", "ocr_text": "Question:\n\nWhat were some of Stephen Crane’s notable works that reflected his\nviews on religion and the harsh realities of life, and how did his\nexperiences at Syracuse University influence his writing? Also,\nwhat was the name of the sports team he was a member of at Syracuse\nUniversity?\n\nRetrieved Knowledge:\n\n[qid: 0206534, name: Stephen Crane, sex or gender: male, place\nof birth: Newark, place of death: Badenweiler, country of\ncitizenship: United States of America, cause of death:\nTuberculosis, place of burial: Evergreen Cemetery, alma mater:\nSyracuse University, instance of: human, given name: Stephen,\nmedical condition: Tuberculosis, occupation: writer, described by\nsource: Great Soviet Encyclopedia (19694€\"“1978), manner of death:\nnatural causes, movement: literary realism, notable works: The\nRed Badge of Courage, surname: Crane’, \"topic’s main category\":\n'Category:Stephen Crane, religion: atheism, languages spoken:\nEnglish, mother: Mary Helen Peck Crane, sport: baseball, member\nof sports team: Syracuse Orange baseball, father: Jonathan\nTownley Crane, name: Stephen Crane, date of birth: 1871-11-01,\ndate of death: 1900-06-05]\n\nGenerated Output:\nStephen Crane was a male writer born on November 1, 1871, in\nNewark, United States of America, and died on June 5, 1900, in\nBadenweiler due to tuberculosis [Q206534, sex or gender: male,\nplace of birth: Newark, place of death: Badenweiler, country of\ncitizenship: United States of America, cause of death:\nTuberculosis, medical condition: Tuberculosis, date of birth:\n1871-11-01, date of death: 1900-06-05]. Crane was a prominent\nliterary realist writer [0Q206534, movement: literary realism] who\nwrote about the harsh realities of life and his atheistic views on\nreligion [Q206534, religion: atheism]. His notable works include\n\"The Red Badge of Courage\" [Q206534, notable works: The Red Badge\nof Courage].\n\nCrane attended Syracuse University and was a member of the Syracuse\nOrange baseball team [0206534, alma mater: Syracuse University,\nsport: baseball, member of sports team: Syracuse Orange\nbaseball]. His experiences at Syracuse University influenced his\nwriting, and he wrote several stories about college life, including\n\"The Eighty-Yard Run,\" which was based on his own experiences as a\nfootball player at Syracuse [NA].\n\nTable 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the\nquestion and brown color for the retrieved knowledge.\n\n", "vlm_text": "Question : \nWhat were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? \nRetrieved Knowledge : \n[qid: Q206534, name: Stephen Crane, sex or gender: male, place of birth: Newark, place of death: Baden weil er, country of citizenship: United States of America, cause of death: Tuberculosis, place of burial: Evergreen Cemetery, alma mater: Syracuse University, instance of: human, given name: Stephen, medical condition: Tuberculosis, occupation: writer, described by source: Great Soviet Encyclopedia (1969âC“1978), manner of death: natural causes, movement: literary realism, notable works: The Red Badge of Courage, surname: Crane’, \"topic’s main category\": ’Category:Stephen Crane, religion: atheism, languages spoken: English, mother: Mary Helen Peck Crane, sport: baseball, member of sports team: Syracuse Orange baseball, father: Jonathan Townley Crane, name: Stephen Crane, date of birth: 1871-11-01, date of death: 1900-06-05] \nGenerated Output : \nStephen Crane was a male writer born on November 1, 1871, in Newark, United States of America, and died on June 5, 1900, in Baden weil er due to tuberculosis [Q206534, sex or gender: male, place of birth: Newark, place of death: Baden weil er, country of citizenship: United States of America, cause of death: Tuberculosis, medical condition: Tuberculosis, date of birth: 1871-11-01, date of death: 1900-06-05]. Crane was a prominent literary realist writer [Q206534, movement: literary realism] who wrote about the harsh realities of life and his atheistic views on religion [Q206534, religion: atheism]. His notable works include \"The Red Badge of Courage\" [Q206534, notable works: The Red Badge of Courage]. \nCrane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including \"The Eighty-Yard Run,\" which was based on his own experiences as a football player at Syracuse [NA]. \nTable 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge. "}
{"page": 23, "image_path": "doc_images/2310.05634v2_23.jpg", "ocr_text": "Question:\n\nWhat were some of Stephen Crane’s notable works that reflected his\nviews on religion and the harsh realities of life, and how did his\nexperiences at Syracuse University influence his writing? Also,\nwhat was the name of the sports team he was a member of at Syracuse\nUniversity?\n\nRetrieved Knowledge:\n\n[qid: 0206534, name: Stephen Crane, sex or gender: male, place\nof birth: Newark, place of death: Badenweiler, country of\ncitizenship: United States of America, cause of death:\nTuberculosis, place of burial: Evergreen Cemetery, alma mater:\nSyracuse University, instance of: human, given name: Stephen,\nmedical condition: Tuberculosis, occupation: writer, described by\nsource: Great Soviet Encyclopedia (19694€\"“1978), manner of death:\nnatural causes, movement: literary realism, notable works: The\nRed Badge of Courage, surname: Crane’, \"topic’s main category\":\n'Category:Stephen Crane, religion: atheism, languages spoken:\nEnglish, mother: Mary Helen Peck Crane, sport: baseball, member\nof sports team: Syracuse Orange baseball, father: Jonathan\nTownley Crane, name: Stephen Crane, date of birth: 1871-11-01,\ndate of death: 1900-06-05]\n\nGenerated Output:\n\nStephen Crane was an American writer born on November 1, 1871, in\nNewark, and died on June 5, 1900, in Badenweiler [0Q206534, date of\nbirth: 1871-11-01, date of death: 1900-06-05, place of birth:\nNewark, place of death: Badenweiler]. He was a part of the\nliterary realism movement and is best known for his notable work,\nThe Red Badge of Courage [0Q206534, movement: literary realism,\nnotable works: The Red Badge of Courage]. Crane’s views on\nreligion were atheistic [0Q206534, religion: atheism]. His works\noften reflected the harsh realities of life, such as war, poverty,\nand human suffering [NA].\n\nCrane attended Syracuse University [0Q206534, alma mater: Syracuse\nUniversity], where his experiences likely influenced his writing.\n\nHe was exposed to a diverse range of ideas and perspectives, which\nmay have contributed to his realistic approach to literature [NA].\n\nWhile at Syracuse University, Stephen Crane was a member of the\nSyracuse Orange baseball team [Q206534, member of sports team:\nSyracuse Orange baseball].\nTable 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown\ncolor for the retrieved knowledge.\n\n", "vlm_text": "Question : \nWhat were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? \nRetrieved Knowledge : \n[qid: Q206534, name: Stephen Crane, sex or gender: male, place of birth: Newark, place of death: Baden weil er, country of citizenship: United States of America, cause of death: Tuberculosis, place of burial: Evergreen Cemetery, alma mater: Syracuse University, instance of: human, given name: Stephen, medical condition: Tuberculosis, occupation: writer, described by source: Great Soviet Encyclopedia (1969âC“1978), manner of death: natural causes, movement: literary realism, notable works: The Red Badge of Courage, surname: Crane’, \"topic’s main category\": ’Category:Stephen Crane, religion: atheism, languages spoken: English, mother: Mary Helen Peck Crane, sport: baseball, member of sports team: Syracuse Orange baseball, father: Jonathan Townley Crane, name: Stephen Crane, date of birth: 1871-11-01, date of death: 1900-06-05] \nGenerated Output : \nStephen Crane was an American writer born on November 1, 1871, in Newark, and died on June 5, 1900, in Baden weil er [Q206534, date of birth: 1871-11-01, date of death: 1900-06-05, place of birth: Newark, place of death: Baden weil er]. He was a part of the literary realism movement and is best known for his notable work, The Red Badge of Courage [Q206534, movement: literary realism, notable works: The Red Badge of Courage]. Crane’s views on religion were atheistic [Q206534, religion: atheism]. His works often reflected the harsh realities of life, such as war, poverty, and human suffering [NA]. \nCrane attended Syracuse University [Q206534, alma mater: Syracuse University], where his experiences likely influenced his writing. He was exposed to a diverse range of ideas and perspectives, which may have contributed to his realistic approach to literature [NA]. \nWhile at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]. \nTable 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge. "}
