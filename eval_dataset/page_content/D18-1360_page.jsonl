{"page": 0, "image_path": "doc_images/D18-1360_0.jpg", "ocr_text": "Multi-Task Identification of Entities, Relations, and Coreference\nfor Scientific Knowledge Graph Construction\n\nYiLuan Luheng He\n\nMari Ostendorf Hannaneh Hajishirzi\n\nUniversity of Washington\n{luanyi, luheng, ostendor, hannaneh} @uw.edu\n\nAbstract\n\nWe introduce a multi-task setup of identifying\nand classifying entities, relations, and coref-\nerence clusters in scientific articles. We cre-\nate SCIERC, a dataset that includes annota-\ntions for all three tasks and develop a uni-\nfied framework called Scientific Information\nExtractor (SCIIE) for with shared span rep-\nresentations. The multi-task setup reduces\ncascading errors between tasks and leverages\ncross-sentence relations through coreference\nlinks. Experiments show that our multi-task\nmodel outperforms previous models in scien-\ntific information extraction without using any\ndomain-specific features. We further show that\nthe framework supports construction of a sci-\nentific knowledge graph, which we use to ana-\nlyze information in scientific literature.!\n\n1 Introduction\n\nAs scientific communities grow and evolve, new\ntasks, methods, and datasets are introduced and\ndifferent methods are compared with each other.\nDespite advances in search engines, it is still hard\nto identify new technologies and their relationships\nwith what existed before. To help researchers more\nquickly identify opportunities for new combina-\ntions of tasks, methods and data, it is important to\ndesign intelligent algorithms that can extract and\norganize scientific information from a large collec-\ntion of documents.\n\nOrganizing scientific information into structured\nknowledge bases requires information extraction\n(IE) about scientific entities and their relationships.\nHowever, the challenges associated with scientific\nIE are greater than for a general domain. First, an-\nnotation of scientific text requires domain expertise\nwhich makes annotation costly and limits resources.\n\n'Data and code are publicly available at: http: //nlp.\n\ncs.washington.edu/scilE/\n\nUsed-for\n\nf Used-for 1 ¢\nTo reduce [ambiguity Jomersr, the [MORphological PArser MORPA ]Methoa\nUsed-tor——\n\nis provided with a [PCFG]Methoa...\n$—— Used -for ————_,\n[It] ceneric combines [context-free grammar]Methoa with...\nUsed-for-\n—— ay 1\n[MORPA }viethoa is a fully implemented [parser] vtetnoa developed for a [text-\n\nto-speech system] risk.\n\nFigure 1: Example annotation: phrases that refer to\nthe same scientific concept are annotated into the\nsame coreference cluster, such as MORphological\nPAser MORPA, it and MORPA (marked as red).\n\nIn addition, most relation extraction systems are de-\nsigned for within-sentence relations. However, ex-\ntracting information from scientific articles requires\nextracting relations across sentences. Figure | il-\nlustrates this problem. The cross-sentence relations\nbetween some entities can only be connected by\nentities that refer to the same scientific concept,\nincluding generic terms (such as the pronoun it,\nor phrases like our method) that are not informa-\ntive by themselves. With co-reference, context-free\ngrammar can be connected to MORPA through the\nintermediate co-referred pronoun it. Applying ex-\nisting IE systems to this data, without co-reference,\nwill result in much lower relation coverage (and a\nsparse knowledge base).\n\nIn this paper, we develop a unified learning\nmodel for extracting scientific entities, relations,\nand coreference resolution. This is different from\nprevious work (Luan et al., 2017b; Gupta and Man-\nning, 2011; Tsai et al., 2013; Gabor et al., 2018)\nwhich often addresses these tasks as independent\n\n3219\n\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232\nBrussels, Belgium, October 31 - November 4, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Multi-Task Identiﬁcation of Entities, Relations, and Coreference for Scientiﬁc Knowledge Graph Construction \nYi Luan Luheng He Mari Ostendorf Hannaneh Hajishirzi University of Washington { luanyi, luheng, ostendor, hannaneh } @uw.edu \nAbstract \nWe introduce a multi-task setup of identifying and classifying entities, relations, and coref- erence clusters in scientiﬁc articles. We cre- ate S CI ERC, a dataset that includes annota- tions for all three tasks and develop a uni- ﬁed framework called Scientiﬁc Information Extractor (S CI IE) for with shared span rep- resentations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scien- tiﬁc information extraction without using any domain-speciﬁc features. We further show that the framework supports construction of a sci- entiﬁc knowledge graph, which we use to ana- lyze information in scientiﬁc literature. \n1 Introduction \nAs scientiﬁc communities grow and evolve, new tasks, methods, and datasets are introduced and different methods are compared with each other. Despite advances in search engines, it is still hard to identify new technologies and their relationships with what existed before. To help researchers more quickly identify opportunities for new combina- tions of tasks, methods and data, it is important to design intelligent algorithms that can extract and organize scientiﬁc information from a large collec- tion of documents. \nOrganizing scientiﬁc information into structured knowledge bases requires information extraction (IE) about scientiﬁc entities and their relationships. However, the challenges associated with scientiﬁc IE are greater than for a general domain. First, an- notation of scientiﬁc text requires domain expertise which makes annotation costly and limits resources. \nThe image contains a text annotation and a diagram. \n\nIn the text annotation (top part), different phrases are linked to show their relationships. These include:\n\n- \"ambiguity,\" \"MORphological PArser MORPA,\" \"PCFG,\" \"It,\" \"context-free grammar,\" \"MORPA,\" \"parser,\" and \"text-to-speech system\" with labels like \"Used-for\" and \"Hyponym-of\".\n\nThe diagram (bottom part) shows a representation of these relationships in a more visual format. The coreference cluster is marked in red and yellow, linking \"MORphological PArser MORPA,\" \"MORPA,\" and \"It.\" Other terms are connected with lines labeled \"Used-for\" or \"Hyponym-of.\"\nIn addition, most relation extraction systems are de- signed for within-sentence relations. However, ex- tracting information from scientiﬁc articles requires extracting relations across sentences. Figure  1  il- lustrates this problem. The cross-sentence relations between some entities can only be connected by entities that refer to the same scientiﬁc concept, including generic terms (such as the pronoun    $i t$  , or phrases like  our method ) that are not informa- tive by themselves. With co-reference,  context-free grammar  can be connected to  MORPA  through the intermediate co-referred pronoun  it . Applying ex- isting IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). \nIn this paper, we develop a uniﬁed learning model for extracting scientiﬁc entities, relations, and coreference resolution. This is different from previous work ( Luan et al. ,  2017b ;  Gupta and Man- ning ,  2011 ;  Tsai et al. ,  2013 ;  G abor et al. ,  2018 ) which often addresses these tasks as independent components of a pipeline. Our uniﬁed model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Speciﬁcally, we extend prior work for learn- ing span representations and coreference resolution ( Lee et al. ,  2017 ;  He et al. ,  2018 ). Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans. It avoids cascading errors between tasks by jointly modeling all spans and span-span relations. "}
{"page": 1, "image_path": "doc_images/D18-1360_1.jpg", "ocr_text": "components of a pipeline. Our unified model is\na multi-task setup that shares parameters across\nlow-level tasks, making predictions by leveraging\ncontext across the document through coreference\nlinks. Specifically, we extend prior work for learn-\ning span representations and coreference resolution\n(Lee et al., 2017; He et al., 2018). Different from a\nstandard tagging system, our system enumerates all\npossible spans during decoding and can effectively\ndetect overlapped spans. It avoids cascading errors\nbetween tasks by jointly modeling all spans and\nspan-span relations.\n\nTo explore this problem, we create a dataset SCI-\nERC for scientific information extraction, which\nincludes annotations of scientific terms, relation\ncategories and co-reference links. Our experiments\nshow that the unified model is better at predict-\ning span boundaries, and it outperforms previous\nstate-of-the-art scientific IE systems on entity and\nrelation extraction (Luan et al., 2017b; Augenstein\net al., 2017). In addition, we build a scientific\nknowledge graph integrating terms and relations\nextracted from each article. Human evaluation\nshows that propagating coreference can signifi-\ncantly improve the quality of the automatic con-\nstructed knowledge graph.\n\nIn summary we make the following contribu-\ntions. We create a dataset for scientific information\nextraction by jointly annotating scientific entities,\nrelations, and coreference links. Extending a previ-\nous end-to-end coreference resolution system, we\ndevelop a multi-task learning framework that can\ndetect scientific entities, relations, and coreference\nclusters without hand-engineered features. We use\nour unified framework to build a scientific knowl-\nedge graph from a large collection of documents\nand analyze information in scientific literature.\n\n2 Related Work\n\nThere has been growing interest in research on au-\ntomatic methods for information extraction from\nscientific articles. Past research in scientific IE\naddressed analyzing citations (Athar and Teufel,\n2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al.,\n2012; Do et al., 2013; Jaidka et al., 2014; Abu-\nJbara and Radev, 2011), analyzing research com-\nmunity (Vogel and Jurafsky, 2012; Anderson et al.,\n2012), and unsupervised methods for extracting sci-\nentific entities and relations (Gupta and Manning,\n2011; Tsai et al., 2013; Gabor et al., 2016).\n\nMore recently, two datasets in SemEval 2017\n\nand 2018 have been introduced, which facilitate\nresearch on supervised and semi-supervised learn-\ning for scientific information extraction. SemEval\n17 (Augenstein et al., 2017) includes 500 para-\ngraphs from articles in the domains of computer\nscience, physics, and material science. It includes\nthree types of entities (called keyphrases): Tasks,\nMethods, and Materials and two relation types:\nhyponym-of and synonym-of. SemEval 18 (Gabor\net al., 2018) is focused on predicting relations be-\ntween entities within a sentence. It consists of six\nrelation types. Using these datasets, neural mod-\nels (Ammar et al., 2017, 2018; Luan et al., 2017b;\nAugenstein and Sggaard, 2017) are introduced for\nextracting scientific information. We extend these\ndatasets by increasing relation coverage, adding\ncross-sentence coreference linking, and removing\nsome annotation constraints. Different from most\nprevious IE systems for scientific literature and gen-\neral domains (Miwa and Bansal, 2016; Xu et al.,\n2016; Peng et al., 2017; Quirk and Poon, 2017;\nLuan et al., 2018; Adel and Schiitze, 2017), which\nuse preprocessed syntactic, discourse or corefer-\nence features as input, our unified framework does\nnot rely on any pipeline processing and is able to\nmodel overlapping spans.\n\nWhile Singh et al. (2013) show improvements\nby jointly modeling entities, relations, and coref-\nerence links, most recent neural models for these\ntasks focus on single tasks (Clark and Manning,\n2016; Wiseman et al., 2016; Lee et al., 2017; Lam-\nple et al., 2016; Peng et al., 2017) or joint entity\nand relation extraction (Katiyar and Cardie, 2017;\nZhang et al., 2017; Adel and Schiitze, 2017; Zheng\net al., 2017). Among those studies, many papers as-\nsume the entity boundaries are given, such as (Clark\nand Manning, 2016), Adel and Schiitze (2017) and\nPeng et al. (2017). Our work relaxes this constraint\nand predicts entity boundaries by optimizing over\nall possible spans. Our model draws from recent\nend-to-end span-based models for coreference res-\nolution (Lee et al., 2017, 2018) and semantic role\nlabeling (He et al., 2018) and extends them for the\nmulti-task framework involving the three tasks of\nidentification of entity, relation and coreference.\n\nNeural multi-task learning has been applied to\na range of NLP tasks. Most of these models share\nword-level representations (Collobert and Weston,\n2008; Klerke et al., 2016; Luan et al., 2016, 2017a;\nRei, 2017), while Peng et al. (2017) uses high-order\ncross-task factors. Our model instead propagates\n\n3220\n", "vlm_text": "\nTo explore this problem, we create a dataset S CI - ERC for scientiﬁc information extraction, which includes annotations of scientiﬁc terms, relation categories and co-reference links. Our experiments show that the uniﬁed model is better at predict- ing span boundaries, and it outperforms previous state-of-the-art scientiﬁc IE systems on entity and relation extraction ( Luan et al. ,  2017b ;  Augenstein et al. ,  2017 ). In addition, we build a scientiﬁc knowledge graph integrating terms and relations extracted from each article. Human evaluation shows that propagating coreference can signiﬁ- cantly improve the quality of the automatic con- structed knowledge graph. \nIn summary we make the following contribu- tions. We create a dataset for scientiﬁc information extraction by jointly annotating scientiﬁc entities, relations, and coreference links. Extending a previ- ous end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientiﬁc entities, relations, and coreference clusters without hand-engineered features. We use our uniﬁed framework to build a scientiﬁc knowl- edge graph from a large collection of documents and analyze information in scientiﬁc literature. \n2 Related Work \nThere has been growing interest in research on au- tomatic methods for information extraction from scientiﬁc articles. Past research in scientiﬁc IE addressed analyzing citations ( Athar and Teufel , 2012b , a ;  Kas ,  2011 ;  Gabor et al. ,  2016 ;  Sim et al. , 2012 ;  Do et al. ,  2013 ;  Jaidka et al. ,  2014 ;  Abu- Jbara and Radev ,  2011 ), analyzing research com- munity ( Vogel and Jurafsky ,  2012 ;  Anderson et al. , 2012 ), and unsupervised methods for extracting sci- entiﬁc entities and relations ( Gupta and Manning , 2011 ;  Tsai et al. ,  2013 ;  G´ abor et al. ,  2016 ). \nMore recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learn- ing for scientiﬁc information extraction. SemEval 17 ( Augenstein et al. ,  2017 ) includes 500 para- graphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of. SemEval 18 ( G abor et al. ,  2018 ) is focused on predicting relations be- tween entities within a sentence. It consists of six relation types. Using these datasets, neural mod- els ( Ammar et al. ,  2017 ,  2018 ;  Luan et al. ,  2017b ; Augenstein and Søgaard ,  2017 ) are introduced for extracting scientiﬁc information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientiﬁc literature and gen- eral domains ( Miwa and Bansal ,  2016 ;  Xu et al. , 2016 ;  Peng et al. ,  2017 ;  Quirk and Poon ,  2017 ; Luan et al. ,  2018 ;  Adel and Sch utze ,  2017 ), which use preprocessed syntactic, discourse or corefer- ence features as input, our uniﬁed framework does not rely on any pipeline processing and is able to model overlapping spans. \n\nWhile  Singh et al.  ( 2013 ) show improvements by jointly modeling entities, relations, and coref- erence links, most recent neural models for these tasks focus on single tasks ( Clark and Manning , 2016 ;  Wiseman et al. ,  2016 ;  Lee et al. ,  2017 ;  Lam- ple et al. ,  2016 ;  Peng et al. ,  2017 ) or joint entity and relation extraction ( Katiyar and Cardie ,  2017 ; Zhang et al. ,  2017 ;  Adel and Sch utze ,  2017 ;  Zheng et al. ,  2017 ). Among those studies, many papers as- sume the entity boundaries are given, such as ( Clark and Manning ,  2016 ),  Adel and Sch utze  ( 2017 ) and Peng et al.  ( 2017 ). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference res- olution ( Lee et al. ,  2017 ,  2018 ) and semantic role labeling ( He et al. ,  2018 ) and extends them for the multi-task framework involving the three tasks of identiﬁcation of entity, relation and coreference. \nNeural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations ( Collobert and Weston , 2008 ;  Klerke et al. ,  2016 ;  Luan et al. ,  2016 ,  2017a ; Rei ,  2017 ), while  Peng et al.  ( 2017 ) uses high-order cross-task factors. Our model instead propagates cross-task information via span representations, which is related to  Swayamdipta et al.  ( 2017 ). "}
{"page": 2, "image_path": "doc_images/D18-1360_2.jpg", "ocr_text": "cross-task information via span representations,\nwhich is related to Swayamdipta et al. (2017).\n\n3 Dataset\n\nOur dataset (called SCIERC) includes annotations\nfor scientific entities, their relations, and corefer-\nence clusters for 500 scientific abstracts. These ab-\nstracts are taken from 12 AI conference/workshop\nproceedings in four AI communities from the Se-\nmantic Scholar Corpus”. SCIERC extends pre-\nvious datasets in scientific articles SemEval 2017\nTask 10 (SemEval 17) (Augenstein et al., 2017) and\nSemEval 2018 Task 7 (SemEval 18) (Gabor et al.,\n2018) by extending entity types, relation types, rela-\ntion coverage, and adding cross-sentence relations\nusing coreference links. Our dataset is publicly\navailable at: http: //nlp.cs.washington.\nedu/scilE/. Table | shows the statistics of SCI-\nERC.\n\nAnnotation Scheme We define six types for an-\nnotating scientific entities (Task, Method, Metric,\nMaterial, Other-ScientificTerm and Generic) and\nseven relation types (Compare, Part-of, Conjunc-\ntion, Evaluate-for, Feature-of, Used-for, Hyponym-\nOf). Directionality is taken into account except\nfor the two symmetric relation types (Conjunction\nand Compare). Coreference links are annotated\nbetween identical scientific entities. A Generic en-\ntity is annotated only when the entity is involved\nin a relation or is coreferred with another entity.\nAnnotation guidelines can be found in Appendix A.\nFigure 1 shows an annotated example.\n\nFollowing annotation guidelines from Qasem-\niZadeh and Schumann (2016) and using the BRAT\ninterface (Stenetorp et al., 2012), our annotators\nperform a greedy annotation for spans and always\nprefer the longer span whenever ambiguity occurs.\nNested spans are allowed when a subspan has a\nrelation/coreference link with another term outside\nthe span.\n\nHuman Agreements One domain expert anno-\ntated all the documents in the dataset; 12% of the\ndata is dually annotated by 4 other domain experts\nto evaluate the user agreements. The kappa score\nfor annotating entities is 76.9%, relation extraction\nis 67.8% and coreference is 63.8%.\n\n?These conferences include general AI (AAAI, CAI),\nNLP (ACL, EMNLP, IJCNLP), speech (ICASSP, Interspeech),\nmachine learning (NIPS, ICML), and computer vision (CVPR,\n\nICCV, ECCV) at http://labs.semanticscholar.\norg/corpus/\n\nStatistics ScIERC SemEval 17 SemEval 18\n#Entities 8089 9946 7483\n#Relations 4716 672 1595\n#Relations/Doc 94 1.3 3.2\n#Coref links 2752 - -\n#Coref clusters 1023 - -\n\nTable 1: Dataset statistics for our dataset SCIERC\nand two previous datasets on scientific information\nextraction. All datasets annotate 500 documents.\n\nComparison with previous datasets SCIERC\nis focused on annotating cross-sentence relations\nand has more relation coverage than SemEval 17\nand SemEval 18, as shown in Table 1. SemEval 17\nis mostly designed for entity recognition and only\ncovers two relation types. The task in SemEval 18\nis to classify a relation between a pair of entities\ngiven entity boundaries, but only intra-sentence re-\nlations are annotated and each entity only appears\nin one relation, resulting in sparser relation cover-\nage than our dataset (3.2 vs. 9.4 relations per ab-\nstract). SCIERC extends these datasets by adding\nmore relation types and coreference clusters, which\nallows representing cross-sentence relations, and\nremoving annotation constraints. Table | gives a\ncomparison of statistics among the three datasets.\nIn addition, SCIERC aims at including broader\ncoverage of general AI communities.\n\n4 Model\n\nWe develop a unified framework (called SCIIE)\nto identify and classify scientific entities, relations,\nand coreference resolution across sentences. SCIIE\nis a multi-task learning setup that extends previous\nspan-based models for coreference resolution (Lee\net al., 2017) and semantic role labeling (He et al.,\n2018). All three tasks of entity recognition, re-\nlation extraction, and coreference resolution are\ntreated as multinomial classification problems with\nshared span representations. SCIIE benefits from\nexpressive contextualized span representations as\nclassifier features. By sharing span representations,\nsentence-level tasks can benefit from information\npropagated from coreference resolution across sen-\ntences, without increasing the complexity of infer-\nence. Figure 2 shows a high-level overview of the\nScHE multi-task framework.\n\n4.1 Problem Definition\n\nThe input is a document represented as a sequence\nof words D = {w1,...,wn}, from which we de-\nrive S = {s1,...,sy}, the set of all possible\n\n3221\n", "vlm_text": "\n3 Dataset \nOur dataset (called S CI ERC) includes annotations for scientiﬁc entities, their relations, and corefer- ence clusters for 500 scientiﬁc abstracts. These ab- stracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Se- mantic Scholar Corpus 2 . S CI ERC extends pre- vious datasets in scientiﬁc articles SemEval 2017 Task 10 (SemEval 17) ( Augenstein et al. ,  2017 ) and SemEval 2018 Task 7 (SemEval 18) ( G abor et al. , 2018 ) by extending entity types, relation types, rela- tion coverage, and adding cross-sentence relations using coreference links. Our dataset is publicly available at:  http://nlp.cs.washington. edu/sciIE/ . Table  1  shows the statistics of S CI - ERC. \nAnnotation Scheme We deﬁne six types for an- notating scientiﬁc entities (Task, Method, Metric, Material, Other-ScientiﬁcTerm and Generic) and seven relation types (Compare, Part-of, Conjunc- tion, Evaluate-for, Feature-of, Used-for, Hyponym- Of). Directionality is taken into account except for the two symmetric relation types (Conjunction and Compare). Coreference links are annotated between identical scientiﬁc entities. A Generic en- tity is annotated only when the entity is involved in a relation or is coreferred with another entity. Annotation guidelines can be found in Appendix  A . Figure  1  shows an annotated example. \nFollowing annotation guidelines from  Qasem- iZadeh and Schumann  ( 2016 ) and using the BRAT interface ( Stenetorp et al. ,  2012 ), our annotators perform a greedy annotation for spans and always prefer the longer span whenever ambiguity occurs. Nested spans are allowed when a subspan has a relation/coreference link with another term outside the span. \nHuman Agreements One domain expert anno- tated all the documents in the dataset;   $12\\%$   of the data is dually annotated by 4 other domain experts to evaluate the user agreements. The kappa score for annotating entities is   $76.9\\%$  , relation extraction is   $67.8\\%$   and coreference is   $63.8\\%$  . \nThe table compares the following statistics across three datasets: SciERC, SemEval 17, and SemEval 18:\n\n1. **#Entities**: \n   - SciERC: 8089\n   - SemEval 17: 9946\n   - SemEval 18: 7483\n\n2. **#Relations**: \n   - SciERC: 4716\n   - SemEval 17: 672\n   - SemEval 18: 1595\n\n3. **#Relations/Doc**:\n   - SciERC: 9.4\n   - SemEval 17: 1.3\n   - SemEval 18: 3.2\n\n4. **#Coref links**: \n   - SciERC: 2752\n   - SemEval 17: -\n   - SemEval 18: -\n\n5. **#Coref clusters**: \n   - SciERC: 1023\n   - SemEval 17: -\n   - SemEval 18: -\nComparison with previous datasets S CI ERC is focused on annotating cross-sentence relations and has more relation coverage than SemEval 17 and SemEval 18, as shown in Table  1 . SemEval 17 is mostly designed for entity recognition and only covers two relation types. The task in SemEval 18 is to classify a relation between a pair of entities given entity boundaries, but only intra-sentence re- lations are annotated and each entity only appears in one relation, resulting in sparser relation cover- age than our dataset (3.2 vs. 9.4 relations per ab- stract). S CI ERC extends these datasets by adding more relation types and coreference clusters, which allows representing cross-sentence relations, and removing annotation constraints. Table  1  gives a comparison of statistics among the three datasets. In addition, S CI ERC aims at including broader coverage of general AI communities. \n4 Model \nWe develop a uniﬁed framework (called S CI IE) to identify and classify scientiﬁc entities, relations, and coreference resolution across sentences. S CI IE is a multi-task learning setup that extends previous span-based models for coreference resolution ( Lee et al. ,  2017 ) and semantic role labeling ( He et al. , 2018 ). All three tasks of entity recognition, re- lation extraction, and coreference resolution are treated as multinomial classiﬁcation problems with shared span representations. S CI IE beneﬁts from expressive contextualized span representations as classiﬁer features. By sharing span representations, sentence-level tasks can beneﬁt from information propagated from coreference resolution across sen- tences, without increasing the complexity of infer- ence. Figure  2  shows a high-level overview of the S CI IE multi-task framework. \n4.1 Problem Deﬁnition \nThe input is a document represented as a sequence of words    $D=\\{w_{1},.\\,.\\,.\\,,w_{n}\\}$  , from which we de- rive    $S~=~\\{s_{1},.\\,.\\,.\\,,s_{N}\\}$  , the set of all possible "}
{"page": 3, "image_path": "doc_images/D18-1360_3.jpg", "ocr_text": "Entity\nRecognition :\n\nCoreference\nResolution\n‘MORphological Parser MORPA\nSpan 5;\nRepresentations POR eboloEIeal\nMORPA is\nis provided with\n\n+Span Features\n\nBiLSTM outputs\n\nSentences\n\n*\\ Relation\n! Extraction\n\nMORPA\n\nMORPA is a\n\na fully implemented parser\n\nparser\n\nFigure 2: Overview of the multitask setup, where all three tasks are treated as classification problems on\ntop of shared span representations. Dotted arcs indicate the normalization space for each task.\n\nwithin-sentence word sequence spans (up to a rea-\nsonable length) in the document. The output con-\ntains three structures: the entity types EF for all\nspans S, the relations R for all pair of spans S x S,\nand the coreference links C’ for all spans in S'. The\noutput structures are represented with a set of dis-\ncrete random variables indexed by spans or pairs\nof spans. Specifically, the output structures are\ndefined as follows.\n\nEntity recognition is to predict the best entity type\nfor every candidate span. Let Lg represent the set\nof all possible entity types including the null-type e.\nThe output structure EF is a set of random variables\nindexed by spans: e; € Lg fori = 1,..., N.\n\nRelation extraction is to predict the best relation\ntype given an ordered pair of spans (s;, s;). Let Lr\nbe the set of all possible relation types including\nthe null-type «. The output structure R is a set of\nrandom variables indexed over pairs of spans (i, j)\nthat belong to the same sentence: rj; € Lp for\n\nCoreference resolution is to predict the best an-\ntecedent (including a special null antecedent) given\na span, which is the same mention-ranking model\nused in Lee et al. (2017). The output structure\nC is a set of random variables defined as: c; €\n{1,...,¢-l,e} fori =1,...,N.\n\n4.2 Model Definition\n\nWe formulate the multi-task learning setup as\nlearning the conditional probability distribution\nP(E, R,C|D). For efficient training and inference,\nwe decompose P(E, R,C|D) assuming spans are\n\nconditionally independent given D:\n\nP(E,R,C|D)=PE,R,C,S|D) — ()\n\ni=1\n\nwhere the conditional probabilities of each random\nvariable are independently normalized:\n\nD)P(c;\n\nD) [2 | D),\njel\n\nexp(®g(e, s;))\n\nP(e; =e| D 2\n(=e P)= ST expel)\n. ; exp(®r(r, 5;, 5;))\n\nPly 71D) = ST expla 8:05)\n\nP(q =3| D) exp(®c(si, 8;))\n\ni-1,e} exp(®c(s;, 5;”))\n\nwhere ®g denotes the unnormalized model score\nfor an entity type e and a span s;, ®g denotes the\nscore for a relation type r and span pairs 5;, $j,\nand ®c denotes the score for a binary coreference\nlink between s; and s;. These ® scores are further\ndecomposed into span and pairwise span scores\ncomputed from feed-forward networks, as will be\nexplained in Section 4.3.\n\nFor simplicity, we omit D from the ® functions\nand S from the observation.\n\nObjective Given a set of all documents D, the\nmodel loss function is defined as a weighted sum of\nthe negative log-likelihood loss of all three tasks:\n\n(D,R*,E*,C*)ED\n\n+ Ap log P(R* | D) + Aclog P(C* | p)}\n\n{re log P(E*|D) 3)\n\n3222\n", "vlm_text": "The image is a diagram illustrating a multitask setup. It focuses on three tasks: Entity Recognition, Coreference Resolution, and Relation Extraction. Each task is treated as a classification problem using shared span representations. The diagram includes:\n\n- **Entity Recognition:** Identifies and classifies parts of the text as entities such as \"Task\" and \"Method.\"\n- **Coreference Resolution:** Connects references to the same entity, shown with arrows pointing back to the original mention.\n- **Relation Extraction:** Identifies relationships, like \"Hyponym-of\" and \"Used-for,\" between entities.\n\nBlue arrows demonstrate BiLSTM outputs connecting to sentences, indicating how sentence information is processed for these tasks. Dotted lines indicate normalization spaces specific to each task.\nwithin-sentence word sequence spans (up to a rea- sonable length) in the document. The output con- tains three structures: the entity types    $E$   for all spans  $S$  , the relations    $R$   f all pair of spans  $S\\times S$  , and the coreference links  C  for all spans in  S . The output structures are represented with a set of dis- crete random variables indexed by spans or pairs of spans. Speciﬁcally, the output structures are deﬁned as follows. \nEntity recognition  is to predict the best entity type for every candidate span. Let    $L_{\\mathrm{E}}$   represent the set of all possible entity types including the null-type    $\\epsilon$  The output structure    $E$   is a set of random variables indexed by spans:    $e_{i}\\in L_{\\mathrm{E}}$   for    $i=1,\\ldots,N$  . \nRelation extraction  is to predict the best relation type given an ordered pair of spans    $(s_{i},s_{j})$  . Let    $L_{\\mathrm{R}}$  be the set of all possible relation types including the null-type  $\\epsilon$  . The output structure    $R$   is a set of random variables indexed over pairs of spans    $(i,j)$  the same sentence:    $r_{i j}\\,\\in\\,L_{\\mathrm{R}}$   for  $i,j=1,\\cdot\\cdot\\cdot,N$  . \nCoreference resolution  is to predict the best an- tecedent (including a special null antecedent) given a span, which is the same mention-ranking model used in  Lee et al.  ( 2017 ). The output structure  $C$  ndo eﬁned as:    $c_{i}\\in$   $\\{1,\\ldots,i-1,\\epsilon\\}$   for  $i=1,\\dots,N$  . \n4.2 Model Deﬁnition \nWe formulate the multi-task learning setup as learning the conditional probability distribution  $P(E,R,C|D)$  . For efﬁcient training and inference, we decompose    $P(E,R,C|D)$   assuming spans are conditionally independent given    $D$  : \n\n\n$$\n\\begin{array}{l}{{P(E,R,C\\mid D)=P(E,R,C,S\\mid D)}}\\\\ {{{}=\\displaystyle\\prod_{i=1}^{N}P(e_{i}\\mid D)P(c_{i}\\mid D)\\displaystyle\\prod_{j=1}^{N}P(r_{i j}\\mid D),}}\\end{array}\n$$\n \nwhere the conditional probabilities of each random variable are independently normalized: \n\n$$\n\\begin{array}{l}{P(e_{i}=e\\mid D)=\\frac{\\exp(\\Phi_{\\mathrm{E}}(e,s_{i}))}{\\sum_{e^{\\prime}\\in L_{\\mathrm{E}}}\\exp(\\Phi_{\\mathrm{E}}(e^{\\prime},s_{i}))}\\qquad(2)}\\\\ {P(r_{i j}=r\\mid D)=\\frac{\\exp(\\Phi_{\\mathrm{R}}(r,s_{i},s_{j}))}{\\sum_{r^{\\prime}\\in L_{\\mathrm{R}}}\\exp(\\Phi_{\\mathrm{R}}(r^{\\prime},s_{i},s_{j}))}}\\\\ {P(c_{i}=j\\mid D)=\\frac{\\exp(\\Phi_{\\mathrm{C}}(s_{i},s_{j}))}{\\sum_{j^{\\prime}\\in\\{1,\\ldots,i-1,\\epsilon\\}}\\exp(\\Phi_{\\mathrm{C}}(s_{i},s_{j^{\\prime}}))},}\\end{array}\n$$\n \nwhere    $\\Phi_{\\mathrm{E}}$   denotes the unnormalized model score for an entity type  $e$   and a span    $s_{i}$  ,  $\\Phi_{\\mathrm{{R}}}$   denotes the score for a relation type    $r$   and span pairs    $s_{i},s_{j}$  , and    $\\Phi_{\\mathbf{C}}$   denotes the score for a binary coreference link between    $s_{i}$   and  $s_{j}$  . These    $\\Phi$   scores are further decomposed into span and pairwise span scores computed from feed-forward networks, as will be explained in Section  4.3 . \nFor simplicity, we omit    $D$   from the    $\\Phi$   functions and    $S$   from the observation. \nObjective Given a set of all documents    $\\mathcal{D}$  , the model loss function is deﬁned as a weighted sum of the negative log-likelihood loss of all three tasks: \n\n$$\n\\begin{array}{r l}{-}&{{}\\displaystyle\\sum_{(D,R^{*},E^{*},C^{*})\\in\\mathcal{D}}\\Big\\{\\lambda_{\\mathrm{E}}\\log P(E^{*}\\mid D)\\quad\\quad}\\\\ {+}&{{}\\lambda_{\\mathsf{R}}\\log P(R^{*}\\mid D)+\\lambda_{\\mathsf{C}}\\log P(C^{*}\\mid D)\\Big\\}}\\end{array}\n$$\n "}
{"page": 4, "image_path": "doc_images/D18-1360_4.jpg", "ocr_text": "where E*, R*, and C* are gold structures of the en-\ntity types, relations, and coreference, respectively.\nThe task weights Ag, Ar, and Ac are introduced as\nhyper-parameters to control the importance of each\ntask.\n\nFor entity recognition and relation extraction,\nP(E* | D) and P(R* | D) are computed with\nthe definition in Equation (2). For coreference\nresolution, we use the marginalized loss follow-\ning Lee et al. (2017) since each mention can have\nmultiple correct antecedents. Let C? be the set\nof all correct antecedents for span i, we have:\n\nlog P(C* | D) = YO,-1. x log eect P(c| D).\n\n4.3, Scoring Architecture\n\nWe use feedforward neural networks (FFNNs) over\nshared span representations g to compute a set\nof span and pairwise span scores. For the span\nscores, e(S;) measures how likely a span s; has\nan entity type e, and @mr(s;) and dmc(s;) measure\nhow likely a span s; is a mention in a relation or a\ncoreference link, respectively. The pairwise scores\ngr(si,8;) and $-(s;,;) measure how likely two\nspans are associated in a relation r or a coreference\nlink, respectively. Let g; be the fixed-length vec-\ntor representation for span s;. For different tasks,\nthe span scores ¢x(s;) for x € {e,mc, mr} and\npairwise span scores ¢y(s;,8;) for y € {r,c} are\ncomputed as follows:\n\n$x(8;) =wx - FFNN;(g;)\ngy (Sis 8;) =Wy- FENN, ([gi; 8,81 © jl);\n\nwhere © is element-wise multiplication, and\n{w x, wy} are neural network parameters to be\nlearned.\n\nWe use these scores to compute the different ®:\n\np(e,5i) = de(si) 4\n\nThe scores in Equation (4) are defined for entity\ntypes, relations, and antecedents that are not the\nnull-type «. Scores involving the null label are\nset to a constant 0: ®g(e,s;) = Or(€, si, 8;) =\n®c(si, 6) =0.\n\nWe use the same span representations g from\n(Lee et al., 2017) and share them across the three\ntasks. We start by building bi-directional LSTMs\n(Hochreiter and Schmidhuber, 1997) from word,\ncharacter and ELMo (Peters et al., 2018) embed-\ndings.\n\nFor a span s;, its vector representation g; is con-\nstructed by concatenating s;’s left and right end\npoints from the BiLSTM outputs, an attention-\nbased soft “headword,” and embedded span width\nfeatures. Hyperparameters and other implementa-\ntion details will be described in Section 6.\n\n4.4 Inference and Pruning\n\nFollowing previous work, we use beam pruning to\nreduce the number of pairwise span factors from\nO(n*) to O(n?) at both training and test time,\nwhere n is the number of words in the document.\nWe define two separate beams: Bc to prune spans\nfor the coreference resolution task, and Bp for rela-\ntion extraction. The spans in the beams are sorted\nby their span scores ¢mc and ¢mr respectively, and\nthe sizes of the beams are limited by \\cn and Agn.\nWe also limit the maximum width of spans to a\nfixed number W, which further reduces the num-\nber of span factors to O(n).\n\n5 Knowledge Graph Construction\n\nWe construct a scientific knowledge graph from\na large corpus of scientific articles. The corpus\nincludes all abstracts (110k in total) from 12 AI\nconference proceedings from the Semantic Scholar\nCorpus. Nodes in the knowledge graph correspond\nto scientific entities. Edges correspond to scientific\nrelations between pairs of entities. The edges are\ntyped according to the relation types defined in Sec-\ntion 3. Figure 4 shows a part of a knowledge graph\ncreated by our method. For example, Statistical\nMachine Translation (SMT) and grammatical error\ncorrection are nodes in the graph, and they are con-\nnected through a Used-for relation type. In order\nto construct the knowledge graph for the whole\ncorpus, we first apply the SCIIE model over sin-\ngle documents and then integrate the entities and\nrelations across multiple documents (Figure 3).\n\nExtracting nodes (entities) The SCIIE model\nextracts entities, their relations, and coreference\n\nAbstract(1) ScllE > fe\n\nAbstract(2) ScilE > & Merging => i\nAbstra\n\npstract(m) SclE -——> Scientific KG\n\nDocument-level KGs\n\nFigure 3: Knowledge graph construction process.\n\n3223\n", "vlm_text": "where    $E^{*}$  ,  $R^{*}$  , and    $C^{*}$  are gold structures of the en- tity types, relations, and coreference, respectively. The task weights    $\\lambda_{\\mathrm{E}},\\,\\lambda_{\\mathrm{R}}$  , and    $\\lambda_{\\mathrm{C}}$   are introduced as hyper-parameters to control the importance of each task. \nFor entity recognition and relation extraction,  $P(E^{*}\\mid D)$   and    $P(R^{*}\\mid D)$   are computed with the deﬁnition in Equation  ( 2 ) . For coreference resolution, we use the marginalized loss follow- ing  Lee et al.  ( 2017 ) since each mention can have multiple correct antecedents. Let    $C_{i}^{*}$    be the set of all correct antecedents for span    $i$  , we have:  $\\begin{array}{r}{\\log P(C^{*}\\mid D)=\\sum_{i=1..N}\\log\\sum_{c\\in C_{i}^{*}}P(c\\mid D)}\\end{array}$  . ∈ \n4.3 Scoring Architecture \nWe use feedforward neural networks (FFNNs) over shared span representations    $\\mathbf{g}$   to compute a set of span and pairwise span scores. For the span scores,  $\\phi_{e}(s_{i})$   measures how likely a span  $s_{i}$   has an entity type    $e$  , and    $\\phi_{\\mathrm{mr}}(s_{i})$   and  $\\phi_{\\mathrm{mc}}(s_{i})$   measure how likely a span    $s_{i}$   is a mention in a relation or a coreference link, respectively. The pairwise scores  $\\phi_{r}(s_{i},s_{j})$   and    $\\phi_{\\mathsf{c}}(s_{i},s_{j})$   measure how likely two spans are associated in a relation  $r$   or a coreference link, respectively. Let    $\\mathbf{g}_{i}$   be the ﬁxed-length vec- tor representation for span    $s_{i}$  . For different tasks, the span scores    $\\phi_{\\mathbf{X}}(s_{i})$   for    $\\mathbf{x}~\\in~\\{e,\\mathsf{m c},\\mathsf{m r}\\}$   and pairwise span scores    $\\phi_{\\mathbf{y}}(s_{i},s_{j})$   for    $\\mathtt{y}\\in\\{r,\\mathtt{c}\\}$   are computed as follows: \n\n$$\n\\begin{array}{r l}&{\\mathrel{\\phantom{=}}\\phi_{\\mathrm{x}}\\bigl(s_{i}\\bigr)=\\!\\mathbf{w}_{\\mathrm{x}}\\cdot\\mathrm{FFNN}_{\\mathrm{x}}\\bigl(\\mathbf{g}_{i}\\bigr)}\\\\ &{\\phi_{\\mathrm{y}}\\bigl(s_{i},s_{j}\\bigr)=\\!\\mathbf{w}_{\\mathrm{y}}\\cdot\\mathrm{FFNN}_{\\mathrm{y}}\\bigl([\\mathbf{g}_{i},\\mathbf{g}_{j},\\mathbf{g}_{i}\\odot\\mathbf{g}_{j}]\\bigr),}\\end{array}\n$$\n \nwhere    $\\odot$  is element-wise multiplication, and  $\\{\\mathbf{w}_{\\mathrm{x}},\\mathbf{w}_{\\mathrm{y}}\\}$   are neural network parameters to be learned. \nWe use these scores to compute the different    $\\Phi$  : \n\n$$\n\\begin{array}{r l r}{\\Phi_{\\mathsf{E}}(e,s_{i})}&{=}&{\\phi_{e}(s_{i})\\qquad\\qquad\\qquad\\qquad\\quad(4)}\\\\ {\\Phi_{\\mathsf{R}}\\!\\left(r,s_{i},s_{j}\\right)}&{=}&{\\phi_{\\mathsf{m r}}(s_{i})+\\phi_{\\mathsf{m r}}(s_{j})+\\phi_{r}\\!\\left(s_{i},s_{j}\\right)}\\\\ {\\Phi_{\\mathsf{C}}\\!\\left(s_{i},s_{j}\\right)}&{=}&{\\phi_{\\mathsf{m c}}(s_{i})+\\phi_{\\mathsf{m c}}(s_{j})+\\phi_{\\mathsf{c}}(s_{i},s_{j})}\\end{array}\n$$\n \nThe scores in Equation  ( 4 )  are deﬁned for entity types, relations, and antecedents that are not the null-type    $\\epsilon$  . Scores involving the null label are set to a constant 0:    $\\begin{array}{r l r}{\\lefteqn{\\Phi_{\\mathrm{E}}(\\epsilon,s_{i})\\,=\\,\\Phi_{\\mathrm{R}}(\\epsilon,s_{i},s_{j})\\,=}}\\end{array}\n\n$   $\\Phi_{\\mathrm{{C}}}(s_{i},\\epsilon)=0$  . \nWe use the same span representations    $\\mathbf{g}$   from\n\n ( Lee et al. ,  2017 ) and share them across the three tasks. We start by building bi-directional LSTMs ( Hochreiter and Schmidhuber ,  1997 ) from word, character and ELMo ( Peters et al. ,  2018 ) embed- dings. \nFor a span    $s_{i}$  , its vector representation    $\\mathbf{g}_{i}$   is con- structed by concatenating    $s_{i}$  ’s left and right end points from the BiLSTM outputs, an attention- based soft “headword,” and embedded span width features. Hyperparameters and other implementa- tion details will be described in Section  6 . \n4.4 Inference and Pruning \nFollowing previous work, we use beam pruning to reduce the number of pairwise span factors from  $O(n^{4})$   to    $O(n^{2})\\,$   at both training and test time, where    $n$   is the number of words in the document. We deﬁne two separate beams:    $B_{\\mathrm{C}}$   to prune spans for the coreference resolution task, and    $B_{\\mathrm{R}}$   for rela- tion extraction. The spans in the beams are sorted by their span scores    $\\phi_{\\mathrm{m c}}$   and    $\\phi_{\\mathrm{mr}}$   respectively, and the sizes of the beams are limited by  $\\lambda_{\\mathbf{C}}n$   and    $\\lambda_{\\mathrm{R}}n$  . We also limit the maximum width of spans to a ﬁxed number    $W$  , which further reduces the num- ber of span factors to    $O(n)$  . \n5 Knowledge Graph Construction \nWe construct a scientiﬁc knowledge graph from a large corpus of scientiﬁc articles. The corpus includes all abstracts (  $110\\mathbf{k}$   in total) from 12 AI conference proceedings from the Semantic Scholar Corpus. Nodes in the knowledge graph correspond to scientiﬁc entities. Edges correspond to scientiﬁc relations between pairs of entities. The edges are typed according to the relation types deﬁned in Sec- tion  3 . Figure  4  shows a part of a knowledge graph created by our method. For example,  Statistical Machine Translation (SMT)  and  grammatical error correction  are nodes in the graph, and they are con- nected through a  Used-for  relation type. In order to construct the knowledge graph for the whole corpus, we ﬁrst apply the S CI IE model over sin- gle documents and then integrate the entities and relations across multiple documents (Figure  3 ). \nExtracting nodes (entities) The S CI IE model extracts entities, their relations, and coreference \nThe image illustrates a knowledge graph construction process. It consists of the following steps:\n\n1. **Extraction from Abstracts**: Multiple abstracts (labeled Abstract(1), Abstract(2), ..., Abstract(m)) are processed.\n2. **SciIE**: Each abstract goes through a step labeled \"SciIE,\" which likely stands for Scientific Information Extraction.\n3. **Document-level KGs**: This process creates document-level knowledge graphs (KGs) from the abstracts, represented by small networks of colored nodes and connecting lines.\n4. **Merging**: These document-level KGs are then merged to form a larger, integrated \"Scientific KG\" – a comprehensive knowledge graph."}
{"page": 5, "image_path": "doc_images/D18-1360_5.jpg", "ocr_text": "2\n% x OC &\nay 4 gs §\n% a 2 5 © & s\n°% . FS os\ny, ° ~ FX\nb i}\neeu, Whe 3 ge\n“eatin % 4 S ‘\nOn «. YN, 2 2\n“, ost!\n° xi entropy\nTetrieval\nSI eWSD\ntranslation’ Mr. s\ngeasohysedi™* Word alignment\nPlog. j\na8 2 ei\n598 G. lineay\nso 7 hy, dy Modes\nss .\noe ym\neh os! og, ei\n£ ee “ey “x, On\nSs < \\ % %, %\nSs t 2.\na é 5h 5% % % My,\ns £ $8 3 % %\n= y S 2B e %\n3s & & o\n¢ ge\n\nFigure 4: A part of an automatically constructed\nscientific knowledge graph with the most frequent\nneighbors of the scientific term statistical machine\ntranslation (SMT) on the graph. For simplicity we\ndenote Used-for (Reverse) as Uses, Evaluated-for\n(Reverse) as Evaluated-by, and replace common\nterms with their acronyms. The original graph and\nmore examples are given Figure 10 in Appendix B.\n\nclusters within one document. Phrases are heuris-\ntically normalized (described in Section 6) using\nentities and coreference links. In particular, we\nlink all entities that belong to the same coreference\ncluster to replace generic terms with any other non-\ngeneric term in the cluster. Moreover, we replace\nall the entities in the cluster with the entity that has\nthe longest string. Our qualitative analysis shows\nthat there are fewer ambiguous phrases using coref-\nerence links (Figure 5). We calculate the frequency\ncounts of all entities that appear in the whole cor-\npus. We assign nodes in the knowledge graph by\nselecting the most frequent entities (with counts\n> k) in the corpus, and merge in any remaining\nentities for which a frequent entity is a substring.\n\nAssigning edges (relations) A pair of entities\nmay appear in different contexts, resulting in differ-\nent relation types between those entities (Figure 6).\nFor every pair of entities in the graph, we calculate\nthe frequency of different relation types across the\nwhole corpus.We assign edges between entities by\nselecting the most frequent relation type.\n\n6 Experimental Setup\n\nWe evaluate our unified framework SCIIE on SCI-\nERC and SemEval 17. The knowledge graph for\n\naction detection\n\n(With Coref. [5] Without Coref.\n\npedestrian detection\nhuman detection\nface detection\n\nobject detection\n\n] 1297\n] 1237\n\ndetection\n\nFigure 5: Frequency of detected entities with and\nwithout coreferece resolution: using coreference\nreduces the frequency of the generic phrase detec-\ntion while significantly increasing the frequency of\nspecific phrases. Linking entities through corefer-\nence helps disambiguate phrases when generating\nthe knowledge graph.\n\n80 30\n% 80 (/Mr-asr 25 []}crF-GM\na\nE 60 20\nE\na\n= 40\n2 10 4\nZ 20 10\n* _ fo\n0 0\neyncliO® 00 (9% verse) sn Of tio 29 {08 Verse)\ncont Niort Bee wyPNCoAT y Fo REN\n‘s ss\n\nFigure 6: Frequency of relation types between pairs\nof entities: (left) automatic speech recognition\n(ASR) and machine translation (MT), (right) con-\nditional random field (CRF) and graphical model\n(GM). We use the most frequent relation between\npairs of entities in the knowledge graph.\n\nscientific community analysis is built using the Se-\nmantic Scholar Corpus (110k abstracts in total).\n\n6.1 Baselines\n\nWe compare our model with the following base-\nlines on SCIER Cdataset:\n\ne LSTM+CRF The state-of-the-art NER sys-\ntem (Lample et al., 2016), which applies CRF\non top of LSTM for named entity tagging, the\napproach has also been used in scientific term\nextraction (Luan et al., 2017b).\n\ne LSTM+CRF+ELMo LSTM+CRF with\nELMO as an additional input feature.\n\ne E2E Rel State-of-the-art joint entity and re-\nlation extraction system (Miwa and Bansal,\n2016) that has also been used in scientific lit-\nerature (Peters et al., 2017; Augenstein et al.,\n2017). This system uses syntactic features\nsuch as part-of-speech tagging and depen-\ndency parsing.\n\n3224\n", "vlm_text": "The image is a part of a scientific knowledge graph centered around the term \"Statistical Machine Translation (SMT).\" It shows the most frequent neighbors of SMT in the graph, categorized by different relationships:\n\n1. **Compare**: \n   - NMT (Neural Machine Translation)\n   - MT (Machine Translation)\n\n2. **Evaluated-by**:\n   - WER (Word Error Rate)\n   - ROUGE\n   - METEOR\n   - BLEU\n   - Perplexity\n\n3. **Conjunction**:\n   - ASR (Automatic Speech Recognition)\n   - Classification\n\n4. **Used-for**:\n   - Retrieval\n   - Translation\n   - Search\n   - Paraphrasing\n   - Semantic Parsing\n   - Grammatical Error Correction\n\n5. **Uses**:\n   - Alignment\n   - Parser\n   - Adaptation\n   - Decoding\n   - RNN (Recurrent Neural Networks)\n   - NN (Neural Networks)\n   - Topic Model\n   - Word Segmentation\n   - Stochastic Local Search\n   - Domain Adaptation\n   - Log-linear Model\n   - Word Alignment\n   - WSD (Word Sense Disambiguation)\n   - Maximum Entropy\n   - Segmentation\n\nThis graph visually represents the connections and interactions of SMT with other terms and fields.\nclusters within one document. Phrases are heuris- tically normalized (described in Section  6 ) using entities and coreference links. In particular, we link all entities that belong to the same coreference cluster to replace generic terms with any other non- generic term in the cluster. Moreover, we replace all the entities in the cluster with the entity that has the longest string. Our qualitative analysis shows that there are fewer ambiguous phrases using coref- erence links (Figure  5 ). We calculate the frequency counts of all entities that appear in the whole cor- pus. We assign nodes in the knowledge graph by selecting the most frequent entities (with counts  $>\\,k)$   in the corpus, and merge in any remaining entities for which a frequent entity is a substring. \nAssigning edges (relations) A pair of entities may appear in different contexts, resulting in differ- ent relation types between those entities (Figure  6 ). For every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.We assign edges between entities by selecting the most frequent relation type. \n6 Experimental Setup \nWe evaluate our uniﬁed framework S CI IE on S CI - ERC and SemEval 17. The knowledge graph for \nThe image is a bar chart comparing performance figures for various detection tasks, measured with and without coreference (Coref). Here are the details:\n\n- **Detection**: \n  - Without Coref: 1297\n  - With Coref: 1237\n\n- **Object Detection**: \n  - Without Coref: 510\n  - With Coref: 585\n\n- **Face Detection**: \n  - Without Coref: 177\n  - With Coref: 124\n\n- **Human Detection**: \n  - Without Coref: 84\n  - With Coref: 90\n\n- **Pedestrian Detection**: \n  - Without Coref: 57\n  - With Coref: 90\n\n- **Action Detection**: \n  - Without Coref: 63\n  - With Coref: 87\n\nThe bars are color-coded: red represents \"Without Coref.\" and blue represents \"With Coref.\"\nFigure 5 : Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase  detec- tion  while signiﬁcantly increasing the frequency of speciﬁc phrases. Linking entities through corefer- ence helps disambiguate phrases when generating the knowledge graph. \nThe image consists of two bar charts comparing the number of relation triples for different categories.\n\n**Left Chart:**\n- Title: None\n- Categories: \"Conjunction,\" \"Used for,\" \"Used for (Reverse)\"\n- Data for \"MT-ASR\":\n  - Conjunction: 80\n  - Used for: 10\n  - Used for (Reverse): 4\n\n**Right Chart:**\n- Title: None\n- Categories: \"Hyponym of,\" \"Conjunction,\" \"Used for,\" \"Used for (Reverse)\"\n- Data for \"CRF-GM\":\n  - Hyponym of: 25\n  - Conjunction: 4\n  - Used for: 2\n  - Used for (Reverse): 2\n\nBoth charts are presented using light blue bars.\nFigure 6 : Frequency of relation types between pairs of entities: ( left ) automatic speech recognition (ASR) and machine translation (MT), ( right ) con- ditional random ﬁeld (CRF) and graphical model (GM). We use the most frequent relation between pairs of entities in the knowledge graph. \nscientiﬁc community analysis is built using the Se- mantic Scholar Corpus (110k abstracts in total). \n6.1 Baselines \nWe compare our model with the following base- lines on S CI ERCdataset: \n•    ${\\bf L S T M+C R F}$   The state-of-the-art NER sys- tem ( Lample et al. ,  2016 ), which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientiﬁc term extraction ( Luan et al. ,  2017b ). \n•  LSTM  $^+$  CRF  $+$  ELMo LSTM  $+$  CRF with ELM O  as an additional input feature. \n•  E2E Rel  State-of-the-art joint entity and re- lation extraction system ( Miwa and Bansal , 2016 ) that has also been used in scientiﬁc lit- erature ( Peters et al. ,  2017 ;  Augenstein et al. , 2017 ). This system uses syntactic features such as part-of-speech tagging and depen- dency parsing. "}
{"page": 6, "image_path": "doc_images/D18-1360_6.jpg", "ocr_text": "e E2E Rel(Pipeline) Pipeline setting of E2E\nRel. Extract entities first and use entity results\nas input to relation extraction task.\n\ne E2E Rel+ELMo E2E Rel with ELMO as an\nadditional input feature.\n\ne E2E Coref State-of-the-art coreference sys-\ntem Lee et al. (2017) combined with ELMo.\nOur system SCIIE extends E2E Coref with\nmulti-task learning.\n\nIn the SemEval task, we compare our model\nSculE with the best reported system in the SemEval\nleaderboard (Peters et al., 2017), which extends\nE2E Rel with several in-domain features such as\ngazetteers extracted from existing knowledge bases\nand model ensembles. We also compare with the\nstate of the art on keyphrase extraction (Luan et al.,\n2017b), which applies semi-supervised methods to\na neural tagging model.\n\n6.2 Implementation details\n\nOur system extends the implementation and hyper-\nparameters from Lee et al. (2017) with the follow-\ning adjustments. We use a | layer BiLSTM with\n200-dimensional hidden layers. All the FFNNs\nhave 2 hidden layers of 150 dimensions each. We\nuse 0.4 variational dropout (Gal and Ghahramani,\n2016) for the LSTMs, 0.4 dropout for the FFNNs,\nand 0.5 dropout for the input embeddings. We\nmodel spans up to 8 words. For beam pruning,\nwe use Ac = 0.3 for coreference resolution and\nAr = 0.4 for relation extraction. For constructing\nthe knowledge graph, we use the following heuris-\ntics to normalize the entity phrases. We replace all\nacronyms with their corresponding full name and\nnormalize all the plural terms with their singular\ncounterparts.\n\n7 Experimental Results\n\nWe evaluate SCIIE on SCIERC and SemEval 17\ndatasets. We provide qualitative results and human\nevaluation of the constructed knowledge graph.\n\n7.1 TE Results\n\nResults on SciERC Table 2 compares the result\nof our model with baselines on the three tasks: en-\ntity recognition (Table 2a), relation extraction (Ta-\nble 2b), and coreference resolution (Table 2c). As\nevidenced by the table, our unified multi-task setup\n\n3We compare with the inductive setting results.\n\nDev Test\nModel P R_ Fl P R_ Fl\nLSTM+CRF 67.2 65.8 66.5 62.9 61.1 62.0\nLSTM+CRF+ELMo 68.1 66.3 67.2 63.8 63.2 63.5\nE2E Rel(Pipeline) 66.7 65.9 66.3 60.8 61.2 61.0\nE2E Rel 64.3 68.6 66.4 60.6 61.9 61.2\nE2E Rel+ELMo 67.5 66.3 66.9 63.5 63.9 63.7\nSciE 70.0 66.3 68.1 67.2 61.5 64.2\n(a) Entity recognition.\nDev Test\nModel P R Fl P R Fl\nE2E Rel(Pipeline) 34.2 33.7 33.9 37.8 34.2 35.9\nE2E Rel 37.3 33.5 35.3 37.1 32.2 34.1\nE2ERel+ELMo 38.5 36.4 37.4 384 349 36.6\nSciE 45.4 34.9 39.5 47.6 33.5 39.3\n(b) Relation extraction.\nDev Test\n\nModel P R Fl P R Fl\nE2ECoref 59.4 52.0 554 60.9 37.3 46.2\nSciE 615 548 58.0 52.0 449 48.2\n\n(c) Coreference resolution.\n\nTable 2: Comparison with previous systems on\nthe development and test set for our three tasks.\nFor coreference resolution, we report the average\nP/R/F1 of MUC, B®, and CEAF4, scores.\n\nScCUE outperforms all the baselines. For entity\nrecognition, our model achieves 1.3% and 2.4%\nrelative improvement over LSTM+CRF with and\nwithout ELMO, respectively. Moreover, it achieves\n.8% and 2.7% relative improvement over E2E Rel\nwith and without ELMo, respectively. For rela-\nion extraction, we observe more significant im-\nprovement with 13.1% relative improvement over\nE2E Rel and 7.4% improvement over E2E Rel with\nELMoO. For coreference resolution, SCIIE outper-\norms E2E Coref with 4.5% relative improvement.\nWe still observe a large gap between human-level\nperformance and a machine learning system. We\ninvite the community to address this challenging\nask.\n\nAblations We evaluate the effect of multi-task\nlearning in each of the three tasks defined in our\ndataset. Table 3 reports the results for individual\ntasks when additional tasks are included in the\nlearning objective function. We observe that per-\nformance improves with each added task in the\nobjective. For example, Entity recognition (65.7)\nbenefits from both coreference resolution (67.5)\nand relation extraction (66.8). Relation extrac-\n\n3225\n", "vlm_text": "•  E2E Rel(Pipeline)  Pipeline setting of E2E Rel. Extract entities ﬁrst and use entity results as input to relation extraction task. \n•  E2E Rel+ELMo  E2E Rel with ELM O  as an additional input feature. \n•  E2E Coref  State-of-the-art coreference sys- tem  Lee et al.  ( 2017 ) combined with ELM O . Our system S CI IE extends E2E Coref with multi-task learning. \nIn the SemEval task, we compare our model S CI IE with the best reported system in the SemEval leaderboard ( Peters et al. ,  2017 ), which extends E2E Rel with several in-domain features such as gazetteers extracted from existing knowledge bases and model ensembles. We also compare with the state of the art on keyphrase extraction ( Luan et al. , 2017b ), which applies semi-supervised methods to a neural tagging model. \n6.2 Implementation details \nOur system extends the implementation and hyper- parameters from  Lee et al.  ( 2017 ) with the follow- ing adjustments. We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the FFNNs have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout ( Gal and Ghahramani , 2016 ) for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings. We model spans up to 8 words. For beam pruning, we use    $\\lambda_{\\mathsf{C}}\\,=\\,0.3$   for coreference resolution and  $\\lambda_{\\mathrm{R}}=0.4$   for relation extraction. For constructing the knowledge graph, we use the following heuris- tics to normalize the entity phrases. We replace all acronyms with their corresponding full name and normalize all the plural terms with their singular counterparts. \n7 Experimental Results \nWe evaluate S CI IE on S CI ERC and SemEval 17 datasets. We provide qualitative results and human evaluation of the constructed knowledge graph. \n7.1 IE Results \nResults on SciERC Table  2  compares the result of our model with baselines on the three tasks: en- tity recognition (Table  2a ), relation extraction (Ta- ble  2b ), and coreference resolution (Table  2c ). As evidenced by the table, our uniﬁed multi-task setup \nThe table presents the results of different models on two tasks: entity recognition and relation extraction. It shows precision (P), recall (R), and F1 scores for both development (Dev) and test sets.\n\n### (a) Entity recognition:\n- **Models Evaluated:**\n  - LSTM+CRF\n  - LSTM+CRF+ELMo\n  - E2E Rel(Pipeline)\n  - E2E Rel\n  - E2E Rel+ELMo\n  - SciIE\n\n- **Development Set Scores:**\n  - SciIE achieves the highest F1 score of 68.1.\n  \n- **Test Set Scores:**\n  - SciIE leads with an F1 score of 64.2.\n\n### (b) Relation extraction:\n- **Models Evaluated:**\n  - E2E Rel(Pipeline)\n  - E2E Rel\n  - E2E Rel+ELMo\n  - SciIE\n\n- **Development Set Scores:**\n  - SciIE reaches the highest F1 score of 39.5.\n\n- **Test Set Scores:**\n  - SciIE has the top F1 score of 39.3.\n\n### (c) Coreference Resolution (mentions \"E2E Coref\" in context):\n- **Models Evaluated:**\n  - E2E Coref\n  - SciIE\n\n- **Development Set Scores:**\n  - SciIE attains the highest F1 score of 58.0.\n  \n- **Test Set Scores:**\n  - SciIE achieves a maximum F1 score of 48.2.\n\nOverall, SciIE demonstrates superior performance across all tasks and datasets tested.\nTable 2 : Comparison with previous systems on the development and test set for our three tasks. For coreference resolution, we report the average P/R/F1 of MUC,  $\\mathbf{B}^{3}$  , and   $\\mathrm{CEAF}_{\\phi_{4}}$   scores. \nS CI IE outperforms all the baselines. For entity recognition, our model achieves   $1.3\\%$   and   $2.4\\%$  relative improvement over   $\\tt L S T M+C R F$   with and without ELM O , respectively. Moreover, it achieves  $1.8\\%$   and  $2.7\\%$   relative improvement over E2E Rel with and without ELM O , respectively. For rela- tion extraction, we observe more signiﬁcant im- provement with   $13.1\\%$   relative improvement over E2E Rel and  $7.4\\%$   improvement over E2E Rel with ELM O . For coreference resolution, S CI IE outper- forms E2E Coref with  $4.5\\%$   relative improvement. We still observe a large gap between human-level performance and a machine learning system. We invite the community to address this challenging task. \nAblations We evaluate the effect of multi-task learning in each of the three tasks deﬁned in our dataset. Table  3  reports the results for individual tasks when additional tasks are included in the learning objective function. We observe that per- formance improves with each added task in the objective. For example, Entity recognition (65.7) beneﬁts from both coreference resolution (67.5) and relation extraction (66.8). Relation extrac- "}
{"page": 7, "image_path": "doc_images/D18-1360_7.jpg", "ocr_text": "Task Entity Rec. Relation Coref.\nMulti Task (SCIIE) 68.1 39.5 58.0\n\nSingle Task 65.7 37.9 55.3\n\n+Entity Rec. - 38.9 57.1\n\n+Relation 66.8 - 57.6\n\n+Coreference 67.5 39.5 -\n\nTable 3: Ablation study for multitask learning on\nSCIERC development set. Each column shows\nresults for the target task.\n\ntion (37.9) significantly benefits when multi-tasked\nwith coreference resolution (7.1% relative improve-\nment). Coreference resolution benefits when multi-\ntasked with relation extraction, with 4.9% relative\nimprovement.\n\nResults on SemEval 17 Table 4 compares the\nresults of our model with the state of the art on the\nSemEval 17 dataset for tasks of span identification,\nkeyphrase extraction and relation extraction as well\nas the overall score. Span identification aims at\nidentifying spans of entities. Keyphrase classifi-\ncation and relation extraction has the same setting\nwith the entity and relation extraction in SCIERC.\nOur model outperforms all the previous models\nthat use hand-designed features. We observe more\nsignificant improvement in span identification than\nkeyphrase classification. This confirms the bene-\nfit of our model in enumerating spans (rather than\nBIO tagging in state-of-the-art systems). More-\nover, we have competitive results compared to the\nprevious state of the art in relation extraction. We\nobserve less gain compared to the SCIERC dataset\nmainly because there are no coference links, and\nthe relation types are not comprehensive.\n\n7.2 Knowledge Graph Analysis\n\nWe provide qualitative analysis and human evalua-\ntions on the constructed knowledge graph.\n\nScientific trend analysis Figure 7 shows the his-\ntorical trend analysis (from 1996 to 2016) of the\nmost popular applications of the phrase neural net-\nwork, selected according to the statistics of the\nextracted relation triples with the ‘Used-for’ rela-\ntion type from speech, computer vision, and NLP\nconference papers. We observe that, before 2000,\nneural network has been applied to a greater per-\ncentage of speech applications compared to the\nNLP and computer vision papers. In NLP, neural\nnetworks first gain popularity in language modeling\n\n0.6 1 _ Language Modeling\n0.4 |. | —e— Machine Translation\n—+— POS Tagging\n0.2\n0\n1,995 2,000 2,005 2,010 2,015\n0.6 [|| —e— Speech Recognition\n0.4 | | > Speech Synthesis\n. —+— Speaker Recognition\n0.2\n0\n1,995 2,000 2,005 2,010 2,015\n—s— Object Recognition\n0.4) | _. Object Detection\n0.2 | | t= Image Segmentation\n0\n1995 2000 2005 2010 2015\nFigure 7: Historical trend for top applications of the\nkeyphrase neural network in NLP, speech, and CV\nconference papers we collected. y-axis indicates\nthe ratio of papers that use neural network in the\ntask to the number of papers that is about the task.\n\n—o— With Coref.\n2990 EE Without Coref.\n=\nS88\n3\n£86\n\n84\n\n0 20 40 60 80 100\n\nPseudo-recall %\n\nFigure 8: Precision/pseudo-recall curves for human\nevaluation by varying cut-off thresholds. The AUC\nis 0.751 with coreference, and 0.695 without.\n\nand then extend to other tasks such as POS Tag-\nging and Machine Translation. In computer vision,\nthe application of neural networks gains popularity\nin object recognition earlier (around 2010) than\nthe other two more complex tasks of object detec-\ntion and image segmentation (hardest and also the\nlatest).\n\nKnowledge Graph Evaluation Figure 8 shows\nthe human evaluation of the constructed knowl-\nedge graph, comparing the quality of automatically\ngenerated knowledge graphs with and without the\ncoreference links. We randomly select 10 frequent\nscientific entities and extract all the relation triples\nthat include one of the selected entities leading to\n1.5k relation triples from both systems. We ask\nfour domain experts to annotate each of these ex-\n\n3226\n", "vlm_text": "The table displays results from a series of experiments related to \"Entity Recognition,\" \"Relation,\" and \"Coreference\" tasks. It compares a multitask approach (specifically called \"SciIE\") with several single-task approaches and combinations. Here are the key points:\n\n- **Multitask (SciIE)**: Achieves values of 68.1 for Entity Recognition, 39.5 for Relation, and 58.0 for Coreference.\n\n- **Single Task**: \n  - Entity Rec.: 65.7 for Entity Recognition, 37.9 for Relation, and 55.3 for Coreference.\n  - +Entity Rec.: - (no value for Entity Recognition), 38.9 for Relation, and 57.1 for Coreference.\n  - +Relation: 66.8 for Entity Recognition, - (no value for Relation), and 57.6 for Coreference.\n  - +Coreference: 67.5 for Entity Recognition, 39.5 for Relation, and - (no value for Coreference).\n\nEach row represents a task configuration, and the values likely represent performance metrics (such as accuracy, F1 score, etc.) for each task. The multitask approach in SciIE seems to perform better overall compared to most single-task configurations.\nTable 3 : Ablation study for multitask learning on S CI ERC development set. Each column shows results for the target task. \ntion (37.9) signiﬁcantly beneﬁts when multi-tasked with coreference resolution (  $7.1\\%$   relative improve- ment). Coreference resolution beneﬁts when multi- tasked with relation extraction, with   $4.9\\%$   relative improvement. \nResults on SemEval 17 Table  4  compares the results of our model with the state of the art on the SemEval 17 dataset for tasks of span identiﬁcation, keyphrase extraction and relation extraction as well as the overall score. Span identiﬁcation aims at identifying spans of entities. Keyphrase classiﬁ- cation and relation extraction has the same setting with the entity and relation extraction in S CI ERC. Our model outperforms all the previous models that use hand-designed features. We observe more signiﬁcant improvement in span identiﬁcation than keyphrase classiﬁcation. This conﬁrms the bene- ﬁt of our model in enumerating spans (rather than BIO tagging in state-of-the-art systems). More- over, we have competitive results compared to the previous state of the art in relation extraction. We observe less gain compared to the S CI ERC dataset mainly because there are no coference links, and the relation types are not comprehensive. \n7.2 Knowledge Graph Analysis \nWe provide qualitative analysis and human evalua- tions on the constructed knowledge graph. \nScientiﬁc trend analysis Figure  7  shows the his- torical trend analysis (from 1996 to 2016) of the most popular applications of the phrase  neural net- work , selected according to the statistics of the extracted relation triples with the ‘Used-for’ rela- tion type from speech, computer vision, and NLP conference papers. We observe that, before 2000, neural network  has been applied to a greater per- centage of speech applications compared to the NLP and computer vision papers. In NLP, neural networks ﬁrst gain popularity in language modeling \nThe image consists of three line graphs, each depicting the historical trend in the ratio of conference papers using neural networks in specific tasks within the fields of natural language processing (NLP), speech, and computer vision (CV) from 1995 to 2015. Each graph represents the proportion of papers using neural networks for different tasks compared to the total number of papers about those tasks.\n\n1. The top graph shows trends in three NLP tasks:\n   - Language Modeling (represented by a blue line with square markers)\n   - Machine Translation (represented by a red line with circular markers)\n   - POS Tagging (represented by a green line with diamond markers)\n\n2. The middle graph shows trends in three speech-related tasks:\n   - Speech Recognition (blue line with square markers)\n   - Speech Synthesis (red line with circular markers)\n   - Speaker Recognition (green line with diamond markers)\n\n3. The bottom graph shows trends in three CV tasks:\n   - Object Recognition (blue line with square markers)\n   - Object Detection (red line with circular markers)\n   - Image Segmentation (green line with diamond markers)\n\nIn all graphs, there is a noticeable increase in the ratio of papers using neural networks for these tasks starting around 2009 and continuing through 2015, with some tasks reaching a ratio of nearly 0.6 by 2015. This indicates the growing adoption of neural networks in these application areas during this period.\nThe image is a graph showing precision versus pseudo-recall curves for a human evaluation. The blue line represents results \"With Coreference,\" while the red line represents results \"Without Coreference.\" The precision percentage is plotted on the vertical axis, ranging from 84% to 92%, while the pseudo-recall percentage is on the horizontal axis, ranging from 0% to 100%.\n\nThe graph indicates that the Area Under the Curve (AUC) is 0.751 for the curve with coreference and 0.695 for the one without, suggesting better performance with coreference.\nand then extend to other tasks such as POS Tag- ging and Machine Translation. In computer vision, the application of neural networks gains popularity in  object recognition  earlier (around 2010) than the other two more complex tasks of  object detec- tion  and  image segmentation  (hardest and also the latest). \nKnowledge Graph Evaluation Figure  8  shows the human evaluation of the constructed knowl- edge graph, comparing the quality of automatically generated knowledge graphs with and without the coreference links. We randomly select 10 frequent scientiﬁc entities and extract all the relation triples that include one of the selected entities leading to  $1.5\\mathrm{k}$   relation triples from both systems. We ask four domain experts to annotate each of these ex- "}
{"page": 8, "image_path": "doc_images/D18-1360_8.jpg", "ocr_text": "Span Indentification Keyphrase Extraction Relation Extraction Overall\nModel P R Fl P R Fl P R Fl P R Fl\n(Luan 2017) - - 56.9 - - 45.3 - - - - - -\nBest SemEval 55 54 55 44 43 44 36 23 28 44 41 43\nScilE 62.2 55.4 58.6 48.5 43.8 46.0 40.4 21.2 27.8 48.1 41.8 44.7\n\nTable 4: Results for scientific keyphrase extraction and extraction on SemEval 2017 Task 10, comparing\n\nwith previous best systems.\n\ntracted relations to define ground truth labels. Each\ndomain expert is assigned 2 or 3 entities and all of\nthe corresponding relations. Figure 8 shows preci-\nsion/recall curves for both systems. Since it is not\nfeasible to compute the actual recall of the systems,\nwe compute the pseudo-recall (Zhang et al., 2015)\nbased on the output of both systems. We observe\nthat the knowledge graph curve with coreference\nlinking is mostly above the curve without corefer-\nence linking. The precision of both systems is high\n(above 84% for both systems), but the system with\ncoreference links has significantly higher recall.\n\n8 Conclusion\n\nIn this paper, we create a new dataset and develop a\nmulti-task model for identifying entities, relations,\nand coreference clusters in scientific articles. By\nsharing span representations and leveraging cross-\nsentence information, our multi-task setup effec-\ntively improves performance across all tasks. More-\nover, we show that our multi-task model is better at\npredicting span boundaries and outperforms previ-\nous state-of-the-art scientific IE systems on entity\nand relation extraction, without using any hand-\nengineered features or pipeline processing. Using\nour model, we are able to automatically organize\nthe extracted information from a large collection\nof scientific articles into a knowledge graph. Our\nanalysis shows the importance of coreference links\nin making a dense, useful graph.\n\nWe still observe a large gap between the perfor-\nmance of our model and human performance, con-\nfirming the challenges of scientific IE. Future work\nincludes improving the performance using semi-\nsupervised techniques and providing in-domain\nfeatures. We also plan to extend our multi-task\nframework to information extraction tasks in other\ndomains.\n\nAcknowledgments\n\nThis research was supported by the Office of Naval\nResearch under the MURI grant NO0014-18-1-\n\n2670, NSF (IIS 1616112, III 1703166), Allen Dis-\ntinguished Investigator Award, and gifts from Allen\nInstitute for AI, Google, Amazon, and Bloomberg.\nWe are grateful to Waleed Ammar and AI2 for\nsharing the Semantic Scholar Corpus. We also\nthank the anonymous reviewers, UW-NLP group\nand Shoou-I Yu for their helpful comments.\n\nReferences\n\nAmjad Abu-Jbara and Dragomir Radev. 2011. Co-\nherent citation-based summarization of scientific pa-\npers. In Proc. Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies. volume 1, pages 500-509.\n\nHeike Adel and Hinrich Schiitze. 2017. Global normal-\nization of convolutional neural networks for joint en-\ntity and relation classification. In Proc. Conf. Empir-\nical Methods Natural Language Process. (EMNLP).\npages 1723-1729.\n\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, et al. 2018. Construction of the litera-\nture graph in semantic scholar. In Proc. Conf. North\nAmerican Assoc. for Computational Linguistics: Hu-\nman Language Technologies (NAACL-HLT), (Indus-\ntry Papers). pages 84-91.\n\nWaleed Ammar, Matthew Peters, Chandra Bhagavat-\nula, and Russell Power. 2017. The ai2 system at\nsemeval-2017 task 10 (scienceie): semi-supervised\nend-to-end entity and relation extraction. In Proc.\nInt. Workshop on Semantic Evaluation (SemEval).\npages 592-596.\n\nAshton Anderson, Dan McFarland, and Dan Jurafsky.\n2012. Towards a computational history of the ACL:\n1980-2008. In Proc. ACL Special Workshop on Re-\ndiscovering 50 Years of Discoveries. pages 13-21.\n\nAwais Athar and Simone Teufel. 2012a. Context-\nenhanced citation sentiment detection. In Proc.\nConf. North American Assoc. for Computational Lin-\nguistics: Human Language Technologies (NAACL-\nHLT). pages 597-601.\n\nAwais Athar and Simone Teufel. 2012b. Detection of\nimplicit citations for sentiment detection. In Proc.\n\n3227\n", "vlm_text": "The table compares the performance of different models on tasks like Span Identification, Keyphrase Extraction, and Relation Extraction. Here's a breakdown:\n\n### Models:\n1. **Luan 2017**\n2. **Best SemEval**\n3. **SciIE**\n\n### Metrics:\n- **P**: Precision\n- **R**: Recall\n- **F1**: F1 Score\n\n### Results:\n- **Span Identification**\n  - Luan 2017: F1 = 56.9\n  - Best SemEval: P = 55, R = 54, F1 = 55\n  - SciIE: P = 62.2, R = 55.4, F1 = 58.6\n\n- **Keyphrase Extraction**\n  - Luan 2017: F1 = 45.3\n  - Best SemEval: P = 44, R = 43, F1 = 44\n  - SciIE: P = 48.5, R = 43.8, F1 = 46.0\n\n- **Relation Extraction**\n  - Luan 2017: F1 = 28\n  - Best SemEval: P = 36, R = 23, F1 = 28\n  - SciIE: P = 40.4, R = 21.2, F1 = 27.8\n\n- **Overall**\n  - Best SemEval: P = 44, R = 41, F1 = 43\n  - SciIE: P = 48.1, R = 41.8, F1 = 44.7\n\nThe SciIE model generally shows better performance in terms of precision, recall, and F1 scores across the tasks compared to the other models.\ntracted relations to deﬁne ground truth labels. Each domain expert is assigned 2 or 3 entities and all of the corresponding relations. Figure  8  shows preci- sion/recall curves for both systems. Since it is not feasible to compute the actual recall of the systems, we compute the pseudo-recall ( Zhang et al. ,  2015 ) based on the output of both systems. We observe that the knowledge graph curve with coreference linking is mostly above the curve without corefer- ence linking. The precision of both systems is high (above   $84\\%$   for both systems), but the system with coreference links has signiﬁcantly higher recall. \n8 Conclusion \nIn this paper, we create a new dataset and develop a multi-task model for identifying entities, relations, and coreference clusters in scientiﬁc articles. By sharing span representations and leveraging cross- sentence information, our multi-task setup effec- tively improves performance across all tasks. More- over, we show that our multi-task model is better at predicting span boundaries and outperforms previ- ous state-of-the-art scientiﬁc IE systems on entity and relation extraction, without using any hand- engineered features or pipeline processing. Using our model, we are able to automatically organize the extracted information from a large collection of scientiﬁc articles into a knowledge graph. Our analysis shows the importance of coreference links in making a dense, useful graph. \nWe still observe a large gap between the perfor- mance of our model and human performance, con- ﬁrming the challenges of scientiﬁc IE. Future work includes improving the performance using semi- supervised techniques and providing in-domain features. We also plan to extend our multi-task framework to information extraction tasks in other domains. \nAcknowledgments \nThis research was supported by the Ofﬁce of Naval Research under the MURI grant N00014-18-1- 2670, NSF (IIS 1616112, III 1703166), Allen Dis- tinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Waleed Ammar and AI2 for sharing the Semantic Scholar Corpus. We also thank the anonymous reviewers, UW-NLP group and Shoou-I Yu for their helpful comments. \n\nReferences \nAmjad Abu-Jbara and Dragomir Radev. 2011. Co- herent citation-based summarization of scientiﬁc pa- pers. In  Proc. Annual Meeting of the Association for Computational Linguistics: Human Language Tech- nologies . volume 1, pages 500–509. \nHeike Adel and Hinrich Sch¨ utze. 2017. Global normal- ization of convolutional neural networks for joint en- tity and relation classiﬁcation. In  Proc. Conf. Empir- ical Methods Natural Language Process. (EMNLP) . pages 1723–1729. \nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat- ula, Iz Beltagy, Miles Crawford, Doug Downey, Ja- son Dunkelberger, Ahmed Elgohary, Sergey Feld- man, Vu Ha, et al. 2018. Construction of the litera- ture graph in semantic scholar. In  Proc. Conf. North American Assoc. for Computational Linguistics: Hu- man Language Technologies (NAACL-HLT), (Indus- try Papers) . pages 84–91. \nWaleed Ammar, Matthew Peters, Chandra Bhagavat- ula, and Russell Power. 2017. The ai2 system at semeval-2017 task 10 (scienceie): semi-supervised end-to-end entity and relation extraction. In  Proc. Int. Workshop on Semantic Evaluation (SemEval) . pages 592–596. Ashton Anderson, Dan McFarland, and Dan Jurafsky. 2012. Towards a computational history of the ACL: 1980-2008. In  Proc. ACL Special Workshop on Re- discovering 50 Years of Discoveries . pages 13–21. Awais Athar and Simone Teufel. 2012a. Context- enhanced citation sentiment detection. In  Proc. Conf. North American Assoc. for Computational Lin- guistics: Human Language Technologies (NAACL- HLT) . pages 597–601. Awais Athar and Simone Teufel. 2012b. Detection of implicit citations for sentiment detection. In  Proc. "}
{"page": 9, "image_path": "doc_images/D18-1360_9.jpg", "ocr_text": "ACL Workshop on Detecting Structure in Scholarly\nDiscourse. pages 18-26.\n\nIsabelle Augenstein, Mrinal Das, Sebastian Riedel,\nLakshmi Vikraman, and Andrew McCallum. 2017.\nSemeval 2017 task 10: ScienceIE - extracting\nkeyphrases and relations from scientific publications.\nIn Proc. Int. Workshop on Semantic Evaluation (Se-\nmEval).\n\nIsabelle Augenstein and Anders Sggaard. 2017. Multi-\ntask learning of keyphrase boundary classification.\nIn Proc. Annu. Meeting Assoc. for Computational\nLinguistics (ACL). pages 341-346.\n\nKevin Clark and Christopher D. Manning. 2016.\nImproving coreference resolution by learning\nentity-level distributed representations. CoRR\nabs/1606.01323.\n\nRonan Collobert and Jason Weston. 2008. A unified\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Proc.\nInt. Conf: Machine Learning (ICML). pages 160-\n167.\n\nHuy Hoang Nhat Do, Muthu Kumar Chandrasekaran,\nPhilip S Cho, and Min Yen Kan. 2013. Extracting\nand matching authors and affiliations in scholarly\ndocuments. In Proc. ACM/IEEE-CS Joint Confer-\nence on Digital libraries. pages 219-228.\n\nKata Gabor, Davide Buscaldi, Anne-Kathrin Schu-\nmann, Behrang QasemiZadeh, Haifa Zargayouna,\nand Thierry Charnois. 2018. Semeval-2018 Task 7:\nSemantic relation extraction and classification in sci-\nentific papers. In Proc. Int. Workshop on Semantic\nEvaluation (SemEval).\n\nKata Gabor, Haifa Zargayouna, Davide Buscaldi, Is-\nabelle Tellier, and Thierry Charnois. 2016. Se-\nmantic annotation of the ACL anthology corpus for\nthe automatic analysis of scientific literature. In\nProc. Language Resources and Evaluation Confer-\nence (LREC).\n\nKata Gabor, Haifa Zargayouna, Isabelle Tellier, Davide\nBuscaldi, and Thierry Charnois. 2016. Unsuper-\nvised relation extraction in specialized corpora using\nsequence mining. In International Symposium on In-\ntelligent Data Analysis. Springer, pages 237-248.\n\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Proc. Annu. Conf. Neural In-\nform. Process. Syst. (NIPS).\n\nSonal Gupta and Christopher D Manning. 2011. An-\nalyzing the dynamics of research by extracting key\naspects of scientific papers. In Proc. IJCNLP. pages\n1-9.\n\nLuheng He, Kenton Lee, Omer Levy, and Luke Zettle-\nmoyer. 2018. Jointly predicting predicates and argu-\nments in neural semantic role labeling. In ACL.\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation 9(8):1735-\n1780.\n\nKokil Jaidka, Muthu Kumar Chandrasekaran, Beat-\nriz Fisas Elizalde, Rahul Jha, Christopher Jones,\nMin-Yen Kan, Ankur Khanna, Diego Molla-Aliod,\nDragomir R Radev, Francesco Ronzano, et al. 2014.\nThe computational linguistics summarization pilot\ntask. In Proc. Text Analysis Conference.\n\nMiray Kas. 2011. Structures and statistics of citation\nnetworks. Technical report, DTIC Document.\n\nArzoo Katiyar and Claire Cardie. 2017. Going out\non a limb: Joint extraction of entity mentions\nand relations without dependency trees. In Proc.\nAnnu. Meeting Assoc. for Computational Linguistics\n(ACL). volume 1, pages 917-928.\n\nSigrid Klerke, Yoav Goldberg, and Anders Sggaard.\n2016. Improving sentence compression by learning\nto predict gaze. In HLT-NAACL.\n\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proc. Conf. North American Assoc. for Compu-\ntational Linguistics (NAACL).\n\nKenton Lee, Luheng He, Mike Lewis, and Luke S.\nZettlemoyer. 2017. End-to-end neural coreference\nresolution. In EMNLP.\n\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nfine inference. In NAACL.\n\nYi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao,\nand Michel Galley. 2017a. Multi-task learning for\nspeaker-role adaptation in neural conversation mod-\nels. In Proc. IJCNLP.\n\nYi Luan, Yangfeng Ji, Hannaneh Hajishirzi, and\nBoyang Li. 2016. Multiplicative representations\nfor unsupervised semantic role induction. In Proc.\nAnnu. Meeting Assoc. for Computational Linguistics\n(ACL). page 118.\n\nYi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.\n2017b. Scientific information extraction with semi-\nsupervised neural tagging. In Proc. Conf. Empirical\nMethods Natural Language Process. (EMNLP).\n\nYi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.\n2018. The uwnlp system at semeval-2018 task 7:\nNeural relation extraction model with selectively in-\ncorporated concept embeddings. In Proc. Int. Work-\nshop on Semantic Evaluation (SemEval). pages 788—\n792.\n\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using Istms on sequences and tree\nstructures. In Proc. Annu. Meeting Assoc. for Com-\nputational Linguistics (ACL). pages 1105-1116.\n\n3228\n", "vlm_text": "ACL Workshop on Detecting Structure in Scholarly Discourse . pages 18–26. \nIsabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum. 2017. Semeval 2017 task 10: ScienceIE - extracting keyphrases and relations from scientiﬁc publications. In  Proc. Int. Workshop on Semantic Evaluation (Se- mEval) . Isabelle Augenstein and Anders Søgaard. 2017. Multi- task learning of keyphrase boundary classiﬁcation. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . pages 341–346. Kevin Clark and Christopher D. Manning. 2016. Improving coreference resolution by learning entity-level distributed representations. CoRR abs/1606.01323. Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In  Proc. Int. Conf. Machine Learning (ICML) . pages 160– 167. Huy Hoang Nhat Do, Muthu Kumar Chandrasekaran, Philip S Cho, and Min Yen Kan. 2013. Extracting and matching authors and afﬁliations in scholarly documents. In  Proc. ACM/IEEE-CS Joint Confer- ence on Digital libraries . pages 219–228. Kata G´ abor, Davide Buscaldi, Anne-Kathrin Schu- mann, Behrang QasemiZadeh, Ha¨ ıfa Zargayouna, and Thierry Charnois. 2018. Semeval-2018 Task 7: Semantic relation extraction and classiﬁcation in sci- entiﬁc papers. In  Proc. Int. Workshop on Semantic Evaluation (SemEval) . Kata Gabor, Haifa Zargayouna, Davide Buscaldi, Is- abelle Tellier, and Thierry Charnois. 2016. Se- mantic annotation of the ACL anthology corpus for the automatic analysis of scientiﬁc literature. In Proc. Language Resources and Evaluation Confer- ence (LREC) . Kata G´ abor, Ha¨ ıfa Zargayouna, Isabelle Tellier, Davide Buscaldi, and Thierry Charnois. 2016. Unsuper- vised relation extraction in specialized corpora using sequence mining. In  International Symposium on In- telligent Data Analysis . Springer, pages 237–248. Yarin Gal and Zoubin Ghahramani. 2016. A theoret- ically grounded application of dropout in recurrent neural networks. In  Proc. Annu. Conf. Neural In- form. Process. Syst. (NIPS) . Sonal Gupta and Christopher D Manning. 2011. An- alyzing the dynamics of research by extracting key aspects of scientiﬁc papers. In  Proc. IJCNLP . pages 1–9. Luheng He, Kenton Lee, Omer Levy, and Luke Zettle- moyer. 2018. Jointly predicting predicates and argu- ments in neural semantic role labeling. In  ACL . \nSepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory.  Neural computation  9(8):1735– 1780. Kokil Jaidka, Muthu Kumar Chandrasekaran, Beat- riz Fisas Elizalde, Rahul Jha, Christopher Jones, Min-Yen Kan, Ankur Khanna, Diego Molla-Aliod, Dragomir R Radev, Francesco Ronzano, et al. 2014. The computational linguistics summarization pilot task. In  Proc. Text Analysis Conference . Miray Kas. 2011. Structures and statistics of citation networks. Technical report, DTIC Document. Arzoo Katiyar and Claire Cardie. 2017. Going out on a limb: Joint extraction of entity mentions and relations without dependency trees. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . volume 1, pages 917–928. Sigrid Klerke, Yoav Goldberg, and Anders Søgaard. 2016. Improving sentence compression by learning to predict gaze. In  HLT-NAACL . Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In  Proc. Conf. North American Assoc. for Compu- tational Linguistics (NAACL) . Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. End-to-end neural coreference resolution. In  EMNLP . Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to- ﬁne inference. In  NAACL . Yi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao, and Michel Galley. 2017a. Multi-task learning for speaker-role adaptation in neural conversation mod- els. In  Proc. IJCNLP . Yi Luan, Yangfeng Ji, Hannaneh Hajishirzi, and Boyang Li. 2016. Multiplicative representations for unsupervised semantic role induction. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . page 118. Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2017b. Scientiﬁc information extraction with semi- supervised neural tagging. In  Proc. Conf. Empirical Methods Natural Language Process. (EMNLP) . Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. The uwnlp system at semeval-2018 task 7: Neural relation extraction model with selectively in- corporated concept embeddings. In  Proc. Int. Work- shop on Semantic Evaluation (SemEval) . pages 788– 792. Makoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using lstms on sequences and tree structures. In  Proc. Annu. Meeting Assoc. for Com- putational Linguistics (ACL) . pages 1105–1116. "}
{"page": 10, "image_path": "doc_images/D18-1360_10.jpg", "ocr_text": "Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina\nToutanova, and Wen-tau Yih. 2017. Cross-sentence\nn-ary relation extraction with graph Istms. Trans.\nAssoc. for Computational Linguistics (TACL) 5:101—\n115.\n\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proc. Annu. Meeting Assoc. for Computational\nLinguistics (ACL). volume 1, pages 1756-1765.\n\nMatthew E. Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\n\nBehrang QasemiZadeh and Anne-Kathrin Schumann.\n2016. The ACL RD-TEC 2.0: A language resource\nfor evaluating term extraction and entity recognition\nmethods. In LREC.\n\nChris Quirk and Hoifung Poon. 2017. Distant su-\npervision for relation extraction beyond the sen-\ntence boundary. In Proc. European Chapter Assoc.\nfor Computational Linguistics (EACL). pages 1171-\n1182.\n\nMarek Rei. 2017. Semi-supervised multitask learning\nfor sequence labeling. In Proc. Annu. Meeting As-\nsoc. for Computational Linguistics (ACL).\n\nYanchuan Sim, Noah A Smith, and David A Smith.\n2012. Discovering factions in the computational lin-\nguistics community. In Proc. ACL Special Workshop\non Rediscovering 50 Years of Discoveries. pages 22—\n32.\n\nSameer Singh, Sebastian Riedel, Brian Martin, Jiaping\nZheng, and Andrew McCallum. 2013. Joint infer-\nence of entities, relations, and coreference. In Proc.\nof the 2013 workshop on Automated knowledge base\nconstruction. ACM, pages 1-6.\n\nPontus Stenetorp, Sampo Pyysalo, Goran Topié,\nTomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-\njii. 2012. Brat: a web-based tool for nlp-assisted\ntext annotation. In Proc. European Chapter Assoc.\nfor Computational Linguistics (EACL). pages 102—\n107.\n\nSwabha Swayamdipta, Sam Thomson, Chris Dyer, and\nNoah A. Smith. 2017. Frame-semantic parsing with\nsoftmax-margin segmental mns and a syntactic scaf-\nfold. CoRR abs/1706.09528.\n\nChen-Tse Tsai, Gourab Kundu, and Dan Roth. 2013.\nConcept-based analysis of scientific literature. In\nProc. ACM Int. Conference on Information & Knowl-\nedge Management. ACM, pages 1733-1738.\n\nAdam Vogel and Dan Jurafsky. 2012. He said, she said:\nGender in the ACL anthology. In Proc. ACL Special\nWorkshop on Rediscovering 50 Years of Discoveries.\npages 33-41.\n\nSam Wiseman, Alexander M. Rush, and Stuart M.\nShieber. 2016. Learning global features for coref-\nerence resolution. In HLT-NAACL.\n\nYan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen,\nYangyang Lu, and Zhi Jin. 2016. Improved rela-\ntion classification by deep recurrent neural networks\nwith data augmentation. In Proc. Int. Conf. Compu-\ntational Linguistics (COLING). pages 1461-1470.\n\nCongle Zhang, Stephen Soderland, and Daniel S. Weld.\n2015. Exploiting parallel news streams for unsuper-\nvised event extraction. TACL 3:117-129.\n\nMeishan Zhang, Yue Zhang, and Guohong Fu. 2017.\nEnd-to-end neural relation extraction with global op-\ntimization. In Proc. Conf. Empirical Methods Natu-\nral Language Process. (EMNLP). pages 1730-1740.\n\nSuncong Zheng, Feng Wang, Hongyun Bao, Yuexing\nHao, Peng Zhou, and Bo Xu. 2017. Joint extrac-\ntion of entities and relations based on a novel tag-\nging scheme. In Proc. Annu. Meeting Assoc. for\nComputational Linguistics (ACL). volume 1, pages\n1227-1236.\n\n3229\n", "vlm_text": "Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph lstms. Trans. Assoc. for Computational Linguistics (TACL)  5:101– 115. Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . volume 1, pages 1756–1765. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In  NAACL . Behrang QasemiZadeh and Anne-Kathrin Schumann. 2016. The ACL RD-TEC 2.0: A language resource for evaluating term extraction and entity recognition methods. In  LREC . Chris Quirk and Hoifung Poon. 2017. Distant su- pervision for relation extraction beyond the sen- tence boundary. In  Proc. European Chapter Assoc. for Computational Linguistics (EACL) . pages 1171– 1182. Marek Rei. 2017. Semi-supervised multitask learning for sequence labeling. In  Proc. Annu. Meeting As- soc. for Computational Linguistics (ACL) . Yanchuan Sim, Noah A Smith, and David A Smith. 2012. Discovering factions in the computational lin- guistics community. In  Proc. ACL Special Workshop on Rediscovering 50 Years of Discoveries . pages 22– 32. Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint infer- ence of entities, relations, and coreference. In  Proc. of the 2013 workshop on Automated knowledge base construction . ACM, pages 1–6. Pontus Stenetorp, Sampo Pyysalo, Goran Topi´ c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu- jii. 2012. Brat: a web-based tool for nlp-assisted text annotation. In  Proc. European Chapter Assoc. for Computational Linguistics (EACL) . pages 102– 107. Swabha Swayamdipta, Sam Thomson, Chris Dyer, and Noah A. Smith. 2017. Frame-semantic parsing with softmax-margin segmental rnns and a syntactic scaf- fold.  CoRR  abs/1706.09528. Chen-Tse Tsai, Gourab Kundu, and Dan Roth. 2013. Concept-based analysis of scientiﬁc literature. In Proc. ACM Int. Conference on Information & Knowl- edge Management . ACM, pages 1733–1738. Adam Vogel and Dan Jurafsky. 2012. He said, she said: Gender in the ACL anthology. In  Proc. ACL Special Workshop on Rediscovering 50 Years of Discoveries . pages 33–41. \nSam Wiseman, Alexander M. Rush, and Stuart M. Shieber. 2016. Learning global features for coref- erence resolution. In  HLT-NAACL . Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin. 2016. Improved rela- tion classiﬁcation by deep recurrent neural networks with data augmentation. In  Proc. Int. Conf. Compu- tational Linguistics (COLING) . pages 1461–1470. Congle Zhang, Stephen Soderland, and Daniel S. Weld. 2015. Exploiting parallel news streams for unsuper- vised event extraction.  TACL  3:117–129. Meishan Zhang, Yue Zhang, and Guohong Fu. 2017. End-to-end neural relation extraction with global op- timization. In  Proc. Conf. Empirical Methods Natu- ral Language Process. (EMNLP) . pages 1730–1740. Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint extrac- tion of entities and relations based on a novel tag- ging scheme. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . volume 1, pages 1227–1236. "}
{"page": 11, "image_path": "doc_images/D18-1360_11.jpg", "ocr_text": "A Annotation Guideline\n\nA.1_ Entity Category\n\nTask: Applications, problems to solve, sys-\ntems to construct.\n\nE.g. information extraction, machine reading\nsystem, image segmentation, etc.\n\nMethod: Methods , models, systems to use,\nor tools, components of a system, frameworks.\n\nE.g. language model, CORENLP, POS parser,\nkernel method, etc.\n\nEvaluation Metric: Metrics, measures, or\nentities that can express quality of a sys-\nem/method.\n\nE.g. Fl, BLEU, Precision, Recall, ROC curve,\nmean reciprocal rank, mean-squared error, ro-\nbustness, time complexity, etc.\n\nMaterial: Data, datasets, resources, Corpus,\nKnowledge base.\n\nE.g. image data, speech data, stereo images,\nbilingual dictionary, paraphrased questions,\nCoNLL, Panntreebank, WordNet, Wikipedia,\netc.\n\nEvaluation Metric: Metric measure or term\nthat can express quality of a system/method.\nE.g. Fl, BLEU, Precision, Recall, ROC\ncurve, mean reciprocal rank, mean-squared\nerror,robustness, compile time, time complex-\nity...\n\nGeneric: General terms or pronouns that may\nrefer to a entity but are not themselves infor-\nmative, often used as connection words.\n\nE.g model, approach, prior knowledge, them,\nit...\n\nOur method models user proficiency.\nOur algorithms exploits local soothness.\n\nFeature-of: B belongs to A, B is a feature of\nA, B is under A domain. E.g.\n\nprior knowledge of the model\ngenre-specific regularities of discourse\nstructure\n\nEnglish text in science domain\n\nHyponym-of: B is a hyponym of A, B is a\ntype of A. E.g.\nTUIT is a software library\n\nNLP applications such as machine trans-\nlation and language generation\n\nPart-of: B is a part of A... E.g.\n\nThe system includes two models: speech\nrecognition and natural language under-\nstanding\n\nWe incorporate NLU module to the sys-\ntem.\n\nCompare: Symmetric relation (use blue to\ndenote entity). Opposite of conjunction, com-\npare two models/methods, or listing two op-\nposing entities. E.g.\n\nUnlike the quantitative prior, the qualita-\ntive prior is often ignored...\n\nWe compare our system with previous\nsequential tagging systems...\n\nConjunction: Symmetric relation (use blue\nto denote entity). Function as similar role or\nuse/incorporate with. E.g.\n\nobtained from human expert or knowl-\nedge base\n\nNLP applications such as machine trans-\nlation and language generation\n\nA.2 Relation Category A.3 Coreference\n\nRelation link can not go beyond sentence boundary.\nWe define 4 asymmetric relation types (Used-for,\nFeature-of, Hyponym-of, Part-of ), together with 2\nsymmetric relation types (Compare, Conjunction).\nB always points to A for asymmetric relations\n\nTwo Entities that points to the same concept.\ne Anaphora and Cataphora:\n\nWe introduce a machine reading system...\nThe system...\nThe prior knowledge include...Such\n\ne Used-for: B is used for A, B models A, A is knowledge can be applied to...\n\ntrained on B, B exploits A, A is based on B.\nEg. e Coreferring noun phrase:\n\nThe TISPER system has been designed\nto enable many text applications.\n\nWe develop a part-of-speech tagging sys-\ntem...The POS tagger...\n\n3230\n", "vlm_text": "A Annotation Guideline \nA.1 Entity Category \n•  Task : Applications, problems to solve, sys- tems to construct. E.g. information extraction, machine reading system, image segmentation, etc. •  Method : Methods , models, systems to use, or tools, components of a system, frameworks. E.g. language model, CORENLP, POS parser, kernel method, etc. •  Evaluation Metric : Metrics, measures, or entities that can express quality of a sys- tem/method. E.g. F1, BLEU, Precision, Recall, ROC curve, mean reciprocal rank, mean-squared error, ro- bustness, time complexity, etc. •  Material : Data, datasets, resources, Corpus, Knowledge base. E.g. image data, speech data, stereo images, bilingual dictionary, paraphrased questions, CoNLL, Panntreebank, WordNet, Wikipedia, etc. •  Evaluation Metric : Metric measure or term that can express quality of a system/method. E.g. F1, BLEU, Precision, Recall, ROC curve, mean reciprocal rank, mean-squared error,robustness, compile time, time complex- ity... •  Generic : General terms or pronouns that may refer to a entity but are not themselves infor- mative, often used as connection words. E.g model, approach, prior knowledge, them, it... \nA.2 Relation Category \nRelation link can not go beyond sentence boundary. We deﬁne 4 asymmetric relation types ( Used-for , Feature-of ,  Hyponym-of ,  Part-of ), together with 2 symmetric relation types ( Compare ,  Conjunction ). B  always points to  A  for asymmetric relations \n•  Used-for :  B  is used for  A ,  B  models  A ,  A  is trained on  B ,  B  exploits  A ,  A  is based on  B . E.g. The  TISPER system  has been designed to enable many  text applications . \nOur  method  models  user proﬁciency . Our  algorithms  exploits  local soothness . \n•  Feature-of :  B  belongs to  A ,  B  is a feature of A ,  B  is under  A  domain. E.g. prior knowledge  of the  model genre-speciﬁc regularities  of  discourse structure English text  in  science domain \n•  Hyponym-of :  B  is a hyponym of  A ,  B  is a type of  A . E.g. TUIT  is a  software library NLP applications  such as  machine trans- lation  and  language generation \n•  Part-of :  B  is a part of  A ... E.g. The  system  includes two models:  speech recognition  and  natural language under- standing We incorporate  NLU module  to the  sys- tem . \n•  Compare : Symmetric relation (use blue to denote entity). Opposite of conjunction, com- pare two models/methods, or listing two op- posing entities. E.g. Unlike the  quantitative prior , the  qualita- tive prior  is often ignored... We compare our  system  with previous sequential tagging systems ... \n•  Conjunction : Symmetric relation (use blue to denote entity). Function as similar role or use/incorporate with. E.g. obtained from  human expert  or  knowl- edge base NLP applications such as  machine trans- lation  and  language generation \nA.3 Coreference \nTwo Entities that points to the same concept.  Anaphora and Cataphora : \n\nWe introduce a  machine reading system ... The  system ... The  prior knowledge  include...Such knowledge  can be applied to... \n•  Coreferring noun phrase : \nWe develop a  part-of-speech tagging sys- tem ...The  POS tagger ... "}
{"page": 12, "image_path": "doc_images/D18-1360_12.jpg", "ocr_text": "A.4 Notes\n1. Entity boundary annotation follows the\nACL RD-TEC Annotation Guideline (Qasem-\niZadeh and Schumann, 2016), with the exten-\ntion that spans can be embedded in longer\nspans, only if the shorter span is involved in a\nrelation.\n\n2. Do not include determinators (such as the, a),\nor adjective pronouns (such as this, its, these,\nsuch) to the span. If generic phrases are not\ninvolved in a relation, do not tag them.\n\n3. Do not tag relation if one entity is:\n\ne Variable bound:\nWe introduce a neural based approach..\nIts benefit is...\n\ne The word which:\nWe introduce a neural based approach,\nwhich is a...\n\n4. Do not tag coreference if the entity is\n\ne Generically-used Other-ScientificTerm:\n..advantage gained from local smooth-\nness which... We present algorithms ex-\nploiting local smoothness in more aggres-\nsive ways...\n\ne Same scientific term but refer to different\nexamples:\n\nWe use a data structure, we also use an-\nother data structure...\n\n5. Do not label negative relations:\nX is not used in Y or X is hard to be applied\nin Y\nB_ Annotation and Knowledge Graph\nExamples\n\nHere we take a screen shot of the BRAT interface\nfor an ACL paper in Figure 9. We also attach the\noriginal figure of Figure 3 in Figure 10. More\nexamples can be found in the project website*.\n\n‘http: //nlp.cs.washington.edu/scilE/\n\n3231\n", "vlm_text": "A.4 Notes \n1.  Entity boundary annotation follows the ACL RD-TEC Annotation Guideline ( Qasem- iZadeh and Schumann ,  2016 ), with the exten- tion that spans can be embedded in longer spans, only if the shorter span is involved in a relation. \n2.  Do not include determinators (such as the, a), or adjective pronouns (such as this,its, these, such) to the span. If generic phrases are not involved in a relation, do not tag them. \n3. Do not tag relation if one entity is: \n•  Variable bound: We introduce a neural based approach.. Its  beneﬁt is... •  The word  which : We introduce a neural based approach, which  is a... \n4. Do not tag coreference if the entity is \n•  Generically-used Other-ScientiﬁcTerm: ...advantage gained from  local smooth- ness  which... We present algorithms ex- ploiting  local smoothness  in more aggres- sive ways... •  Same scientiﬁc term but refer to different examples: We use a  data structure , we also use an- other  data structure ... \n5. Do not label negative relations: \n $\\mathrm{X}$   is not used in   $\\mathrm{Y}$   or   $\\mathrm{X}$   is hard to be applied in Y \nB Annotation and Knowledge Graph Examples \nHere we take a screen shot of the BRAT interface for an ACL paper in Figure  9 . We also attach the original ﬁgure of Figure 3 in Figure  10 . More examples can be found in the project website 4 . "}
{"page": 13, "image_path": "doc_images/D18-1360_13.jpg", "ocr_text": "-COREF:\n\nxy vot yy “pp go\n1 Methods developed for spelling correction for — languages like English\ncone\n\n(see the review by Kukich (Kukich, 1992)) are not\n\nreadily applicable to agglutinative languages .\n-COREF-\n\nCORE:\n-COREF.\n\n'USED-FOR\nUSED-FOR\nUSED-FOR: USED-FOR— gtr ercentificTerm) > \\oatieesetentinrern}——— conuncrion +\n\n2 This poster presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a\n\ncance\n\n-COREF-\ndynamic-programming based search algorithm: .\n\n-COREF:\n———cone\n= USED FOR —~other-ScientificTerm)\n\ninformation retrieval o © parser\n© Uses\nspeech synthesis © Conjunction ©\n3st\nsg cation O oe\nclassification «’ oo\ngs\nsg08 © ss\nsy 12008\nspor\" °\nsoo\noa\npon\n\nParaphrasing\nuone|suen ©\n\nFigure 10: An example of our automatically generated knowledge graph centered on statistical machine\ntranslation. This is the original figure of Figure 4.\n\n3232\n", "vlm_text": "The image is an annotated linguistic example from the ACL (Association for Computational Linguistics). It displays sentences with labels and connections indicating relationships between different parts of the text. Annotations include:\n\n- \"Generic,\" \"Task,\" and \"Other-ScientificTerm\" labels for different phrases.\n- \"USED-FOR,\" \"HYPONYM-OF,\" and \"CONJUNCTION\" indicating the relationships between terms.\n- \"COREF\" lines showing coreference, linking phrases referring to the same entity.\n\nThe sentences discuss methods for spelling correction, particularly in agglutinative languages, using approaches based on morphology and algorithms.\nThis image is a mind map centered around \"statistical machine translation.\" It links to various concepts and categories, including:\n\n- **Evaluated by**: Includes BLEU, ROUGE, METEOR, word error rate, and perplexity.\n- **Uses**: Lists segmentation, maximum entropy, decoder, word sense disambiguation, word alignment, discriminative training, log-linear model, domain adaptation, stochastic local search, translation model, parser, n-gram language model, topic model, word segmentation, discriminative model, recurrent neural network, decoding, adaptation, and neural network.\n- **Used for**: Includes semantic parsing, retrieval, speech translation, grammatical error correction, hybrid system, search, paraphrasing, translation, and alignment.\n- **Conjunction**: Links to neural machine translation, machine translation, answer set programming, information retrieval, speech synthesis, classification, speech recognition, and automatic speech recognition.\n\nThe map shows relationships and connections between different terms and techniques related to statistical machine translation.\nFigure 10 : An example of our automatically generated knowledge graph centered on  statistical machine translation . This is the original ﬁgure of Figure  4 . "}
