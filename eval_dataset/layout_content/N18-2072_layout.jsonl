{"layout": 0, "type": "text", "text": "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations ", "text_level": 1, "page_idx": 0, "bbox": [117, 68, 481, 103], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Sosuke Kobayashi Preferred Networks, Inc., Japan sosk@preferred.jp ", "page_idx": 0, "bbox": [224.63900756835938, 128.530029296875, 375.89617919921875, 170.7364044189453], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [158, 223, 204, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "We propose a novel data augmentation for labeled sentences called  contextual augmen- tation . We assume an invariance that sen- tences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically re- place words with other words that are pre- dicted by a bi-directional language model at the word positions. Words predicted accord- ing to a context are numerous but appropri- ate for the augmentation of the original words. Furthermore, we retroﬁt a language model with a label-conditional architecture, which al- lows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text clas- siﬁcation tasks, we demonstrate that the pro- posed method improves classiﬁers based on the convolutional or recurrent neural networks. ", "page_idx": 0, "bbox": [89, 247.64056396484375, 273, 474.83843994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [71, 486, 155, 500], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "Neural network-based models for NLP have been growing with state-of-the-art results in various tasks, e.g., dependency parsing ( Dyer et al. ,  2015 ), text classiﬁcation ( Socher et al. ,  2013 ;  Kim ,  2014 ), machine translation ( Sutskever et al. ,  2014 ). How- ever, machine learning models often overﬁt the training data by losing their generalization. Gener- alization performance highly depends on the size and quality of the training data and regulariza- tions. Preparing a large annotated dataset is very time-consuming. Instead, automatic data augmen- tation is popular, particularly in the areas of vi- sion ( Simard et al. ,  1998 ;  Krizhevsky et al. ,  2012 ; Szegedy et al. ,  2015 ) and speech ( Jaitly and Hin- ton ,  2015 ;  Ko et al. ,  2015 ). Data augmentation is basically performed based on human knowledge on invariances, rules, or heuristics, e.g., “even if a picture is ﬂipped, the class of an object should be unchanged”. ", "page_idx": 0, "bbox": [72, 508.99896240234375, 290, 766.0303955078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "image", "page_idx": 0, "img_path": "layout_images/N18-2072_0.jpg", "img_caption": "Figure 1: Contextual augmentation with a bi- directional RNN language model, when a sentence “the actors are fantastic”  is augmented by replacing only  actors  with words predicted based on the context. ", "bbox": [306, 220, 527, 495], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "the performances are fantastic\nthe films are fantastic\nthe movies are fantastic\nthe stories are fantastic\n\nperformances\nfilms\nmovies\nstories\n\nthe\n\n", "vlm_text": "The image illustrates the process of contextual augmentation using a bi-directional RNN language model. It begins with the sentence “the actors are fantastic.” The word \"actors\" is highlighted and is replaced with words like \"performances,\" \"films,\" \"movies,\" and \"stories,\" which are contextually predicted by the model. The sentences generated retain a positive sentiment label. The image visually shows how the RNN processes the context to predict suitable substitutions while maintaining the overall sentiment."}
{"layout": 7, "type": "text", "text": "However, usage of data augmentation for NLP has been limited. In natural languages, it is very difﬁcult to obtain universal rules for transforma- tions which assure the quality of the produced data and are easy to apply automatically in various do- mains. A common approach for such a transfor- mation is to replace words with their synonyms se- lected from a handcrafted ontology such as Word- Net ( Miller ,  1995 ;  Zhang et al. ,  2015 ) or word sim- ilarity calculation ( Wang and Yang ,  2015 ). Be- cause words having exactly or nearly the same meanings are very few, synonym-based augmen- tation can be applied to only a small percentage of the vocabulary. Other augmentation methods are known but are often developed for speciﬁc do- mains with handcrafted rules or pipelines, with the loss of generality. ", "page_idx": 0, "bbox": [307, 521.7429809570312, 525, 751.6754150390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "In this paper, we propose a novel data aug- mentation method called  contextual augmenta- tion . Our method offers a wider range of sub- stitute words by using words predicted by a bi- directional language model (LM) according to the context, as shown in Figure  1 . This contextual pre- diction suggests various words that have paradig- matic relations ( Saussure and Riedlinger ,  1916 ) with the original words. Such words can also be good substitutes for augmentation. Furthermore, to prevent word replacement that is incompatible with the annotated labels of the original sentences, we retroﬁt the LM with a label-conditional archi- tecture. Through the experiment, we demonstrate that the proposed conditional LM produces good words for augmentation, and contextual augmen- tation improves classiﬁers using recurrent or con- volutional neural networks (RNN or CNN) in var- ious classiﬁcation tasks. ", "page_idx": 0, "bbox": [318, 752.8849487304688, 525, 766.0303955078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "", "page_idx": 1, "bbox": [72, 63.68604278564453, 290, 307.1686096191406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "2 Proposed Method ", "text_level": 1, "page_idx": 1, "bbox": [70, 319, 182, 331], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "For performing data augmentation by replac- ing words in a text with other words, prior works ( Zhang et al. ,  2015 ;  Wang and Yang ,  2015 ) used synonyms as substitute words for the origi- nal words. However, synonyms are very limited and the synonym-based augmentation cannot pro- duce numerous different patterns from the origi- nal texts. We propose  contextual augmentation , a novel method to augment words with more varied words. Instead of the synonyms, we use words that are predicted by a LM given the context surround- ing the original words to be augmented, as shown in Figure  1 . ", "page_idx": 1, "bbox": [72, 341.1781005859375, 290, 516.9136352539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "2.1 Motivation ", "text_level": 1, "page_idx": 1, "bbox": [71, 528, 149, 540], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "First, we explain the motivation of our pro- posed method by referring to an example with a sentence from the Stanford Sentiment Treebank (SST) ( Socher et al. ,  2013 ), which is a dataset of sentiment-labeled movie reviews. The sentence, “the actors are fantastic.” , is annotated with a pos- itive label. When augmentation is performed for the word (position)  “actors” , how widely can we augment it? According to the prior works, we can use words from a synset for the word  actor  ob- tained from WordNet ( histrion, player, thespian, and  role player ). The synset contains words that have meanings similar to the word  actor  on aver- age.   However, for data augmentation, the word actors  can be further replaced with non-synonym words such as  characters, movies, stories,  and songs  or various other nouns, while retaining the positive sentiment and naturalness. Considering the generalization, training with maximum pat- terns will boost the model performance more. ", "page_idx": 1, "bbox": [72, 545.9011840820312, 290, 735.1864624023438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "", "page_idx": 1, "bbox": [307, 63.68604278564453, 526, 144.57754516601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "We propose using numerous words that have the paradigmatic relations with the original words. A LM has the desirable property to assign high prob- abilities to such words, even if the words them- selves are not similar to the original word to be replaced. ", "page_idx": 1, "bbox": [307, 145.6811065673828, 526, 226.57260131835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "2.2 Word Prediction based on Context ", "text_level": 1, "page_idx": 1, "bbox": [306, 239, 494, 251], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "For our proposed method, we requires a LM for calculating the word probability at a position    $i$  based on its context. The context is a sequence of words surrounding an original word    $w_{i}$   in a sen- tence    $S$  , i.e., cloze sentence    $S\\backslash\\{w_{i}\\}$  . The calcu- lated probability is    $p(\\cdot|S\\backslash\\{w_{i}\\})$  . Speciﬁcally, we use a bi-directional LSTM-RNN ( Hochreiter and Schmidhuber ,  1997 ) LM. For prediction at posi- tion    $i$  , the model encodes the surrounding words individually rightward and leftward (see Figure  1 ). As well as typical uni-directional RNN LMs, the outputs from adjacent positions are used for cal- culating the probability at target position    $i$  . The outputs from both the directions are concatenated and fed into the following feed-forward neural net- work, which produces words with a probability distribution over the vocabulary. ", "page_idx": 1, "bbox": [307, 257.09912109375, 526, 487.0317077636719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "In contextual augmentation, new substitutes for word    $w_{i}$   can be smoothly sampled from a given probability distribution,    $p(\\cdot|S\\backslash\\{w_{i}\\})$  , while prior works selected top-K words conclusively. In this study, we sample words for augmentation at each update during the training of a model. To control the strength of augmentation, we introduce tem- perature parameter    $\\tau$   and use an annealed distri- bution    $p_{\\tau}(\\cdot|S\\backslash\\{w_{i}\\})~\\propto~p(\\cdot|S\\backslash\\{w_{i}\\})^{1/\\tau}$  . If the temperature becomes inﬁnity   $(\\tau\\to\\infty)$  ), the words are sampled from a uniform distribution.   2   If it becomes zero   $(\\tau\\,\\rightarrow\\,0)$  , the augmentation words are always words predicted with the highest prob- ability. The sampled words can be obtained at one time at each word position in the sentences. We re- place each word simultaneously with a probability ", "page_idx": 1, "bbox": [307, 488.13421630859375, 526, 704.5176391601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "2.3 Conditional Constraint ", "text_level": 1, "page_idx": 2, "bbox": [71, 87, 205, 99], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "Finally, we introduce a novel approach to address the issue that context-aware augmentation is not always compatible with annotated labels. For un- derstanding the issue, again, consider the exam- ple,  “the actors are fantastic.” , which is annotated with a positive label. If contextual augmentation, as described so far, is simply performed for the word (position of)  fantastic , a LM often assigns high probabilities to words such as  bad  or  terrible as well as  good  or  entertaining , although they are mutually contradictory to the annotated labels of positive or negative. Thus, such a simple augmen- tation can possibly generate sentences that are im- plausible with respect to their original labels and harmful for model training. ", "page_idx": 2, "bbox": [72, 104.24805450439453, 290, 307.0826110839844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "To address this issue, we introduce a condi- tional constraint that controls the replacement of words to prevent the generated words from revers- ing the information related to the labels of the sen- tences. We alter a LM to a label-conditional LM, i.e., for position    $i$   in sentence    $S$   with label    $y$  , we aim to calculate  $p_{\\tau}(\\cdot|y,S\\backslash\\{w_{i}\\})$   instead of the de- fault  $p_{\\tau}(\\cdot|S\\rangle\\{w_{i}\\})$   within the model. Speciﬁcally, we concatenate each embedded label    $y$   with a hid- den layer of the feed-forward network in the bi- directional LM, so that the output is calculated from a mixture of information from both the label and context. ", "page_idx": 2, "bbox": [72, 307.6141357421875, 290, 483.3506774902344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "3 Experiment ", "text_level": 1, "page_idx": 2, "bbox": [71, 494, 151, 507], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "3.1 Settings ", "text_level": 1, "page_idx": 2, "bbox": [71, 516, 134, 528], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "We tested combinations of three augmentation methods for two types of neural models through six text classiﬁcation tasks. The corresponding code is implemented by Chainer ( Tokui et al. , 2015 ) and available   3 . ", "page_idx": 2, "bbox": [72, 533.378173828125, 290, 600.7196655273438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "The benchmark datasets used are as follows: (1, 2) SST is a dataset for sentiment classiﬁca- tion on movie reviews, which were annotated with ﬁve or two labels (SST5, SST2) ( Socher et al. , 2013 ). (3) Subjectivity dataset (Subj) was anno- tated with whether a sentence was subjective or objective ( Pang and Lee ,  2004 ). (4) MPQA is an opinion polarity detection dataset of short phrases rather than sentences ( Wiebe et al. ,  2005 ). (5) RT is another movie review sentiment dataset ( Pang and Lee ,  2005 ). (6) TREC is a dataset for clas- siﬁcation of the six question types (e.g., person, location) ( Li and Roth ,  2002 ). For a dataset with- out development data, we use   $10\\%$   of its training set for the validation set as well as Kim ( 2014 ). ", "page_idx": 2, "bbox": [72, 601.252197265625, 290, 736.3406372070312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "", "page_idx": 2, "bbox": [307, 63.68604278564453, 525, 131.02853393554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "We tested classiﬁers using the LSTM-RNN or CNN, and both have exhibited good performances. We used typical architectures of classiﬁers based on the LSTM or CNN with dropout using hyperpa- rameters found in preliminary experiments.   4   The reported accuracies of the models were averaged over eight models trained from different seeds. ", "page_idx": 2, "bbox": [307, 131.99708557128906, 525, 226.43759155273438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "The tested augmentation methods are: (1) synonym-based augmentation, and (2, 3) con- textual augmentation with or without a label- conditional architecture. The hyperparameters of the augmentation (temperature    $\\tau$   and probability of word replacement) were also selected by a grid- search using validation set, while retaining the hyperparameters of the models. For contextual augmentation, we ﬁrst pretrained a bi-directional LSTM LM without the label-conditional architec- ture, on WikiText-103 corpus ( Merity et al. ,  2017 ) from a subset of English Wikipedia articles. After the pretraining, the models are further trained on each labeled dataset with newly introduced label- conditional architectures. ", "page_idx": 2, "bbox": [307, 227.4051055908203, 525, 430.2396545410156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "3.2 Results ", "text_level": 1, "page_idx": 2, "bbox": [306, 442, 366, 454], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "Table  1  lists the accuracies of the models with or without augmentation. The results show that our contextual augmentation improves the model per- formances for various datasets from different do- mains more signiﬁcantly than the prior synonym- based augmentation does. Furthermore, our label- conditional architecture boosted the performances on average and achieved the best accuracies. Our methods are effective even for datasets with more than two types of labels, SST5 and TREC. ", "page_idx": 2, "bbox": [307, 460.03118896484375, 525, 595.1196899414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "table", "page_idx": 3, "img_path": "layout_images/N18-2072_1.jpg", "table_caption": "ModelsSTT5 STT2 Subj MPQA RT TREC Avg.", "table_footnote": "Table 1: Accuracies of the models for various bench- marks. The accuracies are averaged over eight models trained from different seeds. ", "bbox": [70, 61, 292, 201], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "92.4\n\nCNN 79:5 86.1\n\nw/ synonym] 40.7 80.0 92.4 86.3 76.0 89.6 |77.50\n\nw/ context | 41.9 80.9 92.7 86.7 75.9 90.0 |78.02\nt+label | 42.1 80.8 93.0 86.7 76.1 90.5 |78.20\n\nRNN 40.2 80.3 92.4 86.0 76.7 89.0 |77.43\n\nw/ synonym] 40.5 80.2 92.8 86.4 76.6 87.9 |77.40\n\nw/ context | 40.9 79.3 92.8 86.4 77.0 89.3 |77.62\n+label | 41.1 80.1 92.8 86.4 77.4 89.2 |77.83\n", "vlm_text": "The table presents the performance metrics of two types of models, CNN and RNN, along with various modifications, across different tasks or datasets. The columns in the table represent specific evaluation tasks or datasets: S1T5, S1T2, Subj. M, Qui. RP, True, and Avg., which likely denote specific metrics or datasets used in the experiments, though their exact meanings are not provided in the table. The rows show performance scores for each model setup:\n\n1. **CNN**:\n   - Baseline: Shows different performance scores across the tasks, resulting in an average score of 77.53.\n   - `w/ synonym`: Maintains relatively consistent scores with slight variations, averaging at 77.50.\n   - `w/ context`: Again, shows similar results, with an average of 78.02.\n   - `+ label`: This configuration yields the highest average score of 78.20 among the CNN variations.\n\n2. **RNN**:\n   - Baseline: Contains initial performance scores, averaging at 77.43.\n   - `w/ synonym`: Similar to its CNN counterpart, leading to an average score of 77.40.\n   - `w/ context`: Shows performance scores, resulting in an average of 77.62.\n   - `+ label`: This modification leads to the highest average score of 77.83 for RNN models.\n\nIn summary, the table evaluates the effectiveness of different modifications (using synonyms, context, and labels) for CNN and RNN models across various tasks/datasets, showing the average performance score for each configuration."}
{"layout": 32, "type": "text", "text": "For investigating our label-conditional bi- directional LM, we show in Figure  2  the top-10 word predictions by the model for a sentence from the SST dataset. Each word in the sentence is fre- quently replaced with various words that are not always synonyms. We present two types of pre- dictions depending on the label fed into the con- ditional LM. With a positive label, the word “fan- tastic” is frequently replaced with  funny, honest, good,  and  entertaining , which are also positive ex- pressions. In contrast, with a negative label, the word “fantastic” is frequently replaced with  tired, forgettable, bad , and  dull , which reﬂect a negative sentiment. At another position, the word “the” can be replaced with “no” (with the seventh highest probability), so that the whole sentence becomes “no actors are fantastic.”, which seems negative as a whole. Aside from such inversions caused by labels, the parts unrelated to the labels (e.g., “ac- tors”) are not very different in the positive or neg- ative predictions. These results also demonstrated that conditional architectures are effective. ", "page_idx": 3, "bbox": [72, 218.4960479736328, 290, 516.1746215820312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 3, "bbox": [71, 527, 163, 541], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "Some works tried text data augmentation by us- ing synonym lists ( Zhang et al. ,  2015 ;  Wang and Yang ,  2015 ), grammar induction ( Jia and Liang , 2016 ), task-speciﬁc heuristic rules ( F¨ urstenau and Lapata ,  2009 ;  Kaﬂe et al. ,  2017 ;  Silfver- berg et al. ,  2017 ), or neural decoders of au- toencoders ( Bergmanis et al. ,  2017 ;  Xu et al. , 2017 ;  Hu et al. ,  2017 ) or encoder-decoder mod- els ( Kim and Rush ,  2016 ;  Sennrich et al. ,  2016 ; Xia et al. ,  2017 ). The works most similar to our research are Kolomiyets et al. ( 2011 ) and Fadaee et al. ( 2017 ). In a task of time expression recog- nition, Kolomiyets et al. replaced only the head- words under a task-speciﬁc assumption that tem- poral trigger words usually occur as headwords. They selected substitute words with top-K scores ", "page_idx": 3, "bbox": [72, 549.6471557617188, 290, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "image", "page_idx": 3, "img_path": "layout_images/N18-2072_2.jpg", "img_caption": "Figure 2: Words predicted with the ten highest prob- abilities by the conditional bi-directional LM applied to the sentence  “the actors are fantastic” . The squares above the sentence list the words predicted with a pos- itive label. The squares below list the words predicted with a negative label. ", "bbox": [305, 61, 528, 353], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "his stories get hilarious\n\nother story have young\n\nall actors seem compelling\n\nits two feel enjoyable 7\nmost performances find engaging\nthose films be fun\nsome movies is entertaining | ) 5\nboth movie were good Sz\nthese film ‘re honest gs\n\ncharacters are funny a\n\n‘positive!\nthe actors are fantastic\nnegative\nthe characters ‘re tired 3s\nsome movie are n't 8S\nthese film were forgettable s g\nsuch plot seem bad\nits story feel good\nall films is dull\nno themes be unfunny 7\nhis movies find flat\n\nstories\nsongs\n\npretentious ©\nbland\n\n", "vlm_text": "The image is a diagram illustrating the predicted words with the ten highest probabilities from a conditional bi-directional language model applied to the sentence \"the actors are fantastic.\" \n\n- The top section lists words predicted with a positive sentiment:\n  - \"the\" - \"funny\" (in order of decreasing probability).\n\n- The bottom section lists words predicted with a negative sentiment:\n  - \"the\" - \"bland\" (in order of decreasing probability).\n\nProbability is represented vertically, with higher probabilities at the top for both positive and negative labels."}
{"layout": 36, "type": "text", "text": "given by the Latent Words LM ( Deschacht and Moens ,  2009 ), which is a LM based on ﬁxed- length contexts. Fadaee et al. ( 2017 ), focusing on the rare word problem in machine transla- tion, replaced words in a source sentence with only rare words, which both of rightward and left- ward LSTM LMs independently predict with top- K conﬁdences. A word in the translated sentence is also replaced using a word alignment method and a rightward LM. These two works share the idea of the usage of language models with our method. We used a bi-directional LSTM LM which captures variable-length contexts with con- sidering both the directions jointly. More impor- tantly, we proposed a label-conditional architec- ture and demonstrated its effect both qualitatively and quantitatively. Our method is independent of any task-speciﬁc knowledge, and effective for classiﬁcation tasks in various domains. ", "page_idx": 3, "bbox": [307, 383.71600341796875, 525, 640.74755859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "We use a label-conditional ﬁll-in-the-blank con- text for data augmentation. Neural models us- ing the ﬁll-in-the-blank context have been invested in other applications. Kobayashi et al.  ( 2016 , 2017 ) proposed to extract and organize informa- tion about each entity in a discourse using the con- text.  Fedus et al.  ( 2018 ) proposed GAN ( Goodfel- low et al. ,  2014 ) for text generation and demon- strated that the mode collapse and training insta- bility can be relieved by in-ﬁlling-task training. ", "page_idx": 3, "bbox": [307, 644.4910888671875, 525, 766.030517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "", "page_idx": 4, "bbox": [72, 63.68604278564453, 278.5212097167969, 76.83151245117188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 4, "bbox": [71, 87, 148, 100], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "We proposed a novel data augmentation using nu- merous words given by a bi-directional LM, and further introduced a label-conditional architecture into the LM. Experimentally, our method pro- duced various words compatibly with the labels of original texts and improved neural classiﬁers more than the synonym-based augmentation. Our method is independent of any task-speciﬁc knowl- edge or rules, and can be generally and easily used for classiﬁcation tasks in various domains. ", "page_idx": 4, "bbox": [72, 108.20307159423828, 290, 243.29159545898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "On the other hand, the improvement by our method is sometimes marginal. Future work will explore comparison and combination with other generalization methods exploiting datasets deeply as well as our method. ", "page_idx": 4, "bbox": [72, 243.6951446533203, 290, 311.0376281738281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 4, "bbox": [72, 322, 166, 334], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "I would like to thank the members of Preferred Networks, Inc., especially Takeru Miyato and Yuta Tsuboi, for helpful comments. I would also like to thank anonymous reviewers for helpful comments. ", "page_idx": 4, "bbox": [72, 342.4091491699219, 290, 396.20263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "References ", "text_level": 1, "page_idx": 4, "bbox": [71, 419, 128, 431], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015.  Scheduled sampling for se- quence prediction with recurrent neural networks . In  NIPS , pages 1171–1179. Toms Bergmanis, Katharina Kann, Hinrich Sch¨ utze, and Sharon Goldwater. 2017. Training data aug- mentation for low-resource morphological inﬂec- tion . In  CoNLL SIGMORPHON , pages 31–39. Koen Deschacht and Marie-Francine Moens. 2009. Semi-supervised semantic role labeling using the la- tent words language model . In  EMNLP , pages 21– 29. Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015.  Transition- based dependency parsing with stack long short- term memory . In  ACL , pages 334–343. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation . In  ACL , pages 567–573. William Fedus, Ian Goodfellow, and Andrew M. Dai. 2018.  MaskGAN: Better text generation via ﬁlling in the . In  ICLR . Hagen F¨ urstenau and Mirella Lapata. 2009. Semi- supervised semantic role labeling . In  EACL , pages 220–228. ", "page_idx": 4, "bbox": [72, 437.93072509765625, 290, 765.7645263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.  Generative ad- versarial nets . In  NIPS , pages 2672–2680. Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory . Neural computation , 9(8):1735–1780. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017.  Toward con- trolled generation of text . In  ICML , pages 1587– 1596. Navdeep Jaitly and Geoffrey E Hinton. 2015. Vo- cal tract length perturbation (vtlp) improves speech recognition . In  ICML . Robin Jia and Percy Liang. 2016.  Data recombination for neural semantic parsing . In  ACL , pages 12–22. Kushal Kaﬂe, Mohammed Yousefhussien, and Christo- pher Kanan. 2017. Data augmentation for visual question answering . In  INLG , pages 198–202. Yoon Kim. 2014.  Convolutional neural networks for sentence classiﬁcation . In  EMNLP , pages 1746– 1751. Yoon Kim and Alexander M. Rush. 2016.  Sequence- level knowledge distillation . In  EMNLP , pages 1317–1327. Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. 2015. Audio augmentation for speech recognition.  In  INTERSPEECH , pages 3586–3589. Sosuke Kobayashi, Naoaki Okazaki, and Kentaro Inui. 2017.  A neural language model for dynamically rep- resenting the meanings of unknown words and enti- ties in a discourse . In  IJCNLP , pages 473–483. Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and Kentaro Inui. 2016. Dynamic entity representation with max-pooling improves machine reading. In Proceedings of NAACL-HLT , pages 850–855. Oleksandr Kolomiyets, Steven Bethard, and Marie- Francine Moens. 2011. Model-portability experi- ments for textual temporal analysis . In  ACL , pages 271–276. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012.  Imagenet classiﬁcation with deep con- volutional neural networks . In  NIPS , pages 1097– 1105. Xin Li and Dan Roth. 2002.  Learning question classi- ﬁers . In  COLING , pages 1–7. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models.  In  ICLR . George A. Miller. 1995.  Wordnet: A lexical database for english .  Commun. ACM , 38(11):39–41. ", "page_idx": 4, "bbox": [307, 64.56060791015625, 525, 765.7643432617188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "Bo Pang and Lillian Lee. 2004.  A sentimental educa- tion: Sentiment analysis using subjectivity summa- rization based on minimum cuts . In  ACL . Bo Pang and Lillian Lee. 2005.  Seeing stars: Exploit- ing class relationships for sentiment categorization with respect to rating scales . In  ACL , pages 115– 124. Charles Bally Albert Sechehaye Saussure, Ferdi- nand de and Albert Riedlinger. 1916.  Cours de lin- guistique generale . Lausanne: Payot. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.  Improving neural machine translation models with monolingual data . In  ACL , pages 86–96. Miikka Silfverberg, Adam Wiemerslage, Ling Liu, and Lingshuang Jack Mao. 2017. Data augmentation for morphological reinﬂection . In  CoNLL SIGMOR- PHON , pages 90–99. Patrice Y. Simard, Yann A. LeCun, John S. Denker, and Bernard Victorri. 1998.  Transformation Invariance in Pattern Recognition — Tangent Distance and Tan- gent Propagation . Springer Berlin Heidelberg. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositional it y over a sentiment tree- bank . In  EMNLP , pages 1631–1642. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works . In  NIPS , pages 3104–3112. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. 2015.  Going deeper with convolutions . In CVPR . Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a next-generation open source framework for deep learning . In  Proceedings of Workshop on LearningSys in NIPS 28 . William Yang Wang and Diyi Yang. 2015. That’s so annoying!!!: A lexical and frame-semantic em- bedding based data augmentation approach to au- tomatic categorization of annoying behaviors using #petpeeve tweets . In  EMNLP , pages 2557–2563. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.  Annotating expressions of opinions and emo- tions in language .  Language Resources and Evalu- ation , 39(2):165–210. Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. 2017.  Dual supervised learn- ing . In  ICML , pages 3789–3798. Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan. 2017.  Variational autoencoder for semi-supervised text classiﬁcation . In  AAAI , pages 3358–3364. ", "page_idx": 5, "bbox": [72, 64.56060791015625, 290, 765.76416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- siﬁcation . In  NIPS , pages 649–657. ", "page_idx": 5, "bbox": [307.2770080566406, 64.56024169921875, 525, 98.48314666748047], "page_size": [595.2760009765625, 841.8900146484375]}
