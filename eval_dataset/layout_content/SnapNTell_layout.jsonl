{"layout": 0, "type": "text", "text": "SnapNTell : Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM ", "text_level": 1, "page_idx": 0, "bbox": [80, 76, 515, 110], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Jielin  $\\mathbf{Q}\\mathbf{i}\\mathbf{u}^{1,2};$  , Andrea Madotto 1 , Zhaojiang  $\\mathbf{Li}\\mathbf{n}^{1}$  , Paul A. Crook 1 , Yifan Ethan   $\\mathbf{X}\\mathbf{u}^{1}$  , Xin Luna   $\\mathbf{Doneg^{1}}$  , Christos Faloutsos 2 , Lei  $\\mathbf{Li^{2}}$  , Babak Damavandi 1 , Seungwhan Moon 1 1  Meta Reality Labs & FAIR, Meta  2 Carnegie Mellon University ", "page_idx": 0, "bbox": [75.25396728515625, 125, 525.4981079101562, 168.67147827148438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "{ jielinq,leili,christos } @cs.cmu.edu,  { andreamad8,zhaojiang,pacrook,ethanxu,lunadong,shanemoon } @meta.com ", "page_idx": 0, "bbox": [96.38398742675781, 170.97511291503906, 501.8815002441406, 187.84988403320312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [157, 213, 203, 225], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "Vision-extended LLMs have made significant strides in Visual Question Answering (VQA). Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a ten- dency to produce erroneous or hallucinated responses. In this work, we introduce a novel evaluative benchmark named  SnapNTell , specifically tailored for entity-centric VQA. This task aims to test the models’ capabilities in identifying entities and providing detailed, entity-specific knowledge. We have developed the  SnapNTell Dataset , distinct from tradi- tional VQA datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features QA pairs that require extensive knowledge for accurate responses. The dataset is organized into 22 major categories, contain- ing 7,568 unique entities in total. For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive QA pairs. To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM. Our approach markedly out- performs existing methods on the SnapNTell dataset, achieving a  $66.5\\%$   improvement in the BELURT score. We will soon make the dataset and the source code publicly accessible. ", "page_idx": 0, "bbox": [87, 236.58258056640625, 273, 583.33251953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [70, 599, 154, 613], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Vision-extended LLMs have shown significant ad- vancements, excelling at capturing complex seman- tics and context-aware attributes needed for intri- cate tasks. However, their abilities in factual VQA tasks, which demand accurate, concrete answers about real-world entities and phenomena, expose certain limitations. Particularly, torso-to-tail or long-tail entities, which constitute a large propor- tion of real-world data but appear infrequently in training datasets, pose a challenge. This scarcity ", "page_idx": 0, "bbox": [70, 620.77001953125, 290, 755.857421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "image", "page_idx": 0, "img_path": "layout_images/SnapNTell_0.jpg", "bbox": [315, 213, 511, 302], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "“What is the\nbuilding in the\nimage?”\n\n}—»\n\nBaseline\n\nThis is a tower.\n\nNot fine-grained\n\nentity-centric answer\n\n‘SnapNTell\n\nThis is Eiffel Tower. It is a wrought-\niron lattice tower on the Champ de\nMars in Paris, France. It is named after\n\nthe engineer\n\nGustave Eiffel, whose\n\ncompany designed and built the tower.\n", "vlm_text": "The image shows a visual comparison of two systems answering the question \"What is the building in the image?\" with a picture of the Eiffel Tower.\n\n- The baseline system provides a simple answer: \"This is a tower.\"\n- The SnapNTell system offers a more detailed response: \"This is the Eiffel Tower. It is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\""}
{"layout": 8, "type": "text", "text": "Figure 1: Comparing SnapNTell with existing methods reveals a distinctive focus. In the SnapNTell benchmark, the answers are predominantly  entity-centric , charac- terized by a greater depth of knowledgeable information pertaining to the specific entity depicted in the image as the answer. ", "page_idx": 0, "bbox": [305, 316.8656005859375, 526, 388.6465148925781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "in representation often leads to VLLMs resorting to generating plausible but incorrect or imaginative content in their outputs, a problem that manifests as “hallucinations\" within the context of model re- sponses. To ensure the confident deployment of VLLMs in practical scenarios, there is an urgent need for dedicated research that not only recognizes but actively strives to tackle and reduce instances of hallucinations, especially in the context of factual queries involving these long-tail entities. ", "page_idx": 0, "bbox": [305, 394.1789855957031, 526, 529.2674560546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "The lack of publicly available evaluation datasets specifically tailored to assess models’ ability in rec- ognizing real-world long-tailed entities presents a notable gap in VQA. Existing datasets fall short in serving this purpose due to a narrow range of entity categories, the prevalence of overly simplis- tic yes/no QA pairs, and a general lack of entity specificity, often using broad terms like “Tiger\" instead of more specific ones like “Siberian Tiger\". To address this gap, we introduce a novel eval- uation task called  SnapNTell , which focuses on entity-centric knowledge-based VQA. The Snap- NTell benchmark has been designed to evaluate models’ abilities in accurately identifying entities and generating responses that showcase a deep un- der standing of these entities. To support this task, we have curated a new evaluation dataset that de- parts from existing datasets in two crucial ways: (1) ", "page_idx": 0, "bbox": [305, 530.6079711914062, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "It includes a wide range of fine-grained and catego- rized entities, each accompanied by corresponding images and clear mention of the entity name within the answer sets. (2) It features QA pairs designed to prompt knowledge-intensive responses, moving beyond the binary yes/no format to challenge and assess the depth of the model’s comprehension. ", "page_idx": 1, "bbox": [70, 71.74500274658203, 291, 166.18545532226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "Furthermore, the limitations identified in factual query generation underscore the need for new so- lutions to address the problem of hallucinations. Recent advancements suggest that retrieval-based approaches hold significant promise in this regard ( Guu et al. ,  2020 ;  Srinivasan et al. ,  2022 ;  Yang et al. ,  2023a , b ). These methods enhance LLMs by integrating external knowledge sources or incor- porating retrieval mechanisms to access relevant information from extensive knowledge bases. The synergy between the advanced inference capabil- ities of LLMs and the wealth of external knowl- edge has the potential to significantly reduce issues related to long-tail entities and, consequently, de- crease the occurrence of hallucinatory responses. ", "page_idx": 1, "bbox": [70, 167.47901916503906, 291, 370.3124694824219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "In this work, we aim to propose an evaluation task to investigate the model’s ability to recog- nize real-world long-tailed entities and provide knowledge-intensive answers. We also propose a retrieval-augmented method to reduce hallucina- tions and enhance the precision and trustworthiness of generated responses. ", "page_idx": 1, "bbox": [70, 371.6050109863281, 291, 466.04547119140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "Our contribution is summarized as follows: ", "page_idx": 1, "bbox": [81, 467.3389892578125, 270.6660461425781, 480.4844665527344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "•  SnapNTell task.  We propose a novel task for entity-centric  VQA, specifically designed to assess the proficiency of models in accurately identifying and generating responses that ex- hibit a deep comprehension of these identified entities. ", "page_idx": 1, "bbox": [81, 488.0352478027344, 291, 569.3194580078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "•  SnapNTell model.  We proposed a retrieval- augmented multimodal LLM, devised as a baseline model capable of undertaking the SnapNTell task, which is scalable, effective, and explain able. ", "page_idx": 1, "bbox": [81, 576.8712768554688, 291, 644.6064453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "•  SnapNTell dataset.  We collected a new eval- uation dataset with distinctive characteristics, which stands out for two key features: (1) It encompasses a diverse range of fine-grained entities, each accompanied by correspond- ing representative images. (2) The question- answer pairs contain knowledge-intensive re- sponses with entity names specifically men- tioned in the answer sets. ", "page_idx": 1, "bbox": [81, 652.1572875976562, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "•  Our model demonstrates superior perfor- mance on the SnapNTell dataset, surpassing current methodologies with a  $66.5\\%$   improve- ment in BELURT score. ", "page_idx": 1, "bbox": [319.1610107421875, 71.74500274658203, 526, 125.53848266601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1, "bbox": [306, 136, 401, 149], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "Knowledge-based VQA Research in vision- language tasks, which necessitate understanding image content to answer questions, has seen sig- nificant advancements over recent years. Begin- ning with datasets like FVQA ( Wang et al. ,  2016 ), which extracted facts from pre-established knowl- edge bases, the field has progressed to more chal- lenging ones like the OK-VQA dataset ( Marino et al. ,  2019 ), encompassing diverse knowledge cat- egories. Multi Modal QA ( Talmor et al. ,  2021 ) intro- duced complexity with questions demanding cross- modal reasoning over snippets, tables, and images. The successor of OK-VQA, AOK-VQA ( Schwenk et al. ,  2022 ), raises the bar by providing ques- tions that transcend simple knowledge base queries. Many Modal QA ( Hannan et al. ,  2020 ) shifts the focus to answer modality selection, MIMOQA ( Singh et al. ,  2021 ) emphasizes multimodal answer extraction, and WebQA ( Chang et al. ,  2021 ) in- troduces real-world knowledge-seeking questions, albeit with some limitations regarding entity catego- rization and granularity. More comparison details can be found in Section  3.5 . ", "page_idx": 1, "bbox": [305, 158.0372772216797, 526, 469.6584777832031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "Multimodal LLMs Integrating visual under- standing into text-based LLM typically combines them with a visual encoder and uses image cap- tioning datasets for alignment ( Koh et al. ,  2023 ; Wu et al. ,  2023 ;  Chowdhery et al. ,  2022 ). Tech- niques like adapter-based tuning ( Alayrac et al. , 2022 ) and prefix tuning ( Tsim po uk ell i et al. ,  2021 ) allow these models to process visual inputs while maintaining their linguistic capabilities, without requiring full model retraining ( Yin et al. ,  2023 ). ", "page_idx": 1, "bbox": [305, 472.84326171875, 526, 608.324462890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "Retrieval-augmented LLM Previous studies have explored retrieval augmentation in text-only settings or image captioning tasks. Guu et al. ( 2020 ) introduced a retriever for language models to access large corpus during various stages.  Srini- vasan et al.  ( 2022 ) showed retrieval-augmented queries enhance LLMs’ context understanding.  Ya- sunaga et al. (2023) and Yang et al. (2023a) de-veloped methods for integrating multimodal doc- uments and speeding up LLM inference, respec- tively.  Yang et al.  ( 2023b ) created a visual lan- guage model, inspired by Flamingo ( Alayrac et al. , 2022 ), for image captioning with external database retrieval. Similarly,  Gui et al.  ( 2021 ) combined im- plicit and explicit knowledge in an encoder-decoder setup to improve answer generation. ", "page_idx": 1, "bbox": [305, 611.5103149414062, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "", "page_idx": 2, "bbox": [70, 71.74500274658203, 291, 125.53848266601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "Open-domain visual entity recognition Hu et al. ( 2023 ) developed OVEN for associating images with Wikipedia entities via text queries, while  Chen et al.  ( 2023 ) introduced INFOSEEK, a dataset for Visual Question Answering focused on informa- tional queries. While OVEN is proficient in entity recognition using a knowledge base, INFOSEEK mainly supplies factual responses. Our study seeks to merge these strengths, creating detailed para- graphs that provide context for a more compre- hensive understanding beyond basic facts. More related work can be found in Appendix  E . ", "page_idx": 2, "bbox": [70, 128.3223114013672, 291, 290.9014587402344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "3 SnapNTell Dataset ", "text_level": 1, "page_idx": 2, "bbox": [70, 301, 184, 315], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "3.1 Entity Categorization ", "text_level": 1, "page_idx": 2, "bbox": [70, 323, 197, 336], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "To tackle the challenge of the new SnapNTell task, the first step involves creating a comprehensive dataset that represents a wide array of real-world entities. Our dataset creation methodology entails selecting a diverse set of entity names from vari- ous categories that mirror the diversity of the real world. This selection encompasses both commonly encountered entities and less frequently encoun- tered ones. We have identified 22 categories that adequately represent a cross-section of entities one might encounter in daily life. These categories include  landmark, painting, sculpture, food, fruit, vegetable, mammal, amphibian, insect, fish, bird, reptile, celebrity, instrument, plant, electronics, tool, transportation, sport, book, household, and car . More details about the categories can be re- ferred to Table  10  in the Appendix. ", "page_idx": 2, "bbox": [70, 340.8059997558594, 291, 570.7384033203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "To populate each category with specific enti- ties, we leveraged Wikipedia as a primary resource due to its extensive and detailed entries. (See Ap- pendix  A  for more details.) Our selection criteria are heavily biased towards specificity; for instance, in the category of mammals, we deliberately opted for precise names such as “German Shepherd” or “Alaskan Malamute” instead of the generic “Dog”. This level of specificity is critical as it enables the model to demonstrate its capacity for fine-grained recognition and its ability to generate detailed, ac- curate information about each entity. This dataset- building approach is what distinguishes our dataset from existing VQA datasets, which often lack fine- grained entities and specificity. ", "page_idx": 2, "bbox": [70, 571.2550048828125, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "3.2 Image collection ", "text_level": 1, "page_idx": 2, "bbox": [306, 72, 407, 84], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "The dataset comprises 22 primary categories, en- cap sula ting a total of 7,568 unique entities. For each individual entity, a set of 10 images has been curated, where the statistic of the entity list is shown in Table  10  in the Appendix. ", "page_idx": 2, "bbox": [305, 89.50397491455078, 526, 156.84646606445312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "Filtering Initially, a comprehensive list of enti- ties, encompassing 22 primary categories, was com- piled, in a total of 14,910 diverse entities. Then the entity list underwent filtering by cross-referencing each entry with its corresponding Wikipedia page. Entities lacking valid Wikipedia pages were sub- sequently removed from the list. For each corre- sponding entity, images were sourced from Cre- ative Commons (CC). Further filtering was con- ducted by removing entities that didn’t have a suffi- cient number of images obtained via Google Image Search engine. The collected metadata was stored in a CSV file containing essential information such as image URLs, source page URLs, renamed im- age names, and the corresponding Wikipedia page URLs. After filtering, the final number of entities in the SnapNTell dataset is 7,568. (More filtering details can be found in Appendix  B .) ", "page_idx": 2, "bbox": [305, 159.73228454589844, 526, 403.6064758300781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "3.3 Knowledge-intensive Question-Answer Pairs ", "text_level": 1, "page_idx": 2, "bbox": [306, 414, 511, 439], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "In our SnapNTell dataset, we considered five types of questions: ", "page_idx": 2, "bbox": [305, 444.6470031738281, 526, 471.3414611816406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "•  Static facts (absolute facts, discrete facts). These are objective facts that are concrete and are not contingent on other conditions. They can usually be answered with a unique answer. i.e., “When was he (Barack Obama) born?\" ", "page_idx": 2, "bbox": [319, 475.90625, 526, 543.6414794921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "•  Narrative facts.  These facts encompass com- prehension of larger contexts (e.g., song lyrics, movie plot). They are factual in the sense that the content of the narrative should accurately reflect the source material or events, but a cor- rect answer is usually not unique, as they can vary in their level of detail and focus. i.e., “What is the plot of that (‘The Godfather’)?\" ", "page_idx": 2, "bbox": [319, 548.206298828125, 526, 656.5884399414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "•  Dynamic facts.  These are facts that are sub- ject to change over time. i.e., “What is the Yelp customer rating of it (the Eleven Madi- son Park restaurant) in NYC?\" ", "page_idx": 2, "bbox": [319, 661.1532592773438, 526, 715.3384399414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "•  Procedural facts.  These are usually answers to “how” questions, outlining a sequence of steps to accomplish a task. While the steps may not be unique and could be subjective, ", "page_idx": 2, "bbox": [319, 719.9032592773438, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "the answer can still be classified as logical or nonsensical. Note that these facts may some- times overlap with dynamic facts or narrative facts, i.e., “How do you check the battery level of my item (Ray-Ban Stories Glasses)?\" ", "page_idx": 3, "bbox": [92, 71.74500274658203, 291, 139.08749389648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "•  Subjective facts. (opinion-based facts). These “facts” are not objective indisputable facts, but based on individual perspectives or experience. Recommendations fall in this category. While there’s generally no single correct answer to questions seeking subjec- tive facts, it still requires the system to un- derstand the topic and provide reasonable an- swers grounded by world facts. i.e., “Why do you like it (Niagara Falls)?\" ", "page_idx": 3, "bbox": [81, 144.8182830810547, 291, 280.2994689941406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "To construct a comprehensive and knowledge- intensive QA dataset, we employ a three-step process. Firstly, we extracted and condensed perti- nent information from Wikipedia for each entity, i.e., the summary of the introduction, the caption of the image, etc. (See Appendix  A  for more details). Following similar approaches proposed by LLaVA ( Liu et al. ,  2023b ),  Dettmers et al.  ( 2023 ) is utilized to generate QA pairs for each entity automatically based on five pre-defined question types, ensuring diversity and informative ness. Then, we enlisted three annotators (2 male and 1 female) from Amazon SageMaker to assess QA pair quality and make necessary revisions to meet specific criteria. The responsibilities of these annotators include: (1) ensuring that the images and QA pairs are semantically aligned, (2) validating the accuracy of the provided answers, (3) making sure the questions are free of particular entity names but demanding such specificity in the answers, (4) assessing if the modified QA pairs adhere to the criteria for knowledge-intensive content, and (5) removing specific entity-related details from the questions. This last step guarantees that the question queries cannot be answered without understanding the accompanying visual context. ", "page_idx": 3, "bbox": [70, 291.40496826171875, 291, 643.2804565429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "Quality and consistency In order to verify the quality of the QA pairs, we conducted a quality evaluation by randomly choosing   $1{,}000\\,\\mathrm{QA}$   pairs from our dataset. We assigned three independent human evaluators (1 male, 2 female) from Amazon SageMaker to review these pairs for accuracy [ ac- curate, inaccurate ] and agreement on whether to save the QA pair by Fleiss’ Kappa ( Fleiss ,  1971 ). The outcome of this assessment revealed   $98\\%$   ac- curacy and    $\\kappa\\,=\\,0.95$   agreement rate among the evaluators, demonstrating a significant degree of uniformity in the quality of the QA pairs. ", "page_idx": 3, "bbox": [70, 652.1572875976562, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "", "page_idx": 3, "bbox": [305, 71.74500274658203, 526, 111.98947143554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "3.4 Statistics and Analysis of Our Dataset ", "text_level": 1, "page_idx": 3, "bbox": [305, 123, 508, 135], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "Entity statistics To provide a clear summary of this comprehensive dataset, we have condensed the details of the entity list into Table  10  and Figure  9 (in Appendix  F ). Our analysis indicates that the dataset displays a well-balanced distribution across different categories, enhancing its balanced and diverse characteristics. Such a balanced and diverse composition enhances the representative ness of our proposed evaluation dataset. ", "page_idx": 3, "bbox": [305, 140.5692596435547, 526, 262.5004577636719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "Popularity The importance of entity popularity in search engines is a key aspect to consider, simi- lar to examining the head, torso, and tail sections of knowledge bases within search engine frameworks. As demonstrated in Figure  11  in Appendix  F , we use the average Wikipedia pageviews per entity over the last 60 days as the metric. This average is calculated by summing up the pageviews and then dividing by the number of entities. The insights from Figure  11  reveal that entities in the celebrity category have the highest average popularity. For a broader comparison among different categories, we also present a comprehensive analysis of total pageviews for all categories in Figure  10  in Ap- pendix  F , which shows that the celebrity category remains at the forefront in terms of overall entity popularity. This is attributed to the combination of a higher number of entities in this category and the generally higher popularity of each entity within it. ", "page_idx": 3, "bbox": [305, 271.3092956542969, 526, 528.7334594726562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "3.5 Comparison with Existing VQA Datasets ", "text_level": 1, "page_idx": 3, "bbox": [306, 540, 523, 552], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "In Table  2  and Figure  2 , we present a compari- son with existing VQA datasets. It is evident that some existing VQA datasets lack categorization, fine-grained entities, and knowledge-intensive an- swers, as observed in VQA 2.0 ( Goyal et al. ,  2016 ) and GQA ( Hudson and Manning ,  2019 ). OK-VQA ( Marino et al. ,  2019 ) contains images that may not be sufficient to answer the questions, encouraging reliance on external knowledge resources. How- ever, the answers in OK-VQA are often simplistic binary (yes/no) responses or selections from the questions. A-OKVQA ( Schwenk et al. ,  2022 ), the successor of OK-VQA, aims to provide questions that require commonsense reasoning about the de- picted scene but use general object names in the answers. Multi Modal QA ( Talmor et al. ,  2021 ) fo- ", "page_idx": 3, "bbox": [305, 557.7059936523438, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "table", "page_idx": 4, "img_path": "layout_images/SnapNTell_1.jpg", "table_caption": "Table 1: More detailed comparison with existing knowledge-based VQA datasets.  Anonymity  means whether the question already contains a knowledge clue related to the entity in question. (  $^{\\ast}$   Unclear) ", "bbox": [69, 70, 526, 142], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Dataset | Categories | Unique Entity | QA Pairs | Images | Average Ans Length | Number of Images / Entity | Anonymity\n\nViQuAE 3 2,400 3,700 3,300 1.8 me x\nEncyclopedic VQA (test) 12 * 5,750 5,750 32. me x\nSnapNTell (Ours) 22 7,568 75,680 75,680 Psyal 10 v\n\n", "vlm_text": "The table compares three datasets based on various attributes:\n\n1. **ViQuAE**\n   - Categories: 3\n   - Unique Entity: 2,400\n   - QA Pairs: 3,700\n   - Images: 3,300\n   - Average Answer Length: 1.8\n   - Number of Images / Entity: *\n   - Anonymity: ✗\n\n2. **Encyclopedic VQA (test)**\n   - Categories: 12\n   - Unique Entity: *\n   - QA Pairs: 5,750\n   - Images: 5,750\n   - Average Answer Length: 3.2\n   - Number of Images / Entity: *\n   - Anonymity: ✗\n\n3. **SnapNTell (Ours)**\n   - Categories: 22\n   - Unique Entity: 7,568\n   - QA Pairs: 75,680\n   - Images: 75,680\n   - Average Answer Length: 25.7\n   - Number of Images / Entity: 10\n   - Anonymity: ✓\n\nThe SnapNTell dataset is highlighted and indicates that it offers more categories, unique entities, QA pairs, images, and features anonymity with a significantly longer average answer length compared to the others."}
{"layout": 49, "type": "table", "page_idx": 4, "img_path": "layout_images/SnapNTell_2.jpg", "table_caption": "Table 2: Comparison with existing VQA datasets  Knowl- edge  means the QA pairs are knowledgeable, not simple yes/no answers or selection questions.  Entities  means whether there are fine-grained entities specifically con- tained in answers.  Categorization  means the entities are categorized, not randomly crawled online. ", "bbox": [68, 155, 291, 322], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Dataset | Knowledge | Entities | Categorization\n\nVQA 2.0 (Goyal et al., 2016)\nGQA (Hudson and Manning, 2019)\nOK-VQA (Marino et al., 2019)\nManyModalQA (Hannan et al., 2020)\nMultiModalQA (Talmor et al., 2021)\nMIMOQA (Singh et al., 2021)\nA-OKVQA (Schwenk et al., 2022)\nWebQA (Chang et al., 2021)\n\nViQuAE (Lerner et al., 2022)\nEncyclopedic VQA (Mensink et al., 2023)\n\nLLL\nSN\nSAN\n\n", "vlm_text": "The table lists various datasets related to visual question answering (VQA) and their attributes regarding knowledge, entities, and categorization. Here's a summary:\n\n1. **VQA 2.0 (Goyal et al., 2016)**: Knowledge ✓\n2. **GQA (Hudson and Manning, 2019)**: Entities ✓\n3. **OK-VQA (Marino et al., 2019)**: Knowledge ✓\n4. **ManyModalQA (Hannan et al., 2020)**: Entities ✓\n5. **MultiModalQA (Talmor et al., 2021)**: Entities ✓\n6. **MIMOQA (Singh et al., 2021)**: Knowledge ✓, Entities ✓\n7. **A-OKVQA (Schwenk et al., 2022)**: Knowledge ✓, Entities ✓\n8. **WebQA (Chang et al., 2021)**: Knowledge ✓, Entities ✓\n9. **ViQuAE (Lerner et al., 2022)**: Knowledge ✓, Entities ✓\n10. **Encyclopedic VQA (Mensink et al., 2023)**: Knowledge ✓, Entities ✓\n11. **SnapNTell (Ours)**: Knowledge ✓, Entities ✓, Categorization ✓\n\nThe checkmarks indicate the presence of these features in each dataset. SnapNTell includes all three features - knowledge, entities, and categorization."}
{"layout": 50, "type": "text", "text": "cuses on cross-modal knowledge extraction but re- lies on question templates for question generation. Many Modal QA ( Hannan et al. ,  2020 ) focuses on answer modality choice rather than knowledge ag- gregation or extraction. In MIMOQA ( Singh et al. , 2021 ), the task of extracting a multimodal answer is not necessarily knowledge-intensive. WebQA ( Chang et al. ,  2021 ) does have categorization but lacks fine-grained entities in many QA pairs, result- ing in more general questions and answers. Our proposed SnapNTell differs by including a wide range of fine-grained entities with representative images and explicit entity names in the answer sets. Additionally, it incorporates question-answer pairs that demand knowledge-intensive responses, going beyond simplistic binary answers. Examples of our dataset can be found in Figure  8  in Appendix  F . ", "page_idx": 4, "bbox": [70, 338.281982421875, 291, 568.2144775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "ViQuAE ( Lerner et al. ,  2022 ) and Encyclope- dic VQA ( Mensink et al. ,  2023 ) both incorporate entity-level knowledge-based information along with categorization. Therefore, we performed a more in-depth analysis comparing them in Table  1 . Our dataset surpasses these in terms of the vari- ety of categories, the number of distinct entities, and the overall number of QA pairs. Additionally, our dataset boasts a higher count of images and a longer average length for answers. Specifically, our dataset is structured to include 10 images for each entity, whereas the exact number of images per en- tity in ViQuAE and Encyclopedic VQA remains unspecified. Most notably, our dataset’s questions are highly anonymous, implying that they do not ", "page_idx": 4, "bbox": [70, 571.2550048828125, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "image", "page_idx": 4, "img_path": "layout_images/SnapNTell_3.jpg", "img_caption": "Figure 2: Comparison with existing datasets, where pre- vious VQA datasets mostly focus on freeform answers (such as yes/no for verification questions and choice for selection questions). ", "bbox": [305, 156, 527, 365], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "GQA OK-VQA\n\nQ: Is the umbrella Q: What animal Q: Is the photo from\n\nupside down? is in the box? the 50’s or the 90's?\nA: No A: Bear A: 50's\nSnapNTell\n\nQ: What is the current status of it?\nA: The Mendenhall Glacier is\ncurrently experiencing a negative\nglacier mass balance and will likely\ncontinue to retreat.\n\n", "vlm_text": "The image contains comparisons of different Visual Question Answering (VQA) datasets with a new dataset called \"SnapNTell.\" \n\n1. **VQA v2**:\n   - Image: A person with an umbrella.\n   - Question: \"Is the umbrella upside down?\"\n   - Answer: \"No\"\n\n2. **GQA**:\n   - Image: A box with a stuffed animal.\n   - Question: \"What animal is in the box?\"\n   - Answer: \"Bear\"\n\n3. **OK-VQA**:\n   - Image: A person in a kitchen.\n   - Question: \"Is the photo from the 50’s or the 90’s?\"\n   - Answer: \"50’s\"\n\n4. **SnapNTell**:\n   - Image: Mendenhall Glacier.\n   - Question: \"What is the current status of it?\"\n   - Answer: \"The Mendenhall Glacier is currently experiencing a negative glacier mass balance and will likely continue to retreat.\"\n\nThe caption notes that previous VQA datasets focus on freeform answers like yes/no or choice selection."}
{"layout": 53, "type": "text", "text": "reveal any knowledge hints about the entity. This design ensures that the questions cannot be straight- forwardly answered without interpreting the image data, setting our dataset apart from both ViQuAE and Encyclopedic VQA. ", "page_idx": 4, "bbox": [305, 372.5830078125, 526, 439.92547607421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4, "bbox": [305, 451, 365, 464], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "In this section, we will introduce the details of our proposed retrieval-augmented multimodal LLM model. The architecture of our model is shown in Figure  3  (larger figure in Appendix  D  due to space limit). Our model can be considered twofold: (1)  Retrieval augmentation . Given the input image-question pair, we retrieve useful entity- centric information within knowledge sources. (2) Entity-centric knowledge-based answer genera- tion . The retrieved information will be combined with the image and question together to generate a knowledgeable answer. ", "page_idx": 4, "bbox": [305, 472.8009948730469, 526, 634.9874267578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "4.1 Retrieval Augmentation ", "text_level": 1, "page_idx": 4, "bbox": [305, 646, 443, 658], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "The retrieval augmentation process can be sub- divided into: (i) Semantic region extraction via language-guided object detection, (ii) Entity recog- nition via image retrieval, and (iii) Knowledge re- trieval via multi-source aggregation. ", "page_idx": 4, "bbox": [305, 662.9349975585938, 526, 730.2774658203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "Semantic Region Extraction via Language- Guided Object Detection To improve recogni- tion performance, we focus on extracting specific ", "page_idx": 4, "bbox": [305, 733.4523315429688, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "image", "page_idx": 5, "img_path": "layout_images/SnapNTell_4.jpg", "bbox": [71, 69, 289, 188], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Input Question\n\n‘What's the building\ninthe image?\n\n", "vlm_text": "The image is a flowchart diagram illustrating a process for image and question processing involving a large language model (LLM).\n\n1. **Input**: An image and a question, e.g., \"What's the building in the image?\".\n2. **Image Encoder**: Processes the input image.\n3. **Entity Detection & Recognition Models**: Detect and recognize entities in the image, identifying the Eiffel Tower in this example.\n4. **Retrieval Augmentation**: Uses recognized entities to retrieve additional information from a database.\n5. **Projection Layers**: Process information from the image and combine it with textual data.\n6. **Word Embedding Layer and LLM**: The word embedding layer processes text, feeding it into the LLM.\n7. **Answer Generation**: The LLM combines image and text data to produce an answer.\n\nArrows indicate the data flow, showing forward and backward passes (gradient flow) across the layers."}
{"layout": 60, "type": "text", "text": "Figure 3: Our SnapNTell model architecture takes an image-question pair as input. It begins with retrieval augmentation to source relevant information about the entity in the image. This information, along with the question, feeds into the word embedding layer. Text embeddings merge with image-projected embeddings before entering the LLM, culminating in a knowledge- able answer as the output. ", "page_idx": 5, "bbox": [70, 196.53057861328125, 291, 292.22149658203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "image regions containing the entity, rather than general image-level recognition. We employ a language-guided object detection model, i.e., GLIP ( Li et al. ,  2021 ), for language-guided object detec- tion, extracting regions relevant to textual queries by understanding the query context. This targeted approach ensures precise region extraction, enhanc- ing the system’s accuracy and contextual relevance. ", "page_idx": 5, "bbox": [70, 301.1510009765625, 291, 409.1404724121094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "Entity Recognition via Image Retrieval We construct a similarity index using CLIP embed- dings ( Radford et al. ,  2021 ) and Faiss ( Johnson et al. ,  2017 ) for indexing. Our database, built on the WIT dataset ( Srinivasan et al. ,  2021 ), maps CLIP image embeddings to their text descriptions, leveraging Faiss’s robust similarity search capa- bilities. After setting up the indexing database, given an input query image    $I$  , we perform a    $k$  - nearest neighbor retrieval based on cosine simi- larity. The retrieval outcomes are represented as  $\\mathcal{R}(I)=\\{(i_{1},c_{1})\\,,\\cdot\\cdot\\cdot\\,,(i_{k},c_{k})\\}$  , where for each    $j$  within the range of  1  to    $k,\\,i_{j}$   and  $c_{j}$   correspond to the retrieved image and its associated caption, respectively. By comparing    $I$   with similar images from the database, we identify the entity in the image region, which enables precise image-level entity recognition. ", "page_idx": 5, "bbox": [70, 415.48626708984375, 291, 659.3614501953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "Knowledge Retrieval via Multi-Source Aggrega- tion Facing diverse user queries, we gather extra information to compile resources for accurate re- sponses. Some queries require up-to-date informa- tion, not present in existing databases. We then turn to external sources to collect critical data like “year built,\" “description,\" and more. By using Knowl- edge Graph (KG) and web searches, we access rele- vant knowledge links, enriching our understanding of the specified image region, and improving our ability to comprehend and contextual ize the ex- tracted content. More details of the method can be found in Appendix  D . ", "page_idx": 5, "bbox": [70, 665.706298828125, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "", "page_idx": 5, "bbox": [305, 71.74500274658203, 526, 139.08749389648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "4.2 Entity-centric Knowledge-based Answer Generation ", "text_level": 1, "page_idx": 5, "bbox": [306, 148, 520, 173], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "Following information collection, we enter the inte- gration phase, blending the input image, question, and retrieved data to generate a knowledgeable response, which is illustrated in Figure  3 . Our method enhances multimodal understanding by pre-training a LLM with image-text paired data. Taking cues from  Moon et al.  ( 2023 ), we employ lightweight adapters for each modality, converting inputs into the text token embedding space of the chosen LLM. ", "page_idx": 5, "bbox": [305, 178.8880157470703, 526, 313.9754943847656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "In our method, the LLM’s text token embedding space morphs into a unified space, representing both text and image content, with each modality assigned 64 to 256 token embeddings. We freeze the LLM’s parameters during alignment training to quicken convergence and retain the LLM’s rea- soning skills for inference. To ensure feature align- ment, we use an image encoder,    $g(\\cdot)$  , previously synchronized with a text embedding space, like in CLIP ( Radford et al. ,  2021 ;  Schuhmann et al. , 2022 ). For text-image pairs    $(\\mathbf{X}_{t e x t},\\mathbf{X}_{i m a g e})$  , we align them using specific objectives and a projec- tion module, like the Perceiver Resampler ( Alayrac et al. ,  2022 ), applied to the vision encoder as: ", "page_idx": 5, "bbox": [305, 314.37896728515625, 526, 503.66448974609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "equation", "text": "\n$$\np(\\mathbf{X}_{t e x t}|\\mathbf{X}_{i m a g e})=\\prod_{i=1}^{L}p_{\\theta}(\\mathbf{X}_{t e x t}^{[i]}|\\mathbf{Z}_{i m a g e},\\mathbf{Z}_{t e x t}^{[1:i-1]})\n$$\n ", "text_format": "latex", "page_idx": 5, "bbox": [328, 505, 501, 536], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "equation", "text": "\n$$\n\\mathbf{Z}_{i m a g e}=P r o j_{\\theta}\\big(h_{l a t e n t s},g\\big(\\mathbf{X}_{i m a g e}\\big)\\big)\n$$\n ", "text_format": "latex", "page_idx": 5, "bbox": [353, 540, 476, 552], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "5 Experiments and Results ", "text_level": 1, "page_idx": 5, "bbox": [306, 563, 453, 576], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5, "bbox": [305, 584, 425, 596], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "Evaluation Metrics (1) In our evaluation pro- cess, the quality of the answers is first assessed using established NLP metrics such as BLEU ( Pa- pineni et al. ,  2002 ), METEOR ( Denkowski and Lavie ,  2014 ), ROUGE ( Lin ,  2004 ), and BLEURT ( Sellam et al. ,  2020 ;  Pu et al. ,  2021 ). (2) Addition- ally, we incorporate accuracy and hallucination rate metrics from ( Sun et al. ,  2023 ). These metrics used GPT4  to automatically measure the proportion of questions for which the model provides correct answers or incorrect/partially incorrect answers, respectively. (3) We conduct human evaluation following  Ye et al.  ( 2023 );  Moon et al.  ( 2023 ). ", "page_idx": 5, "bbox": [305, 600.5802612304688, 526, 776.7084350585938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "table", "page_idx": 6, "img_path": "layout_images/SnapNTell_5.jpg", "table_caption": "Table 3: Performance comparison of different ap- proaches on the SnapNTell dataset. ", "bbox": [69, 70, 291, 177], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Method ROUGE} BLEU+ METEOR} BLEURT t\n\nInstruct-BLIP (Dai et al., 2023) 10.72\nBLIP2 (Li et al., 2023) 15.00\nMini-GPT4 (Zhu et al., 2023) 26.12\nLLaVA (Liu et al., 2023b) 26.86\nOpen-Flamingo (Awadalla et al., 2023) 30.57\nCOGVLM (Wang et al., 2023) 30.25\nmPLUG-Owl2 (Ye et al., 2023) 31.39\n\nLLaVA 1.5 (Liu et al., 2023a) 32.87\n\n0.95\n0.52\n5.62\n6.03\n6.52\n6.67\n6.72\n6.94\n\n7.59\n\n8.49\n25.55\n26.97\n22.53\n23.35\n24.67\n25.23\n\n0.09\n0.16\n0.27\n0.31\n0.32\n0.31\n0.33\n0.33\n\n", "vlm_text": "The table shows a comparison of various methods using four evaluation metrics: ROUGE, BLEU, METEOR, and BLEURT. Each method is associated with a paper:\n\n- **Instruct-BLIP** (Dai et al., 2023)\n- **BLIP2** (Li et al., 2023)\n- **Mini-GPT4** (Zhu et al., 2023)\n- **LLAVA** (Liu et al., 2023b)\n- **Open-Flamingo** (Awadalla et al., 2023)\n- **COGVLM** (Wang et al., 2023)\n- **mPLUG-Owl2** (Ye et al., 2023)\n- **LLAVA 1.5** (Liu et al., 2023a)\n- **SnapNTell (ours)**\n\nThe metrics are given as follows:\n\n- **ROUGE** scores range from 10.72 to 35.28.\n- **BLEU** scores range from 0.52 to 7.81.\n- **METEOR** scores range from 7.59 to 29.27.\n- **BLEURT** scores range from 0.09 to 0.55.\n\nSnapNTell (ours) has the highest scores across all four metrics in this table."}
{"layout": 74, "type": "text", "text": "Model Setting We chose LLaMA2 (70B) ( Tou- vron et al. ,  2023 ) as our LLM. For image encoding, the CLIP image encoder (ViT-B/32) is employed ( Radford et al. ,  2021 ;  Schuhmann et al. ,  2022 ). Ad- ditional configurations comprise a batch size of 2,048, the integration of two resampler layers, and the use of 64 modality tokens. ", "page_idx": 6, "bbox": [70, 184.32127380371094, 291, 279.1544494628906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "Model Training We used a cleaned subset of the LAION-2B dataset, filtered using the CAT method ( Radenovic et al. ,  2023b ) and with any detectable faces blurred ( Radenovic et al. ,  2023a ). Signifi- cant resources are essential to scale pre-training to 70 billion parameter models on a substantial dataset of over 200 million instances. Often, this necessitates the utilization of an FSDP wrapper, as outlined in  Dettmers et al.  ( 2023 ), to distribute the model across multiple GPUs efficiently. To opti- mize our training process, we employ quantization strategies, specifically 4-bit and 8-bit quantization techniques ( Dettmers et al. ,  2023 ), within our mul- timodal framework. In this approach, we maintain the LLM component of our model in a frozen state, allowing only the image modality tokenizers to be trainable. This strategy drastically reduces the memory requirements by an order of magnitude. As a result of these optimization s, we can success- fully train a 70 billion parameter model on a single GPU with 80GB VRAM, using a batch size of 4. ", "page_idx": 6, "bbox": [70, 283.4932556152344, 291, 568.0154418945312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "5.2 Results and Discussion ", "text_level": 1, "page_idx": 6, "bbox": [70, 580, 201, 591], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "Table  3  displays the comparative results between the baseline models and our proposed method. Analysis of this table indicates that for every met- ric assessed, our retrieval-augmented multimodal LLM surpasses the performance of all existing baseline models. This strong performance empha- sizes the efficiency of retrieval augmentation in producing responses enriched with entity-centric information, thereby illustrating its substantial im- pact on the task at hand. ", "page_idx": 6, "bbox": [70, 597.7919921875, 291, 732.8804321289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "Moreover, to gain deeper insights into which evaluation metric more accurately reflects the out- comes, we computed the Kendall correlation coef- ", "page_idx": 6, "bbox": [70, 733.8450317382812, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "table", "page_idx": 6, "img_path": "layout_images/SnapNTell_6.jpg", "table_caption": "Table 4: Effectiveness of evaluation metrics. ", "bbox": [325, 71, 504, 126], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "ROUGE BLEU METEOR BELURT\n\nF 0.999 0.799 0.600 0.999\nP_value 0.014 0.050 0.142 0.014\n\n", "vlm_text": "The table presents two metrics (\\(\\tau\\) and P_value) for four different evaluation measures: ROUGE, BLEU, METEOR, and BELURT.\n\n- **ROUGE**\n  - \\(\\tau\\): 0.999\n  - P_value: 0.014\n\n- **BLEU**\n  - \\(\\tau\\): 0.799\n  - P_value: 0.050\n\n- **METEOR**\n  - \\(\\tau\\): 0.600\n  - P_value: 0.142\n\n- **BELURT**\n  - \\(\\tau\\): 0.999\n  - P_value: 0.014"}
{"layout": 80, "type": "text", "text": "ficient ( Kendall ,  1938 ;  Knight ,  1966 ;  Kendall et al. , 1995 ), comparing the results with those from the human evaluation in Section  5.4 . Kendall’s  $\\tau$   is a measure of the correspondence between two rank- ings. Values close to 1 indicate strong agreement, values close to -1 indicate strong disagreement. Ta- ble  4  revealed that both the ROUGE and BLEURT scores were more indicative in distinguishing the differences among various models. This finding suggests that these two metrics are particularly sig- nificant in evaluating model performance in a way that aligns closely with human judgment. ", "page_idx": 6, "bbox": [305, 135.6909942626953, 526, 297.8774719238281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 6, "bbox": [306, 307, 401, 320], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "For a more in-depth understanding, we conducted several ablation studies to delve into the finer de- tails of our approach. ", "page_idx": 6, "bbox": [305, 324.4429931640625, 526, 364.68646240234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "Effectiveness of Entity Detection To assess the impact of entity detection (ED) in our model, we performed an ablation study. This involved com- paring the performance of our approach with and without the ED component. As indicated in Ta- ble  5 , our approach incorporating entity detection markedly surpasses the variant lacking this feature. This highlights the significant contribution and ne- cessity of the entity detection step in our model’s overall effectiveness. ", "page_idx": 6, "bbox": [305, 366.9282531738281, 526, 502.40948486328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "table", "page_idx": 6, "img_path": "layout_images/SnapNTell_7.jpg", "table_caption": "Table 5: Ablation study on the effectiveness of entity detection (ED). ", "bbox": [305, 511, 525, 577], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Method ROUGE+ BLEUt METEORt BELURT +\n\nw/o ED 28.02 3.73 26.26 0.45\nw/ ED 35.28 7.81 29.27 0.55\n", "vlm_text": "The table compares evaluation metrics for two methods, \"w/o ED\" (without ED) and \"w/ ED\" (with ED), across four metrics:\n\n- **ROUGE**: \n  - w/o ED: 28.02\n  - w/ ED: 35.28 (bold, indicating better performance)\n\n- **BLEU**: \n  - w/o ED: 3.73\n  - w/ ED: 7.81 (bold, indicating better performance)\n\n- **METEOR**: \n  - w/o ED: 26.26\n  - w/ ED: 29.27 (bold, indicating better performance)\n\n- **BELURT**: \n  - w/o ED: 0.45\n  - w/ ED: 0.55 (bold, indicating better performance)\n\nThe method \"w/ ED\" shows improved scores across all metrics. The arrows (↑) signify that higher values are better."}
{"layout": 85, "type": "text", "text": "Head/Torso/Tail Entities Head knowledge per- tains to well-established entities for which there is a wealth of available training data. Ideally, LLMs could be trained to possess this knowledge, fa- cilitating efficient retrieval. On the other hand, torso-to-tail knowledge pertains to less-known or obscure entities, often characterized by scarce or non-existent training data. Providing access to such knowledge involves effectively determining when external information is necessary, retrieving the relevant knowledge efficiently, and seamlessly inte- grating it into responses. ", "page_idx": 6, "bbox": [305, 584.4113159179688, 526, 746.990478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "To assess the performance improvement for head/torso/tail entities, we randomly selected   $10\\%$  ", "page_idx": 6, "bbox": [305, 747.39501953125, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "table", "page_idx": 7, "img_path": "layout_images/SnapNTell_8.jpg", "table_caption": "Table 6: Ablation study on head/torso/tail entities, where RA is short for Retrieval Augmentation and    $\\Delta$  is the performance difference of with and without RA. ", "bbox": [68, 69, 292, 230], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Accuracy ¢ Hallucination |\n\nw/o RA 24.4 75.6\n\nHead w/ RA 27.1 72.9\nA (100%) 11.1% Ft 3.6 % |\n\nw/o RA 19.1 80.9\n\nTorso w/ RA 22.7 17.3\nA (100%) 18.8% Tt 44%]\n\nw/o RA 6.8 93.2\n\nTail w/ RA 12.6 87.4\nA (100%) 85.3. %t 62% 4\n", "vlm_text": "The table shows changes in accuracy and hallucination rates for different categories (Head, Torso, Tail) with and without RA. Here's a breakdown:\n\n- **Head**\n  - Accuracy: \n    - w/o RA: 24.4 \n    - w/ RA: 27.1 \n    - Increase: 11.1%\n  - Hallucination: \n    - w/o RA: 75.6 \n    - w/ RA: 72.9 \n    - Decrease: 3.6%\n\n- **Torso**\n  - Accuracy:\n    - w/o RA: 19.1 \n    - w/ RA: 22.7 \n    - Increase: 18.8%\n  - Hallucination:\n    - w/o RA: 80.9 \n    - w/ RA: 77.3 \n    - Decrease: 4.4%\n\n- **Tail**\n  - Accuracy:\n    - w/o RA: 6.8 \n    - w/ RA: 12.6 \n    - Increase: 85.3%\n  - Hallucination:\n    - w/o RA: 93.2 \n    - w/ RA: 87.4 \n    - Decrease: 6.2%\n\nThe arrows indicate the direction of change, with orange for increases and blue for decreases."}
{"layout": 88, "type": "text", "text": "entities for each category, where head/torso/tail entities are defined based on pageview statistics (popularity) in Section  3.4 . The results presented in Table  6  clearly demonstrate that retrieval aug- mentation can significantly enhance performance across various entity types. Notably, the perfor- mance improvement for torso-to-tail entities far exceeds that of head entities, effectively address- ing the challenge of hallucinations in long-tailed entities through retrieval augmentation. ", "page_idx": 7, "bbox": [70, 241.08399963378906, 290, 376.1714782714844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "Performance of Different VQA Datasets To demonstrate the uniqueness of our SnapNTell dataset compared to existing VQA datasets, we an- alyzed the performance of various baseline models on both traditional VQA datasets and our SnapN- Tell dataset. According to the findings presented in Table  7 , the performance disparities among base- line models on existing datasets are not particularly marked. In contrast, on the SnapNTell dataset, we observed significantly larger differences and notably lower performance. This indicates that our SnapNTell dataset is particularly effective in evaluating the capabilities of different models to recognize entities and produce responses centered around these entities. ", "page_idx": 7, "bbox": [70, 380.2022705078125, 290, 583.429443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "table", "page_idx": 7, "img_path": "layout_images/SnapNTell_9.jpg", "table_caption": "Table 7: Ablation on the  accuracy  performance of dif- ferent VQA datasets. ", "bbox": [69, 587, 291, 658], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Method VQAv2 TextVQA OK-VQA | SnapNTell\n\nInstruct-BLIP (Dai et al., 2023) = 46.6 55:5\nBLIP2 (Li et al., 2023) 52.6 43.1 54.7\nFlamingo (Alayrac et al., 2022) 56.3 37.9 57.8\n\n", "vlm_text": "The table compares different methods across four datasets or benchmarks: VQAv2, TextVQA, OK-VQA, and SnapNTell. It displays performance metrics for each method. Here's a breakdown:\n\n- **Methods:**\n  - Instruct-BLIP (Dai et al., 2023)\n  - BLIP2 (Li et al., 2023)\n  - Flamingo (Alayrac et al., 2022)\n\n- **Metrics:**\n  - VQAv2\n    - Instruct-BLIP: Not available\n    - BLIP2: 52.6\n    - Flamingo: 56.3\n  - TextVQA\n    - Instruct-BLIP: 46.6\n    - BLIP2: 43.1\n    - Flamingo: 37.9\n  - OK-VQA\n    - Instruct-BLIP: 55.5\n    - BLIP2: 54.7\n    - Flamingo: 57.8\n  - SnapNTell\n    - Instruct-BLIP: 8.88\n    - BLIP2: 16.16\n    - Flamingo: 32.17\n\nThe SnapNTell column is highlighted in orange."}
{"layout": 91, "type": "text", "text": "5.4 Human Evaluation Results ", "text_level": 1, "page_idx": 7, "bbox": [70, 675, 222, 688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "In alignment with the methodology presented in Ye et al.  ( 2023 );  Moon et al.  ( 2023 ), we involved a human evaluation process conducted by a panel of five human judges (3 male, 2 female). These judges were given specific instructions for their as- sessment, which encompassed three key aspects: ", "page_idx": 7, "bbox": [70, 693.197998046875, 290, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "image", "page_idx": 7, "img_path": "layout_images/SnapNTell_10.jpg", "img_caption": "Figure 4: Human evaluation results on pairwise com- parisons (  $\\%$   win, tie, lose) with baseline outputs  against the manually annotated ground-truth from SnapNTell. ", "bbox": [304, 74, 527, 232], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "100%\nLose © Tit\ni\nWi\n\n75%\n\n50%\n25%\noe _\n——-\n|_| =\n\npane\npen Fuse\ncoovs,\nA sG-oN”\naval®\ngna Te\n", "vlm_text": "The image is a bar chart comparing human evaluation results of different models in terms of win, tie, and lose percentages against the manually annotated ground truth from SnapNTell. \n\n- Each bar represents a different model: MIni-GPT4, Open-Flamingo, COGVLM, mPLUG-Owl2, LLaVA 1.5, and SnapNTell.\n- The colors indicate different outcomes: blue for lose, yellow for tie, and red for win.\n- SnapNTell has the highest win percentage, while the other models predominantly have a high lose percentage."}
{"layout": 94, "type": "text", "text": "(1) Recognition Accuracy, where they evaluated whether the model correctly identified the entity in the image relevant to the question; (2) Response Accuracy, in which they assessed the factual cor- rectness of the model’s responses while checking for any signs of hallucination ( Rawte et al. ,  2023 ); and (3) Pairwise Comparison, where judges se- lected the response that better addressed the given question in terms of contextual appropriateness and accuracy, categorizing responses as winning, tying, or losing. ", "page_idx": 7, "bbox": [305, 237.8610382080078, 526, 386.49847412109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "In our study, we conducted pairwise compar- isons for each baseline model against ground-truth data across 1,000 samples. As depicted in Figure  4 , our model outperforms the baselines by displaying a significantly smaller difference when measured against manually annotated ground-truth samples, highlighting its robustness. ", "page_idx": 7, "bbox": [305, 386.9020080566406, 526, 481.34246826171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 7, "bbox": [305, 492, 383, 505], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "In this work, we tackle the significant challenge VLLMs face with long-tail entity queries, which of- ten lead to inaccurate or hallucinated responses. To address these issues, we introduce an entity-centric VQA task named SnapNTell. This task is designed to test models on entity recognition and their abil- ity to provide detailed, entity-specific knowledge in their responses. We collected a unique eval- uation dataset for this task, which distinguishes itself from existing VQA datasets by including a wide array of fine-grained categorized entities, sup- ported by images and explicit entity mentions in the answers. This dataset emphasizes knowledge- intensive responses over simple binary answers. In addition, we propose a retrieval-augmented multi- modal LLM solution for the SnapNTell task as an effective baseline. Our experimental results show that our model outperforms existing approaches, providing more accurate and coherent answers. ", "page_idx": 7, "bbox": [305, 512.8760375976562, 526, 769.907470703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 8, "bbox": [70, 71, 130, 84], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "In this study, we introduce a novel SnapNTell task and its accompanying dataset, which features five unique types of questions, each paired with metic- ulously formulated answers. It’s important to rec- ognize that in cases involving human preferences, which are subjective by nature, the given answers might not represent the only correct options. Fur- thermore, the relevancy of some answers may di- minish over time, highlighting the need for peri- odic updates to the dataset to ensure its ongoing relevance and accuracy. Our proposed method ex- hibited superior performance over existing base- lines. However, human evaluation results suggest significant potential for further improvement. Al- though our approach often neared human-level per- formance, it did not consistently outperform human annotations, showing opportunities for future ad- vancements. ", "page_idx": 8, "bbox": [70, 91.92102813720703, 291, 335.4024658203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "Ethics Statement ", "text_level": 1, "page_idx": 8, "bbox": [70, 345, 158, 357], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "In this study, the dataset was sourced from publicly accessible databases, and all author details remain anonymous. We conscientiously excluded any con- tent from our dataset that could be considered ethi- cally sensitive or related to personal privacy, such as images depicting human faces. To our under- standing, and with careful consideration, we do not anticipate any detrimental applications arising from the findings or methodologies presented in this research. ", "page_idx": 8, "bbox": [70, 365.46099853515625, 291, 500.5494689941406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Broader Impact ", "text_level": 1, "page_idx": 8, "bbox": [70, 509, 153, 523], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "Current models have made commendable progress in grasping the nuanced semantics and context- sensitive aspects of Visual Question Answering (VQA). However, their efficacy in factual VQA tasks, which require precise and factual answers about tangible entities and events, reveals certain deficiencies. This is especially true for torso-to-tail or long-tail entities. Despite their prevalence in the real world, these entities are underrepresented in training datasets, leading to a common issue where models produce plausible yet inaccurate or invented responses, a phenomenon often termed “hallucinations\" in the realm of model-generated content. Tackling and minimizing these hallucina- tions is vital for enhancing the trustworthiness and applicability of these models in practical scenarios. The existing VQA datasets, however, are inade- ", "page_idx": 8, "bbox": [70, 530.6079711914062, 291, 760.5404663085938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "quate for evaluating a model’s ability to recognize entities, as they do not explicitly highlight these entities within the dataset. Our newly introduced dataset bridges this gap. It is designed to test mod- els’ capabilities not just in identifying entities but also in generating informed and entity-aware re- sponses. Furthermore, our proposed dataset might serve as resources for either pre-training or fine- tuning existing models, to improve their ability in recognizing entity-level real-world objects. ", "page_idx": 8, "bbox": [70, 760.9440307617188, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "", "page_idx": 8, "bbox": [306, 71.74500274658203, 526, 193.28445434570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [306, 217, 363, 229], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An- toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharif za deh, Miko- laj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a visual language model for few-shot learning .  ArXiv , abs/2204.14198. ", "page_idx": 8, "bbox": [306, 235.7635498046875, 526, 357.3575134277344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes- sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. 2023.  Open flamingo: An open-source framework for training large auto regressive vision-language models . ArXiv , abs/2308.01390. ", "page_idx": 8, "bbox": [306, 365.70855712890625, 526, 454.426513671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "Yingshan Chang, Mridu Baldevraj Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2021.  Webqa: Multihop and multimodal qa . 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 16474–16483. ", "page_idx": 8, "bbox": [306, 462.7775573730469, 526, 518.6174926757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023. Can pre-trained vision and language models answer visual information-seeking questions? In  EMNLP . ", "page_idx": 8, "bbox": [306, 526.9696044921875, 526, 571.8505249023438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "Aakanksha Chowdhery et al. 2022.  Palm: Scaling lan- guage modeling with pathways .  J. Mach. Learn. Res. , 24:240:1–240:113.", "page_idx": 8, "bbox": [306, 580.2015380859375, 526, 614.12451171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. 2023.  Instruct blip: Towards general-purpose vision-language models with instruction tuning . ArXiv , abs/2305.06500. ", "page_idx": 8, "bbox": [306, 622.4755859375, 526, 689.2755126953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Michael J. Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In  WMT@ACL . ", "page_idx": 8, "bbox": [306, 697.6265869140625, 526, 731.5494995117188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Z ett le moyer. 2023.  Qlora: Efficient finetuning of quantized llms .  ArXiv , abs/2305.14314. ", "page_idx": 8, "bbox": [306, 739.9005737304688, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "Joseph L. Fleiss. 1971.  Measuring nominal scale agree- ment among many raters.  Psychological Bulletin , 76:378–382. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016.  Making the v in vqa matter: Elevating the role of image understanding in visual question answering .  International Journal of Computer Vision , 127:398 – 414. Liangke Gui, Borui Wang, Qiuyuan Huang, Alexan- der G. Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021.  Kat: A knowledge augmented transformer for vision-and-language . In  North American Chapter of the Association for Computational Linguistics . Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020.  Realm: Retrieval- augmented language model pre-training . ArXiv , abs/2002.08909. Darryl Hannan, Akshay Jain, and Mohit Bansal. 2020. Many modal qa: Modality disambiguation and qa over diverse inputs . In  AAAI Conference on Artificial Intelligence . Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel- wal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. 2023.  Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities .  ArXiv , abs/2302.11154. Drew A. Hudson and Christopher D. Manning. 2019. Gqa: A new dataset for real-world visual reason- ing and compositional question answering .  2019 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR) , pages 6693–6702. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus . IEEE Transactions on Big Data , 7:535–547. M. G. Kendall. 1938.  A new measure of rank correla- tion .  Biometrika , 30:81–93. M. G. Kendall, Alan L. Stuart, and J. Keith Ord. 1995. Kendall’s advanced theory of statistics .  Journal of the American Statistical Association , 90:398. William Knight. 1966.  A computer method for calcu- lating kendall’s tau with ungrouped data .  Journal of the American Statistical Association , 61:436–439. Jing Yu Koh, Ruslan Salak hut dino v, and Daniel Fried. 2023.  Grounding language models to images for multimodal inputs and outputs . Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G. Moreno, and Jesús Lovón-Melgarejo. 2022.  Viquae, a dataset for knowledge-based visual question an- swering about named entities .  Proceedings of the 45th International ACM SIGIR Conference on Re- search and Development in Information Retrieval . ", "page_idx": 9, "bbox": [70, 72.61956787109375, 290, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023.  Blip-2: Boots trapping language-image pre-training with frozen image encoders and large language models .  ArXiv , abs/2301.12597. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai- Wei Chang, and Jianfeng Gao. 2021. Grounded language-image pre-training .  2022 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR) , pages 10955–10965. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In  ACL 2004 . Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.  Improved baselines with visual instruc- tion tuning .  ArXiv , abs/2310.03744. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning . ArXiv , abs/2304.08485. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.  Ok-vqa: A visual question answering benchmark requiring external knowledge .  2019 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 3190–3199. Thomas Mensink, Jasper R. R. Uijlings, Lluís Castrejón, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, Andre F. de Araújo, and Vittorio Ferrari. 2023.  Ency- clopedic vqa: Visual questions about detailed proper- ties of fine-grained categories .  2023 IEEE/CVF In- ter national Conference on Computer Vision (ICCV) , pages 3090–3101. Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun- Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, and Anuj Kumar. 2023.  Anymal: An efficient and scalable any-modality augmented language model .  ArXiv , abs/2309.16058. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In  ACL . Amy Pu, Hyung Won Chung, Ankur P. Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021.  Learning compact metrics for mt . In  Conference on Empirical Methods in Natural Language Processing . Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vanden he nde, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. 2023a. Filtering, distillation, and hard negatives for vision-language pre-training. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition , pages 6967–6977. ", "page_idx": 9, "bbox": [306, 72.61956787109375, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vanden he nde, Yash J. Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Kumar Mahajan. 2023b. Filtering, distillation, and hard negatives for vision-language pre-training . 2023 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR) , pages 6967–6977. ", "page_idx": 10, "bbox": [70, 72.61956787109375, 290, 150.37852478027344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.  Learn- ing transferable visual models from natural language supervision . In  International Conference on Machine Learning . ", "page_idx": 10, "bbox": [70, 160.15655517578125, 290, 237.9144744873047], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "Vipula Rawte, A. Sheth, and Amitava Das. 2023.  A survey of hallucination in large foundation models . ArXiv , abs/2309.05922. ", "page_idx": 10, "bbox": [70, 247.69256591796875, 290, 281.61553955078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 120, "type": "text", "text": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schr a mow ski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kac z marc zyk, and Jenia Jitsev. 2022.  Laion- 5b: An open large-scale dataset for training next gen- eration image-text models .  ArXiv , abs/2210.08402. ", "page_idx": 10, "bbox": [70, 291.3935546875, 290, 380.11151123046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 121, "type": "text", "text": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A benchmark for visual question answer- ing using world knowledge . In  European Conference on Computer Vision . ", "page_idx": 10, "bbox": [70, 389.8895568847656, 290, 445.7295227050781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 122, "type": "text", "text": "Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020.  Bleurt: Learning robust metrics for text gen- eration . In  Annual Meeting of the Association for Computational Linguistics . ", "page_idx": 10, "bbox": [70, 455.507568359375, 290, 500.3895263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 123, "type": "text", "text": "Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.  Mimoqa: Multimodal input multimodal output question answering . In  North American Chapter of the Association for Computational Linguistics . ", "page_idx": 10, "bbox": [70, 510.1676025390625, 290, 566.0084838867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 124, "type": "text", "text": "Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. 2021.  Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning .  Proceedings of the 44th International ACM SIGIR Conference on Re- search and Development in Information Retrieval . ", "page_idx": 10, "bbox": [70, 575.7865600585938, 290, 642.5855102539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 125, "type": "text", "text": "Krishna Srinivasan, Karthik Raman, Anupam Samanta, Ling-Yen Liao, Luca Bertelli, and Michael Bender- sky. 2022.  Quill: Query intent with large language models using retrieval augmentation and multi-stage distillation . In  Conference on Empirical Methods in Natural Language Processing . ", "page_idx": 10, "bbox": [70, 652.3645629882812, 290, 719.1635131835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 126, "type": "text", "text": "Kai Sun, Y. Xu, Hanwen Zha, Yue Liu, and Xinhsuai Dong. 2023.  Head-to-tail: How knowledgeable are large language models (llm)? a.k.a. will llms replace knowledge graphs?  ArXiv , abs/2308.10168. ", "page_idx": 10, "bbox": [70, 728.9415893554688, 290, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 127, "type": "text", "text": "Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Han- naneh Hajishirzi, and Jonathan Berant. 2021.  Mul- timodalqa: Complex question answering over text, tables and images .  ArXiv , abs/2104.06039. Hugo Touvron et al. 2023.  Llama 2: Open foundation and fine-tuned chat models .  ArXiv , abs/2307.09288. Maria Tsim po uk ell i, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models . In  Neural Information Processing Systems . Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel. 2016.  Fvqa: Fact-based visual question answering .  IEEE Transactions on Pattern Analysis and Machine Intelligence , 40:2413– 2427. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. Cogvlm: Visual expert for pretrained language mod- els .  ArXiv , abs/2311.03079. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat- Seng Chua. 2023.  Next-gpt: Any-to-any multimodal llm .  ArXiv , abs/2309.05519. Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023a. Inference with reference: Lossless acceleration of large language models . ArXiv , abs/2304.04487. Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Anand Kort hik anti, Weili Nie, De-An Huang, Linxi (Jim) Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mingyan Liu, Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023b.  Re- vilm: Retrieval-augmented visual language model for zero and few-shot image captioning . ArXiv , abs/2302.04858. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Z ett le moyer, and Wen tau Yih. 2023.  Retrieval- augmented multimodal language modeling .  ArXiv , abs/2211.12561. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023.  mplug-owl2: Revolutionizing multi-modal large language model with modality col- laboration . Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A sur- vey on multimodal large language models .  ArXiv , abs/2306.13549. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.  Minigpt-4: Enhancing vision-language understanding with advanced large language models .  ArXiv , abs/2304.10592. ", "page_idx": 10, "bbox": [306, 72.61956787109375, 526, 768.2875366210938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 128, "type": "text", "text": "A More Details about the Dataset Building ", "text_level": 1, "page_idx": 11, "bbox": [70, 71, 298, 84], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 129, "type": "text", "text": "More details about the dataset building process are shown in Figure  5 . ", "page_idx": 11, "bbox": [70, 92.06098175048828, 376.3970642089844, 105.20645141601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 130, "type": "image", "page_idx": 11, "img_path": "layout_images/SnapNTell_11.jpg", "bbox": [84, 116, 503, 488], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Source: Wikipedia\n\ney WIKIPEDIA\n\n‘a ‘The Free Encyclopedia\n\nContents hide\n\n(Top)\nToponym\nHistory\nGeography\nGeology\nEcology\nActivities\n\nKnowledge:\nGeneral intro\n\nvvvvy\n\nIn popular cutture\nSee also\n\nCitations\n\nGeneral references\n\nExternal links\n\nKnowledge:\nToponym\n\nQ Search Wikipedia Search\nEntity: Yosemite National Park\n\nArticle Talk\n\nFrom Wikipedia, the free encyclopedia\n\n\"Yosemite\" redirects here. For other uses, see Yosemite (disambiguatio\n\nYosemite National Park (/jou'semttl/ yoh-SEl-ih-tee!)) is a national park in California.\" tis\nbordered on the southeast by Sierra National Forest and on the northwest by Stanisiaus National\nForest. The park is managed by the National Park Service and covers 759,620 acres (1,187 sq mi;\n3,074 km?)\"*! in four counties — centered in Tuolumne and Mariposa, extending north and east to Mono\nand south to Madera. Designated a World Heritage Site in 1984, Yosemite is internationally recognized\nfor its granite cliffs, waterfalls, clear streams, giant sequoia groves, lakes, mountains, meadows,\nglaciers, and biological diversity.'*) Almost 95 percent of the park is designated wilderess.'\"! Yosemite\nis one of the largest and least fragmented habitat biocks in the Sierra Nevada.\n\nIts geology is characterized by granite and remnants of older rock. About 10 million years ago, the\nSierra Nevada was uplifted and tilted to form its unique slopes, which increased the steepness of\nstream and river beds, forming deep, narrow canyons. About one million years ago glaciers formed at\nhigher elevations. They moved downslope, cutting and sculpting the U-shaped Yosemite Valley.!°!\n\nEuropean American settlers first entered the valley in 1851. Other travelers entered earlier, but James\nD. Savage is credited with discovering the area that became Yosemite National Park.!\"°l Native\nAmericans had inhabited the region for nearly 4,000 years, although humans may have first visited as\nJong as 8,000 to 10,000 years ago.!\"l\"2)\n\nYosemite was critical to the development of the concept of national parks. Galen Clark and others\nlobbied to protect Yosemite Valley from development, ultimately leading to President Abraham Lincoln's\nsigning of the Yosemite Grant of 1864 that declared Yosemite as federally preserved land.!\"°! in 1890,\nJohn Muir led a successful movement to motivate Congress to establish Yosemite Valley and its\nsurrounding areas as a National Park. This helped pave the way for the National Park System.{\"°l\nYosemite draws about four million visitors annually.\"4! Most visitors spend the majority of their time in\nthe valley's seven square miles (18 km?).(°! The park set a visitation record in 2016, surpassing five\nmillion visitors for the first time.!\"5)\n\nToponym {esi}\n\nThe word Yosemite (derived from yohhe'meti, \"they are Killers” in Miwok) historically referred to the\nname that the Miwok gave to the Ahwahneechee People, the resident indigenous tribe.{\"®Il'7\"9]\nPreviously, the region had been called \"Ahwahnee” (\"big mouth”) by its only indigenous inhabitants, the\nAhwahneechee.|'®! The term Yosemite in Miwok is easily confused with a similar term for \"grizzly\nbear\", and is still a common misconception.!'61i19)\n\n%p 73 languages v\n\nRead Edit Viewhistory Tools v\n\nCoordinates: @ 37°44'33'N 119°3215°W @))\n\nYosemite National Park\n\nLocation in California\n® Show map of Califoria\n© Show map of the United States\n\nFcolumne, Mariposa, Mono\nMadera Counties, California,\nUnited States\n\n(Nearest city Manposa Galfoma\n\nKnowledge:\nLocation\n", "vlm_text": "The image is a screenshot of the Wikipedia page for Yosemite National Park. It includes various pieces of information:\n\n- **Entity**: The title \"Yosemite National Park\" is highlighted.\n- **Knowledge: General intro**: The introductory paragraph provides a general overview of Yosemite National Park, mentioning its location in California, management by the National Park Service, and notable geographic and geological features.\n- **Knowledge: Toponym**: A section on the origin of the park's name, noting its derivation from the Miwok language and its historical context.\n- **Knowledge: Location**: A map showing the location of Yosemite within California, and specific counties mentioned (Tuolumne, Mariposa, Mono, and Madera Counties).\n- **Source**: The source is identified as Wikipedia through its logo and formatting style.\n"}
{"layout": 131, "type": "text", "text": "Figure 5: The pertinent information collected during dataset building, i.e., from Wikipedia for each entity, which includes the summary of the general introduction, toponym, lococation information, and so on. ", "page_idx": 11, "bbox": [70, 496.8475646972656, 526, 520.8075561523438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 132, "type": "text", "text": "B More Details about the Filtering Process ", "text_level": 1, "page_idx": 11, "bbox": [70, 532, 298, 546], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 133, "type": "text", "text": "More details about the filtering process are shown in Table  8 . ", "page_idx": 11, "bbox": [70, 553.4170532226562, 337.2552490234375, 566.5625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 134, "type": "text", "text": "C Types of Questions ", "text_level": 1, "page_idx": 11, "bbox": [70, 576, 189, 590], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 135, "type": "text", "text": "More introduction of different types of question in the SnapNTell dataset are shown Table  9 . ", "page_idx": 11, "bbox": [70, 596.9010009765625, 473.7715759277344, 610.0464477539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 136, "type": "text", "text": "D Method ", "text_level": 1, "page_idx": 11, "bbox": [70, 619, 133, 632], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 137, "type": "text", "text": "In this section, we will introduce the details of our proposed retrieval-augmented multimodal LLM model. The architecture of our model is shown in Figure  7 . Our model can be considered twofold: (1)  Retrieval augmentation . Given the input image-question pair, we retrieve useful entity-centric information within knowledge sources. (2)  Entity-centric knowledge-based answer generation . The retrieved information will be combined with the image and question together to generate the answer. More details are introduced in the following sections. ", "page_idx": 11, "bbox": [70, 640.3839721679688, 526, 721.2754516601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 138, "type": "text", "text": "D.1 Retrieval Augmentation ", "text_level": 1, "page_idx": 11, "bbox": [70, 730, 210, 743], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 139, "type": "text", "text": "The retrieval augmentation process can be subdivided into three distinct steps: (i) Semantic region extraction via language-guided object detection, (ii) Entity recognition via image retrieval, and (iii) ", "page_idx": 11, "bbox": [70, 747.39501953125, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 140, "type": "text", "text": "Table 8: Filtering statistics of the entity dataset. [1st Wiki filtering]: removing ones without wiki page. [2nd Google filtering]: removing ones without enough images via google search API. [3rd Wiki filtering]: removing entity name with ambiguous wiki pages. ", "page_idx": 12, "bbox": [70, 70.029541015625, 525, 105.94451141357422], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 141, "type": "image", "page_idx": 12, "img_path": "layout_images/SnapNTell_12.jpg", "bbox": [74, 113, 520, 474.75], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Main category Original Entity —_— 1st Wiki filtering | 2nd Google filtering 3rd Wiki filtering\n\nlandmark 1595 1000 899 753\npainting 1057 367 358 288\nsculpture 300 164 164 134\nfood 883 338 337 271\nfruit 361 236 233 180\nvegetable 389 290 286 214\nmammal 718 633 619 434\nhibian 211 148 139 124\ninsect 366 179 176 145\nfish 1089 1054 987 4722\nCategory bird 739 546 545 480\nreptile 279 232, 231 210\ncelebrity 1514 1484 1466 732\ninstrument 477 375 368 277\nplant 606 601 593 489\nelectronics 432 354 342 269\ntool 801 213 209 150\ntransportation 334 296 290 227\nsport 694 478 464 395\nbook 1030 826 717 645\nhousehold 475 319) 299 221\ncar 500 320 320 208\nSummary 22 14910 10453 10102 7568\n\nGoogle eiffel tower L$egn\n\nTime | Creative Commons licenses\n\na @-i.~ @- G-—- @- G~ @\nnight drawing france wallpaper paris sketch\nquery 5\n\nEiffel Tower }—> j\n\nGoogle image\nsearch API\n\n__»| Human\nFiltering\n\nFind the ones\nwith CC license BPoabey\n\n", "vlm_text": "The image consists of two main parts:\n\n1. **Table of Categories and Filtering:**\n   - It shows different main categories like landmarks, paintings, food, etc.\n   - Columns display the number of entities at different filtering stages: Original Entity, 1st Wiki filtering, 2nd Google filtering, and 3rd Wiki filtering.\n   - The table summarizes entities and their reductions through filtering processes, totaling 14,910 originally down to 7,568 after the third filtering.\n\n2. **Google Image Search Example:**\n   - Illustrates searching for \"Eiffel Tower\" using Google Image Search API.\n   - Emphasizes using the \"Creative Commons licenses\" filter to find images.\n   - Indicates a process involving entity queries, Creative Commons license filtering, and human filtering to select images.\n\nThis image appears to represent a workflow or study involving categorization, data filtering, and image selection procedures."}
{"layout": 142, "type": "image", "page_idx": 12, "img_caption": "Figure 6: Collecting images for building the evaluation dataset. Licenses: CC Public domain, CC Attribute, AA Sharealike, CC Noncommercial, or CC Nonderived licenses. Metadata: image URLs, source page URLs, renamed image names, and the corresponding Wikipedia page URL. ", "bbox": [70, 475.25, 526, 621], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 143, "type": "text", "text": "Knowledge retrieval via multi-source aggregation. ", "page_idx": 12, "bbox": [70, 643.2470092773438, 290.0843811035156, 656.3924560546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 144, "type": "text", "text": "Semantic Region Extraction via Language-Guided Object Detection Due to the presence of entities within the image that occupy only a portion of the available space, employing a comprehensive image-level entity recognition approach may lead to a decrease in recognition performance. Instead, we opt to initially extract the image region containing the entity and utilize this specific region in subsequent recognition processes to enhance accuracy. During this phase, we leverage a language-guided object detection model, i.e., GLIP ( Li et al. ,  2021 ), to extract meaningful regions from complex images. This approach helps precisely identify and extract image regions directly relevant to specific textual queries. It accomplishes this by understanding the context of the query and adjusting its object detection method to find the most ", "page_idx": 12, "bbox": [70, 665.706298828125, 525, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 145, "type": "table", "page_idx": 13, "img_path": "layout_images/SnapNTell_13.jpg", "bbox": [71, 90, 521, 354], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Types of questions\n\nDefinition\n\nStatic facts (absolute\nfacts, discrete facts)\n\nThese are objective facts that are concrete and are not contingent on other conditions.\nThey can usually be answered with a short, unique answer. For example: When was\nBarack Obama born?\n\nNarrative facts\n\nThese facts encompass comprehension of larger contexts (e.g., song lyrics, movie plot,\nhistorical events). They are factual in the sense that the content of the narrative should\naccurately reflect the source material or events, but a correct answer is usually not unique,\nas they can vary in their level of detail and focus. For example: What is the plot of “The\nGodfather”?\n\nDynamic facts\n\nThese are facts that are subject to change over time. For example: What is the Yelp\ncustomer rating of the Eleven Madison Park restaurant in NYC?\n\nProcedural facts\n\nThese are usually answers to “how” questions, outlining a sequence of steps to accom-\nplish a task. While the steps may not be unique and could be subjective, in many cases,\nan answer can still be classified as logical (factual) or nonsensical (a hallucination). Note\nthat these facts can overlap with dynamic facts or narrative facts. For example, How do\nyou check the battery level of my Ray-Ban Stories Glasses?\n\nSubjective facts\n(opinion-based facts)\n\nThese “facts” are not objective, indisputable facts, but are based on individual perspec-\ntives or experiences. Recommendations fall in this category. While there’s generally no\nsingle correct answer to questions seeking subjective facts, it still requires the system\nto understand the topic and provide reasonable answers grounded by world facts. For\nexample: Where should I visit Tokyo next month?\n", "vlm_text": "The table categorizes types of questions into five groups and provides definitions and examples for each:\n\n1. **Static facts (absolute facts, discrete facts):**\n   - **Definition:** Objective and concrete, with unique answers not contingent on other conditions.\n   - **Example:** \"When was Barack Obama born?\"\n\n2. **Narrative facts:**\n   - **Definition:** Comprehension of larger contexts like song lyrics, movie plots, or historical events. Answers may vary in detail and focus.\n   - **Example:** \"What is the plot of 'The Godfather'?\"\n\n3. **Dynamic facts:**\n   - **Definition:** Subject to change over time.\n   - **Example:** \"What is the Yelp customer rating of the Eleven Madison Park restaurant in NYC?\"\n\n4. **Procedural facts:**\n   - **Definition:** Answers to \"how\" questions, outlining steps to accomplish tasks. May overlap with other fact types.\n   - **Example:** \"How do you check the battery level of my Ray-Ban Stories Glasses?\"\n\n5. **Subjective facts (opinion-based facts):**\n   - **Definition:** Based on individual perspectives or experiences, not objective or indisputable.\n   - **Example:** \"Where should I visit in Tokyo next month?\""}
{"layout": 146, "type": "text", "text": "important image areas. This step enables the system to better understand the query’s context, resulting in more accurate and con textually meaningful region extraction. ", "page_idx": 13, "bbox": [70, 387.9119873046875, 526, 414.6064758300781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 147, "type": "text", "text": "Entity Recognition via Image Retrieval To accomplish this goal, we begin by constructing a similarity index using CLIP embeddings, specifically employing Faiss ( Johnson et al. ,  2017 ) as our indexing tool. Our indexing database is established based on the WIT dataset ( Srinivasan et al. ,  2021 ). This database follows a key-value mapping structure, where the keys represent CLIP ViT-B/32 image embeddings, and the corresponding text descriptions serve as the values. Faiss, known for its efficiency in similarity search, is utilized for indexing ( Johnson et al. ,  2017 ). ", "page_idx": 13, "bbox": [70, 430.5342712402344, 526, 511.8184814453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 148, "type": "text", "text": "Once the indexing database is set up, we are ready to proceed with the query process. Given an input query image, denoted as    $I$   (which is the entity image region extracted in the preceding step), we perform a    $k$  -nearest neighbor retrieval based on cosine similarity between the embeddings of the query image and those of the database images. The retriev  outcomes are represented as    $\\mathcal{R}(I)=\\{(i_{1},c_{1})\\,,\\cdot\\cdot\\cdot\\,,(i_{k},c_{k})\\}$  , where for each    $j$   within the range of  1  to  k ,  $i_{j}$   and  $c_{j}$   correspond to the retrieved image and its associated caption, respectively. Subsequently, by using the extracted image region as input for a search in the indexing database, we identify the entity within the extracted image region. This identification is achieved by comparing it with the most similar images retrieved from the indexing database, ultimately resulting in image-level entity recognition. ", "page_idx": 13, "bbox": [70, 514.6909790039062, 526, 636.23046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 149, "type": "text", "text": "Knowledge Retrieval via Multi-Source Aggregation Given the wide array of questions users may pose, we need to obtain additional information to compile the necessary resources for crafting accurate responses. Furthermore, certain queries may demand the latest information, which is not readily available within pre-existing databases or knowledge graphs. In such cases, we rely on external sources of knowledge, such as online references, to gather essential data, encompassing elements like “year built,\" “description,\" and other pertinent details. To accomplish this, we leverage Knowledge Graph (KG) and conduct web searches to access relevant knowledge connections. This approach enables us to acquire a wealth of information concerning the specified image region, thereby bolstering our capacity to grasp and contextual ize the extracted content effectively. ", "page_idx": 13, "bbox": [70, 652.1572875976562, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 150, "type": "image", "page_idx": 14, "img_path": "layout_images/SnapNTell_14.jpg", "img_caption": "Figure 7: The architecture of our SnapNTell model. The input to the model is an image-question pair, and our model first uses retrieval augmentation to retrieve useful information regarding the entity in the image. Then, the retrieved information is combined with the question as input to the word embedding layer, where the text embeddings will be combined with image-projected embeddings as the input to LLM, which finally generates a knowledgeable answer as the output. ", "bbox": [68, 68, 527, 382], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "tl tl tl tl tl tl tl\n\naaaoooa\n\nWord Embedding Layer\n\nRetrieval Retrieved 1\nAugmentation Information |} J ¢77-7>>7=7>ooo== :\ni} 4 NoGrad |\nImage t 1 ' '\nEncoder ' Ena mace “Eiffel Tower” <i> t Forward Pass\n' Recognition >a | ' | Backward Pass:\nModel i | tesdbeeweseewscecd\n\n' Database '\n\nInput Question\n", "vlm_text": "The image is a diagram of the SnapNTell model architecture. \n\n1. **Input**: It begins with an image-question pair.\n2. **Image Processing**: \n   - **Image Encoder**: Processes the input image.\n   - **Entity Detection Model**: Detects entities in the image.\n   - **Entity Recognition Model**: Identifies recognized entities (e.g., \"Eiffel Tower\").\n   - **Retrieval Augmentation**: Uses identified entities to retrieve additional information from a database.\n   \n3. **Text Processing**:\n   - The retrieved information and the question are combined.\n   - This combined input is processed by the **Word Embedding Layer**.\n\n4. **Image and Text Fusion**:\n   - **Projection Layers**: Combine image features as embeddings.\n   - These embeddings are integrated with the text embeddings.\n\n5. **Language Model Processing**:\n   - A **Large Language Model (LLM)** takes the embeddings to generate an answer to the input question.\n\nThe diagram includes pathways for forward and backward passes indicating model training and inference flow."}
{"layout": 151, "type": "text", "text": "D.2 Entity-centric Knowledge-based Answer Generation ", "text_level": 1, "page_idx": 14, "bbox": [70, 406, 342, 420], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 152, "type": "text", "text": "Following the preceding step, where we’ve gathered insightful information from diverse sources, we now proceed to the second phase: determining how to integrate the input image, the question, and the retrieved information in order to produce a knowledge-driven response. ", "page_idx": 14, "bbox": [70, 428.3370056152344, 526, 468.58148193359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 153, "type": "text", "text": "Our approach is illustrated in Figure  7 . Our strategy for improving the model’s multimodal compre- hension entails pre-training a LLM using paired multimodal data, which comprises images alongside corresponding textual descriptions. To achieve this, we draw inspiration from  Moon et al.  ( 2023 ) and create lightweight adapters for each modality. These adapters facilitate the transformation of inputs into the text token embedding space of a designated LLM. ", "page_idx": 14, "bbox": [70, 470.81298828125, 526, 538.1554565429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 154, "type": "text", "text": "Our approach transforms the text token embedding space of the LLM into a unified token embedding space, where tokens can represent either textual or image content. The number of token embeddings allocated to each input modality is predetermined for each adapter, ranging from 64 to 256. Throughout the alignment training process, we keep the model parameters of the underlying LLM frozen. This approach not only accelerates convergence compared to training the model from scratch but also allows the model to inherit the reasoning capabilities of the LLM during inference. Additionally, to maximize feature compatibility, we employ an encoder denoted as    $g(\\cdot)$   for the image modality. This encoder has previously been aligned with a text embedding space, for instance, in the case of CLIP ( Radford et al. , 2021 ;  Schuhmann et al. ,  2022 ). For each pair of text and image, represented as    $(\\mathbf{X}_{\\mathsf{t e x t}},\\mathbf{X}_{\\mathsf{i m a g e}})$  , we align them using specific objectives along with a projection module, such as the Perceiver Resampler ( Alayrac et al. ,  2022 ) for the vision encoder. ", "page_idx": 14, "bbox": [70, 540.3870239257812, 526, 689.0244140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 155, "type": "equation", "text": "\n$$\np(\\mathbf{X}_{\\mathrm{text}}|\\mathbf{X}_{\\mathrm{image}})=\\prod_{i=1}^{L}p_{\\theta}(\\mathbf{X}_{\\mathrm{text}}^{[i]}|\\mathbf{Z}_{\\mathrm{image}},\\mathbf{Z}_{\\mathrm{text}}^{[1:i-1]})\n$$\n ", "text_format": "latex", "page_idx": 14, "bbox": [180, 705, 414, 743], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 156, "type": "equation", "text": "\n$$\n\\mathbf{Z}_{\\mathsf{i m a g e}}={\\tt P r o j}_{\\theta}\\big(h_{\\tt l a t e n t s},g(\\mathbf{X}_{\\mathsf{i m a g e}})\\big)\n$$\n ", "text_format": "latex", "page_idx": 14, "bbox": [206, 760, 388, 775], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 157, "type": "text", "text": "E More Related Works ", "text_level": 1, "page_idx": 15, "bbox": [70, 71, 199, 84], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 158, "type": "text", "text": "Knowledge-based VQA Various vision-language tasks often require knowledge to answer questions based on image content and have evolved in recent years. Beginning with datasets like FVQA ( Wang et al. ,  2016 ), which extracted facts from pre-established knowledge bases, the field has progressed to more challenging ones like the OK-VQA dataset ( Marino et al. ,  2019 ), encompassing diverse knowledge categories. Multi Modal QA ( Talmor et al. ,  2021 ) introduced complexity with questions demanding cross-modal reasoning over snippets, tables, and images. The successor of OK-VQA, AOK-VQA ( Schwenk et al. ,  2022 ), raises the bar by providing questions that transcend simple knowledge base queries. Many Modal QA ( Hannan et al. ,  2020 ) shifts the focus to answer modality selection, MIMOQA ( Singh et al. ,  2021 ) emphasizes multimodal answer extraction, and WebQA ( Chang et al. ,  2021 ) introduces real-world knowledge-seeking questions, albeit with some limitations regarding entity categorization and granularity. More comparison details are introduced in Section  3.5 . ", "page_idx": 15, "bbox": [70, 92.26530456542969, 526, 241.29550170898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 159, "type": "text", "text": "Multimodal LLMs Expanding text-only LLMs to interpret visual information typically involves in- tegrating a visual encoder with a frozen LLM, using extensive image captioning datasets for alignment ( Koh et al. ,  2023 ;  Wu et al. ,  2023 ;  Chowdhery et al. ,  2022 ). This integration can be accomplished through methods such as adapter-based tuning ( Alayrac et al. ,  2022 ), which fine-tunes a small portion of the model to process visual inputs, or prefix tuning ( Tsim po uk ell i et al. ,  2021 ), where trained prefixed vectors are inputted to guide the frozen LLM towards con textually relevant text outputs based on the visual data. These techniques allow LLMs to maintain their linguistic prowess while gaining visual understanding without full model retraining ( Yin et al. ,  2023 ). ", "page_idx": 15, "bbox": [70, 248.67027282714844, 526, 357.0524597167969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 160, "type": "text", "text": "Retrieval augmented LLM Several prior approaches have investigated retrieval-augmented in the text-only setting or image captioning tasks.  Guu et al.  ( 2020 ) augmented language model pre training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre training, fine-tuning, and inference.  Srinivasan et al. ( 2022 ) demonstrated that retrieval augmentation of queries provides LLMs with valuable additional context, enabling improved understanding.  Yasunaga et al.  ( 2023 ) proposed a retriever to retrieve relevant multimodal documents from external memory and use the generator to make predictions for the input. Yang et al.  ( 2023a ) proposed an accelerator to losslessly speed up LLM inference with references through retrieval.  Yang et al.  ( 2023b ) introduced a retrieval-augmented visual language model, built upon the Flamingo ( Alayrac et al. ,  2022 ), which supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image captioning. Another related work by  Gui et al.  ( 2021 ) integrated implicit and explicit knowledge in an encoder-decoder architecture for jointly reasoning over both knowledge sources during answer generation. ", "page_idx": 15, "bbox": [70, 364.4272766113281, 526, 540.555419921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 161, "type": "text", "text": "Open-domain visual entity recognition Hu et al.  ( 2023 ) introduced Open-domain Visual Entity Recognition (OVEN) for linking images to Wikipedia entities through text queries.  Chen et al.  ( 2023 ) presented INFOSEEK, a Visual Question Answering dataset designed for information-seeking queries. OVEN excels at entity recognition but relies on a knowledge base for entity names, while INFOSEEK primarily provides factual answers. Our research aims to bridge these gaps by generating informative paragraphs that offer context, enabling a deeper understanding beyond mere facts. ", "page_idx": 15, "bbox": [70, 547.9302978515625, 526, 629.2144775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 162, "type": "text", "text": "F More Statistics of the SnapNTell Dataset ", "text_level": 1, "page_idx": 15, "bbox": [70, 638, 297, 653], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 163, "type": "text", "text": "In Table  10  and Figure  9 , 10 , 11 , we show more statistics of the SnapNTell dataset. ", "page_idx": 15, "bbox": [70, 660.7479858398438, 428, 673.8934326171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 164, "type": "table", "page_idx": 16, "img_path": "layout_images/SnapNTell_15.jpg", "table_caption": "Figure 8: Examples from our SnapNTell dataset. ", "bbox": [94, 104, 500, 379], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Where is it located? Abel Tasman National Park is\nlocated at the northern tip of the\nSouth Island of New Zealand\nbetween Golden Bay and Tasman\nBay\n\nWhat date did the it The Acropolis Museum was\n\nopen to the public? inaugurated on June 20, 2009, after\nmany years of planning and\nconstruction\n\nWhat is the The Saint Alexander Nevsky\narchitectural style of it? | Cathedral has been built in the\nNeo-Byzantine style.\n\n", "vlm_text": "The table contains three rows, each with an image, a question, and an answer:\n\n1. **Row 1:**\n   - **Image:** Landscape with coast.\n   - **Question:** \"Where is it located?\"\n   - **Answer:** \"Abel Tasman National Park is located at the northern tip of the South Island of New Zealand between Golden Bay and Tasman Bay.\"\n\n2. **Row 2:**\n   - **Image:** Interior of a museum with visitors.\n   - **Question:** \"What date did it open to the public?\"\n   - **Answer:** \"The Acropolis Museum was inaugurated on June 20, 2009, after many years of planning and construction.\"\n\n3. **Row 3:**\n   - **Image:** Cityscape with a cathedral.\n   - **Question:** \"What is the architectural style of it?\"\n   - **Answer:** \"The Saint Alexander Nevsky Cathedral has been built in the Neo-Byzantine style.\""}
{"layout": 165, "type": "table", "page_idx": 16, "img_path": "layout_images/SnapNTell_16.jpg", "table_caption": "Table 10: Category statistics of the SnapNTell dataset. ", "bbox": [188, 453, 406, 740], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Category Number of entities\n\nlandmark 753\npainting 288\nsculpture 34\nfood 271\nfruit 80\nvegetable 214\nmammal 434\nfish 24\nbird 45\nreptile 722\namphibian 480\nCategory insect 210\ncelebrity 732\ninstrument 277\nplant 489\nelectronics 269\ntool 50\ntransportation 227\nsport 395\nbook 645\nhousehold 221\ncar 208\n\n", "vlm_text": "The table lists various categories with the corresponding number of entities in each category. Here's the summary:\n\n- **Category**: Number of Entities\n  - Landmark: 753\n  - Painting: 288\n  - Sculpture: 134\n  - Food: 271\n  - Fruit: 180\n  - Vegetable: 214\n  - Mammal: 434\n  - Fish: 124\n  - Bird: 145\n  - Reptile: 722\n  - Amphibian: 480\n  - Insect: 210\n  - Celebrity: 732\n  - Instrument: 277\n  - Plant: 489\n  - Electronics: 269\n  - Tool: 150\n  - Transportation: 227\n  - Sport: 395\n  - Book: 645\n  - Household: 221\n  - Car: 208\n\n- **Summary**: 22 categories with a total of 7,568 entities."}
{"layout": 166, "type": "image", "page_idx": 17, "img_path": "layout_images/SnapNTell_17.jpg", "img_caption": "Figure 9: Statistics of number of entities in each category. ", "bbox": [124, 81, 473, 306], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "household Jandmark\n2.9% 9.9%\nbook —— painting\n8.5% 3.8%\nsport food\n5.2% 3.6%\n\ntransportation fruit\nAG 24%\n3.0%\n\nvegetable\n\nelectronics\n\n5.7%\n\n6.5%\n\nP insect\ninstrument\n\n9.7%\nreptile\n2.8%\n\n6.3%\n", "vlm_text": "The image is a donut chart representing statistics of the number of entities in each category. Here are the categories and their corresponding percentages:\n\n- Household: 2.9%\n- Book: 8.5%\n- Sport: 5.2%\n- Transportation: 3.0%\n- Electronics: 3.6%\n- Plant: 6.5%\n- Instrument: 3.7%\n- Celebrity: 9.7%\n- Reptile: 2.8%\n- Landmark: 9.9%\n- Painting: 3.8%\n- Food: 3.6%\n- Fruit: 2.4%\n- Vegetable: 2.8%\n- Mammal: 5.7%\n- Insect: 1.9%\n- Fish: 9.5%\n- Bird: 6.3%"}
{"layout": 167, "type": "image", "page_idx": 17, "img_path": "layout_images/SnapNTell_18.jpg", "img_caption": "Figure 10: Statistics of all pageviews for all categories. ", "bbox": [123, 320, 475, 546], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "book\n\nsport\n2.9%\ntransportation “4\n1.5%\n\nplant\n\n1.9% /\ncar /\n\ncelebrity\n49.3%\n\nlandmark\n\n9.1%\n\nsculpture\n\n2.5%\nfood\n2.6%\n\nfruit\n\n2.3%\n\n\\ vegetable\n\n2.5%\nmammal\n\n5.8%\nfish\n\n", "vlm_text": "The image is a donut chart showing the percentage distribution of pageviews across various categories. Here's the breakdown:\n\n- Celebrity: 49.3%\n- Landmark: 9.1%\n- Book: 5.7%\n- Mammal: 5.8%\n- Sport: 2.9%\n- Car: 2.7%\n- Food: 2.6%\n- Vegetable: 2.5%\n- Sculpture: 2.5%\n- Fruit: 2.3%\n- Fish: 2.2%\n- Plant: 1.9%\n- Transportation: 1.5%"}
{"layout": 168, "type": "image", "page_idx": 17, "img_path": "layout_images/SnapNTell_19.jpg", "bbox": [124, 560, 474, 735], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "landmark\npainting\nsculpture\nfood\n\nfruit\nvegetable\nmammal\namphibian\ninsect\n\nfish\n\nbird\nreptile\ncelebrity\ninstrument\nplant\nelectronics\ntool\ntransportation\nsport\nbook\nhousehold\ncar\n\n1000\n\n2000\n\n3000\n\n4000\n\n500¢\n", "vlm_text": "The image is a horizontal bar chart with various categories on the y-axis and numbers on the x-axis. The bars represent the quantity or frequency of each category. The categories listed include:\n\n- Landmark\n- Painting\n- Sculpture\n- Food\n- Fruit\n- Vegetable\n- Mammal\n- Amphibian\n- Insect\n- Fish\n- Bird\n- Reptile\n- Celebrity\n- Instrument\n- Plant\n- Electronics\n- Tool\n- Transportation\n- Sport\n- Book\n- Household\n- Car\n\nThe \"celebrity\" category has the highest value, with \"sculpture\" and \"vegetable\" also having notable values."}
{"layout": 169, "type": "text", "text": "Figure 11: Average pageview per entity within each category, where average pageview is defined as the sum of pageviews/ number of entity. ", "page_idx": 17, "bbox": [70, 751.529541015625, 524.41015625, 775.489501953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 170, "type": "text", "text": "G Some Result Examples and Human Evaluation ", "text_level": 1, "page_idx": 18, "bbox": [70, 70, 332, 85], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 171, "type": "text", "text": "In Table  11 , we showed several example result by different models, and the corresponding human evaluation results. ", "page_idx": 18, "bbox": [70, 92.65802764892578, 526, 119.35244750976562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 172, "type": "text", "text": "Table 11: Examples of answers generated by different models, where Ground-truth, BLIP2, MiniGPT4, Open- Flamingo, Instruct BLIP, LLaVA, SnapNTell are assigned as M0, M1, ..., M6 in rating. ", "page_idx": 18, "bbox": [70, 127.51458740234375, 526, 151.47447204589844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 173, "type": "image", "page_idx": 18, "img_path": "layout_images/SnapNTell_20.jpg", "bbox": [200, 164, 328, 237.75], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "", "vlm_text": "The image depicts a scenic landscape with a coastal view. It features lush green vegetation in the foreground, a sandy beach area, and water. In the background, there are hills or low mountains under a partly cloudy sky."}
{"layout": 174, "type": "table", "page_idx": 18, "img_path": "layout_images/SnapNTell_21.jpg", "bbox": [100, 238.25, 492, 745], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Question\n\nWhere is the attraction located?\n\nGround-truth (M0)\n\nBLIP-2 (M1)\n\nMini-GPT4 (M2)\n\nOpen-Flamingo (M3)\nInstruct-BLIP (M4)\n\nLLaVA (MS)\n\nSnapNTell\n(M6)\n\n(Ours)\n\nAbel Tasman National Park is located at the northern tip of the South Island of\nNew Zealand between Golden Bay and Tasman Bay\n\ncalifornia\n\nThe image shows a beach with a body of water in the foreground and trees and\nmountains in the background.\n\nIt is located in New Zealand.\n\nbeach 1\n\nThe image is located at the beach, which is surrounded by a forest of trees.\n\nThe image is not located in the Republic of Altai, but rather in the South Island\nof New Zealand. The location is the Abel Tasman National Park, specifically\nthe area around Awaroa Beach.\n\nHuman Rating\n\nMO = M6 > M3 > M1 = M2=M5 > M4\n\nImage\n\nQuestion\n\nWhat date did it open to the public?\n\nGround-truth (MO)\n\nBLIP-2 (M1)\n\nMini-GPT4 (M2)\nOpen-Flamingo (M3)\nInstruct-BLIP (M4)\n\nLLaVA (MS)\n\nSnapNTell\n(M6)\n\n(Ours)\n\nThe Acropolis Museum was inaugurated on June 20, 2009 after many years of\nplanning and construction\n\n=\n\nJanuary 2009\n\nhe museum opened to the public on January 1, 2018\n\nTl\nThe museum opened to the public on December 12, 1987.\n2007\n\nhe museum opened to the public on April 13, 2019.\n\nTl\nThe Acropolis Museum opened to the public on June 20, 2009.\n\nHuman Rating\n\nMO = M6 > M1 > M2= M3 = M4= M5\n\n", "vlm_text": "The table contains two sections, each with an image, a question, model responses (M0-M6), and human ratings. \n\n### First Section:\n- **Image**: A landscape with trees, water, and mountains.\n- **Question**: \"Where is the attraction located?\"\n- **Responses**:\n  - Ground-truth (M0): Abel Tasman National Park, South Island of New Zealand.\n  - BLIP-2 (M1): California.\n  - Mini-GPT4 (M2): General description of a beach.\n  - Open-Flamingo (M3): New Zealand.\n  - Instruct-BLIP (M4): beach 1.\n  - LLaVA (M5): A beach surrounded by trees.\n  - SnapNTell (M6): Abel Tasman National Park, South Island of New Zealand, Awaroa Beach.\n- **Human Rating**: M0 = M6 > M3 > M1 = M2 = M5 > M4.\n\n### Second Section:\n- **Image**: Interior of a museum with modern design.\n- **Question**: \"What date did it open to the public?\"\n- **Responses**:\n  - Ground-truth (M0): Acropolis Museum, June 20, 2009.\n  - BLIP-2 (M1): January 1, 2009.\n  - Mini-GPT4 (M2): January 1, 2018.\n  - Open-Flamingo (M3): December 12, 1987.\n  - Instruct-BLIP (M4): 2007.\n  - LLaVA (M5): April 13, 2019.\n  - SnapNTell (M6): June 20, 2009.\n- **Human Rating**: M0 = M6 > M1 > M2 = M3 = M4 = M5.\n\nEach section evaluates how accurately different models can answer questions about images compared to the provided ground-truth, with ratings indicating the most accurate responses."}
{"layout": 175, "type": "text", "text": "Image ", "page_idx": 19, "bbox": [109.84871673583984, 149.61737060546875, 134, 161.42813110351562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 176, "type": "image", "page_idx": 19, "img_path": "layout_images/SnapNTell_22.jpg", "bbox": [201, 106, 345, 203], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "", "vlm_text": "The image shows a picturesque lakeside town with colorful buildings nestled against a backdrop of hills or mountains. Several boats are floating on the water in the foreground. The sky is clear with a few clouds, providing a bright and vibrant atmosphere."}
{"layout": 177, "type": "table", "page_idx": 19, "img_path": "layout_images/SnapNTell_23.jpg", "bbox": [104, 209, 492, 373], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Se MSeeevat. TV SORES ek C8OU FOCHEIEY ME SEES Pew Wy SES SLO EPO eer Se\n\nGround-truth (M0) Lake Titicaca is a large freshwater lake located in the Andes Mountains spanning\nthe borders of both Bolivia and Peru\n\nBLIP-2 (M1) a town in Peru\n\nMini-GPT4 (M2) It is a small town with colorful buildings and boats in the water.\n\nOpen-Flamingo (M3) This is the city of Puerto Natales in Chile.\nInstruct-BLIP (M4) harbor 2\n\nLLaVA (M5) It is a cityscape with boats in the water, located near a mountainous area.\nSnapNTell (Ours) It’s Lake Titicaca in the Andes Mountains.\n(M6)\n\nHuman Rating MO = M6 > M1 > M3 > M2=M5 > M4\n", "vlm_text": "The table compares different models' responses to the question \"What is the name of the view in this picture?\" regarding an image possibly of Lake Titicaca.\n\n- **Ground-truth (M0):** Describes Lake Titicaca as a large freshwater lake in the Andes, spanning Bolivia and Peru.\n- **BLIP-2 (M1):** Identifies it as a town in Peru.\n- **Mini-GPT4 (M2):** Mentions a small town with colorful buildings and boats.\n- **Open-Flamingo (M3):** Describes it as the city of Puerto Natales in Chile.\n- **Instruct-BLIP (M4):** Refers to it as \"harbor 2\".\n- **LLaVA (M5):** States it is a cityscape with boats near a mountainous area.\n- **SnapNTell (Ours, M6):** Identifies it accurately as Lake Titicaca in the Andes Mountains.\n\n**Human Rating:** M0 = M6 > M1 > M3 > M2 = M5 > M4, indicating M0 and M6 provided the best answers."}
{"layout": 178, "type": "text", "text": "Image ", "text_level": 1, "page_idx": 19, "bbox": [108, 491, 135, 504], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 179, "type": "image", "page_idx": 19, "img_path": "layout_images/SnapNTell_24.jpg", "bbox": [201, 448, 345, 545], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "", "vlm_text": "The image shows the Kapellbrücke, a famous covered wooden footbridge in Lucerne, Switzerland. It features a distinctive tower and is adorned with flowers along the sides. The bridge spans a body of water, and there are buildings and hills in the background."}
{"layout": 180, "type": "table", "page_idx": 19, "img_path": "layout_images/SnapNTell_25.jpg", "table_caption": "Question ", "bbox": [104, 546, 492, 739], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Ground-truth (MO)\n\nBLIP-2 (M1)\n\nMini-GPT4 (M2)\nOpen-Flamingo (M3)\nInstruct-BLIP (M4)\n\nLLaVA (M5)\n\nSnapNTell\n(M6)\n\n(Ours)\n\nThe Chapel Bridge is over 700 years old having been built in 1333\n\n16th century\n\nThe age of the building in the image is not known.\n\nThe age of the bridge is over 100 years old.\n\nold\n\nThe age of the building in the image is not specified. However, it is described\nas an old building, which suggests that it could be quite old or historic.\n\nThe Chapel Bridge (Chapel Bridge or Kapellbriicke) in Lucerne, Switzerland,\nis a medieval wooden bridge that dates back to the 14th century. It was built\nin 1333 and is one of the oldest covered bridges in Europe, which is over 700\nyears old now.\n\nHuman Rating\n\nM6 >= MO > M1 = M2 = M3 = M4= M5\n\n", "vlm_text": "The table presents different models' responses about the age of the Chapel Bridge and how they compare to the ground truth:\n\n- **Ground-truth (M0):** The Chapel Bridge is over 700 years old, built in 1333.\n- **BLIP-2 (M1):** 16th century.\n- **Mini-GPT4 (M2):** The age of the building in the image is not known.\n- **Open-Flamingo (M3):** The age of the bridge is over 100 years old.\n- **Instruct-BLIP (M4):** Old.\n- **LLAVA (M5):** The age is not specified but is described as old.\n- **SnapNTell (M6):** Detailed description matching the ground truth, built in 1333, over 700 years old.\n\n**Human Rating:** M6 ≥ M0 > M1 = M2 = M3 = M4 = M5, indicating M6 and M0 provide the most accurate information."}
