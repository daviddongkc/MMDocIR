{"page": 0, "image_path": "doc_images/Q18-1021_0.jpg", "ocr_text": "Constructing Datasets\nfor Multi-hop Reading Comprehension Across Documents\n\nJohannes Welbl!\n\nPontus Stenetorp*\n\nSebastian Riedel!”\n\nUniversity College London, ?Bloomsbury AI\n{j.welbl,p.stenetorp,s.riedel}@cs.ucl.ac.uk\n\nAbstract\n\nMost Reading Comprehension methods limit\nthemselves to queries which can be answered\nusing a single sentence, paragraph, or docu-\nment. Enabling models to combine disjoint\npieces of textual evidence would extend the\nscope of machine comprehension methods,\nbut currently no resources exist to train and\ntest this capability. We propose a novel task to\nencourage the development of models for text\nunderstanding across multiple documents and\nto investigate the limits of existing methods.\nIn our task, a model learns to seek and com-\nbine evidence - effectively performing multi-\nhop, alias multi-step, inference. We devise a\nmethodology to produce datasets for this task,\ngiven a collection of query-answer pairs and\nthematically linked documents. Two datasets\nfrom different domains are induced,! and we\nidentify potential pitfalls and devise circum-\nvention strategies. We evaluate two previ-\nously proposed competitive models and find\nthat one can integrate information across doc-\numents. However, both models struggle to se-\nlect relevant information; and providing doc-\numents guaranteed to be relevant greatly im-\nproves their performance. While the mod-\nels outperform several strong baselines, their\nbest accuracy reaches 54.5% on an annotated\ntest set, compared to human performance at\n85.0%, leaving ample room for improvement.\n\n1 Introduction\n\nDevising computer systems capable of answering\nquestions about knowledge described using text has\n\n‘Available at http: //qangaroo.cs.ucl.ac.uk\n\n287\n\nThe Hanging Gardens, in [Mumbai], also known as Pherozeshah\nMehta Gardens, are terraced gardens ... They provide sunset views\nover the [Arabian Sea] ..\n\nMumbai (also known as Bombay, the official name until 1995) is the\ncapital city of the Indian state of Maharashtra. It is the most\npopulous city in India ..\n\nThe Arabian Sea is a region of the northern Indian Ocean bounded\non the north by Pakistan and Iran, on the west by northeastern\nSomalia and the Arabian Peninsula, and on the east by India ..\n\nQ: (Hanging gardens of Mumbai, country, ?)\nOptions: {Iran, India, Pakistan, Somalia, ...}\n\nFigure 1: A sample from the WIKIHOP dataset where it\nis necessary to combine information spread across multi-\nple documents to infer the correct answer.\n\nbeen a longstanding challenge in Natural Language\nProcessing (NLP). Contemporary end-to-end Read-\ning Comprehension (RC) methods can learn to ex-\ntract the correct answer span within a given text\nand approach human-level performance (Kadlec et\nal., 2016; Seo et al., 2017a). However, for exist-\ning datasets, relevant information is often concen-\ntrated locally within a single sentence, emphasizing\nthe role of locating, matching, and aligning informa-\ntion between query and support text. For example,\nWeissenborn et al. (2017) observed that a simple bi-\nnary word-in-query indicator feature boosted the rel-\native accuracy of a baseline model by 27.9%.\n\nWe argue that, in order to further the ability of ma-\nchine comprehension methods to extract knowledge\nfrom text, we must move beyond a scenario where\nrelevant information is coherently and explicitly\nstated within a single document. Methods with this\ncapability would aid Information Extraction (IE) ap-\nplications, such as discovering drug-drug interac-\n\nTransactions of the Association for Computational Linguistics, vol. 6, pp. 287-302, 2018. Action Editor: Katrin Erk.\nSubmission batch: 10/2017; Revision batch: 2/2018; Published 5/2018.\n©2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\n", "vlm_text": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents \nJohannes Welbl 1 Pontus Stenetorp 1 Sebastian Riedel 1 , \n1 University College London, Bloomsbury AI { j.welbl,p.stenetorp,s.riedel } @cs.ucl.ac.uk \nAbstract \nMost Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or docu- ment. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and com- bine evidence – effectively performing multi- hop, alias multi-step, inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced,   and we identify potential pitfalls and devise circum- vention strategies. We evaluate two previ- ously proposed competitive models and ﬁnd that one can integrate information across doc- uments. However, both models struggle to se- lect relevant information; and providing doc- uments guaranteed to be relevant greatly im- proves their performance. While the mod- els outperform several strong baselines, their best accuracy reaches  $54.5\\%$   on an annotated test set, compared to human performance at  $85.0\\%$  , leaving ample room for improvement. \n1 Introduction \nDevising computer systems capable of answering questions about knowledge described using text has \nThe Hanging Gardens, in  [Mumbai] , also known as Pherozeshah  Mehta Gardens, are terraced gardens … They provide sunset views  over the  [Arabian Sea]  … Mumbai  (also known as Bombay, the ofﬁcial name until 1995) is the  capital city of the Indian state of Maharashtra. It is the most  populous city in  India  … The  Arabian Sea  is a region of the northern Indian Ocean bounded  on the north by  Pakistan  and  Iran , on the west by northeastern  Somalia  and the Arabian Peninsula, and on the east by  India  … \nQ:  (Hanging gardens of Mumbai, country, ?)   Options :  { Iran ,  India ,  Pakistan ,  Somalia ,  … } \nFigure 1: A sample from the W IKI H OP  dataset where it is necessary to combine information spread across multi- ple documents to infer the correct answer. \nbeen a longstanding challenge in Natural Language Processing (NLP). Contemporary end-to-end Read- ing Comprehension (RC) methods can learn to ex- tract the correct answer span within a given text and approach human-level performance (Kadlec et al., 2016; Seo et al., 2017a). However, for exist- ing datasets, relevant information is often concen- trated locally within a single sentence, emphasizing the role of locating, matching, and aligning informa- tion between query and support text. For example, Weissenborn et al. (2017) observed that a simple bi- nary  word-in-query  indicator feature boosted the rel- ative accuracy of a baseline model by  $27.9\\%$  . \nWe argue that, in order to further the ability of ma- chine comprehension methods to extract knowledge from text, we must move beyond a scenario where relevant information is coherently and explicitly stated within a single document. Methods with this capability would aid Information Extraction (IE) ap- plications, such as discovering drug-drug interac- tions (Gurulingappa et al., 2012) by connecting pro- tein interactions reported across different publica- tions. They would also beneﬁt search (Carpineto and Romano, 2012) and Question Answering (QA) ap- plications (Lin and Pantel, 2001) where the required information cannot be found in a single location. "}
{"page": 1, "image_path": "doc_images/Q18-1021_1.jpg", "ocr_text": "tions (Gurulingappa et al., 2012) by connecting pro-\ntein interactions reported across different publica-\ntions. They would also benefit search (Carpineto and\nRomano, 2012) and Question Answering (QA) ap-\nplications (Lin and Pantel, 2001) where the required\ninformation cannot be found in a single location.\n\nFigure 1 shows an example from WIKIPEDIA,\nwhere the goal is to identify the count ry property\nof the Hanging Gardens of Mumbai. This cannot be\ninferred solely from the article about them without\nadditional background knowledge, as the answer is\nnot stated explicitly. However, several of the linked\narticles mention the correct answer Jndia (and other\ncountries), but cover different topics (e.g. Mumbai,\nArabian Sea, etc.). Finding the answer requires\nmulti-hop reasoning: figuring out that the Hanging\nGardens are located in Mumbai, and then, from a\nsecond document, that Mumbai is a city in India.\n\nWe define a novel RC task in which a model\nshould learn to answer queries by combining ev-\nidence stated across documents. We introduce a\nmethodology to induce datasets for this task and de-\nrive two datasets. The first, WIKIHOP, uses sets of\nWIKIPEDIA articles where answers to queries about\nspecific properties of an entity cannot be located in\nthe entity’s article. In the second dataset, MEDHoP,\nthe goal is to establish drug-drug interactions based\non scientific findings about drugs and proteins and\ntheir interactions, found across multiple MEDLINE\nabstracts. For both datasets we draw upon existing\nKnowledge Bases (KBs), WIKIDATA and DRUG-\nBANK, as ground truth, utilizing distant supervi-\nsion (Mintz et al., 2009) to induce the data — similar\nto Hewlett et al. (2016) and Joshi et al. (2017).\n\nWe establish that for 74.1% and 68.0% of the\nsamples, the answer can be inferred from the given\ndocuments by a human annotator. Still, construct-\ning multi-document datasets is challenging; we en-\ncounter and prescribe remedies for several pitfalls\nassociated with their assembly — for example, spuri-\nous co-locations of answers and specific documents.\n\nFor both datasets we then establish several strong\nbaselines and evaluate the performance of two pre-\nviously proposed competitive RC models (Seo et al.,\n2017a; Weissenborn et al., 2017). We find that one\ncan integrate information across documents, but nei-\nther excels at selecting relevant information from a\nlarger documents set, as their accuracy increases sig-\n\n288\n\nnificantly when given only documents guaranteed to\nbe relevant. The best model reaches 54.5% on an\nannotated test set, compared to human performance\nat 85.0%, indicating ample room for improvement.\n\nIn summary, our key contributions are as follows:\nFirstly, proposing a cross-document multi-step RC\ntask, as well as a general dataset induction strat-\negy. Secondly, assembling two datasets from dif-\nferent domains and identifying dataset construction\npitfalls and remedies. Thirdly, establishing multiple\nbaselines, including two recently proposed RC mod-\nels, as well as analysing model behaviour in detail\nthrough ablation studies.\n\n2 Task and Dataset Construction Method\n\nWe will now formally define the multi-hop RC task,\nand a generic methodology to construct multi-hop\nRC datasets. Later, in Sections 3 and 4 we will\ndemonstrate how this method is applied in practice\nby creating datasets for two different domains.\n\nTask Formalization A model is given a query q, a\nset of supporting documents Sj, and a set of candi-\ndate answers Cy — all of which are mentioned in Sq.\nThe goal is to identify the correct answer a* € Cy\nby drawing on the support documents Sj. Queries\ncould potentially have several true answers when not\nconstrained to rely on a specific set of support doc-\numents — e.g., queries about the parent of a certain\nindividual. However, in our setup each sample has\nonly one true answer among Cy, and S,. Note that\neven though we will utilize background information\nduring dataset assembly, such information will not\nbe available to a model: the document set will be\nprovided in random order and without any metadata.\nWhile certainly beneficial, this would distract from\nour goal of fostering end-to-end RC methods that in-\nfer facts by combining separate facts stated in text.\n\nDataset Assembly We assume that there exists a\ndocument corpus D, together with a KB containing\nfact triples (s,r, 0) — with subject entity s, relation r,\nand object entity o. For example, one such fact\ncould be (Hanging_Gardens_of Mumbai, country,\nIndia). We start with individual KB facts and trans-\nform them into query-answer pairs by leaving the\nobject slot empty, i.e. g = (s,r,?) and a* = o.\nNext, we define a directed bipartite graph, where\n", "vlm_text": "\nFigure 1 shows an example from W IKIPEDIA , where the goal is to identify the  country  property of the  Hanging Gardens of Mumbai . This cannot be inferred solely from the article about them without additional background knowledge, as the answer is not stated explicitly. However, several of the linked articles mention the correct answer  India  (and other countries), but cover different topics (e.g.  Mumbai , Arabian Sea , etc.). Finding the answer requires multi-hop  reasoning: ﬁguring out that the  Hanging Gardens  are located in  Mumbai , and then, from a second document, that  Mumbai  is a city in  India . \nWe deﬁne a novel RC task in which a model should learn to answer queries by combining ev- idence stated across documents. We introduce a methodology to induce datasets for this task and de- rive two datasets. The ﬁrst, W IKI H OP , uses sets of W IKIPEDIA  articles where answers to queries about speciﬁc properties of an entity cannot be located in the entity’s article. In the second dataset, M ED H OP , the goal is to establish drug-drug interactions based on scientiﬁc ﬁndings about drugs and proteins and their interactions, found across multiple M EDLINE abstracts. For both datasets we draw upon existing Knowledge Bases (KBs), W IKIDATA  and D RUG - B ANK , as ground truth, utilizing distant supervi- sion (Mintz et al., 2009) to induce the data – similar to Hewlett et al. (2016) and Joshi et al. (2017). \nWe establish that for   $74.1\\%$   and   $68.0\\%$   of the samples, the answer can be inferred from the given documents by a human annotator. Still, construct- ing multi-document datasets is challenging; we en- counter and prescribe remedies for several pitfalls associated with their assembly – for example, spuri- ous co-locations of answers and speciﬁc documents. \nFor both datasets we then establish several strong baselines and evaluate the performance of two pre- viously proposed competitive RC models (Seo et al., 2017a; Weissenborn et al., 2017). We ﬁnd that one can integrate information across documents, but nei- ther excels at selecting relevant information from a larger documents set, as their accuracy increases sig- niﬁcantly when given only documents guaranteed to be relevant. The best model reaches   $54.5\\%$   on an annotated test set, compared to human performance at  $85.0\\%$  , indicating ample room for improvement. \n\nIn summary, our key contributions are as follows: Firstly, proposing a cross-document multi-step RC task, as well as a general dataset induction strat- egy. Secondly, assembling two datasets from dif- ferent domains and identifying dataset construction pitfalls and remedies. Thirdly, establishing multiple baselines, including two recently proposed RC mod- els, as well as analysing model behaviour in detail through ablation studies. \n2 Task and Dataset Construction Method \nWe will now formally deﬁne the multi-hop RC task, and a generic methodology to construct multi-hop RC datasets. Later, in Sections 3 and 4 we will demonstrate how this method is applied in practice by creating datasets for two different domains. \nTask Formalization A model is given a query    $q$  , a set of supporting documents  $S_{q}$  , and a set of candi- date answers    $C_{q}-\\mathrm{all}$   of which are mentioned in    $S_{q}$  . The goal is to identify the correct answer    $a^{*}\\ \\in\\ C_{q}$  by drawing on the support documents  $S_{q}$  . Queries could potentially have several true answers when not constrained to rely on a speciﬁc set of support doc- uments – e.g., queries about the parent of a certain individual. However, in our setup each sample has only one true answer among    $C_{q}$   and    $S_{q}$  . Note that even though we will utilize background information during dataset assembly, such information will not be available to a model: the document set will be provided in random order and without any metadata. While certainly beneﬁcial, this would distract from our goal of fostering end-to-end RC methods that in- fer facts by combining separate facts stated in text. \nDataset Assembly We assume that there exists a document corpus    $D$  , together with a KB containing fact triples    $(s,r,o)$   – with subject entity  $s$  , relation  $r$  , and object entity    $o$  . For example, one such fact could be  (Hanging Gardens of Mumbai, country, India) . We start with individual KB facts and trans- form them into query-answer pairs by leaving the object slot empty, i.e.    $q=(s,r,?)$   and    $a^{*}=o$  . Next, we deﬁne a directed bipartite graph, where vertices on one side correspond to documents in  $D$  , and vertices on the other side are entities from the KB – see Figure 2 for an example. A docu- ment node    $d$   is connected to an entity    $e$   if  $e$   is men- tioned in    $d$  , though there may be further constraints when deﬁning the graph connectivity. For a given  $(q,a^{*})$   pair, the candidates    $C_{q}$   and support docu- ments  $S_{q}\\subseteq D$   are identiﬁed by traversing the bipar- tite graph using breadth-ﬁrst search; the documents visited will become the support documents  $S_{q}$  . "}
{"page": 2, "image_path": "doc_images/Q18-1021_2.jpg", "ocr_text": "vertices on one side correspond to documents in\nD, and vertices on the other side are entities from\nhe KB — see Figure 2 for an example. A docu-\nment node d is connected to an entity e if e is men-\nioned in d, though there may be further constraints\nwhen defining the graph connectivity. For a given\n(q,a*) pair, the candidates C, and support docu-\nments S, C D are identified by traversing the bipar-\nite graph using breadth-first search; the documents\nvisited will become the support documents Sy.\n\nAs the traversal starting point, we use the node\nbelonging to the subject entity s of the query g. As\ntraversal end points, we use the set of all entity nodes\nhat are type-consistent answers to qg.2 Note that\nwhenever there is another fact (s,7,0’) in the KB,\nie. a fact producing the same q but with a different\na*, we will not include o’ into the set of end points\nor this sample. This ensures that precisely one of\nhe end points corresponds to a correct answer to q.\n\nWhen traversing the graph starting at s, several\nof the end points will be visited, though generally\nnot all; those visited define the candidate set Cy. If\nhowever the correct answer a* is not among them we\ndiscard the entire (q, a*) pair. The documents visited\n0 reach the end points will define the support docu-\nment set Sj. That is, Sy comprises chains of docu-\nments leading not only from the query subject to the\ncorrect answer candidate, but also to type-consistent\nalse answer candidates.\n\nWith this methodology, relevant textual evidence\nor (q,a*) will be spread across documents along\nhe chain connecting s and a* — ensuring that multi-\nhop reasoning goes beyond resolving co-reference\nwithin a single document. Note that including\nother type-consistent candidates alongside a* as end\npoints in the graph traversal — and thus into the sup-\nport documents — renders the task considerably more\nchallenging (Jia and Liang, 2017). Models could\notherwise identify a* in the documents by simply\nrelying on type-consistency heuristics. It is worth\npointing out that by introducing alternative candi-\ndates we counterbalance a type-consistency bias, in\ncontrast to Hermann et al. (2015) and Hill et al.\n(2016) who instead rely on entity masking.\n\n? To determine entities which are type-consistent for a\nquery q, we consider all entities which are observed as object\nin a fact with r as relation type — including the correct answer.\n\n289\n\nEntities\n\nKB\n\n(s, 7,0)\n\nDocuments\n\n(s,7,0')\n\n(s',r,0\")\n\nFigure 2: A bipartite graph connecting entities and doc-\numents mentioning them. Bold edges are those traversed\nfor the first fact in the small KB on the right; yellow high-\nlighting indicates documents in S, and candidates in Cy.\nCheck and cross indicate correct and false candidates.\n\n3. WIKIHOP\n\nWIKIPEDIA contains an abundance of human-\ncurated, multi-domain information and has sev-\neral structured resources such as infoboxes and\nWIKIDATA (Vrandecié, 2012) associated with it.\nWIKIPEDIA has thus been used for a wealth of re-\nsearch to build datasets posing queries about a single\nsentence (Morales et al., 2016; Levy et al., 2017) or\narticle (Yang et al., 2015; Hewlett et al., 2016; Ra-\njpurkar et al., 2016). However, no attempt has been\nmade to construct a cross-document multi-step RC\ndataset based on WIKIPEDIA.\n\nA recently proposed RC dataset is WIKIREAD-\nING (Hewlett et al., 2016), where WIKIDATA tu-\nples (item, property, answer) are aligned with\nthe WIKIPEDIA articles regarding their item. The\ntuples define a slot filling task with the goal of pre-\ndicting the answer, given an article and property.\nOne problem with using WIKIREADING as an ex-\ntractive RC dataset is that 54.4% of the samples\ndo not state the answer explicitly in the given arti-\ncle (Hewlett et al., 2016). However, we observed\nthat some of the articles accessible by following hy-\nperlinks from the given article often state the answer,\nalongside other plausible candidates.\n\n3.1 Assembly\n\nWe now apply the methodology from Section 2\nto create a multi-hop dataset with WIKIPEDIA as\nthe document corpus and WIKIDATA as structured\nknowledge triples. In this setup, (item, property,\nanswer) WIKIDATA tuples correspond to (s,7,0)\ntriples, and the item and property of each sample\n", "vlm_text": "\nAs the traversal starting point, we use the node belonging to the subject entity    $s$   of the query  $q$  . As traversal end points, we use the set of all entity nodes that are type-consistent answers to    $q$  . Note that whenever there is another fact    $(s,r,o^{\\prime})$   in the KB, i.e. a fact producing the same    $q$   but with a different  $a^{*}$  , we will not include    $o^{\\prime}$    into the set of end points for this sample. This ensures that precisely one of the end points corresponds to a correct answer to  $q$  . \nWhen traversing the graph starting at    $s$  , several of the end points will be visited, though generally not all; those visited deﬁne the candidate set  $C_{q}$  . If however the correct answer    $a^{*}$  is not among them we discard the entire    $(q,a^{*})$   pair. The documents visited to reach the end points will deﬁne the support docu- ment set    $S_{q}$  . That is,    $S_{q}$   comprises chains of docu- ments leading not only from the query subject to the correct answer candidate, but also to type-consistent false answer candidates. \nWith this methodology, relevant textual evidence for    $(q,a^{*})$   will be spread across documents along the chain connecting  $s$   and    $a^{*}$  – ensuring that multi- hop reasoning goes beyond resolving co-reference within a single document. Note that including other type-consistent candidates alongside  $a^{*}$  as end points in the graph traversal – and thus into the sup- port documents – renders the task considerably more challenging (Jia and Liang, 2017). Models could otherwise identify    $a^{*}$  in the documents by simply relying on type-consistency heuristics. It is worth pointing out that by introducing alternative candi- dates we counterbalance a type-consistency bias, in contrast to Hermann et al. (2015) and Hill et al. (2016) who instead rely on entity masking. \nThe image shows a diagram with three main components: Entities, Documents, and KB (Knowledge Base). \n\n- On the left side under \"Entities,\" there are circles of various colors representing different entities, labeled as s, o, o', o'', and s'.\n  \n- In the middle under \"Documents,\" there are several document icons, each connected by lines to different colored entity circles, indicating relationships or associations between the entities and the documents.\n\n- On the right side under \"KB,\" the image includes three tuples: (s, r, o), (s, r, o'), and (s', r, o''), possibly representing subject-predicate-object triples in a knowledge base.\n\n- There is a green check mark next to one grouping and a red X next to another, likely indicating correct and incorrect or validated and unvalidated associations or inferences. \n\nThe diagram appears to illustrate a process of validating relationships between entities and documents within a knowledge base.\nFigure 2: A bipartite graph connecting entities and doc- uments mentioning them. Bold edges are those traversed for the ﬁrst fact in the small KB on the right; yellow high- lighting indicates documents in  $S_{q}$   and candidates in    $C_{q}$  . Check and cross indicate correct and false candidates. \n3 W IKI H OP \nW IKIPEDIA  contains an abundance of human- curated, multi-domain information and has sev- eral structured resources such as infoboxes and W IKIDATA  (Vrandeˇ ci´ c, 2012) associated with it. W IKIPEDIA  has thus been used for a wealth of re- search to build datasets posing queries about a single sentence (Morales et al., 2016; Levy et al., 2017) or article (Yang et al., 2015; Hewlett et al., 2016; Ra- jpurkar et al., 2016). However, no attempt has been made to construct a cross-document multi-step RC dataset based on W IKIPEDIA . \nA recently proposed RC dataset is W IKI R EAD - ING  (Hewlett et al., 2016), where W IKIDATA  tu- ples  (item, property, answer)  are aligned with the W IKIPEDIA  articles regarding their  item . The tuples deﬁne a slot ﬁlling task with the goal of pre- dicting the  answer , given an  article  and  property . One problem with using W IKI R EADING  as an ex- tractive RC dataset is that   $54.4\\%$   of the samples do not state the answer explicitly in the given arti- cle (Hewlett et al., 2016). However, we observed that some of the articles accessible by following hy- perlinks from the given article often state the answer, alongside other plausible candidates. \n3.1 Assembly \nWe now apply the methodology from Section 2 to create a multi-hop dataset with W IKIPEDIA  as the document corpus and W IKIDATA  as structured knowledge triples. In this setup,  (item, property, answer)  W IKIDATA  tuples correspond to    $(s,r,o)$  triples, and the  item  and  property  of each sample together form our query    $q-\\mathbf{e.g}$  .,  (Hanging Gardens of Mumbai, country, ?) . Similar to Yang et al. (2015) we only use the ﬁrst paragraph of an article, as rel- evant information is more often stated in the begin- ning. Starting with all samples in W IKI R EADING , we ﬁrst remove samples where the  answer  is stated explicitly in the W IKIPEDIA  article about the  item . "}
{"page": 3, "image_path": "doc_images/Q18-1021_3.jpg", "ocr_text": "together form our query q — e.g., (Hanging Gardens\nof Mumbai, country, ?). Similar to Yang et al. (2015)\nwe only use the first paragraph of an article, as rel-\nevant information is more often stated in the begin-\nning. Starting with all samples in WIKIREADING,\nwe first remove samples where the answer is stated\nexplicitly in the WIKIPEDIA article about the item.?\n\nThe bipartite graph is structured as follows:\n(1) for edges from articles to entities: all articles\nmentioning an entity e are connected to e; (2) for\nedges from entities to articles: each entity e is only\nconnected to the WIKIPEDIA article about the entity.\nTraversing the graph is then equivalent to iteratively\nfollowing hyperlinks to new articles about the an-\nchor text entities.\n\nFor a given query-answer pair, the item entity\nis chosen as the starting point for the graph traver-\nsal. A traversal will always pass through the article\nabout the item, since this is the only document con-\nnected from there. The end point set includes the\ncorrect answer alongside other type-consistent can-\ndidate expressions, which are determined by consid-\nering all facts belonging to WIKIREADING train-\ning samples, selecting those triples with the same\nproperty as in q and keeping their answer expres-\nsions. As an example, for the WIKIDATA property\ncount ry, this would be the set {France, Russia, ...}.\nWe executed graph traversal up to a maximum chain\nlength of 3 documents. To not pose unreasonable\ncomputational constraints, samples with more than\n64 different support documents or 100 candidates\nare removed, discarding ~1% of the samples.\n\n3.2 Mitigating Dataset Biases\n\nDataset creation is always fraught with the risk of\ninducing unintended errors and biases (Chen et al.,\n2016; Schwartz et al., 2017). As Hewlett et al.\n(2016) only carried out limited analysis of their\nWIKIREADING dataset, we present an analysis of\nthe downstream effects we observe on WIKIHOP.\n\nCandidate Frequency Imbalance A first obser-\nvation is that there is a significant bias in the answer\ndistribution of WIKIREADING. For example, in the\nmajority of the samples the property country has\nthe United States of America as the answer. A simple\n\n3 We thus use a disjoint subset of WIKIREADING compared\nto Levy et al. (2017) to construct WIKIHOP.\n\n290\n\nmajority class baseline would thus prove successful,\nbut would tell us little about multi-hop reasoning. To\ncombat this issue, we subsampled the dataset to en-\nsure that samples of any one particular answer can-\ndidate make up no more than 0.1% of the dataset,\nand omitted articles about the United States.\n\nDocument-Answer Correlations A problem\nunique to our multi-document setting is the possibil-\nity of spurious correlations between candidates and\ndocuments induced by the graph traversal method.\nIn fact, if we were not to address this issue, a model\ndesigned to exploit these regularities could achieve\n74.6% accuracy (detailed in Section 6).\n\nConcretely, we observed that certain documents\nfrequently co-occur with the correct answer, inde-\npendently of the query. For example, if the article\nabout London is present in S,, the answer is likely\nto be the United Kingdom, independent of the query\ntype or entity in question.\n\nWe designed a statistic to measure this effect\nand then used it to sub-sample the dataset. The\nstatistic counts how often a candidate c is observed\nas the correct answer when a certain document is\npresent in Sy across training set samples. More for-\nmally, for a given document d and answer candi-\ndate c, let cooccurrence(d, c) denote the total count\nof how often d co-occurs with c in a sample where\nc is also the correct answer. We use this statistic\nto filter the dataset, by discarding samples with at\nleast one document-candidate pair (d,c) for which\ncooccurrence(d,c) > 20.\n\n4 MEDHopP\n\nFollowing the same general methodology, we next\nconstruct a second dataset for the domain of molec-\nular biology — a field that has been undergoing ex-\nponential growth in the number of publications (Co-\nhen and Hunter, 2004). The promise of applying\nNLP methods to cope with this increase has led to\nresearch efforts in IE (Hirschman et al., 2005; Kim\net al., 2011) and QA for biomedical text (Hersh et\nal., 2007; Nentidis et al., 2017). There are a plethora\nof manually curated structured resources (Ashburner\net al., 2000; The UniProt Consortium, 2017) which\ncan either serve as ground truth or to induce training\ndata using distant supervision (Craven and Kumlien,\n1999; Bobic et al., 2012). Existing RC datasets are\n", "vlm_text": "\nThe bipartite graph is structured as follows: (1) for edges from articles to entities: all articles mentioning an entity    $e$   are connected to    $e$  ; (2) for edges from entities to articles: each entity  $e$   is only connected to the W IKIPEDIA  article about the entity. Traversing the graph is then equivalent to iteratively following hyperlinks to new articles about the an- chor text entities. \nFor a given query-answer pair, the  item  entity is chosen as the starting point for the graph traver- sal. A traversal will always pass through the article about the  item , since this is the only document con- nected from there. The end point set includes the correct  answer  alongside other type-consistent can- didate expressions, which are determined by consid- ering  all  facts belonging to W IKI R EADING  train- ing samples, selecting those triples with the same property  as in    $q$   and keeping their  answer  expres- sions. As an example, for the W IKIDATA  property country , this would be the set  { France ,  Russia ,   $\\ldots\\}$  . We executed graph traversal up to a maximum chain length of 3 documents. To not pose unreasonable computational constraints, samples with more than 64 different support documents or 100 candidates are removed, discarding  ${\\approx}1\\%$   of the samples. \n3.2 Mitigating Dataset Biases \nDataset creation is always fraught with the risk of inducing unintended errors and biases (Chen et al., 2016; Schwartz et al., 2017). As Hewlett et al. (2016) only carried out limited analysis of their W IKI R EADING  dataset, we present an analysis of the downstream effects we observe on W IKI H OP . \nCandidate Frequency Imbalance A ﬁrst obser- vation is that there is a signiﬁcant bias in the answer distribution of W IKI R EADING . For example, in the majority of the samples the property  country  has the  United States of America  as the answer. A simple majority class baseline would thus prove successful, but would tell us little about multi-hop reasoning. To combat this issue, we subsampled the dataset to en- sure that samples of any one particular answer can- didate make up no more than    $0.1\\%$   of the dataset, and omitted articles about the  United States . \n\nDocument-Answer Correlations A problem unique to our multi-document setting is the possibil- ity of spurious correlations between candidates and documents induced by the graph traversal method. In fact, if we were  not  to address this issue, a model designed to exploit these regularities could achieve  $74.6\\%$   accuracy (detailed in Section 6). \nConcretely, we observed that certain documents frequently co-occur with the correct answer, inde- pendently of the query. For example, if the article about  London  is present in    $S_{q}$  , the answer is likely to be the  United Kingdom , independent of the query type or entity in question. \nWe designed a statistic to measure this effect and then used it to sub-sample the dataset. The statistic counts how often a candidate    $c$   is observed as the correct answer when a certain document is present in  $S_{q}$   across training set samples. More for- mally, for a given document    $d$   and answer candi- date    $c$  , let  cooccurrence  $(d,c)$   denote the total count of how often    $d$   co-occurs with    $c$   in a sample where  $c$   is also the correct answer. We use this statistic to ﬁlter the dataset, by discarding samples with at least one document-candidate pair    $(d,c)$   for which cooccurrence  $(d,c)>20$  . \n4 M ED H OP \nFollowing the same general methodology, we next construct a second dataset for the domain of molec- ular biology – a ﬁeld that has been undergoing ex- ponential growth in the number of publications (Co- hen and Hunter, 2004). The promise of applying NLP methods to cope with this increase has led to research efforts in IE (Hirschman et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There are a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neu- ral models that have seen successes for other do- mains (Wiese et al., 2017). "}
{"page": 4, "image_path": "doc_images/Q18-1021_4.jpg", "ocr_text": "either severely limited in size (Hersh et al., 2007)\nor cover a very diverse set of query types (Nentidis\net al., 2017), complicating the application of neu-\nral models that have seen successes for other do-\nmains (Wiese et al., 2017).\n\nA task that has received significant attention is\ndetecting Drug-Drug Interactions (DDIs). Exist-\ning DDI efforts have focused on explicit mentions\nof interactions in single sentences (Gurulingappa\net al., 2012; Percha et al., 2012; Segura-Bedmar\net al., 2013). However, as shown by Peng et al.\n(2017), cross-sentence relation extraction increases\nthe number of available relations. It is thus likely\nthat cross-document interactions would further im-\nprove recall, which is of particular importance con-\nsidering interactions that are never stated explicitly\n— but rather need to be inferred from separate pieces\nof evidence. The promise of multi-hop methods is\nfinding and combining individual observations that\ncan suggest previously unobserved DDIs, aiding the\nprocess of making scientific discoveries, yet not di-\nrectly from experiments, but by inferring them from\nestablished public knowledge (Swanson, 1986).\n\nDDIs are caused by Protein-Protein Interac-\ntion (PPI) chains, forming biomedical pathways.\nIf we consider PPI chains across documents,\nwe find examples like in Figure 3. Here the\nfirst document states that the drug Leuprolide\ncauses GnRH receptor-induced synaptic potenti-\nations, which can be blocked by the protein\nProgonadoliberin-1. The last document states that\nanother drug, Triptorelin, is a superagonist of the\nsame protein. It is therefore likely to affect the po-\ntency of Leuprolide, describing a way in which the\ntwo drugs interact. Besides the true interaction there\nis also a false candidate Urofollitropin for which,\nalthough mentioned together with GnRH receptor\nwithin one document, there is no textual evidence\nindicating interactions with Leuprolide.\n\n4.1 Assembly\n\nWe construct MEDHoP using DRUGBANK (Law\net al., 2014) as structured knowledge resource and\nresearch paper abstracts from MEDLINE as docu-\nments. There is only one relation type for DRUG-\nBANK facts, interact s-with, that connects pairs of\ndrugs — an example of a MEDHOP query would thus\n\nbe (Leuprolide, interacts_with, ?). We start\n\n291\n\nLeuprolide ... elicited a long-lasting potentiation of excitatory postsynaptic\ncurrents... [GnRH receptor]-induced synaptic potentiation was blocked\nby [Progonadoliberin-1], a specific [GnRH receptor] antagonist\n\nour research to study the distribution, co-localization of Urofollitropin and\nits receptor[,] and co-localization of Urofollitropin and GnRH receptor.\n\nAnalyses of gene expression demonstrated a dynamic response to the\nProgonadoliberin-1 superagonist Triptorelin.\n\nQ: (Leuprolide, interacts_with, ?)\nOptions: {Triptorelin, Urofollitropin}\n\nFigure 3: A sample from the MEDHop dataset.\n\nby processing the 2016 MEDLINE release using the\npreprocessing pipeline employed for the BioNLP\n2011 Shared Task (Stenetorp et al., 2011). We re-\nstrict the set of entities in the bipartite graph to\ndrugs in DRUGBANK and human proteins in S wIss-\nPROT (Bairoch et al., 2004). That is, the graph has\ndrugs and proteins on one side, and MEDLINE ab-\nstracts on the other.\n\nThe edge structure is as follows: (1) There is an\nedge from a document to all proteins mentioned in it.\n(2) There is an edge between a document and a drug,\nif this document also mentions a protein known to be\na target for the drug according to DRUGBANK. This\nedge is bidirectional, i.e. it can be traversed both\nways, since there is no canonical document describ-\ning each drug — thus one can “hop” to any document\nmentioning the drug and its target. (3) There is an\nedge from a protein p to a document mentioning p,\nbut only if the document also mentions another pro-\ntein p’ which is known to interact with p according to\nREACTOME (Fabregat et al., 2016). Given our dis-\ntant supervision assumption, these additionally con-\nstraining requirements err on the side of precision.\n\nAs a mention, similar to Percha et al. (2012), we\nconsider any exact match of a name variant of a\ndrug or human protein in DRUGBANK or SwISs-\nPROT. For a given DDI (drugi, interacts_with,\ndrug2), we then select drug: as the starting point\nfor the graph traversal. As possible end points, we\nconsider any other drug, apart from drug, and those\ninteracting with drug, other than drug2. Similar to\nWIKIHOP, we exclude samples with more than 64\nsupport documents and impose a maximum docu-\nment length of 300 tokens plus title.\n\nDocument Sub-sampling The bipartite graph for\nMEDHOP is orders of magnitude more densely con-\nnected than for WIKIHOP. This can lead to poten-\n", "vlm_text": "\nA task that has received signiﬁcant attention is detecting Drug-Drug Interactions (DDIs). Exist- ing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further im- prove recall, which is of particular importance con- sidering interactions that are never stated explicitly – but rather need to be inferred from separate pieces of evidence. The promise of multi-hop methods is ﬁnding and combining individual observations that can suggest previously unobserved DDIs, aiding the process of making scientiﬁc discoveries, yet not di- rectly from experiments, but by inferring them from established public knowledge (Swanson, 1986). \nDDIs are caused by Protein-Protein Interac- tion (PPI) chains, forming biomedical pathways. If we consider PPI chains across documents, we ﬁnd examples like in Figure 3. Here the ﬁrst document states that the drug  Leuprolide causes  GnRH receptor -induced synaptic potenti- ations, which can be blocked by the protein Progonadoliberin-1 . The last document states that another drug,  Triptorelin , is a superagonist of the same protein. It is therefore likely to affect the po- tency of  Leuprolide , describing a way in which the two drugs interact. Besides the true interaction there is also a false candidate  Urofollitropin  for which, although mentioned together with  GnRH receptor within one document, there is no textual evidence indicating interactions with  Leuprolide . \n4.1 Assembly \nWe construct M ED H OP  using D RUG B ANK  (Law et al., 2014) as structured knowledge resource and research paper abstracts from M EDLINE  as docu- ments. There is only one relation type for D RUG - B ANK  facts,  interacts with , that connects pairs of drugs – an example of a M ED H OP  query would thus be  (Leuprolide, interacts with, ?) . We start by processing the 2016 M EDLINE  release using the preprocessing pipeline employed for the BioNLP 2011 Shared Task (Stenetorp et al., 2011). We re- strict the set of entities in the bipartite graph to drugs in D RUG B ANK  and human proteins in S WISS - P ROT  (Bairoch et al., 2004). That is, the graph has drugs and proteins on one side, and M EDLINE  ab- stracts on the other. \n\nThe edge structure is as follows: (1) There is an edge from a document to all proteins mentioned in it. (2) There is an edge between a document and a drug, if this document also mentions a protein known to be a target for the drug according to D RUG B ANK . This edge is bidirectional, i.e. it can be traversed both ways, since there is no canonical document describ- ing each drug – thus one can “hop” to any document mentioning the drug and its target. (3) There is an edge from a protein  $p$   to a document mentioning  $p$  , but only if the document also mentions another pro- tein  $p^{\\prime}$    which is known to interact with  $p$   according to R EACTOME  (Fabregat et al., 2016). Given our dis- tant supervision assumption, these additionally con- straining requirements err on the side of precision. \nAs a mention, similar to Percha et al. (2012), we consider any exact match of a name variant of a drug or human protein in D RUG B ANK  or S WISS - P ROT . For a given DDI  (drug 1 , interacts with, drug 2 ) , we then select  drug 1  as the starting point for the graph traversal. As possible end points, we consider any other drug, apart from  drug 1  and those interacting with  drug 1  other than  drug 2 . Similar to W IKI H OP , we exclude samples with more than 64 support documents and impose a maximum docu- ment length of 300 tokens plus title. \nDocument Sub-sampling The bipartite graph for M ED H OP  is orders of magnitude more densely con- nected than for W IKI H OP . This can lead to poten- tially large support document sets    $S_{q}$  , to a degree where it becomes computationally infeasible for a majority of existing RC models. After the traver- sal has ﬁnished, we subsample documents by ﬁrst adding a set of documents that connects the drug in the query with its answer. We then iteratively add documents to connect alternative candidates until we reach the limit of 64 documents – while ensuring that all candidates have the same number of paths through the bipartite graph. "}
{"page": 5, "image_path": "doc_images/Q18-1021_5.jpg", "ocr_text": "tially large support document sets Sg, to a degree\nwhere it becomes computationally infeasible for a\nmajority of existing RC models. After the traver-\nsal has finished, we subsample documents by first\nadding a set of documents that connects the drug in\nhe query with its answer. We then iteratively add\ndocuments to connect alternative candidates until we\nreach the limit of 64 documents — while ensuring\nhat all candidates have the same number of paths\nhrough the bipartite graph.\n\nMitigating Candidate Frequency Imbalance\nSome drugs interact with more drugs than others\n— Aspirin for example interacts with 743 other\ndrugs, but /sotretinoin with only 34. This leads\no similar candidate frequency imbalance issues\nas with WIKIHoP — but due to its smaller size\nMEDHOpP is difficult to sub-sample. Nevertheless\nwe can successfully combat this issue by masking\nentity names, detailed in Section 6.2.\n\n5 Dataset Analysis\n\nTable 1 shows the dataset sizes. Note that WIK-\nIHOP inherits the train, development, and test set\nsplits from WIKIREADING - ie., the full dataset\ncreation, filtering, and sub-sampling pipeline is ex-\necuted on each set individually. Also note that sub-\nsampling according to document-answer correlation\nsignificantly reduces the size of WIKIHOP from\n528K training samples to ~44K. While in terms of\nsamples, both WIKIHoP and MEDHOP are smaller\nhan other large-scale RC datasets, such as SQuAD\nand WIKIREADING, the supervised learning signal\navailable per sample is arguably greater. One could,\nor example, re-frame the task as binary path clas-\nsification: given two entities and a document path\nconnecting them, determine whether a given rela-\nion holds. For such a case, WIKIHOP and MED-\nHop would have more than 1M and 150K paths to\nbe classified, respectively. Instead, in our formula-\nion, this corresponds to each single sample contain-\ning the supervised learning signal from an average\nof 19.5 and 59.8 unique document paths.\n\nTable 2 shows statistics on the number of candi-\ndates and documents per sample on the respective\ntraining sets. For MEDHopP, the majority of sam-\nples have 9 candidates, due to the way documents\nare selected up until a maximum of 64 documents is\n\n292\n\nTrain Dev Test Total\nWIKIHOP $43,738 5,129 2,451 51,318\nMEDHop 1,620 342 546 =. 2,508\n\nTable 1: Dataset sizes for our respective datasets.\n\nmin max avg median\n# cand. - WH 2 79 19.8 14\n# docs. - WH 3 63 13.7 11\n# tok/doc - WH 4 2,046 100.4 91\n# cand. - MH 2 9 8.9 9\n# docs. - MH 5 64 = 36.4 29\n# tok/doc - MH 5 458 253.9 264\n\nTable 2: Candidates and documents per sample and doc-\nument length statistics. WH: WIKIHOP; MH: MEDHop.\n\nreached. Few samples have less than 9 candidates,\nand samples would have far more false candidates if\nmore than 64 support documents were included. The\nnumber of query types in WIKIHOP is 277, whereas\nin MEDHOFP there is only one: interacts_with.\n\n5.1 Qualitative Analysis\n\nTo establish the quality of the data and analyze po-\ntential distant supervision errors, we sampled and\nannotated 100 samples from each development set.\n\nWIKIHOP Table 3 lists characteristics along with\nthe proportion of samples that exhibit them. For\n45%, the true answer either uniquely follows from\nmultiple texts directly or is suggested as likely. For\n26%, more than one candidate is plausibly sup-\nported by the documents, including the correct an-\nswer. This is often due to hypernymy, where\nthe appropriate level of granularity for the an-\nswer is difficult to predict — e.g. (west suffolk,\nadministrative-entity, ?) with candidates\nsuffolk and england. This is a direct conse-\nquence of including type-consistent false answer\ncandidates from WIKIDATA, which can lead to ques-\ntions with several true answers. For 9% of the\ncases a single document suffices; these samples\ncontain a document that states enough information\nabout item and answer together. For example,\nthe query (Louis Auguste, father, ?) has the\ncorrect answer Louis XIV of France, and French\nking Louis XIvis mentioned within the same doc-\n\n", "vlm_text": "\nMitigating Candidate Frequency Imbalance Some drugs interact with more drugs than others –  Aspirin  for example interacts with 743 other drugs, but  Isotretinoin  with only 34. This leads to similar candidate frequency imbalance issues as with W IKI H OP  – but due to its smaller size M ED H OP  is difﬁcult to sub-sample. Nevertheless we can successfully combat this issue by masking entity names, detailed in Section 6.2. \n5 Dataset Analysis \nTable 1 shows the dataset sizes. Note that W IK - I H OP  inherits the train, development, and test set splits from W IKI R EADING  – i.e., the full dataset creation, ﬁltering, and sub-sampling pipeline is ex- ecuted on each set individually. Also note that sub- sampling according to document-answer correlation signiﬁcantly reduces the size of W IKI H OP  from  ${\\approx}528K$   training samples to  ${\\approx}44\\mathrm{K}$  . While in terms of samples, both W IKI H OP  and M ED H OP  are smaller than other large-scale RC datasets, such as    $S Q u A D$  and W IKI R EADING , the supervised learning signal available per sample is arguably greater. One could, for example, re-frame the task as binary path clas- siﬁcation: given two entities and a document path connecting them, determine whether a given rela- tion holds. For such a case, W IKI H OP  and M ED - H OP  would have more than 1M and 150K paths to be classiﬁed, respectively. Instead, in our formula- tion, this corresponds to each single sample contain- ing the supervised learning signal from an average of 19.5 and 59.8 unique document paths. \nTable 2 shows statistics on the number of candi- dates and documents per sample on the respective training sets. For M ED H OP , the majority of sam- ples have 9 candidates, due to the way documents are selected up until a maximum of 64 documents is \nThe table lists numbers associated with two datasets, WikiHop and MedHop. The numbers in each row likely represent some statistics or counts related to these datasets. For WikiHop, the numbers are 43,738, 5,129, 2,451, and 51,318. For MedHop, the numbers are 1,620, 342, 546, and 2,508. Without additional context or column headers, it is difficult to specify what these numbers represent, but they could pertain to data such as the number of samples, entries, articles, or questions in the respective datasets.\nThe table presents statistical data for two different categories, labeled \"WH\" and \"MH,\" each with three different measurements:\n\n1. **# cand. (Candidates)**\n   - **WH:**\n     - Min: 2\n     - Max: 79\n     - Avg: 19.8\n     - Median: 14\n   - **MH:**\n     - Min: 2\n     - Max: 9\n     - Avg: 8.9\n     - Median: 9\n\n2. **# docs. (Documents)**\n   - **WH:**\n     - Min: 3\n     - Max: 63\n     - Avg: 13.7\n     - Median: 11\n   - **MH:**\n     - Min: 5\n     - Max: 64\n     - Avg: 36.4\n     - Median: 29\n\n3. **# tok/doc (Tokens per Document)**\n   - **WH:**\n     - Min: 4\n     - Max: 2046\n     - Avg: 100.4\n     - Median: 91\n   - **MH:**\n     - Min: 5\n     - Max: 458\n     - Avg: 253.9\n     - Median: 264\n\nEach row provides the minimum (min), maximum (max), average (avg), and median values for the respective categories and measurements.\nreached. Few samples have less than 9 candidates, and samples would have far more false candidates if more than 64 support documents were included. The number of query types in W IKI H OP  is 277, whereas in M ED H OP  there is only one:  interacts with . \n5.1 Qualitative Analysis \nTo establish the quality of the data and analyze po- tential distant supervision errors, we sampled and annotated 100 samples from each development set. \nW IKI H OP Table 3 lists characteristics along with the proportion of samples that exhibit them. For  $45\\%$  , the true answer either uniquely follows from multiple texts directly or is suggested as likely. For  $26\\%$  , more than one candidate is plausibly sup- ported by the documents, including the correct an- swer. This is often due to hypernymy, where the appropriate level of granularity for the an- swer is difﬁcult to predict – e.g.  (west suffolk, administrative entity, ?) with candidates suffolk  and  england . This is a direct conse- quence of including type-consistent false answer candidates from W IKIDATA , which can lead to ques- tions with several true answers. For   $9\\%$   of the cases a single document sufﬁces; these samples contain a document that states enough information about  item  and  answer  together. For example, the query  (Louis Auguste, father, ?)  has the correct answer  Louis XIV of France , and  French king Louis XIV  is mentioned within the same doc- "}
{"page": 6, "image_path": "doc_images/Q18-1021_6.jpg", "ocr_text": "Unique multi-step answer. 36%\nLikely multi-step unique answer. 9%\nMultiple plausible answers. 15%\nAmbiguity due to hypernymy. 11%\nOnly single document required. 9%\nAnswer does not follow. 12%\nWIKIDATA/WIKIPEDIA discrepancy. 8%\n\nTable 3: Qualitiative analysis of WIKIHOP samples.\n\nument as Louis Auguste. Finally, although our\ntask is significantly more complex than most pre-\nvious tasks where distant supervision has been ap-\nplied, the distant supervision assumption is only vi-\nolated for 20% of the samples — a proportion sim-\nilar to previous work (Riedel et al., 2010). These\ncases can either be due to conflicting information be-\ntween WIKIDATA and WIKIPEDIA (8%), e.g. when\nthe date of birth for a person differs between WIKI-\nDATA and what is stated in the WIKIPEDIA article,\nor because the answer is consistent but cannot be\ninferred from the support documents (12%). When\nanswering 100 questions, the annotator knew the an-\nswer prior to reading the documents for 9%, and pro-\nduced the correct answer after reading the document\nsets for 74% of the cases. On 100 questions of a val-\nidated portion of the Dev set (see Section 5.3), 85%\naccuracy was reached.\n\nMEDHOP Since both document complexity and\nnumber of documents per sample were significantly\nlarger compared to WIKIHOP, it was not feasible to\nask an annotator to read all support documents for\n100 samples. We thus opted to verify the dataset\nquality by providing only the subset of documents\nrelevant to support the correct answer, i.e., those tra-\nversed along the path reaching the answer. The an-\nnotator was asked if the answer to the query “fol-\nlows”, “is likely”, or “does not follow”, given the\nrelevant documents. 68% of the cases were consid-\nered as “follows” or as “is likely”. The majority\nof cases violating the distant supervision assumption\nwere errors due to the lack of a necessary PPI in one\nof the connecting documents.\n\n5.2 Crowdsourced Human Annotation\n\nWe asked human annotators on Amazon Mechanical\nTurk to evaluate samples of the WIKIHOP develop-\n\n293\n\nment set. Similar to our qualitative analysis of MED-\nHOP, annotators were shown the query-answer pair\nas a fact and the chain of relevant documents leading\nto the answer. They were then instructed to answer\n(1) whether they knew the fact before; (2) whether\nthe fact follows from the texts (with options “fact\nfollows”, “fact is likely”, and “fact does not fol-\nlow”); and (3); whether a single or several of the\ndocuments are required. Each sample was shown to\nthree annotators and a majority vote was used to ag-\ngregate the annotations. Annotators were familiar\nwith the fact 4.6% of the time; prior knowledge of\nthe fact is thus not likely to be a confounding effect\non the other judgments. Inter-annotator agreement\nas measured by Fleiss’ kappa is 0.253 in (2), and\n0.281 in (3) — indicating a fair overall agreement, ac-\ncording to Landis and Koch (1977). Overall, 9.5%\nof samples have no clear majority in (2).\n\nAmong samples with a majority judgment, 59.8%\nare cases where the fact “follows”, for 14.2% the\nfact is judged as “likely”, and as “not follow” for\n25.9%. This again provides good justification for\nthe distant supervision strategy.\n\nAmong the samples with a majority vote for (2)\nof either “follows” or “likely”, 55.9% were marked\nwith a majority vote as requiring multiple docu-\nments to infer the fact, and 44.1% as requiring only\na single document. The latter number is larger than\ninitially expected, given the construction of samples\nthrough graph traversal. However, when inspecting\ncases judged as “single” more closely, we observed\nthat many indeed provide a clear hint about the cor-\nrect answer within one document, but without stat-\ning it explicitly. For example, for the fact (witold\ncichy, country-of_citizenship, poland) with\ndocuments d,: Witold Cichy (born March 15, 1986\nin Wodzisaw Iski) is a Polish footballer{...] and do:\nWodzisaw Iski[...] is a town in Silesian Voivodeship,\nsouthern Poland{...], the information provided in d,\nsuffices for a human given the background knowl-\nedge that Polish is an attribute related to Poland, re-\nmoving the need for dy to infer the answer.\n\n5.3 Validated Test Sets\n\nWhile training models on distantly supervised data\nis useful, one should ideally evaluate methods on a\nmanually validated test set. We thus identified sub-\nsets of the respective test sets for which the correct\n", "vlm_text": "Unique multi-step answer.  $36\\%$  Likely multi-step unique answer.  $9\\%$  Multiple plausible answers.  $15\\%$  Ambiguity due to hypernymy.  $11\\%$  Only single document required.  $9\\%$  \nAnswer does not follow.  $12\\%$  W IKIDATA /W IKIPEDIA  discrepancy.  $8\\%$  \nTable 3: Qualitiative analysis of W IKI H OP  samples. \nument as  Louis Auguste . Finally, although our task is signiﬁcantly more complex than most pre- vious tasks where distant supervision has been ap- plied, the distant supervision assumption is only vi- olated for   $20\\%$   of the samples – a proportion sim- ilar to previous work (Riedel et al., 2010). These cases can either be due to conﬂicting information be- tween W IKIDATA  and W IKIPEDIA    $(8\\%)$  , e.g. when the date of birth for a person differs between W IKI - DATA  and what is stated in the W IKIPEDIA  article, or because the answer is consistent but cannot be inferred from the support documents   $(12\\%)$  . When answering 100 questions, the annotator knew the an- swer prior to reading the documents for  $9\\%$  , and pro- duced the correct answer after reading the document sets for  $74\\%$   of the cases. On 100 questions of a val- idated portion of the Dev set (see Section 5.3),  $85\\%$  accuracy was reached. \nM ED H OP Since both document complexity and number of documents per sample were signiﬁcantly larger compared to W IKI H OP , it was not feasible to ask an annotator to read  all  support documents for 100 samples. We thus opted to verify the dataset quality by providing only the subset of documents relevant to support the correct answer, i.e., those tra- versed along the path reaching the answer. The an- notator was asked if the answer to the query  “fol- lows” ,  “is likely” , or  “does not follow” , given the relevant documents.   $68\\%$   of the cases were consid- ered as  “follows”  or as  “is likely” . The majority of cases violating the distant supervision assumption were errors due to the lack of a necessary PPI in one of the connecting documents. \n5.2 Crowdsourced Human Annotation \nWe asked human annotators on  Amazon Mechanical Turk  to evaluate samples of the W IKI H OP  develop- ment set. Similar to our qualitative analysis of M ED - H OP , annotators were shown the query-answer pair as a fact and the chain of relevant documents leading to the answer. They were then instructed to answer (1) whether they knew the fact before; (2) whether the fact follows from the texts (with options  “fact follows” ,  “fact is likely” , and  “fact does not fol- low” ); and (3); whether a single or several of the documents are required. Each sample was shown to three annotators and a majority vote was used to ag- gregate the annotations. Annotators were familiar with the fact  $4.6\\%$   of the time; prior knowledge of the fact is thus not likely to be a confounding effect on the other judgments. Inter-annotator agreement as measured by Fleiss’ kappa is 0.253 in (2), and 0.281 in (3) – indicating a fair overall agreement, ac- cording to Landis and Koch (1977). Overall,   $9.5\\%$  of samples have no clear majority in (2). \n\nAmong samples with a majority judgment,  $59.8\\%$  are cases where the fact  “follows” , for   $14.2\\%$   the fact is judged as  “likely” , and as  “not follow”  for  $25.9\\%$  . This again provides good justiﬁcation for the distant supervision strategy. \nAmong the samples with a majority vote for (2) of either  “follows”  or  “likely” ,   $55.9\\%$   were marked with a majority vote as requiring multiple docu- ments to infer the fact, and   $44.1\\%$   as requiring only a single document. The latter number is larger than initially expected, given the construction of samples through graph traversal. However, when inspecting cases judged as  “single”  more closely, we observed that many indeed provide a clear hint about the cor- rect answer within one document, but without stat- ing it explicitly. For example, for the fact  (witold cichy, country of citizenship, poland)  with documents  $d_{1}$  : Witold Cichy (born March 15, 1986 in Wodzisaw lski) is a Polish footballer[...]  and  $d_{2}$  : Wodzisaw lski[...] is a town in Silesian Voivodeship, southern Poland[...] , the information provided in  $d_{1}$  sufﬁces for a human given the background knowl- edge that  Polish  is an attribute related to  Poland , re- moving the need for    $d_{2}$   to infer the answer. \n5.3 Validated Test Sets \nWhile training models on distantly supervised data is useful, one should ideally evaluate methods on a manually validated test set. We thus identiﬁed sub- sets of the respective test sets for which the correct answer can be inferred from the text. This is in con- trast to prior work such as Hermann et al. (2015), Hill et al. (2016), and Hewlett et al. (2016), who evaluate only on distantly supervised samples. For W IKI H OP , we applied the same annotation strategy as described in Section 5.2. The validated test set consists of those samples labeled by a majority of annotators (at least 2 of 3) as  “follows” , and requir- ing  “multiple”  documents. While desirable, crowd- sourcing is not feasible for M ED H OP  since it re- quires specialist knowledge. In addition, the number of document paths is  ${\\approx}3\\mathrm{x}$   larger, which along with the complexity of the documents greatly increases the annotation time. We thus manually annotated  $20\\%$   of the M ED H OP  test set and identiﬁed the sam- ples for which the text implies the correct answer and where multiple documents are required. "}
{"page": 7, "image_path": "doc_images/Q18-1021_7.jpg", "ocr_text": "answer can be inferred from the text. This is in con-\ntrast to prior work such as Hermann et al. (2015),\nHill et al. (2016), and Hewlett et al. (2016), who\nevaluate only on distantly supervised samples. For\nWIKIHOP, we applied the same annotation strategy\nas described in Section 5.2. The validated test set\nconsists of those samples labeled by a majority of\nannotators (at least 2 of 3) as “follows”, and requir-\ning “multiple” documents. While desirable, crowd-\nsourcing is not feasible for MEDHOP since it re-\nquires specialist knowledge. In addition, the number\nof document paths is 3x larger, which along with\nthe complexity of the documents greatly increases\nthe annotation time. We thus manually annotated\n20% of the MEDHOP test set and identified the sam-\nples for which the text implies the correct answer\nand where multiple documents are required.\n\n6 Experiments\n\nThis section describes experiments on WIKIHOP\nand MEDHOpP with the goal of establishing the per-\normance of several baseline models, including re-\ncent neural RC models. We empirically demonstrate\nhe importance of mitigating dataset biases, probe\nwhether multi-step behavior is beneficial for solv-\ning the task, and investigate if RC models can learn\no perform lexical abstraction. Training will be con-\nducted on the respective training sets, and evaluation\non both the full test set and validated portion (Sec-\nion 5.3) allowing for a comparison between the two.\n\n6.1 Models\n\nRandom Selects a random candidate; note that the\nnumber of candidates differs between samples.\n\nMax-mention Predicts the most frequently men-\ntioned candidate in the support documents Sy of a\nsample — randomly breaking ties.\n\nMajority-candidate-per-query-type Predicts the\ncandidate c € C,, that was most frequently observed\nas the true answer in the training set, given the query\ntype of g. For WIKIHOP, the query type is the prop-\nerty p of the query; for MEDHOP there is only the\nsingle query type — interacts_with.\n\nTF-IDF Retrieval-based models are known to be\nstrong QA baselines if candidate answers are pro-\nvided (Clark et al., 2016; Welbl et al., 2017). They\n\n294\n\nsearch for individual documents based on keywords\nin the question, but typically do not combine infor-\nmation across documents. The purpose of this base-\nline is to see if it is possible to identify the correct an-\nswer from a single document alone through lexical\ncorrelations. The model forms its prediction as fol-\nlows: For each candidate c, the concatenation of the\nquery gq with c is fed as an OR query into the whoosh\ntext retrieval engine. It then predicts the candidate\nwith the highest TF-IDF similarity score:\narg max[max(TF-IDF(q + c, s))]\n\nceC, 8 Sq\n\n()\n\nDocument-cue During dataset construction we\nobserved that certain document-answer pairs appear\nmore frequently than others, to the effect that the\ncorrect candidate is often indicated solely by the\npresence of certain documents in S,. This baseline\ncaptures how easy it is for a model to exploit these\ninformative document-answer co-occurrences. It\npredicts the candidate with highest score across Cy:\n\n(2)\n\narg max{max(cooccurrence(d, c))]\nceC, —dESq\nExtractive RC models: FastQA and BiDAF In\nour experiments we evaluate two recently proposed\nLSTM-based extractive QA models: the Bidirec-\ntional Attention Flow model (BiDAF, Seo et al.\n(2017a)), and FastQA (Weissenborn et al., 2017),\nwhich have shown a robust performance across sev-\neral datasets. These models predict an answer span\nwithin a single document. We adapt them to a multi-\ndocument setting by sequentially concatenating all\nd € Sq in random order into a superdocument,\nadding document separator tokens. During training,\nthe first answer mention in the concatenated docu-\nment serves as the gold span.* At test time, we mea-\nsured accuracy based on the exact match between\nthe prediction and answer, both lowercased, after re-\nmoving articles, trailing white spaces and punctu-\nation, in the same way as Rajpurkar et al. (2016).\nTo rule out any signal stemming from the order of\ndocuments in the superdocument, this order is ran-\ndomized both at training and test time. In a prelimi-\nnary experiment we also trained models using differ-\nent random document order permutations, but found\nthat performance did not change significantly.\n4 We also tested assigning the gold span randomly to any\none of the mention of the answer, with insignificant changes.\n", "vlm_text": "\n6 Experiments \nThis section describes experiments on W IKI H OP and M ED H OP  with the goal of establishing the per- formance of several baseline models, including re- cent neural RC models. We empirically demonstrate the importance of mitigating dataset biases, probe whether multi-step behavior is beneﬁcial for solv- ing the task, and investigate if RC models can learn to perform lexical abstraction. Training will be con- ducted on the respective training sets, and evaluation on both the full test set and validated portion (Sec- tion 5.3) allowing for a comparison between the two. \n6.1 Models \nRandom Selects a random candidate; note that the number of candidates differs between samples. \nMax-mention Predicts the most frequently men- tioned candidate in the support documents    $S_{q}$   of a sample – randomly breaking ties. \nMajority-candidate-per-query-type Predicts the candidate  $c\\in C_{q}$   that was most frequently observed as the true answer in the training set, given the query type of    $q$  . For W IKI H OP , the query type is the prop- erty    $p$   of the query; for M ED H OP  there is only the single query type –  interacts with . \nTF-IDF Retrieval-based models are known to be strong QA baselines if candidate answers are pro- vided (Clark et al., 2016; Welbl et al., 2017). They search for individual documents based on keywords in the question, but typically do not combine infor- mation across documents. The purpose of this base- line is to see if it is possible to identify the correct an- swer from a single document alone through lexical correlations. The model forms its prediction as fol- lows: For each candidate    $c$  , the concatenation of the query  $q$   with    $c$   is fed as an  $O R$   query into the  whoosh text retrieval engine. It then predicts the candidate with the highest TF-IDF similarity score: \n\n\n$$\n\\underset{c\\in C_{q}}{\\arg\\operatorname*{max}}\\big[\\underset{s\\in S_{q}}{\\operatorname*{max}}(T F\\mathbf{\\cdot}I D F(q+c,s))\\big]\n$$\n \nDocument-cue During dataset construction we observed that certain document-answer pairs appear more frequently than others, to the effect that the correct candidate is often indicated solely by the presence of certain documents in    $S_{q}$  . This baseline captures how easy it is for a model to exploit these informative document-answer co-occurrences. It predicts the candidate with highest score across    $C_{q}$  : \n\n$$\n\\arg\\operatorname*{max}_{c\\in C_{q}}[\\operatorname*{max}_{d\\in S_{q}}(c o o c c u r r e n c e(d,c))]\n$$\n \nExtractive RC models: FastQA and BiDAF In our experiments we evaluate two recently proposed LSTM -based extractive QA models: the Bidirec- tional Attention Flow model   $(B i D A F$  , Seo et al. (2017a)), and  FastQA  (Weissenborn et al., 2017), which have shown a robust performance across sev- eral datasets. These models predict an answer span within a  single  document. We adapt them to a multi- document setting by sequentially concatenating all  $d\\ \\in\\ S_{q}$   in random order into a superdocument, adding document separator tokens. During training, the ﬁrst answer mention in the concatenated docu- ment serves as the gold span.   At test time, we mea- sured accuracy based on the exact match between the prediction and answer, both lowercased, after re- moving articles, trailing white spaces and punctu- ation, in the same way as Rajpurkar et al. (2016). To rule out any signal stemming from the order of documents in the superdocument, this order is ran- domized both at training and test time. In a prelimi- nary experiment we also trained models using differ- ent random document order permutations, but found that performance did not change signiﬁcantly. "}
{"page": 8, "image_path": "doc_images/Q18-1021_8.jpg", "ocr_text": "For BiDAF, the default hyperparameters from the\nimplementation of Seo et al. (2017a) are used, with\npretrained GloVe (Pennington et al., 2014) embed-\ndings. However, we restrict the maximum docu-\nment length to 8,192 tokens and hidden size to 20,\nand train for 5,000 iterations with batchsize 16 in or-\nder to fit the model into memory.> For FastQA we\nuse the implementation provided by the authors, also\nwith pre-trained GloVe embeddings, no character-\nembeddings, no maximum support length, hidden\nsize 50, and batch size 64 for 50 epochs.\n\nWhile BiDAF and FastQA were initially devel-\noped and tested on single-hop RC datasets, their us-\nage of bidirectional LSTMs and attention over the\nfull sequence theoretically gives them the capacity\nto integrate information from different locations in\nthe (super-)document. In addition, BiDAF employs\niterative conditioning across multiple layers, poten-\ntially making it even better suited to integrate infor-\nmation found across the sequence.\n\n6.2. Lexical Abstraction: Candidate Masking\n\nThe presence of lexical regularities among an-\nswers is a problem in RC dataset assembly — a\nphenomenon already observed by Hermann et al.\n(2015). When comprehending a text, the correct an-\nswer should become clear from its context — rather\nthan from an intrinsic property of the answer ex-\npression. To evaluate the ability of models to rely\non context alone, we created masked versions of\nthe datasets: we replace any candidate expression\nrandomly using 100 unique placeholder tokens, e.g.\n“Mumbai is the most populous city in MASK7.”\nMasking is consistent within one sample, but gen-\nerally different for the same expression across sam-\nples. This not only removes answer frequency cues,\nit also removes statistical correlations between fre-\nquent answer strings and support documents. Mod-\nels consequently cannot base their prediction on in-\ntrinsic properties of the answer expression, but have\nto rely on the context surrounding the mentions.\n\n6.3 Results and Discussion\n\nTable 5 shows the experimental outcomes for WIK-\nIHoP and MEDHOP, together with results for the\nmasked setting; we will first discuss the former. A\n\n> The superdocument has a larger number of tokens com-\npared to e.g. SQuAD, thus the additional memory requirements.\n\n295\n\nModel Unfiltered Filtered\n\nDocument-cue 74.6 36.7\nMaj. candidate 41.2 38.8\nTF-IDF 43.8 25.6\nTrain set size 527,773 43,738\n\nTable 4: Accuracy comparison for simple baseline mod-\nels on WIKIHOP before and after filtering.\n\nfirst observation is that candidate mention frequency\ndoes not produce better predictions than a random\nguess. Predicting the answer most frequently ob-\nserved at training time achieves strong results: as\nmuch as 38.8% / 44.2% and 58.4% / 67.3% on the\ntwo datasets, for the full and validated test sets re-\nspectively. That is, a simple frequency statistic to-\ngether with answer type constraints alone is a rela-\ntively strong predictor, and the strongest overall for\nthe “unmasked” version of MEDHOP.\n\nThe TF-IDF retrieval baseline clearly performs\nbetter than random for WIKIHOP, but is not very\nstrong overall. That is, the question tokens are help-\nful to detect relevant documents, but exploiting only\nthis information compares poorly to the other base-\nlines. On the other hand, as no co-mention of an\ninteracting drug pair occurs within any single doc-\nument in MEDHOopP, the TF-IDF baseline performs\nworse than random. We conclude that lexical match-\ning with a single support document is not enough to\nbuild a strong predictive model for both datasets.\n\nThe Document-cue baseline can predict more than\na third of the samples correctly, for both datasets,\neven after sub-sampling frequent document-answer\npairs for WIKIHOP. The relative strength of this\nand other baselines proves to be an important is-\nsue when designing multi-hop datasets, which we\naddressed through the measures described in Sec-\ntion 3.2. In Table 4 we compare the two relevant\nbaselines on WIKIHOP before and after applying\nfiltering measures. The absolute strength of these\nbaselines before filtering shows how vital address-\ning this issue is: 74.6% accuracy could be reached\nthrough exploiting the cooccurrence(d, c) statistic\nalone. This underlines the paramount importance of\ninvestigating and addressing dataset biases that oth-\nerwise would confound seemingly strong RC model\nperformance. The relative drop demonstrates that\n", "vlm_text": "For  $B i D A F$  , the default hyperparameters from the implementation of Seo et al. (2017a) are used, with pretrained GloVe (Pennington et al., 2014) embed- dings. However, we restrict the maximum docu- ment length to 8,192 tokens and hidden size to 20, and train for 5,000 iterations with batchsize 16 in or- der to ﬁt the model into memory.   For  FastQA  we use the implementation provided by the authors, also with pre-trained GloVe embeddings, no character- embeddings, no maximum support length, hidden size 50, and batch size 64 for 50 epochs. \nWhile    $B i D A F$   and  FastQA  were initially devel- oped and tested on single-hop RC datasets, their us- age of bidirectional LSTMs and attention over the full sequence theoretically gives them the capacity to integrate information from different locations in the (super-)document. In addition,  $B i D A F$   employs iterative conditioning across multiple layers, poten- tially making it even better suited to integrate infor- mation found across the sequence. \n6.2 Lexical Abstraction: Candidate Masking \nThe presence of lexical regularities among an- swers is a problem in RC dataset assembly – a phenomenon already observed by Hermann et al. (2015). When comprehending a text, the correct an- swer should become clear from its context – rather than from an intrinsic property of the answer ex- pression. To evaluate the ability of models to rely on context alone, we created  masked  versions of the datasets: we replace any candidate expression randomly using 100 unique placeholder tokens, e.g. “Mumbai is the most populous city in  MASK7 .” Masking is consistent within one sample, but gen- erally different for the same expression across sam- ples. This not only removes answer frequency cues, it also removes statistical correlations between fre- quent answer strings and support documents. Mod- els consequently cannot base their prediction on in- trinsic properties of the answer expression, but have to rely on the context surrounding the mentions. \n6.3 Results and Discussion \nTable 5 shows the experimental outcomes for W IK - I H OP  and M ED H OP , together with results for the masked  setting; we will ﬁrst discuss the former. A \nThe table presents data with three rows and two columns of numerical values. Each row is associated with a label:\n\n1. Document-cue:\n   - 74.6\n   - 36.7\n\n2. Maj. candidate:\n   - 41.2\n   - 38.8\n\n3. TF-IDF:\n   - 43.8\n   - 25.6\n\nWithout additional context or a caption, it's unclear what these numbers specifically refer to or what the labels mean. They likely represent scores or measurements related to document retrieval, ranking, or some form of text processing/analysis method.\nﬁrst observation is that candidate mention frequency does not produce better predictions than a random guess. Predicting the answer most frequently ob- served at training time achieves strong results: as much as   $38.8\\%~/~44.2\\%$   and   $58.4\\%~/~67.3\\%$   on the two datasets, for the full and validated test sets re- spectively. That is, a simple frequency statistic to- gether with answer type constraints alone is a rela- tively strong predictor, and the strongest overall for the  “unmasked”  version of M ED H OP . \nThe TF-IDF retrieval baseline clearly performs better than random for W IKI H OP , but is not very strong overall. That is, the question tokens are help- ful to detect relevant documents, but exploiting only this information compares poorly to the other base- lines. On the other hand, as no co-mention of an interacting drug pair occurs within any single doc- ument in M ED H OP , the TF-IDF baseline performs worse than random. We conclude that lexical match- ing with a single support document is not enough to build a strong predictive model for both datasets. \nThe  Document-cue  baseline can predict more than a third of the samples correctly, for both datasets, even after sub-sampling frequent document-answer pairs for W IKI H OP . The relative strength of this and other baselines proves to be an important is- sue when designing multi-hop datasets, which we addressed through the measures described in Sec- tion 3.2. In Table 4 we compare the two relevant baselines on W IKI H OP  before and after applying ﬁltering measures. The absolute strength of these baselines before ﬁltering shows how vital address- ing this issue is:   $74.6\\%$   accuracy could be reached through exploiting the  cooccurrence  $(d,c)$   statistic alone. This underlines the paramount importance of investigating and addressing dataset biases that oth- erwise would confound seemingly strong RC model performance. The relative drop demonstrates that "}
{"page": 9, "image_path": "doc_images/Q18-1021_9.jpg", "ocr_text": "WIKIHOP MEDHOopP\nstandard masked standard masked\nModel test test* test test* test test* test test*\nRandom 11.5 12.22 122 130 139 204 141 224\nMax-mention 10.6 15.9 13.9 20.1 95 16.3 9.2 16.3\nMajority-candidate-per-query-type 38.8 442 12.0 13.7 584 67.3 104 6.1\nTF-IDF 25.6 36.7 144 24.2 9.0 143 8.8 14.3\nDocument-cue 36.7 41.7 74 20.3 449 53.1 15.2 163\nFastQA 25.7 27.2 35.8 38.0 23.1 245 31.3 30.6\nBiDAF 42.9 49.7 545 598 47.8 61.2 33.7 42.9\n\nTable 5: Test accuracies for the WIKIHOP and MEDHOpP datasets, both in standard (unmasked) and masked setup.\nColumns marked with asterisk are for the validated portion of the dataset.\n\nWIKIHOP MEDHop\nstandard gold chain standard gold chain\nModel test test* test test* test test* test — test*\nBiDAF 42.9 49.7 57.9 634 47.8 61.2 864 89.8\nBiDAF mask 54.5 59.8 81.2 85.7 33.7 42.9 99.3 100.0\nFastQA 25.7 27.2 445 535 23.1 245 54.6 59.2\nFastQA mask 35.8 38.0 65.3 70.0 31.3 306 51.8 55.1\n\nTable 6: Test accuracy comparison when only using documents leading to the correct answer (gold chain). Columns\n\nwith asterisk hold results for the validated samples.\n\nthe measures undertaken successfully mitigate the\nissue. A downside to aggressive filtering is a signif-\nicantly reduced dataset size, rendering it infeasible\nfor smaller datasets like MEDHOP.\n\nAmong the two neural models, BiDAF is overall\nstrongest across both datasets — this is in contrast to\nthe reported results for SQUAD where their perfor-\nmance is nearly indistinguishable. This is possibly\ndue to the iterative latent interactions in the BiDAF\narchitecture: we hypothesize that these are of in-\ncreased importance for our task, where information\nis distributed across documents. It is worth empha-\nsizing that unlike the other baselines, both FastQA\nand BiDAF predict the answer by extracting a span\nfrom the support documents without relying on the\ncandidate options C,.\n\nIn the masked setup all baseline models reliant on\nlexical cues fail in the face of the randomized answer\nexpressions, since the same answer option has dif-\nferent placeholders in different samples. Especially\non MEDHOpP, where dataset sub-sampling is not a\n\n296\n\nviable option, masking proves to be a valuable alter-\nnative, effectively circumventing spurious statistical\ncorrelations that RC models can learn to exploit.\n\nBoth neural RC models are able to largely retain\nor even improve their strong performance when an-\nswers are masked: they are able to leverage the tex-\ntual context of the candidate expressions. To under-\nstand differences in model behavior between WIK-\nTHop and MEDHOp, it is worth noting that drug\nmentions in MEDHOP are normalized to a unique\nsingle-word identifier, and performance drops under\nmasking. In contrast, for the open-domain setting of\nWIKIHOP, a reduction of the answer vocabulary to\n100 random single-token mask expressions clearly\nhelps the model in selecting a candidate span, com-\npared to the multi-token candidate expressions in the\nunmasked setting. Overall, although both neural RC\nmodels clearly outperform the other baselines, they\nstill have large room for improvement compared to\nhuman performance at 74% / 85% for WIKIHOP.\n\nComparing results on the full and validated test\n", "vlm_text": "The table presents performance metrics for different models evaluated under \"standard\" and \"masked\" conditions. The conditions are further divided into \"test\" and \"test*\" categories. \n\nHere's a breakdown:\n\n- **Models:**\n  - Random\n  - Max-mention\n  - Majority-candidate-per-query-type\n  - TF-IDF\n  - Document-cue\n  - FastQA\n  - BiDAF\n\n- **Columns:**\n  - Standard test\n  - Standard test*\n  - Masked test\n  - Masked test*\n  \nEach entry shows numerical values indicating the performance of each model under those specific conditions. The highest values for each condition tend to be highlighted, suggesting better performance.\nThe table presents the performance of different models on two datasets: WikiHop and MedHop. The models are BiDAF, BiDAF mask, FastQA, and FastQA mask. For each dataset, the table shows results under two conditions: \"standard\" and \"gold chain.\" Additionally, results are provided for two test conditions: \"test\" and \"test*.\"\n\n- **WikiHop:**\n  - **Standard:**\n    - BiDAF: Test (42.9), Test* (49.7)\n    - BiDAF mask: Test (54.5), Test* (59.8)\n    - FastQA: Test (25.7), Test* (27.2)\n    - FastQA mask: Test (35.8), Test* (38.0)\n  - **Gold Chain:**\n    - BiDAF: Test* (57.9), Test* (63.4)\n    - BiDAF mask: Test (81.2), Test* (85.7)\n    - FastQA: Test (44.5), Test* (53.5)\n    - FastQA mask: Test (65.3), Test* (70.0)\n\n- **MedHop:**\n  - **Standard:**\n    - BiDAF: Test (47.8), Test* (61.2)\n    - BiDAF mask: Test (33.7), Test* (42.9)\n    - FastQA: Test (23.1), Test* (24.5)\n    - FastQA mask: Test (31.3), Test* (30.6)\n  - **Gold Chain:**\n    - BiDAF: Test (86.4), Test* (89.8)\n    - BiDAF mask: Test (99.3), Test* (100.0)\n    - FastQA: Test (54.6), Test* (59.2)\n    - FastQA mask: Test (51.8), Test* (55.1)\n\nThe numbers in the table represent the performance scores of each model, likely in terms of accuracy or a similar metric, with higher scores indicating better performance.\nTable 6: Test accuracy comparison when only using documents leading to the correct answer (gold chain). Columns with asterisk hold results for the validated samples. \nthe measures undertaken successfully mitigate the issue. A downside to aggressive ﬁltering is a signif- icantly reduced dataset size, rendering it infeasible for smaller datasets like M ED H OP . \nAmong the two neural models,  $B i D A F$   is overall strongest across both datasets – this is in contrast to the reported results for SQuAD where their perfor- mance is nearly indistinguishable. This is possibly due to the iterative latent interactions in the    $B i D A F$  architecture: we hypothesize that these are of in- creased importance for our task, where information is distributed across documents. It is worth empha- sizing that unlike the other baselines, both  FastQA and  $B i D A F$   predict the answer by extracting a span from the support documents without relying on the candidate options    $C_{q}$  . \nIn the  masked  setup all baseline models reliant on lexical cues fail in the face of the randomized answer expressions, since the same answer option has dif- ferent placeholders in different samples. Especially on M ED H OP , where dataset sub-sampling is not a viable option, masking proves to be a valuable alter- native, effectively circumventing spurious statistical correlations that RC models can learn to exploit. \n\nBoth neural RC models are able to largely retain or even improve their strong performance when an- swers are masked: they are able to leverage the tex- tual context of the candidate expressions. To under- stand differences in model behavior between W IK - I H OP  and M ED H OP , it is worth noting that drug mentions in M ED H OP  are normalized to a unique single-word identiﬁer, and performance drops under masking. In contrast, for the open-domain setting of W IKI H OP , a reduction of the answer vocabulary to 100 random single-token  mask  expressions clearly helps the model in selecting a candidate span, com- pared to the multi-token candidate expressions in the unmasked setting. Overall, although both neural RC models clearly outperform the other baselines, they still have large room for improvement compared to human performance at  $74\\%\\,/\\,85\\%$   for W IKI H OP . \nComparing results on the full and validated test "}
{"page": 10, "image_path": "doc_images/Q18-1021_10.jpg", "ocr_text": "WIKIHOoP MEDHop\n\ntest test* test test*\nBiDAF 545 598 33.7 42.9\nBiDAF rem 44.6 57.7 30.4 36.7\nFastQA 35.8 38.0 31.3 30.6\nFastQArem 38.0 41.2 286 24.5\n\nTable 7: Test accuracy (masked) when only documents\ncontaining answer candidates are given (rem).\n\nsets, we observe that the results consistently improve\non the validated sets. This suggests that the training\nset contains the signal necessary to make inference\non valid samples at test time, and that noisy samples\nare harder to predict.\n\n6.4 Using only relevant documents\n\nWe conducted further experiments to examine the\nRC models when presented with only the relevant\ndocuments in Sy, i.e., the chain of documents lead-\ning to the correct answer. This allows us to investi-\ngate the hypothetical performance of the models if\nthey were able to select and read only relevant docu-\nments: Table 6 summarizes these results. Models\nimprove greatly in this gold chain setup, with up\nto 81.2% / 85.7% on WIKIHOP in the masked set-\nting for BiDAF. This demonstrates that RC models\nare capable of identifying the answer when few or\nno plausible false candidates are mentioned, which\nis particularly evident for MEDHoP, where docu-\nments tend to discuss only single drug candidates.\nIn the masked gold chain setup, models can then\npick up on what the masking template looks like\nand achieve almost perfect scores. Conversely, these\nresults also show that the models’ answer selec-\ntion process is not robust to the introduction of un-\nrelated documents with type-consistent candidates.\nThis indicates that learning to intelligently select rel-\nevant documents before RC may be among the most\npromising directions for future model development.\n\n6.5 Removing relevant documents\n\nTo investigate if the neural RC models can draw\nupon information requiring multi-step inference we\ndesigned an experiment where we discard all doc-\numents that do not contain candidate mentions, in-\ncluding the first documents traversed. Table 7 shows\n\n297\n\nthe results: we can observe that performance drops\nacross the board for BiDAF. There is a significant\ndrop of 3.3%/6.2% on MEDHOP, and 10.0%/2.1%\non WIKIHopP, demonstrating that BiDAF, is able\nto leverage cross-document information. FastQA\nshows a slight increase of 2.2%/3.2% for WIKIHOP\nand a decrease of 2.7%/4.1% on MEDHoP. While\ninconclusive, it is clear that FastQA with fewer la-\ntent interactions than BiDAF has problems integrat-\ning cross-document information.\n\n7 Related Work\n\nRelated Datasets End-to-end text-based QA has\nwitnessed a surge in interest with the advent of large-\nscale datasets, which have been assembled based\non FREEBASE (Berant et al., 2013; Bordes et al.,\n2015), WIKIPEDIA (Yang et al., 2015; Rajpurkar\net al., 2016; Hewlett et al., 2016), web search\nqueries (Nguyen et al., 2016), news articles (Her-\nmann et al., 2015; Onishi et al., 2016), books (Hill\net al., 2016; Paperno et al., 2016), science ex-\nams (Welbl et al., 2017), and trivia (Boyd-Graber\net al., 2012; Dunn et al., 2017). Besides Trivi-\naQA (Joshi et al., 2017), all these datasets are con-\nfined to single documents, and RC typically does not\nrequire a combination of multiple independent facts.\nIn contrast, WIKIHOP and MEDHOpP are specifi-\ncally designed for cross-document RC and multi-\nstep inference. There exist other multi-hop RC re-\nsources, but they are either very limited in size,\nsuch as the FraCaS test suite, or based on synthetic\nlanguage (Weston et al., 2016). TriviaQA partly\ninvolves multi-step reasoning, but the complexity\nlargely stems from parsing compositional questions.\nOur datasets center around compositional inference\nfrom comparatively simple queries and the cross-\ndocument setup ensures that multi-step inference\ngoes beyond resolving co-reference.\n\nCompositional Knowledge Base Inference\nCombining multiple facts is common for structured\nknowledge resources which formulate facts using\nfirst-order logic. KB inference methods include\nInductive Logic Programming (Quinlan, 1990;\nPazzani et al., 1991; Richards and Mooney, 1991)\nand probabilistic relaxations to logic like Markov\nLogic (Richardson and Domingos, 2006; Schoen-\nmackers et al., 2008). These approaches suffer from\n", "vlm_text": "The table presents the performance results of different models on two datasets, WIKIHOP and MEDHOP. The performance is measured with two sets of test data: \"test\" and \"test*\". The models evaluated are BiDAF, BiDAF rem, FastQA, and FastQA rem. Each cell in the table contains a numerical value representing the performance score for the respective model and dataset combination. \n\nHere's a breakdown of the values:\n\n- For WIKIHOP:\n  - BiDAF: 54.5 (test), 59.8 (test*)\n  - BiDAF rem: 44.6 (test), 57.7 (test*)\n  - FastQA: 35.8 (test), 38.0 (test*)\n  - FastQA rem: 38.0 (test), 41.2 (test*)\n\n- For MEDHOP:\n  - BiDAF: 33.7 (test), 42.9 (test*)\n  - BiDAF rem: 30.4 (test), 36.7 (test*)\n  - FastQA: 31.3 (test), 30.6 (test*)\n  - FastQA rem: 28.6 (test), 24.5 (test*)\n\nThis table likely compares the effectiveness of the BiDAF and FastQA models, with and without some additional \"rem\" technique or modification, across different datasets and test conditions.\nsets, we observe that the results consistently improve on the validated sets. This suggests that the training set contains the signal necessary to make inference on valid samples at test time, and that noisy samples are harder to predict. \n6.4 Using only relevant documents \nWe conducted further experiments to examine the RC models when presented with only the relevant documents in    $S_{q}$  , i.e., the chain of documents lead- ing to the correct answer. This allows us to investi- gate the hypothetical performance of the models if they were able to select and read only relevant docu- ments: Table 6 summarizes these results. Models improve greatly in this  gold chain  setup, with up to  $81.2\\%~/~85.7\\%$   on W IKI H OP  in the masked set- ting for    $B i D A F$  . This demonstrates that RC models are capable of identifying the answer when few or no plausible false candidates are mentioned, which is particularly evident for M ED H OP , where docu- ments tend to discuss only single drug candidates. In the  masked  gold chain setup, models can then pick up on what the masking template looks like and achieve almost perfect scores. Conversely, these results also show that the models’ answer selec- tion process is not robust to the introduction of un- related documents with type-consistent candidates. This indicates that learning to intelligently select rel- evant documents before RC may be among the most promising directions for future model development. \n6.5 Removing relevant documents \nTo investigate if the neural RC models can draw upon information requiring multi-step inference we designed an experiment where we discard all doc- uments that do not contain candidate mentions, in- cluding the ﬁrst documents traversed. Table 7 shows the results: we can observe that performance drops across the board for    $B i D A F$  . There is a signiﬁcant drop of   $3.3\\%/6.2\\%$   on M ED H OP , and   $10.0\\%/2.1\\%$  on W IKI H OP , demonstrating that    $B i D A F$  , is able to leverage cross-document information. FastQA shows a slight increase of   $2.2\\%/3.2\\%$   for W IKI H OP and a decrease of   $2.7\\%/4.1\\%$   on M ED H OP . While inconclusive, it is clear that  FastQA  with fewer la- tent interactions than    $B i D A F$   has problems integrat- ing cross-document information. \n\n7 Related Work \nRelated Datasets End-to-end text-based QA has witnessed a surge in interest with the advent of large- scale datasets, which have been assembled based on F REEBASE  (Berant et al., 2013; Bordes et al., 2015), W IKIPEDIA  (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Her- mann et al., 2015; Onishi et al., 2016), books (Hill et al., 2016; Paperno et al., 2016), science ex- ams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 2017). Besides  Trivi- aQA  (Joshi et al., 2017), all these datasets are con- ﬁned to single documents, and RC typically does not require a combination of multiple independent facts. In contrast, W IKI H OP  and M ED H OP  are speciﬁ- cally designed for cross-document RC and multi- step inference. There exist other multi-hop RC re- sources, but they are either very limited in size, such as the  FraCaS  test suite, or based on synthetic language (Weston et al., 2016). TriviaQA  partly involves multi-step reasoning, but the complexity largely stems from parsing compositional questions. Our datasets center around compositional inference from comparatively simple queries and the cross- document setup ensures that multi-step inference goes beyond resolving co-reference. \nCompositional Knowledge Base Inference Combining multiple facts is common for structured knowledge resources which formulate facts using ﬁrst-order logic. KB inference methods include Inductive Logic Programming (Quinlan, 1990; Pazzani et al., 1991; Richards and Mooney, 1991) and probabilistic relaxations to logic like Markov Logic (Richardson and Domingos, 2006; Schoen- mackers et al., 2008). These approaches suffer from limited coverage and inefﬁcient inference, though efforts to circumvent sparsity have been under- taken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to compos- ite rule learning is the Path Ranking Algorithm (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent these sparsity problems by introducing synthetic links via dense latent embeddings. Several other methods have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2017), and memory networks (Jain, 2016). "}
{"page": 11, "image_path": "doc_images/Q18-1021_11.jpg", "ocr_text": "limited coverage and inefficient inference, though\nefforts to circumvent sparsity have been under-\ntaken (Schoenmackers et al., 2008; Schoenmackers\net al., 2010). A more scalable approach to compos-\nite rule learning is the Path Ranking Algorithm (Lao\nand Cohen, 2010; Lao et al., 2011), which performs\nrandom walks to identify salient paths between\nentities. Gardner et al. (2013) circumvent these\nsparsity problems by introducing synthetic links via\ndense latent embeddings. Several other methods\nhave been proposed, using composition functions\nsuch as vector addition (Bordes et al., 2014),\nRNNs (Neelakantan et al., 2015; Das et al., 2017),\nand memory networks (Jain, 2016).\n\nAll of these previous approaches center around\nlearning how to combine facts from a KB, i.e., in\na structured form with pre-defined schema. That\nis, they work as part of a pipeline, and either rely\non the output of a previous IE step (Banko et al.,\n2007), or on direct human annotation (Bollacker et\nal., 2008) which tends to be costly and biased in cov-\nerage. However, recent neural RC methods (Seo et\nal., 2017a; Shen et al., 2017) have demonstrated that\nend-to-end language understanding approaches can\ninfer answers directly from text — sidestepping in-\ntermediate query parsing and IE steps. Our work\naims to evaluate whether end-to-end multi-step RC\nmodels can indeed operate on raw text documents\nonly — while performing the kind of inference most\ncommonly associated with logical inference meth-\nods operating on structured knowledge.\n\nText-Based Multi-Step Reading Comprehension\nFried et al. (2015) have demonstrated that exploit-\ning information from other related documents based\non lexical semantic similarity is beneficial for re-\nranking answers in open-domain non-factoid QA.\nJansen et al. (2017) chain textual background re-\nsources for science exam QA and provide multi-\nsentence answer explanations. Beyond, a rich col-\nlection of neural models tailored towards multi-step\nRC has been developed. Memory networks (We-\nston et al., 2015; Sukhbaatar et al., 2015; Kumar\net al., 2016) define a model class that iteratively\nattends over textual memory items, and they show\npromising performance on synthetic tasks requiring\nmulti-step reasoning (Weston et al., 2016). One\ncommon characteristic of neural multi-hop models\n\n298\n\nis their rich structure that enables matching and in-\nteraction between question, context, answer candi-\ndates and combinations thereof (Peng et al., 2015;\nWeissenborn, 2016; Xiong et al., 2017; Liu and\nPerez, 2017), which is often iterated over several\ntimes (Sordoni et al., 2016; Neumann et al., 2016;\nSeo et al., 2017b; Hu et al., 2017) and may contain\ntrainable stopping mechanisms (Graves, 2016; Shen\net al., 2017). All these methods show promise for\nsingle-document RC, and by design should be capa-\nble of integrating multiple facts across documents.\nHowever, thus far they have not been evaluated for a\ncross-document multi-step RC task — as in this work.\n\nLearning Search Expansion Other research ad-\ndresses expanding the document set available to\na QA system, either in the form of web navi-\ngation (Nogueira and Cho, 2016), or via query\nreformulation techniques, which often use neural\nreinforcement learning (Narasimhan et al., 2016;\nNogueira and Cho, 2017; Buck et al., 2018). While\nrelated, this work ultimately aims at reformulating\nqueries to better acquire evidence documents, and\nnot at answering queries through combining facts.\n\n8 Conclusions and Future Work\n\nWe have introduced a new cross-document multi-\nhop RC task, devised a generic dataset derivation\nstrategy and applied it to two separate domains. The\nresulting datasets test RC methods in their ability to\nperform composite reasoning — something thus far\nlimited to models operating on structured knowledge\nresources. In our experiments we found that contem-\nporary RC models can leverage cross-document in-\nformation, but a sizeable gap to human performance\nremains. Finally, we identified the selection of rele-\nvant document sets as the most promising direction\nfor future research.\n\nThus far, our datasets center around factoid ques-\ntions about entities, and as extractive RC datasets,\nit is assumed that the answer is mentioned verba-\ntim. While this limits the types of questions one can\nask, these assumptions can facilitate both training\nand evaluation, and future work — once free-form ab-\nstractive answer composition has advanced — should\nmove beyond. We hope that our work will foster\nresearch on cross-document information integration,\nworking towards these long term goals.\n\n", "vlm_text": "\nAll of these previous approaches center around learning how to combine facts from a KB, i.e., in a structured form with pre-deﬁned schema. That is, they work as part of a pipeline, and either rely on the output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in cov- erage. However, recent neural RC methods (Seo et al., 2017a; Shen et al., 2017) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping in- termediate query parsing and IE steps. Our work aims to evaluate whether end-to-end multi-step RC models can indeed operate on raw text documents only – while performing the kind of inference most commonly associated with logical inference meth- ods operating on structured knowledge. \nText-Based Multi-Step Reading Comprehension Fried et al. (2015) have demonstrated that exploit- ing information from other related documents based on lexical semantic similarity is beneﬁcial for re- ranking answers in open-domain non-factoid QA. Jansen et al. (2017) chain textual background re- sources for science exam QA and provide multi- sentence answer explanations. Beyond, a rich col- lection of neural models tailored towards multi-step RC has been developed. Memory networks (We- ston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016) deﬁne a model class that iteratively attends over textual memory items, and they show promising performance on synthetic tasks requiring multi-step reasoning (Weston et al., 2016). One common characteristic of neural multi-hop models is their rich structure that enables matching and in- teraction between question, context, answer candi- dates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2017; Liu and Perez, 2017), which is often iterated over several times (Sordoni et al., 2016; Neumann et al., 2016; Seo et al., 2017b; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017). All these methods show promise for single-document RC, and by design should be capa- ble of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-document multi-step RC task – as in this work. \n\nLearning Search Expansion Other research ad- dresses expanding the document set available to a QA system, either in the form of web navi- gation (Nogueira and Cho, 2016), or via query reformulation techniques, which often use neural reinforcement learning (Narasimhan et al., 2016; Nogueira and Cho, 2017; Buck et al., 2018). While related, this work ultimately aims at reformulating queries to better acquire evidence documents, and not at answering queries through combining facts. \n8 Conclusions and Future Work \nWe have introduced a new cross-document multi- hop RC task, devised a generic dataset derivation strategy and applied it to two separate domains. The resulting datasets test RC methods in their ability to perform composite reasoning – something thus far limited to models operating on structured knowledge resources. In our experiments we found that contem- porary RC models can leverage cross-document in- formation, but a sizeable gap to human performance remains. Finally, we identiﬁed the selection of rele- vant document sets as the most promising direction for future research. \nThus far, our datasets center around factoid ques- tions about entities, and as extractive RC datasets, it is assumed that the answer is mentioned verba- tim. While this limits the types of questions one can ask, these assumptions can facilitate both training and evaluation, and future work – once free-form ab- stractive answer composition has advanced – should move beyond. We hope that our work will foster research on cross-document information integration, working towards these long term goals. "}
{"page": 12, "image_path": "doc_images/Q18-1021_12.jpg", "ocr_text": "Acknowledgments\n\nWe would like to thank the reviewers and the ac-\ntion editor for their thoughtful and constructive sug-\ngestions, as well as Matko BoSnjak, Tim Dettmers,\nPasquale Minervini, Jeff Mitchell, and Sebastian\nRuder for several helpful comments and feedback\non drafts of this paper. This work was supported by\nan Allen Distinguished Investigator Award, a Marie\nCurie Career Integration Award, the EU H2020\nSUMMA project (grant agreement number 688139),\nand an Engineering and Physical Sciences Research\nCouncil scholarship.\n\nReferences\n\nMichael Ashburner, Catherine A. Ball, Judith A. Blake,\nDavid Botstein, Heather Butler, J. Michael Cherry, Al-\nlan P. Davis, Kara Dolinski, Selina S. Dwight, Janan T.\nEppig, Midori A. Harris, David P. Hill, Laurie Issel-\nTarver, Andrew Kasarskis, Suzanna Lewis, John C.\nMatese, Joel E. Richardson, Martin Ringwald, Ger-\nald M. Rubin, and Gavin Sherlock. 2000. Gene on-\ntology: tool for the unification of biology. Nature Ge-\nnetics, 25(1):25.\n\nAmos Bairoch, Brigitte Boeckmann, Serenella Ferro, and\nElisabeth Gasteiger. 2004. Swiss-Prot: Juggling be-\ntween evolution and stability. Briefings in Bioinfor-\nmatics, 5(1):39-55.\n\nMichele Banko, Michael J. Cafarella, Stephen Soderland,\nMatt Broadhead, and Oren Etzioni. 2007. Open infor-\nmation extraction from the web. In Proceedings of the\n20th International Joint Conference on Artifical Intel-\nligence, IJCAV 07, pages 2670-2676.\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533-1544.\n\nTamara Bobic, Roman Klinger, Philippe Thomas, and\nMartin Hofmann-Apitius. 2012. Improving distantly\nsupervised extraction of drug-drug and protein-protein\ninteractions. In Proceedings of the Joint Workshop on\nUnsupervised and Semi-Supervised Learning in NLP,\npages 35-43.\n\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collabo-\nratively created graph database for structuring human\nknowledge. In SIGMOD 08 Proceedings of the 2008\nACM SIGMOD international conference on Manage-\nment of data, pages 1247-1250.\n\n299\n\nAntoine Bordes, Sumit Chopra, and Jason Weston. 2014.\nQuestion answering with subgraph embeddings. In\nEmpirical Methods for Natural Language Processing\n(EMNLP), pages 615-620.\n\nAntoine Bordes, Nicolas Usunier, Sumit Chopra, and\nJason Weston. 2015. Large-scale simple ques-\ntion answering with memory networks. CoRR,\nabs/1506.02075.\n\nJordan Boyd-Graber, Brianna Satinoff, He He, and Hal\nDaumé, III. 2012. Besting the quiz master: Crowd-\nsourcing incremental classification games. In Pro-\nceedings of the 2012 Joint Conference on Empir-\nical Methods in Natural Language Processing and\nComputational Natural Language Learning, EMNLP-\nCoNLL ’12, pages 1290-1301.\n\nChristian Buck, Jannis Bulian, Massimiliano Ciaramita,\nAndrea Gesmundo, Neil Houlsby, Wojciech Gajewski,\nand Wei Wang. 2018. Ask the right questions: Ac-\ntive question reformulation with reinforcement learn-\ning. International Conference on Learning Represen-\ntations (ICLR).\n\nClaudio Carpineto and Giovanni Romano. 2012. A sur-\nvey of automatic query expansion in information re-\ntrieval. ACM Comput. Surv., 44(1):1:1-1:50, January.\n\nDanqi Chen, Jason Bolton, and Christopher D. Manning.\n2016. A thorough examination of the CNN/Daily Mail\nreading comprehension task. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2358-2367.\n\nPeter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Turney, and Daniel\nKhashabi. 2016. Combining retrieval, statistics, and\ninference to answer elementary science questions. In\nProceedings of the Thirtieth AAAI Conference on Arti-\nficial Intelligence, AAAY 16, pages 2580-2586.\n\nKevin Bretonnel Cohen and Lawrence Hunter. 2004.\nNatural language processing and systems biology. Ar-\ntificial Intelligence Methods and Tools for Systems Bi-\nology, pages 147-173.\n\nMark Craven and Johan Kumlien. 1999. Constructing\nbiological knowledge bases by extracting information\nfrom text sources. In Proceedings of the Seventh Inter-\nnational Conference on Intelligent Systems for Molec-\nular Biology, pages 77-86.\n\nRajarshi Das, Arvind Neelakantan, David Belanger, and\nAndrew McCallum. 2017. Chains of reasoning over\nentities, relations, and text using recurrent neural net-\nworks. European Chapter of the Association for Com-\nputational Linguistics (EACL), pages 132-141.\n\nMatthew Dunn, Levent Sagun, Mike Higgins, V. Ugur\nGiiney, Volkan Cirik, and Kyunghyun Cho. 2017.\nSearchQA: A new Q&A dataset augmented with con-\ntext from a search engine. CoRR, abs/1704.05179.\n", "vlm_text": "Acknowledgments \nWe would like to thank the reviewers and the ac- tion editor for their thoughtful and constructive sug- gestions, as well as Matko Boˇ snjak, Tim Dettmers, Pasquale Minervini, Jeff Mitchell, and Sebastian Ruder for several helpful comments and feedback on drafts of this paper. This work was supported by an Allen Distinguished Investigator Award, a Marie Curie Career Integration Award, the EU H2020 SUMMA project (grant agreement number 688139), and an Engineering and Physical Sciences Research Council scholarship. \nReferences \nMichael Ashburner, Catherine A. Ball, Judith A. Blake, David Botstein, Heather Butler, J. Michael Cherry, Al- lan P. Davis, Kara Dolinski, Selina S. Dwight, Janan T. Eppig, Midori A. Harris, David P. Hill, Laurie Issel- Tarver, Andrew Kasarskis, Suzanna Lewis, John C. Matese, Joel E. Richardson, Martin Ringwald, Ger- ald M. Rubin, and Gavin Sherlock. 2000. Gene on- tology: tool for the uniﬁcation of biology.  Nature Ge- netics , 25(1):25. Amos Bairoch, Brigitte Boeckmann, Serenella Ferro, and Elisabeth Gasteiger. 2004. Swiss-Prot: Juggling be- tween evolution and stability.  Brieﬁngs in Bioinfor- matics , 5(1):39–55. Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open infor- mation extraction from the web. In  Proceedings of the 20th International Joint Conference on Artiﬁcal Intel- ligence , IJCAI’07, pages 2670–2676. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In  Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing , pages 1533–1544. Tamara Bobic, Roman Klinger, Philippe Thomas, and Martin Hofmann-Apitius. 2012. Improving distantly supervised extraction of drug-drug and protein-protein interactions. In  Proceedings of the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP , pages 35–43. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collabo- ratively created graph database for structuring human knowledge. In  SIGMOD 08 Proceedings of the 2008 ACM SIGMOD international conference on Manage- ment of data , pages 1247–1250. \nAntoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In Empirical Methods for Natural Language Processing (EMNLP) , pages 615–620. Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple ques- tion answering with memory networks. CoRR , abs/1506.02075. Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daum´ e, III. 2012. Besting the quiz master: Crowd- sourcing incremental classiﬁcation games. In  Pro- ceedings of the 2012 Joint Conference on Empir- ical Methods in Natural Language Processing and Computational Natural Language Learning , EMNLP- CoNLL ’12, pages 1290–1301. Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, and Wei Wang. 2018. Ask the right questions: Ac- tive question reformulation with reinforcement learn- ing.  International Conference on Learning Represen- tations (ICLR) . Claudio Carpineto and Giovanni Romano. 2012. A sur- vey of automatic query expansion in information re- trieval.  ACM Comput. Surv. , 44(1):1:1–1:50, January. Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A thorough examination of the CNN/Daily Mail reading comprehension task. In  Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , pages 2358–2367. Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab- harwal, Oyvind Tafjord, Peter Turney, and Daniel Khashabi. 2016. Combining retrieval, statistics, and inference to answer elementary science questions. In Proceedings of the Thirtieth AAAI Conference on Arti- ﬁcial Intelligence , AAAI’16, pages 2580–2586. Kevin Bretonnel Cohen and Lawrence Hunter. 2004. Natural language processing and systems biology.  Ar- tiﬁcial Intelligence Methods and Tools for Systems Bi- ology , pages 147–173. Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In  Proceedings of the Seventh Inter- national Conference on Intelligent Systems for Molec- ular Biology , pages 77–86. Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. 2017. Chains of reasoning over entities, relations, and text using recurrent neural net- works.  European Chapter of the Association for Com- putational Linguistics (EACL) , pages 132–141. Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G¨ uney, Volkan Cirik, and Kyunghyun Cho. 2017. SearchQA: A new Q&A dataset augmented with con- text from a search engine.  CoRR , abs/1704.05179. "}
{"page": 13, "image_path": "doc_images/Q18-1021_13.jpg", "ocr_text": "Antonio Fabregat, Konstantinos Sidiropoulos, Phani\nGarapati, Marc Gillespie, Kerstin Hausmann, Robin\nHaw, Bijay Jassal, Steven Jupe, Florian Korninger,\nSheldon McKay, Lisa Matthews, Bruce May, Mar-\nija Milacic, Karen Rothfels, Veronica Shamovsky,\nMarissa Webber, Joel Weiser, Mark Williams, Guan-\nming Wu, Lincoln Stein, Henning Hermjakob, and\nPeter D’Eustachio. 2016. The Reactome path-\nway knowledgebase. Nucleic Acids Research,\n44(D1):D481-D487.\n\nDaniel Fried, Peter Jansen, Gustave Hahn-Powell, Mihai\nSurdeanu, and Peter Clark. 2015. Higher-order lexi-\ncal semantic models for non-factoid answer reranking.\nTransactions of the Association of Computational Lin-\nguistics, 3:197-210.\n\nMatt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and\nTom M. Mitchell. 2013. Improving learning and infer-\nence in a large knowledge-base using latent syntactic\ncues. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, pages 833—\n838.\n\nAlex Graves. 2016. Adaptive computation time for re-\ncurrent neural networks. CoRR, abs/1603.08983.\n\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius, and\nLuca Toldo. 2012. Development of a benchmark cor-\npus to support the automatic extraction of drug-related\nadverse effects from medical case reports. Journal of\nBiomedical Informatics, 45(5):885 — 892. Text Min-\ning and Natural Language Processing in Pharmacoge-\nnomics.\n\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Information\nProcessing Systems, pages 1693-1701.\n\nWilliam Hersh, Aaron Cohen, Lynn Ruslen, and Phoebe\nRoberts. 2007. TREC 2007 genomics track overview.\nIn NIST Special Publication.\n\nDaniel Hewlett, Alexandre Lacoste, Llion Jones, Illia\nPolosukhin, Andrew Fandrianto, Jay Han, Matthew\nKelcey, and David Berthelot. 2016. WIKIREADING:\nA novel large-scale language understanding task over\nWikipedia. In Proceedings of the The 54th Annual\nMeeting of the Association for Computational Linguis-\ntics (ACL 2016), pages 1535-1545.\n\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason We-\nston. 2016. The goldilocks principle: Reading chil-\ndren’s books with explicit memory representations.\nICLR.\n\nLynette Hirschman, Alexander Yeh, Christian Blaschke,\nand Alfonso Valencia. 2005. Overview of BioCre-\nAtIvE: Critical assessment of information extraction\nfor biology. BMC Bioinformatics, 6(1):S1, May.\n\n300\n\nMinghao Hu, Yuxing Peng, and Xipeng Qiu. 2017.\nMnemonic reader for machine comprehension. CoRR,\nabs/1705.02798.\n\nSarthak Jain. 2016. Question answering over knowledge\nbase using factual memory networks. In Proceedings\nof NAACL-HLT, pages 109-115.\n\nPeter Jansen, Rebecca Sharp, Mihai Surdeanu, and Peter\nClark. 2017. Framing QA as building and ranking in-\ntersentence answer justifications. Computational Lin-\nguistics, 43(2):407-449.\n\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\n\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, July.\n\nRudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan\nKleindienst. 2016. Text understanding with the at-\ntention sum reader network. Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 908-918.\n\nJin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori\nYonezawa. 2011. Overview of Genia event task in\nBioNLP shared task 2011. In Proceedings of BioNLP\nShared Task 2011 Workshop, pages 7-15.\n\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nIshaan Gulrajani James Bradbury, Victor Zhong, Ro-\nmain Paulus, and Richard Socher. 2016. Ask me\nanything: Dynamic memory networks for natural lan-\nguage processing. International Conference on Ma-\nchine Learning, 48:1378-1387.\n\nJ. Richard Landis and Gary G. Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nBiometrics, pages 159-174.\n\nNi Lao and William W Cohen. 2010. Relational re-\ntrieval using a combination of path-constrained ran-\ndom walks. Machine learning, 81(1):53-67.\n\nNi Lao, Tom Mitchell, and William W. Cohen. 2011.\nRandom walk inference and learning in a large scale\nknowledge base. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing,\npages 529-539.\n\nVivian Law, Craig Knox, Yannick Djoumbou, Tim Jew-\nison, An Chi Guo, Yifeng Liu, Adam Maciejew-\nski, David Arndt, Michael Wilson, Vanessa Neveu,\nAlexandra Tang, Geraldine Gabriel, Carol Ly, Sakina\nAdamjee, Zerihun T. Dame, Beomsoo Han, You Zhou,\nand David S. Wishart. 2014. DrugBank 4.0: Shed-\nding new light on drug metabolism. Nucleic Acids Re-\nsearch, 42(D1):D1091-D1097.\n", "vlm_text": "Antonio Fabregat, Konstantinos Sidiropoulos, Phani Garapati, Marc Gillespie, Kerstin Hausmann, Robin Haw, Bijay Jassal, Steven Jupe, Florian Korninger, Sheldon McKay, Lisa Matthews, Bruce May, Mar- ija Milacic, Karen Rothfels, Veronica Shamovsky, Marissa Webber, Joel Weiser, Mark Williams, Guan- ming Wu, Lincoln Stein, Henning Hermjakob, and Peter D’Eustachio. 2016. The Reactome path- way knowledgebase. Nucleic Acids Research , 44(D1):D481–D487.Daniel Fried, Peter Jansen, Gustave Hahn-Powell, Mihai Surdeanu, and Peter Clark. 2015. Higher-order lexi- cal semantic models for non-factoid answer reranking. Transactions of the Association of Computational Lin- guistics , 3:197–210. Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom M. Mitchell. 2013. Improving learning and infer- ence in a large knowledge-base using latent syntactic cues. In  Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 833– 838. Alex Graves. 2016. Adaptive computation time for re- current neural networks.  CoRR , abs/1603.08983. Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. 2012. Development of a benchmark cor- pus to support the automatic extraction of drug-related adverse effects from medical case reports.  Journal of Biomedical Informatics , 45(5):885 – 892. Text Min- ing and Natural Language Processing in Pharmacoge- nomics. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In  Advances in Neural Information Processing Systems , pages 1693–1701. William Hersh, Aaron Cohen, Lynn Ruslen, and Phoebe Roberts. 2007. TREC 2007 genomics track overview. In  NIST Special Publication . Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, and David Berthelot. 2016. WIKIREADING: A novel large-scale language understanding task over Wikipedia. In  Proceedings of the The 54th Annual Meeting of the Association for Computational Linguis- tics (ACL 2016) , pages 1535–1545. Felix Hill, Antoine Bordes, Sumit Chopra, and Jason We- ston. 2016. The goldilocks principle: Reading chil- dren’s books with explicit memory representations. ICLR . Lynette Hirschman, Alexander Yeh, Christian Blaschke, and Alfonso Valencia. 2005. Overview of BioCre- AtIvE: Critical assessment of information extraction for biology.  BMC Bioinformatics , 6(1):S1, May. \nMinghao Hu, Yuxing Peng, and Xipeng Qiu. 2017. Mnemonic reader for machine comprehension.  CoRR , abs/1705.02798. Sarthak Jain. 2016. Question answering over knowledge base using factual memory networks. In  Proceedings of NAACL-HLT , pages 109–115. Peter Jansen, Rebecca Sharp, Mihai Surdeanu, and Peter Clark. 2017. Framing QA as building and ranking in- tersentence answer justiﬁcations.  Computational Lin- guistics , 43(2):407–449. Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In Empirical Methods in Natural Language Processing (EMNLP) . Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehen- sion. In  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , July. Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the at- tention sum reader network.  Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pages 908–918. Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011. Overview of Genia event task in BioNLP shared task 2011. In  Proceedings of BioNLP Shared Task 2011 Workshop , pages 7–15. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, Ishaan Gulrajani James Bradbury, Victor Zhong, Ro- main Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural lan- guage processing.  International Conference on Ma- chine Learning , 48:1378–1387. J. Richard Landis and Gary G. Koch. 1977. The mea- surement of observer agreement for categorical data. Biometrics , pages 159–174. Ni Lao and William W Cohen. 2010. Relational re- trieval using a combination of path-constrained ran- dom walks.  Machine learning , 81(1):53–67. Ni   Lao,   Tom   Mitchell,   and   William   W .  Cohen.   2011. Random walk inference and learning in a large scale knowledge base. In  Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 529–539. Vivian Law, Craig Knox, Yannick Djoumbou, Tim Jew- ison, An Chi Guo, Yifeng Liu, Adam Maciejew- ski, David Arndt, Michael Wilson, Vanessa Neveu, Alexandra Tang, Geraldine Gabriel, Carol Ly, Sakina Adamjee, Zerihun T. Dame, Beomsoo Han, You Zhou, and David S. Wishart. 2014. DrugBank 4.0: Shed- ding new light on drug metabolism.  Nucleic Acids Re- search , 42(D1):D1091–D1097. "}
{"page": 14, "image_path": "doc_images/Q18-1021_14.jpg", "ocr_text": "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettle-\nmoyer. 2017. Zero-shot relation extraction via read-\ning comprehension. In Proceedings of the 21st Con-\nference on Computational Natural Language Learning\n(CoNLL 2017), pages 333-342, August.\n\nDekang Lin and Patrick Pantel. 2001. Discovery of in-\nference rules for question-answering. Nat. Lang. Eng.,\n7(4):343-360, December.\n\nFei Liu and Julien Perez. 2017. Gated end-to-end mem-\nory networks. In Proceedings of the 15th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, EACL 2017, Volume 1: Long\nPapers, pages 1-10.\n\nMike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-\nsky. 2009. Distant supervision for relation extraction\nwithout labeled data. In Proceedings of the Joint Con-\nference of the 47th Annual Meeting of the ACL and\nthe 4th International Joint Conference on Natural Lan-\nguage Processing of the AFNLP, pages 1003-1011.\n\nAlvaro Morales, Varot Premtoon, Cordelia Avery, Sue\nFelshin, and Boris Katz. 2016. Learning to answer\nquestions from Wikipedia infoboxes. In Proceedings\nof the 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1930-1935.\n\nKarthik Narasimhan, Adam Yala, and Regina Barzilay.\n2016. Improving information extraction by acquiring\nexternal evidence with reinforcement learning. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\npages 2355-2365.\n\nArvind Neelakantan, Benjamin Roth, and Andrew Mc-\nCallum. 2015. Compositional vector space models for\nknowledge base completion. Proceedings of the 53rd\nAnnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference\non Natural Language Processing, pages 156-166.\n\nAnastasios Nentidis, Konstantinos Bougiatiotis, Anasta-\nsia Krithara, Georgios Paliouras, and Ioannis Kakadi-\naris. 2017. Results of the fifth edition of the BioASQ\nchallenge. In BioNLP 2017, pages 48-57.\n\nMark Neumann, Pontus Stenetorp, and Sebastian Riedel.\n2016. Learning to reason with adaptive computation.\nIn Interpretable Machine Learning for Complex Sys-\ntems at the 2016 Conference on Neural Information\nProcessing Systems (NIPS), Barcelona, Spain, Decem-\nber.\n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. CoRR,\nabs/1611.09268.\n\nRodrigo Nogueira and Kyunghyun Cho. 2016. WebNav:\nA new large-scale task for natural language based se-\nquential decision making. CoRR, abs/1602.02261.\n\n301\n\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\noriented query reformulation with reinforcement\nlearning. Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 574-583.\n\nTakeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-\npel, and David A. McAllester. 2016. Who did what:\nA large-scale person-centered cloze dataset. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\npages 2230-2235.\n\nDenis Paperno, German Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernandez. 2016. The LAMBADA dataset: Word pre-\ndiction requiring a broad discourse context. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1525-1534.\n\nMichael Pazzani, Clifford Brunk, and Glenn Silverstein.\n1991. A knowledge-intensive approach to learning re-\nlational concepts. In Proceedings of the Eighth Inter-\nnational Workshop on Machine Learning, pages 432-\n436, Evanston, IL.\n\nBaolin Peng, Zhengdong Lu, Hang Li, and Kam-Fai\nWong. 2015. Towards neural network-based reason-\ning. CoRR, abs/1508.05508.\n\nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina\nToutanova, and Wen-tau Yih. 2017. Cross-sentence\nN-ary relation extraction with graph LSTMs. Transac-\ntions of the Association for Computational Linguistics,\n5:101-115.\n\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. GloVe: Global vectors for word rep-\nresentation. In Proceedings of the Empirical Meth-\nods in Natural Language Processing (EMNLP), pages\n1532-1543.\n\nBethany Percha, Yael Garten, and Russ B Altman. 2012.\nDiscovery and explanation of drug-drug interactions\nvia text mining. In Pacific symposium on biocomput-\ning, page 410. NIH Public Access.\n\nJohn Ross Quinlan. 1990. Learning logical definitions\nfrom relations. Machine Learning, 5:239-266.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2383-2392.\n\nBradley L. Richards and Raymond J. Mooney. 1991.\nFirst-order theory revision. In Proceedings of the\n\nEighth International Workshop on Machine Learning,\npages 447-451, Evanston, IL.\n", "vlm_text": "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettle- moyer. 2017. Zero-shot relation extraction via read- ing comprehension. In  Proceedings of the 21st Con- ference on Computational Natural Language Learning (CoNLL 2017) , pages 333–342, August. Dekang Lin and Patrick Pantel. 2001. Discovery of in- ference rules for question-answering.  Nat. Lang. Eng. , 7(4):343–360, December. Fei Liu and Julien Perez. 2017. Gated end-to-end mem- ory networks. In  Proceedings of the 15th Conference of the European Chapter of the Association for Com- putational Linguistics, EACL 2017, Volume 1: Long Papers , pages 1–10. Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky. 2009. Distant supervision for relation extraction without labeled data. In  Proceedings of the Joint Con- ference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan- guage Processing of the AFNLP , pages 1003–1011. Alvaro Morales, Varot Premtoon, Cordelia Avery, Sue Felshin, and Boris Katz. 2016. Learning to answer questions from Wikipedia infoboxes. In  Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing , pages 1930–1935. Karthik Narasimhan, Adam Yala, and Regina Barzilay. 2016. Improving information extraction by acquiring external evidence with reinforcement learning. In  Pro- ceedings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP 2016 , pages 2355–2365. Arvind Neelakantan, Benjamin Roth, and Andrew Mc- Callum. 2015. Compositional vector space models for knowledge base completion.  Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing , pages 156–166. Anastasios Nentidis, Konstantinos Bougiatiotis, Anasta- sia Krithara, Georgios Paliouras, and Ioannis Kakadi- aris. 2017. Results of the ﬁfth edition of the BioASQ challenge. In  BioNLP 2017 , pages 48–57. Mark Neumann, Pontus Stenetorp, and Sebastian Riedel. 2016. Learning to reason with adaptive computation. In  Interpretable Machine Learning for Complex Sys- tems at the 2016 Conference on Neural Information Processing Systems (NIPS) , Barcelona, Spain, Decem- ber. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated ma- chine reading comprehension dataset. CoRR , abs/1611.09268. Rodrigo Nogueira and Kyunghyun Cho. 2016. WebNav: A new large-scale task for natural language based se- quential decision making.  CoRR , abs/1602.02261. \nRodrigo Nogueira and Kyunghyun Cho. 2017. Task- oriented query reformulation with reinforcement learning. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 574–583. Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim- pel, and David A. McAllester. 2016. Who did what: A large-scale person-centered cloze dataset. In  Pro- ceedings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP 2016 , pages 2230–2235. Denis Paperno, Germ´ an Kruszewski, Angeliki Lazari- dou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The LAMBADA dataset: Word pre- diction requiring a broad discourse context. In  Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers) , pages 1525–1534. Michael Pazzani, Clifford Brunk, and Glenn Silverstein. 1991. A knowledge-intensive approach to learning re- lational concepts. In  Proceedings of the Eighth Inter- national Workshop on Machine Learning , pages 432– 436, Evanston, IL. Baolin Peng, Zhengdong Lu, Hang Li, and Kam-Fai Wong. 2015. Towards neural network-based reason- ing.  CoRR , abs/1508.05508. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence N-ary relation extraction with graph LSTMs.  Transac- tions of the Association for Computational Linguistics , 5:101–115. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word rep- resentation. In  Proceedings of the Empirical Meth- ods in Natural Language Processing (EMNLP) , pages 1532–1543. Bethany Percha, Yael Garten, and Russ B Altman. 2012. Discovery and explanation of drug-drug interactions via text mining. In  Paciﬁc symposium on biocomput- ing , page 410. NIH Public Access. John Ross Quinlan. 1990. Learning logical deﬁnitions from relations.  Machine Learning , 5:239–266. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD:   $100{,}000{+}$   questions for machine comprehension of text. In  Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 2383–2392. Bradley L. Richards and Raymond J. Mooney. 1991. First-order theory revision. In  Proceedings of the Eighth International Workshop on Machine Learning , pages 447–451, Evanston, IL. "}
{"page": 15, "image_path": "doc_images/Q18-1021_15.jpg", "ocr_text": "Matthew Richardson and Pedro Domingos. 2006.\nMarkov logic networks. Mach. Learn., 62(1-2):107—\n136.\n\nSebastian Riedel, Limin Yao, and Andrew McCallum.\n2010. Modeling relations and their mentions with-\nout labeled text. In Proceedings of the 2010 European\nConference on Machine Learning and Knowledge Dis-\ncovery in Databases: Part II, ECML PKDD’10,\npages 148-163.\n\nStefan Schoenmackers, Oren Etzioni, and Daniel S.\nWeld. 2008. Scaling textual inference to the web.\nIn EMNLP ’08: Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing,\npages 79-88.\n\nStefan Schoenmackers, Oren Etzioni, Daniel S. Weld,\nand Jesse Davis. 2010. Learning first-order horn\nclauses from web text. In Proceedings of the 2010\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP ’10, pages 1088-1098.\n\nRoy Schwartz, Maarten Sap, Ioannis Konstas, Leila\nZilles, Yejin Choi, and Noah A. Smith. 2017. The\neffect of different writing tasks on linguistic style: A\ncase study of the ROC story cloze task. In Proceed-\nings of the 21st Conference on Computational Natural\nLanguage Learning (CoNLL 2017), pages 15-25.\n\nIsabel Segura-Bedmar, Paloma Martinez, and Maria Her-\nrero Zazo. 2013. SemEval-2013 Task 9: Extraction of\ndrug-drug interactions from biomedical texts (DDIEx-\ntraction 2013). In Second Joint Conference on Lexi-\ncal and Computational Semantics (*SEM), Volume 2:\nProceedings of the Seventh International Workshop on\nSemantic Evaluation (SemEval 2013), pages 341-350.\n\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017a. Bidirectional attention\nflow for machine comprehension. In The International\nConference on Learning Representations (ICLR).\n\nMinjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2017b. Query-reduction networks for\nquestion answering. JCLR.\n\nYelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu\nChen. 2017. ReasoNet: Learning to stop reading in\nmachine comprehension. In Proceedings of the 23rd\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, KDD ’17, pages\n1047-1055.\n\nAlessandro Sordoni, Phillip Bachman, and Yoshua Ben-\ngio. 2016. Iterative alternating neural attention for\nmachine reading. CoRR, abs/1606.02245.\n\nPontus Stenetorp, Goran Topi¢, Sampo Pyysalo, Tomoko\nOhta, Jin-Dong Kim, and Jun’ichi Tsujii. 2011.\nBioNLP shared task 2011: Supporting resources. In\nProceedings of BioNLP Shared Task 2011 Workshop,\npages 112-120.\n\n302\n\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks. In\nAdvances in Neural Information Processing Systems,\npages 2440-2448.\n\nDon R. Swanson. 1986. Undiscovered public knowl-\nedge. The Library Quarterly, 56(2):103-118.\n\nThe UniProt Consortium. 2017. UniProt: the univer-\nsal protein knowledgebase. Nucleic Acids Research,\n45(D1):D158-D169.\n\nDenny Vrande¢éié. 2012. Wikidata: A new platform\nfor collaborative data collection. In Proceedings of\nthe 21st International Conference on World Wide Web,\nWWW ’12 Companion, pages 1063-1064.\n\nDirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017.\nMaking neural QA as simple as possible but not sim-\npler. In Proceedings of the 21st Conference on Com-\nputational Natural Language Learning (CoNLL 2017),\npages 271-280. Association for Computational Lin-\nguistics.\n\nDirk Weissenborn. 2016. Separating answers from\nqueries for neural reading comprehension. CoRR,\nabs/1607.03316.\n\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions. In\nProceedings of the Third Workshop on Noisy User-\ngenerated Text, pages 94-106.\n\nJason Weston, Sumit Chopra, and Antoine Bordes. 2015.\nMemory networks. JCLR.\n\nJason Weston, Antoine Bordes, Sumit Chopra, and\nTomas Mikolov. 2016. Towards Al-complete ques-\ntion answering: A set of prerequisite toy tasks. JCLR.\n\nGeorg Wiese, Dirk Weissenborn, and Mariana Neves.\n2017. Neural question answering at BioASQ 5B. In\nProceedings of the BioNLP 2017, pages 76-79.\n\nCaiming Xiong, Victor Zhong, and Richard Socher.\n2017. Dynamic coattention networks for question an-\nswering. ICLR.\n\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWikiQA: A challenge dataset for open-domain ques-\ntion answering. In Proceedings of the 2015 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 2013-2018.\n", "vlm_text": "Matthew Richardson and Pedro Domingos. 2006. Markov logic networks.  Mach. Learn. , 62(1-2):107– 136. Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions with- out labeled text. In  Proceedings of the 2010 European Conference on Machine Learning and Knowledge Dis- covery in Databases: Part III , ECML PKDD’10, pages 148–163. Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld. 2008. Scaling textual inference to the web. In  EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 79–88. Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and Jesse Davis. 2010. Learning ﬁrst-order horn clauses from web text. In  Proceedings of the 2010 Conference on Empirical Methods in Natural Lan- guage Processing , EMNLP ’10, pages 1088–1098. Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, and Noah A. Smith. 2017. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In  Proceed- ings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017) , pages 15–25. Isabel Segura-Bedmar, Paloma Mart´ ınez, and Mar´ ıa Her- rero Zazo. 2013. SemEval-2013 Task 9: Extraction of drug-drug interactions from biomedical texts (DDIEx- traction 2013). In  Second Joint Conference on Lexi- cal and Computational Semantics   $^{\\prime}{}^{*}S E M)$  , Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013) , pages 341–350. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017a. Bidirectional attention ﬂow for machine comprehension. In  The International Conference on Learning Representations (ICLR) . Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. 2017b. Query-reduction networks for question answering.  ICLR . Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2017. ReasoNet: Learning to stop reading in machine comprehension. In  Proceedings of the 23rd ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining , KDD ’17, pages 1047–1055. Alessandro Sordoni, Phillip Bachman, and Yoshua Ben- gio. 2016. Iterative alternating neural attention for machine reading.  CoRR , abs/1606.02245. Pontus Stenetorp, Goran Topi´ c, Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and Jun’ichi Tsujii. 2011. BioNLP shared task 2011: Supporting resources. In Proceedings of BioNLP Shared Task 2011 Workshop , pages 112–120. \nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory networks. In Advances in Neural Information Processing Systems , pages 2440–2448. Don R. Swanson. 1986. Undiscovered public knowl- edge.  The Library Quarterly , 56(2):103–118. The UniProt Consortium. 2017. UniProt: the univer- sal protein knowledgebase.  Nucleic Acids Research , 45(D1):D158–D169. Denny Vrandeˇ ci´ c. 2012. Wikidata: A new platform for collaborative data collection. In  Proceedings of the 21st International Conference on World Wide Web , WWW ’12 Companion, pages 1063–1064. Dirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017. Making neural QA as simple as possible but not sim- pler. In  Proceedings of the 21st Conference on Com- putational Natural Language Learning (CoNLL 2017) , pages 271–280. Association for Computational Lin- guistics. Dirk Weissenborn. 2016. Separating answers from queries for neural reading comprehension. CoRR , abs/1607.03316. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the Third Workshop on Noisy User- generated Text , pages 94–106. Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory networks.  ICLR . Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2016. Towards AI-complete ques- tion answering: A set of prerequisite toy tasks.  ICLR . Georg Wiese, Dirk Weissenborn, and Mariana Neves. 2017. Neural question answering at BioASQ 5B. In Proceedings of the BioNLP 2017 , pages 76–79. Caiming Xiong, Victor Zhong, and Richard Socher. 2017. Dynamic coattention networks for question an- swering.  ICLR . Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A challenge dataset for open-domain ques- tion answering. In  Proceedings of the 2015 Confer- ence on Empirical Methods in Natural Language Pro- cessing , pages 2013–2018. "}
