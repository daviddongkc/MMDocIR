{"page": 0, "image_path": "doc_images/P19-1416_0.jpg", "ocr_text": "Compositional Questions Do Not Necessitate Multi-hop Reasoning\n\nSewon Min* ', Eric Wallace**, Sameer Singh’,\nMatt Gardner’, Hannaneh Hajishirzi!, Luke Zettlemoyer!\nUniversity of Washington\n?Allen Institute for Artificial Intelligence\n3University of California, Irvine\n\nsewon@cs.washington.edu,\n\nAbstract\n\nMulti-hop reading comprehension (RC) ques-\ntions are challenging because they require\nreading and reasoning over multiple para-\ngraphs. We argue that it can be difficult to con-\nstruct large multi-hop RC datasets. For exam-\nple, even highly compositional questions can\nbe answered with a single hop if they target\nspecific entity types, or the facts needed to\nanswer them are redundant. Our analysis is\ncentered on HOTPOTQA, where we show that\nsingle-hop reasoning can solve much more of\nthe dataset than previously thought. We intro-\nduce a single-hop BERT-based RC model that\nachieves 67 Fl—comparable to state-of-the-\nart multi-hop models. We also design an eval-\nuation setting where humans are not shown all\nof the necessary paragraphs for the intended\nmulti-hop reasoning but can still answer over\n80% of questions. Together with detailed error\nanalysis, these results suggest there should be\nan increasing focus on the role of evidence in\nmulti-hop reasoning and possibly even a shift\ntowards information retrieval style evaluations\nwith large and diverse evidence collections.\n\n1 Introduction\n\nMulti-hop reading comprehension (RC) requires\nreading and aggregating information over multi-\nple pieces of textual evidence (Welbl et al., 2017;\nYang et al., 2018; Talmor and Berant, 2018). In\nthis work, we argue that it can be difficult to con-\nstruct large multi-hop RC datasets. This is because\nmulti-hop reasoning is a characteristic of both the\nquestion and the provided evidence; even highly\ncompositional questions can be answered with a\nsingle hop if they target specific entity types, or the\nfacts needed to answer them are redundant. For\nexample, the question in Figure | is compositional:\na plausible solution is to find “What animal’s habi-\ntat was the Réserve Naturelle Lomako Yokokala\n\n*Equal Contribution.\n\nericw@allenai.org\n\nQuestion: What is the former name of the animal whose\nhabitat the Réserve Naturelle Lomako Yokokala was es-\ntablished to protect?\n\nParagraph 5: The Lomako Forest Reserve is found in\nDemocratic Republic of the Congo. It was established in\n1991 especially to protect the habitat of the Bonobo apes.\nParagraph 1: The bonobo (“Pan paniscus”), formerly\ncalled the pygmy chimpanzee and less often, the dwarf\nor gracile chimpanzee, is an endangered great ape and one\nof the two species making up the genus “Pan”.\n\nFigure 1: A HOTPOTQA example designed to require\nreasoning across two paragraphs. Eight spurious addi-\ntional paragraphs (not shown) are provided to increase\nthe task difficulty. However, since only one of the ten\nparagraphs is about an animal, one can immediately lo-\ncate the answer in Paragraph I using one hop. The full\nexample is provided in Appendix A.\n\nestablished to protect?”, and then answer “What is\nthe former name of that animal?’”’. However, when\nconsidering the evidence paragraphs, the question\nis solvable in a single hop by finding the only para-\ngraph that describes an animal.\n\nOur analysis is centered on HOTPOTQA (Yang\net al., 2018), a dataset of mostly compositional\nquestions. In its RC setting, each question is paired\nwith two gold paragraphs, which should be needed\nto answer the question, and eight distractor para-\ngraphs, which provide irrelevant evidence or incor-\nrect answers. We show that single-hop reasoning\ncan solve much more of this dataset than previously\nthought. First, we design a single-hop QA model\nbased on BERT (Devlin et al., 2018), which, de-\nspite having no ability to reason across paragraphs,\nachieves performance competitive with the state of\nthe art. Next, we present an evaluation demonstrat-\ning that humans can solve over 80% of questions\nwhen we withhold one of the gold paragraphs.\n\nTo better understand these results, we present\na detailed analysis of why single-hop reasoning\nworks so well. We show that questions include\nredundant facts which can be ignored when com-\n\n4249\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4249-4257\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Compositional Questions Do Not Necessitate Multi-hop Reasoning \nSewon Min ∗ 1 , Eric Wallace ∗ 2 , Sameer Singh 3 , Matt Gardner 2 , Hannaneh Hajishirzi 1 , , Luke Zettlemoyer 1 \n1 University of Washington 2 Allen Institute for Artiﬁcial Intelligence 3 University of California, Irvine sewon@cs.washington.edu, ericw@allenai.org \nAbstract \nMulti-hop reading comprehension (RC) ques- tions are challenging because they require reading and reasoning over multiple para- graphs. We argue that it can be difﬁcult to con- struct large multi-hop RC datasets. For exam- ple, even highly compositional questions can be answered with a single hop if they target speciﬁc entity types, or the facts needed to answer them are redundant. Our analysis is centered on H OTPOT QA, where we show that single-hop reasoning can solve much more of the dataset than previously thought. We intro- duce a single-hop BERT-based RC model that achieves 67 F1—comparable to state-of-the- art multi-hop models. We also design an eval- uation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over  $80\\%$   of questions. Together with detailed error analysis, these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections. \n1 Introduction \nMulti-hop reading comprehension (RC) requires reading and aggregating information over multi- ple pieces of textual evidence ( Welbl et al. ,  2017 ; Yang et al. ,  2018 ;  Talmor and Berant ,  2018 ). In this work, we argue that it can be difﬁcult to con- struct large multi-hop RC datasets. This is because multi-hop reasoning is a characteristic of both the question and the provided evidence; even highly compositional questions can be answered with a single hop if they target speciﬁc entity types, or the facts needed to answer them are redundant. For example, the question in Figure  1  is compositional: a plausible solution is to ﬁnd “What animal’s habi- tat was the R eserve Naturelle Lomako Yokokala \nQuestion:  What is the former name of the animal whose habitat the R eserve Naturelle Lomako Yokokala was es- tablished to protect? \nParagraph 5:  The Lomako Forest Reserve is found in Democratic Republic of the Congo. It was established in 1991 especially to protect the habitat of the Bonobo apes. Paragraph 1:  The bonobo (“Pan paniscus”), formerly called the  pygmy chimpanzee  and less often, the dwarf or gracile chimpanzee, is an endangered great ape and one of the two species making up the genus “Pan”. \nestablished to protect?”, and then answer “What is the former name of that animal?”. However, when considering the evidence paragraphs, the question is solvable in a single hop by ﬁnding the only para- graph that describes an animal. \nOur analysis is centered on H OTPOT QA ( Yang et al. ,  2018 ), a dataset of mostly compositional questions. In its RC setting, each question is paired with two gold paragraphs, which should be needed to answer the question, and eight distractor para- graphs, which provide irrelevant evidence or incor- rect answers. We show that single-hop reasoning can solve much more of this dataset than previously thought. First, we design a single-hop QA model based on BERT ( Devlin et al. ,  2018 ), which, de- spite having no ability to reason across paragraphs, achieves performance competitive with the state of the art. Next, we present an evaluation demonstrat- ing that humans can solve over   $80\\%$   of questions when we withhold one of the gold paragraphs. \nTo better understand these results, we present a detailed analysis of why single-hop reasoning works so well. We show that questions include redundant facts which can be ignored when com- puting the answer, and that the ﬁne-grained entity types present in the provided paragraphs in the RC setting often provide a strong signal for answer- ing the question, e.g., there is only one animal in the given paragraphs in Figure  1 , allowing one to immediately locate the answer using one hop. "}
{"page": 1, "image_path": "doc_images/P19-1416_1.jpg", "ocr_text": "puting the answer, and that the fine-grained entity\ntypes present in the provided paragraphs in the RC\nsetting often provide a strong signal for answer-\ning the question, e.g., there is only one animal in\nthe given paragraphs in Figure 1, allowing one to\nimmediately locate the answer using one hop.\n\nThis analysis shows that more carefully cho-\nsen distractor paragraphs would induce questions\nthat require multi-hop reasoning. We thus ex-\nplore an alternative method for collecting distrac-\ntors based on adversarial paragraph selection. Al-\nthough this appears to mitigate the problem, a\nsingle-hop model re-trained on these distractors\ncan recover most of the original single-hop accu-\nracy, indicating that these distractors are still insuf-\nficient. Another method is to consider very large\ndistractor sets such as all of Wikipedia or the en-\ntire Web, as done in open-domain HOTPOTQA and\nComplex WebQuestions (Talmor and Berant, 2018).\nHowever, this introduces additional computational\nchallenges and/or the need for retrieval systems.\nFinding a small set of distractors that induce multi-\nhop reasoning remains an open challenge that is\nworthy of follow up work.\n\n2 Related Work\n\nLarge-scale RC datasets (Hermann et al., 2015;\nRajpurkar et al., 2016; Joshi et al., 2017) have en-\nabled rapid advances in neural QA models (Seo\net al., 2017; Xiong et al., 2018; Yu et al., 2018; De-\nvlin et al., 2018). To foster research on reasoning\nacross multiple pieces of text, multi-hop QA has\nbeen introduced (Koéisky et al., 2018; Talmor and\nBerant, 2018; Yang et al., 2018). These datasets\ncontain compositional or “complex” questions. We\ndemonstrate that these questions do not necessitate\nmulti-hop reasoning.\n\nExisting multi-hop QA datasets are constructed\nusing knowledge bases, e.g., WIKIHOP (Welbl\net al., 2017) and COMPLEX WEBQUESTIONS (Tal-\nmor and Berant, 2018), or using crowd workers,\ne.g., HOTPOTQA (Yang et al., 2018). WIKI-\nHop questions are posed as triples of a relation\nand a head entity, and the task is to determine\nthe tail entity of the relationship. COMPLEXWE-\nBQUESTIONS consists of open-domain composi-\ntional questions, which are constructed by increas-\ning the complexity of SPARQL queries from WE-\nBQUESTIONS (Berant et al., 2013). We focus on\nHOTPOTQA, which consists of multi-hop ques-\ntions written to require reasoning over two para-\n\nQuestion — Paragraph 1 BERT Yempr span/yes/no\n\nQuestion — Paragraph 2 BERT Yempty_| span/yes/no\ni 5 lowest\n\nQuestion — Paragraph N BERT Yempty span/yes/no\n\nFigure 2: Our model, single-paragraph BERT, reads\nand scores each paragraph independently. The answer\nfrom the paragraph with the lowest yempty Score is cho-\nsen as the final answer.\n\ngraphs from Wikipedia.\n\nParallel research from Chen and Durrett (2019)\npresents similar findings on HOTPOTQA. Our work\ndiffers because we conduct human analysis to un-\nderstand why questions are solvable using single-\nhop reasoning. Moreover, we show that selecting\ndistractor paragraphs is difficult using current re-\ntrieval methods.\n\n3 Single-paragraph QA\n\nThis section shows the performance of a single-hop\nmodel on HOTPOTQA.\n\n3.1 Model Description\n\nOur model, single-paragraph BERT, scores and an-\nswers each paragraph independently (Figure 2). We\nthen select the answer from the paragraph with the\nbest score, similar to Clark and Gardner (2018).!\n\nThe model receives a question Q = [q1, .., dm]\nand a single paragraph P = [p,...,pn] as in-\nput. Following Devlin et al. (2018), S =\n[G1 ---; Gm; [SEP], 71, ---; Pn], where [SEP] is a spe-\ncial token, is fed into BERT:\n\nv= BERT(S) € Rex(mtnth |\n\nwhere h is the hidden dimension of BERT. Next, a\nclassifier uses max-pooling and learned parameters\nW, € R\"** to generate four scalars:\n\n[Yspan; Yyes; Yno; Yempty] = W,maxpool($’),\n\nwhere Yspan, Yyes; Yno and Yempty indicate the an-\nswer is either a span, yes, no, or no answer. An\nextractive paragraph span, span, is obtained sep-\narately following Devlin et al. (2018). The final\nmodel outputs are a scalar value Yempty and a text\nof either span, yes or no, based on which of\nYspan; Yyes Yno has the largest value.\n\n‘Bull details in Appendix B. Code available at https:\n//github.com/shmsw25/single-hop-rc.\n\n4250\n", "vlm_text": "\nThis analysis shows that more carefully cho- sen distractor paragraphs would induce questions that require multi-hop reasoning. We thus ex- plore an alternative method for collecting distrac- tors based on adversarial paragraph selection. Al- though this appears to mitigate the problem, a single-hop model re-trained on these distractors can recover most of the original single-hop accu- racy, indicating that these distractors are still insuf- ﬁcient. Another method is to consider very large distractor sets such as all of Wikipedia or the en- tire Web, as done in open-domain H OTPOT QA and Complex Web Questions ( Talmor and Berant ,  2018 ). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multi- hop reasoning remains an open challenge that is worthy of follow up work. \n2 Related Work \nLarge-scale RC datasets ( Hermann et al. ,  2015 ; Rajpurkar et al. ,  2016 ;  Joshi et al. ,  2017 ) have en- abled rapid advances in neural QA models ( Seo et al. ,  2017 ;  Xiong et al. ,  2018 ;  Yu et al. ,  2018 ;  De- vlin et al. ,  2018 ). To foster research on reasoning across  multiple  pieces of text, multi-hop QA has been introduced ( Ko cisk y et al. ,  2018 ;  Talmor and Berant ,  2018 ;  Yang et al. ,  2018 ). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. \nExisting multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP  ( Welbl et al. ,  2017 ) and C OMPLEX W EB Q UESTIONS  ( Tal- mor and Berant ,  2018 ), or using crowd workers, e.g., H OTPOT QA ( Yang et al. ,  2018 ). W IKI - H OP  questions are posed as triples of a relation and a head entity, and the task is to determine the tail entity of the relationship. C OMPLEX W E - B Q UESTIONS  consists of open-domain composi- tional questions, which are constructed by increas- ing the complexity of SPARQL queries from W E - B Q UESTIONS  ( Berant et al. ,  2013 ). We focus on H OTPOT QA, which consists of multi-hop ques- tions written to require reasoning over two para- \nThis image is not a table, but rather a diagram illustrating a process involving the BERT (Bidirectional Encoder Representations from Transformers) model. Here's a breakdown of the diagram:\n\n1. **Inputs:**\n   - It starts with a \"Question\" and a \"Paragraph,\" indicating that the system takes a question based on some text (paragraph) as input.\n\n2. **Processing:**\n   - Each question-paragraph pair is processed by the BERT model. This is shown by the lines connecting \"Question\" and \"Paragraph\" to the BERT section in the diagram.\n\n3. **Outputs:**\n   - After processing by BERT, there are two outputs:\n     - \\(y_{\\text{empty}}\\): This likely indicates whether the paragraph contains an answer to the question.\n     - \"span/yes/no\": This suggests classification outputs where the model might predict:\n       - \"span\" (the specific portion of the paragraph that answers the question),\n       - \"yes\" or \"no\" (indicating that the question can be answered affirmatively or negatively based on the given paragraph).\n\n4. **Selection:**\n   - The process seems iterative or happening in parallel across multiple paragraphs (Paragraph 1 to Paragraph N).\n   - Among the predictions for different paragraphs, the system identifies the \"lowest\" as highlighted in the diagram, which might suggest selecting the most confident or relevant output among the question-paragraph pairs.\n\nThis type of setup is common in machine learning for question-answering systems where the goal is to find the most relevant answer to a question from multiple passages of text.\ngraphs from Wikipedia. \nParallel research from  Chen and Durrett  ( 2019 ) presents similar ﬁndings on H OTPOT QA. Our work differs because we conduct human analysis to un- derstand why questions are solvable using single- hop reasoning. Moreover, we show that selecting distractor paragraphs is difﬁcult using current re- trieval methods. \n3 Single-paragraph QA \nThis section shows the performance of a single-hop model on H OTPOT QA. \n3.1 Model Description \nOur model, single-paragraph BERT, scores and an- swers each paragraph independently (Figure  2 ). We then select the answer from the paragraph with the best score, similar to  Clark and Gardner  ( 2018 ). \nThe model receives a question    $Q=[q_{1},..,q_{m}]$  and a single paragraph    $P~=~[p_{1},...,p_{n}]$   as in- put. Following  Devlin et al.  ( 2018 ),    $\\begin{array}{r l}{S}&{{}=}\\end{array}$   $[q_{1},...,q_{m},[\\mathrm{SEP}],p_{1},...,p_{n}]$   [ SEP ]  is a spe- cial token, is fed into BERT: \n\n$$\nS^{\\prime}=\\operatorname{BRT}(S)\\in\\mathbb{R}^{h\\times(m+n+1)},\n$$\n \nwhere    $h$   is the hidden dimension of BERT. Next, a classiﬁer uses max-pooling and learned parameters  $W_{1}\\in\\mathbb{R}^{h\\times4}$    to generate four scalars: \n\n$$\n[y_{\\mathrm{span}};y_{\\mathrm{geqslant}};y_{\\mathrm{no}};y_{\\mathrm{empty}}]=W_{1}\\mathrm{maxpool}(S^{\\prime}),\n$$\n \nwhere    $y_{\\mathrm{span}},y_{\\mathrm{yes}},y_{\\mathrm{no}}$   and    $y_{\\mathrm{emptysety}}$   indicate the an- swer is either a span,    $\\mathtt{Y}\\!\\in\\!S$  ,  no , or no answer. An extractive paragraph span,  span , is obtained sep- arately following  Devlin et al.  ( 2018 ). The ﬁnal model outputs are a scalar value    $y_{\\mathrm{empty}}$   and a text of either  span ,    $\\mathtt{Y}\\!\\in\\!S$   or  no , based on which of  $y_{\\mathrm{span}},y_{\\mathrm{yes}},y_{\\mathrm{no}}$  "}
{"page": 2, "image_path": "doc_images/P19-1416_2.jpg", "ocr_text": "Model Distractor F1 Open F1\nSingle-paragraph BERT* 67.08 38.40\nBiDAF* 58.28 34.36\nBiDAF 58.99 32.89\nGRN 66.71 36.48\nQFE 68.06 38.06\nDFGN + BERT 68.49 -\nMultiQA - 40.23\nDecompRC 69.63 40.65\nBERT Plus 69.76 -\nCognitive Graph - 48.87\n\nTable 1: Fl scores on HOTPOTQA. * indicates the re-\nsult is on the validation set; the other results are on the\nhidden test set shown in the official leaderboard.\n\nFor a particular HOTPOTQA example, we run\nsingle-paragraph BERT on each paragraph in par-\nallel and select the answer from the paragraph with\nthe smallest vempty-\n\n3.2 Model Results\n\nHOTPOTQA has two settings: a distractor setting\nand an open-domain setting.\n\nDistractor Setting The HOTPOTQA distractor\nsetting pairs the two paragraphs the question was\nwritten for (gold paragraphs) with eight spurious\nparagraphs selected using TF-IDF similarity with\nthe question (distractors). Our single-paragraph\nBERT model achieves 67.08 F1, comparable to\nthe state-of-the-art (Table 1).2 This indicates the\nmajority of HOTPOTQA questions are answerable\nin the distractor setting using a single-hop model.\n\nOpen-domain Setting The HOTPOTQA open-\ndomain setting (Fullwiki) does not provide a set of\nparagraphs—all of Wikipedia is considered. We\nfollow Chen et al. (2017) and retrieve paragraphs\nusing bigram TF-IDF similarity with the question.\n\nWe use the single-paragraph BERT model\ntrained in the distractor setting. We also fine-tune\nthe model using incorrect paragraphs selected by\nthe retrieval system. In particular, we retrieve 30\nparagraphs and select the eight paragraphs with\nthe lowest Yempty Scores predicted by the trained\nmodel. Single-paragraph BERT achieves 38.06 F1\nin the open-domain setting (Table 1). This shows\nthat the open-domain setting is challenging for our\nsingle-hop model and is worthy of future study.\n\nResults as of March 4th, 2019.\n\n4 Compositional Questions Are Not\nAlways Multi-hop\n\nThis section provides a human analysis of HOT-\nPOTQA to understand what phenomena enable\nsingle-hop answer solutions. HOTPOTQA contains\ntwo question types, Bridge and Comparison, which\nwe evaluate separately.\n\n4.1 Categorizing Bridge Questions\n\nBridge questions consist of two paragraphs linked\nby an entity (Yang et al., 2018), e.g., Figure 1. We\nfirst investigate single-hop human performance on\nHOTPOTQA bridge questions using a human study\nconsisting of NLP graduate students. Humans see\nthe paragraph that contains the answer span and the\neight distractor paragraphs, but do not see the other\ngold paragraph. As a baseline, we show a different\nset of people the same questions in their standard\nten paragraph form.\n\nOn a sample of 200 bridge questions from the\nvalidation set, human accuracy shows marginal\ndegradation when using only one hop: humans\nobtain 87.37 F1 using all ten paragraphs and 82.06\nF1 when using only nine (where they only see a\nsingle gold paragraph). This indicates humans, just\nlike models, are capable of solving bridge questions\nusing only one hop.\n\nNext, we manually categorize what enables\nsingle-hop answers for 100 bridge validation exam-\nples (taking into account the distractor paragraphs),\nand place questions into four categories (Table 2).\n\nMulti-hop 27% of questions require multi-hop\nreasoning. The first example of Table 2 requires lo-\ncating the university where “Ralph Hefferline” was\na psychology professor, and multiple universities\nare provided as distractors. Therefore, the answer\ncannot be determined in one hop.?\n\nWeak Distractors 35% of questions allow\nsingle-hop answers in the distractor setting, mostly\nby entity type matching. Consider the question in\nthe second row of Table 2: in the ten provided para-\ngraphs, only one actress has a government position.\nThus, the question is answerable without consider-\ning the film “Kiss and Tell.” These examples may\nbecome multi-hop in the open-domain setting, e.g.,\nthere are numerous actresses with a government\nposition on Wikipedia.\n\n3It is possible that a single-hop model can do well by\n\nrandomly guessing between two or three well-typed options,\nbut we do not evaluate that strategy here.\n\n4251\n", "vlm_text": "The table presents F1 scores for different models under two evaluation settings: \"Distractor\" and \"Open.\" The F1 score is a measure of a model's accuracy considering both precision and recall. The models listed include:\n\n1. Single-paragraph BERT*\n   - Distractor F1: 67.08\n   - Open F1: 38.40\n\n2. BiDAF*\n   - Distractor F1: 58.28\n   - Open F1: 34.36\n\n3. BiDAF\n   - Distractor F1: 58.99\n   - Open F1: 32.89\n\n4. GRN\n   - Distractor F1: 66.71\n   - Open F1: 36.48\n\n5. QFE\n   - Distractor F1: 68.06\n   - Open F1: 38.06\n\n6. DFGN + BERT\n   - Distractor F1: 68.49\n   - Open F1: Not provided\n\n7. MultiQA\n   - Distractor F1: Not provided\n   - Open F1: 40.23\n\n8. DecompRC\n   - Distractor F1: 69.63\n   - Open F1: 40.65\n\n9. BERT Plus\n   - Distractor F1: 69.76\n   - Open F1: Not provided\n\n10. Cognitive Graph\n    - Distractor F1: Not provided\n    - Open F1: 48.87\n\nModels marked with an asterisk (*) likely have some noteworthy or non-standard elements as indicated in the accompanying documentation (not visible here). The table shows variations in performance across models, with some achieving higher scores in the \"Open\" setting than others.\nTable 1: F1 scores on H OTPOT QA. \\* indicates the re- sult is on the validation set; the other results are on the hidden test set shown in the ofﬁcial leaderboard. \nFor a particular H OTPOT QA example, we run single-paragraph BERT on each paragraph in par- allel and select the answer from the paragraph with the smallest  $y_{\\mathrm{emptysety}}$  . \n3.2 Model Results \nH OTPOT QA has two settings: a distractor setting and an open-domain setting. \nDistractor Setting The H OTPOT QA distractor setting pairs the two paragraphs the question was written for ( gold paragraphs ) with eight spurious paragraphs selected using TF-IDF similarity with the question ( distractors ). Our single-paragraph BERT model achieves 67.08 F1, comparable to the state-of-the-art (Table  1 ).   This indicates the majority of H OTPOT QA questions are answerable in the distractor setting using a single-hop model. \nOpen-domain Setting The H OTPOT QA open- domain setting ( Fullwiki ) does not provide a set of paragraphs—all of Wikipedia is considered. We follow  Chen et al.  ( 2017 ) and retrieve paragraphs using bigram TF-IDF similarity with the question. \nWe use the single-paragraph BERT model trained in the distractor setting. We also ﬁne-tune the model using incorrect paragraphs selected by the retrieval system. In particular, we retrieve 30 paragraphs and select the eight paragraphs with the lowest  y empty  scores predicted by the trained model. Single-paragraph BERT achieves 38.06 F1 in the open-domain setting (Table  1 ). This shows that the open-domain setting is challenging for our single-hop model and is worthy of future study. \n4 Compositional Questions Are Not Always Multi-hop \nThis section provides a human analysis of H OT - POT QA to understand what phenomena enable single-hop answer solutions. H OTPOT QA contains two question types,  Bridge  and  Comparison , which we evaluate separately. \n4.1 Categorizing Bridge Questions \nBridge questions consist of two paragraphs linked by an entity ( Yang et al. ,  2018 ), e.g., Figure  1 . We ﬁrst investigate single-hop human performance on H OTPOT QA bridge questions using a human study consisting of NLP graduate students. Humans see the paragraph that contains the answer span and the eight distractor paragraphs, but do not see the other gold paragraph. As a baseline, we show a different set of people the same questions in their standard ten paragraph form. \nOn a sample of 200 bridge questions from the validation set, human accuracy shows marginal degradation when using only one hop: humans obtain  87.37 F1  using all ten paragraphs and  82.06 F1  when using only nine (where they only see a single gold paragraph). This indicates humans, just like models, are capable of solving bridge questions using only one hop. \nNext, we manually categorize what enables single-hop answers for 100 bridge validation exam- ples (taking into account the distractor paragraphs), and place questions into four categories (Table  2 ). \nMulti-hop  $27\\%$   of questions require multi-hop reasoning. The ﬁrst example of Table  2  requires lo- cating the university where “Ralph Hefferline” was a psychology professor, and multiple universities are provided as distractors. Therefore, the answer cannot be determined in one hop. \nWeak Distractors  $35\\%$   of questions allow single-hop answers in the distractor setting, mostly by entity type matching. Consider the question in the second row of Table  2 : in the ten provided para- graphs, only one actress has a government position. Thus, the question is answerable without consider- ing the ﬁlm “Kiss and Tell.” These examples may become multi-hop in the open-domain setting, e.g., there are numerous actresses with a government position on Wikipedia. "}
{"page": 3, "image_path": "doc_images/P19-1416_3.jpg", "ocr_text": "Type Question %\nMulti-hop Ralph Hefferline was a psychology professor at a university that is located in what city? 27\nWeak distractors What government position was held by the woman who portrayed Corliss Archer in 35\nthe film Kiss and Tell?\nRedundant evidence Kaiser Ventures corporation was founded by an American industrialist who became 26\nknown as the father of modern American shipbuilding?\nNon-compositional 1-hop When was Poison’s album ‘Shut Up, Make Love’ released? 8\n\nTable 2: We categorize bridge questions while taking the paragraphs into account. We exclude 4% of questions\nthat we found to have incorrect or ambiguous answer annotations. See Section 4.1 for details on question types.\n\nType Question % *FI\nMulti-hop Who was born first, Arthur Conan Doyle or Penelope Lively? 45 54.46\nContext-dependent Are Hot Rod and the Memory of Our People both magazines? 36 56.16\nSingle-hop Which writer was from England, Henry Roth or Robert Erskine Childers? 17 70.54\n\nTable 3: We automatically categorize comparison questions using rules (2% cannot be automatically categorized).\nSingle-paragraph BERT achieves near chance accuracy on multi-hop questions but exploits single-hop ones.\n\nTo further investigate entity type matching, we\nreduce the question to the first five tokens start-\ning from the wh-word, following Sugawara et al.\n(2018). Although most of these reduced questions\nappear void of critical information, the Fl score\nof single-paragraph BERT only degrades about 15\nFI from 67.08 to 52.13.\n\nRedundant Evidence 26% of questions are\ncompositional but are solvable using only part of\nthe question. For instance, in the third example of\nTable 2 there is only a single founder of “Kaiser\nVentures.” Thus, one can ignore the condition\non “American industrialist” and “father of modern\nAmerican shipbuilding.” This category differs from\nthe weak distractors category because its questions\nare single-hop regardless of the distractors.\n\nNon-compositional Single-hop 8% of ques-\ntions are non-compositional and single-hop. In\nthe last example of Table 2, one sentence contains\nall of the information needed to answer correctly.\n\n4.2 Categorizing Comparison Questions\n\nComparison questions require quantitative or logi-\ncal comparisons between two quantities or events.\nWe create rules (Appendix C) to group comparison\nquestions into three categories: questions which\nrequire multi-hop reasoning (multi-hop), may re-\nquire multi-hop reasoning (context-dependent), and\nrequire single-hop reasoning (single-hop).\n\nMany comparison questions are multi-hop or\ncontext-dependent multi-hop, and single-paragraph\n\nEvaluation Data Training Data\n\nOriginal Adversarial\nOriginal 67.08 59.12\nAdversarial 46.84 60.10\n+ Type 40.73 58.42\n\nTable 4: We train on HOTPOTQA using standard dis-\ntractors (Original) or using adversarial distractors (Ad-\nversarial). The model is then tested on the original dis-\ntractors, adversarial distractors, or adversarial distrac-\ntors with filtering by entity type (+ Type).\n\nBERT achieves near chance accuracy on these\ntypes of questions (Table 3).4 This shows that\nmost comparison questions are not solvable by our\nsingle-hop model.\n\n5 Can We Find Better Distractors?\n\nIn Section 4.1, we identify that 35% of bridge ex-\namples are solvable using single-hop reasoning due\nto weak distractor paragraphs. Here, we attempt to\nautomatically correct these examples by choosing\nnew distractor paragraphs which are likely to trick\nour single-paragraph model.\n\nAdversarial Distractors We select the top-50\nfirst paragraphs of Wikipedia pages using TF-IDF\nsimilarity with the question, following the original\nHOTPOTQA setup. Next, we use single-paragraph\nBERT to adversarially select the eight distractor\nparagraphs from these 50 candidates. In particular,\nwe feed each paragraph to the model and select\n\n4Comparison questions test mainly binary relationships.\n\n4252\n", "vlm_text": "The table consists of three columns with the headings \"Type,\" \"Question,\" and \"%.\" Here is the information outlined:\n\n1. **Type**: This column categorizes the type of question being asked.\n   - Multi-hop\n   - Weak distractors\n   - Redundant evidence\n   - Non-compositional 1-hop\n\n2. **Question**: This column lists specific questions linked to each type.\n   - \"Ralph Hefferline was a psychology professor at a university that is located in what city?\"\n   - \"What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\"\n   - \"Kaiser Ventures corporation was founded by an American industrialist who became known as the father of modern American shipbuilding?\"\n   - \"When was Poison’s album ‘Shut Up, Make Love’ released?\"\n\n3. **%**: This column shows the percentage associated with each question type.\n   - Multi-hop: 27%\n   - Weak distractors: 35%\n   - Redundant evidence: 26%\n   - Non-compositional 1-hop: 8% \n\nThis table seems to categorize different types of questions and provides a percentage for each type, although the context or source of the percentage is not provided.\nThe table contains four columns with the following headers: \"Type,\" \"Question,\" \"%,\" and \"F1.\" \n\n1. **Type**\n   - Multi-hop\n   - Context-dependent\n   - Single-hop\n\n2. **Question**\n   - Who was born first, Arthur Conan Doyle or Penelope Lively?\n   - Are Hot Rod and the Memory of Our People both magazines?\n   - Which writer was from England, Henry Roth or Robert Erskine Childers?\n\n3. **%**\n   - 45\n   - 36\n   - 17\n\n4. **F1**\n   - 54.46\n   - 56.16\n   - 70.54\nTo further investigate entity type matching, we reduce the question to the ﬁrst ﬁve tokens start- ing from the wh-word, following  Sugawara et al. ( 2018 ). Although most of these reduced questions appear void of critical information, the F1 score of single-paragraph BERT only degrades about 15 F1 from 67.08 to 52.13. \nRedundant Evidence  $26\\%$   of questions are compositional but are solvable using only part of the question. For instance, in the third example of Table  2  there is only a single founder of “Kaiser Ventures.” Thus, one can ignore the condition on “American industrialist” and “father of modern American shipbuilding.” This category differs from the weak distractors category because its questions are single-hop regardless of the distractors. \nNon-compositional Single-hop  $8\\%$   of ques- tions are non-compositional and single-hop. In the last example of Table  2 , one sentence contains all of the information needed to answer correctly. \n4.2 Categorizing Comparison Questions \nComparison questions require quantitative or logi- cal comparisons between two quantities or events. We create rules (Appendix  C ) to group comparison questions into three categories: questions which require multi-hop reasoning ( multi-hop ), may re- quire multi-hop reasoning ( context-dependent ), and require single-hop reasoning ( single-hop ). \nMany comparison questions are multi-hop or context-dependent multi-hop, and single-paragraph \nThe table shows a comparison of performance metrics (scores) for different combinations of evaluation and training data. There are two types of training data: \"Original\" and \"Adversarial.\" Evaluation is performed on three types of data: \"Original,\" \"Adversarial,\" and \"Adversarial + Type.\"\n\n- When the evaluation data is \"Original,\" the score is 67.08 for \"Original\" training data and 59.12 for \"Adversarial\" training data.\n- For \"Adversarial\" evaluation data, the score is 46.84 with \"Original\" training data and 60.10 with \"Adversarial\" training data.\n- For the \"+ Type\" evaluation data, which could imply an additional layer of evaluation, the score is 40.73 with \"Original\" training data and 58.42 with \"Adversarial\" training data.\n\nThis table likely represents the accuracy or performance of a model trained on different data types and how well it generalizes to various types of evaluation data, highlighting the impact of adversarial training.\nTable 4: We train on H OTPOT QA using standard dis- tractors ( Original ) or using adversarial distractors ( Ad- versarial ). The model is then tested on the original dis- tractors, adversarial distractors, or adversarial distrac- tors with ﬁltering by entity type   $(+\\it{T y p e})$  . \nBERT achieves near chance accuracy on these types of questions (Table  3 ). This shows that most comparison questions are not solvable by our single-hop model. \n5 Can We Find Better Distractors? \nIn Section  4.1 , we identify that   $35\\%$   of bridge ex- amples are solvable using single-hop reasoning due to weak distractor paragraphs. Here, we attempt to automatically correct these examples by choosing new distractor paragraphs which are likely to trick our single-paragraph model. \nAdversarial Distractors We select the top-50 ﬁrst paragraphs of Wikipedia pages using TF-IDF similarity with the question, following the original H OTPOT QA setup. Next, we use single-paragraph BERT to adversarially select the eight distractor paragraphs from these 50 candidates. In particular, we feed each paragraph to the model and select the paragraphs with the lowest  y empty  score (i.e., the paragraphs that the model thinks contain the answer). These paragraphs are dissimilar to the original distractors—there is a   $9.82\\%$   overlap. "}
{"page": 4, "image_path": "doc_images/P19-1416_4.jpg", "ocr_text": "the paragraphs with the lowest yempty score (i.e.,\nthe paragraphs that the model thinks contain the\nanswer). These paragraphs are dissimilar to the\noriginal distractors—there is a 9.82% overlap.\n\nWe report the Fl score of single-paragraph\nBERT on these new distractors in Table 4: the\naccuracy declines from 67.08 F1 to 46.84 Fl. How-\never, when the same procedure is done on the train-\ning set and the model is re-trained, the accuracy\nincreases to 60.10 F1 on the adversarial distractors.\n\nType Distractors We also experiment with filter-\ning the initial list of 50 paragraph to ones whose\nentity type (e.g., person) matches that of the gold\nparagraphs. This can help to eliminate the entity\ntype bias described in Section 4.1. As shown in\nTable 4, the original model’s accuracy degrades\nsignificantly (drops to 40.73 Fl). However, similar\nto the previous setup, the model trained on the ad-\nversarially selected distractors can recover most of\nits original accuracy (increases to 58.42 Fl).\n\nThese results show that single-paragraph BERT\ncan struggle when the distribution of the distrac-\ntors changes (e.g., using adversarial selection rather\nthan only TF-IDF). Moreover, the model can some-\nwhat recover its original accuracy when re-trained\non distractors from the new distribution.\n\n6 Conclusions\n\nIn summary, we demonstrate that question compo-\nsitionality is not a sufficient condition for multi-hop\nreasoning. Instead, future datasets must carefully\nconsider what evidence they provide in order to\nensure multi-hop reasoning is required. There are\nat least two different ways to achieve this.\n\nOpen-domain Questions Our single-hop model\nstruggles in the open-domain setting. We largely\nattribute this to the insufficiencies of standard TF-\nIDF retrieval for multi-hop questions. For example,\nwe fail to retrieve the paragraph about “Bonobo\napes” in Figure 1, because the question does not\ncontain terms about “Bonobo apes.” Table 5 shows\nthat the model achieves 39.12 Fl given 500 re-\ntrieved paragraphs, but achieves 53.12 Fl when\nadditional two gold paragraphs are given, demon-\nstrating the significant effect of failure to retrieve\ngold paragraphs. In this context, we suggest that\nfuture work can explore better retrieval methods\nfor multi-hop questions.\n\nRetrieving Strong Distractors Another way to\nensure multi-hop reasoning is to select strong dis-\n\nSetting Fl\nDistractor 67.08\nOpen-domain 10 Paragraphs 38.40\nOpen-domain 500 Paragraphs 39.12\n+ Gold Paragraph 53.12\n\nTable 5: The accuracy of single-paragraph BERT in\ndifferent open-domain retrieval settings. TF-IDF often\nfails to retrieve the gold paragraphs even when using\n500 candidates.\n\ntractor paragraphs. For example, we found 35% of\nbridge questions are currently single-hop but may\nbecome multi-hop when combined with stronger\ndistractors (Section 4.1). However, as we demon-\nstrate in Section 5, selecting strong distractors\nfor RC questions is non-trivial. We suspect this\nis also due to the insufficiencies of standard TF-\nIDF retrieval for multi-hop questions. In partic-\nular, Table 5 shows that single-paragraph BERT\nachieves 53.12 Fl even when using 500 distractors\n(rather than eight), indicating that 500 distractors\nare still insufficient. In this end, future multi-hop\nRC datasets can develop improved methods for\ndistractor collection.\n\nAcknowledgements\n\nThis research was supported by ONR (NO00014-18-\n1-2826, NO0014-17-S-B001), NSF (IIS-1616112,\nTIS-1252835, IIS-1562364), ARO (W911NF-16-1-\n0121), an Allen Distinguished Investigator Award,\nSamsung GRO and gifts from Allen Institute for\nAI, Google, and Amazon.\n\nThe authors would like to thank Shi Feng, Nikhil\nKandpal, Victor Zhong, the members of AllenNLP\nand UW NLP, and the anonymous reviewers for\ntheir valuable feedback.\n\nReferences\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In EMNLP.\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In ACL.\n\nJifan Chen and Greg Durrett. 2019. Understanding\ndataset design choices for multi-hop reasoning. In\nNAACL.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\n4253\n", "vlm_text": "\nWe report the F1 score of single-paragraph BERT on these new distractors in Table  4 : the accuracy declines from 67.08 F1 to 46.84 F1. How- ever, when the same procedure is done on the train- ing set and the model is re-trained, the accuracy increases to 60.10 F1 on the adversarial distractors. \nType Distractors We also experiment with ﬁlter- ing the initial list of 50 paragraph to ones whose entity type (e.g., person) matches that of the gold paragraphs. This can help to eliminate the entity type bias described in Section  4.1 . As shown in Table  4 , the original model’s accuracy degrades signiﬁcantly (drops to 40.73 F1). However, similar to the previous setup, the model trained on the ad- versarially selected distractors can recover most of its original accuracy (increases to 58.42 F1). \nThese results show that single-paragraph BERT can struggle when the distribution of the distrac- tors changes (e.g., using adversarial selection rather than only TF-IDF). Moreover, the model can some- what recover its original accuracy when re-trained on distractors from the new distribution. \n6 Conclusions \nIn summary, we demonstrate that question compo- sitionality is not a sufﬁcient condition for multi-hop reasoning. Instead, future datasets must carefully consider what evidence they provide in order to ensure multi-hop reasoning is required. There are at least two different ways to achieve this. \nOpen-domain Questions Our single-hop model struggles in the open-domain setting. We largely attribute this to the insufﬁciencies of standard TF- IDF retrieval for multi-hop questions. For example, we fail to retrieve the paragraph about “Bonobo apes” in Figure  1 , because the question does not contain terms about “Bonobo apes.” Table  5  shows that the model achieves 39.12 F1 given 500 re- trieved paragraphs, but achieves 53.12 F1 when additional two gold paragraphs are given, demon- strating the signiﬁcant effect of failure to retrieve gold paragraphs. In this context, we suggest that future work can explore better retrieval methods for multi-hop questions. \nRetrieving Strong Distractors Another way to ensure multi-hop reasoning is to select strong dis- \nThe table displays the F1 scores for various settings related to a task, presumably in the context of natural language processing or machine learning, judging by the terms used.\n\n- **Setting Column**: This column lists different experimental scenarios or configurations under which the F1 score is measured. The settings are \"Distractor,\" \"Open-domain 10 Paragraphs,\" \"Open-domain 500 Paragraphs,\" and \"Open-domain 500 Paragraphs + Gold Paragraph.\"\n\n- **F1 Column**: This column provides the F1 score corresponding to each setting. F1 score is a measure of a model's accuracy that considers both precision and recall, commonly used in information retrieval and binary classification tasks.\n\n  - For the \"Distractor\" setting, the F1 score is 67.08.\n  - For the \"Open-domain 10 Paragraphs\" setting, the F1 score is 38.40.\n  - For the \"Open-domain 500 Paragraphs\" setting, the F1 score is 39.12.\n  - When a \"Gold Paragraph\" is added to the \"Open-domain 500 Paragraphs\" setting, the F1 score improves to 53.12.\n\nThe table indicates how different configurations affect the performance of a model, with \"Distractor\" achieving the highest score, and the inclusion of a \"Gold Paragraph\" in an open-domain setting significantly boosting the F1 score compared to not having it.\nTable 5: The accuracy of single-paragraph BERT in different open-domain retrieval settings. TF-IDF often fails to retrieve the gold paragraphs even when using 500 candidates. \ntractor paragraphs. For example, we found    $35\\%$   of bridge questions are currently single-hop but may become multi-hop when combined with stronger distractors (Section  4.1 ). However, as we demon- strate in Section  5 , selecting strong distractors for RC questions is non-trivial. We suspect this is also due to the insufﬁciencies of standard TF- IDF retrieval for multi-hop questions. In partic- ular, Table  5  shows that single-paragraph BERT achieves 53.12 F1 even when using 500 distractors (rather than eight), indicating that 500 distractors are still insufﬁcient. In this end, future multi-hop RC datasets can develop improved methods for distractor collection. \nAcknowledgements \nThis research was supported by ONR (N00014-18- 1-2826, N00014-17-S-B001), NSF (IIS-1616112, IIS-1252835, IIS-1562364), ARO (W911NF-16-1- 0121), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, and Amazon. \nThe authors would like to thank Shi Feng, Nikhil Kandpal, Victor Zhong, the members of AllenNLP and UW NLP, and the anonymous reviewers for their valuable feedback. \nReferences \nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In  EMNLP . Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open- domain questions. In  ACL . Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In NAACL . Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In  ACL . "}
{"page": 5, "image_path": "doc_images/P19-1416_5.jpg", "ocr_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\n\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS.\n\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In ACL.\n\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In JCLR.\n\nToma’ Kotisky, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, Gabor Melis, and\nEdward Grefenstette. 2018. The NarrativeQA read-\ning comprehension challenge. TACL.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn MIPS Autodiff Workshop.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQUAD: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\n\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\nflow for machine comprehension. In JCLR.\n\nSaku Sugawara, Kentaro fInui, Satoshi Sekine, and\nAkiko Aizawa. 2018. What makes reading compre-\nhension questions easier? In EMNLP.\n\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn NAACL.\n\nJohannes Welbl, Pontus Stenetorp, and Sebastian\nRiedel. 2017. Constructing Datasets for Multi-hop\nReading Comprehension Across Documents. In\nTACL.\n\nCaiming Xiong, Victor Zhong, and Richard Socher.\n2018. DCN+: Mixed objective and deep residual\ncoattention for question answering. In JCLR.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question\nanswering. In EMNLP.\n\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V.\nLe. 2018. Qanet: Combining local convolution with\nglobal self-attention for reading comprehension. In\nICLR.\n\n4254\n", "vlm_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. In  NAACL . Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In  NIPS . Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In  ACL . Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In  ICLR . Tom´ aˇ s Koˇ cisk\\` y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´ abor Melis, and Edward Grefenstette. 2018. The NarrativeQA read- ing comprehension challenge.  TACL . Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. In  NIPS Autodiff Workshop . Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD:   $100{,}000{+}$   questions for machine comprehension of text. In  EMNLP . Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention ﬂow for machine comprehension. In  ICLR . Saku Sugawara, Kentaro fInui, Satoshi Sekine, and Akiko Aizawa. 2018. What makes reading compre- hension questions easier? In  EMNLP . Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In  NAACL . Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2017. Constructing Datasets for Multi-hop Reading Comprehension Across Documents. In TACL . Caiming Xiong, Victor Zhong, and Richard Socher. 2018.   $\\mathbf{DCN+}$  : Mixed objective and deep residual coattention for question answering. In  ICLR . Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In  EMNLP . Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. In ICLR . "}
{"page": 6, "image_path": "doc_images/P19-1416_6.jpg", "ocr_text": "A Example Distractor Question\n\nWe present the full example from Figure 1 below.\nParagraphs 1| and 5 are the two gold paragraphs.\n\nQuestion What is the former name of the ani-\nmal whose habitat the Réserve Naturelle Lomako\nYokokala was established to protect?\n\nAnswer pygmy chimpanzee\n\n(Gold Paragraph) Paragraph 1 The bonobo\n(or ; “Pan paniscus”), formerly called the\npygmy chimpanzee and less often, the dwarf or\ngracile chimpanzee, is an endangered great ape\nand one of the two species making up the genus\n“Pan”; the other is “Pan troglodytes”, or the com-\nmon chimpanzee. Although the name “chimpanzee”\nis sometimes used to refer to both species together,\nit is usually understood as referring to the common\nchimpanzee, whereas “Pan paniscus” is usually re-\nferred to as the bonobo.\n\nParagraph 2 The Carriére des Nerviens Re-\ngional Nature Reserve (in French “Réserve na-\nturelle régionale de la carriére des Nerviens”) is\na protected area in the Nord-Pas-de-Calais region\nof northern France. It was established on 25 May\n2009 to protect a site containing rare plants and cov-\ners just over 3 ha. It is located in the municipalities\nof Bavay and Saint-Waast in the Nord department.\n\nParagraph 3 Céreste (Occitan: “Ceirésta’’) is\na commune in the Alpes-de-Haute-Provence de-\npartment in southeastern France. It is known for\nits rich fossil beds in fine layers of “Calcaire de\nCampagne Calavon” limestone, which are now pro-\ntected by the Parc naturel régional du Luberon and\nthe Réserve naturelle géologique du Luberon.\n\nParagraph 4 The Grand Cote National Wildlife\nRefuge (French: “Réserve Naturelle Faunique Na-\ntionale du Grand- Cote”) was established in 1989\nas part of the North American Waterfowl Manage-\nment Plan. It is a 6000 acre reserve located in\nAvoyelles Parish, near Marksville, Louisiana, in\nthe United States.\n\n(Gold Paragraph) Paragraph 5 The Lomako\nForest Reserve is found in Democratic Republic of\nthe Congo. It was established in 1991 especially\nto protect the habitat of the Bonobo apes. This site\ncovers 3,601.88 km?.\n\nParagraph 6 Guadeloupe National Park (French:\n“Parc national de la Guadeloupe’) is a national\npark in Guadeloupe, an overseas department of\nFrance located in the Leeward Islands of the eastern\nCaribbean region. The Grand Cul-de-Sac Marin\nNature Reserve (French: “Réserve Naturelle du\nGrand Cul-de-Sac Marin’) is a marine protected\narea adjacent to the park and administered in con-\njunction with it. Together, these protected areas\ncomprise the Guadeloupe Archipelago (French:\n“TY Archipel de la Guadeloupe”) biosphere reserve.\n\nParagraph 7 La Désirade National Nature Re-\nserve (French: “Réserve naturelle nationale de La\nDésirade’’) is a reserve in Désirade Island in Guade-\nloupe. Established under the Ministerial Decree No.\n2011-853 of 19 July 2011 for its special geologi-\ncal features it has an area of 62 ha. The reserve\nrepresents the geological heritage of the Caribbean\ntectonic plate, with a wide spectrum of rock for-\nmations, the outcrops of volcanic activity being\nremnants of the sea level oscillations. It is one of\nthirty three geosites of Guadeloupe.\n\nParagraph 8 La Tortue ou |’Ecalle or Ile Tortue\nis a small rocky islet off the northeastern coast\nof Saint Barthélemy in the Caribbean. Its highest\npoint is 35 m above sea level. Referencing tortoises,\nit forms part of the Réserve naturelle nationale de\nSaint-Barthélemy with several of the other northern\nislets of St Barts.\n\nParagraph 9 Nature Reserve of Saint\nBartholomew (Réserve Naturelle de Saint-\nBarthélemy) is a nature reserve of Saint\nBarthélemy (RNN 132), French West Indies, an\noverseas collectivity of France.\n\nParagraph 10 Ile Fourchue, also known as Ile\nFourche is an island between Saint-Barthélemy and\nSaint Martin, belonging to the Collectivity of Saint\nBarthélemy. The island is privately owned. The\nonly inhabitants are some goats. The highest point\nis 103 meter above sea level. It is situated within\nRéserve naturelle nationale de Saint-Barthélemy.\n\nB_ Full Model Details\n\nSingle-paragraph BERT is a pipeline which first\nretrieves a single paragraph using a classifier and\nthen selects the associated answer. Formally, the\nmodel receives a question Q = [q1,..,¢m] and a\nsingle paragraph P = [p1,...,pn] as input. The\nquestion and paragraph are merged into a single\n\n4255\n", "vlm_text": "A Example Distractor Question \nWe present the full example from Figure  1  below. Paragraphs 1 and 5 are the two gold paragraphs. \nQuestion What is the former name of the ani- mal whose habitat the R eserve Naturelle Lomako Yokokala was established to protect? \nAnswer pygmy chimpanzee\n\n \n(Gold Paragraph) Paragraph 1 The bonobo\n\n (or ; “Pan paniscus”), formerly called the pygmy chimpanzee  and less often, the dwarf or gracile chimpanzee, is an endangered great ape and one of the two species making up the genus “Pan”; the other is “Pan troglodytes”, or the com- mon chimpanzee. Although the name “chimpanzee” is sometimes used to refer to both species together, it is usually understood as referring to the common chimpanzee, whereas “Pan paniscus” is usually re- ferred to as the bonobo. \nParagraph 2 The Carri ere des Nerviens Re- gional Nature Reserve (in French “R eserve na- turelle r egionale de la carri ere des Nerviens”) is a protected area in the Nord-Pas-de-Calais region of northern France. It was established on 25 May 2009 to protect a site containing rare plants and cov- ers just over 3 ha. It is located in the municipalities of Bavay and Saint-Waast in the Nord department. \nParagraph 3 C ereste (Occitan: “Ceir esta”) is a commune in the Alpes-de-Haute-Provence de- partment in southeastern France. It is known for its rich fossil beds in ﬁne layers of “Calcaire de Campagne Calavon” limestone, which are now pro- tected by the Parc naturel r egional du Luberon and the R´ eserve naturelle g´ eologique du Luberon. \nParagraph 4 The Grand Cote National Wildlife Refuge (French: “R eserve Naturelle Faunique Na- tionale du Grand- Cote”) was established in 1989 as part of the North American Waterfowl Manage- ment Plan. It is a 6000 acre reserve located in Avoyelles Parish, near Marksville, Louisiana, in the United States. \n(Gold Paragraph) Paragraph 5 The Lomako Forest Reserve is found in Democratic Republic of the Congo. It was established in 1991 especially to protect the habitat of the Bonobo apes. This site covers  $3{,}601.88\\;\\mathrm{km^{2}}$  . \nParagraph 6 Guadeloupe National Park (French: “Parc national de la Guadeloupe”) is a national park in Guadeloupe, an overseas department of France located in the Leeward Islands of the eastern Caribbean region. The Grand Cul-de-Sac Marin Nature Reserve (French: “R eserve Naturelle du Grand Cul-de-Sac Marin”) is a marine protected area adjacent to the park and administered in con- junction with it. Together, these protected areas comprise the Guadeloupe Archipelago (French: “l’Archipel de la Guadeloupe”) biosphere reserve. \nParagraph 7 La D esirade National Nature Re- serve (French: “R eserve naturelle nationale de La D esirade”) is a reserve in D esirade Island in Guade- loupe. Established under the Ministerial Decree No. 2011-853 of 19 July 2011 for its special geologi- cal features it has an area of 62 ha. The reserve represents the geological heritage of the Caribbean tectonic plate, with a wide spectrum of rock for- mations, the outcrops of volcanic activity being remnants of the sea level oscillations. It is one of thirty three geosites of Guadeloupe. \nParagraph 8 La Tortue ou l’Ecalle or Ile Tortue is a small rocky islet off the northeastern coast of Saint Barth elemy in the Caribbean. Its highest point is  $35\\,\\mathrm{m}$   above sea level. Referencing tortoises, it forms part of the R eserve naturelle nationale de Saint-Barth elemy with several of the other northern islets of St Barts. \nParagraph 9 Nature Reserve of Saint Bartholomew (R eserve Naturelle de Saint- Barth elemy) is a nature reserve of Saint Barth elemy (RNN 132), French West Indies, an overseas collectivity of France. \nParagraph 10 Ile Fourchue, also known as Ile Fourche is an island between Saint-Barth elemy and Saint Martin, belonging to the Collectivity of Saint Barth elemy. The island is privately owned. The only inhabitants are some goats. The highest point is 103 meter above sea level. It is situated within R´ eserve naturelle nationale de Saint-Barth´ elemy. \nB Full Model Details \nSingle-paragraph BERT is a pipeline which ﬁrst retrieves a single paragraph using a classiﬁer and then selects the associated answer. Formally, the model receives a question    $Q\\,=\\,[q_{1},..,q_{m}]$   and a single paragraph    $P\\,=\\,[p_{1},...,p_{n}]$   as input. The question and paragraph are merged into a single "}
{"page": 7, "image_path": "doc_images/P19-1416_7.jpg", "ocr_text": "a\n\nTTt mettle\n\nBERT\n\nQuestion\n\nParagraph\n\nOutput 1\n\nOutput 2\n\nlowest\n\nOutput N\n\nFigure 3: Single-paragraph BERT reads and scores each paragraph independently. The answer from the paragraph\n\nwith the lowest y°™P'Y score is chosen as the final answer.\n\nsequence, S = [q1,..., dm, [SEP], p1, -.-, Pn], where\n[SEP] is a special token indicating the boundary.\nThe sequence is fed into BERT-BASE:\n\n= BERT(S) € Rix(mtnt)\n\nwhere his the hidden dimension of BERT. Next,\na classifier uses max-pooling and learned parame-\nters W, € R’*4 to generate four scalars:\n\n[Yspani Yyes; Yno; Yempty] = W,maxpool(S’),\n\nwhere Yspan; Yyes; Yno aNd Yempty indicate the\nanswer is either a span, yes, no, or no answer.\n\nA candidate answer span is then computed sepa-\nrately from the classifier. We define\n\nPstart = Softmax(W2S’)\nPend = Softmax(W35’),\n\nwhere W2, W3 € R\" are learned parameters. Then,\nYstart ANd Yenq are obtained:\n\n_ i j\nYstart> Yend = ATE MAX PhtartPend\nUSI\n\nwhere piyart and Pina indicate the i-th element of\nPstart and j-th element of pena, respectively.\n\nWe now have four scalar values Yspan, Yyes> Yno>\nand Yempty and a span from the paragraph span =\n\nFor HOTPOTQA, the input is a question and\nN context paragraphs. We create a batch of size\nNN, where each entry is a question and a single\nparagraph. Denote the ouput from i-th entry as\n\nYspan+ Yyes? Yno» Yemptyand span’. The final answer\nis selected as:\n\nj = argmin,(ybnpty)\nYmax = MAX(Yyans Yess Yo)\nspan) ifygpan = Ymax\nanswer = {yes — ifytes = Ymax\nno ifyho = Ymax\n\nDuring training, Yempty is set to 0 for the paragraph\nwhich contains the answer span and | otherwise.\n\nImplementation Details We use _ Py-\nTorch (Paszke et al., 2017) based on Hugging\nFace’s implementation.” We use Adam (Kingma\nand Ba, 2015) with learning rate 5 x 10-5. We\nlowercase the input and set the maximum sequence\nlength |S| to 300. If a sequence is longer than 300,\nwe split it into multiple sequences and treat them\nas different examples.\n\nC_ Categorizing Comparison Questions\n\nThis section describes how we categorize compari-\nson questions. We first identify ten question opera-\ntions that sufficiently cover comparison questions\n(Table 6). Next, for each question, we extract the\ntwo entities under comparison using the Spacy®\nNER tagger on the question and the two HOT-\nPOTQA supporting facts. Using these extracted\n\nShttps://github.com/huggingface/\npytorch-pretrained-BERT\n®nttps://spacy.io/\n\n4256\n", "vlm_text": "The image illustrates a process involving the BERT model for question answering. Here's a breakdown:\n\n1. **Left Side:**\n   - The question \"Where is the company headquartered?\" and a paragraph are input into BERT.\n   - BERT outputs several scores, including `y_span`, `y_yes`, `y_no`, and `y_empty`.\n   - The figure shows weights (`W1`, `W2`, `W3`) applied to the outputs for determining the answer's start and end (`y_start`, `y_end`).\n\n2. **Right Side:**\n   - Multiple paragraphs are processed independently by BERT.\n   - Each paragraph has outputs with `y_empty` and a choice of `span/yes/no`.\n   - The paragraph with the lowest `y_empty` score is selected, and its corresponding answer is chosen as the final answer.\n\nThis process uses BERT to independently evaluate each paragraph and select the paragraph whose result appears most confidently not to be empty.\nsequence,  $S=[q_{1},...,q_{m},[\\mathrm{SEP}],p_{1},...,p_{n}]$  [ SEP ]  is a special token indicating the boundary. The sequence is fed into BERT- BASE : \n\n$$\nS^{\\prime}=\\operatorname{BRT}(S)\\in\\mathbb{R}^{h\\times(m+n+1)},\n$$\n \nwhere  $h$   is the hidden dimension of BERT. Next, a classiﬁer uses max-pooling and learned parame- ters  $W_{1}\\in\\mathbb{R}^{h\\times4}$    to generate four scalars: \n\n$$\n[y_{\\mathrm{span}};y_{\\mathrm{geqslant}};y_{\\mathrm{no}};y_{\\mathrm{empty}}]=W_{1}\\mathrm{maxpool}(S^{\\prime}),\n$$\n \nwhere    $y_{\\mathrm{span}},y_{\\mathrm{yes}},y_{\\mathrm{no}}$   and    $y_{\\mathrm{emptysety}}$   indicate the answer is either a span,    $\\mathtt{Y}\\!\\in\\!S$  ,  no , or no answer. \nA candidate answer span is then computed sepa- rately from the classiﬁer. We deﬁne \n\n$$\n\\begin{array}{r}{p_{\\mathrm{start}}=\\mathrm{Softmax}(W_{2}S^{\\prime})}\\\\ {p_{\\mathrm{end}}=\\mathrm{Softmax}(W_{3}S^{\\prime}),}\\end{array}\n$$\n \nwhere    $W_{2},W_{3}\\in\\mathbb{R}^{h}$    are learned parameters. Then,  $y_{\\mathrm{start}}$   and  $y_{\\mathrm{end}}$   are obtained: \n\n$$\ny_{\\mathrm{start}},y_{\\mathrm{end}}=\\arg\\operatorname*{max}_{i\\leq j}p_{\\mathrm{start}}^{i}p_{\\mathrm{end}}^{j}\n$$\n \nwhere  $p_{\\mathrm{start}}^{i}$    and  $p_{\\mathrm{end}}^{j}$    indicate the  $i$  -th element of  $p_{\\mathrm{start}}$   and  $j$  -th element of  $p_{\\mathrm{end}}$  , respectively. \nWe now have four scalar values    $y_{\\mathrm{span}}$  ,  y yes ,    $y_{\\mathrm{no}}$  , and  $y_{\\mathrm{emptysety}}$   and a span from the paragraph  span  $=$   $[S_{y_{\\mathrm{start}}},.\\cdot\\cdot,S_{y_{\\mathrm{end}}}]$  . \nFor H OTPOT QA, the input is a question and  $N$   context paragraphs. We create a batch of size  $N$  , where each entry is a question and a single paragraph. Denote the ouput from    $i$  -th entry as  $y_{\\mathrm{span}}^{i},y_{\\mathrm{geqslant}}^{i},y_{\\mathrm{no}}^{i},y_{\\mathrm{empty}}^{i}{\\mathrm{and}}\\,{\\mathrm{span}}^{i}$  . The ﬁnal answer is selected as: \n\n\n$$\n\\begin{array}{r c l}{j}&{=}&{\\mathrm{argmin}_{i}(y_{\\mathrm{empty}}^{i})}\\\\ {y_{\\mathrm{max}}}&{=}&{\\operatorname*{max}(y_{\\mathrm{span}}^{j},y_{\\mathrm{geqslant}}^{j},y_{\\mathrm{no}}^{j})}\\\\ {\\mathrm{nswer}}&{=}&{\\left\\{\\begin{array}{l l}{\\mathrm{span}^{j}}&{\\mathrm{if}\\,y_{\\mathrm{span}}^{j}=y_{\\mathrm{max}}}\\\\ {\\mathrm{y\\,e\\,s}}&{\\mathrm{if}\\,y_{\\mathrm{geqslant}}^{j}=y_{\\mathrm{max}}}\\\\ {\\mathrm{no}}&{\\mathrm{if}\\,y_{\\mathrm{no}}^{j}=y_{\\mathrm{max}}}\\end{array}\\right.}\\end{array}\n$$\n \nDuring training,  $y_{\\mathrm{empty}}^{i}$    is set to 0 for the paragraph which contains the answer span and 1 otherwise. \nImplementation Details We use Py- Torch ( Paszke et al. ,  2017 ) based on Hugging Face’s implementation.   We use Adam ( Kingma and Ba ,  2015 ) with learning rate    $5\\times10^{-5}$  . We lowercase the input and set the maximum sequence length    $|S|$   to  300 . If a sequence is longer than  300 , we split it into multiple sequences and treat them as different examples. \nC Categorizing Comparison Questions \nThis section describes how we categorize compari- son questions. We ﬁrst identify ten question opera- tions that sufﬁciently cover comparison questions (Table  6 ). Next, for each question, we extract the two entities under comparison using the Spacy 6 NER tagger on the question and the two H OT - POT QA supporting facts. Using these extracted "}
{"page": 8, "image_path": "doc_images/P19-1416_8.jpg", "ocr_text": "Operation & Example\n\nNumerical Questions\n\nOperations: Is greater / Is smaller / Which is greater / Which is smaller\n\nExample (Which is smaller): Who was born first, Arthur Conan Doyle or Penelope Lively?\n\nLogical Questions\nOperations: And / Or / Which is true\nExample (And): Are Hot Rod and the Memory of Our People both magazines?\n\nString Questions\nOperations: Is equal / Not equal / Intersection\nExample (Is equal): Are Cardinal Health and Kansas City Southern located in the same state?\n\nTable 6: The question operations used for categorizing comparison questions.\n\nAlgorithm 1 Algorithm for Identifying Question Operations\n\n1: procedure CATEGORIZE(question, entity 1, entity2)\n2 coordination, preconjunct < f (question, entity1, entity2)\n3 Determine if the question is either question or both question from coordination and preconjunct\n4 head entity ~— f;eaa(question, entity1, entity2)\n5: if more, most, later, last, latest, longer, larger, younger, newer, taller, higher in question then\n6 if head entity exists then discrete_operation <~ Which is greater\n7 else discrete_operation + Is greater\n8 else if less, earlier, earliest, first, shorter, smaller, older, closer in question then\nif head entity exists then discrete_operation <~ Which is smaller\nelse discrete_operation + Is smaller\nelse if head entity exists then\ndiscrete_operation <~ Which is true\nelse if question is not yes/no question and asks for the property in common then\ndiscrete_operation + Intersection\nelse if question is yes/no question then\nDetermine if question asks for logical comparison or string comparison\nif question asks for logical comparison then\nif either question then discrete_operation < Or\nelse if both question then discrete_-operation <~ And\n\nVee eee ee eee\n\nelse if question asks for string comparison then\n\n21: if asks for same? then discrete_operation + Is equal\n22: else if asks for difference? then discrete_operation « Not equal\n23: return discrete_operation\n\nentities, we identity the suitable question operation\nfollowing Algorithm 1.\n\nBased on the identified operation, questions\nare classified into multi-hop, context-dependent\nmulti-hop, or single-hop. First, numerical ques-\ntions are always multi-hop (e.g., first example of\nTable 6). Next, the operations And, Or, Is\nequal, and Not equal are context-dependent\nmulti-hop. For instance, in the second example\nof Table 6, if “Hot Rod” is not a magazine, one\ncan immediately answer No. Finally, the oper-\nations Which is true and Intersection\nare single-hop because they can be answered us-\ning one paragraph regardless of the context. For\ninstance, in the third example of Table 6, if Henry\nRoth’s paragraph explains he is from England, one\ncan answer Henry Roth, otherwise, the answer is\nRobert Erskine Childers.\n\n4257\n", "vlm_text": "The table categorizes comparison questions into three types based on the nature of the operation: Numerical, Logical, and String questions. Each category includes specific operations and provides an example for clarification.\n\n1. **Numerical Questions**:\n   - **Operations**: Is greater, Is smaller, Which is greater, Which is smaller\n   - **Example**: \"Who was born first, Arthur Conan Doyle or Penelope Lively?\" (This example relates to determining which one is smaller, such as finding the earlier date).\n\n2. **Logical Questions**:\n   - **Operations**: And, Or, Which is true\n   - **Example**: \"Are Hot Rod and the Memory of Our People both magazines?\" (This example uses the logical \"And\" operation to determine if both subjects share a characteristic).\n\n3. **String Questions**:\n   - **Operations**: Is equal, Not equal, Intersection\n   - **Example**: \"Are Cardinal Health and Kansas City Southern located in the same state?\" (This example uses the \"Is equal\" operation to assess if both entities share the same location trait).\n\nOverall, the table helps in understanding how different types of questions can be analyzed based on their logical, numerical, or string nature.\nAlgorithm 1  Algorithm for Identifying Question Operations \nThe table is titled \"Algorithm 1: Algorithm for Identifying Question Operations,\" and it outlines a procedure called \"CATEGORIZE,\" which appears to identify and categorize operations within a given question based on specific criteria.\n\n1. **Procedure Initialize**: The procedure starts by defining a function `CATEGORIZE` that takes three arguments: `question`, `entity1`, and `entity2`.\n\n2. **Coordination and Preconjunct Determination**: It sets the `coordination` and `preconjunct` variables using a function `f(question, entity1, entity2)`.\n\n3. **Operation Identification**:\n   - Checks if the question is an \"either\" or \"both\" type using `coordination` and `preconjunct`.\n   - Determines the `head entity` using another function `f_head(question, entity1, entity2)`.\n   - It then checks for key comparative words in the question to define the `discrete_operation`:\n     - Words like \"more,\" \"most,\" \"later,\" etc., lead to operations like \"Which is greater\" or \"Is greater\".\n     - Words like \"less,\" \"earlier,\" \"first,\" etc., lead to operations like \"Which is smaller\" or \"Is smaller\".\n\n4. **Head Entity Condition**: If a head entity exists, it sets `discrete_operation` to \"Which is true\".\n\n5. **Property Comparison**:\n   - If the question isn’t a yes/no question but asks for a common property, it sets `discrete_operation` to \"Intersection\".\n\n6. **Logical and String Comparison for Yes/No Questions**:\n   - It checks if the question asks for a logical comparison, applying \"Or\" or \"And\" operations based on the presence of \"either\" or \"both\".\n   - For string comparisons, if the question asks for sameness, it sets \"Is equal\". If it asks for a difference, it sets \"Not equal\".\n\n7. **Return Statement**: Finally, the procedure returns the `discrete_operation` identified.\n\nThis algorithm is used to systematically classify and handle operations found in questions, providing a discrete operation for further analysis or processing.\nentities, we identity the suitable question operation following Algorithm  1 . \nBased on the identiﬁed operation, questions are classiﬁed into multi-hop, context-dependent multi-hop, or single-hop. First, numerical ques- tions are always multi-hop (e.g., ﬁrst example of Table  6 ). Next, the operations  And, Or, Is equal,  and  Not equal  are context-dependent multi-hop. For instance, in the second example of Table  6 , if “Hot Rod” is not a magazine, one can immediately answer  No . Finally, the oper- ations  Which is true  and  Intersection are single-hop because they can be answered us- ing one paragraph regardless of the context. For instance, in the third example of Table  6 , if Henry Roth’s paragraph explains he is from England, one can answer Henry Roth, otherwise, the answer is Robert Erskine Childers. "}
