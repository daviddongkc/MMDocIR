{"page": 0, "image_path": "doc_images/D18-1003_0.jpg", "ocr_text": "DeClarE: Debunking Fake News and False Claims\nusing Evidence-Aware Deep Learning\n\nKashyap Popat', Subhabrata Mukherjee”, Andrew Yates', Gerhard Weikum!\n™Max Planck Institute for Informatics, Saarbriicken, Germany\n?Amazon Inc., Seattle, USA\n{kpopat, ayates, weikum}@mpi-inf.mpg.de, subhomj@amazon.com\n\nAbstract\n\nMisinformation such as fake news is one of\nthe big challenges of our society. Research on\nautomated fact-checking has proposed meth-\nods based on supervised learning, but these\napproaches do not consider external evidence\napart from labeled training instances. Recent\napproaches counter this deficit by considering\nexternal sources related to a claim. However,\nthese methods require substantial feature mod-\neling and rich lexicons. This paper overcomes\nthese limitations of prior work with an end-to-\nend model for evidence-aware credibility as-\nsessment of arbitrary textual claims, without\nany human intervention. It presents a neural\nnetwork model that judiciously aggregates sig-\nnals from external evidence articles, the lan-\nguage of these articles and the trustworthiness\nof their sources. It also derives informative\nfeatures for generating user-comprehensible\nexplanations that makes the neural network\npredictions transparent to the end-user. Exper-\niments with four datasets and ablation studies\nshow the strength of our method.\n\n1 Introduction\n\nMotivation: Modern media (e.g., news feeds, mi-\ncroblogs, etc.) exhibit an increasing fraction of\nmisleading and manipulative content, from ques-\ntionable claims and “alternative facts” to com-\npletely faked news. The media landscape is be-\ncoming a twilight zone and battleground. This so-\ncietal challenge has led to the rise of fact-checking\nand debunking websites, such as Snopes.com\nand PolitiFact.com, where people research claims,\nmanually assess their credibility, and present their\nverdict along with evidence (e.g., background ar-\nticles, quotations, etc.). However, this manual ver-\nification is time-consuming. To keep up with the\nscale and speed at which misinformation spreads,\nwe need tools to automate this debunking process.\n\n22\n\nState of the Art and Limitations: Prior work on\n“truth discovery” (see Li et al. (2016) for survey)!\nlargely focused on structured facts, typically in\nthe form of subject-predicate-object triples, or on\nsocial media platforms like Twitter, Sina Weibo,\netc. Recently, methods have been proposed to as-\nsess the credibility of claims in natural language\nform (Popat et al., 2017; Rashkin et al., 2017;\nWang, 2017), such as news headlines, quotes from\nspeeches, blog posts, etc.\n\nThe methods geared for general text input ad-\ndress the problem in different ways. On the one\nhand, methods like Rashkin et al. (2017); Wang\n(2017) train neural networks on labeled claims\nrom sites like PolitiFact.com, providing credibil-\nity assessments without any explicit feature mod-\neling. However, they use only the text of ques-\nionable claims and no external evidence or inter-\nactions that provide limited context for credibil-\nity analysis. These approaches also do not offer\nany explanation of their verdicts. On the other\nhand, Popat et al. (2017) considers external evi-\ndence in the form of other articles (retrieved from\nhe Web) that confirm or refute a claim, and jointly\nassesses the language style (using subjectivity lex-\nicons), the trustworthiness of the sources, and the\ncredibility of the claim. This is achieved via a\npipeline of supervised classifiers. On the upside,\nhis method generates user-interpretable explana-\nions by pointing to informative snippets of evi-\ndence articles. On the downside, it requires sub-\nstantial feature modeling and rich lexicons to de-\nect bias and subjectivity in the language style.\nApproach and Contribution: To overcome the\nlimitations of the prior works, we present De-\nClarE”, an end-to-end neural network model for\nassessing and explaining the credibility of arbi-\n\n‘As fully objective and unarguable truth is often elusive\nor ill-defined, we use the term credibility rather than “truth”.\n?Debunking Claims with Interpretable Evidence\n\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22-32\nBrussels, Belgium, October 31 - November 4, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning \nKashyap Popat 1 , Subhabrata Mukherjee 2 , Andrew Yates 1 , Gerhard Weikum 1 1 Max Planck Institute for Informatics, Saarbr¨ ucken, Germany \nAmazon Inc., Seattle, USA { kpopat,ayates,weikum } @mpi-inf.mpg.de, subhomj@amazon.com \nAbstract \nMisinformation such as fake news is one of the big challenges of our society. Research on automated fact-checking has proposed meth- ods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deﬁcit by considering external sources related to a claim. However, these methods require substantial feature mod- eling and rich lexicons. This paper overcomes these limitations of prior work with an end-to- end model for evidence-aware credibility as- sessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates sig- nals from external evidence articles, the lan- guage of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Exper- iments with four datasets and ablation studies show the strength of our method. \n1 Introduction \nMotivation:  Modern media (e.g., news feeds, mi- croblogs, etc.) exhibit an increasing fraction of misleading and manipulative content, from ques- tionable claims and “alternative facts” to com- pletely faked news. The media landscape is be- coming a twilight zone and battleground. This so- cietal challenge has led to the rise of fact-checking and debunking websites, such as  Snopes.com and  PolitiFact.com , where people research claims, manually assess their credibility, and present their verdict along with evidence (e.g., background ar- ticles, quotations, etc.). However, this manual ver- iﬁcation is time-consuming. To keep up with the scale and speed at which misinformation spreads, we need tools to automate this debunking process. \nState of the Art and Limitations:  Prior work on “truth discovery” (see  Li et al.  ( 2016 ) for survey) 1 largely focused on structured facts, typically in the form of subject-predicate-object triples, or on social media platforms like Twitter, Sina Weibo, etc. Recently, methods have been proposed to as- sess the credibility of claims in natural language form ( Popat et al. ,  2017 ;  Rashkin et al. ,  2017 ; Wang ,  2017 ), such as news headlines, quotes from speeches, blog posts, etc. \nThe methods geared for general text input ad- dress the problem in different ways. On the one hand, methods like  Rashkin et al.  ( 2017 );  Wang ( 2017 ) train neural networks on labeled claims from sites like  PolitiFact.com , providing credibil- ity assessments without any explicit feature mod- eling. However, they use only the text of ques- tionable claims and no external evidence or inter- actions that provide limited context for credibil- ity analysis. These approaches also do not offer any explanation of their verdicts. On the other hand,  Popat et al.  ( 2017 ) considers external evi- dence in the form of other articles (retrieved from the Web) that conﬁrm or refute a claim, and jointly assesses the language style (using subjectivity lex- icons), the trustworthiness of the sources, and the credibility of the claim. This is achieved via a pipeline of supervised classiﬁers. On the upside, this method generates user-interpretable explana- tions by pointing to informative snippets of evi- dence articles. On the downside, it requires sub- stantial feature modeling and rich lexicons to de- tect bias and subjectivity in the language style. \nApproach and Contribution:  To overcome the limitations of the prior works, we present  De-  $C l a r E^{2}$  , an end-to-end neural network model for assessing and explaining the credibility of arbi- trary claims in natural-language text form. Our approach combines the best of both families of prior methods. Similar to  Popat et al.  ( 2017 ), De- ClarE incorporates external evidence or counter- evidence from the Web as well as signals from the language style and the trustworthiness of the un- derlying sources. However, our method does not require any feature engineering, lexicons, or other manual intervention.  Rashkin et al.  ( 2017 );  Wang ( 2017 ) also develop an end-to-end model, but De- ClarE goes far beyond in terms of considering ex- ternal evidence and joint interactions between sev- eral factors, and also in its ability to generate user- interpretable explanations in addition to highly accurate assessments. For example, given the natural-language input claim  “the gun epidemic is the leading cause of death of young African- American men, more than the next nine causes put together”  by Hillary Clinton, DeClarE draws on evidence from the Web to arrive at its verdict  cred- ible , and returns annotated snippets like the one in Table  6  as explanation. These snippets, which contain evidence in the form of statistics and as- sertions, are automatically extracted from web ar- ticles from sources of varying credibility. "}
{"page": 1, "image_path": "doc_images/D18-1003_1.jpg", "ocr_text": "trary claims in natural-language text form. Our\napproach combines the best of both families of\nprior methods. Similar to Popat et al. (2017), De-\nClarE incorporates external evidence or counter-\nevidence from the Web as well as signals from the\nlanguage style and the trustworthiness of the un-\nderlying sources. However, our method does not\nrequire any feature engineering, lexicons, or other\nmanual intervention. Rashkin et al. (2017); Wang\n(2017) also develop an end-to-end model, but De-\nClarE goes far beyond in terms of considering ex-\nternal evidence and joint interactions between sev-\neral factors, and also in its ability to generate user-\ninterpretable explanations in addition to highly\naccurate assessments. For example, given the\nnatural-language input claim “the gun epidemic\nis the leading cause of death of young African-\nAmerican men, more than the next nine causes put\ntogether” by Hillary Clinton, DeClarE draws on\nevidence from the Web to arrive at its verdict cred-\nible, and returns annotated snippets like the one\nin Table 6 as explanation. These snippets, which\ncontain evidence in the form of statistics and as-\nsertions, are automatically extracted from web ar-\nticles from sources of varying credibility.\n\nGiven an input claim, DeClarE searches for web\narticles related to the claim. It considers the con-\ntext of the claim via word embeddings and the\n(language of) web articles captured via a bidirec-\ntional LSTM (biLSTM), while using an attention\nmechanism to focus on parts of the articles accord-\ning to their relevance to the claim. DeClarE then\naggregates all the information about claim source,\nweb article contexts, attention weights, and trust-\nworthiness of the underlying sources to assess the\nclaim. It also derives informative features for in-\nterpretability, like source embeddings that capture\ntrustworthiness and salient words captured via at-\ntention. Key contributions of this paper are:\n\ne Model: An end-to-end neural network model\nwhich automatically assesses the credibility\nof natural-language claims, without any hand-\ncrafted features or lexicons.\n\nInterpretability: An attention mechanism in\nour model that generates user-comprehensible\nexplanations, making credibility verdicts\ntransparent and interpretable.\n\nExperiments: Extensive experiments on four\ndatasets and ablation studies, demonstrating\neffectiveness of our method over state-of-the-\nart baselines.\n\n23\n\n2 End-to-end Framework for Credibility\nAnalysis\n\nConsider a set of N claims (C;,,) from the respec-\ntive origins/sources (C'S,,), where n € [1, N].\nEach claim Cy, is reported by a set of M arti-\ncles (Amn) along with their respective sources\n(ASimjn), Where m € [1, M]. Each corresponding\ntuple of claim and its origin, reporting articles and\narticle sources — (C;,,C'Sn, Amn; ASm.n) forms\na training instance in our setting, along with the\ncredibility label of the claim used as ground-truth\nduring network training. Figure | gives a pictorial\noverview of our model. In the following sections,\nwe provide a detailed description of our approach.\n\n2.1 Input Representations\n\nThe input claim C,, of length / is represented as\n[c1, c2,.-., 1] where c; € R7 is the d-dimensional\nword embedding of the /-th word in the input\nclaim. The source/origin of the claim C'S; is rep-\nresented by a ds-dimensional embedding vector\nC8n, € Re,\n\nA reporting article Am» consisting of k to-\nkens is represented by [am,n,1, @m,n,2; +++; Im,n,kl>\nwhere Qmnjk © R¢ is the d-dimensional word\nembedding vector for the k-th word in the report-\ning article A,,,,. The claim and article word em-\nbeddings have shared parameters. The source of\nthe reporting article AS;,,7 is represented as a d,-\ndimensional vector, a8 © 4s. For the sake\nof brevity, we drop the notation subscripts n and\nm in the following sections by considering only a\nsingle training instance — the input claim C,, from\nsource C'S;,, the corresponding article Aj, and\nits sources AS,» given by: (C,C'S, A, AS).\n\n2.2 Article Representation\n\nTo create a representation of an article, which may\ncapture task-specific features such as whether it\ncontains objective language, we use a bidirectional\nLong Short-Term Memory (LSTM) network as\nproposed by Graves et al. (2005). A basic LSTM\ncell consists of various gates to control the flow of\ninformation through timesteps in a sequence, mak-\ning LSTMs suitable for capturing long and short\nrange dependencies in text that may be difficult\nto capture with standard recurrent neural networks\n(RNNs). Given an input word embedding of to-\nkens (az), an LSTM cell performs various non-\nlinear transformations to generate a hidden vector\nstate h;,, for each token at each timestep k.\n", "vlm_text": "\nGiven an input claim, DeClarE searches for web articles related to the claim. It considers the  con- text  of the claim via word embeddings and the (language of) web articles captured via a bidirec- tional LSTM (biLSTM), while using an  attention mechanism to focus on parts of the articles accord- ing to their relevance to the claim. DeClarE then aggregates all the information about claim source, web article contexts, attention weights, and trust- worthiness of the underlying sources to assess the claim. It also derives informative features for in- terpretability, like source embeddings that capture trustworthiness and salient words captured via at- tention. Key contributions of this paper are:\n\n \n•  Model:  An end-to-end neural network model which automatically assesses the credibility of natural-language claims, without any hand- \ncrafted features or lexicons.\n\n •  Interpret ability:  An  attention  mechanism in our model that generates user-comprehensible explanations, making credibility verdicts transparent and interpretable.\n\n •  Experiments:  Extensive experiments on four datasets and ablation studies, demonstrating effectiveness of our method over state-of-the- art baselines. \n2 End-to-end Framework for Credibility Analysis \nConsider a set of    $N$   claims    $\\left\\langle C_{n}\\right\\rangle$  from the respec- tive origins/sources    $\\left\\langle C S_{n}\\right\\rangle$  , where    $n~\\in~[1,N]$  . Each claim    $C_{n}$   is reported by a set of  M  arti- cles    $\\left<A_{m,n}\\right>$  along with their respective sources  $\\langle A S_{m,n}\\rangle$  , where    $m\\in[1,M]$  . Each corresponding tuple of claim and its origin, reporting articles and article sources –    $\\langle C_{n},C S_{n},A_{m,n},A S_{m,n}\\rangle$  forms a training instance in our setting, along with the credibility label of the claim used as ground-truth during network training. Figure  1  gives a pictorial overview of our model. In the following sections, we provide a detailed description of our approach. \n2.1 Input Representations \nThe input claim    $C_{n}$   of length    $l$   is represented as  $[c_{1},c_{2},...,c_{l}]$   where    $c_{l}\\,\\in\\,\\mathfrak{R}^{d}$    is the    $d$  -dimensional word embedding of the  l -th word in the input claim. The source/origin of the claim    $C S_{n}$   is rep- resented by a    $d_{s}$  -dimensional embedding vector  $c s_{n}\\in\\mathfrak{R}^{d_{s}}$  . \nA reporting article    $A_{m,n}$   consisting of    $k$   to- kens is represented by    $[a_{m,n,1},a_{m,n,2},...,a_{m,n,k}]$  , where    $a_{m,n,k}~\\in~\\mathfrak{R}^{d}$    is t e    $d$  -dimensional word embedding vector for the  k -th word in the report- ing article    $A_{m,n}$  . The claim and article word em- beddings have shared parameters. The source of the reporting article    $A S_{m,n}$   is represented as a    $d_{s}$  - dimensional vector,    $\\mathit{a s}_{m,n}\\ \\in\\ \\Re^{d_{s}}$  . For th sake of brevity, we drop the notation subscripts  n  and  $m$   in the following sections by considering only a single training instance – the input claim    $C_{n}$   from source    $C S_{n}$  , the corresponding article    $A_{m,n}$   and its sources    $A S_{m,n}$   given by:    $\\langle C,C S,A,A S\\rangle$  . \n2.2 Article Representation \nTo create a representation of an article, which may capture task-speciﬁc features such as whether it contains objective language, we use a bidirectional Long Short-Term Memory (LSTM) network as proposed by  Graves et al.  ( 2005 ). A basic LSTM cell consists of various gates to control the ﬂow of information through timesteps in a sequence, mak- ing LSTMs suitable for capturing long and short range dependencies in text that may be difﬁcult to capture with standard recurrent neural networks (RNNs). Given an input word embedding of to- kens    $\\left\\langle a_{k}\\right\\rangle$  , an LSTM cell performs various non- linear transformations to generate a hidden vector state  $h_{k}$   for each token at each timestep    $k$  . "}
{"page": 2, "image_path": "doc_images/D18-1003_2.jpg", "ocr_text": "Claim Word\nEmbeddings\n\nConeatenate Dense\n\nLayer\n\nClaim Source\nEmbedding\n\nC00 000\nOOOO00O0\n\nOOOO0O0\n\nArticle Word\nEmbeddings\n\nBidirectional\nLSTM\n\nArticle Source\nEmbedding\n\nf—»\n(@e®) —1e\nAttention e\nWeights: e\n(e)\n(e)\nOlt LI LI Lee\nje) Credibility\n(e) Score\n(e)\nSoftmax/\ne\nDe De\n(@OO)—_~@) terse dense\n\nConeatenate\nFeatures\n\nFigure 1: Framework for credibility assessment. Upper part of the pipeline combines the article and\nclaim embeddings to get the claim specific attention weights. Lower part of the pipeline captures the\narticle representation through biLSTM. Attention focused article representation along with the source\nembeddings are passed through dense layers to predict the credibility score of the claim.\n\nWe use bidirectional LSTMs in place of stan-\ndard LSTMs. Bidirectional LSTMs capture both\nthe previous timesteps (past features) and the fu-\nture timesteps (future features) via forward and\nbackward states respectively. Correspondingly,\nthere are two hidden states that capture past and\nfuture information that are concatenated to form\nthe final output as: hy = [hy, he].\n\n2.3 Claim Specific Attention\n\nAs we previously discussed, it is important to con-\nsider the relevance of an article with respect to the\nclaim; specifically, focusing or attending to parts\nof the article that discuss the claim. This is in con-\ntrast to prior works (Popat et al., 2017; Rashkin\net al., 2017; Wang, 2017) that ignore either the ar-\nticle or the claim, and therefore miss out on this\nimportant interaction.\n\nWe propose an attention mechanism to help our\nmodel focus on salient words in the article with\nrespect to the claim. To this end, we compute\nthe importance of each term in an article with\nrespect to an overall representation of the corre-\nsponding claim. Additionally, incorporating atten-\ntion helps in making our model transparent and in-\nterpretable, because it provides a way to generate\nthe most salient words in an article as evidence of\nour model’s verdict.\n\nFollowing Wieting et al. (2015), the overall rep-\nresentation of an input claim is generated by tak-\ning an average of the word embeddings of all the\n\n24\n\nwords therein:\n\nWe combine this overall representation of the\nclaim with each article term:\n\nGp = a BE\n\nwhere, a, € R44 and @ denotes the concatenate\noperation. We then perform a transformation to\nobtain claim-specific representations of each arti-\ncle term:\n\nay, = £(Wady + ba)\n\nwhere W, and ba are the corresponding weight\nmatrix and bias terms, and f is an activation func-\ntion?, such as ReLU, tanh, or the identity func-\ntion. Following this, we use a softmax activation\nto calculate an attention score a; for each word\nin the article capturing its relevance to the claim\ncontext:\n\nexp(aj,)\n\n~ Y, exp(aj,)\n2.4 Per-Article Credibility Score of Claim\n\nNow that we have article term representations\ngiven by (h,) and their relevance to the claim\ngiven by (ax), we need to combine them to pre-\ndict the claim’s credibility. In order to create an\n\nqd)\n\nQk\n\n3%n our model, the tanh activation function gives best re-\nsults.\n", "vlm_text": "The image is a diagram depicting a machine learning model architecture for determining the credibility score of a claim and an article. It involves several components:\n\n1. **Claim and Article Word Embeddings**: Inputs representing the claim and article texts, which are processed to create embeddings (vector representations).\n\n2. **Bi-directional LSTM**: A layer that processes the concatenated word embeddings of claims and articles to capture context and dependencies in both directions.\n\n3. **Claim Source and Article Source Embedding**: These vectors represent the sources of the claim and the article.\n\n4. **Attention Mechanism**: Utilizes attention weights to focus on important parts of the processed embeddings, creating weighted representations.\n\n5. **Concatenation and Dense Layers**: Combines various feature vectors and processes them through dense (fully connected) layers.\n\n6. **Output - Credibility Score**: The result of the model, which assigns a credibility score to the input claim and article.\n\nOverall, the diagram illustrates a complex neural network designed to analyze and evaluate the credibility of text sources using advanced sequence and embedding techniques.\nFigure 1 : Framework for credibility assessment. Upper part of the pipeline combines the article and claim embeddings to get the claim speciﬁc attention weights. Lower part of the pipeline captures the article representation through biLSTM. Attention focused article representation along with the source embeddings are passed through dense layers to predict the credibility score of the claim. \nWe use bidirectional LSTMs in place of stan- dard LSTMs. Bidirectional LSTMs capture both the previous timesteps (past features) and the fu- ture timesteps (future features) via forward and backward states respectively. Correspondingly, there are two hidden states that capture past and future information that are concatenated to form the ﬁnal output as:    $h_{k}=[\\overrightarrow{h_{k}},\\overleftarrow{h_{k}}]$    . \n2.3Claim Speciﬁc Attention\nAs we previously discussed, it is important to con- sider the relevance of an article with respect to the claim; speciﬁcally, focusing or  attending  to parts of the article that discuss the claim. This is in con- trast to prior works ( Popat et al. ,  2017 ;  Rashkin et al. ,  2017 ;  Wang ,  2017 ) that ignore either the ar- ticle or the claim, and therefore miss out on this important interaction. \nWe propose an attention mechanism to help our model focus on salient words in the article with respect to the claim. To this end, we compute the importance of each term in an article with respect to an overall representation of the corre- sponding claim. Additionally, incorporating atten- tion helps in making our model transparent and in- terpretable, because it provides a way to generate the most salient words in an article as evidence of our model’s verdict. \nFollowing  Wieting et al.  ( 2015 ), the overall rep- resentation of an input claim is generated by tak- ing an average of the word embeddings of all the words therein: \n\n\n$$\n\\bar{c}=\\frac{1}{l}\\sum_{l}c_{l}\n$$\n \nWe combine this overall representation of the claim with each article term: \n\n$$\n\\hat{a}_{k}=a_{k}\\oplus\\bar{c}\n$$\n \nwhere,    $\\hat{a}_{k}\\in\\Re^{d+d}$   ∈ℜ   and    $\\bigoplus$  denotes the concatenate operation. We then perform a transformation to obtain claim-speciﬁc representations of each arti- cle term: \n\n$$\na_{k}^{\\prime}={\\bf f}\\left(W_{a}\\hat{a}_{k}+b_{a}\\right)\n$$\n \nwhere    $W_{a}$   and    $b_{a}$   are the corresponding weight matrix and bias terms, and    $\\mathbf{f}$  is an activation func-  $\\mathrm{tan}^{3}$  , such as  ReLU ,  tanh , or the identity func- tion. Following this, we use a softmax activation to calculate an attention score    $\\alpha_{k}$   for each word in the article capturing its relevance to the claim context: \n\n$$\n\\alpha_{k}=\\frac{\\exp(a_{k}^{\\prime})}{\\sum_{k}\\exp(a_{k}^{\\prime})}\n$$\n \n2.4 Per-Article Credibility Score of Claim \nNow that we have article term representations given by    $\\langle h_{k}\\rangle$  and their relevance to the claim given by    $\\left\\langle\\alpha_{k}\\right\\rangle$  , we need to combine them to pre- dict the claim’s credibility. In order to create an 3 In our model, the  tanh  activation function gives best re- sults. "}
{"page": 3, "image_path": "doc_images/D18-1003_3.jpg", "ocr_text": "attention-focused representation of the article con-\nsidering both the claim and the article’s language,\nwe calculate a weighted average of the hidden\nstate representations for all article tokens based on\ntheir corresponding attention scores:\n\n1\na\n\nWe then combine all the different feature repre-\nsentations: the claim source embedding (cs), the\nattention-focused article representation (g), and\nthe article source embedding (as). In order to\nmerge the different representations and capture\ntheir joint interactions, we process them with two\nfully connected layers with non-linear activations.\n\n(2)\n\nd, = relu(W-(g @ cs @ as\ndz = relu(Wad1 + ba)\n\n+ be)\n\nwhere, W and 0 are the corresponding weight ma-\ntrix and bias terms.\n\nFinally, to generate the overall credibility label\nof the article for classification tasks, or credibil-\nity score for regression tasks, we process the final\nrepresentation with a final fully connected layer:\n\n(3)\n(4)\n\nClassification: s = sigmoid(d2)\n\nRegression: s = linear(d2)\n\n2.5 Credibility Aggregation\n\nThe credibility score in the above step is obtained\nconsidering a single reporting article. As previ-\nously discussed, we have M reporting articles per\nclaim. Therefore, once we have the per-article\ncredibility scores from our model, we take an av-\nerage of these scores to generate the overall credi-\nbility score for the claim:\n\ncred(C) = a > Sm (3)\n\nThis aggregation is done after the model is\ntrained.\n\n3 Datasets\n\nWe evaluate our approach and demonstrate its gen-\nerality by performing experiments on four differ-\nent datasets: a general fact-checking website, a po-\nlitical fact-checking website, a news review com-\nmunity, and a SemEval Twitter rumour dataset.\n\n25\n\n3.1 Snopes\n\nSnopes (www.snopes.com) is a general fact-\nchecking website where editors manually investi-\ngate various kinds of rumors reported on the In-\nternet. We used the Snopes dataset provided by\nPopat et al. (2017). This dataset consists of ru-\nmors analyzed on the Snopes website along with\ntheir credibility labels (true or false), sets of re-\nporting articles, and their respective web sources.\n\n3.2 PolitiFact\n\nPolitiFact is a political fact-checking website\n(www.politifact.com) in which editors rate\nthe credibility of claims made by various politi-\ncal figures in US politics. We extract all articles\nfrom PolitiFact published before December 2017.\nEach article includes a claim, the speaker (polit-\nical figure) who made the claim, and the claim’s\ncredibility rating provided by the editors.\n\nPolitiFact assigns each claim to one of six pos-\nsible ratings: true, mostly true, half true, mostly\nfalse, false and pants-on-fire. Following Rashkin\net al. (2017), we combine true, mostly true and\nhalf true ratings into the class label true and the\nrest as false — hence considering only binary cred-\nibility labels. To retrieve the reporting articles for\neach claim (similar to Popat et al. (2017)), we is-\nsue each claim as a query to a search engine* and\nretrieve the top 30 search results with their respec-\ntive web sources.\n\n3.3. NewsTrust\n\nNewsTrust is a news review community in which\nmembers review the credibility of news articles.\nWe use the NewsTrust dataset made available by\nMukherjee and Weikum (2015). This dataset con-\ntains NewsTrust stories from May 2006 to May\n2014. Each story consists of a news article along\nwith its source, and a set of reviews and ratings by\ncommunity members. NewsTrust aggregates these\nratings and assigns an overall credibility score (on\na scale of 1 to 5) to the posted article. We map the\nattributes in this data to the inputs expected by De-\nClarE as follows: the title and the web source of\nthe posted (news) article are mapped to the input\nclaim and claim source, respectively. Reviews and\ntheir corresponding user identities are mapped to\nreporting articles and article sources, respectively.\nWe use this dataset for the regression task of pre-\ndicting the credibility score of the posted article.\n\n4We use the Bing search API.\n", "vlm_text": "attention-focused representation of the article con- sidering both the claim and the article’s language, we calculate a weighted average of the hidden state representations for all article tokens based on their corresponding attention scores: \n\n$$\ng=\\frac{1}{k}\\sum_{k}\\alpha_{k}\\cdot h_{k}\n$$\n \nWe then combine all the different feature repre- sentations: the claim source embedding    $(c s)$  , the attention-focused article representation   $(g)$  , and the article source embedding   $(a s)$  . In order to merge the different representations and capture their joint interactions, we process them with two fully connected layers with non-linear activations. \n\n$$\n\\begin{array}{r l}&{d_{1}=r e l u(W_{c}(g\\oplus c s\\oplus a s)+b_{c})}\\\\ &{d_{2}=r e l u(W_{d}d_{1}+b_{d})}\\end{array}\n$$\n \nwhere,    $W$   and  $b$   are the corresponding weight ma- trix and bias terms. \nFinally, to generate the overall credibility label of the article for classiﬁcation tasks, or credibil- ity score for regression tasks, we process the ﬁnal representation with a ﬁnal fully connected layer: \n\n$$\n\\begin{array}{c}{{\\mathrm{Classiffraction:}\\,\\,s=s i g m o i d(d_{2})}}\\\\ {{\\mathrm{Regression:}\\,\\,s=l i n e a r(d_{2})}}\\end{array}\n$$\n \n2.5 Credibility Aggregation \nThe credibility score in the above step is obtained considering a single reporting article. As previ- ously discussed, we have  $M$   reporting articles per claim. Therefore, once we have the per-article credibility scores from our model, we take an av- erage of these scores to generate the overall credi- bility score for the claim: \n\n$$\nc r e d(C)=\\frac{1}{M}\\sum_{m}s_{m}\n$$\n \nThis aggregation is done after the model is trained. \n3 Datasets \nWe evaluate our approach and demonstrate its gen- erality by performing experiments on four differ- ent datasets: a general fact-checking website, a po- litical fact-checking website, a news review com- munity, and a SemEval Twitter rumour dataset. \n3.1 Snopes \nSnopes ( www.snopes.com ) is a general fact- checking website where editors manually investi- gate various kinds of rumors reported on the In- ternet. We used the Snopes dataset provided by Popat et al.  ( 2017 ). This dataset consists of ru- mors analyzed on the Snopes website along with their credibility labels ( true  or  false ), sets of re- porting articles, and their respective web sources. \n3.2 PolitiFact \nPolitiFact is a political fact-checking website ( www.politifact.com ) in which editors rate the credibility of claims made by various politi- cal ﬁgures in US politics. We extract all articles from PolitiFact published before December 2017. Each article includes a claim, the speaker (polit- ical ﬁgure) who made the claim, and the claim’s credibility rating provided by the editors. \nPolitiFact assigns each claim to one of six pos- sible ratings:  true, mostly true, half true, mostly false, false  and  pants-on-ﬁre . Following  Rashkin et al.  ( 2017 ), we combine  true, mostly true  and half true  ratings into the class label  true  and the rest as  false  – hence considering only binary cred- ibility labels. To retrieve the reporting articles for each claim (similar to  Popat et al.  ( 2017 )), we is- sue each claim as a query to a search engine 4   and retrieve the top 30 search results with their respec- tive web sources. \n3.3 NewsTrust \nNewsTrust is a news review community in which members review the credibility of news articles. We use the NewsTrust dataset made available by Mukherjee and Weikum  ( 2015 ). This dataset con- tains NewsTrust stories from May 2006 to May 2014. Each story consists of a news article along with its source, and a set of reviews and ratings by community members. NewsTrust aggregates these ratings and assigns an overall credibility score (on a scale of 1 to 5) to the posted article. We map the attributes in this data to the inputs expected by De- ClarE as follows: the title and the web source of the posted (news) article are mapped to the input claim and claim source, respectively. Reviews and their corresponding user identities are mapped to reporting articles and article sources, respectively. We use this dataset for the regression task of pre- dicting the credibility score of the posted article. "}
{"page": 4, "image_path": "doc_images/D18-1003_4.jpg", "ocr_text": "Dataset SN PF NT SE Parameter SN PF NT SE\n\nTotal claims 4341 3568 5344 272 Word embedding length 100 100 300 100\nTrue claims 1164 1867 - 127 Claim source embedding length - 4 8 4\nFalse claims 3177 1701 - 50 Article source embedding length 8 4 8 4\nUnverified claims - - - 95 LSTM size (for each pass) 644 64 «64° «+16\n\nClaim sources . 95 161 10 Size of fully connected layers 32, 32—Sss 64 8\n\nDropout 05 #05 #03 0.3\nArticles 29242 29556 25128 3717\nArticle sources 336 336 251 89\n\nTable 1: Data statistics (SN: Snopes, PF: Politi-\nFact, NT: NewsTrust, SE: SemEval).\n\n3.4 SemEval-2017 Task 8\n\nAs the fourth dataset, we consider the benchmark\ndataset released by SemEval-2017 for the task of\ndetermining credibility and stance of social media\ncontent (Twitter) (Derczynski et al., 2017). The\nobjective of this task is to predict the credibility\nof a questionable tweet (true, false or unverified)\nalong with a confidence score from the model. It\nhas two sub-tasks: (i) a closed variant in which\nmodels only consider the questionable tweet, and\n(ii) an open variant in which models consider both\nthe questionable tweet and additional context con-\nsisting of snapshots of relevant sources retrieved\nimmediately before the rumor was reported, a\nsnapshot of an associated Wikipedia article, news\narticles from digital news outlets, and preceding\ntweets about the same event. Testing and devel-\nopment datasets provided by organizers have 28\ntweets (1021 reply tweets) and 25 tweets (256 re-\nply tweets), respectively.\n\n3.5 Data Processing\n\nIn order to have a minimum support for training,\nclaim sources with less than 5 claims in the dataset\nare grouped into a single dummy claim source,\nand article sources with less than 10 articles are\ngrouped similarly (5 articles for SemEval as it is a\nsmaller dataset).\n\nFor Snopes and PolitiFact, we need to extract\nrelevant snippets from the reporting articles for\na claim. Therefore, we extract snippets of 100\nwords from each reporting article having the maxi-\nmum relevance score: sim = simpow X Si7Msemantic\nwhere simpow is the fraction of claim words that\nare present in the snippet, and sirmsemantic Tepre-\nsents the cosine similarity between the average\nof claim word embeddings and snippet word em-\nbeddings. We also enforce a constraint that the\nsim score is at least 6. We varied 6 from 0.2\nto 0.8 and found 0.5 to give the optimal perfor-\n\n26\n\nTable 2: Model parameters used for each dataset\n(SN: Snopes, PF: PolitiFact, NT: NewsTrust, SE:\nSemEval).\n\nmance on a withheld dataset. We discard all arti-\ncles related to Snopes and PolitiFact websites from\nour datasets to have an unbiased model. Statis-\ntics of the datasets after pre-processing is pro-\nvided in Table 1. All the datasets are made pub-\nlicly available at https: //www.mpi-inf.\nmpg.de/dl-cred-analysis/.\n\n4 Experiments\n\nWe evaluate our approach by conducting experi-\nments on four datasets, as described in the previ-\nous section. We describe our experimental setup\nand report our results in the following sections.\n\n4.1 Experimental Setup\n\nWhen using the Snopes, PolitiFact and NewsTrust\ndatasets, we reserve 10% of the data as valida-\ntion data for parameter tuning. We report 10-fold\ncross validation results on the remaining 90% of\nthe data; the model is trained on 9-folds and the\nremaining fold is used as test data. When us-\ning the SemEval dataset, we use the data splits\nprovided by the task’s organizers. The objective\nfor Snopes, PolitiFact and SemEval experiments is\nbinary (credibility) classification, while for New-\nsTrust the objective is to predict the credibility\nscore of the input claim on a scale of | to 5 (ie.,\ncredibility regression). We represent terms us-\ning pre-trained Glo Ve Wikipedia 6B word embed-\ndings (Pennington et al., 2014). Since our train-\ning datasets are not very large, we do not tune the\nword embeddings during training. The remaining\nmodel parameters are tuned on the validation data;\nthe parameters chosen are reported in Table 2. We\nuse Keras with a Tensorflow backend to imple-\nment our system. All the models are trained using\nAdam optimizer (Kingma and Ba, 2014) (learn-\ning rate: 0.002) with categorical cross-entropy loss\nfor classification and mean squared error loss for\nregression task. We use L2-regularizers with the\n", "vlm_text": "The table provides data regarding different datasets labeled as SN, PF, NT, and SE. The data is categorized into the following:\n\n1. **Total claims**: \n   - SN: 4341\n   - PF: 3568\n   - NT: 5344\n   - SE: 272\n\n2. **True claims**:\n   - SN: 1164\n   - PF: 1867\n   - NT: Not provided\n   - SE: 127\n\n3. **False claims**:\n   - SN: 3177\n   - PF: 1701\n   - NT: Not provided\n   - SE: 50\n\n4. **Unverified claims**:\n   - SN: Not provided\n   - PF: Not provided\n   - NT: Not provided\n   - SE: 95\n\n5. **Claim sources**:\n   - SN: Not provided\n   - PF: 95\n   - NT: 161\n   - SE: 10\n\n6. **Articles**:\n   - SN: 29242\n   - PF: 29556\n   - NT: 25128\n   - SE: 3717\n\n7. **Article sources**:\n   - SN: 336\n   - PF: 336\n   - NT: 251\n   - SE: 89\n\nThe data illustrates the composition and sources of several datasets in terms of claims and articles, distinguishing between true, false, and unverified claims where available.\n3.4 SemEval-2017 Task 8 \nAs the fourth dataset, we consider the benchmark dataset released by SemEval-2017 for the task of determining credibility and stance of social media content (Twitter) ( Derczynski et al. ,  2017 ). The objective of this task is to predict the credibility of a questionable tweet ( true ,  false  or  unveriﬁed ) along with a conﬁdence score from the model. It has two sub-tasks: (i) a  closed  variant in which models only consider the questionable tweet, and (ii) an  open  variant in which models consider both the questionable tweet and additional context con- sisting of snapshots of relevant sources retrieved immediately before the rumor was reported, a snapshot of an associated Wikipedia article, news articles from digital news outlets, and preceding tweets about the same event. Testing and devel- opment datasets provided by organizers have 28 tweets (1021 reply tweets) and 25 tweets (256 re- ply tweets), respectively. \n3.5 Data Processing \nIn order to have a minimum support for training, claim sources with less than 5 claims in the dataset are grouped into a single dummy claim source, and article sources with less than 10 articles are grouped similarly (5 articles for SemEval as it is a smaller dataset). \nFor Snopes and PolitiFact, we need to extract relevant snippets from the reporting articles for a claim. Therefore, we extract snippets of 100 words from each reporting article having the maxi- mum relevance score:    $s i m=s i m_{\\mathrm{bew}}\\!\\times\\!s i m_{\\mathrm{s}}$  semantic where    $s i m_{\\mathrm{low}}$   is the fraction of claim words that are present in the snippet, and  sim semantic  repre- sents the cosine similarity between the average of claim word embeddings and snippet word em- beddings. We also enforce a constraint that the sim  score is at least    $\\delta$  . We varied    $\\delta$   from 0.2 to 0.8 and found 0.5 to give the optimal perfor- \nThe table displays various parameters and their values for four different configurations labeled as SN, PF, NT, and SE. Here is a summary of the parameters and their respective values:\n\n1. **Word embedding length**:\n   - SN: 100\n   - PF: 100\n   - NT: 300\n   - SE: 100\n\n2. **Claim source embedding length**:\n   - SN: -\n   - PF: 4\n   - NT: 8\n   - SE: 4\n\n3. **Article source embedding length**:\n   - SN: 8\n   - PF: 4\n   - NT: 8\n   - SE: 4\n\n4. **LSTM size (for each pass)**:\n   - SN: 64\n   - PF: 64\n   - NT: 64\n   - SE: 16\n\n5. **Size of fully connected layers**:\n   - SN: 32\n   - PF: 32\n   - NT: 64\n   - SE: 8\n\n6. **Dropout**:\n   - SN: 0.5\n   - PF: 0.5\n   - NT: 0.3\n   - SE: 0.3 \n\nThese values likely correspond to different neural network models or configurations used in a machine learning experiment or study.\nmance on a withheld dataset. We discard all arti- cles related to Snopes and PolitiFact websites from our datasets to have an unbiased model. Statis- tics of the datasets after pre-processing is pro- vided in Table  1 . All the datasets are made pub- licly available at  https://www.mpi-inf. mpg.de/dl-cred-analysis/ . \n4 Experiments \nWe evaluate our approach by conducting experi- ments on four datasets, as described in the previ- ous section. We describe our experimental setup and report our results in the following sections. \n4.1 Experimental Setup \nWhen using the Snopes, PolitiFact and NewsTrust datasets, we reserve   $10\\%$   of the data as valida- tion data for parameter tuning. We report 10-fold cross validation results on the remaining   $90\\%$   of the data; the model is trained on 9-folds and the remaining fold is used as test data. When us- ing the SemEval dataset, we use the data splits provided by the task’s organizers. The objective for Snopes, PolitiFact and SemEval experiments is binary (credibility) classiﬁcation, while for New- sTrust the objective is to predict the credibility score of the input claim on a scale of 1 to 5 (i.e., credibility regression). We represent terms us- ing pre-trained GloVe Wikipedia 6B word embed- dings ( Pennington et al. ,  2014 ). Since our train- ing datasets are not very large, we do not tune the word embeddings during training. The remaining model parameters are tuned on the validation data; the parameters chosen are reported in Table  2 . We use Keras with a Tensorﬂow backend to imple- ment our system. All the models are trained using Adam optimizer ( Kingma and Ba ,  2014 ) (learn- ing rate: 0.002) with categorical cross-entropy loss for classiﬁcation and mean squared error loss for regression task. We use L2-regularizers with the "}
{"page": 5, "image_path": "doc_images/D18-1003_5.jpg", "ocr_text": "True Clai\n\nims False Claims Macro\n\nDataset Configuration Accuracy (%) Accuracy (%) F1-Score AUC\nLSTM-text 64.65 64.21 0.66 0.70\nCNN-text 67.15 63.14 0.66 0.72\nDistant Supervision 83.21 80.78 0.82 0.88\n\nSnopes r\nDeClarE (Plain) 74.37 78.57 0.78 0.83\nDeClarE (Plain+Attn) 78.34 78.91 0.79 0.85\nDeClarE (Plain+SrEmb) 7743 79.80 0.79 0.85\nDeClarE (Full) 78.96 78.32 0.79 0.86\nLSTM-text 63.19 61.96 0.63 0.66\nCNN-text 63.67 63.31 0.64 0.67\nDistant Supervision 62.53 62.08 0.62 0.68\n\nPolitiFact DeClarE (Plain) 62.67 69.05 0.66 0.70\nDeClarE (Plain+Attn) 65.53 68.49 0.66 0.72\nDeClarE (Plain+SrEmb) 66.71 69.28 0.67 0.74\nDeClarE (Full) 67.32 69.62 0.68 0.75\n\nTable 3: Comparison of various approaches for credibility classification on Snopes and PolitiFact datasets.\n\nfully connected layers as well as dropout. For all\nthe datasets, the model is trained using each claim-\narticle pair as a separate training instance.\n\nTo evaluate and compare the performance of\nDeClarE with other state-of-the-art methods, we\nreport the following measures:\n\ne Credibility Classification (Snopes, PolitiFact\nand SemEval): accuracy of the models in clas-\nsifying true and false claims separately, macro\nFl-score and Area-Under-Curve (AUC) for\nthe ROC (Receiver Operating Characteristic)\ncurve.\n\nCredibility Regression (NewsTrust): Mean\nSquare Error (MSE) between the predicted and\ntrue credibility scores.\n\n4.2 Results: Snopes and Politifact\n\nWe compare our approach with the following\nstate-of-the-art models: (i) LSTM-text, a recent\napproach proposed by Rashkin et al. (2017). (i)\nCNN-text: a CNN based approach proposed by\nWang (2017). (ii) Distant Supervision:  state-\nof-the-art distant supervision based approach pro-\nposed by Popat et al. (2017). (iv) DeClare\n(Plain): our approach with only biLSTM (no at-\ntention and source embeddings). (v) DeClarE\n(Plain+Attn): our approach with only biLSTM\nand attention (no source embeddings). (vi) De-\nClarE (Plain+SrEmb): our approach with only\nbiLSTM and source embeddings (no attention).\n(vii) DeClarE (Full): end-to-end system with biL-\nSTM, attention and source embeddings.\n\nThe results when performing credibility classi-\nfication on the Snopes and PolitiFact datasets are\n\n27\n\nshown in Table 3. DeClarE outperforms LSTM-\ntext and CNN-text models by a large margin on\nboth datasets. On the other hand, for the Snopes\ndataset, performance of DeClarE (Full) is slightly\nlower than the Distant Supervision configuration\n(p-value of 0.04 with a pairwise t-test). How-\never, the advantage of DeClarE over Distant Su-\npervision approach is that it does not rely on hand\ncrafted features and lexicons, and can generalize\nwell to arbitrary domains without requiring any\nseed vocabulary. It is also to be noted that both of\nthese approaches use external evidence in the form\nof reporting articles discussing the claim, which\nare not available to the LSTM-text and CNN-text\nbaselines. This demonstrates the value of external\nevidence for credibility assessment.\n\nOn the PolitiFact dataset, DeClarE outperforms\nall the baseline models by a margin of 7-9%\nAUC (p-value of 9.12e—05 with a pairwise t-test)\nwith similar improvements in terms of Macro F1.\nA performance comparison of DeClarE’s various\nconfigurations indicates the contribution of each\ncomponent of our model, i.e, biLSTM capturing\narticle representations, attention mechanism and\nsource embeddings. The additions of both the\nattention mechanism and source embeddings im-\nprove performance over the plain configuration in\nall cases when measured by Macro F1 or AUC.\n\n4.3 Results: NewsTrust\n\nWhen performing credibility regression on the\nNewsTrust dataset, we evaluate the models in\nterms of mean squared error (MSE; lower is bet-\nter) for credibility rating prediction. We use the\n", "vlm_text": "The table presents the performance of different configurations of models on two datasets, Snopes and PolitiFact. It includes measurements of accuracy for true and false claims, macro F1-score, and AUC (Area Under the Curve). Here's a breakdown:\n\n**Snopes Dataset:**\n\n1. **LSTM-text:**\n   - True Claims Accuracy: 64.65%\n   - False Claims Accuracy: 64.21%\n   - Macro F1-Score: 0.66\n   - AUC: 0.70\n\n2. **CNN-text:**\n   - True Claims Accuracy: 67.15%\n   - False Claims Accuracy: 63.14%\n   - Macro F1-Score: 0.66\n   - AUC: 0.72\n\n3. **Distant Supervision:**\n   - True Claims Accuracy: 83.21%\n   - False Claims Accuracy: 80.78%\n   - Macro F1-Score: 0.82\n   - AUC: 0.88\n\n4. **DeClarE Variants:**\n   - Plain:\n     - True Claims Accuracy: 74.37%\n     - False Claims Accuracy: 78.57%\n     - Macro F1-Score: 0.78\n     - AUC: 0.83\n   - Plain+Attn:\n     - True Claims Accuracy: 78.34%\n     - False Claims Accuracy: 78.91%\n     - Macro F1-Score: 0.79\n     - AUC: 0.85\n   - Plain+SrEmb:\n     - True Claims Accuracy: 77.43%\n     - False Claims Accuracy: 79.80%\n     - Macro F1-Score: 0.79\n     - AUC: 0.85\n   - Full:\n     - True Claims Accuracy: 78.96%\n     - False Claims Accuracy: 78.32%\n     - Macro F1-Score: 0.79\n     - AUC: 0.86\n\n**PolitiFact Dataset:**\n\n1. **LSTM-text:**\n   - True Claims Accuracy: 63.19%\n   - False Claims Accuracy: 61.96%\n   - Macro F1-Score: 0.63\n   - AUC: 0.66\n\n2. **CNN-text:**\n   - True Claims Accuracy: 63.67%\n   - False Claims Accuracy: 63.31%\n   - Macro F1-Score: 0.64\n   - AUC: 0.67\n\n3. **Distant Supervision:**\n   - True Claims Accuracy: 62.53%\n   - False Claims Accuracy: 62.08%\n   - Macro F1-Score: 0.62\n   - AUC: 0.68\n\n4. **DeClarE Vari\nfully connected layers as well as dropout. For all the datasets, the model is trained using each claim- article pair as a separate training instance. \nTo evaluate and compare the performance of DeClarE with other state-of-the-art methods, we report the following measures: \n•  Credibility Classiﬁcation (Snopes, PolitiFact and SemEval): accuracy of the models in clas- sifying  true  and  false  claims separately, macro F1-score and Area-Under-Curve (AUC) for the ROC (Receiver Operating Characteristic) curve. •  Credibility Regression (NewsTrust): Mean Square Error (MSE) between the predicted and true credibility scores. \n4.2 Results: Snopes and Politifact \nWe compare our approach with the following state-of-the-art models: (i) LSTM-text, a recent approach proposed by  Rashkin et al.  ( 2017 ). (ii) CNN-text: a CNN based approach proposed by Wang  ( 2017 ). (iii) Distant Supervision: state- of-the-art distant supervision based approach pro- posed by  Popat et al.  ( 2017 ). (iv) DeClare (Plain): our approach with only biLSTM (no at- tention and source embeddings). (v) DeClarE (Plain+Attn): our approach with only biLSTM and attention (no source embeddings). (vi) De- ClarE (  $_\\mathrm{|diamond+SrEmb]}$  ): our approach with only biLSTM and source embeddings (no attention). (vii) DeClarE (Full): end-to-end system with biL- STM, attention and source embeddings. \nThe results when performing credibility classi- ﬁcation on the Snopes and PolitiFact datasets are shown in Table  3 . DeClarE outperforms LSTM- text and CNN-text models by a large margin on both datasets. On the other hand, for the Snopes dataset, performance of DeClarE (Full) is slightly lower than the Distant Supervision conﬁguration (p-value of 0.04 with a pairwise t-test). How- ever, the advantage of DeClarE over Distant Su- pervision approach is that it does not rely on hand crafted features and lexicons, and can generalize well to arbitrary domains without requiring any seed vocabulary. It is also to be noted that both of these approaches use external evidence in the form of reporting articles discussing the claim, which are not available to the LSTM-text and CNN-text baselines. This demonstrates the value of external evidence for credibility assessment. \n\nOn the PolitiFact dataset, DeClarE outperforms all the baseline models by a margin of   $7.9\\%$  AUC (p-value of    $9.12\\mathrm{e}{-05}$   with a pairwise t-test) with similar improvements in terms of Macro F1. A performance comparison of DeClarE’s various conﬁgurations indicates the contribution of each component of our model, i.e, biLSTM capturing article representations, attention mechanism and source embeddings. The additions of both the attention mechanism and source embeddings im- prove performance over the plain conﬁguration in all cases when measured by Macro F1 or AUC. \n4.3 Results: NewsTrust \nWhen performing credibility regression on the NewsTrust dataset, we evaluate the models in terms of mean squared error (MSE; lower is bet- ter) for credibility rating prediction. We use the "}
{"page": 6, "image_path": "doc_images/D18-1003_6.jpg", "ocr_text": "Configuration MSE\nCNN-text 0.53\nCCRF+SVR 0.36\nLSTM-text 0.35\nDistantSup 0.35\nDeClarE (Plain) 0.34\nDeClarE (Full) 0.29\n\nTable 4: Comparison of various approaches for\ncredibility regression on NewsTrust dataset.\n\nfirst three models described in Section 4.2 as base-\nlines. For CNN-text and LSTM-text, we add a lin-\near fully connected layer as the final layer of the\nmodel to support regression. Additionally, we also\nconsider the state-of-the-art CCRF+SVR model\nbased on Continuous Conditional Random Field\n(CCRF) and Support Vector Regression (SVR)\nproposed by Mukherjee and Weikum (2015). The\nresults are shown in Table 4. We observe that De-\nClarE (Full) outperforms all four baselines, with\na 17% decrease in MSE compared to the best-\nperforming baselines (i.e., LSTM-text and Dis-\ntant Supervision). The DeClarE (Plain) model\nperforms substantially worse than the full model,\nillustrating the value of including attention and\nsource embeddings. CNN-text performs substan-\ntially worse than the other baselines.\n\n4.4 Results: SemEval\n\nOn the SemEval dataset, the objective is to per-\nform credibility classification of a tweet while also\nproducing a classification confidence score. We\ncompare the following approaches and consider\nboth variants of the SemEval task: (i) NileTMRG\n(Enayet and El-Beltagy, 2017): the best perform-\ning approach for the close variant of the task, (ii)\nITP (Singh et al., 2017): the best performing ap-\nproach for the open variant of the task, (iii) De-\nClare (Plain): our approach with only biLSTM\n(no attention and source embeddings), and (iv)\nDeClarE (Full): our end-to-end system with biL-\nSTM, attention and source embeddings.\n\nWe use the evaluation measure proposed by the\ntask’s organizers: macro F1-score for overall clas-\nsification and Root-Mean-Square Error (RMSE)\nover confidence scores. Results are shown in Ta-\nble 5. We observe that DeClarE (Full) outperforms\nall the other approaches — thereby, re-affirming\nits power in harnessing external evidence.\n\n28\n\nMacro\n\nConfiguration RMSE\nAccuracy\n\nIITP (Open) 0.39 0.746\n\nNileTMRG (Close) 0.54 0.673\n\nDeClarE (Plain) 0.46 0.687\n\nDeClarE (Full) 0.57 0.604\n\nTable 5: Comparison of various approaches for\ncredibility classification on SemEval dataset.\n\n5 Discussion\n\n5.1 Analyzing Article Representations\n\nIn order to assess how our model separates articles\nreporting false claims from those reporting true\nones, we employ dimensionality reduction using\nPrincipal Component Analysis (PCA) to project\nthe article representations (g in Equation 2) from\na high dimensional space to a 2d plane. The pro-\njections are shown in Figure 2a. We observe that\nDeClarE obtains clear separability between credi-\nble versus non-credible articles in Snopes dataset.\n\n5.2. Analyzing Source Embeddings\n\nSimilar to the treatment of article representations,\nwe perform an analysis with the claim and arti-\ncle source embeddings by employing PCA and\nplotting the projections. We sample a few popu-\nlar news sources from Snopes and claim sources\nfrom PolitiFact. These news sources and claim\nsources are displayed in Figure 2b and Figure 2c,\nrespectively. From Figure 2b we observe that\nDeClarE clearly separates fake news sources like\nnationalreport, empirenews, huzlers, etc. from\nmainstream news sources like nytimes, cnn, wsj,\nfoxnews, washingtonpost, etc. Similarly, from Fig-\nure 2c we observe that DeClarE locates politicians\nwith similar ideologies and opinions close to each\nother in the embedding space.\n\n5.3. Analyzing Attention Weights\n\nAttention weights help understand what DeClarE\nfocuses on during learning and how it affects its\ndecisions — thereby, making our model transparent\nto the end-users. Table 6 illustrates some interest-\ning claims and salient words (highlighted) that De-\nClarE focused on during learning. Darker shades\nindicate higher weights given to the corresponding\nwords. As illustrated in the table, DeClarE gives\nmore attention to important words in the report-\ning article that are relevant to the claim and also\n", "vlm_text": "The table presents a comparison of different model configurations and their Mean Squared Error (MSE) values. The configurations listed are CNN-text, CCRF+SVR, LSTM-text, DistantSup, DeClarE (Plain), and DeClarE (Full). The respective MSE values for these configurations are 0.53, 0.36, 0.35, 0.35, 0.34, and 0.29. The DeClarE (Full) configuration has the lowest MSE value (0.29), suggesting it performs the best among the listed configurations in terms of minimizing the error.\nTable 4 : Comparison of various approaches for credibility regression on NewsTrust dataset. \nﬁrst three models described in Section  4.2  as base- lines. For CNN-text and LSTM-text, we add a lin- ear fully connected layer as the ﬁnal layer of the model to support regression. Additionally, we also consider the state-of-the-art   $\\mathrm{CCRF+SWR}$   model based on Continuous Conditional Random Field (CCRF) and Support Vector Regression (SVR) proposed by  Mukherjee and Weikum  ( 2015 ). The results are shown in Table  4 . We observe that De- ClarE (Full) outperforms all four baselines, with a   $17\\%$   decrease in MSE compared to the best- performing baselines (i.e., LSTM-text and Dis- tant Supervision). The DeClarE (Plain) model performs substantially worse than the full model, illustrating the value of including attention and source embeddings. CNN-text performs substan- tially worse than the other baselines. \n4.4 Results: SemEval \nOn the SemEval dataset, the objective is to per- form credibility classiﬁcation of a tweet while also producing a classiﬁcation conﬁdence score. We compare the following approaches and consider both variants of the SemEval task: (i)  NileTMRG ( Enayet and El-Beltagy ,  2017 ): the best perform- ing approach for the  close  variant of the task, (ii) IITP  ( Singh et al. ,  2017 ): the best performing ap- proach for the  open  variant of the task, (iii) De- Clare (Plain): our approach with only biLSTM (no attention and source embeddings), and (iv) DeClarE (Full): our end-to-end system with biL- STM, attention and source embeddings. \nWe use the evaluation measure proposed by the task’s organizers: macro F1-score for overall clas- siﬁcation and Root-Mean-Square Error (RMSE) over conﬁdence scores. Results are shown in Ta- ble  5 . We observe that DeClarE (Full) outperforms all the other approaches — thereby, re-afﬁrming its power in harnessing external evidence. \nThe table compares different configurations based on their Macro Accuracy and RMSE (Root Mean Square Error). \n\n- **IITP (Open)**: Macro Accuracy is 0.39, RMSE is 0.746\n- **NileTMRG (Close)**: Macro Accuracy is 0.54, RMSE is 0.673\n- **DeClarE (Plain)**: Macro Accuracy is 0.46, RMSE is 0.687\n- **DeClarE (Full)**: Macro Accuracy is 0.57, RMSE is 0.604\n\nThe bold values indicate the best performance for each metric. DeClarE (Full) has the highest Macro Accuracy and the lowest RMSE.\n5 Discussion \n5.1 Analyzing Article Representations \nIn order to assess how our model separates articles reporting false claims from those reporting true ones, we employ dimensionality reduction using Principal Component Analysis (PCA) to project the article representations   $\\acute{g}$   in Equation  2 ) from a high dimensional space to a 2d plane. The pro- jections are shown in Figure  2a . We observe that DeClarE obtains clear separability between credi- ble versus non-credible articles in Snopes dataset. \n5.2 Analyzing Source Embeddings \nSimilar to the treatment of article representations, we perform an analysis with the claim and arti- cle source embeddings by employing PCA and plotting the projections. We sample a few popu- lar news sources from Snopes and claim sources from PolitiFact. These news sources and claim sources are displayed in Figure  2b  and Figure  2c , respectively. From Figure  2b  we observe that DeClarE clearly separates fake news sources like nationalreport ,  empirenews ,  huzlers , etc. from mainstream news sources like  nytimes ,  cnn ,  wsj , foxnews ,  washingtonpost , etc. Similarly, from Fig- ure  2c  we observe that DeClarE locates politicians with similar ideologies and opinions close to each other in the embedding space. \n5.3 Analyzing Attention Weights \nAttention weights help understand what DeClarE focuses on during learning and how it affects its decisions – thereby, making our model transparent to the end-users. Table  6  illustrates some interest- ing claims and salient words (highlighted) that De- ClarE focused on during learning. Darker shades indicate higher weights given to the corresponding words. As illustrated in the table, DeClarE gives more attention to important words in the report- ing article that are relevant to the claim and also "}
{"page": 7, "image_path": "doc_images/D18-1003_7.jpg", "ocr_text": "edailycurrant\nehuzlers\n\nempirenews\nenationalreport\n\n‘eWorldnewsdailyreport\n\ncnn\n° bernie sanders, barack obama\nnytimes,\n\nwashingtonpost hillary clinton ¢\n\neted cruz\n*rudy giuliani\nmike pence\nbbc ‘paul ryan\nfoxnewse\nusatoday» donald trump\nowsj mitch meconnell\n\n(a) Projections of article representations\nusing PCA; DeClarE obtains clear sep-\naration between representations of non-\ncredible articles (red) vs. true ones\n(green).\n\nentic ones.\n\n)) Projections of article source repr\nentations using PCA; DeClarE clear!\neparates fake news sources from au-\n\n'y\n\n(c) Projections of claim source repre-\nsentations using PCA; DeClarE clusters\npoliticians of similar ideologies close to\neach other in the embedding space.\n\nFigure 2: Dissecting the article, article source and claim source representations learned by DeClarE.\n\n[False] Barbara Boxer: \"Fiorina's plan would mean slashing Social Security and Medicare.\"\n\nArticle Sou\n\nleast of Slimimel of With\n\nm\nwhile ignoring critical fa@@t8 that would give a different impression mr adair cited a couple examples of barely true Claims\n\nincluding this one in california democratic sen barbara boxer élaimed that republican challenger carly fiorina s plan would mean slashing social security\n\nbut We found there wi\n\nand\n\nto support that fiorina\n\nSaid much about her ideas on social security and medicare and what\n\nshe has said doe8n t provide much PROOf of slashing and then there s this one in pennsylvania in the pennsylvania senate race republican pat toomey\n\n[True] Hillary Clinton: \"The gun epidemic is the leading cause of death of young African-American men, more than the next nine causes put together.\"\n\nArticle Source: thetrace.org\naway the [@ading cause of death by\nclinton a chilling on\n\nfia the\n\nGiirabile SEpTERBE 27 BOTB curing th\nand the Vietimization of black Wales the gun @pidemie is the EAU cause of death of Young Gfrian\nshe from the centei disease\n\nmore\nblack inales between the GES of WS and BA that died in BOTA a Wiajority 54 PERCE were\n\nfil presidential debate monday night democratic wominee hillary\n\nher assertion of all\n\n‘ontrol and prevention\n\nKilled with a gun (GPE) Wine in TO\n\n[False] : Coca-Cola’\nArticle Source: foxnews.com\n\nthe first diet colas being the first in 1952 @B€dCOId execs at that time were hesitant to\nthe drink wa:\nwas ;\nin the 70s did its damage and the introduction of diet coke in the early 1980s pushed tab even\n\nto those who were keeping tab of their weight according to cola\ngreat story which unfortunately says is completely\n\nthe name\n\niginal diet cola drink, TaB, took its name from an acronym for “totally artificial beverage.”\n\nthe term diet to SOGae0la so the name tab was chosen as a tribute\nactually dubbed tab as an GePORM for totally artificial beverage a\n\na by computer and market research the\n\n[True] : Household paper shredders can pose a danger to children and pets.\nArticle Source: byegoff:com\npackages while still protecting any pr\n\ne information that may be contained in the papers in HheOPy the per:\npersonal or pet injuries from paper shredders a growing number of reported injuries FEVEA that home Shredders Pose\n\nnal home paper shredder makes much sense\na danger to any user and are\n\nespecially dangerous to children and pets in fact the federal consumer product safety commission issued a paper shredder safety alert documenting reports\n\nof incidents involving finger amputations lacerations\n\nand other finger injuries directly connected to the use of home\n\nTable 6: Interpretation via attention (weights) ([True]/[False] indicates the verdict from DeClarE).\n\nplay a major role in deciding the corresponding\nclaim’s credibility. In the first example on Table 6,\nhighlighted words such as “..barely true...” and\n“sketchy evidence...” help our system to identify\nthe claim as not credible. On the other hand, high-\nlighted words in the last example, like, “..reveal...”\nand “..documenting reports...” help our system to\nassess the claim as credible.\n\n6 Related Work\n\nOur work is closely related to the following areas:\nCredibility analysis of Web claims: Our work\nbuilds upon approaches for performing credibility\nanalysis of natural language claims in an open-\ndomain Web setting. The approach proposed in\nPopat et al. (2016, 2017) employs stylistic lan-\n\n29\n\nguage features and the stance of articles to as-\nsess the credibility of the natural language claims.\nHowever, their model heavily relies on hand-\ncrafted language features. Rashkin et al. (2017);\nWang (2017) propose neural network based ap-\nproaches for determining the credibility of a tex-\ntual claim, but it does not consider external\nsources like web evidence and claim sources.\nThese can be important evidence sources for cred-\nibility analysis. The method proposed by Samadi\net al. (2016) uses the Probabilistic Soft Logic\n(PSL) framework to estimate source reliability and\nclaim correctness. Vydiswaran et al. (2011) pro-\nposes an iterative algorithm which jointly learns\nthe veracity of textual claims and trustworthiness\nof the sources. These approaches do not consider\n", "vlm_text": "The image consists of three subplots illustrating the use of PCA (Principal Component Analysis) in projecting different types of data:\n\n1. **Subplot (a)**: Shows projections of article representations. It distinguishes non-credible articles (in red) from true ones (in green).\n\n2. **Subplot (b)**: Displays projections of article source representations, separating fake news sources from authentic ones using PCA. Various news sources are labeled, such as \"cnn,\" \"nytimes,\" and others.\n\n3. **Subplot (c)**: Shows projections of claim source representations, clustering politicians of similar ideologies close to each other in the embedding space. Names like \"bernie sanders\" and \"donald trump\" are present.\n\nEach subplot demonstrates how DeClarE (which appears to be a model or method) effectively separates and clusters data points.\nFigure 2 : Dissecting the article, article source and claim source representations learned by DeClarE. \nThe image contains a compilation of statements with their truth values and article sources:\n\n1. **Statement**: Barbara Boxer claimed \"Fiorina's plan would mean slashing Social Security and Medicare.\"\n   - **Truth Value**: False\n   - **Article Source**: nytimes.com\n\n2. **Statement**: Hillary Clinton stated \"The gun epidemic is the leading cause of death of young African-American men, more than the next nine causes put together.\"\n   - **Truth Value**: True\n   - **Article Source**: thetrace.org\n\n3. **Statement**: \"Coca-Cola’s original diet cola drink, TaB, took its name from an acronym for 'totally artificial beverage.'\"\n   - **Truth Value**: False\n   - **Article Source**: foxnews.com\n\n4. **Statement**: \"Household paper shredders can pose a danger to children and pets.\"\n   - **Truth Value**: True\n   - **Article Source**: byegoff.com\n\nHighlighted words seem to emphasize key points or aspects of the text in each statement.\nTable 6 : Interpretation via attention (weights)   $([T r u e]/[F a l s e]$   indicates the verdict from DeClarE). \nplay a major role in deciding the corresponding claim’s credibility. In the ﬁrst example on Table  6 , highlighted words such as “ ..barely true... ” and “ ..sketchy evidence... ” help our system to identify the claim as  not credible . On the other hand, high- lighted words in the last example, like, “ ..reveal... ” and “ ..documenting reports... ” help our system to assess the claim as  credible . \n6 Related Work \nOur work is closely related to the following areas: Credibility analysis of Web claims:  Our work builds upon approaches for performing credibility analysis of natural language claims in an open- domain Web setting. The approach proposed in Popat et al.  ( 2016 ,  2017 ) employs stylistic lan- guage features and the stance of articles to as- sess the credibility of the natural language claims. However, their model heavily relies on hand- crafted language features.  Rashkin et al.  ( 2017 ); Wang  ( 2017 ) propose neural network based ap- proaches for determining the credibility of a tex- tual claim, but it does not consider external sources like web evidence and claim sources. These can be important evidence sources for cred- ibility analysis. The method proposed by  Samadi et al.  ( 2016 ) uses the Probabilistic Soft Logic (PSL) framework to estimate source reliability and claim correctness.  Vydiswaran et al.  ( 2011 ) pro- poses an iterative algorithm which jointly learns the veracity of textual claims and trustworthiness of the sources. These approaches do not consider \n"}
{"page": 8, "image_path": "doc_images/D18-1003_8.jpg", "ocr_text": "the deeper semantic aspects of language, however.\nWiebe and Riloff (2005); Lin et al. (2011); Re-\ncasens et al. (2013) study the problem of detecting\nbias in language, but do not consider credibility.\n\nTruth discovery: Prior approaches for truth dis-\ncovery (Yin et al., 2008; Dong et al., 2009, 2015;\nLi et al., 2011, 2014, 2015; Pasternack and Roth,\n2011, 2013; Ma et al., 2015; Zhi et al., 2015;\nGao et al., 2015; Lyu et al., 2017) have focused\non structured data with the goal of addressing\nthe problem of conflict resolution amongst multi-\nsource data. Nakashole and Mitchell (2014) pro-\nposed a method to extract conflicting values from\nthe Web in the form of Subject-Predicate-Object\n(SPO) triplets and uses language objectivity analy-\nsis to determine the true value. Like the other truth\ndiscovery approaches, however, this approach is\nmainly suitable for use with structured data.\n\nCredibility analysis in social media: Mukher-\njee et al. (2014); Mukherjee and Weikum (2015)\npropose PGM based approaches to jointly in-\nfer a statement’s credibility and the reliability of\nsources using language specific features. Ap-\nproaches like (Castillo et al., 2011; Qazvinian\net al., 2011; Yang et al., 2012; Xu and Zhao, 2012;\nGupta et al., 2013; Zhao et al., 2015; Volkova\net al., 2017) propose supervised methods for de-\ntecting deceptive content in social media plat-\nforms like Twitter, Sina Weibo, etc. Similarly, ap-\nproaches like Ma et al. (2016); Ruchansky et al.\n(2017) use neural network methods to identify\nfake news and rumors on social media. Ku-\nmar et al. (2016) studies the problem of detect-\ning hoax articles on Wikipedia. All these rely on\ndomain-specific and community-specific features\nlike retweets, likes, upvotes, etc.\n\n7 Conclusion\n\nIn this work, we propose a completely automated\nend-to-end neural network model, DeClarE, for\nevidence-aware credibility assessment of natural\nlanguage claims without requiring hand-crafted\nfeatures or lexicons. DeClarE captures signals\nfrom external evidence articles and models joint\ninteractions between various factors like the con-\ntext of a claim, the language of reporting articles,\nand trustworthiness of their sources. Extensive ex-\nperiments on real world datasets demonstrate our\neffectiveness over state-of-the-art baselines.\n\n30\n\nReferences\n\nCarlos Castillo, Marcelo Mendoza, and Barbara\nPoblete. 2011. Information credibility on twitter. In\nProceedings of the 20th International Conference on\nWorld Wide Web, WWW ’11, pages 675-684, New\nYork, NY, USA. ACM.\n\nLeon Derczynski, Kalina Bontcheva, Maria Liakata,\nRob Procter, Geraldine Wong Sak Hoi, and Arkaitz\nZubiaga. 2017. Semeval-2017 task 8: Rumoureval:\nDetermining rumour veracity and support for ru-\nmours. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation, SemEval@ACL\n2017, Vancouver, Canada, August 3-4, 2017, pages\n69-76.\n\nXin Luna Dong, Laure Berti-Equille, and Divesh Sri-\nvastava. 2009. Integrating conflicting data: The\nrole of source dependence. Proc. VLDB Endow.,\n2(1):550-561.\n\nXin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,\nVan Dang, Wilko Horn, Camillo Lugaresi, Shao-\nhua Sun, and Wei Zhang. 2015. Knowledge-based\ntrust: Estimating the trustworthiness of web sources.\nProc. VLDB Endow., 8(9):938-949.\n\nOmar Enayet and Samhaa R. El-Beltagy. 2017.\nNiletmrg at semeval-2017 task 8: Determining ru-\nmour and veracity support for rumours on twitter.\nIn Proceedings of the 11th International Workshop\non Semantic Evaluation, SemEval@ACL 2017, Van-\ncouver, Canada, August 3-4, 2017, pages 470-474.\n\nJing Gao, Qi Li, Bo Zhao, Wei Fan, and Jiawei Han.\n2015. Truth discovery and crowdsourcing aggrega-\ntion: A unified perspective. PVLDB, 8(12):2048—\n2049.\n\nAlex Graves, Santiago Fernandez, and Jiirgen Schmid-\nhuber. 2005. Bidirectional Istm networks for\nimproved phoneme classification and recognition.\nIn Proceedings of the 15th International Con-\nference on Artificial Neural Networks: Formal\nModels and Their Applications - Volume Part II,\nICANN’05, pages 799-804, Berlin, Heidelberg.\nSpringer-Verlag.\n\nAditi Gupta, Hemank Lamba, Ponnurangam Ku-\nmaraguru, and Anupam Joshi. 2013. Faking sandy:\nCharacterizing and identifying fake images on twit-\nter during hurricane sandy. In Proceedings of the\n22Nd International Conference on World Wide Web,\nWWW °13 Companion, pages 729-736, New York,\nNY, USA. ACM.\n\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\n\nSrijan Kumar, Robert West, and Jure Leskovec. 2016.\nDisinformation on the web: Impact, characteristics,\nand detection of wikipedia hoaxes. In Proceed-\nings of the 25th International Conference on World\nWide Web, WWW ’ 16, pages 591-602, Republic and\n", "vlm_text": "the deeper semantic aspects of language, however. Wiebe and Riloff  ( 2005 );  Lin et al.  ( 2011 );  Re- casens et al.  ( 2013 ) study the problem of detecting bias in language, but do not consider credibility. \nTruth discovery:  Prior approaches for truth dis- covery ( Yin et al. ,  2008 ;  Dong et al. ,  2009 ,  2015 ; Li et al. ,  2011 ,  2014 ,  2015 ;  Pasternack and Roth , 2011 ,  2013 ;  Ma et al. ,  2015 ;  Zhi et al. ,  2015 ; Gao et al. ,  2015 ;  Lyu et al. ,  2017 ) have focused on structured data with the goal of addressing the problem of conﬂict resolution amongst multi- source data.  Nakashole and Mitchell  ( 2014 ) pro- posed a method to extract conﬂicting values from the Web in the form of Subject-Predicate-Object (SPO) triplets and uses language objectivity analy- sis to determine the true value. Like the other truth discovery approaches, however, this approach is mainly suitable for use with structured data. \nCredibility analysis in social media:  Mukher- jee et al.  ( 2014 );  Mukherjee and Weikum  ( 2015 ) propose PGM based approaches to jointly in- fer a statement’s credibility and the reliability of sources using language speciﬁc features. Ap- proaches like ( Castillo et al. ,  2011 ;  Qazvinian et al. ,  2011 ;  Yang et al. ,  2012 ;  Xu and Zhao ,  2012 ; Gupta et al. ,  2013 ;  Zhao et al. ,  2015 ;  Volkova et al. ,  2017 ) propose supervised methods for de- tecting deceptive content in social media plat- forms like Twitter, Sina Weibo, etc. Similarly, ap- proaches like  Ma et al.  ( 2016 );  Ruchansky et al. ( 2017 ) use neural network methods to identify fake news and rumors on social media. Ku- mar et al.  ( 2016 ) studies the problem of detect- ing hoax articles on Wikipedia. All these rely on domain-speciﬁc and community-speciﬁc features like retweets, likes, upvotes, etc. \n7 Conclusion \nIn this work, we propose a completely automated end-to-end neural network model, DeClarE, for evidence-aware credibility assessment of natural language claims without requiring hand-crafted features or lexicons. DeClarE captures signals from external evidence articles and models joint interactions between various factors like the con- text of a claim, the language of reporting articles, and trustworthiness of their sources. Extensive ex- periments on real world datasets demonstrate our effectiveness over state-of-the-art baselines. \nReferences \nCarlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on twitter. In Proceedings of the 20th International Conference on \nWorld Wide Web, WWW ’11, pages 675–684, NewYork, NY, USA. ACM. Leon Derczynski, Kalina Bontcheva, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz Zubiaga. 2017. Semeval-2017 task 8: Rumoureval: Determining rumour veracity and support for ru- mours. In  Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017 , pages 69–76. Xin Luna Dong, Laure Berti-Equille, and Divesh Sri- vastava. 2009. Integrating conﬂicting data: The role of source dependence. Proc. VLDB Endow. , 2(1):550–561. Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy, Van Dang, Wilko Horn, Camillo Lugaresi, Shao- hua Sun, and Wei Zhang. 2015. Knowledge-based trust: Estimating the trustworthiness of web sources. Proc. VLDB Endow. , 8(9):938–949. Omar Enayet and Samhaa R. El-Beltagy. 2017. Niletmrg at semeval-2017 task 8: Determining ru- mour and veracity support for rumours on twitter. In  Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Van- couver, Canada, August 3-4, 2017 , pages 470–474. Jing Gao, Qi Li, Bo Zhao, Wei Fan, and Jiawei Han. 2015. Truth discovery and crowdsourcing aggrega- tion: A uniﬁed perspective.  PVLDB , 8(12):2048– 2049. Alex Graves, Santiago Fern´ andez, and J¨ urgen Schmid- huber. 2005. Bidirectional lstm networks for improved phoneme classiﬁcation and recognition. In  Proceedings of the 15th International Con- ference on Artiﬁcial Neural Networks: Formal Models and Their Applications - Volume Part II , ICANN’05, pages 799–804, Berlin, Heidelberg. Springer-Verlag. Aditi Gupta, Hemank Lamba, Ponnurangam Ku- maraguru, and Anupam Joshi. 2013. Faking sandy: Characterizing and identifying fake images on twit- ter during hurricane sandy. In  Proceedings of the 22Nd International Conference on World Wide Web , WWW   $^{'}13$   Companion, pages 729–736, New York, NY, USA. ACM. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR , abs/1412.6980.Srijan Kumar, Robert West, and Jure Leskovec. 2016. Disinformation on the web: Impact, characteristics, and detection of wikipedia hoaxes. In  Proceed- ings of the 25th International Conference on World "}
{"page": 9, "image_path": "doc_images/D18-1003_9.jpg", "ocr_text": "Canton of Geneva, Switzerland. International World\nWide Web Conferences Steering Committee.\n\nQi Li, Yaliang Li, Jing Gao, Lu Su, Bo Zhao, Mu-\nrat Demirbas, Wei Fan, and Jiawei Han. 2014. A\nconfidence-aware approach for truth discovery on\nlong-tail data. Proc. VLDB Endow., 8(4):425—-436.\n\nXian Li, Weiyi Meng, and Clement Yu. 2011. T-\n\nverifier: Verifying truthfulness of fact statements.\nIn Proceedings of the 2011 IEEE 27th International\nConference on Data Engineering, ICDE ’11, pages\n63-74, Washington, DC, USA. IEEE Computer So-\nciety.\n\nYaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su,\n\nBo Zhao, Wei Fan, and Jiawei Han. 2016. A sur-\nvey on truth discovery. SIGKDD Explor. Newsl.,\n17(2):1-16.\n\nYaliang Li, Qi Li, Jing Gao, Lu Su, Bo Zhao, Wei Fan,\n\nand Jiawei Han. 2015. On the discovery of evolv-\ning truth. In Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining, KDD ’15, pages 675-684, New\nYork, NY, USA. ACM.\n\nChenghua Lin, Yulan He, and Richard Everson.\n\n2011. Sentence subjectivity detection with weakly-\nsupervised learning. In Proceedings of 5th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 1153-1161. Asian Federation of Nat-\nural Language Processing.\n\nShanshan Lyu, Wentao Ouyang, Huawei Shen, and\n\nXueqi Cheng. 2017. Truth discovery by claim and\nsource embedding. In Proceedings of the 2017 ACM\non Conference on Information and Knowledge Man-\nagement, CIKM ’17, pages 2183-2186, New York,\nNY, USA. ACM.\n\nFenglong Ma, Yaliang Li, Qi Li, Minghui Qiu, Jing\nGao, Shi Zhi, Lu Su, Bo Zhao, Heng Ji, and Jiawei\nHan. 2015. Faitcrowd: Fine grained truth discovery\nfor crowdsourced data aggregation. In Proceedings\nof the 21th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, KDD\n°15, pages 745-754, New York, NY, USA. ACM.\n\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,\n\nBernard J. Jansen, Kam-Fai Wong, and Meeyoung\nCha. 2016. Detecting rumors from microblogs with\nrecurrent neural networks. In Proceedings of the\nTwenty-Fifth International Joint Conference on Ar-\ntificial Intelligence, IJCAV16, pages 3818-3824.\nAAAI Press.\n\nSubhabrata Mukherjee and Gerhard Weikum. 2015.\n\nLeveraging joint interactions for credibility analysis\nin news communities. In Proceedings of the 24th\nACM International on Conference on Information\nand Knowledge Management, CIKM °15.\n\nSubhabrata Mukherjee, Gerhard Weikum, and Cristian\n\nDanescu-Niculescu-Mizil. 2014. People on drugs:\n\n31\n\nCredibility of user statements in health communi-\nties. In Proceedings of the 20th ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining, KDD ’14, pages 65-74, New\nYork, NY, USA. ACM.\n\nNdapandula Nakashole and Tom M. Mitchell. 2014.\nLanguage-aware truth assessment of fact candidates.\nIn Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2014, June 22-27, 2014, Baltimore, MD, USA, Vol-\nume 1: Long Papers, pages 1009-1019.\n\nJeff Pasternack and Dan Roth. 2011. Making bet-\nter informed trust decisions with generalized fact-\nfinding. In JJCAI 2011, Proceedings of the 22nd\nInternational Joint Conference on Artificial Intel-\nligence, Barcelona, Catalonia, Spain, July 16-22,\n2011, pages 2324-2329.\n\nJeff Pasternack and Dan Roth. 2013. Latent credibility\nanalysis. In Proceedings of the 22Nd International\nConference on World Wide Web, WWW ’13, pages\n009-1020, New York, NY, USA. ACM.\n\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Natu-\nral Language Processing, EMNLP ’ 14.\n\nKashyap Popat, Subhabrata Mukherjee, Jannik\nStrétgen, and Gerhard Weikum. 2016. Credibil-\nity assessment of textual claims on the web. In\nProceedings of the 25th ACM International on\nConference on Information and Knowledge Man-\nagement, CIKM ’16, pages 2173-2178, New York,\nNY, USA. ACM.\n\nKashyap Popat, Subhabrata Mukherjee, Jannik\nStrétgen, and Gerhard Weikum. 2017. Where the\ntruth lies: Explaining the credibility of emerging\nclaims on the web and social media. In Proceedings\nof the 26th International Conference on World Wide\nWeb Companion, WWW ’17 Companion.\n\nVahed Qazvinian, Emily Rosengren, Dragomir R.\nRadev, and Qiaozhu Mei. 2011. Rumor has it: Iden-\ntifying misinformation in microblogs. In Proceed-\nings of the Conference on Empirical Methods in\nNatural Language Processing, EMNLP ’11, pages\n1589-1599, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\n\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nVolkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and polit-\nical fact-checking. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP ’17.\n\nMarta Recasens, Cristian Danescu-Niculescu-Mizil,\nand Dan Jurafsky. 2013. Linguistic models for an-\nalyzing and detecting biased language. In Proceed-\nings of the 51st Annual Meeting of the Association\n", "vlm_text": "Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee. \nQi Li, Yaliang Li, Jing Gao, Lu Su, Bo Zhao, Mu- rat Demirbas, Wei Fan, and Jiawei Han. 2014. A conﬁdence-aware approach for truth discovery on long-tail data.  Proc. VLDB Endow. , 8(4):425–436. Xian Li, Weiyi Meng, and Clement Yu. 2011. T- veriﬁer: Verifying truthfulness of fact statements. In  Proceedings of the 2011 IEEE 27th International Conference on Data Engineering , ICDE ’11, pages 63–74, Washington, DC, USA. IEEE Computer So- ciety. Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su, Bo Zhao, Wei Fan, and Jiawei Han. 2016. A sur- vey on truth discovery. SIGKDD Explor. Newsl. , 17(2):1–16. Yaliang Li, Qi Li, Jing Gao, Lu Su, Bo Zhao, Wei Fan, and Jiawei Han. 2015. On the discovery of evolv- ing truth. In  Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’15, pages 675–684, New York, NY, USA. ACM. Chenghua Lin, Yulan He, and Richard Everson. 2011. Sentence subjectivity detection with weakly- supervised learning. In  Proceedings of 5th Interna- tional Joint Conference on Natural Language Pro- cessing , pages 1153–1161. Asian Federation of Nat- ural Language Processing. Shanshan Lyu, Wentao Ouyang, Huawei Shen, and Xueqi Cheng. 2017. Truth discovery by claim and source embedding. In  Proceedings of the 2017 ACM on Conference on Information and Knowledge Man- agement , CIKM ’17, pages 2183–2186, New York, NY, USA. ACM. Fenglong Ma, Yaliang Li, Qi Li, Minghui Qiu, Jing Gao, Shi Zhi, Lu Su, Bo Zhao, Heng Ji, and Jiawei Han. 2015. Faitcrowd: Fine grained truth discovery for crowdsourced data aggregation. In  Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’15, pages 745–754, New York, NY, USA. ACM. Jing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon, Bernard J. Jansen, Kam-Fai Wong, and Meeyoung Cha. 2016. Detecting rumors from microblogs with recurrent neural networks. In  Proceedings of the Twenty-Fifth International Joint Conference on Ar- tiﬁcial Intelligence , IJCAI’16, pages 3818–3824. AAAI Press.Subhabrata Mukherjee and Gerhard Weikum. 2015. Leveraging joint interactions for credibility analysis in news communities. In  Proceedings of the 24th ACM International on Conference on Information and Knowledge Management , CIKM ’15. Subhabrata Mukherjee, Gerhard Weikum, and Cristian Danescu-Niculescu-Mizil. 2014. People on drugs: \nCredibility of user statements in health communi- ties. In  Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14, pages 65–74, New York, NY, USA. ACM. \nNdapandula Nakashole and Tom M. Mitchell. 2014. Language-aware truth assessment of fact candidates. In  Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Vol- ume 1: Long Papers , pages 1009–1019. \nJeff Pasternack and Dan Roth. 2011. Making bet- ter informed trust decisions with generalized fact- ﬁnding. In  IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artiﬁcial Intel- ligence, Barcelona, Catalonia, Spain, July 16-22, 2011 , pages 2324–2329. \nJeff Pasternack and Dan Roth. 2013. Latent credibility analysis. In  Proceedings of the 22Nd International Conference on World Wide Web , WWW ’13, pages 1009–1020, New York, NY, USA. ACM. \nJeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In  Empirical Methods in Natu- ral Language Processing , EMNLP ’14. \nKashyap Popat, Subhabrata Mukherjee, Jannik Str¨ otgen, and Gerhard Weikum. 2016. Credibil- ity assessment of textual claims on the web. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Man- agement , CIKM ’16, pages 2173–2178, New York, NY, USA. ACM. \nKashyap Popat, Subhabrata Mukherjee, Jannik Str¨ otgen, and Gerhard Weikum. 2017. Where the truth lies: Explaining the credibility of emerging claims on the web and social media. In  Proceedings of the 26th International Conference on World Wide Web Companion , WWW ’17 Companion. \nVahed Qazvinian, Emily Rosengren, Dragomir R. Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden- tifying misinformation in microblogs. In  Proceed- ings of the Conference on Empirical Methods in Natural Language Processing , EMNLP ’11, pages 1589–1599, Stroudsburg, PA, USA. Association for Computational Linguistics. \nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and polit- ical fact-checking. In  Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing , EMNLP ’17. \nMarta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic models for an- alyzing and detecting biased language. In  Proceed- ings of the 51st Annual Meeting of the Association "}
{"page": 10, "image_path": "doc_images/D18-1003_10.jpg", "ocr_text": "for Computational Linguistics (Volume 1: Long Pa-\npers), pages 1650-1659. Association for Computa-\ntional Linguistics.\n\nNatali Ruchansky, Sungyong Seo, and Yan Liu. 2017.\nCsi: A hybrid deep model for fake news detection.\nIn Proceedings of the 2017 ACM on Conference on\nInformation and Knowledge Management, CIKM\n°17, pages 797-806, New York, NY, USA. ACM.\n\nMehdi Samadi, Partha Talukdar, Manuela Veloso, and\nManuel Blum. 2016. Claimeval: Integrated and\nflexible framework for claim evaluation using cred-\nibility of sources. In Proceedings of the Thir-\ntieth AAAI Conference on Artificial Intelligence,\nAAAT 16, pages 222-228. AAAI Press.\n\nVikram Singh, Sunny Narayan, Md. Shad Akhtar, Asif\nEkbal, and Pushpak Bhattacharyya. 2017. IITP at\nsemeval-2017 task 8 : A supervised approach for\nrumour evaluation. In Proceedings of the 11th In-\nternational Workshop on Semantic Evaluation, Se-\nmEval@ACL 2017, Vancouver, Canada, August 3-4,\n2017, pages 497-501.\n\nSvitlana Volkova, Kyle Shaffer, Jin Yea Jang, and\nNathan Hodas. 2017. Separating facts from fiction:\nLinguistic models to classify suspicious and trusted\nnews posts on twitter. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 647—\n653. Association for Computational Linguistics.\n\nV.G. Vinod Vydiswaran, ChengXiang Zhai, and Dan\nRoth. 2011. Content-driven trust propagation frame-\nwork. In Proceedings of the 17th ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining, KDD ’11, pages 974-982, New\nYork, NY, USA. ACM.\n\nWilliam Yang Wang. 2017. “liar, liar pants on fire”: A\nnew benchmark dataset for fake news detection. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2017,\nVancouver, Canada, July 30 - August 4, Volume 2:\nShort Papers, pages 422-426.\n\nJanyce Wiebe and Ellen Riloff. 2005. Creating subjec-\ntive and objective sentence classifiers from unanno-\ntated texts. In Proceedings of the 6th International\nConference on Computational Linguistics and Intel-\nligent Text Processing, CICLing’05, pages 486-497,\nBerlin, Heidelberg. Springer-Verlag.\n\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2015. Towards universal paraphrastic sen-\ntence embeddings. In Proceedings of the Inter-\nnational Conference on Learning Representations\n(ICLR).\n\nQiongkai Xu and Hai Zhao. 2012. Using deep lin-\nguistic features for finding deceptive opinion spam.\nIn Proceedings of COLING 2012: Posters, pages\n1341-1350. The COLING 2012 Organizing Com-\nmittee.\n\n32\n\nFan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.\nAutomatic detection of rumor on sina weibo. In Pro-\nceedings of the ACM SIGKDD Workshop on Mining\nData Semantics, MDS °12, pages 13:1-13:7, New\nYork, NY, USA. ACM.\n\nXiaoxin Yin, Jiawei Han, and Philip S. Yu. 2008.\nTruth discovery with multiple conflicting informa-\ntion providers on the web. JEEE Trans. on Knowl.\nand Data Eng., 20(6):796-808.\n\nZhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. En-\nquiring minds: Early detection of rumors in social\nmedia from enquiry posts. In Proceedings of the\n24th International Conference on World Wide Web,\nWWW °15, pages 1395-1405, Republic and Canton\nof Geneva, Switzerland. International World Wide\nWeb Conferences Steering Committee.\n\nShi Zhi, Bo Zhao, Wenzhu Tong, Jing Gao, Dian Yu,\nHeng Ji, and Jiawei Han. 2015. Modeling truth ex-\nistence in truth discovery. In Proceedings of the 21th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, KDD ’15, pages\n1543-1552, New York, NY, USA. ACM.\n", "vlm_text": "for Computational Linguistics (Volume 1: Long Pa- pers) , pages 1650–1659. Association for Computa- \ntional Linguistics. Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017. Csi: A hybrid deep model for fake news detection. In  Proceedings of the 2017 ACM on Conference on Information and Knowledge Management , CIKM ’17, pages 797–806, New York, NY, USA. ACM. Mehdi Samadi, Partha Talukdar, Manuela Veloso, and Manuel Blum. 2016. Claimeval: Integrated and ﬂexible framework for claim evaluation using cred- ibility of sources. In  Proceedings of the Thir- tieth AAAI Conference on Artiﬁcial Intelligence , AAAI’16, pages 222–228. AAAI Press. Vikram Singh, Sunny Narayan, Md. Shad Akhtar, Asif Ekbal, and Pushpak Bhattacharyya. 2017. IITP at semeval-2017 task 8 : A supervised approach for rumour evaluation. In  Proceedings of the 11th In- ternational Workshop on Semantic Evaluation, Se- mEval@ACL 2017, Vancouver, Canada, August 3-4, 2017 , pages 497–501. Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and Nathan Hodas. 2017. Separating facts from ﬁction: Linguistic models to classify suspicious and trusted news posts on twitter. In  Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 647– 653. Association for Computational Linguistics. V.G. Vinod Vydiswaran, ChengXiang Zhai, and Dan Roth. 2011. Content-driven trust propagation frame- work. In  Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’11, pages 974–982, New York, NY, USA. ACM. William Yang Wang. 2017. ”liar, liar pants on ﬁre”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 2: Short Papers , pages 422–426. Janyce Wiebe and Ellen Riloff. 2005. Creating subjec- tive and objective sentence classiﬁers from unanno- tated texts. In  Proceedings of the 6th International Conference on Computational Linguistics and Intel- ligent Text Processing , CICLing’05, pages 486–497, Berlin, Heidelberg. Springer-Verlag. John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal paraphrastic sen- tence embeddings. In  Proceedings of the Inter- national Conference on Learning Representations (ICLR) . Qiongkai Xu and Hai Zhao. 2012. Using deep lin- guistic features for ﬁnding deceptive opinion spam. In  Proceedings of COLING 2012: Posters , pages 1341–1350. The COLING 2012 Organizing Com- mittee. \nFan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012. Automatic detection of rumor on sina weibo. In  Pro- ceedings of the ACM SIGKDD Workshop on Mining Data Semantics , MDS ’12, pages 13:1–13:7, New York, NY, USA. ACM. Xiaoxin Yin, Jiawei Han, and Philip S. Yu. 2008. Truth discovery with multiple conﬂicting informa- tion providers on the web.  IEEE Trans. on Knowl. and Data Eng. , 20(6):796–808. Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. En- quiring minds: Early detection of rumors in social media from enquiry posts. In  Proceedings of the 24th International Conference on World Wide Web , WWW ’15, pages 1395–1405, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee. Shi Zhi, Bo Zhao, Wenzhu Tong, Jing Gao, Dian Yu, Heng Ji, and Jiawei Han. 2015. Modeling truth ex- istence in truth discovery. In  Proceedings of the 21th ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining , KDD ’15, pages 1543–1552, New York, NY, USA. ACM. "}
