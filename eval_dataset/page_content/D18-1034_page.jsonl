{"page": 0, "image_path": "doc_images/D18-1034_0.jpg", "ocr_text": "Neural Cross-Lingual Named Entity Recognition with Minimal Resources\n\nJiateng Xie,' Zhilin Yang,'!Graham Neubig,'\nNoah A. Smith,” and Jaime Carbonell!\n‘Language Technologies Institute, Carnegie Mellon University\nPaul G. Allen School of Computer Science & Engineering, University of Washington\n3Allen Institute for Artificial Intelligence\n{jiatengx, zhiliny, gneubig, jgc}@cs.cmu.edu,\nnasmith@cs.washington.edu\n\nAbstract\n\nFor languages with no annotated resources,\nunsupervised transfer of natural language pro-\ncessing models such as named-entity recog-\nnition (NER) from resource-rich languages\nwould be an appealing capability. However,\ndifferences in words and word order across\nlanguages make it a challenging problem. To\nimprove mapping of lexical items across lan-\nguages, we propose a method that finds trans-\nlations based on bilingual word embeddings.\nTo improve robustness to word order differ-\nences, we propose to use self-attention, which\nallows for a degree of flexibility with respect\nto word order. We demonstrate that these\nmethods achieve state-of-the-art or competi-\ntive NER performance on commonly tested\nlanguages under a cross-lingual setting, with\nmuch lower resource requirements than past\napproaches. We also evaluate the challenges\nof applying these methods to Uyghur, a low-\nresource language. !\n\n1 Introduction\n\nNamed entity recognition (NER), the task of de-\ntecting and classifying named entities from text\ninto a few predefined categories such as people, lo-\ncations or organizations, has seen the state-of-the-\nart greatly advanced by the introduction of neu-\nral architectures (Collobert et al., 2011; Huang\net al., 2015; Chiu and Nichols, 2016; Lample et al.,\n2016; Yang et al., 2016; Ma and Hovy, 2016; Pe-\nters et al., 2017; Liu et al., 2018; Peters et al.,\n2018). However, the success of these methods is\nhighly dependent on a reasonably large amount of\nannotated training data, and thus it remains a chal-\nlenge to apply these models to languages with lim-\nited amounts of labeled data. Cross-lingual NER\nattempts to address this challenge by transferring\n\n'The source code is available at https://github.\ncom/thespect rewithin/cross-lingual_NER\n\n369\n\nknowledge from a high-resource source language\nwith abundant entity labels to a low-resource tar-\nget language with few or no labels. Specifically,\nin this paper we attempt to tackle the extreme sce-\nnario of unsupervised transfer, where no labeled\ndata is available in the target language. Within\nthis paradigm, there are two major challenges to\ntackle: how to effectively perform lexical mapping\nbetween the languages, and how to address word\norder differences.\n\nTo cope with the first challenge of lexical map-\nping, a number of methods use parallel corpora\nto project annotations between languages through\nword alignment (Ehrmann et al., 2011; Kim et al.,\n2012; Wang and Manning, 2014; Ni et al., 2017).\nSince parallel corpora may not be always avail-\nable, Mayhew et al. (2017) proposed a “cheap\ntranslation” approach that uses a bilingual dictio-\nnary to perform word-level translation. The above\napproaches provide a reasonable proxy for the\nactual labeled training data, largely because the\nwords that participate in entities can be translated\nrelatively reliably given extensive parallel dictio-\nnaries or corpora (e.g., with 1 million word pairs\nor sentences). Additionally, as a side benefit of\nhaving explicitly translated words, models can di-\nrectly exploit features extracted from the surface\nforms (e.g. through character-level neural feature\nextractors), which has proven essential for high\naccuracy in the monolingual scenario (Ma and\nHovy, 2016). However, these methods are largely\npredicated on the availability of large-scale paral-\nlel resources, and thus, their applicability to low-\nresource languages is limited.\n\nIn contrast, it is also possible to learn lex-\nical mappings through bilingual word embed-\ndings (BWE). These bilingual embeddings can\nbe obtained by using a small dictionary to\nproject two sets of embeddings into a consistent\nspace (Mikolov et al., 2013a; Faruqui and Dyer,\n\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369-379\nBrussels, Belgium, October 31 - November 4, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Neural Cross-Lingual Named Entity Recognition with Minimal Resources \nJiateng Xie,   Zhilin Yang, Graham Neubig, Noah A. Smith, ,   and Jaime Carbonell 1 \n1 Language Technologies Institute, Carnegie Mellon University 2 Paul G. Allen School of Computer Science & Engineering, University of Washington 3 Allen Institute for Artiﬁcial Intelligence { jiatengx,zhiliny,gneubig,jgc } @cs.cmu.edu , nasmith@cs.washington.edu \nAbstract \nFor languages with no annotated resources, unsupervised transfer of natural language pro- cessing models such as named-entity recog- nition (NER) from resource-rich languages would be an appealing capability. However, differences in words and word order across languages make it a challenging problem. To improve mapping of lexical items across lan- guages, we propose a method that ﬁnds trans- lations based on bilingual word embeddings. To improve robustness to word order differ- ences, we propose to use self-attention, which allows for a degree of ﬂexibility with respect to word order. We demonstrate that these methods achieve state-of-the-art or competi- tive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a low- resource language. \n1 Introduction \nNamed entity recognition (NER), the task of de- tecting and classifying named entities from text into a few predeﬁned categories such as people, lo- cations or organizations, has seen the state-of-the- art greatly advanced by the introduction of neu- ral architectures ( Collobert et al. ,  2011 ;  Huang et al. ,  2015 ;  Chiu and Nichols ,  2016 ;  Lample et al. , 2016 ;  Yang et al. ,  2016 ;  Ma and Hovy ,  2016 ;  Pe- ters et al. ,  2017 ;  Liu et al. ,  2018 ;  Peters et al. , 2018 ). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a chal- lenge to apply these models to languages with lim- ited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring knowledge from a high-resource source language with abundant entity labels to a low-resource tar- get language with few or no labels. Speciﬁcally, in this paper we attempt to tackle the extreme sce- nario of  unsupervised transfer , where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. \n\nTo cope with the ﬁrst challenge of lexical map- ping, a number of methods use parallel corpora to project annotations between languages through word alignment ( Ehrmann et al. ,  2011 ;  Kim et al. , 2012 ;  Wang and Manning ,  2014 ;  Ni et al. ,  2017 ). Since parallel corpora may not be always avail- able, Mayhew et al.  ( 2017 ) proposed a “cheap translation” approach that uses a bilingual dictio- nary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictio- naries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side beneﬁt of having explicitly translated words, models can di- rectly exploit features extracted from the surface forms (e.g. through character-level neural feature extractors), which has proven essential for high accuracy in the monolingual scenario ( Ma and Hovy ,  2016 ). However, these methods are largely predicated on the availability of large-scale paral- lel resources, and thus, their applicability to low- resource languages is limited. \nIn contrast, it is also possible to learn lex- ical mappings through bilingual word embed- dings (BWE). These bilingual embeddings can be obtained by using a small dictionary to project two sets of embeddings into a consistent space ( Mikolov et al. ,  2013a ;  Faruqui and Dyer , 2014 ;  Artetxe et al. ,  2016 ;  Smith et al. ,  2017 ), or even in an entirely unsupervised manner using adversarial training or identical character strings ( Zhang et al. ,  2017 ;  Artetxe et al. ,  2017 ;  Lam- ple et al., 2018).Many approaches in the pasthave leveraged the shared embedding space for cross-lingual applications ( Guo et al. ,  2015 ;  Am- mar et al. ,  2016b ;  Zhang et al. ,  2016 ;  Fang and Cohn ,  2017 ), including NER ( Bharadwaj et al. , 2016 ;  Ni et al. ,  2017 ). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource lan- guages. However, since different languages have different linguistic properties, it is hard, if not im- possible, to align the two embedding spaces per- fectly (see Figure  1 ). Meanwhile, because sur- face forms are not available, character-level fea- tures cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). "}
{"page": 1, "image_path": "doc_images/D18-1034_1.jpg", "ocr_text": "2014; Artetxe et al., 2016; Smith et al., 2017),\nor even in an entirely unsupervised manner using\nadversarial training or identical character strings\n(Zhang et al., 2017; Artetxe et al., 2017; Lam-\nple et al., 2018). Many approaches in the past\nhave leveraged the shared embedding space for\ncross-lingual applications (Guo et al., 2015; Am-\nmar et al., 2016b; Zhang et al., 2016; Fang and\nCohn, 2017), including NER (Bharadwaj et al.,\n2016; Ni et al., 2017). The minimal dependency\non parallel resources makes the embedding-based\nmethod much more suitable for low-resource lan-\nguages. However, since different languages have\ndifferent linguistic properties, it is hard, if not im-\npossible, to align the two embedding spaces per-\nfectly (see Figure 1). Meanwhile, because sur-\nface forms are not available, character-level fea-\ntures cannot be used, resulting in reduced tagging\naccuracy (as demonstrated in our experiments).\n\nTo address the above issues, we propose a new\nlexical mapping approach that combines the ad-\nvantages of both discrete dictionary-based meth-\nods and continuous embedding-based methods.\nSpecifically, we first project embeddings of dif-\nferent languages into the shared BWE space, then\nlearn discrete word translations by looking for\nnearest neighbors in this projected space, and fi-\nnally train a model on the translated data. This\nallows our method to inherit the benefits of both\nembedding-based and dictionary-based methods:\nits resource requirements are low as in the former,\nbut it suffers less from misalignment of the em-\nbedding spaces and has access to character-level\ninformation like the latter.\n\nTurning to differences in word ordering, to\nour knowledge there are no methods that explic-\nitly deal with this problem in unsupervised cross-\nlingual transfer for NER. Our second contribu-\ntion is a method to alleviate this issue by incor-\nporating an order-invariant self-attention mech-\nanism (Vaswani et al., 2017; Lin et al., 2017)\ninto our neural architecture. Self-attention al-\nlows re-ordering of information within a partic-\nular encoded sequence, which makes it possible\nto account for word order differences between the\nsource and the target languages.\n\nIn our experiments, we start with models trained\nin English as the source language on the CoNLL\n2002 and 2003 datasets and transfer them into\nSpanish, Dutch, and German as the target lan-\nguages. Our approach obtains new state-of-the-\n\nart cross-lingual results in Spanish and Dutch, and\ncompetitive results in German, even without a\ndictionary, completely removing the need for re-\nsources such as Wikipedia and parallel corpora.\nNext, we transfer English using the same approach\ninto Uyghur, a truly low-resource language. With\nsignificantly fewer cross-lingual resources, our ap-\nproach can still perform competitively with previ-\nous best results.\n\n2 Approach\n\nWe establish our problem setting (82.1), then\npresent our methods in detail (§2.2), and provide\nsome additional motivation (§2.3).\n\n2.1 Problem Setting\n\nNER takes a sentence as the input and outputs a se-\nquence of labels corresponding to the named entity\ncategories of the words in the sentence, such as lo-\ncation, organization, person, or none. In standard\nsupervised NER, we are provided with a labeled\ncorpus of sentences in the target language along\nwith tags indicating which spans correspond to en-\ntities of each type.\n\nAs noted in the introduction, we study the prob-\nlem of unsupervised cross-lingual NER: given la-\nbeled training data only in a separate source lan-\nguage, we aim to learn a model that is able to per-\nform NER in the target language. This transfer\ncan be performed using a variety of resources, in-\ncluding parallel corpora (Tackstrom et al., 2012;\nNiet al., 2017), Wikipedia (Nothman et al., 2013),\nand large dictionaries (Ni et al., 2017; Mayhew\net al., 2017). In this work, we limit ourselves to\na setting where we have the following resources,\nmaking us comparable to other methods such as\nMayhew et al. (2017) and Ni et al. (2017):\n\ne Labeled training data in the source language.\n\ne Monolingual corpora in both source and target\nlanguages.\n\ne A dictionary, either a small pre-existing one, or\none induced by unsupervised methods.\n\n2.2 Method\n\nOur method follows the process below:\n\n1. Train separate word embeddings using mono-\nlingual corpora using standard embedding train-\ning methods (§2.2.1).\n\n2. Project word embeddings in the two languages\ninto a shared embedding space by optimizing\n\n370\n", "vlm_text": "\nTo address the above issues, we propose a new lexical mapping approach that combines the ad- vantages of both discrete dictionary-based meth- ods and continuous embedding-based methods. Speciﬁcally, we ﬁrst project embeddings of dif- ferent languages into the shared BWE space, then learn discrete word translations by looking for nearest neighbors in this projected space, and ﬁ- nally train a model on the translated data. This allows our method to inherit the beneﬁts of both embedding-based and dictionary-based methods: its resource requirements are low as in the former, but it suffers less from misalignment of the em- bedding spaces and has access to character-level information like the latter. \nTurning to differences in word ordering, to our knowledge there are no methods that explic- itly deal with this problem in unsupervised cross- lingual transfer for NER. Our second contribu- tion is a method to alleviate this issue by incor- porating an order-invariant self-attention mech- anism ( Vaswani et al. ,  2017 ;  Lin et al. ,  2017 ) into our neural architecture. Self-attention al- lows re-ordering of information within a partic- ular encoded sequence, which makes it possible to account for word order differences between the source and the target languages. \nIn our experiments, we start with models trained in English as the source language on the CoNLL 2002 and 2003 datasets and transfer them into Spanish, Dutch, and German as the target lan- guages. Our approach obtains new state-of-the- art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for re- sources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With signiﬁcantly fewer cross-lingual resources, our ap- proach can still perform competitively with previ- ous best results. \n\n2 Approach \nWe establish our problem setting ( § 2.1 ), then present our methods in detail ( § 2.2 ), and provide some additional motivation   $(\\S2.3)$  . \n2.1 Problem Setting \nNER takes a sentence as the input and outputs a se- quence of labels corresponding to the named entity categories of the words in the sentence, such as lo- cation, organization, person, or none. In standard supervised NER, we are provided with a labeled corpus of sentences in the target language along with tags indicating which spans correspond to en- tities of each type. \nAs noted in the introduction, we study the prob- lem of unsupervised cross-lingual NER: given la- beled training data only in a separate source lan- guage, we aim to learn a model that is able to per- form NER in the target language. This transfer can be performed using a variety of resources, in- cluding parallel corpora ( T¨ ackstr¨ om et al. ,  2012 ; Ni et al. ,  2017 ), Wikipedia ( Nothman et al. ,  2013 ), and large dictionaries ( Ni et al. ,  2017 ;  Mayhew et al. ,  2017 ). In this work, we limit ourselves to a setting where we have the following resources, making us comparable to other methods such as Mayhew et al.  ( 2017 ) and  Ni et al.  ( 2017 ):\n\n \n•  Labeled training data in the source language.\n\n •  Monolingual corpora in both source and target languages.\n\n •  A dictionary, either a small pre-existing one, or one induced by unsupervised methods. \n2.2 Method \nOur method follows the process below:\n\n \n1. Train separate word embeddings using mono- lingual corpora using standard embedding train- ing methods ( § 2.2.1 ).\n\n 2. Project word embeddings in the two languages into a shared embedding space by optimizing "}
{"page": 2, "image_path": "doc_images/D18-1034_2.jpg", "ocr_text": "erintures\n\nbid 06}\n\nnojade\no4 9909 Spezclas\n\nervestras\n\nProjection\n60°f5f ies 02\n\nencounter\n\neamples\n02 pabilidades\n\nrastorno\n\nenor\n\n02 -024\n\néolapso\n\nAEE\n\nsecognize}\n\noliangontsAle\n\n06-04 02 00 02 04 06\n\n044\n\n‘seconocer 004\n“Encuentro\n\nogee\n\nerary | #0)aK0\n\nTranslation\n\nencounter Nearest|Neighbor |__°\"9 esp\n— isd disorder | trastorno\nrecognize | reconocer\n@rixtyres, collapse colapso\nangry enojado\n\nSBoIMOE Ne SEBLCGGFize gape FP an we\nebdebiidades ———__—\n\n“04-02 00 O02 04 06\n\nFigure 1: Example of the result of our approach on Spanish-English words not included in the dictionary (em-\nbeddings are reduced to 2 dimensions for visual clarity). We first project word embeddings into a shared space,\nand then use the nearest neighbors for word translation. Notice that the word pairs are not perfectly aligned in the\nshared embedding space, but after word translation we obtain correct alignments.\n\nthe word embedding alignment using the given\ndictionary (§2.2.2).\n\n3. For each word in the source language training\ndata, translate it by finding its nearest neighbor\nin the shared embedding space (§2.2.3).\n\n4. Train an NER model using the translated words\nalong with the named entity tags from the En-\nglish corpus (§2.2.4).\n\nWe consider each in detail.\n\n2.2.1 Learning Monolingual Embeddings\n\nGiven text in the source and target language, we\nfirst independently learn word embedding matri-\nces X and Y in the source and target languages\nrespectively. These embeddings can be learned on\nmonolingual text in both languages with any of\nthe myriad of word embedding methods (Mikolov\net al., 2013b; Pennington et al., 2014; Bojanowski\net al., 2017).\n\n2.2.2 Learning Bilingual Embeddings\n\nNext, we learn a cross-lingual projection of X\nand Y into a shared space. Assume we are given\na dictionary {x;, y;}2, where x; and y; denote\nthe embeddings of a word pair. Let Xp =\n[v1,22,-+-,ap]' and Yp = [y1,y2,--- yo)\"\ndenote two embedding matrices consisting of\nword pairs from the dictionary.\n\nFollowing previous work (Zhang et al., 2016;\nArtetxe et al., 2016; Smith et al., 2017), we opti-\nmize the following objective:\n\nd\nain) Wa; —y|? st. WW! =T,\ni=\n\nwhere W is a square parameter matrix. This ob-\n\njective can be further simplified as\nmax Tr(XpWYp, ) st. WWI=T,\n\nHere, the transformation matrix W is constrained\n0 be orthogonal so that the dot product similarity\nof words is invariant with respect to the transfor-\nmation both within and across languages.\nTo optimize the above objective (the Procrustes\nproblem), we decompose the matrix Yn Xp us-\ning singular value decomposition. Let the results\nbe Yp Xp = UV\", then W = UV\" gives\nhe exact solution. We define the similarity ma-\ntrix between X and Y to be S = YWX! =\nYU(XV)\", where each column contains the co-\nsine similarity between source word 2; and all tar-\nget words y;. We can then define X’ = XV and\nY' = YU, which are X and Y transformed into a\nshared embedding space.\n\nTo refine the alignment in this shared space fur-\nther, we iteratively perform a self-learning refine-\nment step k * times by:\n\n1. Using the aligned embeddings to generate a new\ndictionary that consists of mutual nearest neigh-\nbors obtained using the same metric as intro-\nduced below.\n\n2. Solving the Procrustes problem based on the\nnewly generated dictionary to get a new set of\nbilingual embeddings.\n\nThe bilingual embeddings at the end of the kth\nstep, X;, and Y{, will be used to perform trans-\nlation.\n\n2.2.3 Learning Word Translations\n\nTo learn actual word translations, we next pro-\nceed to perform nearest-neighbor search in the\n\n2We use k = 3 in this paper.\n\n371\n", "vlm_text": "The image illustrates a method for translating Spanish-English words that aren't included in a dictionary using word embeddings. The process involves projecting word embeddings into a shared space and then using the concept of nearest neighbors for translating words.\n\nOn the left side of the image, there is a scatter plot showing Spanish (red points) and English (blue points) words that are not perfectly aligned in a shared embedding space. Some examples include \"trastorno\" (Spanish) and \"disorder\" (English), or \"enojo\" (Spanish) and \"angry\" (English).\n\nAn arrow labeled \"Projection\" points to a second scatter plot on the right side of the image, where the words have been aligned more closely by finding their nearest neighbors. The pairs seem better aligned compared to the initial scatter plot.\n\nThere is also a table labeled \"Translation\" that provides example English-Spanish word pairs:\n- disorder - trastorno\n- recognize - reconocer\n- collapse - colapso\n- angry - enojado\n\nThe table indicates that the approach results in correct alignments despite the initial projection not perfectly aligning the word pairs in the shared embedding space.\nthe word embedding alignment using the given dictionary ( 2.2.2 ).\n\n \n3. For each word in the source language training data, translate it by ﬁnding its nearest neighbor in the shared embedding space ( § 2.2.3 ).\n\n 4. Train an NER model using the translated words along with the named entity tags from the En- glish corpus ( § 2.2.4 ). \nWe consider each in detail. \n2.2.1 Learning Monolingual Embeddings \nGiven text in the source and target language, we ﬁrst independently learn word embedding matri- ces    $X$   and    $Y$   in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages with any of the myriad of word embedding methods ( Mikolov et al. ,  2013b ;  Pennington et al. ,  2014 ;  Bojanowski et al. ,  2017 ). \n2.2.2 Learning Bilingual Embeddings \nNext, we learn a cross-lingual projection of    $X$  and  $Y$   into a shared space. Assume we are given a dictionary    $\\{x_{i},y_{i}\\}_{i=1}^{D}$  , where    $x_{i}$   and    $y_{i}$   denote the embeddings of a word pair. Let    $\\begin{array}{r l}{X_{D}}&{{}=}\\end{array}$   $[x_{1},x_{2},\\cdot\\cdot\\cdot,x_{D}]^{\\top}$  and    $Y_{D}~=~[y_{1},y_{2},\\cdot\\cdot\\cdot,y_{D}]^{\\top}$  denote two embedding matrices consisting of word pairs from the dictionary. \nFollowing previous work ( Zhang et al. ,  2016 ; Artetxe et al. ,  2016 ;  Smith et al. ,  2017 ), we opti- mize the following objective: \n\n$$\n\\operatorname*{min}_{W}\\sum_{i=1}^{d}\\|W x_{i}-y_{i}\\|^{2}\\;\\;\\mathrm{s.t.}\\;\\;W W^{\\top}=I,\n$$\n \njective can be further simpliﬁed as \n\n$$\n\\operatorname*{max}_{W}\\mathrm{Tr}(X_{D}W Y_{D}^{\\top})\\;\\;\\mathrm{s.t.}\\;\\;W W^{\\top}=I.\n$$\n \nHere, the transformation matrix    $W$   is constrained to be orthogonal so that the dot product similarity of words is invariant with respect to the transfor- mation both within and across languages. \nTo optimize the above objective (the Procrustes problem), we decompose the matrix    $Y_{D}^{\\top}X_{D}$   us- ing singular value decomposition. Let the results be    $Y_{D}^{\\top}\\bar{X}_{D}\\;=\\;U\\sum V^{\\top}$   P , then    $\\boldsymbol{W}\\,=\\,\\boldsymbol{U}\\boldsymbol{V}^{\\top}$  the exact solution. We deﬁne the similarity ma- trix between  X  Y  $Y$   to be  S  $S~=~Y W X^{\\top}~=~$   $Y U(X V)^{\\top}$  , where each column contains the co- sine similarity between source word  $x_{i}$   and all tar- get words  $y_{i}$  . We can then deﬁne    $X^{\\prime}=X V$  and  $Y^{\\prime}=Y U$  , which are    $X$   and    $Y$  transformed into a shared embedding space. \nTo reﬁne the alignment in this shared space fur- ther, we iteratively perform a self-learning reﬁne- ment step    $\\textit{k}^{2}$    times by:\n\n \n1. Using the aligned embeddings to generate a new dictionary that consists of mutual nearest neigh- bors obtained using the same metric as intro- duced below.\n\n 2. Solving the Procrustes problem based on the newly generated dictionary to get a new set of bilingual embeddings. \nThe bilingual embeddings at the end of the    $k$  th step,    $X_{k}^{\\prime}$    and    $Y_{k}^{\\prime}$  , will be used to perform trans- lation. \n2.2.3 Learning Word Translations \nTo learn actual word translations, we next pro- ceed to perform nearest-neighbor search in the where    $W$   is a square parameter matrix. This ob- \n"}
{"page": 3, "image_path": "doc_images/D18-1034_3.jpg", "ocr_text": "common space. Instead of using a common dis-\ntance metric such as cosine similarity, we adopt\nthe cross-domain similarity local scaling (CSLS)\nmetric (Lample et al., 2018), which is designed\nto address the hubness problem common to the\nshared embedding space (Dinu and Baroni, 2014).\nSpecifically,\n\nCSLS(x;, yj) = 2. cos(xi, yj) — rr (vi) — rs (ys)\n\nwhere r7(2;) Koen (ay) CO8(@i, Ye) de-\n\nnotes the mean cosine similarity between x; and\n\nits Kk neighbors y;. Using this metric, we find\n\ntranslations for each source word s by selecting\n\ntarget word t, where f, = arg max CSLS(x5, yz).\nt\n\n2.2.4 Training the NER Model\n\nFinally, we translate the entire English NER train-\ning data into the target language by taking English\nsentences S = 581, S9,..., Sp and translating them\ninto target sentences T = fi, fo, wet: The la-\nbel of each English word is copied to be the la-\nbel of the target word. We can then train an NER\nmodel directly using the translated data. Notably,\nbecause the model has access to the surface forms\nof the target sentences, it can use the character se-\nquences of the target language as part of its input.\n\nDuring learning, all word embeddings are nor-\nmalized to lie on the unit ball, allowing every\ntraining pair an equal contribution to the objective\nand improving word translation accuracy (Artetxe\net al., 2016). When training the NER model, how-\never, we do not normalize the word embeddings,\nbecause preliminary experiments showed the orig-\ninal unnormalized embeddings gave superior re-\nsults. We suspect this is due to frequency infor-\nmation conveyed by vector length, an important\nsignal for NER. (Named entities appear less fre-\nquently in the monolingual corpus.)\n\n2.3. Discussion\n\nFigure | shows an example of the embeddings and\ntranslations learned with our approach trained on\nSpanish and English data from the experiments\n(see §4 for more details). As shown in the figure,\nthere is usually a noticeable difference between\nthe word embeddings of a word pair in different\nlanguages, which is inevitable because different\nlanguages have distinct traits and different mono-\nlingual data, and as a result it is intrinsically hard\nto learn a perfect alignment. This indicates that\nmodels trained directly on data using the source\n\n372\n\nCRF\nLayer\n\nGlobal\nContext\n\nLST\nOutput\n\nWord\nEmbedding\nChar\nEmbedding\n\ni\n\nflights\n\ny,\n7\n\n—_|\n\n> F\nA\nNe,\n\nFigure 2: Self-attentive Bi-LSTM-CRF Model\n\nembeddings may not generalize well to the slightly\ndifferent embeddings of the target language.\n\nInstead of directly modeling the shared embed-\nding space (Guo et al., 2015; Zhang et al., 2016;\nFang and Cohn, 2017; Ni et al., 2017), we lever-\nage the shared embedding space for word transla-\ntion. As shown in Figure 1, unaligned word pairs\ncan still be translated correctly with our method, as\nthe embeddings are still closer to the correct trans-\nlations than the closest incorrect one.\n\n3. NER Model Architecture\n\nWe describe the model we use to perform NER.\nWe will first describe the basic hierarchical neural\nCRF tagging model (Lample et al., 2016; Ma and\nHovy, 2016; Yang et al., 2016), and introduce the\nself-attention mechanism that we propose to deal\nwith divergence of word order.\n\n3.1 Hierarchical Neural CRF\n\nThe hierarchical CRF model consists of three\ncomponents: a character-level neural network, ei-\nther an RNN or a CNN, that allows the model to\ncapture subword information, such as morpholog-\nical variations and capitalization patterns; a word-\nlevel neural network, usually an RNN, that con-\nsumes word representations and produces context\n", "vlm_text": "common space. Instead of using a common dis- tance metric such as cosine similarity, we adopt the cross-domain similarity local scaling (CSLS) metric ( Lample et al. ,  2018 ), which is designed to address the hubness problem common to the shared embedding space ( Dinu and Baroni ,  2014 ). Speciﬁcally,\n\n$$\n\\mathrm{CS}(x_{i},y_{j})=2\\cos(x_{i},y_{j})-r_{T}(x_{i})-r_{S}(y_{j})\n$$\n \nwhere    $\\begin{array}{r l r}{r_{T}(x_{i})}&{{}\\!=\\!}&{{\\frac{1}{K}}\\sum_{y_{t}\\in N_{T}(x_{i})}\\cos(x_{i},y_{t})}\\end{array}$  P  de- ∈ notes the mean cosine similarity between    $x_{i}$   and its    $K$   neighbors    $y_{t}$  . Using this metric, we ﬁnd translations for each source word    $s$   by selecting target word  $\\hat{t_{s}}$   where  $\\hat{t_{s}}=\\arg\\operatorname*{max}_{t}\\mathrm{CSLS}(x_{s},y_{t})$  . \n2.2.4 Training the NER Model \nFinally, we translate the entire English NER train- ing data into the target language by taking English sentences    $S\\,=\\,s_{1},s_{2},...,s_{n}$   and translating them into target sentences  $\\hat{T}\\;=\\;t_{1},t_{2},...,t_{n}$  . The la- bel of each English word is copied to be the la- bel of the target word. We can then train an NER model directly using the translated data. Notably, because the model has access to the surface forms of the target sentences, it can use the character se- quences of the target language as part of its input. \nDuring learning, all word embeddings are nor- malized to lie on the unit ball, allowing every training pair an equal contribution to the objective and improving word translation accuracy ( Artetxe et al. ,  2016 ). When training the NER model, how- ever, we do not normalize the word embeddings, because preliminary experiments showed the orig- inal unnormalized embeddings gave superior re- sults. We suspect this is due to frequency infor- mation conveyed by vector length, an important signal for NER. (Named entities appear less fre- quently in the monolingual corpus.) \n2.3 Discussion \nFigure  1  shows an example of the embeddings and translations learned with our approach trained on Spanish and English data from the experiments (see  $\\S4$   for more details). As shown in the ﬁgure, there is usually a noticeable difference between the word embeddings of a word pair in different languages, which is inevitable because different languages have distinct traits and different mono- lingual data, and as a result it is intrinsically hard to learn a perfect alignment. This indicates that models trained directly on data using the source \nThe image depicts a detailed architecture of a Self-attentive Bi-LSTM-CRF model. This model is often used for tasks like Named Entity Recognition (NER) in Natural Language Processing. Here's a breakdown of the components visible in the image:\n\n1. **Char Bi-LSTM**: \n   - This layer processes character-level information of the words in the input sentence. It uses a Bidirectional LSTM (Bi-LSTM) to capture character-level features and create character embeddings for each word.\n\n2. **Char Embedding**: \n   - Represents character-level embeddings derived from the Char Bi-LSTM layer.\n\n3. **Word Embedding**: \n   - Represents word-level embeddings, capturing the semantic representation of each word in a vector form.\n\n4. **Word Bi-LSTM**: \n   - A Bidirectional LSTM layer processes the concatenation of char and word embeddings, allowing the model to consider context from both directions, which is useful for understanding the sentence’s overall meaning.\n\n5. **Masked Self-attention**: \n   - This component applies self-attention with a mask to focus on relevant parts of the input, which helps the model weigh different positions of the input, enhancing its understanding of relationships between words.\n\n6. **Global Context**: \n   - Captures the context of entire sentences beyond the individual representations gained from LSTM outputs, allowing for improved understanding and prediction.\n\n7. **CRF Layer**: \n   - The Conditional Random Field layer is used for sequence prediction, decoding the optimal label sequence for the input, which is valuable in structured prediction tasks like NER.\n   - It uses label sequences such as `O`, `B-LOC`, and `I-LOC` to tag parts of the sentence, where:\n     - `O` indicates tokens that are not named entities.\n     - `B-LOC` marks the beginning of a location entity.\n     - `I-LOC` marks the inside of a location entity.\n\nThe image illustrates the flow of data and transformations from raw input text (\"flights from Diego\") through embeddings, processing layers, attention mechanisms, and finally, sequence tagging by the CRF layer.\nembeddings may not generalize well to the slightly different embeddings of the target language. \nInstead of directly modeling the shared embed- ding space ( Guo et al. ,  2015 ;  Zhang et al. ,  2016 ; Fang and Cohn ,  2017 ;  Ni et al. ,  2017 ), we lever- age the shared embedding space for word transla- tion. As shown in Figure  1 , unaligned word pairs can still be translated correctly with our method, as the embeddings are still closer to the correct trans- lations than the closest incorrect one. \n3 NER Model Architecture \nWe describe the model we use to perform NER. We will ﬁrst describe the basic hierarchical neural CRF tagging model ( Lample et al. ,  2016 ;  Ma and Hovy ,  2016 ;  Yang et al. ,  2016 ), and introduce the self-attention mechanism that we propose to deal with divergence of word order. \n3.1 Hierarchical Neural CRF \nThe hierarchical CRF model consists of three components: a character-level neural network, ei- ther an RNN or a CNN, that allows the model to capture subword information, such as morpholog- ical variations and capitalization patterns; a word- level neural network, usually an RNN, that con- sumes word representations and produces context sensitive hidden representations for each word; and a linear-chain CRF layer that models the de- pendency between labels and performs inference. "}
{"page": 4, "image_path": "doc_images/D18-1034_4.jpg", "ocr_text": "sensitive hidden representations for each word;\nand a linear-chain CRF layer that models the de-\npendency between labels and performs inference.\n\nIn this paper, we closely follow the architecture\nproposed by Lample et al. (2016), and use bi-\ndirectional LSTMs for both the character level and\nword level neural networks. Specifically, given\nan input sequence of words (w , wa, ..., Wn), and\neach word’s corresponding character sequence,\nthe model first produces a representation for each\nword, x;, by concatenating its character rep-\nresentation with its word embedding. Subse-\nquently, the word representations of the input se-\nquence (21, 22,-++ ,U,»,) are fed into a word level\nBi-LSTM, which models the contextual depen-\ndency within each sentence and outputs a se-\nquence of context sensitive hidden representations\n(hi, h2,-++ , hn). A CRF layer is then applied\non top of the word level LSTM and takes in as\nits input the sequence of hidden representations\n(hi, he,--+ , hn), and defines the joint distribution\nof all possible output label sequences. The Viterbi\nalgorithm is used during decoding.\n\n3.2 Self-Attention\n\nThe training-time inputs to our model are in\nessence corrupted sentences from the target lan-\nguage (e.g., Spanish), which have a different or-\nder from natural target sentences. We propose to\nalleviate this problem by adding a self-attention\nlayer (Vaswani et al., 2017) on top of the word-\nlevel Bi-LSTM. Self-attention provides each word\nwith a context feature vector based on all the\nwords of a sentence. As the context vectors are\nobtained irrespective of the words’ positions in a\nsentence, at test time, the model is more likely to\nsee vectors similar to those seen at training time,\nwhich we posit introduces a level of flexibility\nwith respect to the word order, and thus may al-\nlow for better generalization.\n\nLet H = [hi,ho,-+- ,hn]' be a sequence of\nword-level hidden representations. We apply a\nsingle layer MLP on H to obtain the queries Q\nand keys K = tanh(HW +b), where W ¢ R?*¢\nis a parameter matrix and b € R? is a bias term,\nwith d being the hidden state size. The output of\nattention layer is defined as:\n\nH* = softmax(QK') © (E—I)H\n\n= [ht n$,..., hg]\n\nwhere J is an identity matrix and F is an all-one\n\nmatrix. The term (£ — J) serves as an atten-\ntion mask that prevents the weights from center-\ning on the word itself, as we would like to provide\neach word with sentence level context. The out-\nputs from the self-attention layer are then concate-\nnated with the original hidden representations to\nform the final inputs to the CRF layer, which are\n({hi, h{], [he, 3], ..., [hg, 23])-\n\n4 Experiments\n\nTo examine the effectiveness of both of our pro-\nposed methods, we conduct four sets of experi-\nments. First, we evaluate our model both with\nand without provided dictionaries on a benchmark\nNER dataset and compare with previous state-of-\nhe-art results. Second, we compare our meth-\nods against a recently proposed dictionary-based\ntranslation baseline (Mayhew et al., 2017) by di-\nrectly applying our model on their translated data.?\nSubsequently, we conduct an ablation study to fur-\nher understand our proposed methods. Lastly,\nwe apply our methods to a truly low-resource lan-\nguage, Uyghur.\n\n4.1 Experimental Settings\n\nWe evaluate our proposed methods on the bench-\nmark CoNLL 2002 and 2003 NER datasets\n(Tjong Kim Sang, 2002; Tjong Kim Sang and\nDe Meulder, 2003), which contain 4 European lan-\nguages, English, German, Dutch and Spanish. For\nall experiments, we use English as the source lan-\nguage and translate its training data into the target\nlanguage. We train a model on the translated data,\nand test it on the target language. For each exper-\niment, we run our models 5 times using different\nseeds and report the mean and standard deviation,\nas suggested by Reimers and Gurevych (2017).\nWord Embeddings For all languages, we use\ntwo different embedding methods, fastText (Bo-\njanowski et al., 2017) and GloVe (Pennington\net al., 2014), to perform word-embedding based\ntranslations and train the NER model, respectively.\nFor fastText, we use the publicly available em-\nbeddings trained on Wikipedia for all languages.\nFor GloVe, we use the publicly available embed-\ndings pre-trained on Gigaword and Wikipedia for\nEnglish. For Spanish, German and Dutch, we\nuse Spanish Gigaword and Wikipedia, German\nWMT News Crawl data and Wikipedia, and Dutch\n\n3We thank the authors of Mayhew et al. (2017) for shar-\ning their data.\n\n373\n", "vlm_text": "\nIn this paper, we closely follow the architecture proposed by Lample et al.  ( 2016 ), and use bi- directional LSTMs for both the character level and word level neural networks. Speciﬁcally, given an input sequence of words    $(w_{1},w_{2},...,w_{n})$  , and each word’s corresponding character sequence, the model ﬁrst produces a representation for each word,    $x_{i}$  , by concatenating its character rep- resentation with its word embedding. Subse- quently, the word representations of the input se- quence    $\\left(x_{1},x_{2},\\cdot\\cdot\\cdot,x_{n}\\right)$   are fed into a word level Bi-LSTM, which models the contextual depen- dency within each sentence and outputs a se- quence of context sensitive hidden representations  $\\left(h_{1},h_{2},\\cdot\\cdot\\cdot,h_{n}\\right)$  . A CRF layer is then applied on top of the word level LSTM and takes in as its input the sequence of hidden representations  $\\left(h_{1},h_{2},\\cdot\\cdot\\cdot,h_{n}\\right)$  , and deﬁnes the joint distribution of all possible output label sequences. The Viterbi algorithm is used during decoding. \n3.2 Self-Attention \nThe training-time inputs to our model are in essence corrupted sentences from the target lan- guage (e.g., Spanish), which have a different or- der from natural target sentences. We propose to alleviate this problem by adding a self-attention layer ( Vaswani et al. ,  2017 ) on top of the word- level Bi-LSTM. Self-attention provides each word with a context feature vector based on  all  the words of a sentence. As the context vectors are obtained irrespective of the words’ positions in a sentence, at test time, the model is more likely to see vectors similar to those seen at training time, which we posit introduces a level of ﬂexibility with respect to the word order, and thus may al- low for better generalization. \nLet    $H\\;=\\;[h_{1},h_{2},\\cdot\\cdot\\cdot\\;,h_{n}]^{\\top}$  be a sequence of word-level hidden representations. We apply a single layer MLP on    $H$   to obtain the queries    $Q$  and keys    $K=\\operatorname{tanh}(H W+b)$  ere    $W\\in\\mathbb{R}^{d\\times d}$  is a  rameter matrix and  $b\\,\\in\\,\\mathbb{R}^{d}$   ∈   is a bias term, with  d  being the hidden state size. The output of attention layer is deﬁned as: \n\n$$\n\\begin{array}{r l}&{H^{a}=\\mathrm{softmax}(Q K^{\\top})\\odot(E-I)H}\\\\ &{\\quad\\quad=[h_{1}^{a},h_{2}^{a},...,h_{3}^{a}]^{\\top}}\\end{array}\n$$\n \nwhere  $I$   is an identity matrix and    $E$   is an all-one matrix. The term    $(E\\mathrm{~-~}I)$   serves as an atten- tion mask that prevents the weights from center- ing on the word itself, as we would like to provide each word with sentence level context. The out- puts from the self-attention layer are then concate- nated with the original hidden representations to form the ﬁnal inputs to the CRF layer, which are  $\\left([h_{1},h_{1}^{a}],[h_{2},h_{2}^{a}],...,[h_{3},h_{3}^{a}]\\right)$  . \n\n4 Experiments \nTo examine the effectiveness of both of our pro- posed methods, we conduct four sets of experi- ments. First, we evaluate our model both with and without provided dictionaries on a benchmark NER dataset and compare with previous state-of- the-art results. Second, we compare our meth- ods against a recently proposed dictionary-based translation baseline ( Mayhew et al. ,  2017 ) by di- rectly applying our model on their translated data. Subsequently, we conduct an ablation study to fur- ther understand our proposed methods. Lastly, we apply our methods to a truly low-resource lan- guage, Uyghur. \n4.1 Experimental Settings \nWe evaluate our proposed methods on the bench- mark CoNLL 2002 and 2003 NER datasets ( Tjong Kim Sang ,  2002 ;  Tjong Kim Sang and De Meulder ,  2003 ), which contain 4 European lan- guages, English, German, Dutch and Spanish. For all experiments, we use English as the source lan- guage and translate its training data into the target language. We train a model on the translated data, and test it on the target language. For each exper- iment, we run our models 5 times using different seeds and report the mean and standard deviation, as suggested by  Reimers and Gurevych  ( 2017 ). \nWord Embeddings  For all languages, we use two different embedding methods, fastText ( Bo- janowski et al. ,  2017 ) and GloVe ( Pennington et al. ,  2014 ), to perform word-embedding based translations and train the NER model, respectively. For fastText, we use the publicly available em- beddings trained on Wikipedia for all languages. For GloVe, we use the publicly available embed- dings pre-trained on Gigaword and Wikipedia for English. For Spanish, German and Dutch, we use Spanish Gigaword and Wikipedia, German WMT News Crawl data and Wikipedia, and Dutch Wikipedia, respectively, to train the GloVe word embeddings. We use a vocabulary size of 100,000 for both embedding methods. "}
{"page": 5, "image_path": "doc_images/D18-1034_5.jpg", "ocr_text": "Wikipedia, respectively, to train the GloVe word\nembeddings. We use a vocabulary size of 100,000\nfor both embedding methods.\n\nDictionary We consider three different settings\nto obtain the seed dictionary, including two meth-\nods that do not use parallel resources:\n\n1. Use identical character strings shared between\n\nthe two vocabularies as the seed dictionary.\n\n. Lample et al. (2018)’s method of using adver-\nsarial learning to induce a mapping that aligns\nthe two embedding spaces, and the mutual near-\nest neighbors in the shared space will be used as\na dictionary. The learning procedure is formu-\nlated as a two player game, where a discrim-\ninator is trained to distinguish words from the\ntwo embedding spaces, and a linear mapping is\ntrained to align the two embedding spaces and\nthus fool the discriminator.\n\n. Use a provided dictionary. In our experiments,\nwe use the ones provided by Lample et al.\n(2018),* each of which contain 5,000 source\nwords and about 10,000 entries.\n\nTranslation We follow the general procedure\ndescribed in Section 2, and replace each word\nfrom the English training data with its correspond-\ning word in the target language. For out-of-\nvocabulary (OOV) words, we simply keep them\nas-is. We capitalize the resulting sentences fol-\nlowing the pattern of the original English words.\nNote that for German, simply following the En-\nglish capitalization pattern does not work, because\nall nouns in German are capitalized. To handle\nthis problem, we count the number of times each\nword is capitalized in Wikipedia, and capitalize\nthe word if the probability is greater than 0.6.\n\nNetwork Parameters For our experiments, we\nset the character embedding size to be 25, char-\nacter level LSTM hidden size to be 50, and word\nlevel LSTM hidden size to be 200. For OOV\nwords, we initialize an unknown embedding by\n3 3\n\nuniformly sampling from range [/ 5, +/ <5]:\nwhere emb is the size of embedding, 100 in our\ncase. We replace each number with 0 when used\nas input to the character level Bi-LSTM.\nNetwork Training We use SGD with momen-\ntum to train the NER model for 30 epochs, and\nselect the best model on the target language de-\nvelopment set. We choose the initial learning rate\n4\n\nhttps://github.com/facebookresearch/\nMUSE\n\n374\n\nto be 79 = 0.015, and update it using a learning\ndecay mechanism after each epoch, 7, = ara\nwhere ¢ is the number of completed epoch and\np 0.05 is the decay rate. We use a batch\nsize of 10 and evaluate the model per 150 batches\nwithin each epoch. We apply dropout on the in-\nputs to the word-level Bi-LSTM, the outputs of\nthe word-level Bi-LSTM, and the outputs of the\nself-attention layer to prevent overfitting. The self-\nattention dropout rate is set to 0.5 when using\nour translated data, and 0.2 when using cheap-\ntranslation data. We use 0.5 for all other dropouts.\nThe word embeddings are not fine-tuned during\ntraining.\n\n4.2 Results\n\nTable 1 presents our results on transferring from\nEnglish to three other languages, alongside results\nfrom previous studies. Here “BWET” (bilingual\nword embedding translation) denotes using the hi-\nerarchical neural CRF model trained on data trans-\nlated from English. As can be seen from the ta-\nble, our methods outperform previous state-of-the-\nart results on Spanish and Dutch by a large mar-\ngin and perform competitively on German even\nwithout using any parallel resources. We achieve\nsimilar results using different seed dictionaries,\nand produce the best results when adding the self-\nattention mechanism to our model.\n\nDespite the good performance on Spanish and\nDutch, our model does not outperform the previ-\nous best result on German, and we speculate that\nhere are a few reasons. First, German has rich\nmorphology and contains many compound words,\nmaking the word embeddings less reliable. Our\nsupervised result on German indicates the same\nproblem, as it is about 8 F) points worse than\nSpanish and Dutch. Second, these difficulties be-\ncome more pronounced in the cross-lingual set-\ning, leading to a noisier embedding space align-\nment, which lowers the quality of BWE-based\ntranslation. We believe that this is a problem\nwith all methods using word embeddings. In such\ncases, more resource-intensive methods may be\nnecessary.\n\n4.2.1 Comparison with Dictionary-Based\nTranslation\n\nTable | also presents results of a comparison be-\n\ntween our proposed BWE translation method and\n\nthe “cheap translation” baseline of (Mayhew et al.,\n\n2017). The size of the dictionaries used by both\n", "vlm_text": "\nDictionary  We consider three different settings to obtain the seed dictionary, including two meth- ods that do not use parallel resources:\n\n \n1. Use identical character strings shared between the two vocabularies as the seed dictionary.\n\n \n2.  Lample et al.  ( 2018 )’s method of using adver- sarial learning to induce a mapping that aligns the two embedding spaces, and the mutual near- est neighbors in the shared space will be used as a dictionary. The learning procedure is formu- lated as a two player game, where a discrim- inator is trained to distinguish words from the two embedding spaces, and a linear mapping is trained to align the two embedding spaces and thus fool the discriminator.\n\n \n3. Use a provided dictionary. In our experiments, we use the ones provided by  Lample et al. ( 2018 ),   each of which contain 5,000 source words and about 10,000 entries. \nTranslation  We follow the general procedure described in Section  2 , and replace each word from the English training data with its correspond- ing word in the target language. For out-of- vocabulary (OOV) words, we simply keep them as-is. We capitalize the resulting sentences fol- lowing the pattern of the original English words. Note that for German, simply following the En- glish capitalization pattern does not work, because all nouns in German are capitalized. To handle this problem, we count the number of times each word is capitalized in Wikipedia, and capitalize the word if the probability is greater than  0 . 6 . \nNetwork Parameters  For our experiments, we set the character embedding size to be 25, char- acter level LSTM hidden size to be 50, and word level LSTM hidden size to be 200. For OOV words, we initialize an unknown embedding by uniformly sampling from range  $\\textstyle[{\\sqrt{\\frac{3}{\\mathrm{emb}}}},+{\\sqrt{\\frac{3}{\\mathrm{emb}}}}]$  q q , where emb is the size of embedding, 100 in our case. We replace each number with 0 when used as input to the character level Bi-LSTM. \nNetwork Training  We use SGD with momen- tum to train the NER model for 30 epochs, and select the best model on the target language de- velopment set. We choose the initial learning rate to be    $\\eta_{\\mathrm{0}}\\,=\\,0.015$  , and update it using a learning decay mechanism after each epoch,    $\\begin{array}{r}{\\eta_{t}\\ =\\ \\frac{\\eta_{0}}{1+\\rho t}}\\end{array}$  , where    $t$   is the number of completed epoch and  $\\rho~=~0.05$   is the decay rate. We use a batch size of 10 and evaluate the model per 150 batches within each epoch. We apply dropout on the in- puts to the word-level Bi-LSTM, the outputs of the word-level Bi-LSTM, and the outputs of the self-attention layer to prevent overﬁtting. The self- attention dropout rate is set to 0.5 when using our translated data, and 0.2 when using cheap- translation data. We use 0.5 for all other dropouts. The word embeddings are not ﬁne-tuned during training. \n\n4.2 Results \nTable  1  presents our results on transferring from English to three other languages, alongside results from previous studies. Here “BWET” (bilingual word embedding translation) denotes using the hi- erarchical neural CRF model trained on data trans- lated from English. As can be seen from the ta- ble, our methods outperform previous state-of-the- art results on Spanish and Dutch by a large mar- gin and perform competitively on German even without using any parallel resources. We achieve similar results using different seed dictionaries, and produce the best results when adding the self- attention mechanism to our model. \nDespite the good performance on Spanish and Dutch, our model does not outperform the previ- ous best result on German, and we speculate that there are a few reasons. First, German has rich morphology and contains many compound words, making the word embeddings less reliable. Our supervised result on German indicates the same problem, as it is about 8    $F_{1}$   points worse than Spanish and Dutch. Second, these difﬁculties be- come more pronounced in the cross-lingual set- ting, leading to a noisier embedding space align- ment, which lowers the quality of BWE-based translation. We believe that this is a problem with all methods using word embeddings. In such cases, more resource-intensive methods may be necessary. \n4.2.1 Comparison with Dictionary-Based Translation \nTable  1  also presents results of a comparison be- tween our proposed BWE translation method and the “cheap translation” baseline of ( Mayhew et al. , 2017 ). The size of the dictionaries used by both "}
{"page": 6, "image_path": "doc_images/D18-1034_6.jpg", "ocr_text": "Model Spanish Dutch German Extra Resources\n* Tickstrém et al. (2012) 59.30 58.40 40.40 parallel corpus\n* Nothman et al. (2013) 61.0 64.00 55.80 Wikipedia\n* Tsai et al. (2016) 60.55 61.60 48.10 Wikipedia\n* Niet al. (2017) 65.10 65.40 58.50 Wikipedia, parallel corpus, 5K dict.\n*T Mayhew et al. (2017) 65.95 66.50 59.11 Wikipedia, 1M dict.\n* Mayhew et al. (2017) (only Eng. data) 51.82 53.94 50.96 IM dict.\nOur methods:\nBWET (id.c.) 71.14+0.60 | 70.24+1.18 | 57.03+0.25 | -\nBWET (id.c.) + self-att. 72.37 + 0.65 | 70.4041.16 | 57.76 +0.12 | —\nBWET (adv.) 70.54 +£0.85 | 70.134+1.04 | 55.7140.47 | -\nBWET (adv.) + self-att. 71.03 +0.44 | 71.25 40.79 | 56.90+0.76 | —\nBWET 71.33 41.26 | 69.39+0.53 | 56.95+1.20 | 10K dict.\nBWET + self-att. 71.67 £0.86 | 70.904 1.09 | 57.43+0.95 | 10K dict.\n*  BWET on data from Mayhew et al. (2017) 66.53 £1.12 | 69.24+0.66 | 55.39+0.98 | 1M dict.\n*  BWET + self-att. on data from Mayhew et al. (2017) | 66.90 £0.65 | 69.31+0.49 | 55.98+0.65 | 1M dict.\n* Our supervised results. 86.26 + 0.40 | 86.40+0.17 | 78.16+0.45 | annotated corpus\n\nTable 1: NER F\\ scores. *Approaches that use more resources than ours (“Wikipedia” means Wikipedia is used\nnot as a monolingual corpus, but to provide external knowledge). ‘Approaches that use multiple languages for\ntransfer. “Only Eng. data” is the model used in Mayhew et al. (2017) trained on their data translated from English\nwithout using Wikipedia and other languages. The “data from Mayhew et al. (2017)” is the same data translated\nfrom only English they used. “Id.c.” indicates using identical character strings between the two languages as\nthe seed dictionary. “Adv.” indicates using adversarial training and mutual nearest neighbors to induce a seed\ndictionary. Our supervised results are obtained using models trained on annotated corpus from CoNLL.\n\napproaches are given in the right-most column.\nUsing our model on their translated data from En-\nglish outperforms the baseline scores produced by\ntheir models over all languages, a testament to the\nstrength of our neural CRF baseline. The results\nproduced by our model on their data indicate that\nour approach is effective, as we manage to outper-\nform their approaches on all three languages using\nmuch smaller dictionaries and even without dictio-\nnaries. Also, we see that self-attention is effective\nwhen applied on their data, which also does not\ncarry the correct word order.\n\n4.2.2. Why Does Translation Work Better?\n\nIn this section, we study the effects of differ-\nent ways of using bilingual word embeddings and\nthe resulting induced translations. As we pointed\nout previously, finding translations has two advan-\ntages: (1) the model can be trained on the exact\npoints from the target embedding space, and (2)\nthe model has access to the target language’s orig-\ninal character sequences. Here, we conduct abla-\ntion studies over these two variables. Specifically,\nwe consider the following three variants.”\n\ne Common space This is the most common set-\nting for using bilingual word embeddings, and\nhas recently been applied in NER (Ni et al.,\n2017). In short, the source and target word em-\nbeddings are cast into a common space, namely\n\nIn this study, we use GloVe for learning bilingual embed-\ndings and word translations instead of fastText.\n\n375\n\nX' = XV and Y’ = YU, and the model is\ntrained with the source side embedding and the\nsource character sequence, and directly applied\non the target side.\n\ne Replace In this setting, we replace each original\nword embedding x; with its nearest neighbor y;\nin the common space but do not perform trans-\nlation. This way, the model will be trained with\ntarget word embeddings and source-side char-\nacter sequences.\n\ne Translation This is our proposed approach,\nwhere the model is trained on both exact points\nin the target space and target language character\nsequences.\n\nThe three variants are compared in Table 2.\nThe “common space” variant performs the worst\nby a large margin, confirming our hypothesis that\ndiscrepancy between the two embedding spaces\nharms the model’s ability to generalize. From the\ncomparison between the “replace” and “transla-\ntion,” we observe that having access to the target\nlanguage’s character sequence helps performance,\nespecially for German, perhaps due in part to its\ncapitalization patterns, which differ from English.\nIn this case, we have to lower-case all the words\nfor character inputs in order to prevent the model\nfrom overfitting the English capitalization pattern.\n", "vlm_text": "This table displays a comparative analysis of different models and methods for Spanish, Dutch, and German text processing or translation tasks, using specific extra resources. The table compares the performance of various models and methods, with scores given for each language (Spanish, Dutch, and German). The models listed include previous works by Täckström et al. (2012), Nothman et al. (2013), Tsai et al. (2016), Ni et al. (2017), and Mayhew et al. (2017), with some models marked with asterisks or other symbols possibly indicating special notes or conditions relevant to the study. \n\nThe \"Our methods\" section presents new methods abbreviated as \"BWET\" with variations or enhancements like \"self-att.\" The performance scores for these methods are shown with a mean and standard deviation. The listed extra resources detail the type of resources utilized by each method or model, such as parallel corpora, Wikipedia, different sizes of dictionaries (5K dict., 10K dict., 1M dict.), and annotated corpora. Some entries under \"Extra Resources\" have a dash (\"–\"), likely indicating no additional resources were used for those particular methods.\nTable 1: NER    $F_{1}$   scores.   ∗ Approaches that use more resources than ours (“Wikipedia” means Wikipedia is used not as a monolingual corpus, but to provide external knowledge). † Approaches that use multiple languages for transfer. “Only Eng. data” is the model used in  Mayhew et al.  ( 2017 ) trained on their data translated from English without using Wikipedia and other languages. The “data from  Mayhew et al.  ( 2017 )” is the same data translated from only English they used. “Id.c.” indicates using identical character strings between the two languages as the seed dictionary. “Adv.” indicates using adversarial training and mutual nearest neighbors to induce a seed dictionary. Our supervised results are obtained using models trained on annotated corpus from CoNLL. \napproaches are given in the right-most column. Using our model on their translated data from En- glish outperforms the baseline scores produced by their models over all languages, a testament to the strength of our neural CRF baseline. The results produced by our model on their data indicate that our approach is effective, as we manage to outper- form their approaches on all three languages using much smaller dictionaries and even without dictio- naries. Also, we see that self-attention is effective when applied on their data, which also does not carry the correct word order. \n4.2.2 Why Does Translation Work Better? \nIn this section, we study the effects of differ- ent ways of using bilingual word embeddings and the resulting induced translations. As we pointed out previously, ﬁnding translations has two advan- tages: (1) the model can be trained on the exact points from the target embedding space, and (2) the model has access to the target language’s orig- inal character sequences. Here, we conduct abla- tion studies over these two variables. Speciﬁcally, we consider the following three variants. \n•  Common space  This is the most common set- ting for using bilingual word embeddings, and has recently been applied in NER ( Ni et al. , 2017 ). In short, the source and target word em- beddings are cast into a common space, namely  $X^{\\prime}\\;=\\;X V$   and    $Y^{\\prime}\\;=\\;Y U$  , and the model is trained with the source side embedding and the source character sequence, and directly applied on the target side.\n\n \n\n•  Replace  In this setting, we replace each original word embedding    $x_{i}$   with its nearest neighbor    $y_{i}$  in the common space but do not perform trans- lation. This way, the model will be trained with target word embeddings and source-side char- acter sequences.\n\n \n•  Translation  This is our proposed approach, where the model is trained on both exact points in the target space and target language character sequences. \nThe three variants are compared in Table  2 . The “common space” variant performs the worst by a large margin, conﬁrming our hypothesis that discrepancy between the two embedding spaces harms the model’s ability to generalize. From the comparison between the “replace” and “transla- tion,” we observe that having access to the target language’s character sequence helps performance, especially for German, perhaps due in part to its capitalization patterns, which differ from English. In this case, we have to lower-case all the words for character inputs in order to prevent the model from overﬁtting the English capitalization pattern. "}
{"page": 7, "image_path": "doc_images/D18-1034_7.jpg", "ocr_text": "Model Spanish Dutch German\n\nCommon space | 65.40 41.22 | 66.15 + 1.62 | 43.73 +0.94\nReplace 68.21 41.22 | 69.37+1.33 | 48.59 +1.21\nTranslation 69.21 + 0.95 | 69.39 + 1.21 | 53.94 + 0.66\n\nTable 2: Comparison of different ways of using bilingual word embeddings, within our method (NER F\\).\n\nModel Uyghur Unsequestered Set | Extra Resources\n*T Mayhew et al. (2017) 51.32 Wikipedia, 100K dict.\n* Mayhew et al. (2017) (only Eng. data) 27.20 Wikipedia, 100K dict.\nBWET 25.73 + 0.89 SK dict.\nBWET + self-att. 26.38 - SK dict.\n\n* BWET on data from Mayhew et al. (2017)\n\n30.20 Wikipedia, 100K dict.\n\n* BWET + self-att. on data from Mayhew et al. (2017) | 30.68 + 0.45 Wikipedia, 100K dict.\n* Combined (see text) 31.61 + 0.46 Wikipedia, 100K dict., 5K dict.\n* Combined + self-att. 32.09 + 0.61 Wikipedia, 100K dict., 5K dict.\n\nTable 3: NER F scores on Uyghur. *Approaches using language-specific features and resources (“Wikipedia”\n\nmeans Wikipedia is used not as a monolingual corpus,\n\nbut to provide external knowledge). 'Approaches that\n\ntransfer from multiple languages and use language-specific techniques.\n\n4.3 Case Study: Uyghur\n\nIn this section, we directly apply our approach\nto Uyghur, a truly low-resource language with\nvery limited monolingual and parallel resources.\nWe test our model on 199 annotated evaluation\ndocuments from the DARPA LORELEI program\n(the “unsequestered set’) and compare with previ-\nously reported results in the cross-lingual setting\nby Mayhew et al. (2017). Similar to our previous\nexperiments, we transfer from English, use fast-\nText embeddings trained on Common Crawl and\nWikipedia® and a provided dictionary to perform\ntranslation, and use GloVe trained on a monolin-\ngual corpus that has 30 million tokens to perform\nNER. Results are presented in Table 3.\n\nOur method performs competitively, consid-\nering that we use a much smaller dictionary\nthan Mayhew et al. (2017) and no knowledge from\nWikipedia in Uyghur. Our best results come from\na combined approach: using word embeddings to\ntranslate words that are not covered by Mayhew\net al. (2017)’s dictionary (last line of Table 3).\nNote that for the CoNLL languages, Mayhew\net al. (2017) used Wikipedia for the Wikifier fea-\ntures (Tsai et al., 2016), while for Uyghur they\nused it for translating named entities, which is cru-\ncial for low-resource languages when some named\nentities are not covered by the dictionary or the\ntranslation is not reliable. We suspect that the un-\nreliable translation of named entities is the ma-\n\n®nttps://github.com/facebookresearch/\nfastText/blob/master/docs/crawl-vectors.\nmd\n\njor reason why our method alone performs worse\nbut performs better when combined with their data\nthat has access to higher quality translations of\nnamed entities.\n\nThe table omits results using adversarial learn-\ning and identical character strings, as both failed\n(F, scores around 10). We attribute these failures\nto the low quality of Uyghur word embeddings and\nthe fact that the two languages are distant. Also,\nUyghur is mainly written in Arabic script, mak-\ning the identical character method inappropriate.\nOverall, this reveals a practical challenge for mul-\ntilingual embedding methods, where the underly-\ning distributions of the text in the two languages\nare divergent.\n\n5 Related Work\n\nCross-Lingual Learning Cross-lingual learning\napproaches can be loosely classified into two\ncategories: annotation projection and language-\nindependent transfer.\n\nAnnotation projection methods create training\ndata by using parallel corpora to project annota-\ntions from the source to the target language. Such\napproaches have been applied to many tasks un-\nder the cross-lingual setting, such as POS tag-\nging (Yarowsky et al., 2001; Das and Petrov, 2011;\nTackstrém et al., 2013; Fang and Cohn, 2016),\nmention detection (Zitouni and Florian, 2008) and\nparsing (Hwa et al., 2005; McDonald et al., 2011).\n\nLanguage independent transfer-based ap-\nproaches build models using language indepen-\ndent and delexicalized features. For instance,\n\n376\n", "vlm_text": "The table presents data comparing three models (Common space, Replace, and Translation) across three languages (Spanish, Dutch, and German). \n\nFor each language and model, performance is given as a value ± a margin, likely representing a mean and some measure of variability (e.g., standard deviation or standard error).\n\n- **Spanish:**\n  - Common space: 65.40 ± 1.22\n  - Replace: 68.21 ± 1.22\n  - Translation: 69.21 ± 0.95\n\n- **Dutch:**\n  - Common space: 66.15 ± 1.62\n  - Replace: 69.37 ± 1.33\n  - Translation: 69.39 ± 1.21\n\n- **German:**\n  - Common space: 43.73 ± 0.94\n  - Replace: 48.59 ± 1.21\n  - Translation: 53.94 ± 0.66\n\nThe Translation model appears to perform the best across all languages.\nThe table presents the results of different models evaluated on an \"Original Unsequestered Set,\" with the use of various extra resources. Here's a breakdown:\n\n### Columns:\n1. **Model**: Lists different model variants.\n2. **Original Unsequestered Set**: Results of the models on a specific test set, showing both the value and, where applicable, the uncertainty (e.g., \\( \\pm \\)).\n3. **Extra Resources**: Resources used alongside the model.\n\n### Row Details:\n\n- **Mayhew et al. (2017)**: Achieved a score of 51.32 using Wikipedia and a 100K dictionary.\n- **Mayhew et al. (2017) (only Eng. data)**: Achieved a score of 27.20 with the same resources.\n- **BWET**: Scored 25.73 ± 0.89 using a 5K dictionary.\n- **BWET + self-att.**: Scored 26.38 ± 0.34 with a 5K dictionary.\n- **BWET on data from Mayhew et al. (2017)**: Scored 30.20 ± 0.98 using Wikipedia and a 100K dictionary.\n- **BWET + self-att. on data from Mayhew et al. (2017)**: Scored 30.68 ± 0.45 with the same resources.\n- **Combined (see text)**: Scored 31.61 ± 0.46 using Wikipedia, a 100K dictionary, and a 5K dictionary.\n- **Combined + self-att.**: Scored 32.09 ± 0.61 using the same resources.\n\nThe table essentially compares the performance of different models and configurations using varying data resources.\nTable 3: NER    $F_{1}$   scores on Uyghur.   ∗ Approaches using language-speciﬁc features and resources (“Wikipedia” means Wikipedia is used not as a monolingual corpus, but to provide external knowledge). † Approaches that transfer from multiple languages and use language-speciﬁc techniques. \n4.3 Case Study: Uyghur \nIn this section, we directly apply our approach to Uyghur, a truly low-resource language with very limited monolingual and parallel resources. We test our model on 199 annotated evaluation documents from the DARPA LORELEI program (the “unsequestered set”) and compare with previ- ously reported results in the cross-lingual setting by  Mayhew et al.  ( 2017 ). Similar to our previous experiments, we transfer from English, use fast- Text embeddings trained on Common Crawl and Wikipedia 6   and a provided dictionary to perform translation, and use GloVe trained on a monolin- gual corpus that has 30 million tokens to perform NER. Results are presented in Table  3 . \nOur method performs competitively, consid- ering that we use a much smaller dictionary than  Mayhew et al.  ( 2017 ) and no knowledge from Wikipedia in Uyghur. Our best results come from a combined approach: using word embeddings to translate words that are not covered by  Mayhew et al.  ( 2017 )’s dictionary (last line of Table  3 ). Note that for the CoNLL languages, Mayhew et al.  ( 2017 ) used Wikipedia for the Wikiﬁer fea- tures ( Tsai et al. ,  2016 ), while for Uyghur they used it for translating named entities, which is cru- cial for low-resource languages when some named entities are not covered by the dictionary or the translation is not reliable. We suspect that the un- reliable translation of named entities is the ma- jor reason why our method alone performs worse but performs better when combined with their data that has access to higher quality translations of named entities. \n\nThe table omits results using adversarial learn- ing and identical character strings, as both failed  ${\\cal F}_{1}$   scores around 10). We attribute these failures to the low quality of Uyghur word embeddings and the fact that the two languages are distant. Also, Uyghur is mainly written in Arabic script, mak- ing the identical character method inappropriate. Overall, this reveals a practical challenge for mul- tilingual embedding methods, where the underly- ing distributions of the text in the two languages are divergent. \n5 Related Work \nCross-Lingual Learning  Cross-lingual learning approaches can be loosely classiﬁed into two categories: annotation projection and language- independent transfer. \nAnnotation projection methods create training data by using parallel corpora to project annota- tions from the source to the target language. Such approaches have been applied to many tasks un- der the cross-lingual setting, such as POS tag- ging ( Yarowsky et al. ,  2001 ;  Das and Petrov ,  2011 ; T¨ ackstr¨ om et al. ,  2013 ;  Fang and Cohn ,  2016 ), mention detection ( Zitouni and Florian ,  2008 ) and parsing ( Hwa et al. ,  2005 ;  McDonald et al. ,  2011 ). \nLanguage independent transfer-based ap- proaches build models using language indepen- dent and delexicalized features. For instance, Zirikly and Hagiwara  ( 2015 ) transfers word cluster and gazetteer features through the use of comparable copora.  Tsai et al.  ( 2016 ) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently,  Ni et al.  ( 2017 ) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. "}
{"page": 8, "image_path": "doc_images/D18-1034_8.jpg", "ocr_text": "Zirikly and Hagiwara (2015) transfers word\ncluster and gazetteer features through the use of\ncomparable copora. Tsai et al. (2016) links words\nto Wikipedia entries and uses the entry category\nas features to train language independent NER\nmodels. Recently, Ni et al. (2017) propose to\nproject word embeddings into a common space as\nlanguage independent features. These approaches\nutilize such features by training a model on the\nsource language and directly applying it to the\ntarget language.\n\nAnother way of performing language indepen-\ndent transfer resorts to multi-task learning, where\na model is trained jointly across different lan-\nguages by sharing parameters to allow for knowl-\nedge transfer (Ammar et al., 2016a; Yang et al.,\n2017; Cotterell and Duh, 2017; Lin et al., 2018).\nHowever, such approaches usually require some\namounts of training data in the target language\nfor bootstrapping, which is different from our un-\nsupervised approach that requires no labeled re-\nsources in the target language.\n\nBilingual Word Embeddings There have been\ntwo general paradigms in obtaining bilingual word\nvectors besides using dictionaries: through paral-\nlel corpora and through joint training. Approaches\nbased on parallel corpora usually learn bilingual\nword embeddings that can produce similar repre-\nsentations for aligned sentences (Hermann and\nBlunsom, 2014; Chandar et al., 2014). Jointly-\ntrained models combine the common monolin-\ngual training objective with a cross-lingual train-\ning objective that often comes from parallel corpus\n(Zou et al., 2013; Gouws et al., 2015). Recently,\nunsupervised approaches also have been used to\nalign two sets of word embeddings by learning\na mapping through adversarial learning or self-\nlearning (Zhang et al., 2017; Artetxe et al., 2017;\nLample et al., 2018).\n\n6 Conclusion\n\nIn this paper, we propose two methods to tackle\nthe cross-lingual NER problem under the unsuper-\nvised transfer setting. To address the challenge of\nlexical mapping, we find translations of words in\na shared embedding space built from a seed lex-\nicon. To alleviate word order divergence across\nlanguages, we add a self-attention mechanism to\nour neural architecture. With these methods com-\nbined, we are able to achieve state-of-the-art or\ncompetitive results on commonly tested languages\n\n377\n\nunder a cross-lingual setting, with lower resource\nrequirements than past approaches. We also eval-\nuate the challenges of applying these methods to\nan extremely low-resource language, Uyghur.\n\nAcknowledgments\n\nWe thank Stephen Mayhew for sharing the data,\nand Zihang Dai for meaningful discussion.\n\nThis research was sponsored by Defense Ad-\nvanced Research Projects Agency Information In-\nnovation Office (I20) under the Low Resource\nLanguages for Emergent Incidents (LORELEI)\nprogram, issued by DARPA/I20 under Contract\nNo. HROO11-15-C0114. The views and conclu-\nsions contained in this document are those of the\nauthors and should not be interpreted as repre-\nsenting the official policies, either expressed or\nimplied, of the U.S. government. The U.S. gov-\nernment is authorized to reproduce and distribute\nreprints for government purposes notwithstanding\nany copyright notation here on.\n\nReferences\n\nWaleed Ammar, George Mulcaire, Miguel Ballesteros,\nChris Dyer, and Noah A. Smith. 2016a. Many lan-\nguages, one parser. Transactions of the Association\nfor Computational Linguistics, 4:431-444.\n\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A. Smith.\n2016b. Massively multilingual word embeddings.\nhttps://arxiv.org/pdf/1602.01925.\n\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.\nLearning principled bilingual mappings of word em-\nbeddings while preserving monolingual invariance.\nIn EMNLP, pages 2289-2294.\n\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In ACL, pages 451-462.\n\nAkash Bharadwaj, David Mortensen, Chris Dyer, and\nJaime Carbonell. 2016. Phonologically aware neu-\nral model for named entity recognition in low re-\nsource transfer settings. In EMNLP, pages 1462—\n1472.\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135-146.\n\nSarath Chandar, Stanislas Lauly, Hugo Larochelle,\nMitesh Khapra, Balaraman Ravindran, Vikas C\nRaykar, and Amrita Saha. 2014. An autoencoder\napproach to learning bilingual word representations.\nIn NIPS, pages 1853-1861.\n", "vlm_text": "\nAnother way of performing language indepen- dent transfer resorts to multi-task learning, where a model is trained jointly across different lan- guages by sharing parameters to allow for knowl- edge transfer ( Ammar et al. ,  2016a ;  Yang et al. , 2017 ;  Cotterell and Duh ,  2017 ;  Lin et al. ,  2018 ). However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our un- supervised approach that requires no labeled re- sources in the target language. \nBilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through paral- lel corpora and through joint training. Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar repre- sentations for aligned sentences ( Hermann and Blunsom ,  2014 ;  Chandar et al. ,  2014 ). Jointly- trained models combine the common monolin- gual training objective with a cross-lingual train- ing objective that often comes from parallel corpus ( Zou et al. ,  2013 ;  Gouws et al. ,  2015 ). Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or self- learning ( Zhang et al. ,  2017 ;  Artetxe et al. ,  2017 ; Lample et al. ,  2018 ). \n6 Conclusion \nIn this paper, we propose two methods to tackle the cross-lingual NER problem under the unsuper- vised transfer setting. To address the challenge of lexical mapping, we ﬁnd translations of words in a shared embedding space built from a seed lex- icon. To alleviate word order divergence across languages, we add a self-attention mechanism to our neural architecture. With these methods com- bined, we are able to achieve state-of-the-art or competitive results on commonly tested languages under a cross-lingual setting, with lower resource requirements than past approaches. We also eval- uate the challenges of applying these methods to an extremely low-resource language, Uyghur. \n\nAcknowledgments \nWe thank Stephen Mayhew for sharing the data, and Zihang Dai for meaningful discussion. \nThis research was sponsored by Defense Ad- vanced Research Projects Agency Information In- novation Ofﬁce (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA/I2O under Contract No. HR0011-15-C0114. The views and conclu- sions contained in this document are those of the authors and should not be interpreted as repre- senting the ofﬁcial policies, either expressed or implied, of the U.S. government. The U.S. gov- ernment is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation here on. \nReferences \nWaleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A. Smith. 2016a. Many lan- guages, one parser.  Transactions of the Association for Computational Linguistics , 4:431–444. Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A. Smith. 2016b. Massively multilingual word embeddings. https://arxiv.org/pdf/1602.01925 . Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016. Learning principled bilingual mappings of word em- beddings while preserving monolingual invariance. In  EMNLP , pages 2289–2294. Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. Learning bilingual word embeddings with (almost) no bilingual data. In  ACL , pages 451–462. Akash Bharadwaj, David Mortensen, Chris Dyer, and Jaime Carbonell. 2016. Phonologically aware neu- ral model for named entity recognition in low re- source transfer settings. In  EMNLP , pages 1462– 1472. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information.  Transactions of the Associa- tion for Computational Linguistics , 5:135–146. Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In  NIPS , pages 1853–1861. "}
{"page": 9, "image_path": "doc_images/D18-1034_9.jpg", "ocr_text": "Jason Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional Istm-cnns. Transac-\n\ntions of the Association for Computational Linguis-\ntics, 4:357-370.\n\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research,\n12(Aug):2493-2537.\n\nRyan Cotterell and Kevin Duh. 2017. Low-\nresource named entity recognition with cross-\nlingual, character-level neural conditional random\nfields. In IJCNLP, pages 91-96.\n\nDipanjan Das and Slav Petrov. 2011. Unsupervised\npart-of-speech tagging with bilingual graph-based\nprojections. In ACL, pages 600-609.\n\nGeorgiana Dinu and Marco Baroni. 2014. Improving\nzero-shot learning by mitigating the hubness prob-\nlem. CoRR, abs/1412.6568.\n\nMaud Ehrmann, Marco Turchi, and Ralf Steinberger.\n2011. Building a multilingual named entity-\nannotated corpus using annotation projection. In\nRANLP, pages 118-124.\n\nMeng Fang and Trevor Cohn. 2016. Learning when\nto trust distant supervision: An application to low-\nresource POS tagging using cross-lingual projection.\nIn CoNLL, pages 178-186.\n\nMeng Fang and Trevor Cohn. 2017. Model transfer\nfor tagging low-resource languages using a bilingual\ndictionary. In ACL, pages 587-593.\n\nManaal Faruqui and Chris Dyer. 2014. Improving vec-\ntor space word representations using multilingual\ncorrelation. In ACL, pages 462-471.\n\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015. Bilbowa: Fast bilingual distributed represen-\ntations without word alignments. In JCML, pages\n748-756.\n\nJiang Guo, Wanxiang Che, David Yarowsky, Haifeng\nWang, and Ting Liu. 2015. Cross-lingual depen-\ndency parsing based on distributed representations.\nIn ACL, volume 1, pages 1234-1244.\n\nKarl Moritz Hermann and Phil Blunsom. 2014. Multi-\nlingual models for compositional distributed seman-\ntics. In ACL, pages 58-68.\n\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-\nrectional LSTM-CRF models for sequence tagging.\nCoRR, abs/1508.01991.\n\nRebecca Hwa, Philip Resnik, Amy Weinberg, Clara\nCabezas, and Okan Kolak. 2005. Bootstrapping\nparsers via syntactic projection across parallel texts.\nNatural language engineering, 11(3):311-325.\n\n378\n\nSungchul Kim, Kristina Toutanova, and Hwanjo Yu.\n2012. Multilingual named entity recognition using\nparallel data and metadata from wikipedia. In ACL,\npages 694-702.\n\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn NAACL, pages 260-270.\n\nGuillaume Lample, Alexis Conneau, Marc’ Aurelio\nRanzato, Ludovic Denoyer, and Herv Jgou. 2018.\nWord translation without parallel data. In JCLR.\n\nYing Lin, Shengqi Yang, Veselin Stoyanov, and Heng\nJi. 2018. A multi-lingual multi-task architecture\nfor low-resource sequence labeling. In ACL, pages\n799-809.\n\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. In JCLR.\n\nL. Liu, J. Shang, F. Xu, X. Ren, H. Gui, J. Peng, and\nJ. Han. 2018. Empower sequence labeling with task-\naware neural language model. In AAAI.\n\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional Istm-cnns-crf. In\nACL, pages 1064-1074.\n\nStephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.\nCheap translation for cross-lingual named entity\nrecognition. In EMNLP, pages 2526-2535.\n\nRyan McDonald, Slav Petrov, and Keith Hall. 2011.\nMulti-source transfer of delexicalized dependency\nparsers. In EMNLP, pages 62-72.\n\nTomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013a.\nExploiting similarities among languages for ma-\nchine translation. CoRR, abs/1309.4168.\n\nTomas Mikolovy, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In NPS, pages 3111-3119.\n\nJian Ni, Georgiana Dinu, and Radu Florian. 2017.\nWeakly supervised cross-lingual named entity\nrecognition via effective annotation and representa-\ntion projection. In ACL, pages 1470-1480.\n\nJoel Nothman, Nicky Ringland, Will Radford, Tara\nMurphy, and James R Curran. 2013. Learning mul-\ntilingual named entity recognition from wikipedia.\nArtificial Intelligence, 194:151-175.\n\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP, pages 1532-1543.\n\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn ACL, pages 1756-1765.\n", "vlm_text": "Jason Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional lstm-cnns.  Transac- tions of the Association for Computational Linguis- tics , 4:357–370. Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research , 12(Aug):2493–2537. Ryan Cotterell and Kevin Duh. 2017. Low- resource named entity recognition with cross- lingual, character-level neural conditional random ﬁelds. In  IJCNLP , pages 91–96. Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In  ACL , pages 600–609. Georgiana Dinu and Marco Baroni. 2014. Improving zero-shot learning by mitigating the hubness prob- lem.  CoRR , abs/1412.6568. Maud Ehrmann, Marco Turchi, and Ralf Steinberger. 2011. Building a multilingual named entity- annotated corpus using annotation projection. In RANLP , pages 118–124. Meng Fang and Trevor Cohn. 2016. Learning when to trust distant supervision: An application to low- resource POS tagging using cross-lingual projection. In  CoNLL , pages 178–186. Meng Fang and Trevor Cohn. 2017. Model transfer for tagging low-resource languages using a bilingual dictionary. In  ACL , pages 587–593. Manaal Faruqui and Chris Dyer. 2014. Improving vec- tor space word representations using multilingual correlation. In  ACL , pages 462–471. Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual distributed represen- tations without word alignments. In  ICML , pages 748–756. Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang, and Ting Liu. 2015. Cross-lingual depen- dency parsing based on distributed representations. In  ACL , volume 1, pages 1234–1244. Karl Moritz Hermann and Phil Blunsom. 2014. Multi- lingual models for compositional distributed seman- tics. In  ACL , pages 58–68. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi- rectional LSTM-CRF models for sequence tagging. CoRR , abs/1508.01991. Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering , 11(3):311–325. \nSungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012. Multilingual named entity recognition using parallel data and metadata from wikipedia. In    $A C L$  , pages 694–702. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In  NAACL , pages 260–270. Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv Jgou. 2018. Word translation without parallel data. In  ICLR . Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng Ji. 2018. A multi-lingual multi-task architecture for low-resource sequence labeling. In  ACL , pages 799–809. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In  ICLR . L. Liu, J. Shang, F. Xu, X. Ren, H. Gui, J. Peng, and J. Han. 2018. Empower sequence labeling with task- aware neural language model. In  AAAI . Xuezhe Ma and Eduard Hovy. 2016. End-to-end se- quence labeling via bi-directional lstm-cnns-crf. In ACL , pages 1064–1074. Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017. Cheap translation for cross-lingual named entity recognition. In  EMNLP , pages 2526–2535. Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In  EMNLP , pages 62–72. Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for ma- chine translation.  CoRR , abs/1309.4168. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed representa- tions of words and phrases and their compositional- ity. In  NIPS , pages 3111–3119. Jian Ni, Georgiana Dinu, and Radu Florian. 2017. Weakly supervised cross-lingual named entity recognition via effective annotation and representa- tion projection. In  ACL , pages 1470–1480. Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy, and James R Curran. 2013. Learning mul- tilingual named entity recognition from wikipedia. Artiﬁcial Intelligence , 194:151–175. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In  EMNLP , pages 1532–1543. Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In  ACL , pages 1756–1765. "}
{"page": 10, "image_path": "doc_images/D18-1034_10.jpg", "ocr_text": "Matthew E. Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In NAACL, pages 2227-2237.\n\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of Istm-networks for sequence tagging. In\nEMNLP, pages 338-348.\n\nSamuel L. Smith, David H. P. Turban, Steven Hamblin,\nand Nils Y. Hammerla. 2017. Offline bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In JCLR.\n\nOscar Tickstrém, Dipanjan Das, Slav Petrov, Ryan T.\nMcDonald, and Joakim Nivre. 2013. Token and type\nconstraints for cross-lingual part-of-speech tagging.\nTACL, 1:1-12.\n\nOscar Tiackstrém, Ryan McDonald, and Jakob Uszko-\nreit. 2012. Cross-lingual word clusters for direct\ntransfer of linguistic structure. In NAACL, pages\n477-487.\n\nErik F. Tjong Kim Sang. 2002. Introduction to the\nCoNLL-2002 shared task: Language-independent\nnamed entity recognition. In CoNLL, pages 1-4.\n\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL, pages 142-147.\n\nChen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016.\nCross-lingual named entity recognition via wikifica-\ntion. In CoNLL, pages 219-228.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, L ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIJPS, pages 6000-6010.\n\nMengqiu Wang and Christopher D. Manning. 2014.\nCross-lingual projected expectation regularization\nfor weakly supervised learning. Transactions of the\nAssociation for Computational Linguistics (TACL),\n2(5):55-66.\n\nZhilin Yang, Ruslan Salakhutdinov, and William W.\nCohen. 2016. Multi-task cross-lingual sequence tag-\nging from scratch. CoRR, abs/1603.06270.\n\nZhilin Yang, Ruslan Salakhutdinov, and William W.\nCohen. 2017. Transfer learning for sequence tag-\nging with hierarchical recurrent networks.\n\nD. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-\nducing multilingual text analysis tools via robust\nprojection across aligned corpora. In HLT.\n\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017. Adversarial training for unsupervised\nbilingual lexicon induction. In ACL, volume 1,\npages 1959-1970.\n\n379\n\nYuan Zhang, David Gaddy, Regina Barzilay, and\nTommi S. Jaakkola. 2016. Ten pairs to tag - mul-\ntilingual POS tagging via coarse mapping between\nembeddings. In NAACL, pages 1307-1317.\n\nAyah Zirikly and Masato Hagiwara. 2015. Cross-\nlingual transfer of named entity recognizers without\nparallel corpora. In ACL, pages 390-396. Associa-\ntion for Computational Linguistics.\n\nImed Zitouni and Radu Florian. 2008. Mention detec-\ntion crossing the language barrier. In EMNLP, pages\n600-609.\n\nWill Y Zou, Richard Socher, Daniel Cer, and Christo-\npher D Manning. 2013. Bilingual word embeddings\nfor phrase-based machine translation. In EMNLP,\npages 1393-1398.\n", "vlm_text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In  NAACL , pages 2227–2237. Nils Reimers and Iryna Gurevych. 2017. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. In EMNLP , pages 338–348. Samuel L. Smith, David H. P. Turban, Steven Hamblin, and Nils Y. Hammerla. 2017. Ofﬂine bilingual word vectors, orthogonal transformations and the inverted softmax. In  ICLR . Oscar T¨ ackstr¨ om, Dipanjan Das, Slav Petrov, Ryan T. McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. TACL , 1:1–12. Oscar T¨ ackstr¨ om, Ryan McDonald, and Jakob Uszko- reit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In  NAACL , pages 477–487. Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In  CoNLL , pages 1–4. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL , pages 142–147. Chen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016. Cross-lingual named entity recognition via wikiﬁca- tion. In  CoNLL , pages 219–228. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In  NIPS , pages 6000–6010. Mengqiu Wang and Christopher D. Manning. 2014. Cross-lingual projected expectation regularization for weakly supervised learning.  Transactions of the Association for Computational Linguistics (TACL) , 2(5):55–66. Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. 2016. Multi-task cross-lingual sequence tag- ging from scratch.  CoRR , abs/1603.06270. Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. 2017. Transfer learning for sequence tag- ging with hierarchical recurrent networks. D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In- ducing multilingual text analysis tools via robust projection across aligned corpora. In  HLT . Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Adversarial training for unsupervised bilingual lexicon induction. In  ACL , volume 1, pages 1959–1970. \nYuan Zhang, David Gaddy, Regina Barzilay, and Tommi S. Jaakkola. 2016. Ten pairs to tag - mul- tilingual POS tagging via coarse mapping between embeddings. In  NAACL , pages 1307–1317. Ayah Zirikly and Masato Hagiwara. 2015. Cross- lingual transfer of named entity recognizers without parallel corpora. In  ACL , pages 390–396. Associa- tion for Computational Linguistics. Imed Zitouni and Radu Florian. 2008. Mention detec- tion crossing the language barrier. In  EMNLP , pages 600–609. Will Y Zou, Richard Socher, Daniel Cer, and Christo- pher D Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In  EMNLP , pages 1393–1398. "}
