{"layout": 0, "type": "text", "text": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks ", "text_level": 1, "page_idx": 0, "bbox": [132, 68, 464, 101], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Ivan Titov University of Edinburgh University of Amsterdam ititov@inf.ed.ac.uk ", "page_idx": 0, "bbox": [380.0129699707031, 123.88201904296875, 516.3023681640625, 180.03541564941406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Wilker Aziz University of Amsterdam w.aziz@uva.nl ", "page_idx": 0, "bbox": [237.9149932861328, 123.88201904296875, 359.6308898925781, 166.08741760253906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Nicola De Cao University of Edinburgh University of Amsterdam nicola.decao@uva.nl ", "page_idx": 0, "bbox": [82.73599243164062, 123.88201904296875, 219.0253448486328, 180.03541564941406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "image", "page_idx": 0, "img_path": "layout_images/N19-1240_0.jpg", "bbox": [311, 220, 520, 318], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Thorildsplan is a small park in Kristineberg in\nStockholm, named in 1925 after the writer [..]\n\nStockholm is the capital of Sweden\nand the most populous city in [..]\n\nquery: country Thorildsplan\ncandidates: {Denmark, Finland, Sweden, Italy,\nanswer: Sweden\n", "vlm_text": "The image is a visual representation of a query process to determine the country associated with Thorildsplan, which is a small park in Kristineberg, Stockholm. The text explains how information from sentences is used to deduce that Stockholm is in Sweden. The query asks for the country of Thorildsplan, and among the candidate countries (Denmark, Finland, Sweden, Italy, etc.), the correct answer is identified as Sweden. The image conveys this logical inference through highlighted keywords and arrows."}
{"layout": 5, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 235], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Most research in reading comprehension has focused on answering questions based on in- dividual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple docu- ments. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross- document coreference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and com- pact, and it achieves state-of-the-art results on a multi-document question answering dataset, W IKI H OP  ( Welbl et al. ,  2018 ). ", "page_idx": 0, "bbox": [89, 246.39154052734375, 273, 449.679443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "Figure 1: A sample from W IKI H OP  where multi-step reasoning and information combination from different documents is necessary to infer the correct answer. ", "page_idx": 0, "bbox": [307, 331.9125671386719, 525, 367.8274841308594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "relying only on local information cannot achieve competitive performance. ", "page_idx": 0, "bbox": [307, 395.0129699707031, 525, 421.70745849609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "Even though these new datasets are challeng- ing and require reasoning within documents, many question answering and search applications re- quire aggregation of information across multiple documents. The W IKI H OP  dataset ( Welbl et al. , 2018 ) was explicitly created to facilitate the devel- opment of systems dealing with these scenarios. Each example in W IKI H OP  consists of a collec- tion of documents, a query and a set of candidate answers (Figure  1 ). Though there is no guaran- tee that a question cannot be answered by relying just on a single sentence, the authors ensure that it is answerable using a chain of reasoning crossing document boundaries. ", "page_idx": 0, "bbox": [307, 424.9079895019531, 525, 614.1934814453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 460, 155, 473], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "The long-standing goal of natural language under- standing is the development of systems which can acquire knowledge from text collections. Fresh in- terest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD ( Rajpurkar et al. ,  2016 ) and CNN/Daily Mail ( Hermann et al. ,  2015 ), enabling end-to-end training of neural models ( Seo et al. ,  2016 ;  Xiong et al. ,  2016 ;  Shen et al. ,  2017 ). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the doc- ument, but they can be answered relying on in- formation contained in a single sentence ( Weis- senborn et al. ,  2017 ). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA ( Kocisky et al. ,  2018 ), Trivi- aQA ( Joshi et al. ,  2017 ), and RACE ( Lai et al. , 2017 ), have been created in such a way as to ad- dress this shortcoming and to ensure that systems ", "page_idx": 0, "bbox": [71, 481.90191650390625, 290, 766.0313720703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "Though an important practical problem, the multi-hop setting has so far received little at- tention. The methods reported by  Welbl et al. ( 2018 ) approach the task by merely concatenat- ing all documents into a single long text and train- ing a standard RNN-based reading comprehen- sion model, namely, BiDAF ( Seo et al. ,  2016 ) and FastQA ( Weissenborn et al. ,  2017 ). Docu- ment concatenation in this setting is also used in Weaver ( Raison et al. ,  2018 ) and MHPGM ( Bauer et al. ,  2018 ). The only published paper which goes beyond concatenation is due to  Dhingra et al.  ( 2018 ), where they augment RNNs with jump-links corresponding to co-reference edges. Though these edges provide a structural bias, the RNN states are still tasked with passing the infor- mation across the document and performing multi- hop reasoning. ", "page_idx": 0, "bbox": [307, 617.3941040039062, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "", "page_idx": 1, "bbox": [71, 63.68701934814453, 290, 158.12753295898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "Instead, we frame question answering as an inference problem on a graph representing the document collection. Nodes in this graph corre- spond to named entities in a document whereas edges encode relations between them (e.g., cross- and within-document coreference links or simply co-occurrence in a document). We assume that reasoning chains can be captured by propagat- ing local contextual information along edges in this graph using a graph convolutional network (GCN) ( Kipf and Welling ,  2017 ). ", "page_idx": 1, "bbox": [71, 159.0010528564453, 290, 307.6385803222656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "The multi-document setting imposes scalabil- ity challenges. In realistic scenarios, a system needs to learn to answer a query for a given col- lection (e.g., Wikipedia or a domain-speciﬁc set of documents). In such scenarios one cannot af- ford to run expensive document encoders (e.g., RNN or transformer-like self-attention ( Vaswani et al. ,  2017 )), unless the computation can be pre- processed both at train and test time. Even if (similarly to W IKI H OP  creators) one considers a coarse-to-ﬁne approach, where a set of potentially relevant documents is provided, re-encoding them in a query-speciﬁc way remains the bottleneck. In contrast to other proposed methods (e.g., ( Dhingra et al. ,  2018 ;  Raison et al. ,  2018 ;  Seo et al. ,  2016 )), we avoid training expensive document encoders. ", "page_idx": 1, "bbox": [71, 308.5111083984375, 290, 524.8946533203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "In our approach, only a small query encoder, the GCN layers and a simple feed-forward an- swer selection component are learned. Instead of training RNN encoders, we use contextualized embeddings (ELMo) to obtain initial (local) rep- resentations of nodes. This implies that only a lightweight computation has to be performed on- line, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived W IKI H OP  setting, where fairly small sets of can- didates are provided, the model is at least 5 times faster to train than BiDAF.   Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question- aware recurrent document encoders. ", "page_idx": 1, "bbox": [71, 525.7682495117188, 290, 715.0526733398438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "", "page_idx": 1, "bbox": [307, 63.68701934814453, 526, 90.38247680664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "Despite not using recurrent document encoders, the full Entity-GCN model achieves over   $2\\%$   im- provement over the best previously-published re- sults. As our model is efﬁcient, we also reported results of an ensemble which brings further  $3.6\\%$  of improvement and only   $3\\%$   below the human performance reported by  Welbl et al.  ( 2018 ). Our contributions can be summarized as follows: ", "page_idx": 1, "bbox": [307, 91.14203643798828, 526, 199.13156127929688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "•  we present a novel approach for multi-hop QA that relies on a (pre-trained) document encoder and information propagation across multiple documents using graph neural net- works; •  we provide an efﬁcient training technique which relies on a slower ofﬂine and a faster on-line computation that does not require ex- pensive document processing; •  we empirically show that our algorithm is ef- fective, presenting an improvement over pre- vious results. ", "page_idx": 1, "bbox": [318, 209.92811584472656, 526, 392.89764404296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 1, "bbox": [307, 405, 366, 417], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "In this section we explain our method. We ﬁrst introduce the dataset we focus on, W IKI H OP by  Welbl et al.  ( 2018 ), as well as the task ab- straction. We then present the building blocks that make up our Entity-GCN model, namely, an  en- tity graph  used to relate mentions to entities within and across documents, a  document encoder  used to obtain representations of mentions in context, and a  relational graph convolutional network  that propagates information through the entity graph. ", "page_idx": 1, "bbox": [307, 426.5491638183594, 526, 561.63671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "2.1 Dataset and Task Abstraction ", "text_level": 1, "page_idx": 1, "bbox": [307, 573, 470, 585], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "Data The W IKI H OP  dataset comprises of tuples  $\\langle q,S_{q},C_{q},a^{\\star}\\rangle$  where:    $q$   is a query/question,    $S_{q}$   is a set of supporting documents,  $C_{q}$   is a set of candi- date answers (all of which are entities mentioned in    $S_{q})$  ), and    $a^{\\star}\\,\\in\\,C_{q}$   is the entity that correctly answers the question. W IKI H OP  is assembled as- suming that there exists a corpus and a knowledge base (KB) related to each other. The KB contains triples    $\\langle s,r,o\\rangle$  where    $s$   is a subject entity,    $o$   an ob- ject entity, and    $r$   a unidirectional relation between them.  Welbl et al.  ( 2018 ) used W IKIPEDIA  as cor- pus and W IKIDATA  ( Vrandeˇ ci´ ,  2012 ) as KB. The KB is only used for constructing W IKI H OP :  Welbl et al.  ( 2018 ) retrieved the supporting documents  $S_{q}$   from the corpus looking at mentions of subject and object entities in the text. Note that the set    $S_{q}$  (not the KB) is provided to the QA system, and not all of the supporting documents are relevant for the query but some of them act as distractors. Queries, on the other hand, are not expressed in natural lan- guage, but instead consist of tuples    $\\langle s,r,?\\rangle$  where the object entity is unknown and it has to be in- ferred by reading the support documents. There- fore, answering a query corresponds to ﬁnding the entity    $a^{\\star}$  that is the object of a tuple in the KB with subject  $s$   and relation    $r$   among the provided set of candidate answers  $C_{q}$  . ", "page_idx": 1, "bbox": [307, 589.9025268554688, 526, 766.0316772460938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "", "page_idx": 2, "bbox": [70, 63.68701934814453, 290, 253], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "Task The goal is to learn a model that can iden- tify the correct answer    $a^{\\star}$  from the set of support- ing documents    $S_{q}$  . To that end, we exploit the available supervision to train a neural network that computes scores for candidates in  $C_{q}$  . We estimate the parameters of the architecture by maximizing the likelihood of observations. For prediction, we then output the candidate that achieves the high- est probability. In the following, we present our model discussing the design decisions that enable multi-step reasoning and an efﬁcient computation. ", "page_idx": 2, "bbox": [70, 263.4793395996094, 290, 412.5086364746094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "2.2 Reasoning on an Entity Graph ", "text_level": 1, "page_idx": 2, "bbox": [71, 425, 240, 438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "Entity graph In an ofﬂine step, we organize the content of each training instance in a graph con- necting mentions of candidate answers within and across supporting documents. For a given query  $q=\\langle s,r,?\\rangle$  , we identify mentions in    $S_{q}$   of the en- tities in  $C_{q}\\cup\\{s\\}$   and create one node per mention. This process is based on the following heuristic: ", "page_idx": 2, "bbox": [70, 443.76043701171875, 290, 538.5936279296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "1. we consider mentions spans in    $S_{q}$   exactly matching an element of    $C_{q}\\cup\\{s\\}$  . Admit- tedly, this is a rather simple strategy which may suffer from low recall. 2. we use predictions from a coreference reso- lution system to add mentions of elements in  $C_{q}\\cup\\{s\\}$   beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end corefer- ence resolution by  Lee et al.  ( 2017 ). 3. we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacriﬁce recall, but avoids propagating ambiguity. ", "page_idx": 2, "bbox": [80, 551.5802001953125, 290, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "image", "page_idx": 2, "img_path": "layout_images/N19-1240_1.jpg", "img_caption": "Figure 2: Supporting documents (dashed ellipses) or- ganized as a graph where nodes are mentions of ei- ther candidate entities or query entities. Nodes with the same color indicates they refer to the same entity (ex- act match, coreference or both). Nodes are connected by three simple relations: one indicating co-occurrence in the same document (solid edges), another connect- ing mentions that exactly match (dashed edges), and a third one indicating a coreference (bold-red line). ", "bbox": [305, 61, 528, 298], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "", "vlm_text": "The image is a graph representing supporting documents as nodes, where nodes are mentions of either candidate entities or query entities. Nodes are color-coded to show they refer to the same entity based on exact match, coreference, or both. There are three types of connections between the nodes:\n\n- Solid edges indicate co-occurrence in the same document.\n- Dashed edges connect mentions that exactly match.\n- A bold-red line indicates a coreference.\n\nThe nodes are contained within dashed ellipses, organizing them as a graph."}
{"layout": 30, "type": "text", "text": "To each node    $v_{i}$  , we associate a continuous an- notation  $\\mathbf{x}_{i}\\,\\in\\,\\mathbb{R}^{D}$    which represents an entity in the context where it was mentioned (details in Sec- tion  2.3 ). We then proceed to connect these men- tions i) if they co-occur within the same document (we will refer to this as  DOC-BASED  edges), ii) if the pair of named entity mentions is identical ( MATCH  edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the exter- nal coreference system ( COREF  edges). Note that MATCH  edges when connecting mentions in the same document are mostly included in the set of edges predicted by the coreference system. Hav- ing the two types of edges lets us distinguish be- tween less reliable edges provided by the coref- erence system and more reliable (but also more sparse) edges given by the exact-match heuristic. We treat these three types of connections as three different types of relations. See Figure  2  for an illustration. In addition to that, and to prevent hav- ing disconnected graphs, we add a fourth type of relation ( COMPLEMENT  edge) between any two nodes that are not connected with any of the other relations. We can think of these edges as those in the complement set of the entity graph with re- spect to a fully connected graph. ", "page_idx": 2, "bbox": [307, 321.5180969238281, 525, 686.942626953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "Multi-step reasoning Our model then ap- proaches multi-step reasoning by transforming node representations (Section  2.3  for details) with a differentiable message passing algorithm that propagates information through the entity graph. The algorithm is parameterized by a graph convolutional network (GCN) ( Kipf and Welling ,  2017 ), in particular, we employ relational-GCNs ( Schlichtkrull et al. ,  2018 ), an ex- tended version that accommodates edges of differ- ent types. In Section  2.4  we describe the propaga- tion rule. ", "page_idx": 2, "bbox": [307, 698.2965087890625, 525, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "", "page_idx": 3, "bbox": [71, 63.68701934814453, 290, 158.12753295898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "Each step of the algorithm (also referred to as a  hop ) updates all node representations in parallel. In particular, a node is updated as a function of messages from its direct neighbours, and a mes- sage is possibly speciﬁc to a certain relation. At the end of the ﬁrst step, every node is aware of ev- ery other node it connects directly to. Besides, the neighbourhood of a node may include mentions of the same entity as well as others (e.g., same- document relation), and these mentions may have occurred in different documents. Taking this idea recursively, each further step of the algorithm al- lows a node to indirectly interact with nodes al- ready known to their neighbours. After    $L$   layers of R-GCN, information has been propagated through paths connecting up to    $L+1$   nodes. ", "page_idx": 3, "bbox": [71, 158.66004943847656, 290, 375.0426330566406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "We start with node representat ns    $\\{\\mathbf{h}_{i}^{(0)}\\}_{i=1}^{N}$    } , and transform them by applying  L  layers of R- GCN obtaining    $\\{\\mathbf{h}_{i}^{(L)}\\}_{i=1}^{N}$  . Together with a rep- resentation  q  of the query, we deﬁne a distribution over candidate answers and we train maximizing the likelihood of observations. The probability of selecting a candidate  $c\\in C_{q}$   as an answer is then ", "page_idx": 3, "bbox": [71, 374, 290, 474.7246398925781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "equation", "text": "\n$$\nP(c|q,C_{q},S_{q})\\propto\\exp\\left(\\operatorname*{max}_{i\\in\\mathcal{M}_{c}}f_{o}([\\mathbf{q},\\mathbf{h}_{i}^{(L)}])\\right)\\;,\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [78, 482, 281, 512], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "where    $f_{o}$   is a parameterized afﬁne transforma- tion, and    $\\mathcal{M}_{c}$   is the set of node indices such that  $i\\ \\in\\ \\mathcal{M}_{c}$   only if node    $v_{i}$   is a mention of    $c$  . The max  operator in Equation  1  is necessary to select the node with highest predicted probability since a candidate answer is realized in multiple locations via different nodes. ", "page_idx": 3, "bbox": [71, 523.6351928710938, 290, 618.0756225585938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "2.3 Node Annotations ", "text_level": 1, "page_idx": 3, "bbox": [71, 628, 181, 640], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "Keeping in mind we want an efﬁcient model, we encode words in supporting documents and in the query using only a pre-trained model for contex- tualized word representations rather than training our own encoder. Speciﬁcally, we use ELMo 2   ( Pe- ters et al. ,  2018 ), a pre-trained bi-directional lan- guage model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec  ( Mikolov et al. ,  2013 ) or GloVe ( Pen- nington et al. ,  2014 )), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence). ", "page_idx": 3, "bbox": [71, 645.4891967773438, 290, 726.380615234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "", "page_idx": 3, "bbox": [306, 63.68701934814453, 525, 158.12850952148438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "We choose not to ﬁne tune nor propagate gradi- ents through the ELMo architecture, as it would have deﬁed the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model behaves using non-contextualized word represen- tations (we use GloVe). ", "page_idx": 3, "bbox": [306, 159.7710723876953, 525, 254.21157836914062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "Documents pre-processing ELMo encodings are used to produce a set of representations  $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$  , where  $\\mathbf{x}_{i}\\in\\mathbb{R}^{D}$   denotes the  i th candidate mention in context. Note that these representa- tions do not depend on the query yet and no train- able model was used to process the documents so far, that is, we use ELMo as a ﬁxed pre-trained en- coder. Therefore, we can pre-compute representa- tion of mentions once and store them for later use. ", "page_idx": 3, "bbox": [306, 265.8793640136719, 525, 387.8116149902344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "Query-dependent mention encodings ELMo encodings are used to produce a query represen- tation    $\\mathbf{q}\\,\\in\\,\\mathbb{R}^{K}$    as well. Here,    $\\mathbf{q}$   is a concatena- tion of the ﬁnal outputs from a bidirectional RNN layer trained to re-encode ELMo representations of words in the query. The vector    $\\mathbf{q}$   is used to com- pute a query-dependent representation of mentions  $\\{\\hat{\\mathbf{x}}_{i}\\}_{i=1}^{N}$    as well as to compute a probability distri- bution over candidates (as in Equation  1 ). Query- dependent mention encodings    ${\\hat{\\mathbf{x}}}_{i}=f_{x}(\\mathbf{q},\\mathbf{x}_{i})$   are generated by a trainable function    $f_{x}$   which is pa- rameterized by a feed-forward neural network. ", "page_idx": 3, "bbox": [306, 399.4794006347656, 525, 562.0586547851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "2.4 Entity Relational Graph Convolutional Network ", "text_level": 1, "page_idx": 3, "bbox": [307, 576, 514, 601], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "Our model uses a gated version of the original R-GCN propagation rule. At the ﬁrst layer, all hidden node representation are initialized with the query-aware encodings    $\\mathbf{h}_{i}^{(0)}\\,=\\,\\hat{\\mathbf{x}}_{i}$  . Then, at each layer    $0\\,\\leq\\,\\ell\\,\\leq\\,L$  , the update message    $\\mathbf{u}_{i}^{(\\ell)}$  to the i th node is a sum of a transformation    $f_{s}$   of the cur- rent node representation    $\\mathbf{h}_{i}^{(\\ell)}$  and transformations of its neighbours: ", "page_idx": 3, "bbox": [306, 609.0781860351562, 525, 722.3326416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "equation", "text": "\n$$\n\\mathbf{u}_{i}^{(\\ell)}=f_{s}(\\mathbf{h}_{i}^{(\\ell)})+\\frac{1}{|\\mathcal{N}_{i}|}\\sum_{j\\in\\mathcal{N}_{i}}\\sum_{r\\in\\mathcal{R}_{i j}}f_{r}(\\mathbf{h}_{j}^{(\\ell)})\\ ,\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [312, 734, 506, 768], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "where    $\\mathcal{N}_{i}$   is the set of indices of nodes neighbour- ing the  $i$  th node,  $\\mathcal{R}_{i j}$   is the set of edge annotations between  $i$   and    $j$  , and    $f_{r}$   is a parametrized func- tion speciﬁc to an edge type    $r\\,\\in\\,\\mathcal{R}$  . Recall the available relations from Section  2.2 , namely,  $\\mathcal{R}=$  { DOC-BASED ,  MATCH ,  COREF ,  $\\mathsf{C O M P L E M E N T}\\big\\}$  . ", "page_idx": 4, "bbox": [71, 63.68701934814453, 291, 151.9857940673828], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "A gating mechanism regulates how much of the update message propagates to the next step. This provides the model a way to prevent completely overwriting past information. Indeed, if all neces- sary information to answer a question is present at a layer which is not the last, then the model should learn to stop using neighbouring information for the next steps. Gate levels are computed as ", "page_idx": 4, "bbox": [71, 145.16505432128906, 291, 253.15554809570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "equation", "text": "\n$$\n\\begin{array}{r}{\\mathbf{a}_{i}^{(\\ell)}=\\sigma\\left(f_{a}\\left([\\mathbf{u}_{i}^{(\\ell)},\\mathbf{h}_{i}^{(\\ell)}]\\right)\\right)\\ ,}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [113, 261, 247, 285], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "where    $\\sigma(\\cdot)$   is the sigmoid function and    $f_{a}$   a parametrized transformation. Ultimately, the up- dated representation is a gated combination of the previous representation and a non-linear transfor- mation of the update message: ", "page_idx": 4, "bbox": [71, 295, 291, 362.5386047363281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "equation", "text": "\n$$\n\\mathbf{h}_{i}^{(\\ell+1)}=\\phi(\\mathbf{u}_{i}^{(\\ell)})\\odot\\mathbf{a}_{i}^{(\\ell)}+\\mathbf{h}_{i}^{(\\ell)}\\odot(1-\\mathbf{a}_{i}^{(\\ell)})\\ ,\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [74, 371, 272, 390], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "where    $\\phi(\\cdot)$   is any nonlinear function (we used tanh ) and    $\\odot$  stands for element-wise multiplica- tion. All transformations  $f_{*}$  are afﬁne and they are not layer-dependent (since we would like to use as few parameters as possible to decrease model complexity promoting efﬁciency and scalability). ", "page_idx": 4, "bbox": [71, 401, 291, 481.9336242675781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4, "bbox": [71, 493, 155, 506], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "In this section, we compare our method against re- cent work as well as preforming an ablation study using the W IKI H OP  dataset ( Welbl et al. ,  2018 ). See Appendix  A  in the supplementary material for a description of the hyper-parameters of our model and training details. ", "page_idx": 4, "bbox": [71, 514.5531616210938, 291, 595.444580078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "W IKI H OP We use W IKI H OP  for training, val- idation/development and test. The test set is not publicly available and therefore we measure per- formance on the validation set in almost all ex- periments. W IKI H OP  has   $43{,}738/\\ 5{,}129/\\ 2{,}451$  query-documents samples in the training, valida- tion and test sets respectively for a total of 51,318 samples. Authors constructed the dataset as de- scribed in Section  2.1  selecting samples with a graph traversal up to a maximum chain length of 3 documents (see Table  1  for additional dataset statistics). W IKI H OP  comes in two versions, a ", "page_idx": 4, "bbox": [71, 603.4524536132812, 291, 766.0315551757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "table", "page_idx": 4, "img_path": "layout_images/N19-1240_2.jpg", "table_footnote": "Table 1: W IKI H OP  dataset statistics from  Welbl et al. ( 2018 ): number of candidates and documents per sam- ple and document length. ", "bbox": [306, 61, 530, 174], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Min Max Avg. Median\n\n# candidates 2 719 19.8 14\n# documents 3 63 13.7 11\n# tokens/doc. 4 2,046 100.4 91\n", "vlm_text": "This table provides statistical data across three different categories: the number of candidates, the number of documents, and the number of tokens per document. For each category, the table displays the minimum value (Min), maximum value (Max), average value (Avg.), and the median value.\n\n- For \"# candidates\": \n  - Min: 2 \n  - Max: 79 \n  - Avg.: 19.8 \n  - Median: 14 \n\n- For \"# documents\": \n  - Min: 3 \n  - Max: 63 \n  - Avg.: 13.7 \n  - Median: 11 \n\n- For \"# tokens/doc.\": \n  - Min: 4 \n  - Max: 2,046 \n  - Avg.: 100.4 \n  - Median: 91"}
{"layout": 56, "type": "text", "text": "standard (unmasked) one and a masked one. The masked version was created by the authors to test whether methods are able to learn lexical abstrac- tion. In this version, all candidates and all men- tions of them in the support documents are re- placed by random but consistent placeholder to- kens. Thus, in the masked version, mentions are always referred to via unambiguous surface forms. We do not use coreference systems in the masked version as they rely crucially on lexical realization of mentions and cannot operate on masked tokens. ", "page_idx": 4, "bbox": [307, 196.3510284423828, 525, 344.9885559082031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "3.1 Comparison ", "text_level": 1, "page_idx": 4, "bbox": [307, 356, 390, 368], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "In this experiment, we compare our Enitity- GCN against recent prior work on the same task. We present test and development re- sults (when present) for both versions of the dataset in Table  2 . From  Welbl et al.  ( 2018 ), we list an oracle based on human performance as well as two standard reading comprehension models, namely BiDAF ( Seo et al. ,  2016 ) and FastQA ( Weissenborn et al. ,  2017 ). We also com- pare against Coref-GRU ( Dhingra et al. ,  2018 ), MHPGM ( Bauer et al. ,  2018 ), and Weaver ( Rai- son et al. ,  2018 ). Additionally, we include results of MHQA-GRN ( Song et al. ,  2018 ), from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the un- masked test set (recall that the test set is not pub- licly available and the task organizers only report unmasked results) as well as both versions of the validation set. ", "page_idx": 4, "bbox": [307, 373.2290954589844, 525, 657.358642578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "Entity-GCN (best single model without coref- erence edges) outperforms all previous work by over   $2\\%$   points. We additionally re-ran BiDAF baseline to compare training time: when using a single Titan X GPU, BiDAF and Entity-GCN pro- cess 12.5 and 57.8 document sets per second, re- spectively. Note that  Welbl et al.  ( 2018 ) had to use BiDAF with very small state dimensional i ties ", "page_idx": 4, "bbox": [307, 658.0411987304688, 525, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "table", "page_idx": 5, "img_path": "layout_images/N19-1240_3.jpg", "table_footnote": "Table 2: Accuracy of different models on W IKI H OP  closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pre- trained one (ELMo – without ﬁne-tunning it) and applying R-GCN to reason among entities in the text. \\* with coreference for unmasked dataset and without coreference for the masked one. ", "bbox": [71, 61, 526, 303], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Unmasked Masked\nModel Test Dev | Test Dev\nHuman (Welbl et al., 2018) 74.1 - - -\nFastQA (Welbl et al., 2018) 25.7 - | 358 —-\nBiDAF (Welbl et al., 2018) 429 - |545  -\nCoref-GRU (Dhingra et al., 2018) 59.3 56.0 - =\nMHPGM (Bauer et al., 2018) - 58.2) - -\nWeaver / Jenga (Raison et al., 2018) 65.3 64.1 - -\nMHQA-GRN (Song et al., 2018) 65.4 62.8) —- -\nEntity-GCN without coreference (single model) | 67.6 64.8) - 70.5\nEntity-GCN with coreference (single model) 66.4 65.3) — -\nEntity-GCN* (ensemble 5 models) 71.2 68.5) -— 71.6\n\n", "vlm_text": "The table presents a comparison of different models' performances in terms of test and development (Dev) accuracy scores across two tasks: Unmasked and Masked. The table includes the following models and their results:\n\n1. **Models Referenced from previous works (2018):**\n   - Human (Welbl et al., 2018)\n     - Unmasked Test: 74.1\n     - Other columns (-): No data provided\n   - FastQA (Welbl et al., 2018)\n     - Unmasked Test: 25.7\n     - Masked Test: 35.8\n   - BiDAF (Welbl et al., 2018)\n     - Unmasked Test: 42.9\n     - Masked Test: 54.5\n   - Coref-GRU (Dhingra et al., 2018)\n     - Unmasked Test: 59.3\n     - Unmasked Dev: 56.0\n   - MHPGM (Bauer et al., 2018)\n     - Unmasked Dev: 58.2\n     - Other columns (-): No data provided\n   - Weaver / Jenga (Raison et al., 2018)\n     - Unmasked Test: 65.3\n     - Unmasked Dev: 64.1\n   - MHQA-GRN (Song et al., 2018)\n     - Unmasked Test: 65.4\n     - Unmasked Dev: 62.8\n\n2. **Entity-GCN Models:**\n   - Entity-GCN without coreference (single model)\n     - Unmasked Test: 67.6\n     - Unmasked Dev: 64.8\n     - Masked Dev: 70.5\n   - Entity-GCN with coreference (single model)\n     - Unmasked Test: 66.4\n     - Unmasked Dev: 65.3\n   - Entity-GCN* (ensemble 5 models)\n     - Unmasked Test: 71.2\n     - Unmasked Dev: 68.5\n     - Masked Dev: 71.6\n\nThe table indicates the superior performance of the \"Entity-GCN\" models, particularly when using an ensemble model (Entity-GCN*), which achieves the highest scores on the Unmasked Test and Dev, and Masked Dev sets."}
{"layout": 61, "type": "text", "text": "(20), and smaller batch size due to the scalabil- ity issues (both memory and computation costs). We compare applying the same reductions.   Even- tually, we also report an ensemble of 5 indepen- dently trained models. All models are trained on the same dataset splits with different weight ini- tializations. The ensemble prediction is obtained as    $\\arg\\operatorname*{max}_{c}\\prod_{i=1}^{5}P_{i}(c|q,C_{q},\\bar{S_{q}})$   from each model. ", "page_idx": 5, "bbox": [72, 324.3430480957031, 290, 447], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "3.2 Ablation Study ", "text_level": 1, "page_idx": 5, "bbox": [71, 456, 167, 469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "To help determine the sources of improvements, we perform an ablation study using the publicly available validation set (see Table  3 ). We per- form two groups of ablation, one on the embed- ding layer, to study the effect of ELMo, and one on the edges, to study how different relations af- fect the overall model performance. ", "page_idx": 5, "bbox": [72, 473.3141174316406, 290, 567.754638671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "Embedding ablation We argue that ELMo is crucial, since we do not rely on any other context encoder. However, it is interesting to explore how our R-GCN performs without it. Therefore, in this experiment, we replace the deep contextualized embeddings of both the query and the nodes with GloVe ( Pennington et al. ,  2014 ) vectors (insensi- tive to context). Since we do not have any compo- nent in our model that processes the documents, we expect a drop in performance. In other words, in this ablation our model tries to answer questions without reading the context at all . For example, in Figure  1 , our model would be aware that “Stock- holm” and “Sweden” appear in the same document but any context words, including the ones encod- ing relations (e.g., “is the capital of”) will be hid- den. Besides, in the masked case all mentions be- come ‘unknown’ tokens with GloVe and therefore the predictions are equivalent to a random guess. Once the strong pre-trained encoder is out of the way, we also ablate the use of our R-GCN com- ponent, thus completely depriving the model from inductive biases that aim at multi-hop reasoning. ", "page_idx": 5, "bbox": [72, 576.3974609375, 290, 725.4276123046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "", "page_idx": 5, "bbox": [307, 324.3429870605469, 525, 486.530517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "The ﬁrst important observation is that replacing ELMo by GloVe (GloVe with R-GCN in Table  3 ) still yields a competitive system that ranks far above baselines from ( Welbl et al. ,  2018 ) and even above the Coref-GRU of  Dhingra et al.  ( 2018 ), in terms of accuracy on (unmasked) validation set. The second important observation is that if we then remove R-GCN (GloVe w/o R-GCN in Ta- ble  3 ), we lose 8.0 points. That is, the R-GCN component pushes the model to perform above Coref-GRU still without accessing context, but rather by updating mention representations based on their relation to other ones. These results high- light the impact of our R-GCN component. ", "page_idx": 5, "bbox": [307, 487.1920471191406, 525, 676.4775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "Graph edges ablation In this experiment we in- vestigate the effect of the different relations avail- able in the entity graph and processed by the R- GCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connect- ing mentions in the supporting documents (i.e., us- ", "page_idx": 5, "bbox": [307, 684.7473754882812, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "table", "page_idx": 6, "img_path": "layout_images/N19-1240_4.jpg", "table_footnote": "Table 3: Ablation study on W IKI H OP  validation set. The  full model  is our Entity-GCN with all of its com- ponents and other rows indicate models trained without a component of interest. We also report baselines using GloVe instead of ELMo with and without R-GCN. For the  full model  we report  mean  $\\pm1$   std  over 5 runs. ", "bbox": [72, 61, 290, 329], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model unmasked masked\nfull (ensemble) 68.5 71.6\nfull (single) 65.1 +011 70.4 +0.12\nGloVe with R-GCN 59.2 11.1\nGloVe w/o R-GCN 51.2 11.6\nNo R-GCN 62.4 63.2\nNo relation types 62.7 63.9\nNo DOC-BASED 62.9 65.8\nNo MATCH 64.3 67.4\nNo COREF 64.8 —\nNo COMPLEMENT 64.1 70.3\nInduced edges 61.5 56.4\n", "vlm_text": "The table appears to show the performance of different models or configurations on some task, possibly related to natural language processing or machine learning, given the mention of GloVe, R-GCN, and different components like MATCH, COREF, and COMPLEMENT. The table is divided into columns labeled \"unmasked\" and \"masked,\" suggesting these are two different experimental conditions or evaluation settings. Each row represents a different model or model configuration, and the corresponding values in the \"unmasked\" and \"masked\" columns reflect the performance metrics, possibly accuracy or F1 score.\n\nHere's a breakdown of the rows:\n\n1. **`full (ensemble)`**: This configuration has the highest performance with values of 68.5 (unmasked) and 71.6 (masked).\n   \n2. **`full (single)`**: The single model configuration yields lower performance than the ensemble, with 65.1 ± 0.11 for unmasked and 70.4 ± 0.12 for masked.\n\n3. **`GloVe with R-GCN`**: Shows performance of 59.2 (unmasked) and 11.1 (masked), indicating potentially poor performance in the masked condition.\n\n4. **`GloVe w/o R-GCN`**: Offers 51.2 (unmasked) and 11.6 (masked), suggesting performance drops without R-GCN.\n\n5. **`No R-GCN`**: Performance of 62.4 (unmasked) and 63.2 (masked), showing the effect of removing R-GCN.\n\n6. **`No relation types`**: Scores of 62.7 (unmasked) and 63.9 (masked), indicating results without relation types.\n\n7. **`No DOC–BASED`**: Results of 62.9 (unmasked) and 65.8 (masked), evaluating performance without document-based feature/approach.\n\n8. **`No MATCH`**: Achieves 64.3 (unmasked) and 67.4 (masked), assessing the impact of removing match features.\n\n9. **`No COREF`**: Obtains 64.8 (unmasked) with no corresponding value for masked, testing without coreference features.\n\n10. **`No COMPLEMENT`**: Yields 64.1 (unmasked) and 70.3 (masked), evaluating without complementary features.\n\n11. **`Induced edges`**: Has performance of 61.5 (unmasked) and 56.4 (masked), reflecting results with induced edges.\n\nThe table provides comparative insights into how different features or configurations affect performance in two distinct settings, \"unmasked\" and \"masked.\""}
{"layout": 69, "type": "text", "text": "ing only self-loops – No R-GCN in Table  3 ). The results suggest that W IKIP H OP  genuinely requires multihop inference, as our best model is   $6.1\\%$   and  $8.4\\%$   more accurate than this local model, in un- masked and masked settings, respectively.   How- ever, it also shows that ELMo representations cap- ture predictive context features, without being ex- plicitly trained for the task. It conﬁrms that our goal of getting away with training expensive doc- ument encoders is a realistic one. ", "page_idx": 6, "bbox": [70, 352.7240295410156, 290, 487.81256103515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "We then inspect our model’s effectiveness in making use of the structure encoded in the graph. We start naively by fully-connecting all nodes within and across documents without distinguish- ing edges by type (No relation types in Table  3 ). We observe only marginal improvements with re- spect to ELMo alone (No R-GCN in Table  3 ) in both the unmasked and masked setting suggest- ing that a GCN operating over a naive entity graph would not add much to this task and a more infor- mative graph construction and/or a more sophisti- cated parameter iz ation is indeed needed. ", "page_idx": 6, "bbox": [70, 489.0760803222656, 290, 651.2625732421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "Next, we ablate each type of relations inde- pendently, that is, we either remove connections of mentions that co-occur in the same docu- ment ( DOC-BASED ), connections between men- tions matching exactly ( MATCH ), or edges pre- dicted by the coreference system ( COREF ). The ﬁrst thing to note is that the model makes better use of  DOC-BASED  connections than  MATCH  or COREF  connections. This is mostly because i) the majority of the connections are indeed between mentions in the same document, and ii) without connecting mentions within the same document we remove important information since the model is unaware they appear closely in the document. Secondly, we notice that coreference links and complement edges seem to play a more marginal role. Though it may be surprising for coreference edges, recall that the  MATCH  heuristic already cap- tures the easiest coreference cases, and for the rest the out-of-domain coreference system may not be reliable. Still, modelling all these different rela- tions together gives our Entity-GCN a clear advan- tage. This is our best system evaluating on the de- velopment. Since Entity-GCN seems to gain little advantage using the coreference system, we report test results both with and without using it. Surpris- ingly, with coreference, we observe performance degradation on the test set. It is likely that the test documents are harder for the coreference system. ", "page_idx": 6, "bbox": [70, 652.5250854492188, 290, 733.4165649414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "", "page_idx": 6, "bbox": [307, 63.68701934814453, 525, 374.9156188964844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "We do perform one last ablation, namely, we re- place our heuristic for assigning edges and their labels by a model component that predicts them. The last row of Table  3  (Induced edges) shows model performance when edges are not predeter- mined but predicted. For this experiment, we use a bilinear function    $f_{e}\\big(\\hat{\\mathbf{x}}_{i},\\hat{\\mathbf{x}}_{j}\\big)\\,=\\,\\sigma\\left(\\hat{\\mathbf{x}}_{i}^{\\top}\\mathbf{W}_{e}\\hat{\\mathbf{x}}_{j}\\right)$  \u0000 \u0001 that predicts the importance of a single edge connect- ing two nodes    $i,j$   using the query-dependent rep- resentation of mentions (see Section  2.3 ). The performance drops below ‘No R-GCN’ suggesting that it cannot learn these dependencies on its own. ", "page_idx": 6, "bbox": [307, 376.92913818359375, 525, 539.1156616210938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "Most results are stronger for the masked set- tings even though we do not apply the coreference resolution system in this setting due to masking. It is not surprising as coreferred mentions are la- beled with the same identiﬁer in the masked ver- sion, even if their original surface forms did not match ( Welbl et al.  ( 2018 ) used W IKIPEDIA  links for masking). Indeed, in the masked version, an entity is always referred to via the same unique surface form (e.g.,  MASK1 ) within and across doc- uments. In the unmasked setting, on the other hand, mentions to an entity may differ (e.g., “US” vs “United States”) and they might not be retrieved by the coreference system we are employing, mak- ", "page_idx": 6, "bbox": [307, 541.1292114257812, 525, 730.4146118164062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "table", "page_idx": 7, "img_path": "layout_images/N19-1240_5.jpg", "table_footnote": "Table 4: Accuracy and precision at K (  $\\mathrm{Pe}\\mathrm{w}\\mathrm{K}$   in the table) analysis overall and per query type. Avg.    $|C_{q}|$   indicates the average number of candidates with one standard deviation. ", "bbox": [71, 61, 526, 240], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Relation Accuracy P@2 P@5 Avg. |C,| Supports\n\noverall (ensemble) 68.5 81.0 94.1 20.4+166 5129\n\noverall (single model) 65.3 79.7 92.9 20.4 +166 5129\nmember-_of_political_party 85.5 95.7 98.6 5.4424 70\n\n3 best record_label 83.0 93.6 99.3 124461 283\npublisher 81.5 96.3 100.0 9.645. 54\nplace_of_birth 51.0 67.2 86.8 27.2 +145 309\n\n3 worst place_of_death 50.0 67.3 89.1 25.1 +143 159\ninception 29.9 53.2 83.1 21.9+110 77\n", "vlm_text": "The table displays a comparison of model performance metrics for different relations. It includes measurements of accuracy and precision at 2 and 5 (P@2, P@5), the average size of some quantity (\\(|C_q|\\)), and the number of supports or instances.\n\n### Sections:\n\n1. **Overall Performance:**\n   - **Ensemble:**\n     - **Accuracy:** 68.5\n     - **P@2:** 81.0\n     - **P@5:** 94.1\n     - **Avg. \\(|C_q|\\):** 20.4 ± 16.6\n     - **Supports:** 5129\n   - **Single Model:**\n     - **Accuracy:** 65.3\n     - **P@2:** 79.7\n     - **P@5:** 92.9\n     - **Avg. \\(|C_q|\\):** 20.4 ± 16.6\n     - **Supports:** 5129\n\n2. **Top 3 Best Performing Relations:**\n   - **member_of_political_party:**\n     - **Accuracy:** 85.5\n     - **P@2:** 95.7\n     - **P@5:** 98.6\n     - **Avg. \\(|C_q|\\):** 5.4 ± 2.4\n     - **Supports:** 70\n   - **record_label:**\n     - **Accuracy:** 83.0\n     - **P@2:** 93.6\n     - **P@5:** 99.3\n     - **Avg. \\(|C_q|\\):** 12.4 ± 6.1\n     - **Supports:** 283\n   - **publisher:**\n     - **Accuracy:** 81.5\n     - **P@2:** 96.3\n     - **P@5:** 100.0\n     - **Avg. \\(|C_q|\\):** 9.6 ± 5.1\n     - **Supports:** 54\n\n3. **Top 3 Worst Performing Relations:**\n   - **place_of_birth:**\n     - **Accuracy:** 51.0\n     - **P@2:** 67.2\n     - **P@5:** 86.8\n     - **Avg. \\(|C_q|\\):** 27.2 ± 14.5\n     - **Supports:** 309\n   - **place_of_death:**\n     - **Accuracy:** 50.0\n     - **P@2:** 67.3\n     - **P@5:** 89.1\n     - **Avg. \\(|C_q|\\):** 25.1 ± 14.3\n     - **Supports:** 159\n   - **inception:**\n     -"}
{"layout": 76, "type": "text", "text": "ing the task harder for all models. Therefore, as we rely mostly on exact matching when constructing our graph for the masked case, we are more effec- tive in recovering coreference links on the masked rather than unmasked version. ", "page_idx": 7, "bbox": [71, 262.53704833984375, 290, 329.8795471191406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "4 Error Analysis ", "text_level": 1, "page_idx": 7, "bbox": [71, 341, 166, 353], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model per- forms well or poorly. There are more than 150 query types in the validation set but we ﬁltered the three with the best and with the worst accu- racy that have at least 50 supporting documents and at least 5 candidates. We show results in Ta- ble  4 . We observe that questions regarding places (birth and death) are considered harder for Entity- GCN. We then inspect samples where our model fails while assigning highest likelihood and no- ticed two principal sources of failure i) a mismatch between what is written in W IKIPEDIA  and what is annotated in W IKIDATA , and ii) a different degree of granularity (e.g., born in “London” vs “UK” could be considered both correct by a human but not when measuring accuracy). See Table  6  in the supplement material for some reported samples. ", "page_idx": 7, "bbox": [71, 361.44207763671875, 290, 618.4736328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "Secondly, we study how the model performance degrades when the input graph is large. In particu- lar, we observe a negative Pearson’s correlation (- 0.687) between accuracy and the number of candi- date answers. However, the performance does not decrease steeply. The distribution of the number of candidates in the dataset peaks at 5 and has an av- erage of approximately 20. Therefore, the model does not see many samples where there are a large number of candidate entities during training. Dif- ferently, we notice that as the number of nodes in the graph increases, the model performance drops but more gently (negative but closer to zero Pear- son’s correlation). This is important as document sets can be large in practical applications. See Fig- ure  3  in the supplemental material for plots. ", "page_idx": 7, "bbox": [71, 618.8821411132812, 290, 726.8726196289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "", "page_idx": 7, "bbox": [307, 262.5369873046875, 525, 370.5265197753906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 7, "bbox": [307, 380, 396, 393], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "In previous work, BiDAF ( Seo et al. ,  2016 ), FastQA ( Weissenborn et al. , 2017 ), Coref- GRU ( Dhingra et al. ,  2018 ), MHPGM ( Bauer et al. ,  2018 ), and Weaver / Jenga ( Raison et al. , 2018 ) have been applied to multi-document ques- tion answering. The ﬁrst two mainly focus on sin- gle document QA and  Welbl et al.  ( 2018 ) adapted both of them to work with W IKI H OP . They pro- cess each instance of the dataset by concatenat- ing all    $d~\\in~S_{q}$   in a random order adding doc- ument separator tokens. They trained using the ﬁrst answer mention in the concatenated document and evaluating exact match at test time. Coref- GRU, similarly to us, encodes relations between entity mentions in the document. Instead of us- ing graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform mul- tiple hops of reasoning. Weaver is a deep co- encoding model that uses several alternating bi- LSTMs to process the concatenated documents and the query. ", "page_idx": 7, "bbox": [307, 400.6070556640625, 525, 725.383544921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "Graph neural networks have been shown suc- cessful on a number of NLP tasks ( Marcheggiani and Titov ,  2017 ;  Bastings et al. ,  2017 ;  Zhang et al. , 2018a ), including those involving document level modeling ( Peng et al. ,  2017 ). They have also been applied in the context of asking questions about knowledge contained in a knowledge base ( Zhang et al. ,  2018b ). In  Schlichtkrull et al.  ( 2018 ), GCNs are used to capture reasoning chains in a knowl- edge base. Our work and unpublished concurrent work by  Song et al.  ( 2018 ) are the ﬁrst to study graph neural networks in the context of multi- document QA. Besides differences in the architec- ture,  Song et al.  ( 2018 ) propose to train a combi- nation of a graph recurrent network and an RNN encoder. We do not train any RNN document en- coders in this work. ", "page_idx": 7, "bbox": [307, 725.787109375, 525, 766.0315551757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "", "page_idx": 8, "bbox": [71, 63.68701934814453, 290, 252.97256469726562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8, "bbox": [71, 263, 148, 275], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "We designed a graph neural network that oper- ates over a compact graph representation of a set of documents where nodes are mentions to en- tities and edges signal relations such as within and cross-document coreference. The model learns to answer questions by gathering evidence from different documents via a differentiable mes- sage passing algorithm that updates node repre- sentations based on their neighbourhood. Our model outperforms published results where abla- tions show substantial evidence in favour of multi- step reasoning. Moreover, we make the model fast by using pre-trained (contextual) embeddings. ", "page_idx": 8, "bbox": [71, 282.86712646484375, 290, 458.6026916503906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 8, "bbox": [72, 469, 166, 481], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "We would like to thank Johannes Welbl for help- ing to test our system on W IKI H OP . This project is supported by SAP Innovation Center Network, ERC Starting Grant BroadSem (678254) and the Dutch Organization for Scientiﬁc Re- search (NWO) VIDI 639.022.518. Wilker Aziz is supported by the Dutch Organisation for Scientiﬁc Research (NWO) VICI Grant nr. 277-89-002. ", "page_idx": 8, "bbox": [71, 488.4981994628906, 290, 596.4876708984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [71, 619, 128, 631], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation . In  Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing , pages 1957–1967. Association for Com- putational Linguistics. ", "page_idx": 8, "bbox": [71, 636.8677978515625, 290, 714.625732421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question an- swering tasks . In  Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language ", "page_idx": 8, "bbox": [71, 720.8837890625, 290, 765.6561279296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "Processing , pages 4220–4230. Association for Com- putational Linguistics. ", "page_idx": 8, "bbox": [318, 64.56182861328125, 525, 87.52574920654297], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding.  arXiv preprint arXiv:1810.04805 . ", "page_idx": 8, "bbox": [307, 96.0858154296875, 525, 140.9676971435547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co- hen, and Ruslan Salakhutdinov. 2018.  Neural mod- els for reasoning over multiple mentions using coref- erence . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers) , pages 42–48, New Orleans, Louisiana. Association for Computa- tional Linguistics. ", "page_idx": 8, "bbox": [307, 149.52874755859375, 525, 249.20457458496094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In  Advances in Neu- ral Information Processing Systems , pages 1693– 1701. ", "page_idx": 8, "bbox": [307, 257.7646484375, 525, 324.56451416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In  Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , volume 1, pages 1601–1611. ", "page_idx": 8, "bbox": [307, 333.12457275390625, 525, 410.8834228515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization.  Kinga, D., and J. Ba Adam. ”A method for stochastic optimization.” International Conference on Learning Representa- tions (ICLR). , 5. ", "page_idx": 8, "bbox": [307, 419.4434814453125, 525, 475.28436279296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Thomas N Kipf and Max Welling. 2017. Semi- supervised classiﬁcation with graph convolutional networks. International Conference on Learning Representations (ICLR) . ", "page_idx": 8, "bbox": [307, 483.84442138671875, 525, 528.726318359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018.  The NarrativeQA read- ing comprehension challenge .  Transactions of the Association for Computational Linguistics , 6:317– 328. ", "page_idx": 8, "bbox": [307, 537.286376953125, 525, 604.0862426757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.  RACE: Large-scale read- ing comprehension dataset from examinations . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 785–794, Copenhagen, Denmark. Association for Computational Linguistics. ", "page_idx": 8, "bbox": [307, 612.6463623046875, 525, 690.4052734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017.  End-to-end neural coreference reso- lution . In  Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing , pages 188–197. Association for Computational Linguistics. ", "page_idx": 8, "bbox": [307, 698.96533203125, 525, 765.7652587890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Diego Marcheggiani and Ivan Titov. 2017.  Encoding sentences with graph convolutional networks for se- mantic role labeling . In  Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing , pages 1506–1515. Association for Computational Linguistics. ", "page_idx": 9, "bbox": [71, 64.56158447265625, 290, 131.36143493652344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In  Advances in neural information processing systems , pages 3111–3119. ", "page_idx": 9, "bbox": [71, 142.35748291015625, 290, 198.19737243652344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017.  Cross-sentence n-ary relation extraction with graph lstms .  Transac- tions of the Association for Computational Linguis- tics , 5:101–115. ", "page_idx": 9, "bbox": [71, 209.19342041015625, 290, 265.0343017578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In  Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP) , pages 1532–1543. ", "page_idx": 9, "bbox": [71, 276.02935791015625, 290, 331.8702392578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.  Deep contextualized word rep- resentations . In  Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers) , pages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics. ", "page_idx": 9, "bbox": [71, 342.8663024902344, 290, 442.54217529296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning.  Technical re- port, OpenAI . ", "page_idx": 9, "bbox": [71, 453.5382385253906, 290, 498.419189453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "Martin Raison, Pierre-Emmanuel Mazar´ e, Rajarshi Das, and Antoine Bordes. 2018. Weaver: Deep co- encoding of questions and documents for machine reading.  In Proceedings of the International Con- ference on Machine Learning (ICML) . ", "page_idx": 9, "bbox": [71, 509.365234375, 290, 565.2561645507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.  SQuAD:   $100{,}000{+}$   questions for machine comprehension of text . In  Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing , pages 2383–2392, Austin, Texas. Association for Computational Linguistics. ", "page_idx": 9, "bbox": [71, 576.251220703125, 290, 643.0511474609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolu- tional networks. In  The Semantic Web , pages 593– 607, Cham. Springer International Publishing. ", "page_idx": 9, "bbox": [71, 654.0472412109375, 290, 709.8871459960938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2017. Reasonet: Learning to stop reading in machine comprehension. In  Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1047–1055. ACM. Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. 2018. Exploring Graph-structured Passage Representation for Multi- hop Reading Comprehension with Graph Neural Networks.  arXiv preprint arXiv:1809.02040 . Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overﬁtting.  The Journal of Machine Learning Research , 15(1):1929–1958. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In  Advances in Neural Information Pro- cessing Systems , pages 5998–6008. Denny Vrandeˇ ci´ c. 2012. Wikidata: A new platform for collaborative data collection. In  Proceedings of the 21st International Conference on World Wide Web , pages 1063–1064. ACM. Dirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017.  Making neural qa as simple as possible but not simpler . In  Proceedings of the 21st Confer- ence on Computational Natural Language Learn- ing (CoNLL 2017) , pages 271–280. Association for Computational Linguistics. Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018.  Constructing datasets for multi-hop reading comprehension across documents .  Transac- tions of the Association for Computational Linguis- tics , 6:287–302. Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering.  arXiv preprint arXiv:1611.01604 . Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018a.  Graph convolution over pruned dependency trees improves relation extraction . In  Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing , pages 2205–2215. Asso- ciation for Computational Linguistics. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan- der J Smola, and Le Song. 2018b. Variational reasoning for question answering with knowledge graph.  The Thirty-Second AAAI Conference on Ar- tiﬁcial Intelligence (AAAI-18) . ", "page_idx": 9, "bbox": [307, 64.56121826171875, 525, 694.249755859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional atten- tion ﬂow for machine comprehension.  International Conference on Learning Representations (ICLR) . ", "page_idx": 9, "bbox": [71.99995422363281, 720.8831787109375, 290, 765.76513671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "A Implementation and Experiments Details ", "text_level": 1, "page_idx": 10, "bbox": [71, 63, 266, 89], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "A.1 Architecture ", "text_level": 1, "page_idx": 10, "bbox": [71, 99, 158, 110], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "See table  5  for an outline of Entity-GCN architec- tural detail. Here the computational steps ", "page_idx": 10, "bbox": [70, 115.35199737548828, 290, 142.04647827148438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "1. ELMo embeddings are a concatenation of three 1024-dimensional vectors resulting in 3072-dimensional input vectors    $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$  . ", "page_idx": 10, "bbox": [80, 148.6599884033203, 290, 196.3117218017578], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "2. For the query representation  q , we apply 2 bi-LSTM layers of 256 and 128 hidden units to its ELMo vectors. The concatenation of the forward and backward states results in a 256-dimensional question representation. ", "page_idx": 10, "bbox": [80, 196.9099884033203, 290, 264.2514953613281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "3. ELMo embeddings of candidates are pro- jected to 256-dimensional vectors, concate- nated to the    $\\mathbf{q}$  , and further transformed with a two layers MLP of 1024 and 512 hidden units in 512-dimensional query aware entity representations    $\\{\\hat{\\mathbf{x}}_{i}\\}_{i=1}^{N}\\in\\bar{\\mathbb{R}}^{51\\bar{2}}$  . ", "page_idx": 10, "bbox": [80, 272.25701904296875, 290, 360.5558166503906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "4. All transformations    $f_{*}$  in R-GCN-layers are afﬁne and they do maintain the input and out- put dimensionality of node representations the same (512-dimensional). ", "page_idx": 10, "bbox": [80, 361, 290, 414.9475402832031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 120, "type": "text", "text": "5. Eventually, a 2-layers MLP with [256, 128] hidden units takes the concatenation between  $\\{\\mathbf{h}_{i}^{(L)}\\}_{i=1}^{N}$    and    $\\mathbf{q}$   to predict the probability that a candidate node    $v_{i}$   may be the answer to the query  $q$   (see Equation  1 ). ", "page_idx": 10, "bbox": [80, 422.95306396484375, 290, 490.2955627441406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 121, "type": "text", "text": "During preliminary trials, we experimented with different numbers of R-GCN-layers (in the range 1-7). We observed that with W IKI H OP , for  $L\\geq3$   models reach essentially the same perfor- mance, but more layers increase the time required to train them. Besides, we observed that the gating mechanism learns to keep more and more informa- tion from the past at each layer making unneces- sary to have more layers than required. ", "page_idx": 10, "bbox": [70, 496.9090881347656, 290, 618.4475708007812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 122, "type": "text", "text": "B Error Analysis ", "text_level": 1, "page_idx": 10, "bbox": [306, 63, 404, 77], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 123, "type": "text", "text": "In Table  6 , we report three samples from W IKI - H OP  development set where out Entity-GCN fails. In particular, we show two instances where our model presents high conﬁdence on the answer, and one where is not. We commented these sam- ples explaining why our model might fail in these cases. ", "page_idx": 10, "bbox": [307, 84.60016632080078, 525, 179.04067993164062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 124, "type": "text", "text": "C Ablation Study ", "text_level": 1, "page_idx": 10, "bbox": [306, 189, 406, 202], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 125, "type": "text", "text": "In Figure  3 , we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of can- didate answers or the number of nodes increases. ", "page_idx": 10, "bbox": [307, 210.57423400878906, 525, 264.3667297363281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 126, "type": "image", "page_idx": 10, "img_path": "layout_images/N19-1240_6.jpg", "bbox": [305, 273, 524, 384], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "", "vlm_text": "The image is a histogram plot showing two overlaid distributions, colored in blue and brown. The x-axis has a range from 0 to 70, and the y-axis ranges from 0.0 to 1.0, indicating normalized frequency or proportion. The plot also features a horizontal dashed line at approximately 0.6 on the y-axis. This line could indicate a threshold or reference value against which the data in the histogram is being compared. The blue histogram appears to extend higher and covers the entire range, while the brown histogram is shorter and more concentrated towards the left of the plot."}
{"layout": 127, "type": "image", "page_idx": 10, "img_path": "layout_images/N19-1240_7.jpg", "img_caption": "(a) Candidates set size (  $\\mathbf{\\acute{X}}$  -axis) and accuracy (  $\\mathbf{\\bar{y}}$  -axis). Pear- son’s correlation of    $-0.687$     $(p<10^{-7})$  . ", "bbox": [305, 387, 526, 518], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "", "vlm_text": "The image is a bar chart depicting two data sets represented by stacked blue and brown bars. The x-axis is labeled \"Candidates set size,\" which increases from 0 to 200. The y-axis is labeled \"accuracy,\" ranging from 0.0 to 1.0. A horizontal dashed line across the chart likely signifies a particular threshold or mean value for accuracy, around 0.6.\n\nThe chart also displays a summary statistic in the caption text, mentioning Pearson's correlation coefficient of -0.687 with a p-value of less than 10^-7, indicating a strong negative correlation between candidates set size and accuracy that is highly statistically significant."}
{"layout": 128, "type": "text", "text": "Figure 3: Accuracy (blue) of our best single model with respect to the candidate set size (on the  top ) and nodes set size (on the  bottom ) on the validation set. Re- scaled data distributions (orange) per number of candi- date   $(t o p)$   and nodes ( bottom ). Dashed lines indicate average accuracy. ", "page_idx": 10, "bbox": [307, 553.0245971679688, 525, 624.8045043945312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 129, "type": "text", "text": "A.2 Training Details ", "text_level": 1, "page_idx": 10, "bbox": [71, 628, 175, 640], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 130, "type": "text", "text": "We train our models with a batch size of 32 for at most 20 epochs using the Adam opti- mizer ( Kingma and Ba ,  2015 ) with    $\\beta_{1}~=~0.9,$  ,  $\\beta_{2}\\,=\\,0.999$   and a learning rate of    $10^{-4}$  . To help against overﬁtting, we employ dropout (drop rate  $\\in0,0.1,0.15,0.2,0.25)$   ( Srivastava et al. ,  2014 ) and early-stopping on validation accuracy. We re- port the best results of each experiment based on accuracy on validation set. ", "page_idx": 10, "bbox": [70, 644.4921264648438, 290, 766.0315551757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 131, "type": "table", "page_idx": 11, "img_path": "layout_images/N19-1240_8.jpg", "table_caption": "Table 5: Model architecture. ", "bbox": [151, 90, 447, 284], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Input -q, (vi),\n\nquery ELMo 3072-dim | candidates ELMo 3072-dim\n\n2 layers bi-LSTM [256, 128]-dim | 1 layer FF 256-dim\n\nconcatenation 512-dim\n\n2 layer FF [1024, 512]-dim: : {%:},\n\n3 layers R-GCN 512-dim each (shared parameters)\n\nconcatenation with q 768-dim\n\n3 layers FF [256,128,1]-dim\n\nOutput - probabilities over C,\n\n", "vlm_text": "The table outlines a neural network architecture used for processing queries and candidates, leveraging ELMo embeddings and several neural network layers. Here's a breakdown of the components:\n\n1. **Input**:\n   - **q**: The query input.\n   - \\(\\{v_i\\}_{i=1}^N\\): The candidate set for each query.\n\n2. **Query and Candidates Representation**:\n   - Query and candidates are represented using **ELMo embeddings**, each having a 3072-dimensional vector.\n\n3. **Processing Layers**:\n   - The query is passed through **2 layers of bi-directional LSTM** resulting in dimensions [256, 128].\n   - The candidates are passed through **1 layer of a Feed-Forward network** with a 256-dimensional output.\n\n4. **Concatenation Step**:\n   - A concatenation operation resulting in a 512-dimensional vector.\n\n5. **Intermediate Processing**:\n   - The concatenated embeddings are further processed by a **2 layer Feed-Forward network** with dimensions [1024, 512], represented as \\(\\{\\hat{x}_i\\}_{i=1}^N\\).\n\n6. **Graph Convolutional Network**:\n   - A **3-layer Relational Graph Convolutional Network (R-GCN)** with each layer having 512 dimensions, utilizing shared parameters.\n\n7. **Integration with Query**:\n   - Another concatenation operation that combines the processed embedding from R-GCN with the query, resulting in a 768-dimensional representation.\n\n8. **Final Processing**:\n   - The concatenated result undergoes further processing through **3 layers of a Feed-Forward network** with dimensions [256, 128, 1].\n\n9. **Output**:\n   - The final output is a set of **probabilities over \\(C_q\\)**, where \\(C_q\\) likely represents the candidate space or classes related to the query.\n  \nThis architecture seems to be designed for tasks like question answering or selection of relevant candidates based on a query, employing ELMo for contextual embeddings and layers of LSTMs, R-GCN, and feed-forward networks for processing and decision making."}
{"layout": 132, "type": "table", "page_idx": 11, "img_path": "layout_images/N19-1240_9.jpg", "table_footnote": "(c) In this sample, there is ambiguity between two entities since the city Esl¨ ov is located in the Scania County (English name of Sk˚ ane County). The model assigning high probability to the city and it cannot select the county. ", "bbox": [70, 348, 527, 713], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "ID | WH_dev_2257 | Gold answer | 2003 (p = 14.1)\nQuery | inception (of) Derrty Entertainment | Predicted answer | 2000 (p = 15.8)\nSupport 1 | Derrty Entertainment is a record label founded by [...]. The first album released under\nDerrty Entertainment was Nelly ’s Country Grammar.\nSupport 2\n\nCountry Grammar is the debut single by American rapper Nelly. The song was pro-\nduced by Jason Epperson. It was released in 2000, [...]\n\n(a) In this example, the model predicts the answer correctly.\nWIKIPEDIA and\n\nID\n\nHowever, there is a mismatch between what is written in\nwhat is annotated in WIKIDATA. In WIKIHOP, answers are generated with WIKIDATA.\n\nWH.-dev_2401 | Gold answer | Adolph Zukor (p = 7.le—4%)\n\nQuery\n\nproducer (of) Forbidden Paradise | Predicted answer | Jesse L. Lask (p = 99.9%)\n\nSupport 1\n\nForbidden Paradise is a [...] drama film produced by Famous Players-Lasky [...]\n\nSupport 2\n\nFamous Players-Lasky Corporation was [...] from the merger of Adolph Zukor’s Fa-\nmous Players Film Company [..] and the Jesse L. Lasky Feature Play Company.\n\n(b) In this sample, there is ambiguity between two entities since both are correct answers reading the passages but only one is\nmarked as correct. The model fails assigning very high probability to only on one of them.\n\nID | WH_dev_3030 | Gold answer | Scania (p = 0.029%)\n\nQuery | place_of_birth (of) Erik Penser | Predicted answer | Eslév (p = 97.3%)\nSupport 1 | Nils Wilhelm Erik Penser (born August 22, 1942, in Eslév, Skane) is a Swedish [...]\nSupport 2\n\nSkane County, sometimes referred to as “ Scania County ” in English, is the [...]\n", "vlm_text": "This table presents examples from a model's predictions versus gold (correct) answers across different queries. It shows:\n\n1. **First Example (ID: WH_dev_2257)**\n   - **Query:** Inception of Derrty Entertainment\n   - **Gold Answer:** 2003\n   - **Predicted Answer:** 2000\n   - **Support Passages:** Discuss the debut album \"Country Grammar.\"\n\n2. **Second Example (ID: WH_dev_2401)**\n   - **Query:** Producer of Forbidden Paradise\n   - **Gold Answer:** Adolph Zukor\n   - **Predicted Answer:** Jesse L. Lask\n   - **Support Passages:** Reference Famous Players-Lasky Corporation and its founders.\n\n3. **Third Example (ID: WH_dev_3030)**\n   - **Query:** Place of birth of Erik Penser\n   - **Gold Answer:** Scania\n   - **Predicted Answer:** Eslöv\n   - **Support Passages:** Mention Eslöv, Skåne as part of Scania County.\n\nFootnotes explain discrepancies, such as mismatches between Wikipedia and Wikidata for the first example, and ambiguity in support passages for the second example."}
{"layout": 133, "type": "text", "text": "Table 6: Samples from W IKI H OP  set where Entity-GCN fails.    $p$   indicates the predicted likelihood. ", "page_idx": 11, "bbox": [101.88200378417969, 721.9885864257812, 495.662353515625, 734], "page_size": [595.2760009765625, 841.8900146484375]}
