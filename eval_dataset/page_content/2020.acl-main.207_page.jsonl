{"page": 0, "image_path": "doc_images/2020.acl-main.207_0.jpg", "ocr_text": "SPECTER: Document-level Representation Learning using\nCitation-informed Transformers\n\nArman Cohan!*\n\nSergey Feldman‘* Iz Beltagy'\n\nDoug Downey' Daniel S. Weld‘?\n\n‘Allen Institute for Artificial Intelligence\nPaul G. Allen School of Computer Science & Engineering, University of Washington\n\n{armanc, sergey,beltagy, dougd, danw}@allenai -Org\n\nAbstract\n\nRepresentation learning is a critical ingre-\ndient for natural language processing sys-\ntems. Recent Transformer language mod-\nels like BERT learn powerful textual repre-\nsentations, but these models are targeted to-\nwards token- and sentence-level training ob-\njectives and do not leverage information on\ninter-document relatedness, which limits their\ndocument-level representation power. For ap-\nplications on scientific documents, such as\nclassification and recommendation, the em-\nbeddings power strong performance on end\ntasks. We propose SPECTER, a new method to\ngenerate document-level embedding of scien-\ntific documents based on pretraining a Trans-\n‘ormer language model on a powerful signal\nof document-level relatedness: the citation\ngraph. Unlike existing pretrained language\nmodels, SPECTER can be easily applied to\ndownstream applications without task-specific\nfine-tuning. Additionally, to encourage further\nresearch on document-level models, we intro-\nduce SCIDOCS, a new evaluation benchmark\nconsisting of seven document-level tasks rang-\ning from citation prediction, to document clas-\nsification and recommendation. We show that\nSPECTER outperforms a variety of competitive\nbaselines on the benchmark.!\n\n1 Introduction\n\nAs the pace of scientific publication continues to\nincrease, Natural Language Processing (NLP) tools\nthat help users to search, discover and understand\nthe scientific literature have become critical. In re-\ncent years, substantial improvements in NLP tools\nhave been brought about by pretrained neural lan-\nguage models (LMs) (Radford et al., 2018; Devlin\net al., 2019; Yang et al., 2019). While such models\nare widely used for representing individual words\n\n* Equal contribution\n'https://github.com/allenai/specter\n\nor sentences, extensions to whole-document em-\nbeddings are relatively underexplored. Likewise,\nmethods that do use inter-document signals to pro-\nduce whole-document embeddings (Tu et al., 2017;\nChen et al., 2019) have yet to incorporate state-\nof-the-art pretrained LMs. Here, we study how to\nleverage the power of pretrained language models\nto learn embeddings for scientific documents.\n\nA paper’s title and abstract provide rich seman-\ntic content about the paper, but, as we show in\nthis work, simply passing these textual fields to an\n“off-the-shelf” pretrained language model—even a\nstate-of-the-art model tailored to scientific text like\nthe recent SciBERT (Beltagy et al., 2019)—does\nnot result in accurate paper representations. The\nlanguage modeling objectives used to pretrain the\nmodel do not lead it to output representations that\nare helpful for document-level tasks such as topic\nclassification or recommendation.\n\nIn this paper, we introduce a new method for\nlearning general-purpose vector representations of\nscientific documents. Our system, SPECTER, in-\ncorporates inter-document context into the Trans-\nformer (Vaswani et al., 2017) language models\n(e.g., SciBERT (Beltagy et al., 2019)) to learn\ndocument representations that are effective across\na wide-variety of downstream tasks, without the\nneed for any task-specific fine-tuning of the pre-\ntrained language model. We specifically use cita-\ntions as a naturally occurring, inter-document in-\ncidental supervision signal indicating which docu-\nments are most related and formulate the signal into\na triplet-loss pretraining objective. Unlike many\nprior works, at inference time, our model does not\nrequire any citation information. This is critical\nfor embedding new papers that have not yet been\ncited. In experiments, we show that SPECTER’s\nrepresentations substantially outperform the state-\n\n? SPECTER: Scientific Paper Embeddings using Citation-\ninformed TransformERs\n\n2270\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270-2282\nJuly 5 - 10, 2020. ©2020 Association for Computational Linguistics\n", "vlm_text": "SPECTER: Document-level Representation Learning using Citation-informed Transformers \nArman Cohan † ∗ Sergey Feldman † ∗ Iz Beltagy † Doug Downey † Daniel S. Weld † , ‡ † Allen Institute for Artiﬁcial Intelligence \n\n‡ Paul G. Allen School of Computer Science & Engineering, University of Washington { armanc,sergey,beltagy,dougd,danw } @allenai.org \nAbstract \nRepresentation learning is a critical ingre- dient for natural language processing sys- tems. Recent Transformer language mod- els like BERT learn powerful textual repre- sentations, but these models are targeted to- wards token- and sentence-level training ob- jectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For ap- plications on scientiﬁc documents, such as classiﬁcation and recommendation, the em- beddings power strong performance on end tasks. We propose S PECTER , a new method to generate document-level embedding of scien- tiﬁc documents based on pretraining a Trans- former language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, S PECTER  can be easily applied to downstream applications without task-speciﬁc ﬁne-tuning. Additionally, to encourage further research on document-level models, we intro- duce S CI D OCS , a new evaluation benchmark consisting of seven document-level tasks rang- ing from citation prediction, to document clas- siﬁcation and recommendation. We show that S PECTER  outperforms a variety of competitive baselines on the benchmark. \n1 Introduction \nAs the pace of scientiﬁc publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientiﬁc literature have become critical. In re- cent years, substantial improvements in NLP tools have been brought about by pretrained neural lan- guage models (LMs) ( Radford et al. ,  2018 ;  Devlin et al. ,  2019 ;  Yang et al. ,  2019 ). While such models are widely used for representing individual words or sentences, extensions to whole-document em- beddings are relatively underexplored. Likewise, methods that do use inter-document signals to pro- duce whole-document embeddings ( Tu et al. ,  2017 ; Chen et al. ,  2019 ) have yet to incorporate state- of-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientiﬁc documents. \n\nA paper’s title and abstract provide rich seman- tic content about the paper, but, as we show in this work, simply passing these textual ﬁelds to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientiﬁc text like the recent SciBERT ( Beltagy et al. ,  2019 )—does not  result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classiﬁcation or recommendation. \nIn this paper, we introduce a new method for learning general-purpose vector representations of scientiﬁc documents. Our system, S PECTER ,   in- corporates inter-document context into the Trans- former ( Vaswani et al. ,  2017 ) language models (e.g., SciBERT ( Beltagy et al. ,  2019 )) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-speciﬁc ﬁne-tuning of the pre- trained language model. We speciﬁcally use cita- tions as a naturally occurring, inter-document in- cidental supervision signal indicating which docu- ments are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that S PECTER ’s representations substantially outperform the state- of-the-art on a variety of document-level tasks, in- cluding topic classiﬁcation, citation prediction, and recommendation. "}
{"page": 1, "image_path": "doc_images/2020.acl-main.207_1.jpg", "ocr_text": "of-the-art on a variety of document-level tasks, in-\ncluding topic classification, citation prediction, and\nrecommendation.\n\nAs an additional contribution of this work, we in-\ntroduce and release ScIDOCS? , a novel collection\nof data sets and an evaluation suite for document-\nlevel embeddings in the scientific domain. Sct-\nDOCS covers seven tasks, and includes tens of thou-\nsands of examples of anonymized user signals of\ndocument relatedness. We also release our training\nset (hundreds of thousands of paper titles, abstracts\nand citations), along with our trained embedding\nmodel and its associated code base.\n\n2 Model\n\n2.1 Overview\n\nOur goal is to learn task-independent representa-\ntions of academic papers. Inspired by the recent\nsuccess of pretrained Transformer language models\nacross various NLP tasks, we use the Transformer\nmodel architecture as basis of encoding the input\npaper. Existing LMs such as BERT, however, are\nprimarily based on masked language modeling ob-\njective, only considering intra-document context\nand do not use any inter-document information.\nThis limits their ability to learn optimal document\nrepresentations. To learn high-quality document-\nlevel representations we propose using citations as\nan inter-document relatedness signal and formu-\nlate it as a triplet loss learning objective. We then\npretrain the model on a large corpus of citations\nusing this objective, encouraging it to output rep-\nresentations that are more similar for papers that\nshare a citation link than for those that do not. We\ncall our model SPECTER, which learns Scientific\nPaper Embeddings using Citation-informed Trans-\nformERs. With respect to the terminology used by\nDevlin et al. (2019), unlike most existing LMs that\nare “fine-tuning based”, our approach results in em-\nbeddings that can be applied to downstream tasks\nin a “feature-based” fashion, meaning the learned\npaper embeddings can be easily used as features,\nwith no need for further task-specific fine-tuning.\nIn the following, as background information, we\nbriefly describe how pretrained LMs can be applied\nfor document representation and then discuss the\ndetails of SPECTER.\n\nShttps://github.com/allenai/scidocs\n\nQuery paper (P®)| | Related paper (P*)| | Unrelated paper (P~)\n_/\n¢\nSN | -\n\n‘Transformer (initialized with SciBERT)\n\n. |\n\n‘Triplet loss max { (a (P2, P+) —d (P2, P )+m) o}\n\nFigure 1: Overview of SPECTER.\n\n2.2 Background: Pretrained Transformers\n\nRecently, pretrained Transformer networks have\ndemonstrated success on various NLP tasks (Rad-\nord et al., 2018; Devlin et al., 2019; Yang et al.,\n2019; Liu et al., 2019); we use these models as\nhe foundation for SPECTER. Specifically, we use\nSciBERT (Beltagy et al., 2019) which is an adap-\nation of the original BERT (Devlin et al., 2019)\narchitecture to the scientific domain. The BERT\nmodel architecture (Devlin et al., 2019) uses multi-\nple layers of Transformers (Vaswani et al., 2017) to\nencode the tokens in a given input sequence. Each\nlayer consists of a self-attention sublayer followed\nby a feedforward sublayer. The final hidden state\nassociated with the special [CLS] token is usually\ncalled the “pooled output”, and is commonly used\nas an aggregate representation of the sequence.\n\nDocument Representation Our goal is to repre-\nsent a given paper P as a dense vector v that best\nrepresents the paper and can be used in downstream\ntasks. SPECTER builds embeddings from the title\nand abstract of a paper. Intuitively, we would ex-\npect these fields to be sufficient to produce accurate\nembeddings, since they are written to provide a suc-\ncinct and comprehensive summary of the paper.*\nAs such, we encode the concatenated title and ab-\nstract using a Transformer LM (e.g., SciBERT) and\ntake the final representation of the [CLS] token as\nthe output representation of the paper:\n\nv=Transformer(input);crs}, (1)\n\nwhere Transformer is the Transformer’s for-\nward function, and input is the concatenation of\nthe [CLS] token and WordPieces (Wu et al., 2016)\nof the title and abstract of a paper, separated by\n\n*We also experimented with additional fields such as\nvenues and authors but did not find any empirical advantage\nin using those (see §6). See §7 for a discussion of using the\nfull text of the paper as input.\n\nSIt is also possible to encode title and abstracts individually\nand then concatenate or combine them to get the final embed-\nding. However, in our experiments this resulted in sub-optimal\nperformance.\n\n2271\n", "vlm_text": "\nAs an additional contribution of this work, we in- troduce and release S CI D OCS 3   , a novel collection of data sets and an evaluation suite for document- level embeddings in the scientiﬁc domain. S CI - D OCS  covers seven tasks, and includes tens of thou- sands of examples of anonymized user signals of document relatedness. We also release our training set (hundreds of thousands of paper titles, abstracts and citations), along with our trained embedding model and its associated code base. \n2 Model \n2.1 Overview \nOur goal is to learn task-independent representa- tions of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling ob- jective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality document- level representations we propose using citations as an inter-document relatedness signal and formu- late it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output rep- resentations that are more similar for papers that share a citation link than for those that do not. We call our model S PECTER , which learns Scientiﬁc Paper Embeddings using Citation-informed Trans- formERs. With respect to the terminology used by Devlin et al.  ( 2019 ), unlike most existing LMs that are “ﬁne-tuning based”, our approach results in em- beddings that can be applied to downstream tasks in a “feature-based” fashion, meaning the learned paper embeddings can be easily used as features, with no need for further task-speciﬁc ﬁne-tuning. In the following, as background information, we brieﬂy describe how pretrained LMs can be applied for document representation and then discuss the details of S PECTER . \nThe image provides an overview of the SPECTER model, which is used for document embedding, specifically for scientific paper analysis. It utilizes a transformer model that is initialized with SciBERT, which is a BERT-based model pre-trained on scientific text data. The image outlines the inputs to the transformer which include a query paper (denoted as \\(P^Q\\)), a related paper (denoted as \\(P^+\\)), and an unrelated paper (denoted as \\(P^-\\)). \n\nThe outputs are embeddings of these papers, which are used to compute a triplet loss. The formula for triplet loss given in the image is:\n\\[ \\text{Triplet loss} = \\max \\left( d(P^Q, P^+) - d(P^Q, P^-) + m, 0 \\right) \\]\nwhere \\(d\\) represents a distance measure between the embeddings, and \\(m\\) is a margin parameter used in the triplet loss calculation. The goal is to ensure that the distance between the query and the related paper is smaller than the distance between the query and the unrelated paper by at least the margin \\(m\\).\n2.2 Background: Pretrained Transformers \nRecently, pretrained Transformer networks have demonstrated success on various NLP tasks ( Rad- ford et al. ,  2018 ;  Devlin et al. ,  2019 ;  Yang et al. , 2019 ;  Liu et al. ,  2019 ); we use these models as the foundation for S PECTER . Speciﬁcally, we use SciBERT ( Beltagy et al. ,  2019 ) which is an adap- tation of the original BERT ( Devlin et al. ,  2019 ) architecture to the scientiﬁc domain. The BERT model architecture ( Devlin et al. ,  2019 ) uses multi- ple layers of Transformers ( Vaswani et al. ,  2017 ) to encode the tokens in a given input sequence. Each layer consists of a self-attention sublayer followed by a feedforward sublayer. The ﬁnal hidden state associated with the special  [CLS]  token is usually called the “pooled output”, and is commonly used as an aggregate representation of the sequence. \nDocument Representation Our goal is to repre- sent a given paper  $\\mathcal{P}$   as a dense vector  v  that best represents the paper and can be used in downstream tasks. S PECTER  builds embeddings from the title and abstract of a paper. Intuitively, we would ex- pect these ﬁelds to be sufﬁcient to produce accurate embeddings, since they are written to provide a suc- cinct and comprehensive summary of the paper. As such, we encode the concatenated title and ab- stract using a Transformer LM (e.g., SciBERT) and take the ﬁnal representation of the  [CLS]  token as the output representation of the paper: 5 \n\n$$\n\\mathbf{v}={\\tt T r a n s f o r m e r(i n p u t)}_{\\tt[C L S]}\\,,\n$$\n \nwhere  Transformer  is the Transformer’s for- ward function, and  input  is the concatenation of the  [CLS]  token and WordPieces ( Wu et al. ,  2016 ) of the title and abstract of a paper, separated by the  [SEP]  token. We use SciBERT as our model initialization as it is optimized for scientiﬁc text, though our formulation is general and any Trans- former language model instead of SciBERT. Using the above method with an “off-the-shelf” SciBERT does not take global inter-document information into account. This is because SciBERT, like other pretrained language models, is trained via language modeling objectives, which only predict words or sentences given their in-document, nearby textual context. In contrast, we propose to incorporate ci- tations into the model as a signal of inter-document relatedness, while still leveraging the model’s ex- isting strength in modeling language. "}
{"page": 2, "image_path": "doc_images/2020.acl-main.207_2.jpg", "ocr_text": "the [SEP] token. We use SciBERT as our model\ninitialization as it is optimized for scientific text,\nthough our formulation is general and any Trans-\nformer language model instead of SciBERT. Using\nthe above method with an “off-the-shelf” SciBERT\ndoes not take global inter-document information\ninto account. This is because SciBERT, like other\npretrained language models, is trained via language\nmodeling objectives, which only predict words or\nsentences given their in-document, nearby textual\ncontext. In contrast, we propose to incorporate ci-\ntations into the model as a signal of inter-document\nrelatedness, while still leveraging the model’s ex-\nisting strength in modeling language.\n\n2.3 Citation-Based Pretraining Objective\n\nA citation from one document to another suggests\nthat the documents are related. To encode this relat-\nedness signal into our representations, we design\na loss function that trains the Transformer model\nto learn closer representations for papers when one\ncites the other, and more distant representations\notherwise. The high-level overview of the model is\nshown in Figure 1.\n\nIn particular, each training instance is a triplet of\npapers: a query paper P®, a positive paper P* and\na negative paper P~. The positive paper is a paper\nthat the query paper cites, and the negative paper\nis a paper that is not cited by the query paper (but\nthat may be cited by P*). We then train the model\nusing the following triplet margin loss function:\n\n£= max { (4 (P°,Pt) -a (P2,P~) +m), of (2)\n\nwhere d is a distance function and m is the loss\nmargin hyperparameter (we empirically choose\nm = 1). Here, we use the L2 norm distance:\n\nd(P4,P) = Iva — valle.\n\nwhere v4 is the vector corresponding to the pooled\noutput of the Transformer run on paper A (Equation\n1).6 Starting from the trained SciBERT model, we\npretrain the Transformer parameters on the citation\nobjective to learn paper representations that capture\ndocument relatedness.\n\n2.4 Selecting Negative Distractors\n\nThe choice of negative example papers P~ is im-\nportant when training the model. We consider two\nsets of negative examples: the first set simply con-\nsists of randomly selected papers from the corpus.\n\nWe also experimented with other distance functions (e..g,\nnormalized cosine), but they underperformed the L2 loss.\n\nGiven a query paper, intuitively we would expect\nthe model to be able to distinguish between cited\npapers, and uncited papers sampled randomly from\nthe entire corpus. This inductive bias has been\nalso found to be effective in content-based citation\nrecommendation applications (Bhagavatula et al.,\n2018). But, random negatives may be easy for the\nmodel to distinguish from the positives. To provide\na more nuanced training signal, we augment the\nrandomly drawn negatives with a more challenging\nsecond set of negative examples. We denote as\n“hard negatives” the papers that are not cited by the\nquery paper, but are cited by a paper cited by the\nquery paper, i.e. if P! £18, Pp? and Pp? HS ps\nbut P! RN P>, then P? is a candidate hard nega-\ntive example for P!. We expect the hard negatives\nto be somewhat related to the query paper, but typi-\ncally less related than the cited papers. As we show\nin our experiments (86), including hard negatives\nresults in more accurate embeddings compared to\nusing random negatives alone.\n\n2.5 Inference\n\nAt inference time, the model receives one paper, P,\nand it outputs the SPECTER’s Transfomer pooled\noutput activation as the paper representation for P\n(Equation 1). We note that for inference, SPECTER\nrequires only the title and abstract of the given\ninput paper; the model does not need any citation\ninformation about the input paper. This means that\nSPECTER can produce embeddings even for new\npapers that have yet to be cited, which is critical\nfor applications that target recent scientific papers.\n\n3 SciDocs Evaluation Framework\n\nPrevious evaluations of scientific document repre-\nsentations in the literature tend to focus on small\ndatasets over a limited set of tasks, and extremely\nhigh (99%+) AUC scores are already possible on\nthese data for English documents (Chen et al., 2019;\nWang et al., 2019). New, larger and more diverse\nbenchmark datasets are necessary. Here, we intro-\nduce a new comprehensive evaluation framework\nto measure the effectiveness of scientific paper em-\nbeddings, which we call SctDocs. The framework\nconsists of diverse tasks, ranging from citation pre-\ndiction, to prediction of user activity, to document\nclassification and paper recommendation. Note that\nSPECTER will not be further fine-tuned on any of\nthe tasks; we simply plug in the embeddings as fea-\ntures for each task. Below, we describe each of the\n\n2272\n", "vlm_text": "\n2.3 Citation-Based Pretraining Objective \nA citation from one document to another suggests that the documents are related. To encode this relat- edness signal into our representations, we design a loss function that trains the Transformer model to learn closer representations for papers when one cites the other, and more distant representations otherwise. The high-level overview of the model is shown in Figure  1 . \nIn particular, each training instance is a triplet of papers: a query p r  $\\mathcal{P}^{Q}$  , a positive paper  $\\mathcal{P}^{+}$    and a negative paper  P  $\\mathcal{P}^{-}$  . The positive paper is a paper that the query paper cites, and the negative paper is a paper that is not cited by the query paper (but that may be cited by    $\\mathcal{P}^{+}$  ). We then train the model using the following triplet margin loss function: \n\n$$\n\\mathcal{L}=\\operatorname*{max}\\left\\{\\bigg(\\mathrm{d}\\left(\\mathcal{P}^{Q},\\mathcal{P}^{+}\\right)-\\mathrm{d}\\left(\\mathcal{P}^{Q},\\mathcal{P}^{-}\\right)+m\\bigg),0\\right\\}\n$$\n \nwhere    $d$   is a distance function and    $m$   is the loss margin hyperparameter (we empirically choose  $m=1$  ). Here, we use the L2 norm distance: \n\n$$\n\\mathrm{d}(\\mathcal{P}^{A},\\mathcal{P}^{B})=||\\mathbf{v}_{A}-\\mathbf{v}_{B}||_{2},\n$$\n \nwhere    $\\mathbf{v}_{A}$   is the vector corresponding to the pooled output of the Transformer run on paper    $A$   (Equation 1 ).   Starting from the trained SciBERT model, we pretrain the Transformer parameters on the citation objective to learn paper representations that capture document relatedness. \n2.4 Selecting Negative Distractors \nThe choice of negative example papers  $\\mathcal{P}^{-}$  is im- portant when training the model. We consider two sets of negative examples: the ﬁrst set simply con- sists of randomly selected papers from the corpus. \nGiven a query paper, intuitively we would expect the model to be able to distinguish between cited papers, and uncited papers sampled randomly from the entire corpus. This inductive bias has been also found to be effective in content-based citation recommendation applications ( Bhagavatula et al. , 2018 ). But, random negatives may be easy for the model to distinguish from the positives. To provide a more nuanced training signal, we augment the randomly drawn negatives with a more challenging second set of negative examples. We denote as “hard negatives” the papers that are not cited by the query paper, but  are  cited by a paper cited by the query paper, i.e. if    $\\mathcal{P}^{1}\\xrightarrow{c i t e}\\mathcal{P}^{2}$  − − →P   and    $\\mathcal{P}^{2}\\xrightarrow{c i t e}\\mathcal{P}^{3}$  − − →P but  $\\mathcal{P}^{1}\\xrightarrow{c i t e}\\mathcal{P}^{3}$  − − →P , n    $\\mathcal{P}^{3}$    is a candidate hard nega- tive example for  P  $\\mathcal{P}^{1}$  . We expect the hard negatives to be somewhat related to the query paper, but typi- cally less related than the cited papers. As we show in our experiments ( § 6 ), including hard negatives results in more accurate embeddings compared to using random negatives alone. \n2.5 Inference \nAt inference time, the model receives one paper,  $\\mathcal{P}$  , and it outputs the S PECTER ’s Transfomer pooled output activation as the paper representation for    $\\mathcal{P}$  (Equation  1 ). We note that for inference, S PECTER requires only the title and abstract of the given input paper; the model does not need any citation information about the input paper. This means that S PECTER  can produce embeddings even for new papers that have yet to be cited, which is critical for applications that target recent scientiﬁc papers. \n3 S CI D OCS  Evaluation Framework \nPrevious evaluations of scientiﬁc document repre- sentations in the literature tend to focus on small datasets over a limited set of tasks, and extremely high   $(99\\%+)$   AUC scores are already possible on these data for English documents ( Chen et al. ,  2019 ; Wang et al. ,  2019 ). New, larger and more diverse benchmark datasets are necessary. Here, we intro- duce a new comprehensive evaluation framework to measure the effectiveness of scientiﬁc paper em- beddings, which we call S CI D OCS . The framework consists of diverse tasks, ranging from citation pre- diction, to prediction of user activity, to document classiﬁcation and paper recommendation. Note that S PECTER  will not be further ﬁne-tuned on any of the tasks; we simply plug in the embeddings as fea- tures for each task. Below, we describe each of the tasks in detail and the evaluation data associated with it. In addition to our training data, we release all the datasets associated with the evaluation tasks. "}
{"page": 3, "image_path": "doc_images/2020.acl-main.207_3.jpg", "ocr_text": "tasks in detail and the evaluation data associated\nwith it. In addition to our training data, we release\nall the datasets associated with the evaluation tasks.\n\n3.1 Document Classification\n\nAn important test of a document-level embedding is\nwhether it is predictive of the class of the document.\nHere, we consider two classification tasks in the\nscientific domain:\n\nMeSH Classification —_ In this task, the goals is to\nclassify scientific papers according to their Medi-\ncal Subject Headings (MeSH) (Lipscomb, 2000).7\nWe construct a dataset consisting of 23K academic\nmedical papers, where each paper is assigned one\nof 11 top-level disease classes such as cardiovas-\ncular diseases, diabetes, digestive diseases derived\nfrom the MeSH vocabulary. The most populated\ncategory is Neoplasms (cancer) with 5.4K instances\n(23.3% of the total dataset) while the category with\nleast number of samples is Hepatitis (1.7% of the\ntotal dataset). We follow the approach of Feldman\net al. (2019) in mapping the MeSH vocabulary to\nthe disease classes.\n\nPaper Topic Classification This task is predict-\ning the topic associated with a paper using the pre-\ndefined topic categories of the Microsoft Academic\nGraph (MAG) (Sinha et al., 2015)8. MAG pro-\nvides a database of papers, each tagged with a list\nof topics. The topics are organized in a hierarchy\nof 5 levels, where level | is the most general and\nlevel 5 is the most specific. For our evaluation,\nwe derive a document classification dataset from\nthe level 1 topics, where a paper is labeled by its\ncorresponding level 1 MAG topic. We construct a\ndataset of 25K papers, almost evenly split over the\n19 different classes of level 1 categories in MAG.\n\n3.2. Citation Prediction\n\nAs argued above, citations are a key signal of re-\nlatedness between papers. We test how well differ-\nent paper representations can reproduce this signal\nthrough citation prediction tasks. In particular, we\nfocus on two sub-tasks: predicting direct citations,\nand predicting co-citations. We frame these as\nranking tasks and evaluate performance using MAP\nand nDCG, standard ranking metrics.\n\nThttps://www.nlm.nih.gov/mesh/meshhome.\nhtml\nShttps://academic.microsoft.com/\n\nDirect Citations In this task, the model is asked\n0 predict which papers are cited by a given query\npaper from a given set of candidate papers. The\nevaluation dataset includes approximately 30K to-\nal papers from a held-out pool of papers, con-\nsisting of 1K query papers and a candidate set of\nup to 5 cited papers and 25 (randomly selected)\nuncited papers. The task is to rank the cited papers\nhigher than the uncited papers. For each embed-\nding method, we require only comparing the L2\ndistance between the raw embeddings of the query\nand the candidates, without any additional trainable\nparameters.\n\nCo-Citations This task is similar to the direct\ncitations but instead of predicting a cited paper,\nhe goal is to predict a highly co-cited paper with\na given paper. Intuitively, if papers A and B are\ncited frequently together by several papers, this\nshows that the papers are likely highly related and\na good paper representation model should be able\nto identify these papers from a given candidate\nset. The dataset consists of 30K total papers and is\nconstructed similar to the direct citations task.\n\n3.3. User Activity\n\nThe embeddings for similar papers should be close\nto each other; we use user activity as a proxy for\nidentifying similar papers and test the model’s abil-\nity to recover this information. Multiple users con-\nsuming the same items as one another is a classic\nrelatedness signal and forms the foundation for rec-\nommender systems and other applications (Schafer\net al., 2007). In our case, we would expect that\nwhen users look for academic papers, the papers\nthey view in a single browsing session tend to be\nrelated. Thus, accurate paper embeddings should,\nall else being equal, be relatively more similar for\npapers that are frequently viewed in the same ses-\nsion than for other papers. To build benchmark\ndatasets to test embeddings on user activity, we\nobtained logs of user sessions from a major aca-\ndemic search engine. We define the following two\ntasks on which we build benchmark datasets to test\nembeddings:\n\nCo-Views Our co-views dataset consists of ap-\nproximately 30K papers. To construct it, we take\n1K random papers that are not in our train or de-\nvelopment set and associate with each one up to 5\nfrequently co-viewed papers and 25 randomly se-\nlected papers (similar to the approach for citations).\nThen, we require the embedding model to rank the\n\n2273\n", "vlm_text": "\n3.1 Document Classiﬁcation \nAn important test of a document-level embedding is whether it is predictive of the class of the document. Here, we consider two classiﬁcation tasks in the scientiﬁc domain: \nMeSH Classiﬁcation In this task, the goals is to classify scientiﬁc papers according to their Medi- cal Subject Headings (MeSH) ( Lipscomb ,  2000 ). We construct a dataset consisting of 23K academic medical papers, where each paper is assigned one of 11 top-level disease classes such as cardiovas- cular diseases, diabetes, digestive diseases derived from the MeSH vocabulary. The most populated category is Neoplasms (cancer) with 5.4K instances  ${\\it23.3\\%}$   of the total dataset) while the category with least number of samples is Hepatitis (  $1.7\\%$   of the total dataset). We follow the approach of  Feldman et al.  ( 2019 ) in mapping the MeSH vocabulary to the disease classes. \nPaper Topic Classiﬁcation This task is predict- ing the topic associated with a paper using the pre- deﬁned topic categories of the Microsoft Academic Graph (MAG) ( Sinha et al. ,  2015 ) 8 . MAG pro- vides a database of papers, each tagged with a list of topics. The topics are organized in a hierarchy of 5 levels, where level 1 is the most general and level 5 is the most speciﬁc. For our evaluation, we derive a document classiﬁcation dataset from the level 1 topics, where a paper is labeled by its corresponding level 1 MAG topic. We construct a dataset of 25K papers, almost evenly split over the 19 different classes of level 1 categories in MAG. \n3.2 Citation Prediction \nAs argued above, citations are a key signal of re- latedness between papers. We test how well differ- ent paper representations can reproduce this signal through citation prediction tasks. In particular, we focus on two sub-tasks: predicting  direct citations , and predicting  co-citations . We frame these as ranking tasks and evaluate performance using  MAP and n DCG , standard ranking metrics. \nDirect Citations In this task, the model is asked to predict which papers are cited by a given query paper from a given set of candidate papers. The evaluation dataset includes approximately 30K to- tal papers from a held-out pool of papers, con- sisting of 1K query papers and a candidate set of up to 5 cited papers and 25 (randomly selected) uncited papers. The task is to rank the cited papers higher than the uncited papers. For each embed- ding method, we require only comparing the L2 distance between the raw embeddings of the query and the candidates, without any additional trainable parameters. \nCo-Citations This task is similar to the  direct citations  but instead of predicting a cited paper, the goal is to predict a highly co-cited paper with a given paper. Intuitively, if papers A and B are cited frequently together by several papers, this shows that the papers are likely highly related and a good paper representation model should be able to identify these papers from a given candidate set. The dataset consists of 30K total papers and is constructed similar to the  direct citations  task. \n3.3 User Activity \nThe embeddings for similar papers should be close to each other; we use user activity as a proxy for identifying similar papers and test the model’s abil- ity to recover this information. Multiple users con- suming the same items as one another is a classic relatedness signal and forms the foundation for rec- ommender systems and other applications ( Schafer et al. ,  2007 ). In our case, we would expect that when users look for academic papers, the papers they view in a single browsing session tend to be related. Thus, accurate paper embeddings should, all else being equal, be relatively more similar for papers that are frequently viewed in the same ses- sion than for other papers. To build benchmark datasets to test embeddings on user activity, we obtained logs of user sessions from a major aca- demic search engine. We deﬁne the following two tasks on which we build benchmark datasets to test embeddings: \nCo-Views Our co-views dataset consists of ap- proximately 30K papers. To construct it, we take 1K random papers that are not in our train or de- velopment set and associate with each one up to 5 frequently co-viewed papers and 25 randomly se- lected papers (similar to the approach for citations). Then, we require the embedding model to rank the co-viewed papers higher than the random papers by comparing the L2 distances of raw embeddings. We evaluate performance using standard ranking metrics, n DCG  and  MAP . "}
{"page": 4, "image_path": "doc_images/2020.acl-main.207_4.jpg", "ocr_text": "co-viewed papers higher than the random papers\nby comparing the L2 distances of raw embeddings.\nWe evaluate performance using standard ranking\nmetrics, nDCG and MAP.\n\nCo-Reads If the user clicks to access the PDF\nof a paper from the paper description page, this\nis a potentially stronger sign of interest in the pa-\nper. In such a case we assume the user will read at\nleast parts of the paper and refer to this as a “read”\naction. Accordingly, we define a “co-reads” task\nand dataset analogous to the co-views dataset de-\nscribed above. This dataset is also approximately\n30K papers.\n\n3.4 Recommendation\n\nIn the recommendation task, we evaluate the abil-\nity of paper embeddings to boost performance in\na production recommendation system. Our rec-\nommendation task aims to help users navigate the\nscientific literature by ranking a set of “‘similar pa-\npers” for a given paper. We use a dataset of user\nclickthrough data for this task which consists of\n22K clickthrough events from a public scholarly\nsearch engine. We partitioned the examples tem-\nporally into train (20K examples), validation (1K),\nand test (1K) sets. As is typical in clickthrough data\non ranked lists, the clicks are biased toward the top\nof original ranking presented to the user. To coun-\nteract this effect, we computed propensity scores\nusing a swap experiment (Agarwal et al., 2019).\nThe propensity scores give, for each position in the\nranked list, the relative frequency that the position\nis over-represented in the data due to exposure bias.\nWe can then compute de-biased evaluation metrics\nby dividing the score for each test example by the\npropensity score for the clicked position. We report\npropensity-adjusted versions of the standard rank-\ning metrics Precision@ | (P@1) and Normalized\nDiscounted Cumulative Gain (nDCG).\n\nWe test different embeddings on the recommen-\ndation task by including cosine embedding dis-\ntance? as a feature within an existing recommenda-\ntion system that includes several other informative\nfeatures (title/author similarity, reference and ci-\ntation overlap, etc.). Thus, the recommendation\nexperiments measure whether the embeddings can\nboost the performance of a strong baseline system\non an end task. For SPECTER, we also perform an\nonline A/B test to measure whether its advantages\n\n°Embeddings are L2 normalized and in this case cosine\ndistance is equivalent to L2 distance.\n\non the offline dataset translate into improvements\non the online recommendation task (85).\n\n4 Experiments\n\nTraining Data To train our model, we use a\nsubset of the Semantic Scholar corpus (Ammar\net al., 2018) consisting of about 146K query papers\n(around 26.7M tokens) with their corresponding\noutgoing citations, and we use an additional 32K\npapers for validation. For each query paper we con-\nstruct up to 5 training triples comprised of a query,\na positive, and a negative paper. The positive pa-\npers are sampled from the direct citations of the\nquery, while negative papers are chosen either ran-\ndomly or from citations of citations (as discussed in\n§2.4). We empirically found it helpful to use 2 hard\nnegatives (citations of citations) and 3 easy neg-\natives (randomly selected papers) for each query\npaper. This process results in about 684K training\ntriples and 145K validation triples.\n\nTraining and Implementation We implement\nour model in AllenNLP (Gardner et al., 2018).\nWe initialize the model from SciBERT pretrained\nweights (Beltagy et al., 2019) since it is the state-\nof-the-art pretrained language model on scientific\ntext. We continue training all model parameters on\nour training objective (Equation 2). We perform\nminimal tuning of our model’s hyperparameters\nbased on the performance on the validation set,\nwhile baselines are extensively tuned. Based on\ninitial experiments, we use a margin m=1 for the\ntriplet loss. For training, we use the Adam opti-\nmizer (Kingma and Ba, 2014) following the sug-\ngested hyperparameters in Devlin et al. (2019) (LR:\n2e-5, Slanted Triangular LR scheduler!® (Howard\nand Ruder, 2018) with number of train steps equal\nto training instances and cut fraction of 0.1). We\ntrain the model on a single Titan V GPU (12G\nmemory) for 2 epochs, with batch size of 4 (the\nmaximum that fit in our GPU memory) and use\ngradient accumulation for an effective batch size of\n32. Each training epoch takes approximately 1-2\ndays to complete on the full dataset. We release\nour code and data to facilitate reproducibility. !!\n\nTask-Specific Model Details For the classifica-\ntion tasks, we used a linear SVM where embed-\nding vectors were the only features. The C’ hyper-\nparameter was tuned via a held-out validation set.\n\n10] earning rate linear warmup followed by linear decay.\n\"https://github.com/allenai/specter\n\n2274\n", "vlm_text": "\nCo-Reads If the user clicks to access the PDF of a paper from the paper description page, this is a potentially stronger sign of interest in the pa- per. In such a case we assume the user will read at least parts of the paper and refer to this as a “read” action. Accordingly, we deﬁne a “co-reads” task and dataset analogous to the co-views dataset de- scribed above. This dataset is also approximately 30K papers. \n3.4 Recommendation \nIn the recommendation task, we evaluate the abil- ity of paper embeddings to boost performance in a production recommendation system. Our rec- ommendation task aims to help users navigate the scientiﬁc literature by ranking a set of “similar pa- pers” for a given paper. We use a dataset of user clickthrough data for this task which consists of 22K clickthrough events from a public scholarly search engine. We partitioned the examples tem- porally into train (20K examples), validation (1K), and test (1K) sets. As is typical in clickthrough data on ranked lists, the clicks are biased toward the top of original ranking presented to the user. To coun- teract this effect, we computed propensity scores using a swap experiment ( Agarwal et al. ,  2019 ). The propensity scores give, for each position in the ranked list, the relative frequency that the position is over-represented in the data due to exposure bias. We can then compute de-biased evaluation metrics by dividing the score for each test example by the propensity score for the clicked position. We report propensity-adjusted versions of the standard rank- ing metrics Precision  $@\\,1$     $(\\operatorname{P@1})$  ) and Normalized Discounted Cumulative Gain ( n DCG ). \nWe test different embeddings on the recommen- dation task by including cosine embedding dis- tance 9   as a feature within an existing recommenda- tion system that includes several other informative features (title/author similarity, reference and ci- tation overlap, etc.). Thus, the recommendation experiments measure whether the embeddings can boost the performance of a strong baseline system on an end task. For S PECTER , we also perform an online A/B test to measure whether its advantages on the ofﬂine dataset translate into improvements on the online recommendation task ( 5 ). \n\n4 Experiments \nTraining Data To train our model, we use a subset of the Semantic Scholar corpus ( Ammar et al. ,  2018 ) consisting of about 146K query papers (around 26.7M tokens) with their corresponding outgoing citations, and we use an additional 32K papers for validation. For each query paper we con- struct up to 5 training triples comprised of a query, a positive, and a negative paper. The positive pa- pers are sampled from the direct citations of the query, while negative papers are chosen either ran- domly or from citations of citations (as discussed in § 2.4 ). We empirically found it helpful to use 2 hard negatives (citations of citations) and 3 easy neg- atives (randomly selected papers) for each query paper. This process results in about 684K training triples and 145K validation triples. \nTraining and Implementation We implement our model in AllenNLP ( Gardner et al. ,  2018 ). We initialize the model from SciBERT pretrained weights ( Beltagy et al. ,  2019 ) since it is the state- of-the-art pretrained language model on scientiﬁc text. We continue training all model parameters on our training objective (Equation  2 ). We perform minimal tuning of our model’s hyperparameters based on the performance on the validation set, while baselines are extensively tuned. Based on initial experiments, we use a margin    $m{=}1$   for the triplet loss. For training, we use the Adam opti- mizer ( Kingma and Ba ,  2014 ) following the sug- gested hyperparameters in  Devlin et al.  ( 2019 ) (LR: 2e-5, Slanted Triangular LR scheduler 10   ( Howard and Ruder ,  2018 ) with number of train steps equal to training instances and cut fraction of 0.1). We train the model on a single Titan V GPU (12G memory) for 2 epochs, with batch size of 4 (the maximum that ﬁt in our GPU memory) and use gradient accumulation for an effective batch size of 32. Each training epoch takes approximately 1-2 days to complete on the full dataset. We release our code and data to facilitate reproducibility.   11 \nTask-Speciﬁc Model Details For the classiﬁca- tion tasks, we used a linear SVM where embed- ding vectors were the only features. The    $C$   hyper- parameter was tuned via a held-out validation set. "}
{"page": 5, "image_path": "doc_images/2020.acl-main.207_5.jpg", "ocr_text": "For the recommendation tasks, we use a feed-\nforward ranking neural network that takes as input\nten features designed to capture the similarity be-\ntween each query and candidate paper, including\nthe cosine similarity between the query and candi-\ndate embeddings and manually-designed features\ncomputed from the papers’ citations, titles, authors,\nand publication dates.\n\nBaseline Methods Our work falls into the inter-\nsection of textual representation, citation mining,\nand graph learning, and we evaluate against state-\nof-the-art baselines from each of these areas. We\ncompare with several strong textual models: SIF\n(Arora et al., 2017), a method for learning docu-\nment representations by removing the first prin-\ncipal component of aggregated word-level embed-\ndings which we pretrain on scientific text; SciBERT\n(Beltagy et al., 2019) a state-of-the-art pretrained\nTransformer LM for scientific text; and Sent-BERT\n(Reimers and Gurevych, 2019), a model that uses\nnegative sampling to tune BERT for producing op-\ntimal sentence embeddings. We also compare with\nCiteomatic (Bhagavatula et al., 2018), a closely\nrelated paper representation model for citation pre-\ndiction which trains content-based representations\nwith citation graph information via dynamically\nsampled triplets, and SGC (Wu et al., 2019a), a\nstate-of-the-art graph-convolutional approach. For\ncompleteness, additional baselines are also in-\ncluded; due to space constraints we refer to Ap-\npendix A for detailed discussion of all baselines.\nWe tune hyperparameters of baselines to maximize\nperformance on a separate validation set.\n\n5 Results\n\nTable | presents the main results corresponding\nto our evaluation tasks (described in §3). Overall,\nwe observe substantial improvements across all\ntasks with average performance of 80.0 across all\nmetrics on all tasks which is a 3.1 point absolute\nimprovement over the next-best baseline. We now\ndiscuss the results in detail.\n\nFor document classification, we report macro\nFI, a standard classification metric. We observe\nthat the classifier performance when trained on our\nrepresentations is better than when trained on any\nother baseline. Particularly, on the MeSH (MAG)\ndataset, we obtain an 86.4 (82.0) Fl score which is\nabout a A= + 2.3 (+1.5) point absolute increase\nover the best baseline on each dataset respectively.\nOur evaluation of the learned representations on\n\npredicting user activity is shown in the “User activ-\nity” columns of Table 1. SPECTER achieves a MAP\nscore of 83.8 on the co-view task, and 84.5 on co-\nread, improving over the best baseline (Citeomatic\nin this case) by 2.7 and 4.0 points, respectively.\nWe observe similar trends for the “citation” and\n“co-citation” tasks, with our model outperforming\nvirtually all other baselines except for SGC, which\nhas access to the citation graph at training and test\ntime.'? Note that methods like SGC cannot be\nused in real-world setting to embed new papers\nthat are not cited yet. On the other hand, on co-\ncitation data our method is able to achieve the best\nresults with nDCG of 94.8, improving over SGC\nwith 2.3 points. Citeomatic also performs well on\nthe citation tasks, as expected given that its primary\ndesign goal was citation prediction. Nevertheless,\nour method slightly outperforms Citeomatic on the\ndirect citation task, while substantially outperform-\ning it on co-citations (+2.0 nDCG).\n\nFinally, for recommendation task, we observe\nthat SPECTER outperforms all other models on this\ntask as well, with nDcG of 53.9. On the recom-\nmendations task, as opposed to previous experi-\nments, the differences in method scores are gen-\nerally smaller. This is because for this task the\nembeddings are used along with several other in-\nformative features in the ranking model (described\nunder task-specific models in §4), meaning that em-\nbedding variants have less opportunity for impact\non overall performance.\n\nWe also performed an online study to evaluate\nwhether SPECTER embeddings offer similar advan-\ntages in a live application. We performed an online\nA/B test comparing our SPECTER-based recom-\nmender to an existing production recommender sys-\ntem for similar papers that ranks papers by a textual\nsimilarity measure. In a dataset of 4,113 clicks, we\nfound that SPECTER ranker improved clickthrough\nrate over the baseline by 46.5%, demonstrating its\nsuperiority.\n\nWe emphasize that our citation-based pretrain-\ning objective is critical for the performance of\nSPECTER; removing this and using a vanilla SciB-\nERT results in decreased performance on all tasks.\n\n\"For SGC, we remove development and test set citations\nand co-citations during training. We also remove incoming\ncitations from development and test set queries as these would\nnot be available at test time in production.\n\n2275\n", "vlm_text": "For the recommendation tasks, we use a feed- forward ranking neural network that takes as input ten features designed to capture the similarity be- tween each query and candidate paper, including the cosine similarity between the query and candi- date embeddings and manually-designed features computed from the papers’ citations, titles, authors, and publication dates. \nBaseline Methods Our work falls into the inter- section of textual representation, citation mining, and graph learning, and we evaluate against state- of-the-art baselines from each of these areas. We compare with several strong textual models: SIF ( Arora et al. ,  2017 ), a method for learning docu- ment representations by removing the ﬁrst prin- cipal component of aggregated word-level embed- dings which we pretrain on scientiﬁc text; SciBERT ( Beltagy et al. ,  2019 ) a state-of-the-art pretrained Transformer LM for scientiﬁc text; and Sent-BERT ( Reimers and Gurevych ,  2019 ), a model that uses negative sampling to tune BERT for producing op- timal sentence embeddings. We also compare with Citeomatic ( Bhagavatula et al. ,  2018 ), a closely related paper representation model for citation pre- diction which trains content-based representations with citation graph information via dynamically sampled triplets, and SGC ( Wu et al. ,  2019a ), a state-of-the-art graph-convolutional approach. For completeness, additional baselines are also in- cluded; due to space constraints we refer to Ap- pendix  A  for detailed discussion of all baselines. We tune hyperparameters of baselines to maximize performance on a separate validation set. \n5 Results \nTable  1  presents the main results corresponding to our evaluation tasks (described in  § 3 ). Overall, we observe substantial improvements across all tasks with average performance of 80.0 across all metrics on all tasks which is a 3.1 point absolute improvement over the next-best baseline. We now discuss the results in detail. \nFor document classiﬁcation, we report macro F1, a standard classiﬁcation metric. We observe that the classiﬁer performance when trained on our representations is better than when trained on any other baseline. Particularly, on the MeSH (MAG) dataset, we obtain an 86.4 (82.0) F1 score which is about a    $\\Delta{=}+2.3$     $(+1.5)$   point absolute increase over the best baseline on each dataset respectively. Our evaluation of the learned representations on predicting user activity is shown in the “User activ- ity” columns of Table  1 . S PECTER  achieves a  MAP score of 83.8 on the co-view task, and 84.5 on co- read, improving over the best baseline (Citeomatic in this case) by 2.7 and 4.0 points, respectively. We observe similar trends for the “citation” and “co-citation” tasks, with our model outperforming virtually all other baselines except for SGC, which has access to the citation graph at training and test time.   Note that methods like SGC cannot be used in real-world setting to embed new papers that are not cited yet. On the other hand, on co- citation data our method is able to achieve the best results with n DCG  of 94.8, improving over SGC with 2.3 points. Citeomatic also performs well on the citation tasks, as expected given that its primary design goal was citation prediction. Nevertheless, our method slightly outperforms Citeomatic on the direct citation task, while substantially outperform- ing it on co-citations   $(+2.0\\;\\mathrm{nDCG})$  . \n\nFinally, for recommendation task, we observe that S PECTER  outperforms all other models on this task as well, with n DCG  of 53.9. On the recom- mendations task, as opposed to previous experi- ments, the differences in method scores are gen- erally smaller. This is because for this task the embeddings are used along with several other in- formative features in the ranking model (described under task-speciﬁc models in  § 4 ), meaning that em- bedding variants have less opportunity for impact on overall performance. \nWe also performed an online study to evaluate whether S PECTER  embeddings offer similar advan- tages in a live application. We performed an online A/B test comparing our S PECTER -based recom- mender to an existing production recommender sys- tem for similar papers that ranks papers by a textual similarity measure. In a dataset of 4,113 clicks, we found that S PECTER  ranker improved clickthrough rate over the baseline by   $46.5\\%$  , demonstrating its superiority. \nWe emphasize that our citation-based pretrain- ing objective is critical for the performance of S PECTER ; removing this and using a vanilla SciB- ERT results in decreased performance on all tasks. "}
{"page": 6, "image_path": "doc_images/2020.acl-main.207_6.jpg", "ocr_text": "Task > Classification User activity prediction Citation prediction\nRecomm.\nSubtask MAG MeSH  Co-View Co-Read Cite Co-Cite Avg.\nModel | / Metric > Fl Fl MAP nDCG MAP nDCG MAP nDCG MAP nDCG nDCG PQ@l\nRandom 48 94 25.2 516 256 519 25.1 S515 249 51.4 51.3 16.8 32.5\nDoc2vec (2014) 66.2 69.2 67.8 82.9 64.9 81.6 65.3 82.2 67. 83.4 51.7 16.9 66.6\nFasttext-sum (2017) 78.1 84.1 76.5 87.9 75.3 87.4 74.6 88.1 77.8 89.6 52.5 18.0 74.1\nSIF (2017) 78.4 81.4 79.4 89.4 78.2 889 794 90.5 80.8 90.9 53.4 19.5 75.9\nELMo (2018) 77.0 75.7 70.3 84.3 67.4 82.6 65.8 82.6 68.5 83.8 52.5 18.2 69.0\nCiteomatic (2018) 67.1 75.7 81.1 90.2 80.5 90.2 86.3 94.1 84.4 92.8 52.5 17.3 76.0\nSGC (2019a) 76.8 82.7. 77.2 88.0 75.7 87.5 91.6 96.2 84. 92.5 52.7. 18.2 76.9\nSciBERT (2019) 79.7 80.7. 50.7. 73.1 47.7 T11 48.3 71.7 49.7 72.6 52.1 17.9 59.6\nSent-BERT (2019) 80.5 69.1 68.2 83.3 64.8 81.3 63.5 81.6 66.4 82.8 51.6 17.1 67.5\nSPECTER (Ours) 82.0 86.4 83.6 91.5 845 92.4 883 94.9 88.1 94.8 53.9 20.0 80.0\nTable 1: Results on the SCIDOCS evaluation suite consisting of 7 tasks.\n\n6 Analysis CLS USR CITE REC Avg.\nIn this section, we analyze several design deci- SPECTER 84.2 884 91S 36.9 80.0\n\n: : I : sue : — abstract 82.2 72.2 73.6 345 68.1\nsions in SPECTER, provide a visualization of its\n\n: : + venue 84.5 88.0 91.2 36.7 79.9\n\nembedding space, and experimentally compare + author 927 723 110 346 673\nSPECTER’S use of fixed embeddings against a fine- Ng hard negatives 82.4 85.8 89.8 36.8 78.4\ntuning approach. Start w/BERT-Large 81.7 85.9 87.8 36.1 77.5\n\nAblation Study We start by analyzing how\nadding or removing metadata fields from the in-\nput to SPECTER alters performance. The results\nare shown in the top four rows of Table 2 (for\nbrevity, here we only report the average of the met-\nrics from each task). We observe that removing\nthe abstract from the textual input and relying only\non the title results in a substantial decrease in per-\nformance. More surprisingly, adding authors as an\ninput (along with title and abstract) hurts perfor-\nmance.!? One possible explanation is that author\nnames are sparse in the corpus, making it difficult\nfor the model to infer document-level relatedness\nfrom them. As another possible reason of this be-\nhavior, tokenization using Wordpieces might be\nsuboptimal for author names. Many author names\nare out-of-vocabulary for SciBERT and thus, they\nmight be split into sub-words and shared across\nnames that are not semantically related, leading\nto noisy correlation. Finally, we find that adding\nvenues slightly decreases performance,!* except\non document classification (which makes sense, as\nwe would expect venues to have high correlation\n\n'3We experimented with both concatenating authors with\nthe title and abstract and also considering them as an additional\nfield. Neither were helpful.\n\n'4Venue information in our data came directly from pub-\nlisher provided metadata and thus was not normalized. Venue\nnormalization could help improve results.\n\nTable 2: Ablations: Numbers are averages of metrics\nfor each evaluation task: CLS: classification, USR:\nUser activity, CITE: Citation prediction, REC: Recom-\nmendation, Avg. average over all tasks & metrics.\n\nwith paper topics). The fact that SPECTER does not\nrequire inputs like authors or venues makes it appli-\ncable in situations where this metadata is not avail-\nable, such as matching reviewers with anonymized\nsubmissions, or performing recommendations of\nanonymized preprints (e.g., on OpenReview).\n\nOne design decision in SPECTER is to use a set of\nhard negative distractors in the citation-based fine-\ntuning objective. The fifth row of Table 2 shows\nthat this is important—using only easy negatives re-\nduces performance on all tasks. While there could\nbe other potential ways to include hard negatives in\nthe model, our simple approach of including cita-\ntions of citations is effective. The sixth row of the\ntable shows that using a strong general-domain lan-\nguage model (BERT-Large) instead of SciBERT in\nSPECTER reduces performance considerably. This\nis reasonable because unlike BERT-Large, SciB-\nERT is pretrained on scientific text.\n\nVisualization Figure 2 shows t-SNE (van der\nMaaten, 2014) projections of our embeddings\n(SPECTER) compared with the SciBERT baseline\n\n2276\n", "vlm_text": "The table highlights the performance of various models on different tasks related to document understanding or recommendation. It is organized into several sections:\n\n1. **Tasks:**\n   - Classification\n   - User Activity Prediction\n   - Citation Prediction\n   - Recommendation\n\n2. **Subtasks:**\n   - For Classification: MAG, MeSH\n   - For User Activity Prediction: Co-View, Co-Read\n   - For Citation Prediction: Cite, Co-Cite\n   - For Recommendation: No distinct subtasks listed\n\n3. **Metrics:**\n   - For Classification: F1 score\n   - For User Activity Prediction: MAP (Mean Average Precision), nDCG (Normalized Discounted Cumulative Gain)\n   - For Citation Prediction: MAP, nDCG\n   - For Recommendation: nDCG, P@1 (Precision at 1)\n   - Avg.: Indicates the average performance across tasks or metrics\n\n4. **Models:**\n   - Random\n   - Doc2vec (Mikolov et al., 2014)\n   - Fasttext-sum (Bojanowski et al., 2017)\n   - SIF (Arora et al., 2017)\n   - ELMo (Peters et al., 2018)\n   - Citeomatic (Lo et al., 2018)\n   - SGC (Wu et al., 2019a)\n   - SciBERT (Beltagy et al., 2019)\n   - Sent-BERT (Reimers & Gurevych, 2019)\n   - SPECTER (Ours)\n\n5. **Performance Results:**\n   - The table displays the performance of each model using different metrics for each subtask. Scores are presented for each task and metric combination.\n   - SPECTER, the last model, shows the best or nearly the best performance across almost all tasks and metrics, especially excelling in classification tasks and citation prediction.\n\nOverall, the table compares the effectiveness of different textual models for academic-related tasks. SPECTER appears to be the most promising model among those listed, based on the metrics evaluated in this table.\n6 Analysis \nIn this section, we analyze several design deci- sions in S PECTER , provide a visualization of its embedding space, and experimentally compare S PECTER ’s use of ﬁxed embeddings against a ﬁne- tuning approach. \nAblation Study We start by analyzing how adding or removing metadata ﬁelds from the in- put to S PECTER  alters performance. The results are shown in the top four rows of Table  2  (for brevity, here we only report the average of the met- rics from each task). We observe that removing the abstract from the textual input and relying only on the title results in a substantial decrease in per- formance. More surprisingly, adding authors as an input (along with title and abstract) hurts perfor- mance.   One possible explanation is that author names are sparse in the corpus, making it difﬁcult for the model to infer document-level relatedness from them. As another possible reason of this be- havior, tokenization using Wordpieces might be suboptimal for author names. Many author names are out-of-vocabulary for SciBERT and thus, they might be split into sub-words and shared across names that are not semantically related, leading to noisy correlation. Finally, we ﬁnd that adding venues slightly decreases performance,   except on document classiﬁcation (which makes sense, as we would expect venues to have high correlation \nThis table presents performance metrics for different models or configurations. The columns represent different evaluation criteria: CLS, USR, CITE, REC, and an average (Avg.). Each row lists these metrics for different configurations related to the SPECTER model:\n\n- \"SPECTER\" shows the base model scores.\n- \"− abstract\" indicates scores when the abstract is removed.\n- \"+ venue\" indicates scores with venue information added, showing the highest score for CLS.\n- \"+ author\" indicates scores with author information added.\n- \"No hard negatives\" and \"Start w/ BERT-Large\" represent variations in the model training approach.\n\nThe best scores in each category are bolded.\nwith paper topics). The fact that S PECTER  does not require inputs like authors or venues makes it appli- cable in situations where this metadata is not avail- able, such as matching reviewers with anonymized submissions, or performing recommendations of anonymized preprints (e.g., on OpenReview). \nOne design decision in S PECTER  is to use a set of hard negative distractors in the citation-based ﬁne- tuning objective. The ﬁfth row of Table  2  shows that this is important—using only easy negatives re- duces performance on all tasks. While there could be other potential ways to include hard negatives in the model, our simple approach of including cita- tions of citations is effective. The sixth row of the table shows that using a strong  general-domain  lan- guage model (BERT-Large) instead of SciBERT in S PECTER  reduces performance considerably. This is reasonable because unlike BERT-Large, SciB- ERT is pretrained on scientiﬁc text. \nVisualization Figure  2  shows t-SNE ( van der Maaten ,  2014 ) projections of our embeddings (S PECTER ) compared with the SciBERT baseline "}
{"page": 7, "image_path": "doc_images/2020.acl-main.207_7.jpg", "ocr_text": "(a) SPECTER\n\n(b) SciBERT\n\nFigure 2: t-SNE visualization of paper embeddings and\ntheir corresponding MAG topics.\n\nfor a random set of papers. When comparing\nSPECTER embeddings with SciBERT, we observe\nthat our embeddings are better at encoding topi-\ncal information, as the clusters seem to be more\ncompact. Further, we see some examples of cross-\ntopic relatedness reflected in the embedding space\n(e.g., Engineering, Mathematics and Computer\nScience are close to each other, while Business\nand Economics are also close to each other). To\nquantify the comparison of visualized embeddings\nin Figure 2, we use the DBScan clustering algo-\nrithm (Ester et al., 1996) on this 2D projection.\nWe use the completeness and homogeneity cluster-\ning quality measures introduced by Rosenberg and\nHirschberg (2007). For the points corresponding to\nFigure 2, the homogeneity and completeness val-\nues for SPECTER are respectively 0.41 and 0.72\ncompared with SciBERT’s 0.19 and 0.63, a clear\nimprovement on separating topics using the pro-\njected embeddings.\n\nComparison with Task Specific Fine-Tuning\nWhile the fact that SPECTER does not require fine-\ntuning makes its paper embeddings less costly to\nuse, often the best performance from pretrained\nTransformers is obtained when the models are fine-\ntuned directly on each end task. We experiment\nwith fine-tuning SciBERT on our tasks, and find\nthis to be generally inferior to using our fixed rep-\nresentations from SPECTER. Specifically, we fine-\ntune SciBERT directly on task-specific signals in-\nstead of citations. To fine-tune on task-specific\ndata (e.g., user activity), we used a dataset of co-\nviews with 65K query papers, co-reads with 14K\nquery papers, and co-citations (instead of direct\ncitations) with 83K query papers. As the end tasks\nare ranking tasks, for all datasets we construct up\nto 5 triplets and fine-tune the model using triplet\nranking loss. The positive papers are sampled from\n\nCLS USR CITEREC All\n\n84.2 88.4 91.5 36.9 80.0\nSciBERT fine-tune on co-view 83.0 84.2 84.1 36.4 76.0\nSciBERT fine-tune on co-read 82.3 85.4 86.7 36.3 77.1\nSciBERT fine-tune on co-citation 82.9 84.3 85.2 36.6 76.4\nSciBERT fine-tune on multitask 83.3 86.1 88.2 36.0 78.0\n\nTraining signal\n\nSPECTER\n\nTable 3: Comparison with task-specific fine-tuning.\n\nthe most co-viewed (co-read, or co-cited) papers\ncorresponding to the query paper. We also include\nboth easy and hard distractors as when training\nSPECTER (for hard negatives we choose the least\nnon-zero co-viewed (co-read, or co-cited) papers).\nWe also consider training jointly on all task-specific\ntraining data sources in a multitask training process,\nwhere the model samples training triplets from a\ndistribution over the sources. As illustrated in Ta-\nble 3, without any additional final task-specific\nfine-tuning, SPECTER still outperforms a SciBERT\nmodel fine-tuned on the end tasks as well as their\nmultitask combination, further demonstrating the\neffectiveness and versatility of SPECTER embed-\ndings.!>\n\n7 Related Work\n\nRecent representation learning methods in NLP\nrely on training large neural language models on un-\nsupervised data (Peters et al., 2018; Radford et al.,\n2018; Devlin et al., 2019; Beltagy et al., 2019; Liu\net al., 2019). While successful at many sentence-\nand token-level tasks, our focus is on using the\nmodels for document-level representation learning,\nwhich has remained relatively under-explored.\nThere have been other efforts in document repre-\nsentation learning such as extensions of word vec-\ntors to documents (Le and Mikolov, 2014; Ganesh\net al., 2016; Liu et al., 2017; Wu et al., 2018; Gy-\nsel et al., 2017), convolution-based methods (Liu\net al., 2018; Zamani et al., 2018), and variational\nautoencoders (Holmer and Marfurt, 2018; Wang\net al., 2019). Relevant to document embedding, sen-\ntence embedding is a relatively well-studied area of\nresearch. Successful approaches include seq2seq\nmodels (Kiros et al., 2015), BiLSTM Siamese\nnetworks (Williams et al., 2018), leveraging su-\npervised data from other corpora (Conneau et al.,\n2017), and using discourse relations (Nie et al.,\n2019), and BERT-based methods (Reimers and\nGurevych, 2019). Unlike our proposed method,\n5We also experimented with further task-specific fine-\n\ntuning of our SPECTER on the end tasks but we did not observe\nadditional improvements.\n\n2277\n", "vlm_text": "The image shows two plots comparing the clustering of academic topics using two different machine learning models: SPECTER and SciBERT. Each plot visualizes a set of points, where each point represents a document or text, and the color of the points indicates the academic discipline or topic the document belongs to. The topics include Business, Chemistry, Sociology, Economics, Computer Science, Physics, Environmental Science, Mathematics, Engineering, and Medicine. \n\nThe left plot is labeled \"SPECTER,\" and the right plot is labeled \"SciBERT.\" Each model produces a different clustering pattern, indicating how the models group the documents based on their semantic similarities. The position and clustering of the colored points can give insight into each model's performance in distinguishing between different academic disciplines.\nFigure 2: t-SNE visualization of paper embeddings and their corresponding MAG topics. \nfor a random set of papers. When comparing S PECTER  embeddings with SciBERT, we observe that our embeddings are better at encoding topi- cal information, as the clusters seem to be more compact. Further, we see some examples of cross- topic relatedness reﬂected in the embedding space (e.g., Engineering, Mathematics and Computer Science are close to each other, while Business and Economics are also close to each other). To quantify the comparison of visualized embeddings in Figure  2 , we use the DBScan clustering algo- rithm ( Ester et al. ,  1996 ) on this 2D projection. We use the completeness and homogeneity cluster- ing quality measures introduced by  Rosenberg and Hirschberg  ( 2007 ). For the points corresponding to Figure  2 , the homogeneity and completeness val- ues for S PECTER  are respectively 0.41 and 0.72 compared with SciBERT’s 0.19 and 0.63, a clear improvement on separating topics using the pro- jected embeddings. \nComparison with Task Speciﬁc Fine-Tuning While the fact that S PECTER  does not require ﬁne- tuning makes its paper embeddings less costly to use, often the best performance from pretrained Transformers is obtained when the models are ﬁne- tuned directly on each end task. We experiment with ﬁne-tuning SciBERT on our tasks, and ﬁnd this to be generally inferior to using our ﬁxed rep- resentations from S PECTER . Speciﬁcally, we ﬁne- tune SciBERT directly on task-speciﬁc signals in- stead of citations. To ﬁne-tune on task-speciﬁc data (e.g., user activity), we used a dataset of co- views with 65K query papers, co-reads with 14K query papers, and co-citations (instead of direct citations) with 83K query papers. As the end tasks are ranking tasks, for all datasets we construct up to 5 triplets and ﬁne-tune the model using triplet ranking loss. The positive papers are sampled from \nThe table presents a comparison of different training signals and their performance across several metrics for document or text classification tasks. The columns labeled \"CLS,\" \"USR,\" \"CITE,\" \"REC,\" and \"All\" represent different evaluation metrics or tasks used to assess the models:\n\n- **SPECTER**:\n  - CLS: 84.2\n  - USR: 88.4 (highest in the table)\n  - CITE: 91.5 (highest in the table)\n  - REC: 36.9 (highest in the table)\n  - All: 80.0 (highest in the table)\n\n- **SciBERT fine-tune on co-view**:\n  - CLS: 83.0\n  - USR: 84.2\n  - CITE: 84.1\n  - REC: 36.4\n  - All: 76.0\n\n- **SciBERT fine-tune on co-read**:\n  - CLS: 82.3\n  - USR: 85.4\n  - CITE: 86.7\n  - REC: 36.3\n  - All: 77.1\n\n- **SciBERT fine-tune on co-citation**:\n  - CLS: 82.9\n  - USR: 84.3\n  - CITE: 85.2\n  - REC: 36.6\n  - All: 76.4\n\n- **SciBERT fine-tune on multitask**:\n  - CLS: 83.3\n  - USR: 86.1\n  - CITE: 88.2\n  - REC: 36.0\n  - All: 78.0\n\nThe numbers in the table are likely indicative of performance metrics such as accuracy or F1-score, with higher values representing better performance. \"SPECTER\" appears to be the model that achieves the highest scores across most metrics compared to the variations of \"SciBERT\" fine-tuned on different tasks.\nthe most co-viewed (co-read, or co-cited) papers corresponding to the query paper. We also include both easy and hard distractors as when training S PECTER  (for hard negatives we choose the least non-zero co-viewed (co-read, or co-cited) papers). We also consider training jointly on all task-speciﬁc training data sources in a multitask training process, where the model samples training triplets from a distribution over the sources. As illustrated in Ta- ble  3 , without any additional ﬁnal task-speciﬁc ﬁne-tuning, S PECTER  still outperforms a SciBERT model ﬁne-tuned on the end tasks as well as their multitask combination, further demonstrating the effectiveness and versatility of S PECTER  embed- dings. \n7 Related Work \nRecent representation learning methods in NLP rely on training large neural language models on un- supervised data ( Peters et al. ,  2018 ;  Radford et al. , 2018 ;  Devlin et al. ,  2019 ;  Beltagy et al. ,  2019 ;  Liu et al. ,  2019 ). While successful at many sentence- and token-level tasks, our focus is on using the models for document-level representation learning, which has remained relatively under-explored. \nThere have been other efforts in document repre- sentation learning such as extensions of word vec- tors to documents ( Le and Mikolov ,  2014 ;  Ganesh et al. ,  2016 ;  Liu et al. ,  2017 ;  Wu et al. ,  2018 ;  Gy- sel et al. ,  2017 ), convolution-based methods ( Liu et al. ,  2018 ;  Zamani et al. ,  2018 ), and variational autoencoders ( Holmer and Marfurt ,  2018 ;  Wang et al. ,  2019 ). Relevant to document embedding, sen- tence embedding is a relatively well-studied area of research. Successful approaches include seq2seq models ( Kiros et al. ,  2015 ), BiLSTM Siamese networks ( Williams et al. ,  2018 ), leveraging su- pervised data from other corpora ( Conneau et al. , 2017 ), and using discourse relations ( Nie et al. , 2019 ), and BERT-based methods ( Reimers and Gurevych ,  2019 ). Unlike our proposed method, the majority of these approaches do not consider any notion of inter-document relatedness when em- bedding documents. "}
{"page": 8, "image_path": "doc_images/2020.acl-main.207_8.jpg", "ocr_text": "the majority of these approaches do not consider\nany notion of inter-document relatedness when em-\nbedding documents.\n\nOther relevant work combines textual features\nwith network structure (Tu et al., 2017; Zhang et al.,\n2018; Bhagavatula et al., 2018; Shen et al., 2018;\nChen et al., 2019; Wang et al., 2019). These works\ntypically do not leverage the recent pretrained con-\ntextual representations and with a few exceptions\nsuch as the recent work by Wang et al. (2019), they\ncannot generalize to unseen documents like our\nSPECTER approach. Context-based citation rec-\nommendation is another related application where\nmodels rely on citation contexts (Jeong et al., 2019)\nto make predictions. These works are orthogonal\nto ours as the input to our model is just paper title\nand abstract. Another related line of work is graph-\nbased representation learning methods (Bruna et al.,\n2014; Kipf and Welling, 2017; Hamilton et al.,\n2017a,b; Wu et al., 2019a,b). Here, we compare to\na graph representation learning model, SGC (Sim-\nple Graph Convolution) (Wu et al., 2019a), which\nis a state-of-the-art graph convolution approach for\nrepresentation learning. SPECTER uses pretrained\nlanguage models in combination with graph-based\ncitation signals, which enables it to outperform the\ngraph-based approaches in our experiments.\n\nSPECTER embeddings are based on only the title\nand abstract of the paper. Adding the full text of the\npaper would provide a more complete picture of the\npaper’s content and could improve accuracy (Co-\nhen et al., 2010; Lin, 2008; Schuemie et al., 2004).\nHowever, the full text of many academic papers\nis not freely available. Further, modern language\nmodels have strict memory limits on input size,\nwhich means new techniques would be required in\norder to leverage the entirety of the paper within\nthe models. Exploring how to use the full paper\ntext within SPECTER is an item of future work.\n\nFinally, one pain point in academic paper rec-\nommendation research has been a lack of publicly\navailable datasets (Chen and Lee, 2018; Kanakia\net al., 2019). To address this challenge, we re-\nlease SCIDOCS, our evaluation benchmark which\nincludes an anonymized clickthrough dataset from\nan online recommendations system.\n\n8 Conclusions and Future Work\n\nWe present SPECTER, a model for learning repre-\nsentations of scientific papers, based on a Trans-\nformer language model that is pretrained on cita-\n\ntions. We achieve substantial improvements over\nthe strongest of a wide variety of baselines, demon-\nstrating the effectiveness of our model. We ad-\nditionally introduce SCIDOCS, a new evaluation\nsuite consisting of seven document-level tasks and\nrelease the corresponding datasets to foster further\nresearch in this area.\n\nThe landscape of Transformer language models\nis rapidly changing and newer and larger models\nare frequently introduced. It would be interest-\ning to initialize our model weights from more re-\ncent Transformer models to investigate if additional\ngains are possible. Another item of future work is\nto develop better multitask approaches to leverage\nmultiple signals of relatedness information during\ntraining. We used citations to build triplets for our\nloss function, however there are other metrics that\nhave good support from the bibliometrics literature\n(Klavans and Boyack, 2006) that warrant exploring\nas a way to create relatedness graphs. Including\nother information such as outgoing citations as ad-\nditional input to the model would be yet another\narea to explore in future.\n\nAcknowledgements\n\nWe thank Kyle Lo, Daniel King and Oren Etzioni\nfor helpful research discussions, Russel Reas for\nsetting up the public API, Field Cady for help in\ninitial data collection and the anonymous reviewers\n(especially Reviewer 1) for comments and sugges-\ntions. This work was supported in part by NSF\nConvergence Accelerator award 1936940, ONR\ngrant NO0014-18-1-2193, and the University of\nWashington WRF/Cable Professorship.\n\nReferences\n\nAnant K. Agarwal, Ivan Zaitsev, Xuanhui Wang,\nCheng Yen Li, Marc Najork, and Thorsten Joachims.\n2019. Estimating position bias without intrusive in-\nterventions. In WSDM.\n\nWaleed Ammar, Dirk Groeneveld, Chandra Bha-\ngavatula, Iz Beltagy, Miles Crawford, Doug\nDowney, Jason Dunkelberger, Ahmed Elgohary,\nSergey Feldman, Vu Ha, Rodney Kinney, Sebas-\ntian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu-\nHan Ooi, Matthew E. Peters, Joanna Power, Sam\nSkjonsberg, Lucy Lu Wang, Christopher Wilhelm,\nZheng Yuan, Madeleine van Zuylen, and Oren Et-\nzioni. 2018. Construction of the literature graph in\nsemantic scholar. In NAACL-HLT.\n\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\n\n2278\n", "vlm_text": "\nOther relevant work combines textual features with network structure ( Tu et al. ,  2017 ;  Zhang et al. , 2018 ;  Bhagavatula et al. ,  2018 ;  Shen et al. ,  2018 ; Chen et al. ,  2019 ;  Wang et al. ,  2019 ). These works typically do not leverage the recent pretrained con- textual representations and with a few exceptions such as the recent work by  Wang et al.  ( 2019 ), they cannot generalize to unseen documents like our S PECTER  approach. Context-based citation rec- ommendation is another related application where models rely on citation contexts ( Jeong et al. ,  2019 ) to make predictions. These works are orthogonal to ours as the input to our model is just paper title and abstract. Another related line of work is graph- based representation learning methods ( Bruna et al. , 2014 ;  Kipf and Welling ,  2017 ;  Hamilton et al. , 2017a , b ;  Wu et al. ,  2019a , b ). Here, we compare to a graph representation learning model, SGC (Sim- ple Graph Convolution) ( Wu et al. ,  2019a ), which is a state-of-the-art graph convolution approach for representation learning. S PECTER  uses pretrained language models in combination with graph-based citation signals, which enables it to outperform the graph-based approaches in our experiments. \nS PECTER  embeddings are based on only the title and abstract of the paper. Adding the full text of the paper would provide a more complete picture of the paper’s content and could improve accuracy ( Co- hen et al. ,  2010 ;  Lin ,  2008 ;  Schuemie et al. ,  2004 ). However, the full text of many academic papers is not freely available. Further, modern language models have strict memory limits on input size, which means new techniques would be required in order to leverage the entirety of the paper within the models. Exploring how to use the full paper text within S PECTER  is an item of future work. \nFinally, one pain point in academic paper rec- ommendation research has been a lack of publicly available datasets ( Chen and Lee ,  2018 ;  Kanakia et al. ,  2019 ). To address this challenge, we re- lease S CI D OCS , our evaluation benchmark which includes an anonymized clickthrough dataset from an online recommendations system. \n8 Conclusions and Future Work \nWe present S PECTER , a model for learning repre- sentations of scientiﬁc papers, based on a Trans- former language model that is pretrained on cita- tions. We achieve substantial improvements over the strongest of a wide variety of baselines, demon- strating the effectiveness of our model. We ad- ditionally introduce S CI D OCS , a new evaluation suite consisting of seven document-level tasks and release the corresponding datasets to foster further research in this area. \n\nThe landscape of Transformer language models is rapidly changing and newer and larger models are frequently introduced. It would be interest- ing to initialize our model weights from more re- cent Transformer models to investigate if additional gains are possible. Another item of future work is to develop better multitask approaches to leverage multiple signals of relatedness information during training. We used citations to build triplets for our loss function, however there are other metrics that have good support from the bibliometrics literature ( Klavans and Boyack ,  2006 ) that warrant exploring as a way to create relatedness graphs. Including other information such as outgoing citations as ad- ditional input to the model would be yet another area to explore in future. \nAcknowledgements \nWe thank Kyle Lo, Daniel King and Oren Etzioni for helpful research discussions, Russel Reas for setting up the public API, Field Cady for help in initial data collection and the anonymous reviewers (especially Reviewer 1) for comments and sugges- tions. This work was supported in part by NSF Convergence Accelerator award 1936940, ONR grant N00014-18-1-2193, and the University of Washington WRF/Cable Professorship. \nReferences \nAnant K. Agarwal, Ivan Zaitsev, Xuanhui Wang, Cheng Yen Li, Marc Najork, and Thorsten Joachims. 2019. Estimating position bias without intrusive in- terventions. In  WSDM . \nWaleed Ammar, Dirk Groeneveld, Chandra Bha- gavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebas- tian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu- Han Ooi, Matthew E. Peters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang, Christopher Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Et- zioni. 2018. Construction of the literature graph in semantic scholar. In  NAACL-HLT . \nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. "}
{"page": 9, "image_path": "doc_images/2020.acl-main.207_9.jpg", "ocr_text": "A simple but tough-to-beat baseline for sentence em-\nbeddings. In JCLR.\n\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientific\nText. In EMNLP.\n\nChandra Bhagavatula, Sergey Feldman, Russell Power,\nand Waleed Ammar. 2018. Content-Based Citation\nRecommendation. In NAACL-HLT.\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikoloy. 2017. Enriching word vectors with\nsubword information. TACL.\n\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and\nYann LeCun. 2014. Spectral networks and locally\nconnected networks on graphs. JCLR.\n\nLiqun Chen, Guoyin Wang, Chenyang Tao, Ding-\nhan Shen, Pengyu Cheng, Xinyuan Zhang, Wenlin\nWang, Yizhe Zhang, and Lawrence Carin. 2019. Im-\nproving textual network embedding with global at-\ntention via optimal transport. In ACL.\n\nTsung Teng Chen and Maria Lee. 2018. Research Pa-\nper Recommender Systems on Big Scholarly Data.\nIn Knowledge Management and Acquisition for In-\ntelligent Systems.\n\nK. Bretonnel Cohen, Helen L. Johnson, Karin M. Ver-\nspoor, Christophe Roeder, and Lawrence Hunter.\n2010. The structural and content aspects of abstracts\nversus bodies of full text journal articles are different.\nBMC Bioinformatics, 11:492-492.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\nBarrault, and Antoine Bordes. 2017. Supervised\nLearning of Universal Sentence Representations\nfrom Natural Language Inference Data. In EMNLP.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\n\nMartin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei\nXu, et al. 1996. A Density-based Algorithm for Dis-\ncovering Clusters in Large Spatial Databases with\nNoise. In KDD.\n\nSergey Feldman, Waleed Ammar, Kyle Lo, Elly Trep-\nman, Madeleine van Zuylen, and Oren Etzioni. 2019.\nQuantifying Sex Bias in Clinical Studies at Scale\nWith Automated Data Extraction. JAMA.\n\nJ Ganesh, Manish Gupta, and Vijay K. Varma. 2016.\nDoc2sent2vec: A novel two-phase approach for\nlearning document representation. In SIGIR.\n\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A Deep Semantic Natural Language Pro-\ncessing Platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS).\n\nChristophe Van Gysel, Maarten de Rijke, and Evange-\nlos Kanoulas. 2017. Neural Vector Spaces for Un-\nsupervised Information Retrieval. ACM Trans. Inf.\nSyst.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017a.\nInductive Representation Learning on Large Graphs.\nIn NIPS.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec.\n2017b. Inductive representation learning on large\ngraphs. In NIPS.\n\nErik Holmer and Andreas Marfurt. 2018. Explaining\naway syntactic structure in semantic document rep-\nresentations. ArXiv, abs/1806.01620.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classification.\nIn ACL.\n\nChanwoo Jeong, Sion Jang, Hyuna Shin, Eun-\njeong Lucy Park, and Sungchul Choi. 2019. A\ncontext-aware citation recommendation model with\nbert and graph convolutional networks. ArXiv,\nabs/1903.06464.\n\nAnshul Kanakia, Zhihong Shen, Darrin Eide, and\nKuansan Wang. 2019. A Scalable Hybrid Research\nPaper Recommender System for Microsoft Aca-\ndemic. In WWW.\n\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA Method for Stochastic Optimization. ArXiv,\nabs/1412.6980.\n\nThomas N Kipf and Max Welling. 2017. Semi-\n\nsupervised classification with graph convolutional\nnetworks. JCLR.\n\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Antonio Torralba, Raquel Urta-\nsun, and Sanja Fidler. 2015. Skip-thought vectors.\nIn NIPS.\n\nRichard Klavans and Kevin W. Boyack. 2006. Iden-\ntifying a better measure of relatedness for mapping\nscience. Journal of the Association for Information\nScience and Technology, 57:251-263.\n\nJey Han Lau and Timothy Baldwin. 2016. An\nempirical evaluation of doc2vec with practical in-\nsights into document embedding generation. In\nRep4NLP@ACL.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed Repre-\nsentations of Sentences and Documents. In JCML.\n\nJimmy J. Lin. 2008. Is searching full text more effec-\ntive than searching abstracts? BMC Bioinformatics,\n10:46-46.\n\nCarolyn E Lipscomb. 2000. Medical Subject Headings\n(MeSH). Bulletin of the Medical Library Associa-\ntion.\n\n2279\n", "vlm_text": "A simple but tough-to-beat baseline for sentence em- \nbeddings. In  ICLR . Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB- ERT: A Pretrained Language Model for Scientiﬁc Text. In  EMNLP . Chandra Bhagavatula, Sergey Feldman, Russell Power, and Waleed Ammar. 2018. Content-Based Citation Recommendation. In  NAACL-HLT . Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.  Enriching word vectors with subword information. TACL.Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. Spectral networks and locally connected networks on graphs.  ICLR . Liqun Chen, Guoyin Wang, Chenyang Tao, Ding- han Shen, Pengyu Cheng, Xinyuan Zhang, Wenlin Wang, Yizhe Zhang, and Lawrence Carin. 2019. Im- proving textual network embedding with global at- tention via optimal transport. In  ACL . Tsung Teng Chen and Maria Lee. 2018. Research Pa- per Recommender Systems on Big Scholarly Data. In  Knowledge Management and Acquisition for In- telligent Systems . K. Bretonnel Cohen, Helen L. Johnson, Karin M. Ver- spoor, Christophe Roeder, and Lawrence Hunter. 2010. The structural and content aspects of abstracts versus bodies of full text journal articles are different. BMC Bioinformatics , 11:492–492. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ ıc Barrault, and Antoine Bordes. 2017. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data . In  EMNLP . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In  NAACL-HLT . Martin Ester, Hans-Peter Kriegel, J¨ org Sander, Xiaowei Xu, et al. 1996. A Density-based Algorithm for Dis- covering Clusters in Large Spatial Databases with Noise. In  KDD . Sergey Feldman, Waleed Ammar, Kyle Lo, Elly Trep- man, Madeleine van Zuylen, and Oren Etzioni. 2019. Quantifying Sex Bias in Clinical Studies at Scale With Automated Data Extraction .  JAMA . J Ganesh, Manish Gupta, and Vijay K. Varma. 2016. Doc2sent2vec: A novel two-phase approach for learning document representation. In  SIGIR . Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A Deep Semantic Natural Language Pro- cessing Platform . In  Proceedings of Workshop for NLP Open Source Software (NLP-OSS) . \nChristophe Van Gysel, Maarten de Rijke, and Evange- los Kanoulas. 2017. Neural Vector Spaces for Un- supervised Information Retrieval.  ACM Trans. Inf. Syst. Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017a. Inductive Representation Learning on Large Graphs. In  NIPS . William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017b. Inductive representation learning on large graphs. In  NIPS . Erik Holmer and Andreas Marfurt. 2018. Explaining away syntactic structure in semantic document rep- resentations.  ArXiv , abs/1806.01620. Jeremy Howard and Sebastian Ruder. 2018.  Universal Language Model Fine-tuning for Text Classiﬁcation . In  ACL . Chanwoo Jeong, Sion Jang, Hyuna Shin, Eun- jeong Lucy Park, and Sungchul Choi. 2019. A context-aware citation recommendation model with bert and graph convolutional networks. ArXiv , abs/1903.06464. Anshul Kanakia, Zhihong Shen, Darrin Eide, and Kuansan Wang. 2019. A Scalable Hybrid Research Paper Recommender System for Microsoft Aca- demic. In  WWW . Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. ArXiv , abs/1412.6980.Thomas N Kipf and Max Welling. 2017. Semi- supervised classiﬁcation with graph convolutional networks.  ICLR . Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urta- sun, and Sanja Fidler. 2015. Skip-thought vectors. In  NIPS . Richard Klavans and Kevin W. Boyack. 2006. Iden- tifying a better measure of relatedness for mapping science.  Journal of the Association for Information Science and Technology , 57:251–263. Jey Han Lau and Timothy Baldwin. 2016. An empirical evaluation of doc2vec with practical in- sights into document embedding generation. In Rep4NLP@ACL . Quoc Le and Tomas Mikolov. 2014. Distributed Repre- sentations of Sentences and Documents. In  ICML . Jimmy J. Lin. 2008. Is searching full text more effec- tive than searching abstracts?  BMC Bioinformatics , 10:46–46. Carolyn E Lipscomb. 2000. Medical Subject Headings (MeSH).  Bulletin of the Medical Library Associa- tion . "}
{"page": 10, "image_path": "doc_images/2020.acl-main.207_10.jpg", "ocr_text": "Chundi Liu, Shunan Zhao, and Maksims Volkovs.\n2018. Unsupervised Document Embedding with\nCNNs. ArXiv, abs/1711.04168v3.\n\nPengfei Liu, King Keung Wu, and Helen M. Meng.\n2017. A Model of Extended Paragraph Vector\nfor Document Categorization and Trend Analysis.\nIICNN.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv, abs/1907.11692.\n\nLaurens van der Maaten. 2014. Accelerating t-SNE\nUsing Tree-based Algorithms. Journal of Machine\nLearning Research.\n\nAllen Nie, Erin Bennett, and Noah Goodman. 2019.\nDisSent: Learning Sentence Representations from\nExplicit Discourse Relations. In ACL.\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research,\n12:2825-2830.\n\nMatthew E. Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. arXiv.\n\nRadim Rehiifek and Petr Sojka. 2010. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nLREC.\n\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence Embeddings using Siamese BERT-\nNetworks. In EMNLP.\n\nAndrew Rosenberg and Julia Hirschberg. 2007. V-\nmeasure: A Conditional Entropy-based External\nCluster Evaluation Measure. In EMNLP.\n\nJ Ben Schafer, Dan Frankowski, Jon Herlocker, and\nShilad Sen. 2007. Collaborative filtering recom-\nmender systems. In The adaptive web. Springer.\n\nMartijn J. Schuemie, Marc Weeber, Bob J. A. Schijve-\nnaars, Erik M. van Mulligen, C. Christiaan van der\nEyjk, Rob Jelier, Barend Mons, and Jan A. Kors.\n2004. Distribution of information in biomedical ab-\nstracts and full-text publications. Bioinformatics,\n20(16):2597-604.\n\nDinghan Shen, Xinyuan Zhang, Ricardo Henao, and\nLawrence Carin. 2018. Improved semantic-aware\nnetwork embedding with fine-grained word align-\nment. In EMNLP.\n\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\nrin Eide, Bo-June Paul Hsu, and Kuansan Wang.\n2015. An Overview of Microsoft Academic Service\n(MAS) and Applications. In WWW.\n\nCunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun.\n2017. Cane: Context-aware network embedding for\nrelation modeling. In ACL.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In NIPS.\n\nWenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang,\nLiqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian\nYang, Ricardo Henao, and Lawrence Carin. 2019.\nImproving textual network learning with variational\nhomophilic embeddings. In Advances in Neural In-\nformation Processing Systems, pages 2074-2085.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A Broad-Coverage Challenge Corpus for Sen-\ntence Understanding through Inference. In NAACL-\nHLT.\n\nFelix Wu, Amauri H. Souza, Tianyi Zhang, Christo-\npher Fifty, Tao Yu, and Kilian Q. Weinberger.\n2019a. Simplifying graph convolutional networks.\nIn ICML.\n\nLingfei Wu, Jan En-Hsu Yen, Kun Xu, Fangli\nXu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep\nRavikumar, and Michael J Witbrock. 2018. Word\nMover’s Embedding: From Word2Vec to Document\nEmbedding. In EMNLP.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. ArXiv, abs/1609.08 144.\n\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong\nLong, Chengqi Zhang, and Philip S Yu. 2019b. A\nComprehensive Survey on Graph Neural Networks.\nArXiv, abs/1901.00596.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.\nXInet: Generalized autoregressive pretraining for\nlanguage understanding. ArXiv, abs/1906.08237.\n\nHamed Zamani, Mostafa Dehghani, W. Bruce Croft,\nErik G. Learned-Miller, and Jaap Kamps. 2018.\nFrom neural re-ranking to neural ranking: Learn-\ning a sparse representation for inverted indexing. In\nCIKM.\n\nXinyuan Zhang, Yitong Li, Dinghan Shen, and\nLawrence Carin. 2018. Diffusion maps for textual\nnetwork embedding. In Neur/PS.\n\n2280\n", "vlm_text": "Chundi Liu, Shunan Zhao, and Maksims Volkovs. 2018. Unsupervised Document Embedding with CNNs.  ArXiv , abs/1711.04168v3. Pengfei Liu, King Keung Wu, and Helen M. Meng. 2017. A Model of Extended Paragraph Vector for Document Categorization and Trend Analysis. IJCNN . Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretrain- ing Approach.  ArXiv , abs/1907.11692. Laurens van der Maaten. 2014. Accelerating t-SNE Using Tree-based Algorithms.  Journal of Machine Learning Research . Allen Nie, Erin Bennett, and Noah Goodman. 2019. DisSent: Learning Sentence Representations from Explicit Discourse Relations . In  ACL . F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch- esnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training.  arXiv . Radim  Reh˚ uˇ rek and Petr Sojka. 2010. Software Frame- work for Topic Modelling with Large Corpora. In LREC . Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence Embeddings using Siamese BERT- Networks . In  EMNLP . Andrew Rosenberg and Julia Hirschberg. 2007. V- measure: A Conditional Entropy-based External Cluster Evaluation Measure. In  EMNLP . J Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. 2007. Collaborative ﬁltering recom- mender systems. In  The adaptive web . Springer. Martijn J. Schuemie, Marc Weeber, Bob J. A. Schijve- naars, Erik M. van Mulligen, C. Christiaan van der Eijk, Rob Jelier, Barend Mons, and Jan A. Kors. 2004. Distribution of information in biomedical ab- stracts and full-text publications. Bioinformatics , 20(16):2597–604. Dinghan Shen, Xinyuan Zhang, Ricardo Henao, and Lawrence Carin. 2018. Improved semantic-aware network embedding with ﬁne-grained word align- ment. In  EMNLP . \nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar- rin Eide, Bo-June Paul Hsu, and Kuansan Wang. 2015. An Overview of Microsoft Academic Service (MAS) and Applications. In  WWW . Cunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun. 2017. Cane: Context-aware network embedding for relation modeling. In  ACL . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In  NIPS . Wenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang, Liqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian Yang, Ricardo Henao, and Lawrence Carin. 2019. Improving textual network learning with variational homophilic embeddings. In  Advances in Neural In- formation Processing Systems , pages 2074–2085. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.  A Broad-Coverage Challenge Corpus for Sen- tence Understanding through Inference . In  NAACL- HLT . Felix Wu, Amauri H. Souza, Tianyi Zhang, Christo- pher Fifty, Tao Yu, and Kilian Q. Weinberger. 2019a. Simplifying graph convolutional networks. In  ICML . Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep Ravikumar, and Michael J Witbrock. 2018. Word Mover’s Embedding: From Word2Vec to Document Embedding. In  EMNLP . Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation.  ArXiv , abs/1609.08144. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. 2019b. A Comprehensive Survey on Graph Neural Networks. ArXiv , abs/1901.00596. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car- bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding.  ArXiv , abs/1906.08237. Hamed Zamani, Mostafa Dehghani, W. Bruce Croft, Erik G. Learned-Miller, and Jaap Kamps. 2018. From neural re-ranking to neural ranking: Learn- ing a sparse representation for inverted indexing. In CIKM . Xinyuan Zhang, Yitong Li, Dinghan Shen, and Lawrence Carin. 2018. Diffusion maps for textual network embedding. In  NeurIPS . "}
{"page": 11, "image_path": "doc_images/2020.acl-main.207_11.jpg", "ocr_text": "A Appendix A - Baseline Details\n\n1. Random Zero-mean 25-dimensional vectors\nwere used as representations for each document.\n\n2. Doc2Vec Doc2Vec is one of the earlier neural\ndocument/paragraph representation methods (Le\nand Mikolov, 2014), and is a natural comparison.\nWe trained Doc2Vec on our training subset using\nGensim (Rehiifek and Sojka, 2010), and chose the\nhyperparameter grid using suggestions from Lau\nand Baldwin (2016). The hyperparameter grid\nused:\n\n{’window’: [5, 10, 15],\n‘sample’: [0, 10 «* -6, 10 «x -5]\n‘epochs’: [50, 100, 200]},\n\nfor a total of 27 models. The other parameters\nwere set as follows: vector_size=300,\nmin_count=3, alpha=0.025,\nmin_alpha=0.0001, negative=5, dm=0,\ndbow=1, dbow_words=0.\n\n3. Fasttext-Sum This simple baseline is a\nweighted sum of pretrained word vectors. We\ntrained our own 300 dimensional fasttext embed-\ndings (Bojanowski et al., 2017) on a corpus of\naround 3.1B tokens from scientific papers which\nis similar in size to the SciBERT corpus (Beltagy\net al., 2019). We found that these pretrained embed-\ndings substantially outperform alternative off-the-\nshelf embeddings. We also use these embeddings in\nother baselines that require pretrained word vectors\n(i.e., SIF and SGC that are described below). The\nsummed bag of words representation has a number\nof weighting options, which are extensively tuned\non a validation set for best performance.\n\n4. SIF The SIF method of Arora et al. (2017) is\na strong text representation baseline that takes a\nweighted sum of pretrained word vectors (we use\nfasttext embeddings described above), then com-\nputes the first principal component of the document\nembedding matrix and subtracts out each document\nembedding’s projection to the first principal com-\nponent.\n\nWe used a held-out validation set to choose a\nfrom the range [1.0e-5, 1.0e-3] spaced evenly\non a log scale. The word probability p(w) was\nestimated on the training set only. When com-\nputing term-frequency values for SIF, we used\nscikit-learn’s TfidfVectorizer with the same pa-\nrameters as enumerated in the preceding sec-\ntion. sublinear_tf, binary, use_idf,\n\n,\n\nsmooth_idf were all set to False. Since SIF\nis a sum of pretrained fasttext vectors, the resulting\ndimensionality is 300.\n\n5. ELMo ELMo (Peters et al., 2018) provides con-\ntextualized representations of tokens in a document.\nIt can provide paragraph or document embeddings\nby averaging each token’s representation for all 3\nLSTM layers. We used the 768-dimensional pre-\ntrained ELMo model in AllenNLP (Gardner et al.,\n2018).\n\n6. Citeomatic The most relevant baseline is Citeo-\nmatic (Bhagavatula et al., 2018), which is an aca-\ndemic paper representation model that is trained on\nhe citation graph via sampled triplets. Citeomatic\nrepresentations are an L2 normalized weighted sum\nof title and abstract embeddings, which are trained\non the citation graph with dynamic negative sam-\npling. Citeomatic embeddings are 75-dimensional.\n\n7. SGC Since our algorithm is trained on data from\nhe citation graph, we also compare to a state-of-\nhe-art graph representation learning model: SGC\n(Simple Graph Convolution) (Wu et al., 2019a),\nwhich is a graph convolution network. An al-\nernative comparison would have been Graph-\nSAGE (Hamilton et al., 2017b), but SGC (with\nno learning) outperformed an unsupervised variant\nof GraphSAGE on the Reddit dataset!®, Note that\nSGC with no learning boils down to graph prop-\nagation on node features (in our case nodes are\nacademic documents). Following Hamilton et al.\n(2017a), we used SIF features as node representa-\ntions, and applied SGC with a range of parameter\nk, which is the number of times the normalized\nadjacency is multiplied by the SIF feature matrix.\nOur range of k was 1 through 8 (inclusive), and was\nchosen with a validation set. For the node features,\nwe chose the SIF model with a = 0.0001, as this\nmodel was observed to be a high-performing one.\nThis baseline is also 300 dimensional.\n\n8. SciBERT To isolate the advantage of\nSPECTER’s citation-based fine-tuning objective,\nwe add a controlled comparison with SciBERT\n(Beltagy et al., 2019). Following Devlin et al.\n(2019) we take the last layer hidden state corre-\nsponding to the [CLS] token as the aggregate\ndocument representation.!”\n\n‘There were no other direct comparisons in Wu et al.\n(2019a)\n\n‘We also tried the alternative of averaging all token repre-\nsentations, but this resulted in a slight performance decrease\ncompared with the [CLS] pooled token.\n\n2281\n", "vlm_text": "A Appendix A - Baseline Details \n1.  Random  Zero-mean 25-dimensional vectors were used as representations for each document. \n2.  Doc2Vec  Doc2Vec is one of the earlier neural document/paragraph representation methods ( Le and Mikolov ,  2014 ), and is a natural comparison. We trained Doc2Vec on our training subset using Gensim ( Reh u rek and Sojka ,  2010 ), and chose the hyperparameter grid using suggestions from  Lau and Baldwin  ( 2016 ). The hyperparameter grid used: \n\n$$\n\\begin{array}{r l}&{\\mathrm{\\{\\,\\prime\\,\\sfindofw^{\\prime}:\\quad[\\,5\\,,\\quad10\\,,\\quad15\\,]\\,\\,,}}}\\\\ &{\\mathrm{\\{\\,\\prime\\,\\sfvarpipl e^{\\prime}:\\quad[\\,0\\,,\\quad10\\,\\,\\star\\star\\,\\,-6\\,,\\quad10\\,\\,\\star\\star\\,\\,-5\\,]\\,\\,,}}}\\\\ &{\\mathrm{\\{\\,\\prime\\,\\sfvarpichs^{\\prime}:\\quad[\\,50\\,,\\quad10\\,0\\,,\\quad200\\,]\\,\\}\\,\\,,}}\\end{array}\n$$\n \nfor a total of 27 models. The other parameters were set as follows: vector_si  $z\\!\\in\\!=\\!300$  , min_count  $=\\!3$  , alpha  $=\\!0\\cdot0\\,2\\,5$  , min_alpha  $=\\!0$  .0001 ,  negative  $=\\!5$  ,    $\\mathtt{d m}{=}0$  , dbow  $\\mathbf{\\Psi}\\!=\\!\\!1$  ,  dbow_words  $\\mathsf{\\Gamma}\\!=\\!0$  . \n3.  Fasttext-Sum This simple baseline is a weighted sum of pretrained word vectors. We trained our own 300 dimensional fasttext embed- dings ( Bojanowski et al. ,  2017 ) on a corpus of around 3.1B tokens from scientiﬁc papers which is similar in size to the SciBERT corpus ( Beltagy et al. ,  2019 ). We found that these pretrained embed- dings substantially outperform alternative off-the- shelf embeddings. We also use these embeddings in other baselines that require pretrained word vectors (i.e., SIF and SGC that are described below). The summed bag of words representation has a number of weighting options, which are extensively tuned on a validation set for best performance. \n4.  SIF  The SIF method of  Arora et al.  ( 2017 ) is a strong text representation baseline that takes a weighted sum of pretrained word vectors (we use fasttext embeddings described above), then com- putes the ﬁrst principal component of the document embedding matrix and subtracts out each document embedding’s projection to the ﬁrst principal com- ponent. \nWe used a held-out validation set to choose    $a$  from the range [1.0e-5, 1.0e-3] spaced evenly on a log scale. The word probability    $p(w)$   was estimated on the training set only. When com- puting term-frequency values for SIF, we used scikit-learn’s TﬁdfVectorizer with the same pa- rameters as enumerated in the preceding sec- tion. sublinear_tf ,  binary ,  use_idf , smooth_idf  were all set to  False . Since SIF is a sum of pretrained fasttext vectors, the resulting dimensionality is 300. \n\n5.  ELMo  ELMo ( Peters et al. ,  2018 ) provides con- textualized representations of tokens in a document. It can provide paragraph or document embeddings by averaging each token’s representation for all 3 LSTM layers. We used the 768-dimensional pre- trained ELMo model in AllenNLP ( Gardner et al. , 2018 ). \n6.  Citeomatic  The most relevant baseline is Citeo- matic ( Bhagavatula et al. ,  2018 ), which is an aca- demic paper representation model that is trained on the citation graph via sampled triplets. Citeomatic representations are an L2 normalized weighted sum of title and abstract embeddings, which are trained on the citation graph with dynamic negative sam- pling. Citeomatic embeddings are 75-dimensional. \n7.  SGC  Since our algorithm is trained on data from the citation graph, we also compare to a state-of- the-art graph representation learning model: SGC (Simple Graph Convolution) ( Wu et al. ,  2019a ), which is a graph convolution network. An al- ternative comparison would have been Graph- SAGE ( Hamilton et al. ,  2017b ), but SGC (with no learning) outperformed an unsupervised variant of GraphSAGE on the Reddit dataset 16 , Note that SGC with no learning boils down to graph prop- agation on node features (in our case nodes are academic documents). Following  Hamilton et al. ( 2017a ), we used SIF features as node representa- tions, and applied SGC with a range of parameter $k$  , which is the number of times the normalized adjacency is multiplied by the SIF feature matrix. Our range of  $k$   was 1 through 8 (inclusive), and was chosen with a validation set. For the node features, we chose the SIF model with    $a=0.0001$  , as this model was observed to be a high-performing one. This baseline is also 300 dimensional. \n8.  SciBERT To isolate the advantage of S PECTER ’s citation-based ﬁne-tuning objective, we add a controlled comparison with SciBERT\n\n ( Beltagy et al. ,  2019 ). Following  Devlin et al.\n\n ( 2019 ) we take the last layer hidden state corre- sponding to the  [CLS]  token as the aggregate document representation. "}
{"page": 12, "image_path": "doc_images/2020.acl-main.207_12.jpg", "ocr_text": "9. Sentence BERT Sentence BERT (Reimers and\nGurevych, 2019) is a general-domain pretrained\nmodel aimed at embedding sentences. The au-\nthors fine-tuned BERT using a triplet loss, where\npositive sentences were from the same document\nsection as the seed sentence, and distractor sen-\ntences came from other document sections. The\nmodel is designed to encode sentences as opposed\nto paragraphs, so we embed the title and each sen-\ntence in the abstract separately, sum the embed-\ndings, and L2 normalize the result to produce a\nfinal 768-dimensional paper embedding.!®\n\nDuring hyperparameter optimization we chose\nhow to compute TF and IDF values weights by\ntaking the following non-redundant combinations\nof scikit-learn’s Tfidf Vectorizer (Pedregosa et al.,\n2011) parameters: sublinear_tf, binary,\nuse_idf, smooth_idf. There were a total\nof 9 parameter combinations. The IDF values\nwere estimated on the training set. The other\nparameters were set as follows: min_df=3,\nmax_df=0.75, strip_accents=’ascii’,\nstop_words=’english’, norm=None,\nlowercase=True. For training of fasttext, we\nused all default parameters with the exception of\nsetting dimension to 300 and minCount was set\nto 25 due to the large corpus.\n\n'SWe used the ‘bert-base-wikipedia-sections-mean-tokens’\nmodel released by the authors: https: //github.com/\nUKPLab/sentence-transformers\n\n2282\n", "vlm_text": "9.  Sentence BERT  Sentence BERT ( Reimers and Gurevych ,  2019 ) is a general-domain pretrained model aimed at embedding sentences. The au- thors ﬁne-tuned BERT using a triplet loss, where positive sentences were from the same document section as the seed sentence, and distractor sen- tences came from other document sections. The model is designed to encode sentences as opposed to paragraphs, so we embed the title and each sen- tence in the abstract separately, sum the embed- dings, and L2 normalize the result to produce a ﬁnal 768-dimensional paper embedding. \nDuring hyperparameter optimization we chose how to compute TF and IDF values weights by taking the following non-redundant combinations of scikit-learn’s TﬁdfVectorizer ( Pedregosa et al. , 2011) parameters: sublinear_tf, binary,use_idf ,  smooth_idf . There were a total of 9 parameter combinations. The IDF values were estimated on the training set. The other parameters were set as follows: min_df  $=\\!3$  , max_ ${\\mathsf{d f}}{=}0\\cdot7\\,5$ , strip_accents $\\mathfrak{s}\\!=\\!\\prime$ ascii’,stop_  $\\mathtt{W O T d S}\\!=\\!\\prime\\in\\!\\mathtt{n g l i s h}$  h’ , norm  $\\mathrm{i}{=}$  None , lowercase  $=$  True . For training of fasttext, we used all default parameters with the exception of setting dimension to 300 and  minCount  was set to 25 due to the large corpus. "}
