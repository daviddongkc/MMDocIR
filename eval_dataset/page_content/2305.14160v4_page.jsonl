{"page": 0, "image_path": "doc_images/2305.14160v4_0.jpg", "ocr_text": "arX1iv:2305.14160v4 [cs.CL] 19 Dec 2023\n\nLabel Words are Anchors: An Information Flow Perspective for\nUnderstanding In-Context Learning\n\nLean Wang'*, Lei Li', Damai Dai‘, Deli Chen’,\n\nHao Zhou', Fandong Meng’, Jie Zhou’, Xu Sun‘\nNational Key Laboratory for Multimedia Information Processing,\nSchool of Computer Science, Peking University\n$Pattern Recognition Center, WeChat AI, Tencent Inc., China\n\n{lean, daidamai , xusun}@pku. edu. cn\n{tuxzhou, fandongmeng,withtomzhou}@tencent.com\n\nvictorchen@deepseek.com\nAbstract\n\nIn-context learning (ICL) emerges as a promis-\ning capability of large language models (LLMs)\nby providing them with demonstration exam-\nples to perform diverse tasks. However, the un-\nderlying mechanism of how LLMs learn from\nthe provided context remains under-explored.\nIn this paper, we investigate the working mech-\nanism of ICL through an information flow lens.\nOur findings reveal that label words in the\ndemonstration examples function as anchors:\n(1) semantic information aggregates into label\nword representations during the shallow compu-\ntation layers’ processing; (2) the consolidated\ninformation in label words serves as a reference\nfor LLMs’ final predictions. Based on these in-\nsights, we introduce an anchor re-weighting\nmethod to improve ICL performance, a demon-\nstration compression technique to expedite in-\nference, and an analysis framework for diag-\nnosing ICL errors in GPT2-XL. The promising\napplications of our findings again validate the\nuncovered ICL working mechanism and pave\nthe way for future studies.!\n\n1 Introduction\n\nIn-context Learning (ICL) has emerged as a power-\nful capability alongside the development of scaled-\nup large language models (LLMs) (Brown et al.,\n2020). By instructing LLMs using few-shot demon-\nstration examples, ICL enables them to perform a\nwide range of tasks, such as text classification (Min\net al., 2022a) and mathematical reasoning (Wei\net al., 2022). Since ICL does not require updates\nto millions or trillions of model parameters and\nrelies on human-understandable natural language\ninstructions (Dong et al., 2023), it has become a\npromising approach for harnessing the full poten-\ntiality of LLMs. Despite its significance, the inner\nworking mechanism of ICL remains an open ques-\ntion, garnering considerable interest from research\n\n‘https: //github.com/lancopku/\nlabel-words-are-anchors\n\nnlp.lilei@gmail.com\n\nReview Review Review Review\nA Y) A A A\ngood 4, good good good\nmovie yy movie movie movie\nSent _, Sent Sent Sent\nts, iment iment iment\nPositive Positive Positive Positive\nReview Review Review Review\nwaste waste “\"\" waste waste\nof of of of\nmoney money money money\nSent Sent Sent Sent\niment iment iment iment\n\"Negative Negative Negative Negative\nReview Review Review Review\nfantastic fantastic fantastic fantastic\nSent Sent Sent Sent\niment iment iment iment\n\nShallow layer Deep layer\n\nFigure 1: Visualization of the information flow in a GPT\nmodel performing ICL. The line depth reflects the sig-\nnificance of the information flow from the right word to\nthe left. The flows involving label words are highlighted.\nLabel words gather information from demonstrations in\nshallow layers, which is then extracted in deep layers\nfor final prediction.\n\ncommunities (Xie et al., 2022; Dai et al., 2022;\nAkyiirek et al., 2022; Li et al., 2023b).\n\nIn this paper, we find that the label words serve\nas anchors that aggregate and distribute information\nin ICL. We first visualize the attention interactive\npattern between tokens with a GPT model (Brown\net al., 2020) on sentiment analysis (Figure 1). Ini-\ntial observations suggest that label words aggregate\ninformation in shallow layers and distribute it in\ndeep layers.” To draw a clearer picture of this phe-\nnomenon, we design two metrics based on saliency\n\n7In this paper, “shallow” or “first” layers refer to those\ncloser to the input, while “deep” or “last” layers are closer\nto the output. Here, “deep layers” include those around the\nmidpoint, e.g., layers 25-48 in a 48-layer GPT2-XL.\n", "vlm_text": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning \nLean Wang † , § , Lei  $\\mathbf{L}\\mathbf{i}^{\\dagger}$  , Damai Dai † , Deli Chen § , Hao Zhou § , Fandong Meng § , Jie Zhou § , Xu Sun † \nNational Key Laboratory for Multimedia Information Processing, \n§ Pattern Recognition Center, WeChat AI, Tencent Inc., China {lean,daidamai,xusun}@pku.edu.cn nlp.lilei@gmail.com victorchen@deepseek.com {tuxzhou,fan dong meng,with tom zhou}@tencent.com \nAbstract \nIn-context learning (ICL) emerges as a promis- ing capability of large language models (LLMs) by providing them with demonstration exam- ples to perform diverse tasks. However, the un- derlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mech- anism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow compu- tation layers’ processing; (2) the consolidated information in label words serves as a reference for LLMs’ final predictions. Based on these in- sights, we introduce an anchor re-weighting method to improve ICL performance, a demon- stration compression technique to expedite in- ference, and an analysis framework for diag- nosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies. \n1 Introduction \nIn-context Learning (ICL) has emerged as a power- ful capability alongside the development of scaled- up large language models (LLMs) ( Brown et al. , 2020 ). By instructing LLMs using few-shot demon- stration examples, ICL enables them to perform a wide range of tasks, such as text classification ( Min et al. ,  2022a ) and mathematical reasoning ( Wei et al. ,  2022 ). Since ICL does not require updates to millions or trillions of model parameters and relies on human-understandable natural language instructions ( Dong et al. ,  2023 ), it has become a promising approach for harnessing the full poten- tiality of LLMs. Despite its significance, the inner working mechanism of ICL remains an open ques- tion, garnering considerable interest from research \nThe image appears to depict a diagram related to sentiment analysis. It shows the mapping of reviews to sentiment labels such as \"Positive\" and \"Negative.\" Phrases like \"A good movie\" and \"waste of money\" are connected to their respective sentiments. The diagram seems to illustrate the process or flow of data through multiple stages or models, with emphasis on how reviews are categorized by sentiment over time or iterations. Each connection line indicates the association between the review text and its sentiment classification.\nFigure 1: Visualization of the information flow in a GPT model performing ICL. The line depth reflects the sig- nificance of the information flow from the right word to the left. The flows involving label words are highlighted. Label words gather information from demonstrations in shallow layers, which is then extracted in deep layers for final prediction. \ncommunities ( Xie et al. ,  2022 ;  Dai et al. ,  2022 ; Akyürek et al. ,  2022 ;  Li et al. ,  2023b ). \nIn this paper, we find that the label words serve as anchors that aggregate and distribute information in ICL. We first visualize the attention interactive pattern between tokens with a GPT model ( Brown et al. ,  2020 ) on sentiment analysis (Figure  1 ). Ini- tial observations suggest that label words aggregate information in shallow layers and distribute it in deep layers.   To draw a clearer picture of this phe- nomenon, we design two metrics based on saliency "}
{"page": 1, "image_path": "doc_images/2305.14160v4_1.jpg", "ocr_text": "\\nformation aBEregation\n\n\\nformation aB8'egation\n\nLabel Prediction\n\na KN\n\nReview: | disike... Sentiment: Negative Review: A good ... Sentiment: Positive Review: .. Sentiment:\n\nFigure 2: Illustration of our hypothesis. In shallow layers, label words gather information from demonstrations to\nform semantic representations for deeper processing, while deep layers extract and utilize this information from\n\nlabel words to formulate the final prediction.\n\nscores to portray the information flow in ICL and\nfurther propose the following hypothesis:\n\nInformation Flow with Labels as Anchors\nH,: In shallow layers, label words gather the\ninformation of demonstrations to form seman-\ntic representations for deeper layers.\n\nHz: In deep layers, the model extracts the\ninformation from label words to form the final\nprediction.\n\nTwo experiments are designed to validate the hy-\npothesis using GPT2-XL (Radford et al., 2019) and\nGPT-J (Wang and Komatsuzaki, 2021) across sev-\neral text classification benchmarks. (1) By blocking\nthe information aggregation path to label words in\ncertain layers, we find that such isolation in shal-\nlow layers significantly impairs model performance.\nThis indicates that label words collect useful in-\nformation during forward propagation in shallow\nlayers. (2) We investigate the relationship between\nthe attention distributions on the label words of\nthe target position and the model’s final prediction.\nOur results illustrate a strong positive correlation,\nwhere a candidate label’s probability increases with\nmore attention weight on its corresponding label\ntoken. In summary, these experimental findings\nsuggest that our hypothesis holds well with large\nlanguage models on real-world datasets.\n\nDrawing on insights from the information flow\nperspective, we explore three approaches to en-\nhance ICL’s effectiveness, efficiency, and inter-\npretability. (1) An anchor re-weighting method\nis introduced, which employs a learnable vector to\nadjust the significance of different label words in\ndemonstrations, leading to a 16.7% average accu-\nracy boost compared to standard ICL baselines. (2)\nFor quicker ICL inference, inputs are compressed\n\ninto pre-calculated anchor representations since\nmodel predictions primarily rely on label word acti-\nvations. Testing shows a 1.8 x speedup in inference\nwith only a minimal performance trade-off. (3) An\nerror analysis of ICL on GPT2-XL demonstrates\nthat the label confusion matrix aligns closely with\nthe distance distribution of anchor key vectors, im-\nplying that errors might result from similar anchor\nrepresentations. These promising applications fur-\nther validate our hypothesis and shed light on future\nICL studies for better transparency of LLMs.\n\n2 Label Words are Anchors\n\nThis section confirms the intuitive findings using\ntwo saliency score-based metrics as discussed in\n§ 2.1. The quantitative results lead to a proposed\nhypothesis for the ICL working mechanism: 11: In\nshallow layers, label words aggregate information\nfrom demonstration examples to form semantic\nrepresentations for later computations. 12: In deep\nlayers, the model makes predictions by extracting\ninformation from label words. The validation for\nthese hypotheses is presented in § 2.2 and § 2.3,\nrespectively.\n\n2.1 Hypothesis Motivated by Saliency Scores\n\nThis section aims to discover the inherent patterns\nin the attention interaction between tokens for a\nGPT model. The saliency technique (Simonyan\net al., 2013), a common interpretation tool, is em-\nployed for highlighting critical token interactions.\nFollowing common practice, we use the Taylor\nexpansion (Michel et al., 2019) to calculate the\nsaliency score for each element of the attention\n\nmatrix:\n_ JL£(z)\nDo An (0) Ana\n\nh= : (dy\n\n", "vlm_text": "The image is an illustration explaining a hypothesis about how information is processed in different layers of a model for sentiment analysis.\n\n- **Shallow Layers**: These layers perform \"information aggregation\" where they gather semantic information from demonstration examples. The words \"Negative\" and \"Positive\" are used to indicate sentiment in reviews.\n- **Deep Layers**: These utilize the aggregated information to make a final prediction, as indicated by the label \"Label Prediction.\" The predictions are based on previously identified sentiments (negative and positive).\n\nThe image shows a flow of information, indicating processing from shallow to deep layers.\nscores to portray the information flow in ICL and further propose the following hypothesis: \nInformation Flow with Labels as Anchors  $\\mathcal{H}_{1}$  : In shallow layers, label words gather the information of demonstrations to form seman- tic representations for deeper layers.  $\\mathcal{H}_{2}$  : In deep layers, the model extracts the information from label words to form the final prediction. \nTwo experiments are designed to validate the hy- pothesis using GPT2-XL ( Radford et al. ,  2019 ) and GPT-J ( Wang and Komatsu zak i ,  2021 ) across sev- eral text classification benchmarks. (1) By blocking the information aggregation path to label words in certain layers, we find that such isolation in shal- low layers significantly impairs model performance. This indicates that label words collect useful in- formation during forward propagation in shallow layers. (2) We investigate the relationship between the attention distributions on the label words of the target position and the model’s final prediction. Our results illustrate a strong positive correlation, where a candidate label’s probability increases with more attention weight on its corresponding label token. In summary, these experimental findings suggest that our hypothesis holds well with large language models on real-world datasets. \nDrawing on insights from the information flow perspective, we explore three approaches to en- hance ICL’s effectiveness, efficiency, and inter- pre t ability. (1) An anchor re-weighting method is introduced, which employs a learnable vector to adjust the significance of different label words in demonstrations, leading to a   $16.7\\%$   average accu- racy boost compared to standard ICL baselines. (2) For quicker ICL inference, inputs are compressed into pre-calculated anchor representations since model predictions primarily rely on label word acti- vations. Testing shows a  $1.8\\times$   speedup in inference with only a minimal performance trade-off. (3) An error analysis of ICL on GPT2-XL demonstrates that the label confusion matrix aligns closely with the distance distribution of anchor key vectors, im- plying that errors might result from similar anchor representations. These promising applications fur- ther validate our hypothesis and shed light on future ICL studies for better transparency of LLMs. \n\n2 Label Words are Anchors \nThis section confirms the intuitive findings using two saliency score-based metrics as discussed in  $\\S~2.1$  . The quantitative results lead to a proposed hypothesis for the ICL working mechanism:    $\\mathcal{H}_{1}$  : In shallow layers, label words aggregate information from demonstration examples to form semantic representations for later computations.    $\\mathcal{H}_{2}$  : In deep layers, the model makes predictions by extracting information from label words. The validation for these hypotheses is presented in   $\\S~2.2$   and   $\\S~2.3$  , respectively. \n2.1 Hypothesis Motivated by Saliency Scores \nThis section aims to discover the inherent patterns in the attention interaction between tokens for a GPT model. The saliency technique ( Simonyan et al. ,  2013 ), a common interpretation tool, is em- ployed for highlighting critical token interactions. Following common practice, we use the Taylor expansion ( Michel et al. ,  2019 ) to calculate the saliency score for each element of the attention matrix: \n\n$$\nI_{l}=\\left|\\sum_{h}A_{h,l}\\odot\\frac{\\partial\\mathcal{L}(\\boldsymbol{x})}{\\partial A_{h,l}}\\right|.\n$$\n "}
{"page": 2, "image_path": "doc_images/2305.14160v4_2.jpg", "ocr_text": "Here, Ap, is the value of the attention matrix of\nthe h-th attention head in the /-th layer, x is the\ninput, and L(x) is the loss function of the task,\ne.g., the cross-entropy objective for a classification\nproblem. We average all attention heads to obtain\nthe saliency matrix I, for the I-th layer? I,(i, )\nrepresents the significance of the information flow\nfrom the j-th word to the i-th word for ICL. By\nobserving J), we can get an intuitive impression that\nas the layer goes deeper, demonstration label words\nwill become more dominant for the prediction, as\ndepicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon,\nwe propose three quantitative metrics based on Jj.\nOur focus lies in three components: (i) the label\nwords, such as “Negative” and “Positive” in Fig-\nure 2, denoted as pj, ...,pc, where C’ represents\nthe total number of label words:/ (ii) the target po-\nsition, where the model generates prediction labels\n(ie., the final token in the input), which we denote\nas q; and (iii) the text part, i.e., the tokens before\nlabel words in the demonstration.\n\nThe definitions of the three quantitative metrics\nfollow below.\n\nSwp, the mean significance of information flow\nfrom the text part to label words:\n\nLGieCup UGS)\n|Cup| , (2)\nCup = {(Pk,J) 2k € [1,C],9 < pe}.\n\nSup =\n\nS>pq, the mean significance of information flow\nfrom label words to the target position:\n\n6, = ede HED)\nm Coal ; (3)\nCoa = {(4, Pk) 2 k € [1, C]}-\n\nSww, the mean significance of the information\nflow amongst all words, excluding influences\nrepresented by S,,,, and S;,, :\n\nS. Ga eCuw Ti, j)\nwe [Cowl , (4)\nCow ={(4,5) 25 < 1} — Cup — Cpa.\n\nSwps Spq, and Siw help assess different informa-\ntion flows in the model. S,,, indicates the intensity\nof information aggregation onto label words. A\n\n3 Another choice is to use J) = dX, |Ans © an , Which\nraises quite similar results.\n\n‘In this study, the term ‘label words’ is approximately\nequal to ’label tokens’. The only deviation is the ’Abbrevia-\ntion’ in the TREC dataset, where we use the first subword in\n\nexperiments, following Zhao et al. (2021).\n\nhigh S,,, demonstrates a strong information extrac-\ntion from label words for final decision-making.\nSww assesses average information flow among\nwords, serving as a benchmark to gauge the in-\ntensity of the patterns identified by Si.) and Spq.\n\nExperimental Settings We choose GPT2-XL\nfrom the GPT series (Radford et al., 2019) as our\nprimary model for investigation, due to its moder-\nate model size (of 1.5B parameters) that is suitable\nfor our hardware resource and its decent ICL perfor-\nmance (Dai et al., 2022). For datasets, we use Stan-\nord Sentiment Treebank Binary (SST-2) (Socher\net al., 2013) for sentiment analysis, Text REtrieval\nConference Question Classification (TREC) (Li\nand Roth, 2002; Hovy et al., 2001) for question\nype classification, AG’s news topic classification\ndataset (AGNews) (Zhang et al., 2015) for topic\nclassification, and EmoContext (EmoC) (Chatterjee\net al., 2019) for emotion classification. Templates\n‘or constructing demonstrations are provided in\nAppendix A. 1000 examples are sampled from the\nest set for evaluation, with one demonstration per\nclass sampled from the training set. Experiments\nwith more demonstrations yield similar outcomes\n(refer to Appendix F.1 for details). Results reflect\naverages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1)\nin shallow layers, S,,,, the significance of the infor-\nmation flow from label words to targeted positions,\nis low, while S,,,, the information flow from the\ntext part to label words is high; (2) in deep layers,\nS'pq, the importance of information flow from label\nwords to the targeted position becomes the dom-\ninant one. Notably, Sp and S,,, usually surpass\nSww, Suggesting that interactions involving label\nwords outweigh others.\n\nProposed Hypothesis Based on this, we propose\nthe hypothesis that label words function as anchors\nin the ICL information flow. In shallow layers,\nlabel words gather information from demonstra-\ntion examples to form semantic representations for\ndeeper layers, while in deep layers, the model ex-\ntracts the information from label words to form the\nfinal prediction. Figure 2 gives an illustration for\nour hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis’ first compo-\nnent. We assume that the information aggregation\nin ICL relies on the information flow from the text\n", "vlm_text": "Here,    $A_{h,l}$   is the value of the attention matrix of the  $h$  -th attention head in the    $l$  -th layer,    $x$   is the input, and    $\\mathcal{L}(\\boldsymbol{x})$   is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix    $I_{l}$   for the    $l.$  -th layer.    $I_{l}(i,j)$  represents the significance of the information flow from the    $j$  -th word to the    $i$  -th word for ICL. By observing  $I_{l}$  , we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure  1 . \nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on    $I_{l}$  . Our focus lies in three components: (i) the label words, such as “Negative” and “Positive” in Fig- ure  2 , denoted as    $p_{1},...,p_{C}$  , where    $C$   represents the total number of label words; 4   (ii) the target po- sition, where the model generates prediction labels (i.e., the final token in the input), which we denote as    $q$  ; and (iii) the text part, i.e., the tokens before label words in the demonstration. \nThe definitions of the three quantitative metrics follow below. \n $S_{w p}$  , the mean significance of information flow from the text part to label words: \n\n$$\n\\begin{array}{l}{S_{w p}=\\displaystyle\\frac{\\sum_{(i,j)\\in C_{w p}}I_{l}\\big(i,j\\big)}{\\vert C_{w p}\\vert},}\\\\ {C_{w p}=\\{(p_{k},j):k\\in[1,C],j<p_{k}\\}.}\\end{array}\n$$\n \n $S_{p q}$  , the mean significance of information flow from label words to the target position: \n\n$$\n\\begin{array}{l l}{S_{p q}=\\frac{\\sum_{(i,j)\\in C_{p q}}I_{l}(i,j)}{|C_{p q}|},}\\\\ {C_{p q}=\\{(q,p_{k}):k\\in[1,C]\\}.}\\end{array}\n$$\n \n $S_{w w}$  , the mean significance of the information flow amongst all words, excluding influences represented by  $S_{w p}$   and    $S_{p q}$   : \n\n$$\n\\begin{array}{l}{S_{w w}=\\!\\frac{\\sum_{(i,j)\\in C_{w w}}I_{l}(i,j)}{|C_{w w}|},}\\\\ {C_{w w}=\\!\\{(i,j):j<i\\}-C_{w p}-C_{p q}.}\\end{array}\n$$\n \n $S_{w p},S_{p q}$  , and    $S_{w w}$   help assess different informa- tion flows in the model.    $S_{w p}$   indicates the intensity of information aggregation onto label words. A high    $S_{p q}$   demonstrates a strong information extrac- tion from label words for final decision-making.  $S_{w w}$   assesses average information flow among words, serving as a benchmark to gauge the in- tensity of the patterns identified by    $S_{w p}$   and    $S_{p q}$  . \n\nExperimental Settings We choose GPT2-XL from the GPT series ( Radford et al. ,  2019 ) as our primary model for investigation, due to its moder- ate model size (of 1.5B parameters) that is suitable for our hardware resource and its decent ICL perfor- mance ( Dai et al. ,  2022 ). For datasets, we use Stan- ford Sentiment Treebank Binary (SST-2) ( Socher et al. ,  2013 ) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) ( Li and Roth ,  2002 ;  Hovy et al. ,  2001 ) for question type classification, AG’s news topic classification dataset (AGNews) ( Zhang et al. ,  2015 ) for topic classification, and EmoContext (EmoC) ( Chatterjee et al. ,  2019 ) for emotion classification. Templates for constructing demonstrations are provided in Appendix  A .  1000  examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix  F.1  for details). Results reflect averages from five random seeds. \nResults and Analysis Figure  3  reveals that: (1) in shallow layers,    $S_{p q}$  , the significance of the infor- mation flow from label words to targeted positions, is low, while    $S_{w p}$  , the information flow from the text part to label words is high; (2) in deep layers,  $S_{p q}$  , the importance of information flow from label words to the targeted position becomes the dom- inant one. Notably,    $S_{p q}$   and    $S_{w p}$   usually surpass  $S_{w w}$  , suggesting that interactions involving label words outweigh others. \nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstra- tion examples to form semantic representations for deeper layers, while in deep layers, the model ex- tracts the information from label words to form the final prediction. Figure  2  gives an illustration for our hypothesis. \n2.2 Shallow Layers: Information Aggregation \nIn this part, we validate our hypothesis’ first compo- nent. We assume that the information aggregation in ICL relies on the information flow from the text "}
{"page": 3, "image_path": "doc_images/2305.14160v4_3.jpg", "ocr_text": "1.0\n0.8\n0.6 — Swp\n0.4 — Sww\n0.2\n\n0.0\n\n°\n\n10 20 30 40\nLayer\n\n(a) Results on the SST-2 dataset\n\n1.0\n0.8\n0.6 — Sup\nun — $pq\n0.4 — Sww\n0.2\n0.0\n0 10 20 30 40\n\nLayer\n\n(b) Results on the AGNews dataset\n\nFigure 3: Relative sizes of Sy,p, Spg, and Syw in dif-\nferent layers on SST-2 and AGNews. Results of other\ndatasets can be found in Appendix B. Initially, S,,,, oc-\ncupies a significant proportion, but it gradually decays\nover layers, while S;,, becomes the dominant one.\n\npart to label tokens, which is facilitated by the trans-\nformer’s attention mechanism. By manipulating\nthe attention layer in the model to block this flow\nand examining the model behavior change, we val-\nidate the existence of the information aggregation\nprocess and its contribution to the final prediction.\n\nExperimental Settings We retain the same test\nsample size of 1000 inputs as § 2.1. We use the\nsame demonstration for a single random seed. To\nfurther validate our findings on larger models, we\nincorporate GPT-J (6B) (Wang and Komatsuzaki,\n2021) in experiments, which exceeds GPT2-XL in\nmodel size and capacity.\n\nImplementation Details To block the informa-\ntion flow to label words, we isolate label words\nby manipulating the attention matrix A. Specifi-\ncally, we set A;(p,i)(i < p) to 0 in the attention\nmatrix A; of the /-th layer, where p represents label\n\n=== No Isolation\njm Label Words (First)\n\nj= Label Words (Last)\njm Random (First)\n\nmm Random (Last)\n\nLabel Loyalty\n(GPT2-XL)\n\nWord Loyalty Label Loyalty\n(GPT2-XL) (GPT-))\n\nWord Loyalty\n(GPT-J)\n\nFigure 4: The impact of isolating label words versus\nrandomly isolating non-label words within the first or\nlast 5 layers. Isolating label words within the first 5 lay-\ners exerts the most substantial impact, highlighting the\nimportance of shallow-layer information aggregation\nvia label words.\n\nwords and i represents preceding words. Conse-\nquently, in the /-th layer, label words cannot access\ninformation from the prior demonstration text.\n\nMetrics We use the following metrics to assess\nthe impact of blocking information flow from the\ntext part to label tokens: (1) Label Loyalty: mea-\nsures the consistency of output labels with and\nwithout isolation. (2) Word Loyalty: employs the\nJaccard similarity to compare the top-5 predicted\nwords with and without isolation, capturing more\nsubtle model output alterations (See Appendix C\nfor details). Low loyalty indicates a profound im-\npact of isolation on model predictions.\n\nResults and Analysis Figure 4 illustrates a no-\ntable influence on the model’s behavior when label\nwords are isolated within the first 5 layers. Yet, this\ninfluence becomes inconsequential within the last\n5 layers, or when random non-label words are used.\nThis observation underlines the fundamental impor-\ntance of shallow-layer information aggregation via\nlabel words in ICL. It also emphasizes the superi-\nority of label words over non-label words. Further\ntests with variable numbers of layers reaffirm these\nfindings (Appendix D). Moreover, similar results\nwere obtained when testing ICL with semantically\nunrelated labels (refer to Appendix F.2).\n\n2.3. Deep Layers: Information Extraction\n\nWe proceed to validate the latter part of our hy-\npothesis that the model extracts information from\nlabel words to form the final prediction. We de-\nnote the sum of the attention matrices in the /-th\n", "vlm_text": "The image is a line graph showing the results on the SST-2 dataset. It displays three different lines, each representing a variable, against the number of layers (from 0 to 50) on the x-axis. The y-axis represents the values of \"S,\" ranging from 0.0 to 1.0.\n\n- The blue line represents \\( S_{wp} \\).\n- The orange line represents \\( S_{pq} \\).\n- The green line represents \\( S_{ww} \\).\n\nThe graph includes a legend indicating which color corresponds to each variable. The general trend shows that \\( S_{pq} \\) increases significantly as the layer number increases, while \\( S_{wp} \\) decreases, and \\( S_{ww} \\) remains relatively stable around a lower value.\nThe image is a line graph showing results on the AGNews dataset. It has three lines representing different metrics or scores measured across layers of a model:\n\n- The blue line represents \\( S_{wp} \\).\n- The orange line represents \\( S_{pq} \\).\n- The green line represents \\( S_{ww} \\).\n\nThe x-axis is labeled \"Layer,\" and the y-axis is labeled \"S,\" which might represent a score or similarity measure. The graph shows how these values change across different layers of a model. The orange line \\( S_{pq} \\) reaches a value of 1 early on and remains stable, while the blue \\( S_{wp} \\) and green \\( S_{ww} \\) lines have fluctuations and generally lower values.\nFigure 3: Relative sizes of    $S_{w p}$  ,  $S_{p q}$  , and  $S_{w w}$   in dif- ferent layers on SST-2 and AGNews. Results of other datasets can be found in Appendix  B . Initially,    $S_{w p}$   oc- cupies a significant proportion, but it gradually decays over layers, while    $S_{p q}$   becomes the dominant one. \npart to label tokens, which is facilitated by the trans- former’s attention mechanism. By manipulating the attention layer in the model to block this flow and examining the model behavior change, we val- idate the existence of the information aggregation process and its contribution to the final prediction. \nExperimental Settings We retain the same test sample size of 1000 inputs as   $\\S~2.1$  . We use the same demonstration for a single random seed. To further validate our findings on larger models, we incorporate GPT-J (6B) ( Wang and Komatsu zak i , 2021 ) in experiments, which exceeds GPT2-XL in model size and capacity. \nImplementation Details To block the informa- tion flow to label words, we isolate label words by manipulating the attention matrix    $A$  . Specifi- cally, we set    $A_{l}(p,i)(i<p)$   to 0 in the attention matrix    $A_{l}$   of the    $l$  -th layer, where  $p$   represents label \nThe image is a bar graph illustrating the impact of isolating label words versus randomly isolating non-label words within the first or last five layers of a model. The x-axis represents different conditions: \"Label Loyalty (GPT2-XL),\" \"Word Loyalty (GPT2-XL),\" \"Label Loyalty (GPT-J),\" and \"Word Loyalty (GPT-J).\" The y-axis shows \"Loyalty,\" presumably a measure of performance.\n\nThere are four types of isolation labeled in the legend:\n\n- **No Isolation** (dashed green line)\n- **Label Words (Last)** (orange)\n- **Label Words (First)** (blue)\n- **Random (First)** (pink)\n- **Random (Last)** (purple)\n\nKey insights include that isolating label words within the first five layers shows a substantial impact, emphasizing the significance of shallow-layer information aggregation through label words.\nwords and  $i$   represents preceding words. Conse- quently, in the  $l$  -th layer, label words cannot access information from the prior demonstration text. \nMetrics We use the following metrics to assess the impact of blocking information flow from the text part to label tokens:  (1) Label Loyalty:  mea- sures the consistency of output labels with and without isolation.  (2) Word Loyalty:  employs the Jaccard similarity to compare the top-5 predicted words with and without isolation, capturing more subtle model output alterations (See Appendix  C for details). Low loyalty indicates a profound im- pact of isolation on model predictions. \nResults and Analysis Figure  4  illustrates a no- table influence on the model’s behavior when label words are isolated within the first 5 layers. Yet, this influence becomes inconsequential within the last 5 layers, or when random non-label words are used. This observation underlines the fundamental impor- tance of shallow-layer information aggregation via label words in ICL. It also emphasizes the superi- ority of label words over non-label words. Further tests with variable numbers of layers reaffirm these findings (Appendix  D ). Moreover, similar results were obtained when testing ICL with semantically unrelated labels (refer to Appendix  F.2 ). \n2.3 Deep Layers: Information Extraction \nWe proceed to validate the latter part of our hy- pothesis that the model extracts information from label words to form the final prediction. We de- note the sum of the attention matrices in the    $l$  -th layer as    $A_{l}$  .   In deeper layers, we find a strong correlation between the attention distributions on the label words of the target position, represented as    $(A_{l}(q,p_{1}),...,A_{l}(q,p_{C}))$  , and the model’s final prediction, affirming our hypothesis. The experi- mental setup mirrors that discussed in  $\\S~2.2$  . "}
{"page": 4, "image_path": "doc_images/2305.14160v4_4.jpg", "ocr_text": "layer as A;.> In deeper layers, we find a strong\ncorrelation between the attention distributions on\nthe label words of the target position, represented\nas (A; (q, p1), ---, Ar(q, pc)), and the model’s final\nprediction, affirming our hypothesis. The experi-\nmental setup mirrors that discussed in § 2.2.\n\n2.3.1 Experiments\n\nWe utilize the AUC-ROC score to quantify the cor-\nrelation between A;(q, pi) and model prediction,\nwhich we denote as AUCROC; for the /-th layer.\nWe prefer the AUC-ROC metric due to two pri-\nmary reasons: (1) A;(q,p;) might differ from the\nprobability of the model outputting label i by a\nconstant factor. As Kobayashi et al. (2020) points\nout, attention should be multiplied by the norm of\nthe key vector to yield ’more interpretable atten-\ntion’. The AUC-ROC metric can implicitly account\nfor these factors, thus allowing us to uncover the\ncorrelation more effectively. (2) The proportion\nof different labels output by the model may be un-\nbalanced. Using the AUC-ROC metric can help\nmitigate this issue, reducing disturbances caused\nby class imbalance.\n\nConsidering the residual mechanism of trans-\nformers, we can view each layer’s hidden state as\nthe cumulative effect of all prior layer calculations.\nTo quantify the accumulated contribution of the\nfirst | layers to model prediction, we introduce R;:\n\n_, (AUCROC; — 0.5)\n\nk= Sa\n3, (AUCROC; — 0.5)\n\n(5)\nThis measure tracks the positive contribution above\na baseline AUC-ROC threshold of 0.5. The value\nof R; signifies the proportional contribution of the\nfirst | layers to the model prediction.\n\n2.3.2 Results and Analysis\n\nFigures 5a and 5b delineate correlation metrics for\nGPT2-XL and GPT-J, averaged across four datasets.\nThe AUCROC; for deep layers approaches 0.8, il-\nlustrating a strong correlation between the attention\ndistributions on label words of the target position\nand the model’s final prediction. Moreover, shal-\nlow layers show negligible cumulative contribu-\ntions (R), with a significant increase in middle and\ndeep layers. These results signify the crucial role\nof deep layers for final prediction, validating that\nthe model extracts information from label words in\ndeep layers to form the final prediction.\n\nHere we sum up the attention matrices of all attention\nheads in the /th layer for convenience of analysis.\n\n0.85] --- AUCROC, 1.0\n\n0.80\n0.8\n0.75\n0.70 0.6\nfo}\n5 0.65\n0.4\n= 0.60\n\n0.55 02\n\n0.50\n0.0\n\n0.45\n\n0 10 20 30 40 50\nLayers\n\n(a) GPT2-XL (total 48 layers).\n\n0.9 ri 10\n\n--- AUCROC,\n\n08 0.8\n\n0.6\n\nAUCROC;\n°\n\n0.4\n0.6\n0.2\n\n0.0\n\n0 5 10 15 20 25\nLayers\n\n(b) GPT-J (total 28 layers).\n\nFigure 5: AUCROC; and R; of each layer in GPT mod-\nels. The result is averaged over SST-2, TREC, AGNews,\nand Emoc. AUCROC; reaches 0.8 in deep layers, and\nR; increases mainly in the middle and later layers.\n\n2.4 Discussion of Our Hypothesis\n\nIn § 2.2, we have affirmed that the model’s shallow\nlayers assemble information from demonstrations\nvia label words to form semantic representations.\nIn § 2.3, we verify that the aforementioned aggre-\ngated information on label words is then extracted\nto form the final prediction in the deep layers. Rec-\nognizing the crucial function of label words in this\nprocess, we have introduced the term “Anchors”\nto denote them. Given the considerable role these\n“anchors” fulfill, we find it intuitive to design ICL\nimprovements based on them, as elaborated in § 3.\n\n3 Applications of Our Anchor-Based\nUnderstanding\n\nWith insights from the validated hypothesis, we\npropose strategies to boost ICL’s accuracy and in-\nference speed. We propose an anchor re-weighting\nmethod in § 3.1 to adjust the demonstrations’ contri-\nbutions and improve accuracy. In § 3.2, we explore\na context compression technique that reduces origi-\nnal demonstrations to anchor hidden states to speed\nup ICL inference. Besides, in § 3.3, we utilize an-\nchor distances to perform an analysis to understand\n", "vlm_text": "\n2.3.1 Experiments \nWe utilize the AUC-ROC score to quantify the cor- relation between    $A_{l}(q,p_{i})$   and model prediction, which we denote as    $\\mathrm{AUCRO C}_{l}$   for the    $l$  -th layer. We prefer the AUC-ROC metric due to two pri- mary reasons: (1)    $A_{l}(q,p_{i})$   might differ from the probability of the model outputting label    $i$   by a constant factor. As  Kobayashi et al.  ( 2020 ) points out, attention should be multiplied by the norm of the key vector to yield ’more interpret able atten- tion’. The AUC-ROC metric can implicitly account for these factors, thus allowing us to uncover the correlation more effectively. (2) The proportion of different labels output by the model may be un- balanced. Using the AUC-ROC metric can help mitigate this issue, reducing disturbances caused by class imbalance. \nConsidering the residual mechanism of trans- formers, we can view each layer’s hidden state as the cumulative effect of all prior layer calculations. To quantify the accumulated contribution of the first  $l$   layers to model prediction, we introduce  $R_{l}$  : \n\n$$\nR_{l}=\\frac{\\sum_{i=1}^{l}(\\mathrm{ALCROC}_{i}-0.5)}{\\sum_{i=1}^{N}(\\mathrm{ALCROC}_{i}-0.5)}.\n$$\n \nThis measure tracks the positive contribution above a baseline AUC-ROC threshold of 0.5. The value of  $R_{l}$   signifies the proportional contribution of the first  $l$   layers to the model prediction. \n2.3.2 Results and Analysis \nFigures  5a  and  5b  delineate correlation metrics for GPT2-XL and GPT-J, averaged across four datasets. The    $\\mathrm{AUCRO C}_{l}$   for deep layers approaches  0 . 8 , il- lustrating a strong correlation between the attention distributions on label words of the target position and the model’s final prediction. Moreover, shal- low layers show negligible cumulative contribu- tions   $(R_{l})$  , with a significant increase in middle and deep layers. These results signify the crucial role of deep layers for final prediction, validating that the model extracts information from label words in deep layers to form the final prediction. \nThe image contains two plots comparing the performance metrics $\\mathrm{AUCRO C}_{l}$ and $R_{l}$ across different layers of GPT models.\n\n1. **Top Plot (a) GPT2-XL:**\n   - Shows results for GPT2-XL with a total of 48 layers.\n   - $\\mathrm{AUCRO C}_{l}$ (blue dashed line) measures a value on the left y-axis and generally increases, reaching around 0.85 in the deeper layers.\n   - $R_{l}$ (solid red line) measures a value on the right y-axis and mostly increases from the middle to later layers.\n\n2. **Bottom Plot (b) GPT-J:**\n   - Shows results for GPT-J with a total of 28 layers.\n   - $\\mathrm{AUCRO C}_{l}$ (blue dashed line) measures a value on the left y-axis and increases, reaching around 0.9 in the deeper layers.\n   - $R_{l}$ (solid red line) follows a similar trend as in GPT2-XL, increasing in the middle and later layers.\n\nBoth plots suggest that as layers deepen in the models, the performance metrics tend to improve.\n2.4 Discussion of Our Hypothesis \nIn  $\\S~2.2$  , we have affirmed that the model’s shallow layers assemble information from demonstrations via label words to form semantic representations. In   $\\S~2.3$  , we verify that the aforementioned aggre- gated information on label words is then extracted to form the final prediction in the deep layers. Rec- ognizing the crucial function of label words in this process, we have introduced the term “Anchors” to denote them. Given the considerable role these “anchors” fulfill, we find it intuitive to design ICL improvements based on them, as elaborated in   $\\S\\ 3$  . \n3 Applications of Our Anchor-Based Understanding \nWith insights from the validated hypothesis, we propose strategies to boost ICL’s accuracy and in- ference speed. We propose an anchor re-weighting method in  $\\S\\,3.1$   to adjust the demonstrations’ contri- butions and improve accuracy. In   $\\S\\ 3.2$  , we explore a context compression technique that reduces origi- nal demonstrations to anchor hidden states to speed up ICL inference. Besides, in  $\\S\\ 3.3$  , we utilize an- chor distances to perform an analysis to understand the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements. "}
{"page": 5, "image_path": "doc_images/2305.14160v4_5.jpg", "ocr_text": "the errors ICL made in real-world scenarios. These\napproaches corroborate our hypothesis, pointing to\npotential paths for future ICL enhancements.\n\n3.1 Anchor Re-weighting\n\nBased on our analysis in § 2, we draw parallels\nbetween ICL and logistic regression and propose\nan approach to improve ICL’s accuracy by re-\nweighting label anchors.\n\n3.1.1 Method\n\n§ 2.3 illustrates a strong correlation between the\nmodel’s output category and the attention distri-\nbution (A (q,p1),..-, A(¢,pc)) on label words\nPi,---,po of the target position gq in deep layers.\nWe can view the attention module as a classifier f,\n\nPr¢(Y = i|X = 2)\n=A(q, pi)\n__ xp(dakp,/VD\n>, exp(agk? /Va)\nBy setting q,/Vd = X and kp, — ky, = (;, we\ndeduce:\nPre (Y =i|X = 2)\n\n— Ax\nPr, (VY =O|x =a) 8 *\n\nlog\nThis approximates a logistic regression model\nwhere:\nPr¢(Y = i|X = 2)\n\n_ gi T\nPry(Y =O|X =a) = Bo + B; x. (8)\n\nlog\nIn this equation, 3 and 3 are parameters that can\nbe learned, while x is the input feature.\n\nInspired by the similarity between ICL and lo-\ngistic regression, we’ve incorporated a learnable\nBi into Eq. (7), which is equivalent to adjusting the\nattention weights A(q, pi):\n\nA(q, pi) = exp(8)) A(a Pi) (9)\n\nEach (3 is a learnable parameter, set uniquely for\ndifferent attention heads and layers. Refer to Ap-\npendix G for more details.\n\nTo train the re-weighting vector 3 = { Bit, we\nutilize an auxiliary training set (X train, Y train):\nHere, we perform ICL with normal demonstrations\nand optimize @ with respect to the classification\nloss £ on (Xtrain, Y train):\n\nB* =arg min L(X train, Ytrain). (10)\n\nThis approach can be metaphorically described\nas \"re-weighting the anchors,\" leading us to term it\n\nas Anchor Re-weighting. It can also be viewed as\na modification of the demonstration contributions\nsince demonstration information has been incorpo-\nrated into the anchors as suggested by our prior\nanalysis in § 2.2. Additionally, it can be interpreted\nas a unique adapter variant, introducing minimal\nparameters while preserving most of the original\nmodel. However, it is specifically designed based\non our anchor hypothesis and requires fewer pa-\nrameters than traditional adapters.\n\n3.1.2 Experiments\n\nWe choose one sample per class as normal demon-\nstrations and choose four extra samples per class\nto form the auxiliary training set (X train, Y train)-\nThe setup follows § 2.2, with results averaged over\nfive random seeds. Owing to computational con-\nstraints, we employ GPT2-XL for evaluation, ex-\ncluding GPT-J. The parameters {Bit are trained\nusing gradient descent. More details can be found\nin Appendix H.\n\nWe compare Anchoring Re-weighting with two\nbaselines: (1) Vanilla ICL with the same demon-\nstration (1-shot per class) (2) Vanilla ICL, where\nthe auxiliary training set of G is included as demon-\nstrations (5-shot per class) for a fair comparison.\n\n3.1.3 Results\n\nAs Table 1 shows, the proposed anchor re-\nweighting significantly enhances ICL performance,\nparticularly on the SST-2 and EmoC datasets. Be-\nsides, adding more demonstrations for vanilla ICL\nmay not bring a stable accuracy boost due to the po-\ntential noise introduced, as discussed in Zhao et al.\n(2021). Different from vanilla ICL which utilizes\nthe extra examples to form a demonstration, we\ntrain a re-weighting vector G to modulate label an-\nchor contributions. This shortens the input context\nand thus brings (almost) no extra cost to the infer-\nence speed. The consistent improvements of our\nmethod suggest that the re-weighting mechanism\ncould be a better alternative to utilize demonstra-\ntion examples. Furthermore, it reiterates the crucial\nrole that anchors play in ICL.\n\n3.2. Anchor-Only Context Compression\n\nWe further explore a context compression tech-\nnique that reduces the full demonstration to anchor\nhidden states for accelerating ICL inference.\n\n3.2.1 Method\n\nIn § 2.3, we find that the model output heavily re-\nlies on the label words, which collect information\n", "vlm_text": "\n3.1 Anchor Re-weighting \nBased on our analysis in   $\\S~2$  , we draw parallels between ICL and logistic regression and propose an approach to improve ICL’s accuracy by re- weighting label anchors. \n3.1.1 Method \n $\\S~2.3$   illustrates a strong correlation between the model’s output category and the attention distri- bution    $\\left(A\\left(q,p_{1}\\right),\\ldots,A\\left(q,p_{C}\\right)\\right)$   on label words  $p_{1},...,p_{C}$   of the target position    $q$   in deep layers. We can view the attention module as a classifier    $\\pmb{f}$  , \n\n$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}_{\\pmb{f}}(Y=i|X=x)}\\\\ &{{\\approx}A(q,p_{i})}\\\\ &{{=}\\frac{\\exp(\\mathbf{q}_{q}\\mathbf{k}_{p_{i}}^{T}/\\sqrt{d})}{\\sum_{j=1}^{N}\\exp(\\mathbf{q}_{q}\\mathbf{k}_{j}^{T}/\\sqrt{d})}.}\\end{array}\n$$\n \nBy setting  $\\ensuremath{\\mathbf{q}}_{q}/\\sqrt{d}=\\hat{\\ensuremath{\\mathbf{x}}}$   and  $\\mathbf{k}_{p_{i}}-\\mathbf{k}_{p_{C}}=\\beta_{i}$  , we deduce: \n\n$$\n\\log{\\frac{\\operatorname*{Pr}_{f}(Y=i|X=x)}{\\operatorname*{Pr}_{f}(Y=C|X=x)}}={\\boldsymbol{\\beta}}_{i}^{T}{\\hat{\\mathbf{x}}}.\n$$\n \nThis approximates a logistic regression model where: \n\n$$\n\\log\\frac{\\operatorname*{Pr}_{\\boldsymbol{f}}(Y=i|X=x)}{\\operatorname*{Pr}_{\\boldsymbol{f}}(Y=C|X=x)}={\\beta_{0}^{i}}+{\\beta_{i}^{T}}\\mathbf{x}.\n$$\n \nIn this equation,  $\\beta_{0}^{i}$    and  $\\beta_{i}^{T}$    are parameters that can be learned, while  $\\mathbf{x}$   is the input feature. \nInspired by the similarity between ICL and lo- gistic regression, we’ve incorporated a learnable  $\\beta_{0}^{i}$    into Eq. ( 7 ), which is equivalent to adjusting the attention weights  $A(q,p_{i})$  : \n\n$$\n\\hat{A}(q,p_{i})=\\exp(\\beta_{0}^{i})A(q,p_{i})\n$$\n \nEach  $\\beta_{0}^{i}$    is a learnable parameter, set uniquely for different attention heads and layers. Refer to Ap- pendix  G  for more details. \nTo train the re-weighting vector    $\\beta=\\left\\{\\beta_{0}^{i}\\right\\}$  \t , we utilize an auxiliary training set  $(X_{t r a i n},Y_{t r a i n})$  . Here, we perform ICL with normal demonstrations and optimize    $\\beta$   with respect to the classification loss  $\\mathcal{L}$   on    $(X_{t r a i n},Y_{t r a i n})$  : \n\n$$\n\\beta^{\\star}=\\arg\\operatorname*{min}_{\\beta}\\mathcal{L}(X_{t r a i n},Y_{t r a i n}).\n$$\n \nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it as  Anchor Re-weighting . It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorpo- rated into the anchors as suggested by our prior analysis in   $\\S\\ 2.2$  . Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer pa- rameters than traditional adapters. \n\n3.1.2 Experiments \nWe choose one sample per class as normal demon- strations and choose four extra samples per class to form the auxiliary training set    $(\\boldsymbol{X}_{t r a i n},\\boldsymbol{Y}_{t r a i n})$  . The setup follows   $\\S~2.2$  , with results averaged over five random seeds. Owing to computational con- straints, we employ GPT2-XL for evaluation, ex- cluding GPT-J. The parameters  $\\left\\{\\beta_{0}^{i}\\right\\}$  \t are trained using gradient descent. More details can be found in Appendix  H . \nWe compare  Anchoring Re-weighting  with two baselines: (1) Vanilla ICL with the same demon- stration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of  $\\beta$   is included as demon- strations (5-shot per class) for a fair comparison. \n3.1.3 Results \nAs Table  1  shows, the proposed anchor re- weighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Be- sides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the po- tential noise introduced, as discussed in  Zhao et al. ( 2021 ). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector    $\\beta$   to modulate label an- chor contributions. This shortens the input context and thus brings (almost) no extra cost to the infer- ence speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstra- tion examples. Furthermore, it reiterates the crucial role that anchors play in ICL. \n3.2 Anchor-Only Context Compression \nWe further explore a context compression tech- nique that reduces the full demonstration to anchor hidden states for accelerating ICL inference. \n3.2.1 Method \nIn   $\\S~2.3$  , we find that the model output heavily re- lies on the label words, which collect information "}
{"page": 6, "image_path": "doc_images/2305.14160v4_6.jpg", "ocr_text": "Method | SST-2. TREC AGNews EmoC | Average\nVanilla In-Context Learning ( 1-shot per class ) | 61.28 57.56 73.32 15.44 | 51.90\nVanilla In-Context Learning ( 5-shot per class ) | 64.75 60.40 52.52 9.80 46.87\nAnchor Re-weighting (1-shot per class) 90.07 60.92 81.94 41.64 | 68.64\n\nTable 1: The effect after adding parameter (3. For AGNews, due to the length limit, we only use three demonstrations\nper class. Our Anchor Re-weighting method achieves the best performance overall tasks.\n\nfrom the demonstrations. Given the auto-regressive\nnature of GPT-like models, where hidden states\nof tokens depend solely on preceding ones, label\nwords’ information aggregation process is inde-\npendent of subsequent words. This allows for the\ncalculation and caching of the label word hidden\nstates H = {{hi}%_}§, (hij is the /-th layer’s\nhidden state of the i-th label word in the demon-\nstration). By concatenating hi, hey ne at the front\nin each layer during inference, instead of using the\nfull demonstration, we can speed up inference.\n\nIn our preliminary experiments, concatenating\nhidden states of label words alone was inadequate\nfor completing the ICL task.® This might be due\nto the critical role of formatting information in\nhelping the model to determine the output space\nat the target position,’ as highlighted in Min et al.\n(2022b). As a solution, we amalgamate the hidden\nstates of both the formatting and the label words, a\nmethod we’ve termed Hiddenanchor-\n\n3.2.2 Experiments\n\nWe follow the same experimental settings as § 2.2.\nWe compare our Hiddenanchor input compression\nmethod with two equally efficient baselines.\nTextanchor: This method concatenates the format-\nting and label text with the input, as opposed to\nconcatenating the hidden states at each layer.\nHiddenyandom: This approach concatenates the hid-\nden states of formatting and randomly selected non-\nlabel words (equal in number to Hiddenanchor)-\nHidden,;andom-top: To establish a stronger baseline,\nwe randomly select 20 sets of non-label words in\nHiddenyandom and report the one with the highest\nlabel loyalty.\n\nThe Textanchor method is included to demon-\nstrate that the effectiveness of Hiddenanchor is at-\ntributed to the aggregation of information in label\n\nOmitting formatting significantly reduces accuracy, as the\nmodel will favor common tokens like “the” over label words,\nindicating confusion about the expected output type.\n\nHere, “formatting” refers to elements like “Review:” and\n“Sentiment:” in Figure 2.\n\nMethod Label Loyalty Word Loyalty Acc.\nICL (GPT2-XL) 100.00 100.00 51.90\nTexXtanchor 51.05 36.65 38.77\nHiddenyandom 48.96 5.59 39.96\nHiddenyandom-top 57.52 4.49 41.72\nHiddenanchor 79.47 62.17 45.04\nICL (GPT-J) 100.00 100.00 56.82\nTexXtanchor 53.45 43.85 40.83\nHiddetyandom 49.03 2.16 31.51\nHiddenyandom-top 71.10 11.36 52.34\nHiddenanchor 89.06 75.04 55.59\nTable 2: Results of different compression methods on\n\nGPT2-XL and GPT-J (averaged over SST-2, TREC, AG-\nNews, and EmoC). Acc. denotes accuracy. The best\nresults are shown in bold. Our method achieves the best\ncompression performance.\n\nwords, rather than the mere text of label words.\nIf we find that Hiddenanchor Surpasses TeXtanchor IN\nperformance, it solidifies the notion that the ag-\ngregated information within label words carries\nsignificant importance. The Hidden;andom Method\nis introduced to illustrate that anchor hidden states\nencapsulate most of the demonstration information\namong all hidden states.\n\nWe assess all compression methods using the\nlabel loyalty and word loyalty introduced in § 2.2,\nin addition to classification accuracy.\n\n3.2.3. Results\n\nWe can see from Table 2 that the proposed com-\npression method Hiddenanchor achieves the best\nresults among all three compression methods on all\nmetrics and for both models. For example, with the\nGPT-J model, the compression method with anchor\nstates only leads to a 1.5 accuracy drop compared\nto the uncompressed situation, indicating that the\ncompression introduces negligible information loss.\nFurther, we estimate the efficiency improvements\nover the original ICL. As shown in Table 3, the\nspeed-up ratio ranges from 1.1x to 2.9x, as the\nefficiency gain is influenced by the length of the\ndemonstrations. We refer readers to Appendix I for\n\n", "vlm_text": "The table compares the performance of different methods across several datasets and their average performance. Here are the details:\n\n- **Methods Compared:**\n  1. Vanilla In-Context Learning (1-shot per class)\n  2. Vanilla In-Context Learning (5-shot per class)\n  3. Anchor Re-weighting (1-shot per class)\n\n- **Datasets:**\n  - SST-2\n  - TREC\n  - AGNews\n  - EmoC\n\n- **Performance Scores:**\n  - Each method has a performance score for every dataset, and an average score:\n    - **Vanilla In-Context Learning (1-shot per class):** \n      - SST-2: 61.28\n      - TREC: 57.56\n      - AGNews: 73.32\n      - EmoC: 15.44\n      - Average: 51.90\n    - **Vanilla In-Context Learning (5-shot per class):**\n      - SST-2: 64.75\n      - TREC: 60.40\n      - AGNews: 52.52\n      - EmoC: 9.80\n      - Average: 46.87\n    - **Anchor Re-weighting (1-shot per class):**\n      - SST-2: 90.07\n      - TREC: 60.92\n      - AGNews: 81.94\n      - EmoC: 41.64\n      - Average: 68.64\n\nThe Anchor Re-weighting method generally performs better, especially in the SST-2 and AGNews datasets.\nfrom the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words’ information aggregation process is inde- pendent of subsequent words. This allows for the calculation and caching of the label word hidden states    ${\\cal H}\\,=\\,\\{\\{h_{l}^{i}\\}_{i=1}^{C}\\}_{l=1}^{N}$  } }    $(h_{l}^{i}$    is the  $l$  -th layer’s hidden state of the    $i$  -th label word in the demon- stration). By concatenating    $h_{l}^{1},...,h_{l}^{C}$    at the front in each layer during inference, instead of using the full demonstration, we can speed up inference. \nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task.   This might be due to the critical role of formatting information in helping the model to determine the output space at the target position,   as highlighted in  Min et al. ( 2022b ). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we’ve termed  Hidden anchor . \n3.2.2 Experiments \nWe follow the same experimental settings as  $\\S~2.2$  . We compare our Hidden anchor  input compression method with two equally efficient baselines. \n $\\mathbf{Set_{anchor}}$  : This method concatenates the format- ting and label text with the input, as opposed to concatenating the hidden states at each layer. Hidden random : This approach concatenates the hid- den states of formatting and randomly selected non- label words (equal in number to Hidden anchor ). Hidden random-top : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden random  and report the one with the highest label loyalty. \nThe  Text anchor  method is included to demon- strate that the effectiveness of Hidden anchor  is at- tributed to the aggregation of information in label \nThe table presents data comparing different methods or configurations for two models: ICL (GPT2-XL) and ICL (GPT-J). The table is divided into three columns: Label Loyalty, Word Loyalty, and Accuracy (Acc.). Here is a breakdown of the contents:\n\n- **ICL (GPT2-XL)**\n  - Label Loyalty: 100.00\n  - Word Loyalty: 100.00\n  - Accuracy: 51.90\n  \n  For different configurations under GPT2-XL:\n  - Text_anchor: \n    - Label Loyalty: 51.05\n    - Word Loyalty: 36.65\n    - Accuracy: 38.77\n  - Hidden_random: \n    - Label Loyalty: 48.96\n    - Word Loyalty: 5.59\n    - Accuracy: 39.96\n  - Hidden_random-top: \n    - Label Loyalty: 57.52\n    - Word Loyalty: 4.49\n    - Accuracy: 41.72\n  - Hidden_anchor: \n    - Label Loyalty: 79.47\n    - Word Loyalty: 62.17\n    - Accuracy: 45.04\n\n- **ICL (GPT-J)**\n  - Label Loyalty: 100.00\n  - Word Loyalty: 100.00\n  - Accuracy: 56.82\n  \n  For different configurations under GPT-J:\n  - Text_anchor: \n    - Label Loyalty: 53.45\n    - Word Loyalty: 43.85\n    - Accuracy: 40.83\n  - Hidden_random: \n    - Label Loyalty: 49.03\n    - Word Loyalty: 2.16\n    - Accuracy: 31.51\n  - Hidden_random-top: \n    - Label Loyalty: 71.10\n    - Word Loyalty: 11.36\n    - Accuracy: 52.34\n  - Hidden_anchor: \n    - Label Loyalty: 89.06\n    - Word Loyalty: 75.04\n    - Accuracy: 55.59\n\nThis table seems to be measuring how well different methods maintain loyalty to labels and words, as well as their respective accuracies. The \"ICL\" likely refers to \"In-Context Learning,\" and different configurations seem to influence these metrics across the two models.\nwords, rather than the mere text of label words. If we find that Hidden anchor  surpasses Text anchor  in performance, it solidifies the notion that the ag- gregated information within label words carries significant importance. The  Hidden random  method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states. \nWe assess all compression methods using the label loyalty and word loyalty introduced in   $\\S~2.2$  , in addition to classification accuracy. \n3.2.3 Results \nWe can see from Table  2  that the proposed com- pression method  Hidden anchor  achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a  1 . 5  accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table  3 , the speed-up ratio ranges from    $1.1\\times$   to    $2.9\\times$  , as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix  I  for "}
{"page": 7, "image_path": "doc_images/2305.14160v4_7.jpg", "ocr_text": "Model | SST-2 TREC AGNews EmoC\nGPT2-XL | 1.1x 1.5x 2.5x 14x\nGPT-J 1.5x 2.2x 2.9x 1.9x\n\nTable 3: Acceleration ratios of the Hiddenanchor method.\n\na more elaborated analysis of the speed-up ratios.\nBesides, we observe that the acceleration effect is\nmore pronounced in the GPT-J model compared\nto GPT2-XL, demonstrating its great potential to\napply to larger language models.\n\n3.3, Anchor Distances for Error Diagnosis\n\nLastly, we perform an error analysis for ICL by\nexamining the distances between the key vectors\nin the attention module that correspond to the label\nwords.\n\n3.3.1 Method\n\nOur previous analysis in § 2.3 shows a strong cor-\nrelation between the model output and A(q, pi),\nwhich is determined by ak}, as per Eq. 7. Should\nthe key vectors k for label words p; and p, be\nsimilar, A(q,p;) and A(q, px) will also likely be\nsimilar, leading to potential label confusion. Fur-\nthermore, considering the distribution of query vec-\ntors qq, we employ a PCA-like method to extract\nthe components of the key vectors along the direc-\ntions with significant variations in q,, denoted as\nk (see Appendix J for details). We anticipate that\nthe distances between these ks can correspond to\nthe category confusion of the model, thus revealing\none possible origin of ICL errors. Here, we normal-\nize the distances to a scale of 0-1, with 0 indicating\nthe highest degree of category confusion:\n\nky, — kp.\nConfusion)? = lpi = Keyl (11)\nmaxs7t ||Kp, — kp, ||\n\n3.3.2. Experiments\n\nWe utilize the GPT2-XL model and TREC dataset,\nas the model displays varying confusion levels be-\ntween categories on this dataset. We use all 500\nsamples of the TREC test set and use 1 demonstra-\ntion per class for convenience of analysis.\n\nWe calculate the actual model confusion score,\nConfusion;;, between category 7 and category k us-\ning the AUC-ROC metric (detailed in Appendix K).\nWe then compare the predicted confusion score,\nConfusion?*\"\", and the actual confusion score,\nConfusion;;, via heatmaps.\n\n1.0\nAbbreviation\n0.9\n\nEntity\n08\n\nD. ti\nescription 07\n\nPerson 06\nLocation -0.5\n\nNgee 0.83 0.92 0. .! . -0.4\n\nAbbreviation\nEntity\nDescription\nPerson\nLocation\nNumber\n\n(a) Confusion matrix of Confusion?*\".\n\n1.0\nAbbreviation 0.84 1 1 041\n\nEntity 1 jose RE 0.97 0.87 0.9\n1 0.98 0.99 0.97\n08\n\n095098 1 1\n\nDescription\nPerson\n\n-0.7\n\nLocation 0.97 0.99 1\n\nNumber\n\n0.87 0.97\n\n2\ni\na\n\n- 0.6\n\nAbbreviation\nDescription\nPerson\nLocation\nNumber\n\n(b) Confusion matrix of Confusion;;.\n\nFigure 6: Predicted and real confusion matrix on TREC.\nWe set undefined diagonals to 1 for better visualization.\nThe heatmaps display similarity in confusing category\npairs, particularly in lighter-colored blocks.\n\n3.3.3 Results\n\nFigure 6 shows that the proposed approximation\nmetric, Confusion’, can identify the most confus-\ning case (Description-Entity) and performs reason-\nably well for highly confusing categories (Entity-\nAbbreviation, Description-Abbreviation). This\nhigh correlation indicates that ICL makes errors\nin categories with similar label anchors. Overall,\nthis result demonstrates that our anchor-based anal-\nysis framework could serve as an interpretation tool\n\nfor better understanding ICL’s errors.\n\n4 Related Work\n\nThe existing literature on in-context learning analy-\nsis can be broadly divided into two streams, each\nfocusing on different aspects. The first stream ex-\nplores the influencing factors of ICL based on input\nperturbation, such as the order (Min et al., 2022b),\nthe formatting (Yoo et al., 2022; Wei et al., 2022),\nand the selection of the demonstration (Liu et al.,\n2022). Designing proper demonstration construc-\n", "vlm_text": "The table compares performance metrics for two models, GPT2-XL and GPT-J, across four datasets: SST-2, TREC, AGNews, and EmoC. The values are as follows:\n\n- **GPT2-XL:**\n  - SST-2: 1.1×\n  - TREC: 1.5×\n  - AGNews: 2.5×\n  - EmoC: 1.4×\n\n- **GPT-J:**\n  - SST-2: 1.5×\n  - TREC: 2.2×\n  - AGNews: 2.9×\n  - EmoC: 1.9×\n\nThe values seem to indicate a relative performance or improvement factor across these datasets for each model.\nTable 3: Acceleration ratios of the Hidden anchor  method. \na more elaborated analysis of the speed-up ratios. Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL, demonstrating its great potential to apply to larger language models. \n3.3 Anchor Distances for Error Diagnosis \nLastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words. \n3.3.1 Method \nOur previous analysis in  $\\S~2.3$   shows a strong cor- relation between the model output and    $A(q,p_{i})$  , which is determined by  $\\mathbf{q}_{q}\\mathbf{k}_{p_{i}}^{T}$    as per Eq.  7 . Should the key vectors    $\\mathbf{k}$   for label words    $p_{i}$   and    $p_{k}$   be similar,    $A(q,p_{i})$   and    $A(q,p_{k})$   will also likely be similar, leading to potential label confusion. Fur- thermore, considering the distribution of query vec- tors    $\\mathbf{q}_{q}$  , we employ a PCA-like method to extract the components of the key vectors along the direc- tions with significant variations in  $\\mathbf{q}_{q}$  , denoted as  $\\hat{\\mathbf{k}}$   (see Appendix  J  for details). We anticipate that the distances between these  $\\hat{\\mathbf{k}}\\mathbf{s}$  s can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normal- ize the distances to a scale of 0-1, with 0 indicating the highest degree of category confusion: \n\n$$\n\\mathrm{Confinement}_{i j}^{\\mathrm{pred}}=\\frac{\\|\\hat{\\mathbf{k_{pi}}}-\\hat{\\mathbf{k_{pi_{j}}}}\\|}{\\operatorname*{max}_{s\\neq t}\\|\\hat{\\mathbf{k_{p_{s}}}}-\\hat{\\mathbf{k_{pi}}}\\|},\n$$\n \n3.3.2 Experiments \nWe utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels be- tween categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstra- tion per class for convenience of analysis. \nWe calculate the actual model confusion score, Confus  $\\mathbf{ion}_{i j}$  , between category    $i$   and category  $k$   us- ing the AUC-ROC metric (detailed in Appendix  K ). We then compare the predicted confusion score, pred Confusion   , and the actual confusion score, ij Confus  $\\mathbf{ion}_{i j}$  , via heatmaps. \nThe image is a confusion matrix visualizing classification results for different categories: Abbreviation, Entity, Description, Person, Location, and Number. The matrix uses a color gradient scale from light to dark, representing values from 0 to 1. Each cell shows the classification accuracy or correlation between the predicted and true classes, indicating how often the predicted class fits the true label. The diagonal values (from top-left to bottom-right) typically represent the accuracy of each class, with values closer to 1 indicating better performance.\nThe image shows a confusion matrix, which is a table used to evaluate the performance of a classification model. It includes categories like \"Abbreviation,\" \"Entity,\" \"Description,\" \"Person,\" \"Location,\" and \"Number,\" with predicted categories on one axis and actual categories on the other.\n\nThe values range from 0.58 to 1, indicating the model's performance in each category and how many instances from one category were classified into another. The diagonal values represent correct classifications, with values near 1.0 indicating high accuracy. The color intensity corresponds to the numerical values, with darker colors showing higher values. The matrix highlights strengths and weaknesses in classification performance across different categories.\nFigure 6: Predicted and real confusion matrix on TREC. We set undefined diagonals to  1  for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks. \n3.3.3 Results \nFigure  6  shows that the proposed approximation pred metric,  Confusion   , can identify the most confus- ij ing case (Description-Entity) and performs reason- ably well for highly confusing categories (Entity- Abbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based anal- ysis framework could serve as an interpretation tool for better understanding ICL’s errors. \n4 Related Work \nThe existing literature on in-context learning analy- sis can be broadly divided into two streams, each focusing on different aspects. The first stream ex- plores the influencing factors of ICL based on input perturbation, such as the order ( Min et al. ,  2022b ), the formatting ( Yoo et al. ,  2022 ;  Wei et al. ,  2022 ), and the selection of the demonstration ( Liu et al. , 2022 ). Designing proper demonstration construc- tion strategies ( Ye et al. ,  2023 ;  Li et al. ,  2023a ) and calibration techniques ( Zhao et al. ,  2021 ;  Min et al. ,  2022a ) could bring clear boosts to the ICL performance. The second stream investigates the inner working mechanism of ICL through different conceptual lenses, such as making an analogy of ICL to gradient descent ( von Oswald et al. ,  2022 ; Dai et al. ,  2022 ) and viewing the process of ICL as a Bayesian inference ( Xie et al. ,  2022 ). "}
{"page": 8, "image_path": "doc_images/2305.14160v4_8.jpg", "ocr_text": "tion strategies (Ye et al., 2023; Li et al., 2023a)\nand calibration techniques (Zhao et al., 2021; Min\net al., 2022a) could bring clear boosts to the ICL\nperformance. The second stream investigates the\ninner working mechanism of ICL through different\nconceptual lenses, such as making an analogy of\nICL to gradient descent (von Oswald et al., 2022;\nDai et al., 2022) and viewing the process of ICL as\na Bayesian inference (Xie et al., 2022).\n\nIn this paper, we provide a novel perspective by\nexamining the information flow in language mod-\nels to gain an understanding of ICL. Our approach\noffers new insights and demonstrates the potential\nfor leveraging this understanding to improve the ef-\nfectiveness, efficiency, and interpretability of ICL.\n\n5 Conclusion\n\nIn this paper, we propose a hypothesis that label\nwords serve as anchors in in-context learning for\naggregating and distributing the task-relevant infor-\nmation flow. Experimental results with attention\nmanipulation and analysis of predictions correla-\ntion consolidate the hypothesis holds well in GPT2-\nXL and GPT-J models. Inspired by the new under-\nstanding perspective, we propose three practical\napplications. First, an anchor re-weighting method\nis proposed to improve ICL accuracy. Second, we\nexplore a demonstration compression technique to\naccelerate ICL inference. Lastly, we showcase an\nanalysis framework to diagnose ICL errors on a\nreal-world dataset. These promising applications\nagain verify the hypothesis and open up new direc-\ntions for future investigations on ICL.\n\nLimitations\n\nOur study, while providing valuable insights into\nin-context learning (ICL), has several limitations.\nFirstly, our research scope was limited to classi-\nfication tasks and did not delve into the realm of\ngenerative tasks. Additionally, our hypothesis was\nonly examined within conventional ICL paradigms,\nleaving other ICL paradigms such as the chain of\nthought prompting (CoT) (Wei et al., 2022) unex-\nplored. Secondly, due to hardware constraints, we\nmainly investigated models up to a scale of 6 bil-\nlion parameters. Further research that replicates\nour study using larger-scale models would be bene-\nficial in corroborating our findings and refining the\nhypotheses set forth in our investigation.\n\nAcknowledgement\n\nWe thank all reviewers for their thoughtful and in-\nsightful suggestions. This work is supported in part\nby a Tencent Research Grant and National Natural\nScience Foundation of China (No. 62176002). Xu\nSun is the corresponding author.\n\nReferences\n\nEkin Akyiirek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2022. What learn-\ning algorithm is in-context learning? investigations\nwith linear models. ArXiv preprint, abs/2211.15661.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\n\nAnkush Chatterjee, Kedhar Nath Narahari, Meghana\nJoshi, and Puneet Agrawal. 2019. SemEval-2019 task\n3: EmoContext contextual emotion detection in text.\nIn Proceedings of the 13th International Workshop\non Semantic Evaluation, pages 39-48, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui,\nand Furu Wei. 2022. Why can gpt learn in-context?\nlanguage models secretly perform gradient descent\nas meta optimizers. ArXiv preprint, abs/2212.10559.\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2023. A survey for in-context learning.\nArXiv preprint, abs/2301.00234.\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-\nYew Lin, and Deepak Ravichandran. 2001. Toward\nsemantics-based answer pinpointing. In Proceedings\nof the First International Conference on Human Lan-\nguage Technology Research.\n\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\n", "vlm_text": "\nIn this paper, we provide a novel perspective by examining the information flow in language mod- els to gain an understanding of ICL. Our approach offers new insights and demonstrates the potential for leveraging this understanding to improve the ef- fec ti ve ness, efficiency, and interpret ability of ICL. \n5 Conclusion \nIn this paper, we propose a hypothesis that label words serve as anchors in in-context learning for aggregating and distributing the task-relevant infor- mation flow. Experimental results with attention manipulation and analysis of predictions correla- tion consolidate the hypothesis holds well in GPT2- XL and GPT-J models. Inspired by the new under- standing perspective, we propose three practical applications. First, an anchor re-weighting method is proposed to improve ICL accuracy. Second, we explore a demonstration compression technique to accelerate ICL inference. Lastly, we showcase an analysis framework to diagnose ICL errors on a real-world dataset. These promising applications again verify the hypothesis and open up new direc- tions for future investigations on ICL. \nLimitations \nOur study, while providing valuable insights into in-context learning (ICL), has several limitations. Firstly, our research scope was limited to classi- fication tasks and did not delve into the realm of generative tasks. Additionally, our hypothesis was only examined within conventional ICL paradigms, leaving other ICL paradigms such as the chain of thought prompting (CoT) ( Wei et al. ,  2022 ) unex- plored. Secondly, due to hardware constraints, we mainly investigated models up to a scale of 6 bil- lion parameters. Further research that replicates our study using larger-scale models would be bene- ficial in corroborating our findings and refining the hypotheses set forth in our investigation. \nAcknowledgement \nWe thank all reviewers for their thoughtful and in- sightful suggestions. This work is supported in part by a Tencent Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author. \nReferences \nEkin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022.  What learn- ing algorithm is in-context learning? investigations with linear models .  ArXiv preprint , abs/2211.15661. \nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.  Language models are few-shot learners . In  Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . \nAnkush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019.  SemEval-2019 task 3: EmoContext contextual emotion detection in text . In  Proceedings of the 13th International Workshop on Semantic Evaluation , pages 39–48, Minneapo- lis, Minnesota, USA. Association for Computational Linguistics. \nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022.  Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers .  ArXiv preprint , abs/2212.10559. \nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy- ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2023.  A survey for in-context learning . ArXiv preprint , abs/2301.00234. \nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin- Yew Lin, and Deepak Ravi chandra n. 2001.  Toward semantics-based answer pinpointing . In  Proceedings of the First International Conference on Human Lan- guage Technology Research . \nDiederik P. Kingma and Jimmy Ba. 2015.  Adam: A method for stochastic optimization . In  3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings . \nGoro Kobayashi, Tatsuki Kuri bay a shi, Sho Yokoi, and Kentaro Inui. 2020.  Attention is not only a weight: "}
{"page": 9, "image_path": "doc_images/2305.14160v4_9.jpg", "ocr_text": "Analyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057-7075, Online. Association for Computa-\ntional Linguistics.\n\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu,\nYuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng\nQiu. 2023a. Unified demonstration retriever for in-\ncontext learning. ArXiv preprint, abs/2305.04320.\n\nXin Li and Dan Roth. 2002. Learning question clas-\nsifiers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\n\nYingcong Li, Muhammed Emrullah Idiz, Dimitris Pa-\npailiopoulos, and Samet Oymak. 2023b. Transform-\ners as algorithms: Generalization and stability in\nin-context learning.\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100-114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\n\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 14014-14024.\n\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022a. Noisy channel language\nmodel prompting for few-shot text classification. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5316-5330, Dublin, Ireland. As-\nsociation for Computational Linguistics.\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022b. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048-11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2013. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. CoRR, abs/1312.6034.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\n\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631-1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Roziére, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\n\nJohannes von Oswald, Eyvind Niklasson, E. Randazzo,\nJoao Sacramento, Alexander Mordvintsev, Andrey\nZhmoginov, and Max Vladymyrov. 2022. Trans-\nformers learn in-context by gradient descent. ArXiv\npreprint, abs/2212.07677.\n\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https: //github.com/kingoflolz/\nmesh-transformer-jax.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and\nDenny Zhou. 2022. Chain of thought prompting\nelicits reasoning in large language models. ArXiv\npreprint, abs/2201.11903.\n\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al-\nbert Webson, Yifeng Lu, Xinyun Chen, Hanxiao\nLiu, Da Huang, Denny Zhou, and Tengyu Ma. 2023.\nLarger language models do in-context learning dif-\nferently. ArXiv, abs/2303.03846.\n\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\n\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and\nLingpeng Kong. 2023. Compositional exemplars for\nin-context learning. ArXiv preprint, abs/2302.05698.\n\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-\nsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\nand Taeuk Kim. 2022. Ground-truth labels matter: A\ndeeper look into input-label demonstrations. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2422-\n2437, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\n\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 649-657.\n", "vlm_text": "Analyzing transformers with vector norms . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7057–7075, Online. Association for Computa- tional Linguistics. \nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023a.  Unified demonstration retriever for in- context learning .  ArXiv preprint , abs/2305.04320. \nXin Li and Dan Roth. 2002.  Learning question clas- sifiers . In  COLING 2002: The 19th International Conference on Computational Linguistics . \nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Pa- pai lio poul os, and Samet Oymak. 2023b. Transform- ers as algorithms: Generalization and stability in in-context learning. \nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extrac- tion and Integration for Deep Learning Architectures , pages 100–114, Dublin, Ireland and Online. Associa- tion for Computational Linguistics. \nPaul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In  Ad- vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Process- ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pages 14014–14024. \nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Z ett le moyer. 2022a.  Noisy channel language model prompting for few-shot text classification . In Proceedings of the 60th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers) , pages 5316–5330, Dublin, Ireland. As- sociation for Computational Linguistics. \nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle- moyer. 2022b.  Rethinking the role of demonstrations: What makes in-context learning work?  In  Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 11048–11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners.  OpenAI blog , 1(8):9. \nKaren Simonyan, Andrea Vedaldi, and Andrew Zis- serman. 2013. Deep inside convolutional networks: Visual ising image classification models and saliency maps.  CoRR , abs/1312.6034. \nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013.  Recursive deep models for \nsemantic compositional it y over a sentiment treebank . In  Proceedings of the 2013 Conference on Empiri- cal Methods in Natural Language Processing , pages 1631–1642, Seattle, Washington, USA. Association for Computational Linguistics. \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models .  ArXiv , abs/2302.13971. \nJohannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mor dv in tse v, Andrey Zhmoginov, and Max Vladymyrov. 2022. Trans- formers learn in-context by gradient descent .  ArXiv preprint , abs/2212.07677. \nBen Wang and Aran Komatsu zak i. 2021. GPT-J- 6B: A 6 Billion Parameter Auto regressive Lan- guage Model.  https://github.com/kingoflolz/ mesh-transformer-jax . \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022.  Chain of thought prompting elicits reasoning in large language models .  ArXiv preprint , abs/2201.11903. \nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al- bert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning dif- ferently .  ArXiv , abs/2303.03846. \nSang Michael Xie, Aditi Raghu nathan, Percy Liang, and Tengyu Ma. 2022.  An explanation of in-context learning as implicit bayesian inference . In  The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. \nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023.  Compositional exemplars for in-context learning .  ArXiv preprint , abs/2302.05698. \nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun- soo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022.  Ground-truth labels matter: A deeper look into input-label demonstrations . In  Pro- ceedings of the 2022 Conference on Empirical Meth- ods in Natural Language Processing , pages 2422– 2437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. \nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- sification . In  Advances in Neural Information Pro- cessing Systems 28: Annual Conference on Neural In- formation Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , pages 649–657. "}
{"page": 10, "image_path": "doc_images/2305.14160v4_10.jpg", "ocr_text": "Table 4: Demonstration templates and label words. Here\n<S1> represents the demonstration, <S> represents the\ninput to be predicted, and <L> represents the label word\ncorresponding to the demonstration. To save space, we\nonly show one demonstration for each task.\n\nTask Template Label Words\n\nSST-2 Review: <S1> Positive, Negative\nSentiment: <L>\nReview: <S>\nSentiment:\n\nTREC Question: <S1> Abbreviation, Entity\nAnswer Type: <L> Description, Person\nQuestion: <S> Location, Number\nAnswer Type:\n\nAGNews Article: <S1> World, Sports\n\nAnswer: <L> Business, Technology\nArticle: <S>\nAnswer:\n\nEmoC Dialogue: <S1> Others, Happy\n\nEmotion: <L>\nDialogue: <S>\nEmotion:\n\nSad, Angry\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 12697-12706. PMLR.\n\nAppendix\nA Experimental Settings\n\nFor models, we use GPT2-XL (1.5B) (Radford\net al., 2019) and GPT-J (6B) (Wang and Komat-\nsuzaki, 2021) in this paper.\n\nFor datasets, we use a sentiment analysis\ntask, Stanford Sentiment Treebank Binary (SST-\n2) (Socher et al., 2013), a question type classifi-\ncation task, Text REtrieval Conference Question\nClassification (TREC) (Li and Roth, 2002; Hovy\net al., 2001), a topic classification task, AG’s news\ntopic classification dataset (AGNews) (Zhang et al.,\n2015), and an emotion classification task, Emo-\nContext (EmoC) (Chatterjee et al., 2019). The ICL\ntemplates of these tasks are shown in Table 4.\n\nB_ Results of S,,,, 5,4, and S,,, on TREC\nand EmoC\n\nFigure 7 illustrates the relative sizes of Swp, Spq,\nand $1. on TREC and EmoC, mirroring results on\nSST-2 and AGNews. In shallow layers, 5, (the\ninformation flow from the text part to label words)\n\n08\n06\n04\n\n02\n0.0\n\nfy 10 20 30 40\nLayer\n\n(a) Results on the TREC dataset\n\n08\n\n06\n\n04\n\n02\n\n0.0\n\nfy 10 20 30 40\nLayer\n\n(b) Results on the EmoC dataset\n\nFigure 7: Relative size of Si», Spg, and Sy. on TREC\nand EmoC, which is similar to that on SST-2 and AG-\nNews.\n\nis prominent, while Sp, (the information flow from\nlabel words to targeted positions) is less signifi-\ncant. However, in deeper layers, Sg dominates.\nImportantly, S,,, and S,q generally exceed S,,.,,\nindicating that interactions involving label words\nare predominant.\n\nC_ Reason for Using Word Loyalty\nBesides Label Loyalty\n\nLabel loyalty alone may not capture changes in the\nprobability distribution of non-label words or the\nrelative ratio of the probability of the label words\nwithin the entire vocabulary. Word loyalty helps\naddress this limitation, which is shown in Table 5.\n\nD Isolating Different Numbers of Layers\n\nWe study the impact of the numbers of isolated\nlayers, as shown in Figures 8a and 8b. It can be\nfound that isolating shallow layers cause a signifi-\ncant impact, isolating deep layers has a negligible\nimpact on the model, even when the number of\nisolation layers increases. This further illustrates\n", "vlm_text": "Table 4: Demonstration templates and label words. Here  $_{<\\!S1>}$   represents the demonstration,  ${<}S>$   represents the input to be predicted, and  ${<}\\mathrm{L}{>}$   represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task. \nThe table outlines templates and label words for different tasks. Here's a breakdown:\n\n1. **SST-2 Task**\n   - **Template**: \n     - Review: `<S1>`\n     - Sentiment: `<L>`\n     - Review: `<S>`\n     - Sentiment:\n   - **Label Words**: Positive, Negative\n\n2. **TREC Task**\n   - **Template**: \n     - Question: `<S1>`\n     - Answer Type: `<L>`\n     - Question: `<S>`\n     - Answer Type:\n   - **Label Words**: Abbreviation, Entity, Description, Person, Location, Number\n\n3. **AGNews Task**\n   - **Template**: \n     - Article: `<S1>`\n     - Answer: `<L>`\n     - Article: `<S>`\n     - Answer:\n   - **Label Words**: World, Sports, Business, Technology\n\n4. **EmoC Task**\n   - **Template**: \n     - Dialogue: `<S1>`\n     - Emotion: `<L>`\n     - Dialogue: `<S>`\n     - Emotion:\n   - **Label Words**: Others, Happy, Sad, Angry\n\nEach task involves different text inputs (S1, S) and corresponding label outputs (L).\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.  Calibrate before use: Improv- ing few-shot performance of language models . In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir- tual Event , volume 139 of  Proceedings of Machine Learning Research , pages 12697–12706. PMLR. \nAppendix \nA Experimental Settings \nFor models, we use GPT2-XL (1.5B) ( Radford et al. ,  2019 ) and GPT-J (6B) ( Wang and Komat- suzaki ,  2021 ) in this paper. \nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST- 2) ( Socher et al. ,  2013 ), a question type classifi- cation task, Text REtrieval Conference Question Classification (TREC) ( Li and Roth ,  2002 ;  Hovy et al. ,  2001 ), a topic classification task, AG’s news topic classification dataset (AGNews) ( Zhang et al. , 2015 ), and an emotion classification task, Emo- Context (EmoC) ( Chatterjee et al. ,  2019 ). The ICL templates of these tasks are shown in Table  4 . \nB Results of    $S_{w p},S_{p q},$  , and    $S_{w w}$   on TREC and EmoC \nFigure  7  illustrates the relative sizes of    $S_{w p}$  ,    $S_{p q}$  , and  $S_{w w}$   on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers,    $S_{w p}$   (the information flow from the text part to label words) \nThe image contains two line graphs comparing the relative sizes of \\(S_{wp}\\), \\(S_{pq}\\), and \\(S_{ww}\\) across different layers on the TREC and EmoC datasets.\n\n- **Graph (a):** Results on the TREC dataset.\n  - **\\(S_{wp}\\):** Represented by a blue line, showing a generally decreasing trend across layers.\n  - **\\(S_{pq}\\):** Represented by an orange line, starting with fluctuations and stabilizing at high values as layers increase.\n  - **\\(S_{ww}\\):** Represented by a green line, remaining relatively low and stable across layers.\n\n- **Graph (b):** Results on the EmoC dataset.\n  - **\\(S_{wp}\\):** Blue line with a similar decreasing trend.\n  - **\\(S_{pq}\\):** Orange line, showing fluctuations before stabilizing at high values in later layers.\n  - **\\(S_{ww}\\):** Green line, remaining consistently low.\n\nBoth graphs indicate a pattern where \\(S_{pq}\\) dominates in higher layers.\nis prominent, while  $S_{p q}$   (the information flow from label words to targeted positions) is less signifi- cant. However, in deeper layers,    $S_{p q}$   dominates. Importantly,    $S_{w p}$   and    $S_{p q}$   generally exceed    $S_{w w}$  , indicating that interactions involving label words are predominant. \nC Reason for Using Word Loyalty Besides Label Loyalty \nLabel loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table  5 . \nD Isolating Different Numbers of Layers \nWe study the impact of the numbers of isolated layers, as shown in Figures  8a  and  8b . It can be found that isolating shallow layers cause a signifi- cant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates "}
{"page": 11, "image_path": "doc_images/2305.14160v4_11.jpg", "ocr_text": "Isolation Layer Output Label V;, (sorted by probability)\nFirst 5 layers World “\\n”, “ The’, “ Google”,“<lendoftextl>”, “ A”\nNo isolation World “World”, “ Technology”, “ Politics”, “ Israel”, ““ Human”\n\nTable 5: Results on a test sample with the label “World” from AGNews.\n\n0.4\n\nLoyalty\n\n— Label Loyalty (First)\n0.24 --- Label Loyalty (Last)\n— Word Loyalty (First)\n\n=-- Word Loyalty (Last) ————_____\n\nCY) 1 2 3\nIsolation Layer Num\n\n0.0\n\n(a) Effect of different numbers of isolated layers on GPT2-\nXL\n\n1.0\n\n0.8\n\n— Label Loyalty (First)\n\nEy 0.6 --- Label Loyalty (Last)\nze — Word Loyalty (First)\n=== Word Loyalty (Last)\n0.4\n0.2\n\n0 1 2 3\nIsolation Layer Num\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 8: The chart demonstrates variations in label\nloyalty and word loyalty, dependent on whether label\nor non-label words are isolated in various layers. ’First’\nrefers to the first several layers, while ’Last’ to the last\nones. Deep-colored lines represent label word isolation,\nwhereas light colors denote non-label words. Remark-\nably, isolating label words in the shallow layers signif-\nicantly influences the outcome, regardless of whether\nthis is compared to isolation in deep layers or to non-\nlabel word isolation.\n\nthe important role of information aggregation via\nlabel words in the shallow layers.\n\nE Details for the Calculation of\nAUCROC,\n\nSuppose the positions of the label words in the\ninput x are p1,...,pc (without loss of generality,\nwe suppose p; corresponds to the ith class), the\ntargeted position is q, the sum of the attention ma-\n\ntrices of all attention heads at the / layer is A).\nWe postulate that there’s a strong correlation be-\ntween the attention distributions on the label words\nof the target position (A;(q, p1), ..-, Ar(q, pc)) and\nthe model’s final prediction. We use the AUC-\nROC score to quantify this correlation. We re-\ngard (A;(q, pi), .--, Ai(q, pc)) as a classifier’s pre-\ndiction for the model output label (that is, A;(q, pi)\nis equivalent to the probability of model outputting\nlabel 7), and compute the AUC-ROC value of this\nprediction relative to the actual model output. We\ndenote this as AUCROC). For the case with more\ndemonstrations (Appendix F.1), we simply sum up\nall A;(q, p) of the same class.\n\nF Additional Experimental Results\n\nF.1_ Results with More Demonstrations\n\nWe implement our experimental analysis utilizing\ntwo demonstrations per class, resulting in a total\nof 4, 12, 8, and 8 demonstrations respectively for\nSST-2, TREC, AGNews, and EmoC. Our findings,\nas depicted in Figure 9, Figure 10, and Figure 11,\nexhibit a high degree of similarity to the results\nobtained from experiments that employ one demon-\nstration per class.\n\nF2_ Results for In-Context Learning with\nsemantically-unrelated labels\n\nThe applicability of our analytical conclusions to\nICL variants, such as the semantically unrelated la-\nbel ICL (Wei et al., 2023), is an intriguing subject.\nGiven that both GPT2-XL and GPT-J-6B perform\nat levels akin to random guessing in this ICL set-\nting, we chose LLaMA-33B (Touvron et al., 2023)\nand SST-2 for our experiment. We substituted la-\nbels with ’A’/’B’, and adhered to a similar experi-\nmental setup as in sections § 2.2 and § 2.3. How-\never, we applied eight shots per class to facilitate\nthe model in achieving an accuracy of 83.0% on\nSST-2. The outcomes align with those derived in\n§ 2.2 and § 2.3. Figure 12 shows the more pro-\nnounced impact of isolating labels in the shallow\nlayers compared to their isolation in the deep layers\nor the isolation of non-label tokens. Figure 13 con-\n", "vlm_text": "The table appears to compare results from different configurations of an isolation layer in a model, possibly related to natural language processing or text prediction. It consists of three columns:\n\n1. **Isolation Layer**: This column specifies the configuration used.\n   - \"First 5 layers\" indicates that the first five layers of the model were isolated.\n   - \"No isolation\" indicates that no layers were isolated.\n\n2. **Output Label**: This column shows the resulting label from the model.\n   - In both cases (\"First 5 layers\" and \"No isolation\"), the output label is \"World.\"\n\n3. **V₅ (sorted by probability)**: This column lists the most probable next elements predicted by the model, sorted by probability.\n   - For \"First 5 layers\", the predicted elements are: \"\\n\", \"The\", \"Google\", \"<endoftext>\", \"A\".\n   - For \"No isolation\", the predicted elements are: \"World\", \"Technology\", \"Politics\", \"Israel\", \"Human\".\n\nThe table is essentially showing how the isolation of the first 5 layers affects the model's output predictions compared to when there is no isolation. By isolating the first 5 layers, the model's predictions seem to become more generic or placeholder-like, while without isolation, the predictions are more content-specific.\nThis image is a line graph showing the effect of different numbers of isolated layers on GPT-2 XL. The x-axis represents the \"Isolation Layer Num\" ranging from 0 to 3. The y-axis represents \"Loyalty\" scores ranging from 0 to 1. \n\nThere are four lines representing different types of loyalty:\n- Blue solid line: Label Loyalty (First)\n- Blue dashed line: Label Loyalty (Last)\n- Red solid line: Word Loyalty (First)\n- Red dashed line: Word Loyalty (Last)\n\nThe blue lines (Label Loyalty) show a moderate decline as the number of isolation layers increases. The red solid line (Word Loyalty First) shows a steep decline with the increase in isolation layers, while the red dashed line (Word Loyalty Last) remains constant at the top.\nThe image is a line graph showing the effect of different numbers of isolated layers on GPT-J. The x-axis is labeled \"Isolation Layer Num,\" and the y-axis is labeled \"Loyalty.\" \n\nThere are four lines on the graph:\n\n- A solid blue line representing \"Label Loyalty (First)\"\n- A dashed blue line representing \"Label Loyalty (Last)\"\n- A solid red line representing \"Word Loyalty (First)\"\n- A dashed red line representing \"Word Loyalty (Last)\"\n\nThe graph indicates how loyalty metrics change as the number of isolation layers increases from 0 to 3.\nFigure 8: The chart demonstrates variations in label loyalty and word loyalty, dependent on whether label or non-label words are isolated in various layers. ’First’ refers to the first several layers, while ’Last’ to the last ones. Deep-colored lines represent label word isolation, whereas light colors denote non-label words. Remark- ably, isolating label words in the shallow layers signif- icantly influences the outcome, regardless of whether this is compared to isolation in deep layers or to non- label word isolation. \nthe important role of information aggregation via label words in the shallow layers. \nE Details for the Calculation of AUCROC l \nSuppose the positions of the label words in the input    $x$   are  $p_{1},...,p_{C}$   (without loss of generality, we suppose    $p_{i}$   corresponds to the  i th class), the targeted position is    $q$  , the sum of the attention ma- trices of all attention heads at the    $l$   layer is    $A_{l}$  . We postulate that there’s a strong correlation be- tween the attention distributions on the label words of the target position    $(A_{l}(q,p_{1}),...,A_{l}(q,p_{C}))$   and the model’s final prediction. We use the AUC- ROC score to quantify this correlation. We re- gard    $(A_{l}(q,p_{1}),...,A_{l}(q,p_{C}))$   as a classifier’s pre- diction for the model output label (that is,    $A_{l}(q,p_{i})$  is equivalent to the probability of model outputting label    $i$  ), and compute the AUC-ROC value of this prediction relative to the actual model output. We denote this as    $\\mathrm{AUCRO C}_{l}$  . For the case with more demonstrations (Appendix  F.1 ), we simply sum up all    $A_{l}(q,p)$   of the same class. \n\nF Additional Experimental Results \nF.1 Results with More Demonstrations \nWe implement our experimental analysis utilizing two demonstrations per class, resulting in a total of 4, 12, 8, and 8 demonstrations respectively for SST-2, TREC, AGNews, and EmoC. Our findings, as depicted in Figure  9 , Figure  10 , and Figure  11 , exhibit a high degree of similarity to the results obtained from experiments that employ one demon- stration per class. \nF.2 Results for In-Context Learning with semantically-unrelated labels \nThe applicability of our analytical conclusions to ICL variants, such as the semantically unrelated la- bel ICL ( Wei et al. ,  2023 ), is an intriguing subject. Given that both GPT2-XL and GPT-J-6B perform at levels akin to random guessing in this ICL set- ting, we chose LLaMA-33B ( Touvron et al. ,  2023 ) and SST-2 for our experiment. We substituted la- bels with   ${}^{\\prime}\\mathrm{A}^{\\prime}/{}^{\\prime}\\mathrm{B}^{\\prime}$  , and adhered to a similar experi- mental setup as in sections   $\\S~2.2$   and   $\\S~2.3$  . How- ever, we applied eight shots per class to facilitate the model in achieving an accuracy of   $83.0\\%$   on SST-2. The outcomes align with those derived in  $\\S~2.2$   and   $\\S~2.3$  . Figure  12  shows the more pro- nounced impact of isolating labels in the shallow layers compared to their isolation in the deep layers or the isolation of non-label tokens. Figure  13  con- "}
{"page": 12, "image_path": "doc_images/2305.14160v4_12.jpg", "ocr_text": "1.0\n\n08\n\n06\n\n0.2\n\n0.0\n\nfd 10 20 30 40\nLayer\n\n(a) Results on the SST-2 dataset\n\n1.0\n\n08\n\n06\n\n04\n\n0.2\n\n0.0\n\ni 10 20 30 40\nLayer\n\n(b) Results on the TREC dataset\n\n1.0\n\n08\n\n06\n\n0.2\n\n0.0\n\nfd 10 20 30 40\nLayer\n\n(c) Results on the AGNews dataset\n\n1.0\n\n08\n\n06\n\n04\n\n0.2\n\n0.0\n\nfy 10 20 30 40\nLayer\n\n(d) Results on the EmoC dataset\n\nFigure 9: Relative sizes of Swp, Spq, and Sw when\nmore demonstrations are employed.\n\n1.0\n\n0.8\n\nLoyalty\n\n0.4\n\n— Label Loyalty (First)\n0.24 --- Label Loyalty (Last)\n— Word Loyalty (First)\n\n--- Word Loyalty (Last) —————_________\n\n0 1 2 3\nIsolation Layer Num\n\n0.0\n\n(a) Effect of different numbers of isolated layers on GPT2-\nXL\n\n104 ----sessss\n0.8\n206\nre\ng\n3\n0.4\n— Label Loyalty (First)\n--- Label Loyalty (Last)\n0.2 | —— Word Loyalty (First)\n=== Word Loyalty (Last)\n\nt) 1 2 3\nIsolation Layer Num\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 10: Variations in label loyalty and word loyalty\nwhen more demonstrations are employed.\n\nfirmed that the model leverages information from\nanchors in the deeper layers to perform classifica-\ntion.\n\nG_ Implementation of Anchor\nRe-weighting\n\nIn order to implement anchor re-weighting, spe-\ncific adjustments are made in the model’s compu-\ntational process. After calculating the attention\nmatrix Ab of the hth head in the /th layer, we mul-\ntiply each Al(q, p;) by exp(3),1 ;,) before proceed-\ning with further computations. This means that for\neach attention head, we introduce the following\nmodifications:\n\nAttention} (Q, K,V) = ArV,\nT\nAlt = softmax (% ) ;\n\nvd\nch anAr(k, 3), ifk=4,5 =p:\nAtk. 7) = exp(8o,1n) Ar (k, J), , ‘,\ni(k, 9) {arcs otherwise\n\n(12)\n", "vlm_text": "The image is a line graph depicting results on the SST-2 dataset across different layers of a model, with the x-axis labeled \"Layer\" ranging from 0 to 50 and the y-axis labeled \"S\" ranging from 0 to 1. The graph shows three different lines:\n\n1. A blue line labeled \\( S_{wp} \\) which starts with some fluctuations, decreases towards zero around layer 20, and remains close to zero in later layers.\n2. An orange line labeled \\( S_{pq} \\) which shows some initial fluctuations, increases sharply around layer 10, and remains close to 1 past layer 20.\n3. A green line labeled \\( S_{ww} \\) which starts with small fluctuations and remains close to zero across all layers.\n\nThese lines represent some measures or metrics plotted over the layers of a neural network trained or evaluated on the SST-2 (Stanford Sentiment Treebank) dataset.\nThe image is a line graph showing results on the TREC dataset. It has three lines that represent different variables: \n\n- \\(S_{wp}\\) (in blue)\n- \\(S_{pq}\\) (in orange)\n- \\(S_{ww}\\) (in green)\n\nThe x-axis is labeled \"Layer,\" and the y-axis is labeled \"S.\" The graph indicates how these three variables change across different layers, with \\(S_{pq}\\) generally having higher values compared to the other two.\nThe image is a line graph illustrating the results on the AGNews dataset. It plots three different measurements across various layers (0 to 50) on the x-axis. The y-axis represents the values of these measurements (denoted as \\( S \\)) ranging from 0 to 1. \n\nThe three lines are:\n\n- \\( S_{wp} \\): Blue line, showing a peak around layer 5 and then gradually decreasing.\n- \\( S_{pq} \\): Orange line, starting low, peaking sharply around layer 10, and then stabilizing near 1.\n- \\( S_{ww} \\): Green line, maintaining a low and consistent value across all layers.\n\nEach line represents different metrics or measurements across the model's layers.\nThe image is a line graph depicting results on the EmoC dataset. It measures the variable \\( S \\) across different layers, ranging from 0 to 50. There are three different lines represented:\n\n- The blue line (\\( S_{wp} \\)) starts fluctuating around 0.3, increases and decreases erratically, and then stabilizes at a lower level around 0.1 after layer 30.\n- The orange line (\\( S_{pq} \\)) starts around 0.5, exhibits some fluctuations, and then rises sharply to stabilize near 1.0 from around layer 20 onward.\n- The green line (\\( S_{ww} \\)) starts just above 0 and shows a series of minor fluctuations around the 0.1 mark but remains relatively low compared to the other two lines.\n\nThe x-axis represents the layer number, while the y-axis represents the \\( S \\) value.\nFigure 9: Relative sizes of    $S_{w p}$  ,    $S_{p q}$  , and    $S_{w w}$   when more demonstrations are employed. \nThe image is a line graph showing the effect of different numbers of isolated layers on GPT-2 XL. The x-axis represents the \"Isolation Layer Num,\" and the y-axis represents \"Loyalty.\"\n\nThere are four lines on the graph:\n\n1. Blue solid line: Label Loyalty (First)\n2. Blue dashed line: Label Loyalty (Last)\n3. Red solid line: Word Loyalty (First)\n4. Red dashed line: Word Loyalty (Last)\n\nAs the number of isolated layers increases from 0 to 3, Label Loyalty (First and Last) remains relatively stable, while Word Loyalty (First and Last) decreases significantly.\nThe image is a line graph showing the effect of different numbers of isolated layers on GPT-J, focusing on \"Loyalty.\" The x-axis represents the \"Isolation Layer Num,\" ranging from 0 to 3, and the y-axis represents \"Loyalty,\" ranging from 0 to 1.\n\nThere are four lines plotted:\n\n- **Label Loyalty (First)**: Solid blue line.\n- **Label Loyalty (Last)**: Dashed blue line.\n- **Word Loyalty (First)**: Solid red line.\n- **Word Loyalty (Last)**: Dashed red line.\n\nGenerally, the graph shows how these loyalty metrics change with the number of isolated layers. The blue lines appear to slightly decrease, while the solid red line remains constant and the dashed red line slightly decreases.\nFigure 10: Variations in label loyalty and word loyalty when more demonstrations are employed. \nfirmed that the model leverages information from anchors in the deeper layers to perform classifica- tion. \nG Implementation of Anchor Re-weighting \nIn order to implement anchor re-weighting, spe- cific adjustments are made in the model’s compu- tational process. After calculating the attention matrix    $A_{l}^{h}$    of the  h th head in the  l th layer, we mul- tiply each  $A_{l}^{h}(q,p_{i})$   by    $\\exp(\\beta_{0,l h}^{i})$   before proceed- ing with further computations. This means that for each attention head, we introduce the following modifications: \n\n$$\n\\begin{array}{l l}{\\mathrm{Attention}_{l}^{h}(Q,K,V)=\\hat{A}_{l}^{h}V,}\\\\ {A_{l}^{h}=\\mathrm{softmax}\\left(\\displaystyle\\frac{Q K^{T}}{\\sqrt{d}}\\right),}\\\\ {\\hat{A}_{l}^{h}(k,j)=\\left\\{\\exp(\\beta_{0,l h}^{i})A_{l}^{h}(k,j),}&{\\mathrm{if}\\;k=q,j=p_{i}\\;.}\\\\ {A_{l}^{h}(k,j),}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$\n "}
{"page": 13, "image_path": "doc_images/2305.14160v4_13.jpg", "ocr_text": "0.94 --- AUCROC, fh\n\n0.8\n\n0.5\n\n0 10 20 30 40 50\nLayers\n\n(a) GPT2-XL (total 48 layers).\n\n=== AUCROC, rN\n09\n\n0.8\n\n0.2\n\n0.5\n\n0.0\n\n(b) GPT-J (total 28 layers).\n\nFigure 11: AUCROC; and R; of each layer in GPT\nmodels when more demonstrations are employed.\n\n=== No Isolation\njm Label Words (First)\n\nj= Label Words (Last)\njm Random (First)\n\nlm Random (Last)\n\nLoyalty\n\nLabel Loyalty\n(LLaMA-30B)\n\nWord Loyalty\n(LLaMA-30B)\n\nFigure 12: The impact of isolating label words versus\nrandomly isolating non-label words within the first or\nlast 5 layers. Isolating label words within the first 5\nlayers exerts a more pronounced effect, highlighting the\nimportance of shallow-layer information aggregation\nvia label words.\n\nH_ Training Settings of Anchor\nRe-weighting\n\nFor each random seed, we fix the demonstration\nand sample 1000 test samples from the test datasets\nas described in § 2.2. The optimization of parame-\n\n--- AUCROC; mT. 10\n08\n\n0.6\n\nR,\n\n0.4\n\n0.2\n\n0.0\n\n0 10 20 30 40 50 60\n\nFigure 13: AUCROC; and R; of each layer of LLaMA-\n33B on SST-2. Still, deep layers display higher rele-\nvance to model prediction, reinforcing the idea that the\nmodel extracts information from deep-layer anchors for\nclassification.\n\nter vector (3 is carried out using gradient descent,\nspecifically with the Adam optimizer (Kingma and\nBa, 2015). The learning rate is set at 0.01, with\n3, = 0.9 and 62 = 0.999. Due to memory con-\nstraints, we use a batch size of 1. This optimization\nprocess is repeated for 10 epochs. Owing to limi-\ntations in computational resources, we restrict our\nevaluation to the GPT2-XL model and exclude the\nGPT-J model from our assessment.\n\nI The Factor of Laem and L,\n\n| SST-2. TREC AGNews EmoC\n\nGPT2-XL | 1.1x 1.5x 2.5x 1.4x\n\nGPT-J 1.5x 2.2x 2.9x 1.9x\nLaemo 18 61 151 53\nLy 19 7 37 12\n\nTable 6: Acceleration ratios, Lgemo and Lx.\n\nFrom Table 6, we observe a correlation between\nthe acceleration ratios and the ratio of the total\ndemonstration length (Ldemo) to the length of the\ntext predicted (L,.). It suggests that a greater ratio\nof total length to predicted text length may yield a\nhigher acceleration ratio.\n\nIn addition, the table illustrates that datasets with\nlonger demonstration lengths tend to exhibit higher\nacceleration ratios. For instance, the AGNews\ndataset, which has the longest Luemo, presents the\nhighest acceleration ratio among the datasets ana-\nlyzed. These findings could indicate an increased\nefficiency of the Hiddenanchor method in contexts\ninvolving longer demonstration lengths.\n", "vlm_text": "This graph is a plot of two metrics over 48 layers of the GPT2-XL model. \n\n- The x-axis represents the layers (from 0 to 48).\n- The left y-axis is labeled AUROC₁, and the blue dashed line represents this metric. It appears to show the value increasing across the layers, peaking around the 30th to 40th layers, and then it slightly decreases.\n- The right y-axis is labeled R₁, and the red solid line represents this metric. It steadily increases with the number of layers.\n\nThe plot suggests a relationship between the model layers and the metrics AUROC₁ and R₁.\nThe image is a line graph showing two metrics: AUROC and Rₗ, plotted against the layers of GPT-J, which has a total of 28 layers.\n\n- The blue dashed line represents AUROCₗ.\n- The red solid line represents Rₗ.\n\nBoth metrics are plotted against the number of layers, with AUROC ranging from 0.5 to around 1.0 and Rₗ ranging from 0.0 to around 1.0. The graph indicates how these metrics change across the layers.\nFigure 11:    $\\mathrm{AUCRO C}_{l}$   and    $R_{l}$   of each layer in GPT models when more demonstrations are employed. \nThe image is a bar graph comparing the impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers on loyalty for the LLaMA-30B model. \n\nKey observations:\n\n- **No Isolation** is represented by a dashed green line, indicating 100% loyalty.\n- **Label Words (First)**, shown in blue, results in lower loyalty for both label and word loyalty compared to other methods.\n- **Label Words (Last)**, shown in orange, has a high loyalty, similar to the purple bars.\n- **Random (First)**, in red, shows moderate loyalty.\n- **Random (Last)**, shown in purple, achieves high loyalty comparable to isolating label words last.\n\nThe data suggests that isolating label words in the first 5 layers has a significant effect, underlining the importance of early layer processing in the model.\nH Training Settings of Anchor Re-weighting \nFor each random seed, we fix the demonstration and sample  1000  test samples from the test datasets as described in   $\\S~2.2$  . The optimization of parame- \nThe image is a graph that illustrates the performance of the LLaMA-33B model on the SST-2 dataset. The x-axis represents the different layers of the model, while there are two y-axes representing different metrics: AUCROC (in blue) on the left and \\( R_l \\) (in red) on the right.\n\n- The blue dashed line corresponds to the AUCROC metric across the layers, starting relatively low and then demonstrating fluctuations with peaks as high as near 1.0.\n- The red line represents the \\( R_l \\) metric which starts from a lower value and increases steadily across the layers, indicating a trend of increasing relevance to model prediction as the layers deepen. \n\nThe graph caption suggests that deeper layers in the model provide higher relevance for prediction, which aligns with the idea that significant information for classification purposes is extracted from deeper layers in the neural network.\nter vector    $\\beta$   is carried out using gradient descent, specifically with the Adam optimizer ( Kingma and Ba ,  2015 ). The learning rate is set at  0 . 01 , with  $\\beta_{1}=0.9$   and    $\\beta_{2}\\,=\\,0.999$  . Due to memory con- straints, we use a batch size of 1. This optimization process is repeated for  10  epochs. Owing to limi- tations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment. \nThe table presents comparative performance metrics and lengths for two models, GPT2-XL and GPT-J, across four datasets: SST-2, TREC, AGNews, and EmoC. Here's a breakdown:\n\n**Performance Ratios:**\n- **SST-2:** \n  - GPT2-XL: 1.1×\n  - GPT-J: 1.5×\n\n- **TREC:**\n  - GPT2-XL: 1.5×\n  - GPT-J: 2.2×\n\n- **AGNews:**\n  - GPT2-XL: 2.5×\n  - GPT-J: 2.9×\n\n- **EmoC:**\n  - GPT2-XL: 1.4×\n  - GPT-J: 1.9×\n\n**Lengths:**\n- **\\( L_{\\text{demo}} \\):**\n  - SST-2: 18\n  - TREC: 61\n  - AGNews: 151\n  - EmoC: 53\n\n- **\\( L_x \\):**\n  - SST-2: 19\n  - TREC: 7\n  - AGNews: 37\n  - EmoC: 12\n\nThis data likely reflects the efficiency or capability of the models in handling specific tasks or dataset sizes.\nFrom Table  6 , we observe a correlation between the acceleration ratios and the ratio of the total demonstration length   $(L_{\\mathrm{demo}})$   to the length of the text predicted   $(L_{\\mathbf{x}})$  . It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio. \nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest    $L_{\\mathrm{demo}}$  , presents the highest acceleration ratio among the datasets ana- lyzed. These findings could indicate an increased efficiency of the Hidden anchor  method in contexts involving longer demonstration lengths. "}
{"page": 14, "image_path": "doc_images/2305.14160v4_14.jpg", "ocr_text": "J Calculation of k\n\nFor the sampled sequence 21,...,a7 to be pre-\ndicted, we denote the query vectors of the target po-\nsitions as qj, ..., Q@7. We then compute the matrix\nQ= (qi — Y, ..., av — q) by subtracting the mean\nvector, q, from each query vector. Subsequently,\nwe determine the M directions, v1,..., vz, that\ncorrespond to the M largest variation directions for\nthe centralized query vectors qi, ...,q@r. The i*”\ndirection, v;, is chosen to maximize the variance of\nthe projection of the centralized query vectors onto\nit, while also being orthogonal to the previously\nchosen directions, vj, ...,Vj_1. This process can\nbe formalized as follows:\n\nv1 = arg max Var {vq} ;\nllvil=t\n\nvo = argmax Var {vq} ;\n|vVl=lvtvi (13)\nVu = arg max Var {vq} :\n\n||lv|=1,vtvi,...vlva—1\n\nWe define o; as the square root of the variance\nof the projection of Q onto the i” direction, i.e.,\n\nVar {vi Qh.\n\nTo derive features ks, we project the key vector\nk onto the directions vj, ..., vag and scale the pro-\njections by the corresponding standard deviations\n01,---, 0m. Each feature, k;, is thus calculated as\noivik.\n\nWe further examine the influence of / on the\nprediction confusion matrix, Confusioni pred, as\ndepicted in Figure 14. Given the similarity in\noutcomes for various V/, we settle on a value of\nM = 10 for computation of Confusionij?™4,\n\nK_ Calculation of Confusion,;\n\nTo gauge the true degree of confusion between\ncategories i and k for a given model, we suggest\nutilizing the Confusion,; metric:\n\nFirst, we procure all test samples x; bearing true\nlabels i or k. We then obtain the probabilities p!\nand pi yielded by the model for categories i and k,\nrespectively, on these samples. These probabilities\nare normalized to a total of 1. Essentially, we derive\na classifier f that delivers the probabilities pt and\npi for the categories i and k respectively, on the\ntest samples x;. By calculating the Area Under\n\nthe Receiver Operating Characteristic Curve (AUC-\nROC) value of this classifier f, we get the degree\nof confusion between category i and k, termed as\nConfusion;;.\n\nThe computed Confusion? is a value that never\nexceeds |. The closer Confusionij approximates 1,\nthe less pronounced the confusion, and vice versa.\n\nWe use the above metric instead of directly ana-\nlyzing the output labels of the model because pre-\nvious work has indicated the issue of insufficient\noutput probability calibration in ICL (Zhao et al.,\n2021), which is greatly affected by factors such as\nsample ordering and model preferences for specific\nlabel words. By leveraging our defined degree of\nconfusion, Confusion;;, we can implicitly alleviate\nthe disturbances arising from insufficient probabil-\nity calibration on the output labels. This allows\nfor a more accurate representation of the model’s\ndegree of confusion for different categories, miti-\ngating the impact of randomness.\n\nL_ Reproducibility\n\nIn the supplementary material, we have provided\ncodes that allow for the faithful replication of our\nexperiments and subsequent result analysis. To\nensure consistency and reproducibility across dif-\nferent devices, we have fixed the five random seeds\nto the values of 42, 43, 44, 45, and 46. We invite\nreaders to delve into the code for additional imple-\nmentation details that may arouse their interest.\n", "vlm_text": "J Calculation of  $\\hat{\\mathbf{k}}$  \nFor the sampled sequence    $x_{1},...,x_{T}$   to be pre- dicted, we denote the query vectors of the target po- sitions as  $\\mathbf{q}_{1},...,\\mathbf{q}_{T}$  . We then compute the matrix  $\\hat{\\mathbf{Q}}=(\\mathbf{q}_{1}-\\overline{{\\mathbf{q}}},...,\\mathbf{q}_{T}-\\overline{{\\mathbf{q}}})$   −  −  by subtracting the mean vector,  q , from each query vector. Subsequently, we determine the    $M$   directions,    $\\mathbf{v}_{1},...,\\mathbf{v}_{M}$  , that correspond to the M largest variation directions for the centralized query vectors    $\\hat{\\mathbf{q}}_{1},...,\\hat{\\mathbf{q}}_{T}$   . The    $i^{t h}$  direction,  $\\mathbf{v}_{i}$  , is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions,    $\\mathbf{v}_{1},...,\\mathbf{v}_{i-1}$  . This process can be formalized as follows: \n\n$$\n\\begin{array}{r l}&{{\\mathbf v}_{1}=\\underset{\\left\\|{\\mathbf v}\\right\\|=1}{\\arg\\operatorname*{max}}\\,\\mathrm{Var}\\left\\{{\\mathbf v}^{\\top}\\hat{\\mathbf Q}\\right\\},}\\\\ &{{\\mathbf v}_{2}=\\underset{\\left\\|{\\mathbf v}\\right\\|=1,{\\mathbf v}\\bot{\\mathbf v}_{1}}{\\arg\\operatorname*{max}}\\,\\mathrm{Var}\\left\\{{\\mathbf v}^{\\top}\\hat{\\mathbf Q}\\right\\},}\\\\ &{\\dots}\\\\ &{{\\mathbf v}_{M}=\\underset{\\left\\|{\\mathbf v}\\right\\|=1,{\\mathbf v}\\bot{\\mathbf v}_{1},\\dots,{\\mathbf v}\\bot{\\mathbf v}_{M-1}}{\\arg\\operatorname*{max}}\\,\\mathrm{Var}\\left\\{{\\mathbf v}^{\\top}\\hat{\\mathbf Q}\\right\\}.}\\end{array}\n$$\n \nWe define    $\\sigma_{i}$   as the square root of the variance of the projection of  $\\hat{\\mathbf{Q}}$   onto the    $i^{t h}$    direction, i.e.,\n\n  $\\sqrt{\\mathrm{Var}\\left\\{\\mathbf{v}_{i}^{\\top}\\hat{\\mathbf{Q}}\\right\\}}$  . \nTo derive features  $\\hat{\\mathbf{k}}\\mathbf{s}$  s, we project the key vector\n\n  $\\mathbf{k}$   onto the directions  $\\mathbf{v}_{1},...,\\mathbf{v}_{M}$   and scale the pro- jections by the corresponding standard deviations\n\n  $\\sigma_{1},...,\\sigma_{M}$  . Each feature,  $\\hat{\\mathbf{k}}_{i}$  , is thus calculated as\n\n  $\\sigma_{i}\\mathbf{v}_{i}^{T}\\mathbf{k}$  . \nthe Receiver Operating Characteristic Curve (AUC- ROC) value of this classifier    $f$  , we get the degree of confusion between category    $i$   and    $k$  , termed as Confusion ij . \nThe computed  Confusion i  $j$   is a value that never exceeds 1. The closer  Confusion i  $j$   approximates 1, the less pronounced the confusion, and vice versa. \nWe use the above metric instead of directly ana- lyzing the output labels of the model because pre- vious work has indicated the issue of insufficient output probability calibration in ICL ( Zhao et al. , 2021 ), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion,  Confus  $\\mathrm{ion}_{i j}$  , we can implicitly alleviate the disturbances arising from insufficient probabil- ity calibration on the output labels. This allows for a more accurate representation of the model’s degree of confusion for different categories, miti- gating the impact of randomness. \nL Reproducibility \nIn the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across dif- ferent devices, we have fixed the five random seeds to the values of 42, 43, 44, 45, and 46. We invite readers to delve into the code for additional imple- mentation details that may arouse their interest. \nWe further examine the influence of    $M$   on the prediction confusion matrix,  Confusion i  $j^{\\mathrm{pred}}$  , as depicted in Figure  14 . Given the similarity in outcomes for various  $M$  , we settle on a value of  $M=10$   for computation of Confusion i  $j^{\\mathrm{pred}}$  . \nK Calculation of Confusion ij \nTo gauge the true degree of confusion between categories  $i$   and    $k$   for a given model, we suggest utilizing the   $\\mathsf{C o n f u s i o n}_{i j}$   metric: \nFirst, we procure all test samples    $x_{t}$   bearing true labels    $i$   or    $k$  . We then obtain the probabilities    $p_{i}^{t}$  and  $p_{j}^{t}$    yielded by the model for categories    $i$   and    $k$  , respectively, on these samples. These probabilities are normalized to a total of 1. Essentially, we derive a classifier  $f$   that delivers the probabilities    $p_{i}^{t}$    and  $p_{j}^{t}$    for the categories    $i$   and    $k$   respectively, on the test samples    $x_{t}$  . By calculating the Area Under "}
{"page": 15, "image_path": "doc_images/2305.14160v4_15.jpg", "ocr_text": "1.0\nAbbreviation oo Abbreviation\nEntity 1 027 ae 0.9 08 Entity 0.89\nDescription 1 (0.67 0.74 0.6 07 Description 0.75\nem 0.9 0.76 0.67 1 0.74 0.6 Person 0.73\nos\nLocation Location\n0.4\nNumber 03 Diam 0.83 0.92 |0.72\n2 2 8 a 2 2 8 a\n$6 eg § go fe eg §\n2 B S 2 2 B 3 2\n5 3 5 3\n3 f=1 3 f=1\nEa Ea\n(a)M (b) M = 10\n1.0\nAbbreviation Abbreviation\n09\nEntity Entity 0.9\n08\nDescription 07 Description 0.76 0.72\nPee 0.89 0.73 0.68 06 Person [ery 0.72\nLocation 0.76 0. . os Location\nDiag 0.85 0.91 |0.72 . 04 Number\nfe eg §& gG @ ge g §&\nFd a 2 2 Fd a 2\n3 5 3\nf=1 3 f=1\n2\n(c) M = 20 (d) M =50\n1.0\nAbbreviation Abbreviation\n09\nEntity Entity\n08\nDescription Description\n07\nCem 0.89 |0.73 0.68 1 (0. Person 0.73 0.68\n0.6\nue 1 0.88 0.75 0.72 . Location 0.88 0.75 0.72\n05\nNm 0.85 0.9 |0.72 . oa Dim 0.85 0.9 |0.72\ngo @ 2 8 & g G6 2@ ge g &\n2 a S 2 2 g 3 2\n3 8 3 8\nEa Ea\n(e) M = 100 (f) M = 200\n\nFigure 14: Predicted confusion matrices under M = 5, 10, 20,50, 100, 200.\n\n1.0\n\n09\n\n08\n\n07\n\n0.6\n\n05\n\n0.4\n\n1.0\n\n0.9\n\n0.8\n\n07\n\n0.6\n\n0.5\n\n0.4\n\n1.0\n\n0.9\n\n08\n\n0.7\n\n0.6\n\n0.5\n\n-0.4\n", "vlm_text": "The image contains six confusion matrices labeled (a) to (f) corresponding to different values of \\( M = 5, 10, 20, 50, 100, 200 \\). Each matrix compares predicted and actual categories such as \"Abbreviation,\" \"Entity,\" \"Description,\" \"Person,\" \"Location,\" and \"Number.\" The entries in the matrices represent the prediction accuracy for each pair of predicted and true categories, depicted with a color scale ranging from light (lower accuracy) to dark (higher accuracy)."}
