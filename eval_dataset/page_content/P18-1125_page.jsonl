{"page": 0, "image_path": "doc_images/P18-1125_0.jpg", "ocr_text": "Conversations Gone Awry:\nDetecting Early Signs of Conversational Failure\n\nJustine Zhang and Jonathan P. Chang and Cristian Danescu-Niculescu-Mizil*\nCornell University\n{32727, jpc362}@cornell.edu, cristian@cs.cornell.edu\n\nLucas Dixon and Nithum Thain\nJigsaw\n\nAbstract\n\nOne of the main challenges online social\nsystems face is the prevalence of antiso-\ncial behavior, such as harassment and per-\nsonal attacks. In this work, we introduce\nthe task of predicting from the very start\nof a conversation whether it will get out\nof hand. As opposed to detecting undesir-\nable behavior after the fact, this task aims\nto enable early, actionable prediction at a\ntime when the conversation might still be\nsalvaged.\n\nTo this end, we develop a framework\nfor capturing pragmatic devices—such\nas politeness strategies and rhetorical\nprompts—used to start a conversation, and\nanalyze their relation to its future trajec-\ntory. Applying this framework in a con-\ntrolled setting, we demonstrate the feasi-\nbility of detecting early warning signs of\nantisocial behavior in online discussions.\n\n1 Introduction\n“Or vedi l’anime di color cui vinse I’ira.”!\n\n— Dante Alighieri, Divina Commedia, Inferno\nOnline conversations have a reputation for go-\ning awry (Hinds and Mortensen, 2005; Gheitasy\net al., 2015): antisocial behavior (Shepherd et al.,\n2015) or simple misunderstandings (Churchill and\nBly, 2000; Yamashita and Ishida, 2006) hamper\nthe efforts of even the best intentioned collabo-\nrators. Prior computational work has focused on\ncharacterizing and detecting content exhibiting an-\ntisocial online behavior: trolling (Cheng et al.,\n2015, 2017), hate speech (Warner and Hirschberg,\n2012; Davidson et al., 2017), harassment (Yin\net al., 2009), personal attacks (Wulczyn et al.,\n\n* Corresponding senior author.\n'“Now you see the souls of those whom anger overcame.”\n\nYiqing Hua\nCornell University\n{ldixon,nthain}@google.com yh663@cornell.edu\n\nDario Taraborelli\nWikimedia Foundation\ndario@wikimedia.org\n\n2017) or, more generally, toxicity (Chandrasekha-\nran et al., 2017; Pavlopoulos et al., 2017b).\n\nOur goal is crucially different: instead of identi-\nfying antisocial comments after the fact, we aim to\ndetect warning signs indicating that a civil conver-\nsation is at risk of derailing into such undesirable\nbehaviors. Such warning signs could provide po-\ntentially actionable knowledge at a time when the\nconversation is still salvageable.\n\nAs a motivating example, consider the pair of\nconversations in Figure |. Both exchanges took\nplace in the context of the Wikipedia discussion\npage for the article on the Dyatlov Pass Incident,\nand both show (ostensibly) civil disagreement be-\ntween the participants. However, only one of these\nconversations will eventually turn awry and de-\nvolve into a personal attack (““Wow, you’re com-\ning off as a total d**k. [...] What the hell is wrong\nwith you?”), while the other will remain civil.\n\nAs humans, we have some intuition about which\nconversation is more likely to derail.2_ We may\nnote the repeated, direct questioning with which\nAl opens the exchange, and that A2 replies\nwith yet another question. In contrast, B1’s\nsofter, hedged approach (“it seems”, “I don’t\nthink”) appears to invite an exchange of ideas,\nand B2 actually addresses the question instead of\nstonewalling. Could we endow artificial systems\nwith such intuitions about the future trajectory of\nconversations?\n\nIn this work we aim to computationally cap-\nture linguistic cues that predict a conversation’s\nfuture health. Most existing conversation mod-\neling approaches aim to detect characteristics of\nan observed discussion or predict the outcome af-\nter the discussion concludes—e.g., whether it in-\nvolves a present dispute (Allen et al., 2014; Wang\nand Cardie, 2014) or contributes to the even-\n\n?In fact, humans achieve an accuracy of 72% on this bal-\nanced task, showing that it is feasible, but far from trivial.\n\n1350\n\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1350-1361\nMelbourne, Australia, July 15 - 20, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Conversations Gone Awry: Detecting Early Signs of Conversational Failure \nJustine Zhang  and  Jonathan P. Chang  and  Cristian Danescu-Niculescu-Mizil ∗ Cornell University { jz727,jpc362 } @cornell.edu, cristian@cs.cornell.edu \nLucas Dixon  and  Nithum Thain Yiqing Hua Dario Taraborelli Jigsaw Cornell University Wikimedia Foundation { ldixon,nthain } @google.com yh663@cornell.edu dario@wikimedia.org \nAbstract \nOne of the main challenges online social systems face is the prevalence of antiso- cial behavior, such as harassment and per- sonal attacks. In this work, we introduce the task of predicting from the very start of a conversation whether it will get out of hand. As opposed to detecting undesir- able behavior after the fact, this task aims to enable early, actionable prediction at a time when the conversation might still be salvaged. \nTo this end, we develop a framework for capturing pragmatic devices—such as politeness strategies and rhetorical prompts—used to start a conversation, and analyze their relation to its future trajec- tory. Applying this framework in a con- trolled setting, we demonstrate the feasi- bility of detecting early warning signs of antisocial behavior in online discussions. \n1 Introduction \n“Or vedi l’anime di color cui vinse l’ira.” 1 – Dante Alighieri, Divina Commedia, Inferno \n\nOnline conversations have a reputation for go- ing awry ( Hinds and Mortensen ,  2005 ;  Gheitasy et al. ,  2015 ): antisocial behavior ( Shepherd et al. , 2015 ) or simple misunderstandings ( Churchill and Bly ,  2000 ;  Yamashita and Ishida ,  2006 ) hamper the efforts of even the best intentioned collabo- rators. Prior computational work has focused on characterizing and detecting content exhibiting an- tisocial online behavior: trolling ( Cheng et al. , 2015 ,  2017 ), hate speech ( Warner and Hirschberg , 2012 ;  Davidson et al. ,  2017 ), harassment ( Yin et al. ,  2009 ), personal attacks ( Wulczyn et al. , \n2017 ) or, more generally, toxicity ( Chandrasekha- ran et al. ,  2017 ;  Pavlopoulos et al. ,  2017b ). \nOur goal is crucially different: instead of identi- fying antisocial comments  after the fact , we aim to detect  warning signs  indicating that a civil conver- sation is at risk of derailing into such undesirable behaviors. Such warning signs could provide po- tentially actionable knowledge at a time when the conversation is still salvageable. \nAs a motivating example, consider the pair of conversations in Figure  1 . Both exchanges took place in the context of the Wikipedia discussion page for the article on the Dyatlov Pass Incident, and both show (ostensibly) civil disagreement be- tween the participants. However, only one of these conversations will eventually turn awry and de- volve into a personal attack (“Wow, you’re com- ing off as a total   $\\mathrm{d}^{**}\\mathbf{k}$  . [...] What the hell is wrong with you?”), while the other will remain civil. \nAs humans, we have some intuition about which conversation is more likely to derail. We may note the repeated, direct questioning with which A1  opens the exchange, and that  A2  replies with yet another question. In contrast,  B1 ’s softer, hedged approach (“it seems”, “I don’t think”) appears to invite an exchange of ideas, and  B2  actually addresses the question instead of stonewalling. Could we endow artiﬁcial systems with such intuitions about the future trajectory of conversations? \nIn this work we aim to computationally cap- ture linguistic cues that predict a conversation’s future health. Most existing conversation mod- eling approaches aim to detect characteristics of an observed discussion or predict the outcome af- ter the discussion concludes—e.g., whether it in- volves a present dispute ( Allen et al. ,  2014 ;  Wang and Cardie ,  2014 ) or contributes to the even- "}
{"page": 1, "image_path": "doc_images/P18-1125_1.jpg", "ocr_text": "A1: Why there’s no mention of it here? Namely, an altercation\nwith a foreign intelligence group? True, by the standards of\nsources some require it wouln’t even come close, not to men-\ntion having some really weak points, but it doesn’t mean that it\ndoesn’t exist.\n\nA2: So what you’re saying is we should put a bad\nsource in the article because it exists?\n\nB1: Is the St. Petersberg Times considered a reliable source by\nwikipedia? It seems that the bulk of this article is coming from\nthat one article, which speculates about missile launches and\nUFOs. I’m going to go through and try and find corroborating\nsources and maybe do a rewrite of the article. I don’t think this\narticle should rely on one so-so source.\n\nB2: I would assume that it’s as reliable as any other\nmainstream news source.\n\nFigure 1: Two examples of initial exchanges from conversations concerning disagreements between\neditors working on the Wikipedia article about the Dyatlov Pass Incident. Only one of the conversations\nwill eventually turn awry, with an interlocutor launching into a personal attack.\n\ntual solution of a problem (Niculae and Danescu-\nNiculescu-Mizil, 2016). In contrast, for this new\ntask we need to discover interactional signals of\nthe future trajectory of an ongoing conversation.\n\nWe make a first approach to this problem by an-\nalyzing the role of politeness (or lack thereof) in\nkeeping conversations on track. Prior work has\nshown that politeness can help shape the course\nof offline (Clark, 1979; Clark and Schunk, 1980),\nas well as online interactions (Burke and Kraut,\n2008), through mechanisms such as softening the\nperceived force of a message (Fraser, 1980), act-\ning as a buffer between conflicting interlocutor\ngoals (Brown and Levinson, 1987), and enabling\nall parties to save face (Goffman, 1955). This sug-\ngests the potential of politeness to serve as an in-\ndicator of whether a conversation will sustain its\ninitial civility or eventually derail, and motivates\nits consideration in the present work.\n\nRecent studies have computationally opera-\ntionalized prior formulations of politeness by\nextracting linguistic cues that reflect politeness\nstrategies (Danescu-Niculescu-Mizil et al., 2013;\nAubakirova and Bansal, 2016). Such research\nhas additionally tied politeness to social fac-\ntors such as individual status (Danescu-Niculescu-\nMizil et al., 2012; Krishnan and Eisenstein, 2015),\nand the success of requests (Althoff et al., 2014)\nor of collaborative projects (Ortu et al., 2015).\nHowever, to the best of our knowledge, this is the\nfirst computational investigation of the relation be-\ntween politeness strategies and the future trajec-\ntory of the conversations in which they are de-\nployed. Furthermore, we generalize beyond pre-\ndefined politeness strategies by using an unsu-\npervised method to discover additional rhetorical\nprompts used to initiate different types of conver-\nsations that may be specific to online collaborative\nsettings, such as coordinating work (Kittur and\nKraut, 2008) or conducting factual checks.\n\nWe explore the role of such pragmatic and\nrhetorical devices in foretelling a particularly per-\nplexing type of conversational failure: when par-\nticipants engaged in previously civil discussion\nstart to attack each other. This type of derailment\n“from within” is arguably more disruptive than\nother forms of antisocial behavior, such as vandal-\nism or trolling, which the interlocutors have less\ncontrol over or can choose to ignore.\n\nWe study this phenomenon in a new dataset of\nWikipedia talk page discussions, which we com-\npile through a combination of machine learning\nand crowdsourced filtering. The dataset consists\nof conversations which begin with ostensibly civil\ncomments, and either remain healthy or derail into\npersonal attacks. Starting from this data, we con-\nstruct a setting that mitigates effects which may\ntrivialize the task. In particular, some topical con-\nexts (such as politics and religion) are naturally\nmore susceptible to antisocial behavior (Kittur\net al., 2009; Cheng et al., 2015). We employ tech-\nniques from causal inference (Rosenbaum, 2010)\n0 establish a controlled framework that focuses\nour study on topic-agnostic linguistic cues.\n\nIn this controlled setting, we find that prag-\nmatic cues extracted from the very first exchange\nin a conversation (i.e., the first comment-reply\npair) can indeed provide some signal of whether\nthe conversation will subsequently go awry. For\nexample, conversations prompted by hedged re-\nmarks sustain their initial civility more so than\nthose prompted by forceful questions, or by direct\nlanguage addressing the other interlocutor.\n\nIn summary, our main contributions are:\n\ne We articulate the new task of detecting early\non whether a conversation will derail into\npersonal attacks;\n\ne We devise a controlled setting and build a la-\nbeled dataset to study this phenomenon;\n\n1351\n", "vlm_text": "The table presents a dialogue between two pairs of people (A1 and A2, B1 and B2) discussing the reliability of sources for an article. \n\nIn the first exchange (A1 and A2):\n- A1 questions why a certain event involving a foreign intelligence group is not mentioned, suggesting that even if some sources have weaknesses, their existence should be acknowledged.\n- A2 counters by questioning whether a bad source should be used simply because it exists.\n\nIn the second exchange (B1 and B2):\n- B1 wonders about the reliability of the St. Petersburg Times for Wikipedia, expressing concern about an article that mainly relies on one source, which discusses missile launches and UFOs. B1 suggests finding corroborating sources for a potential rewrite.\n- B2 assumes that the St. Petersburg Times is as reliable as any other mainstream news source.\ntual solution of a problem ( Niculae and Danescu- Niculescu-Mizil ,  2016 ). In contrast, for this new task we need to discover interactional signals of the  future  trajectory of an  ongoing  conversation. \nWe make a ﬁrst approach to this problem by an- alyzing the role of politeness (or lack thereof) in keeping conversations on track. Prior work has shown that politeness can help shape the course of ofﬂine ( Clark ,  1979 ;  Clark and Schunk ,  1980 ), as well as online interactions ( Burke and Kraut , 2008 ), through mechanisms such as softening the perceived force of a message ( Fraser ,  1980 ), act- ing as a buffer between conﬂicting interlocutor goals ( Brown and Levinson ,  1987 ), and enabling all parties to save face ( Goffman ,  1955 ). This sug- gests the potential of politeness to serve as an in- dicator of whether a conversation will sustain its initial civility or eventually derail, and motivates its consideration in the present work. \nRecent studies have computationally opera- tionalized prior formulations of politeness by extracting linguistic cues that reﬂect politeness strategies ( Danescu-Niculescu-Mizil et al. ,  2013 ; Aubakirova and Bansal ,  2016 ). Such research has additionally tied politeness to social fac- tors such as individual status ( Danescu-Niculescu- Mizil et al. ,  2012 ;  Krishnan and Eisenstein ,  2015 ), and the success of requests ( Althoff et al. ,  2014 ) or of collaborative projects ( Ortu et al. ,  2015 ). However, to the best of our knowledge, this is the ﬁrst computational investigation of the relation be- tween politeness strategies and the future trajec- tory of the conversations in which they are de- ployed. Furthermore, we generalize beyond pre- deﬁned politeness strategies by using an unsu- pervised method to discover additional rhetorical prompts used to initiate different types of conver- sations that may be speciﬁc to online collaborative settings, such as coordinating work ( Kittur and Kraut ,  2008 ) or conducting factual checks. \nWe explore the role of such pragmatic and rhetorical devices in foretelling a particularly per- plexing type of conversational failure: when par- ticipants engaged in previously civil discussion start to attack each other. This type of derailment “from within” is arguably more disruptive than other forms of antisocial behavior, such as vandal- ism or trolling, which the interlocutors have less control over or can choose to ignore. \nWe study this phenomenon in a new dataset of Wikipedia talk page discussions, which we com- pile through a combination of machine learning and crowdsourced ﬁltering. The dataset consists of conversations which begin with ostensibly civil comments, and either remain healthy or derail into personal attacks. Starting from this data, we con- struct a setting that mitigates effects which may trivialize the task. In particular, some topical con- texts (such as politics and religion) are naturally more susceptible to antisocial behavior ( Kittur et al. ,  2009 ;  Cheng et al. ,  2015 ). We employ tech- niques from causal inference ( Rosenbaum ,  2010 ) to establish a controlled framework that focuses our study on topic-agnostic linguistic cues. \nIn this controlled setting, we ﬁnd that prag- matic cues extracted from the very ﬁrst exchange in a conversation (i.e., the ﬁrst comment-reply pair) can indeed provide some signal of whether the conversation will subsequently go awry. For example, conversations prompted by hedged re- marks sustain their initial civility more so than those prompted by forceful questions, or by direct language addressing the other interlocutor. \nIn summary, our main contributions are: •  We articulate the new task of detecting early on whether a conversation will derail into personal attacks; •  We devise a controlled setting and build a la- beled dataset to study this phenomenon; "}
{"page": 2, "image_path": "doc_images/P18-1125_2.jpg", "ocr_text": "e We investigate how politeness strategies and\nother rhetorical devices are tied to the future\ntrajectory of a conversation.\n\nMore broadly, we show the feasibility of auto-\nmatically detecting warning signs of future mis-\nbehavior in collaborative interactions. By provid-\ning a labeled dataset together with basic method-\nology and several baselines, we open the door to\nfurther work on understanding factors which may\nderail or sustain healthy online conversations. To\nfacilitate such future explorations, we distrubute\nthe data and code as part of the Cornell Conversa-\ntional Analysis Toolkit.?\n\n2 Further Related Work\n\nAntisocial behavior. Prior work has studied a\nwide range of disruptive interactions in various on-\nline platforms like Reddit and Wikipedia, exam-\nining behaviors like aggression (Kayany, 1998),\nharassment (Chatzakou et al., 2017; Vitak et al.,\n2017), and bullying (Akbulut et al., 2010; Kwak\net al., 2015; Singh et al., 2017), as well as their im-\npact on aspects of engagement like user retention\n(Collier and Bear, 2012; Wikimedia Support and\nSafety Team, 2015) or discussion quality (Arazy\net al., 2013). Several studies have sought to de-\nvelop machine learning techniques to detect sig-\nnatures of online toxicity, such as personal in-\nsults (Yin et al., 2009), harassment (Sood et al.,\n2012) and abusive language (Nobata et al., 2016;\nGambick and Sikdar, 2017; Pavlopoulos et al.,\n2017a; Wulczyn et al., 2017). These works fo-\ncus on detecting toxic behavior after it has al-\nready occurred; a notable exception is Cheng et al.\n(2017), which predicts future community enforce-\nment against users in news-based discussions. Our\nwork similarly aims to understand future antiso-\ncial behavior; however, our focus is on studying\nthe trajectory of a conversation rather than the be-\nhavior of individuals across disparate discussions.\nDiscourse analysis. Our present study builds on a\nlarge body of prior work in computationally mod-\neling discourse. Both unsupervised (Ritter et al.,\n2010) and supervised (Zhang et al., 2017a) ap-\nproaches have been used to categorize behavioral\npatterns on the basis of the language that ensues in\na conversation, in the particular realm of online\ndiscussions. Models of conversational behavior\nhave also been used to predict conversation out-\ncomes, such as betrayal in games (Niculae et al.,\n\nShttp://convokit.infosci.cornell.edu\n\n2015), and success in team problem solving set-\ntings (Fu et al., 2017) or in persuading others (Tan\net al., 2016; Zhang et al., 2016).\n\nWhile we are inspired by the techniques em-\nployed in these approaches, our work is concerned\nwith predicting the future trajectory of an ongoing\nconversation as opposed to a post-hoc outcome.\nIn this sense, we build on prior work in modeling\nconversation trajectory, which has largely consid-\nered structural aspects of the conversation (Kumar\net al., 2010; Backstrom et al., 2013). We comple-\nment these structural models by seeking to extract\npotential signals of future outcomes from the lin-\nguistic discourse within the conversation.\n\n3 Finding Conversations Gone Awry\n\nWe develop our framework for understanding lin-\nguistic markers of conversational trajectories in\nthe context of Wikipedia’s talk page discussions—\npublic forums in which contributors convene to\ndeliberate on editing matters such as evaluating\nthe quality of an article and reviewing the com-\npliance of contributions with community guide-\nlines. The dynamic of conversational derailment\nis particularly intriguing and consequential in this\nsetting by virtue of its collaborative, goal-oriented\nnature. In contrast to unstructured commenting fo-\nrums, cases where one collaborator turns on an-\nother over the course of an initially civil exchange\nconstitute perplexing pathologies. In turn, these\noxic attacks are especially disruptive in Wikipedia\nsince they undermine the social fabric of the com-\nmunity as well as the ability of editors to con-\ntribute (Henner and Sefidari, 2016).\nTo approach this domain we reconstruct a com-\nplete view of the conversational process in the edit\nhistory of English Wikipedia by translating se-\nquences of revisions of each talk page into struc-\nured conversations. This yields roughly 50 mil-\nlion conversations across 16 million talk pages.\nRoughly one percent of Wikipedia comments\nare estimated to exhibit antisocial behavior (Wul-\nczyn et al., 2017). This illustrates a challenge\nfor studying conversational failure: one has to sift\nthrough many conversations in order to find even\na small set of examples. To avoid such a pro-\nhibitively exhaustive analysis, we first use a ma-\nchine learning classifier to identify candidate con-\nversations that are likely to contain a toxic contri-\nbution, and then use crowdsourcing to vet the re-\nsulting labels and construct our controlled dataset.\n\n1352\n", "vlm_text": "•  We investigate how politeness strategies and other rhetorical devices are tied to the future trajectory of a conversation. \nMore broadly, we show the feasibility of auto- matically detecting warning signs of future mis- behavior in collaborative interactions. By provid- ing a labeled dataset together with basic method- ology and several baselines, we open the door to further work on understanding factors which may derail or sustain healthy online conversations. To facilitate such future explorations, we distrubute the data and code as part of the Cornell Conversa- tional Analysis Toolkit. \n2 Further Related Work \nAntisocial behavior. Prior work has studied a wide range of disruptive interactions in various on- line platforms like Reddit and Wikipedia, exam- ining behaviors like aggression ( Kayany ,  1998 ), harassment ( Chatzakou et al. ,  2017 ;  Vitak et al. , 2017 ), and bullying ( Akbulut et al. ,  2010 ;  Kwak et al. ,  2015 ;  Singh et al. ,  2017 ), as well as their im- pact on aspects of engagement like user retention ( Collier and Bear ,  2012 ;  Wikimedia Support and Safety Team ,  2015 ) or discussion quality ( Arazy et al. ,  2013 ). Several studies have sought to de- velop machine learning techniques to detect sig- natures of online toxicity, such as personal in- sults ( Yin et al. ,  2009 ), harassment ( Sood et al. , 2012 ) and abusive language ( Nobata et al. ,  2016 ; Gamb¨ ack and Sikdar ,  2017 ;  Pavlopoulos et al. , 2017a ;  Wulczyn et al. ,  2017 ). These works fo- cus on detecting toxic behavior after it has al- ready occurred; a notable exception is  Cheng et al. ( 2017 ), which predicts future community enforce- ment against users in news-based discussions. Our work similarly aims to understand  future  antiso- cial behavior; however, our focus is on studying the trajectory of a conversation rather than the be- havior of individuals across disparate discussions. Discourse analysis.  Our present study builds on a large body of prior work in computationally mod- eling discourse. Both unsupervised ( Ritter et al. , 2010 ) and supervised ( Zhang et al. ,  2017a ) ap- proaches have been used to categorize behavioral patterns on the basis of the language that ensues in a conversation, in the particular realm of online discussions. Models of conversational behavior have also been used to predict conversation out- comes, such as betrayal in games ( Niculae et al. , 2015 ), and success in team problem solving set- tings ( Fu et al. ,  2017 ) or in persuading others ( Tan et al. ,  2016 ;  Zhang et al. ,  2016 ). \n\nWhile we are inspired by the techniques em- ployed in these approaches, our work is concerned with predicting the future trajectory of an ongoing conversation as opposed to a post-hoc outcome. In this sense, we build on prior work in modeling conversation trajectory, which has largely consid- ered  structural  aspects of the conversation ( Kumar et al. ,  2010 ;  Backstrom et al. ,  2013 ). We comple- ment these structural models by seeking to extract potential signals of future outcomes from the  lin- guistic discourse  within the conversation. \n3 Finding Conversations Gone Awry \nWe develop our framework for understanding lin- guistic markers of conversational trajectories in the context of Wikipedia’s  talk page  discussions— public forums in which contributors convene to deliberate on editing matters such as evaluating the quality of an article and reviewing the com- pliance of contributions with community guide- lines. The dynamic of conversational derailment is particularly intriguing and consequential in this setting by virtue of its collaborative, goal-oriented nature. In contrast to unstructured commenting fo- rums, cases where one  collaborator  turns on an- other over the course of an initially civil exchange constitute perplexing pathologies. In turn, these toxic attacks are especially disruptive in Wikipedia since they undermine the social fabric of the com- munity as well as the ability of editors to con- tribute ( Henner and Seﬁdari ,  2016 ). \nTo approach this domain we reconstruct a com- plete view of the conversational process in the edit history of English Wikipedia by translating se- quences of revisions of each talk page into struc- tured conversations. This yields roughly   $50~\\mathrm{mil}.$  - lion conversations across 16 million talk pages. \nRoughly one percent of Wikipedia comments are estimated to exhibit antisocial behavior ( Wul- czyn et al. ,  2017 ). This illustrates a challenge for studying conversational failure: one has to sift through many conversations in order to ﬁnd even a small set of examples. To avoid such a pro- hibitively exhaustive analysis, we ﬁrst use a ma- chine learning classiﬁer to identify candidate con- versations that are likely to contain a toxic contri- bution, and then use crowdsourcing to vet the re- sulting labels and construct our controlled dataset. "}
{"page": 3, "image_path": "doc_images/P18-1125_3.jpg", "ocr_text": "Job 1: Ends in personal attack. We show three annotators a\nconversation and ask them to determine if its last comment is\na personal attack toward someone else in the conversation.\n\nAnnotators Conversations Agreement\n367 4,022 67.8%\n\nJob 2: Civil start. We split conversations into snip-\npets of three consecutive comments. We ask three annotators\nto determine whether any of the comments in a snippet is toxic.\n\nAnnotators Conversations Snippets\n247 1,252 2,181\n\nAgreement\n87.5%\n\nTable 1: Descriptions of crowdsourcing jobs, with relevant statistics. More details in Appendix A.\n\nCandidate selection. Our goal is to analyze how\nthe start of a civil conversation is tied to its poten-\ntial future derailment into personal attacks. Thus,\nwe only consider conversations that start out as os-\ntensibly civil, i.e., where at least the first exchange\ndoes not exhibit any toxic behavior, and that con-\ntinue beyond this first exchange. To focus on the\nespecially perplexing cases when the attacks come\nfrom within, we seek examples where the attack is\ninitiated by one of the two participants in the ini-\ntial exchange.\n\nTo select candidate conversations to include in\nour collection, we use the toxicity classifier pro-\nvided by the Perspective API,> which is trained on\nWikipedia talk page comments that have been an-\nnotated by crowdworkers (Wulczyn et al., 2016).\nThis provides a toxicity score t for all comments\nin our dataset, which we use to preselect two sets\nof conversations: (a) candidate conversations that\nare civil throughout, i.e., conversations in which\nall comments (including the initial exchange) are\nnot labeled as toxic (t < 0.4); and (b) candidate\nconversations that turn toxic after the first (civil)\nexchange, i.e., conversations in which the N-th\ncomment (V > 2) is labeled toxic (t > 0.6), but\nall the preceding comments are not (t < 0.4).\nCrowdsourced filtering. Starting from these can-\ndidate sets, we use crowdsourcing to vet each con-\nversation and select a subset that are perceived\nby humans to either stay civil throughout (‘“‘on-\ntrack” conversations), or start civil but end with\na personal attack (““awry-turning” conversations).\nTo inform the design of this human-filtering pro-\ncess and to check its effectiveness, we start from\na seed set of 232 conversations manually ver-\nified by the authors to end in personal attacks\n(more details about the selection of the seed set\nand its role in the crowd-sourcing process can be\nfound in Appendix A). We take particular care to\nnot over-constrain crowdworker interpretations of\n\n4For the sake of generality, in this work we focus on this\nmost basic conversational unit: the first comment-reply pair\nstarting a conversation.\n\nShttps://www.perspectiveapi.com/\n\nwhat personal attacks may be, and to separate tox-\nicity from civil disagreement, which is recognized\nas a key aspect of effective collaborations (Coser,\n1956; De Dreu and Weingart, 2003).\n\nWe design and deploy two filtering jobs using\nhe CrowdFlower platform, summarized in Table 1\nand detailed in Appendix A. Job 1 is designed to\nselect conversations that contain a “rude, insulting,\nor disrespectful” comment towards another user in\nhe conversation—i.e., a personal attack. In con-\ntrast to prior work labeling antisocial comments in\nisolation (Sood et al., 2012; Wulczyn et al., 2017),\nannotators are asked to label personal attacks in\nhe context of the conversations in which they oc-\ncur, since antisocial behavior can often be context-\ndependent (Cheng et al., 2017). In fact, in order to\nensure that the crowdworkers read the entire con-\nversation, we also ask them to indicate who is the\narget of the attack. We apply this task to the se\nof candidate awry-turning conversations, selecting\nhe 14% which all three annotators perceived as\nending in a personal attack.°\n\nJob 2 is designed to filter out conversations tha\ndo not actually start out as civil. We run this\njob to ensure that the awry-turning conversations\nare civil up to the point of the attack—i.e., they\nturn awry—discarding 5% of the candidates thai\npassed Job 1. We also use it to verify that the\ncandidate on-track conversations are indeed civil\nhroughout, discarding 1% of the respective candi-\ndates. In both cases we filter out conversations in\nwhich three annotators could identify at least one\ncomment that is “rude, insulting, or disrespectful”.\nControlled setting. Finally, we need to construct\na setting that affords for meaningful comparison\nbetween conversations that derail and those that\nstay on track, and that accounts for trivial topical\nconfounds (Kittur et al., 2009; Cheng et al., 2015).\nWe mitigate topical confounds using matching, a\ntechnique developed for causal inference in obser-\nvational studies (Rubin, 2007). Specifically, start-\n\nWe opted to use unanimity in this task to account for the\nhighly subjective nature of the phenomenon.\n\n1353\n", "vlm_text": "The table presents results from two assessment jobs related to conversation analysis.\n\n- **Job 1: Ends in Personal Attack**\n  - Task: Annotators determine if the last comment in a conversation is a personal attack.\n  - Annotators: 367\n  - Conversations: 4,022\n  - Agreement: 67.8%\n\n- **Job 2: Civil Start**\n  - Task: Annotators evaluate snippets of three consecutive comments to determine if any are toxic.\n  - Annotators: 247\n  - Conversations: 1,252\n  - Snippets: 2,181\n  - Agreement: 87.5%\nCandidate selection.  Our goal is to analyze how the start of a  civil  conversation is tied to its poten- tial future derailment into personal attacks. Thus, we only consider conversations that start out as os- tensibly civil, i.e., where at least the ﬁrst exchange does not exhibit any toxic behavior,   and that con- tinue beyond this ﬁrst exchange. To focus on the especially perplexing cases when the attacks come from within , we seek examples where the attack is initiated by one of the two participants in the ini- tial exchange. \nTo select candidate conversations to include in our collection, we use the toxicity classiﬁer pro- vided by the Perspective API,   which is trained on Wikipedia talk page comments that have been an- notated by crowdworkers ( Wulczyn et al. ,  2016 ). This provides a toxicity score    $t$   for all comments in our dataset, which we use to preselect two sets of conversations: (a) candidate conversations that are civil throughout, i.e., conversations in which all comments (including the initial exchange) are not labeled as toxic   $\\mathit{(t\\,<\\,0.4)}$  ; and (b) candidate conversations that turn toxic after the ﬁrst (civil) exchange, i.e., conversations in which the    $N$  -th comment   $(N>2)$  ) is labeled toxic   $(t\\geq0.6)$  , but all the preceding comments are not   $(t<0.4)$  . \nCrowdsourced ﬁltering.  Starting from these can- didate sets, we use crowdsourcing to vet each con- versation and select a subset that are perceived by humans to either stay civil throughout (“on- track” conversations), or start civil but end with a  personal attack  (“awry-turning” conversations). To inform the design of this human-ﬁltering pro- cess and to check its effectiveness, we start from a seed set of 232 conversations manually ver- iﬁed by the authors to end in personal attacks (more details about the selection of the seed set and its role in the crowd-sourcing process can be found in Appendix  A ). We take particular care to not over-constrain crowdworker interpretations of what personal attacks may be, and to separate tox- icity from civil disagreement, which is recognized as a key aspect of effective collaborations ( Coser , 1956 ;  De Dreu and Weingart ,  2003 ). \n\nWe design and deploy two ﬁltering jobs using the CrowdFlower platform, summarized in Table  1 and detailed in Appendix  A .  Job 1  is designed to select conversations that contain a “rude, insulting, or disrespectful” comment towards another user in the conversation—i.e., a personal attack. In con- trast to prior work labeling antisocial comments in isolation ( Sood et al. ,  2012 ;  Wulczyn et al. ,  2017 ), annotators are asked to label personal attacks in the  context  of the conversations in which they oc- cur, since antisocial behavior can often be context- dependent ( Cheng et al. ,  2017 ). In fact, in order to ensure that the crowdworkers read the entire con- versation, we also ask them to indicate who is the target of the attack. We apply this task to the set of candidate awry-turning conversations, selecting the   $14\\%$   which all three annotators perceived as ending in a personal attack. \nJob 2  is designed to ﬁlter out conversations that do not actually start out as civil. We run this job to ensure that the  awry-turning  conversations are civil up to the point of the attack—i.e., they turn  awry—discarding   $5\\%$   of the candidates that passed Job 1. We also use it to verify that the candidate  on-track  conversations are indeed civil throughout, discarding   $1\\%$   of the respective candi- dates. In both cases we ﬁlter out conversations in which three annotators could identify at least one comment that is “rude, insulting, or disrespectful”. \nControlled setting.  Finally, we need to construct a setting that affords for meaningful comparison between conversations that derail and those that stay on track, and that accounts for trivial topical confounds ( Kittur et al. ,  2009 ;  Cheng et al. ,  2015 ). We mitigate topical confounds using matching, a technique developed for causal inference in obser- vational studies ( Rubin ,  2007 ). Speciﬁcally, start- ing from our human-vetted collection of conver- sations, we pair each  awry-turning  conversation, with an  on-track  conversation, such that both took place on the same talk page. If we ﬁnd multi- ple such pairs, we only keep the one in which the paired conversations take place closest in time, to tighten the control for topic. Conversations that cannot be paired are discarded. "}
{"page": 4, "image_path": "doc_images/P18-1125_4.jpg", "ocr_text": "ing from our human-vetted collection of conver-\nsations, we pair each awry-turning conversation,\nwith an on-track conversation, such that both took\nplace on the same talk page. If we find multi-\nple such pairs, we only keep the one in which the\npaired conversations take place closest in time, to\ntighten the control for topic. Conversations that\ncannot be paired are discarded.\n\nThis procedure yields a total of 1,270 paired\nawry-turning and on-track conversations (includ-\ning our initial seed set), spanning 582 distinct talk\npages (averaging 1.1 pairs per page, maximum 8)\nand 1,876 (overlapping) topical categories. The\naverage length of a conversation is 4.6 comments.\n\n4 Capturing Pragmatic Devices\n\nWe now describe our framework for capturing lin-\nguistic cues that might inform a conversation’s fu-\nture trajectory. Crucially, given our focus on con-\nversations that start seemingly civil, we do not ex-\npect overtly hostile language—such as insults (Yin\net al., 2009)—1o be informative. Instead, we seek\nto identify pragmatic markers within the initial ex-\nchange of a conversation that might serve to reveal\nor exacerbate underlying tensions that eventually\ncome to the fore, or conversely suggest sustainable\ncivility. In particular, in this work we explore how\npoliteness strategies and rhetorical prompts reflect\nthe future health of a conversation.\n\nPoliteness strategies. Politeness can reflect\na-priori good will and help navigate potentially\nface-threatening acts (Goffman, 1955; Lakoff,\n1973), and also offers hints to the underlying in-\ntentions of the interlocutors (Fraser, 1980). Hence,\nwe may naturally expect certain politeness strate-\ngies to signal that a conversation is likely to stay\non track, while others might signal derailment.\n\nIn particular, we consider a set of pragmatic\ndevices signaling politeness drawn from Brown\nand Levinson (1987). These linguistic features re-\nflect two overarching types of politeness. Posi-\ntive politeness strategies encourage social connec-\ntion and rapport, perhaps serving to maintain co-\nhesion throughout a conversation; such strategies\ninclude gratitude (“thanks for your help”), greet-\nings (“hey, how is your day so far”) and use of\n“please”, both at the start (“Please find sources for\nyour edit...”) and in the middle (“Could you please\nhelp with...?”) of a sentence. Negative politeness\nstrategies serve to dampen an interlocutor’s impo-\nsition on an addressee, often through conveying\n\nindirectness or uncertainty on the part of the com-\nmenter. Both commenters in example B (Fig. 1)\nemploy one such strategy, hedging, perhaps seek-\ning to soften an impending disagreement about\na source’s reliability (“I don’t think...’, “I would\nassume...”). We also consider markers of impo-\nlite behavior, such as the use of direct questions\n(“Why’s there no mention of it?’) and sentence-\ninitial second person pronouns (“Your sources\ndon’t matter...”), which may serve as forceful-\nsounding contrasts to negative politeness markers.\nFollowing Danescu-Niculescu-Mizil et al. (2013),\nwe extract such strategies by pattern matching on\nthe dependency parses of comments.\n\nTypes of conversation prompts. To complement\nour pre-defined set of politeness strategies, we\nseek to capture domain-specific rhetorical patterns\nused to initiate conversations. For instance, in a\ncollaborative setting, we may expect conversations\nthat start with an invitation for working together to\nsignal less tension between the participants than\nthose that start with statements of dispute. We dis-\ncover types of such conversation prompts in an un-\nsupervised fashion by extending a framework used\nto infer the rhetorical role of questions in (offline)\npolitical debates (Zhang et al., 2017b) to more\ngenerally extract the rhetorical functions of com-\nments. The procedure follows the intuition that the\nrhetorical role of a comment is reflected in the type\nof replies it is likely to elicit. As such, comments\nwhich tend to trigger similar replies constitute a\nparticular type of prompt.\n\nTo implement this intuition, we derive two dif-\nerent low-rank representations of the common\nlexical phrasings contained in comments (agnos-\nic to the particular topical content discussed), au-\nomatically extracted as recurring sets of arcs in\nhe dependency parses of comments. First, we\nderive reply-vectors of phrasings, which reflect\nheir propensities to co-occur. In particular, we\nperform singular value decomposition on a term-\ndocument matrix R of phrasings and replies as\nReR= URSV2, where rows of Up are low-\nrank reply-vectors for each phrasing.\n\nNext, we derive prompt-vectors for the phras-\nings, which reflect similarities in the subsequent\nreplies that a phrasing prompts. We construct a\nprompt-reply matrix P = (pjj) where pj = 1 if\nphrasing j occurred in a reply to a comment con-\ntaining phrasing i. We project P into the same\nspace as Up by solving for P in P = PSve as\n\n1354\n", "vlm_text": "\nThis procedure yields a total of 1,270 paired awry-turning and on-track conversations (includ- ing our initial seed set), spanning 582 distinct talk pages (averaging 1.1 pairs per page, maximum 8) and 1,876 (overlapping) topical categories. The average length of a conversation is 4.6 comments. \n4 Capturing Pragmatic Devices \nWe now describe our framework for capturing lin- guistic cues that might inform a conversation’s fu- ture trajectory. Crucially, given our focus on con- versations that start seemingly civil, we do not ex- pect overtly hostile language—such as insults ( Yin et al. ,  2009 )—to be informative. Instead, we seek to identify pragmatic markers within the initial ex- change of a conversation that might serve to reveal or exacerbate underlying tensions that eventually come to the fore, or conversely suggest sustainable civility. In particular, in this work we explore how politeness strategies and rhetorical prompts reﬂect the future health of a conversation. \nPoliteness strategies. Politeness can reﬂect a-priori good will and help navigate potentially face-threatening acts ( Goffman ,  1955 ;  Lakoff , 1973 ), and also offers hints to the underlying in- tentions of the interlocutors ( Fraser ,  1980 ). Hence, we may naturally expect certain politeness strate- gies to signal that a conversation is likely to stay on track, while others might signal derailment. \nIn particular, we consider a set of pragmatic devices signaling politeness drawn from  Brown and Levinson  ( 1987 ). These linguistic features re- ﬂect two overarching types of politeness.  Posi- tive  politeness strategies encourage social connec- tion and rapport, perhaps serving to maintain co- hesion throughout a conversation; such strategies include gratitude (“ thanks  for your help”), greet- ings (“ hey , how is your day so far”) and use of “please”, both at the start (“ Please  ﬁnd sources for your edit...”) and in the middle (“Could you  please help with...?”) of a sentence.  Negative  politeness strategies serve to dampen an interlocutor’s impo- sition on an addressee, often through conveying indirectness or uncertainty on the part of the com- menter. Both commenters in example  B  (Fig.  1 ) employ one such strategy, hedging, perhaps seek- ing to soften an impending disagreement about a source’s reliability (“I  don’t think ...”, “I would assume ...”). We also consider markers of  impo- lite  behavior, such as the use of direct questions (“ Why ’s there no mention of it?’) and sentence- initial second person pronouns (“ Your  sources don’t matter...”), which may serve as forceful- sounding contrasts to negative politeness markers. Following  Danescu-Niculescu-Mizil et al.  ( 2013 ), we extract such strategies by pattern matching on the dependency parses of comments. \n\nTypes of conversation prompts.  To complement our pre-deﬁned set of politeness strategies, we seek to capture domain-speciﬁc rhetorical patterns used to initiate conversations. For instance, in a collaborative setting, we may expect conversations that start with an invitation for working together to signal less tension between the participants than those that start with statements of dispute. We dis- cover types of such  conversation prompts  in an un- supervised fashion by extending a framework used to infer the rhetorical role of questions in (ofﬂine) political debates ( Zhang et al. ,  2017b ) to more generally extract the rhetorical functions of com- ments. The procedure follows the intuition that the rhetorical role of a comment is reﬂected in the type of replies it is likely to elicit. As such, comments which tend to trigger similar replies constitute a particular type of prompt. \nTo implement this intuition, we derive two dif- ferent low-rank representations of the common lexical phrasings contained in comments (agnos- tic to the particular topical content discussed), au- tomatically extracted as recurring sets of arcs in the dependency parses of comments. First, we derive  reply-vectors  of phrasings, which reﬂect their propensities to  co-occur . In particular, we perform singular value decomposition on a term- document matrix    $\\mathcal{R}$   of phrasings and replies as  $\\mathcal{R}\\approx\\hat{\\mathcal{R}}=U_{R}S V_{R}^{T}$  R , where rows of    $U_{R}$   are low- rank reply-vectors for each phrasing. \nNext, we derive  prompt-vectors  for the phras- ings, which reﬂect similarities in the subsequent replies that a phrasing  prompts . We construct a prompt-reply matrix    $\\mathcal{P}=(p_{i j})$   where    $p_{i j}\\,=\\,1$   if phrasing    $j$   occurred in a reply to a comment con- taining phrasing    $i$  . We pro ct    $\\mathcal{P}$   into the same space as    $U_{R}$   by solving for    $\\dot{\\mathcal{P}}$  P  in    $\\mathcal{P}=\\hat{\\mathcal{P}}S V_{R}^{T}$  P   as "}
{"page": 5, "image_path": "doc_images/P18-1125_5.jpg", "ocr_text": "Prompt Type\n\nDescription\n\nExamples\n\nFactual check\n\nModeration\n\nCoordination\n\nCasual remark\n\nStatements about article content, pertaining to or\ncontending issues like factual accuracy.\n\nRebukes or disputes concerning moderation decisions\nsuch as blocks and reversions.\n\nRequests, questions, and statements of intent\npertaining to collaboratively editing an article.\n\nCasual, highly conversational aside-remarks.\n\nThe terms are used interchangeably in the US.\nThe census is not talking about families here.\n\nIf you continue, you may be blocked from editing.\nHe’s accused me of being a troll.\n\nIt’s a long list so I could do with your help.\nLet me know if you agree with this and I'll go ahead [...]\n\nWhat’s with this flag image?\n\nAction statement\nvarious editing actions.\nOpinion\nediting challenges and decisions.\n\nRequests, statements, and explanations about\n\nStatements seeking or expressing opinions about\n\nI’m surprised there wasn’t an article before.\nPlease consider improving the article to address the issues [...]\nThe page was deleted as self-promotion.\n\nI think that it should be the other way around.\nThis article seems to have a lot of bias.\n\nTable 2: Prompt types automatically extracted from talk page conversations, with interpretations and\nexamples from the data. Bolded text indicate common prompt phrasings extracted by the framework.\n\nFurther examples are shown in Appendix B, Table 4.\n\nP = PVpRS~. Each row of P is then a prompt-\nvector of a phrasing, such that the prompt-vector\nfor phrasing is close to the reply-vector for phras-\ning j if comments with phrasing i tend to prompt\nreplies with phrasing j. Clustering the rows of P\nthen yields k conversational prompt types that are\nunified by their similarity in the space of replies.\nTo infer the prompt type of a new comment, we\nrepresent the comment as an average of the repre-\nsentations of its constituent phrasings (i.e., rows of\nP) and assign the resultant vector to a cluster.’\n\nTo determine the prompt types of comments in\nour dataset, we first apply the above procedure to\nderive a set of prompt types from a disjoint (un-\nlabeled) corpus of Wikipedia talk page conversa-\ntions (Danescu-Niculescu-Mizil et al., 2012). Af-\nter initial examination of the framework’s output\non this external data, we chose to extract k = 6\nprompt types, shown in Table 2 along with our in-\nterpretations.® These prompts represent signatures\nof conversation-starters spanning a wide range of\ntopics and contexts which reflect core elements of\nWikipedia, such as moderation disputes and co-\nordination (Kittur et al., 2007; Kittur and Kraut,\n2008). We assign each comment in our present\ndataset to one of these types.”\n\n7We scale rows of Ur and P to unit norm. We assign\ncomments whose vector representation has (£2) distance > 1\nto all cluster centroids to an extra, infrequently-occurring null\ntype which we ignore in subsequent analyses.\n\n’We experimented with more prompt types as well, find-\ning that while the methodology recovered finer-grained types,\nand obtained qualitatively similar results and prediction ac-\ncuracies as described in Sections 5 and 6, the assignment of\ncomments to types was relatively sparse due to the small data\nsize, resulting in a loss of statistical power.\n\n°While the particular prompt types we discover are spe-\n\n5 Analysis\n\nWe are now equipped to computationally explore\nhow the pragmatic devices used to start a con-\nversation can signal its future health. Concretely,\nto quantify the relative propensity of a linguistic\nmarker to occur at the start of awry-turning ver-\nsus on-track conversations, we compute the log-\nodds ratio of the marker occurring in the initial\nexchange—.e., in the first or second comments—\nof awry-turning conversations, compared to initial\nexchanges in the on-track setting. These quantities\nare depicted in Figure 2A.!°\n\nFocusing on the first comment (represented\nas <s), we find a rough correspondence between\nlinguistic directness and the likelihood of future\npersonal attacks. In particular, comments which\ncontain direct questions, or exhibit sentence-\ninitial you (i.e., “ond person start’), tend to start\nawry-turning conversations significantly more of-\nten than ones that stay on track (both p < 0.001). ul\nThis effect coheres with our intuition that direct-\nness signals some latent hostility from the conver-\nsation’s initiator, and perhaps reinforces the force-\nfulness of contentious impositions (Brown and\nLevinson, 1987). This interpretation is also sug-\n\ncific to Wikipedia, the methodology for inferring them is un-\nsupervised and is applicable in other conversational settings.\n\n‘To reduce clutter we only depict features which occur a\nminimum of 50 times and have absolute log-odds > 0.2 in at\nleast one of the data subsets. The markers indicated as statis-\ntically significant for Figure 2A remain so after a Bonferroni\ncorrection, with the exception of factual checks, hedges (lex-\nicon, <>), gratitude (<>), and opinion.\n\n'l All p values in this section are computed as two-tailed bi-\nnomial tests, comparing the proportion of awry-turning con-\nversations exhibiting a particular device to the proportion of\non-track conversations.\n\n1355\n", "vlm_text": "The table categorizes different types of prompts along with their descriptions and examples. Here's a summary:\n\n1. **Factual check**\n   - **Description**: Statements about article content related to factual accuracy.\n   - **Examples**: \n     - \"The terms *are used* interchangeably in the US.\"\n     - \"The census *is not talking about* families here.\"\n\n2. **Moderation**\n   - **Description**: Concerns about moderation decisions, such as blocks and reversions.\n   - **Examples**: \n     - \"If you continue, you may *be blocked* from editing.\"\n     - \"He’s *accused me* of being a troll.\"\n\n3. **Coordination**\n   - **Description**: Requests and questions about collaboratively editing an article.\n   - **Examples**: \n     - \"It’s a long list so I *could do with your help*.\"\n     - \"*Let me know* if you agree with this and I’ll go ahead […]\"\n\n4. **Casual remark**\n   - **Description**: Casual, conversational remarks.\n   - **Examples**: \n     - \"*What’s with* this flag image?\"\n     - \"I’m *surprised* there wasn’t an article before.\"\n\n5. **Action statement**\n   - **Description**: Editing actions requests and explanations.\n   - **Examples**: \n     - \"*Please consider improving* the article to address the issues […]\"\n     - \"The page *was deleted* as self-promotion.\"\n\n6. **Opinion**\n   - **Description**: Expressing opinions about editing challenges and decisions.\n   - **Examples**: \n     - \"I think that it *should be* the other way around.\"\n     - \"This article *seems to have* a lot of bias.\"\n $\\begin{array}{r}{\\hat{\\mathcal P}=\\mathcal P V_{R}S^{-1}}\\end{array}$  P  P . Each row of  $\\hat{\\mathcal P}$  P  is then a prompt- vector of a phrasing, such that the prompt-vector for phrasing  $i$   is close to the reply-vector for phras- ing  $j$   if comments with phrasing    $i$   tend to prompt replies with phrasing    $j$  . Clustering the rows of  $\\bar{\\mathcal P}$  P then yields  $k$   conversational  prompt types  that are uniﬁed by their similarity in the space of replies. To infer the prompt type of a new comment, we represent the comment as an average of the repre- sentations of its constituent phrasings (i.e., rows of  $\\hat{\\mathcal{P}}$  P ) and assign the resultant vector to a cluster. \nTo determine the prompt types of comments in our dataset, we ﬁrst apply the above procedure to derive a set of prompt types from a  disjoint  (un- labeled) corpus of Wikipedia talk page conversa- tions ( Danescu-Niculescu-Mizil et al. ,  2012 ). Af- ter initial examination of the framework’s output on this external data, we chose to extract    $k\\,=\\,6$  prompt types, shown in Table  2  along with our in- terpretations.   These prompts represent signatures of conversation-starters spanning a wide range of topics and contexts which reﬂect core elements of Wikipedia, such as moderation disputes and co- ordination ( Kittur et al. ,  2007 ;  Kittur and Kraut , 2008 ). We assign each comment in our present dataset to one of these types. \n5 Analysis \nWe are now equipped to computationally explore how the pragmatic devices used to start a con- versation can signal its future health. Concretely, to quantify the relative propensity of a linguistic marker to occur at the start of awry-turning ver- sus on-track conversations, we compute the log- odds ratio of the marker occurring in the initial exchange—i.e., in the ﬁrst or second comments— of awry-turning conversations, compared to initial exchanges in the on-track setting. These quantities are depicted in Figure  2 A. \nFocusing on the  ﬁrst  comment (represented as  $\\diamondsuit$  ), we ﬁnd a rough correspondence between linguistic  directness  and the likelihood of future personal attacks. In particular, comments which contain  direct questions , or exhibit  sentence- initial you  (i.e.,   $\\mathrm{{}^{\\ast}2^{n d}}$    person start”), tend to start awry-turning conversations signiﬁcantly more of- ten than ones that stay on track (both    $p<0.001]$  ). This effect coheres with our intuition that direct- ness signals some latent hostility from the conver- sation’s initiator, and perhaps reinforces the force- fulness of contentious impositions ( Brown and Levinson ,  1987 ). This interpretation is also sug- "}
{"page": 6, "image_path": "doc_images/P18-1125_6.jpg", "ocr_text": "Direct question\nweek ee\n\n2°¢ person start\n\nwore ba\n\nPrompt: Factual check\n*\n\nPlease start\n\n2\"4 person\n\n1st person start\nPrompt: Coordination\n\nee\n\nHedge (lexicon)\nee\n\nGratitude\n\nees\n\nHedge (dep. tree)\neee\n\nPrompt: Opinion\nPp’ pl ¢o\nGreetings\nwoe | 60\n\nv\n\nev\n\nev\n\nov\n\nev\nva\nev\nvq\nv oO\n\noO\n\non-track -0.5 0\nlog-odds ratio\n\n0.5 awry\n\nA. First & second comment\n\non-track -0.5 0\n\nB. Attacker initiated\n\non-track -0.5 0\nlog-odds ratio\n\n0.5 awry 0.5 awry\n\nlog-odds ratio\n\nC. Non-attacker initiated\n\nFigure 2: Log-odds ratios of politeness strategies and prompt types exhibited in the first and second\ncomments of conversations that turn awry, versus those that stay on-track. All: Purple and green markers\ndenote log-odds ratios in the first and second comments, respectively; points are solid if they reflect\n\nsignificant (p < 0.05) log-odds ratios with an effect size of at least 0.2. A: <>s and\n\ns denote first\n\nand second comment log-odds ratios, respectively; * denotes statistically significant differences at the\np < 0.05 (*), p < 0.01 (**) and p < 0.001 (***) levels for the first comment (two-tailed binomial test); +\ndenotes corresponding statistical significance for the second comment. B and C: Vs and Os correspond\nto effect sizes in the comments authored by the attacker and non-attacker, respectively, in attacker\ninitiated (B) and non-attacker initiated (C) conversations.\n\ngested by the relative propensity of the factual\ncheck prompt, which tends to cue disputes re-\ngarding an article’s factual content (p < 0.05).\n\nIn contrast, comments which initiate on-track\nconversations tend to contain gratitude (p < 0.05)\nand greetings (p < 0.001), both positive polite-\nness strategies. Such conversations are also\nmore likely to begin with coordination\nprompts (p < 0.05), signaling active efforts to\nfoster constructive teamwork. Negative polite-\nness strategies are salient in on-track conversa-\ntions as well, reflected by the use of hedges\n(p < 0.01) and opinion prompts (p < 0.05),\nwhich may serve to soften impositions or factual\ncontentions (Hiibler, 1983).\n\nin the second\nthe first reply (represented\ns). Interestingly, in this case we note that\nthe difference in pronoun use is especially marked.\nFirst replies in conversations that eventually de-\n\nThese effects are echoed\ncomment—i.e.,\nas\n\nrail tend to contain more second person pro-\nnouns (p < 0.001), perhaps signifying a replier\npushing back to contest the initiator; in con-\ntrast, on-track conversations have more sentence-\ninitial I/We (i.e., “1° person start”, p < 0.001), po-\ntentially indicating the replier’s willingness to step\ninto the conversation and work with—rather than\nargue against—the initiator (Tausczik and Pen-\nnebaker, 2010).\n\nDistinguishing interlocutor behaviors. Are the\nlinguistic signals we observe solely driven by the\neventual attacker, or do they reflect the behavior of\nboth actors? To disentangle the attacker and non-\nattackers’ roles in the initial exchange, we exam-\nine their language use in these two possible cases:\nwhen the future attacker initiates the conversation,\nor is the first to reply. In attacker-initiated con-\nversations (Figure 2B, 608 conversations), we see\nthat both actors exhibit a propensity for the lin-\nguistically direct markers (e.g., direct questions)\n\n1356\n", "vlm_text": "The image contains three panels showing log-odds ratios of politeness strategies and prompt types in conversation comments. The panels compare conversations that go awry versus those that stay on track.\n\n### Key Features:\n\n- **Panels A, B, C**: \n  - **A**: First & second comment comparisons.\n  - **B**: Attacker-initiated conversations.\n  - **C**: Non-attacker initiated conversations.\n\n- **Markers**:\n  - Purple and green markers are used to denote log-odds ratios for the first and second comments, respectively.\n  - Solid points indicate statistically significant log-odds ratios \\((p<0.05)\\) with an effect size of at least 0.2.\n\n- **Symbols**:\n  - \\(\\diamondsuit\\) and \\(\\sqsupset\\!\\)s represent the first and second comment log-odds ratios.\n  - Statistical significance symbols:\n    - * for \\(p<0.05\\)\n    - ** for \\(p<0.01\\)\n    - *** for \\(p<0.001\\)\n  - \\(+\\) denotes significance for second comment differences.\n\n- **Axis Information**:\n  - X-axis: Log-odds ratio ranging from on-track to awry.\n  \n- **Categories Assessed**:\n  - Includes direct questions, type of initial address (second person start), prompts (e.g., factual checks), hedges, gratitude, and greetings.\n\nThese analyses help understand the relationship between politeness strategies and conversation outcomes.\ngested by the relative propensity of the  factual check  prompt, which tends to cue disputes re- garding an article’s factual content   $(p<0.05)$  . \nIn contrast, comments which initiate on-track conversations tend to contain  gratitude    $(p<0.05)$  and  greetings    $(p<0.001)$  , both positive polite- ness strategies. Such conversations are also more likely to begin with  coordination prompts   $(p<0.05)$  , signaling active efforts to foster constructive teamwork. Negative polite- ness strategies are salient in on-track conversa- tions as well, reﬂected by the use of  hedges  $(p<0.01)$   and  opinion  prompts   $(p<0.05)$  , which may serve to soften impositions or factual contentions ( H¨ ubler ,  1983 ). \nThese effects are echoed in the second comment—i.e., the ﬁrst reply (represented as    $\\boxdot$  ). Interestingly, in this case we note that the difference in pronoun use is especially marked. First replies in conversations that eventually de- rail tend to contain more  second person pro- nouns    $(p<0.001)$  , perhaps signifying a replier pushing back to contest the initiator; in con- trast, on-track conversations have more  sentence- initial I/We  (i.e.,   $^{**}1^{\\mathrm{st}}$    person start”,  $p<0.001)$  ), po- tentially indicating the replier’s willingness to step into the conversation and work with—rather than argue against—the initiator ( Tausczik and Pen- nebaker ,  2010 ). \n\nDistinguishing interlocutor behaviors.  Are the linguistic signals we observe solely driven by the eventual attacker, or do they reﬂect the behavior of both actors? To disentangle the attacker and non- attackers’ roles in the initial exchange, we exam- ine their language use in these two possible cases: when the  future  attacker initiates the conversation, or is the ﬁrst to reply. In  attacker-initiated  con- versations (Figure  2 B, 608 conversations), we see that both actors exhibit a propensity for the lin- guistically direct markers (e.g.,  direct questions ) "}
{"page": 7, "image_path": "doc_images/P18-1125_7.jpg", "ocr_text": "that tend to signal future attacks. Some of these\nmarkers are used particularly often by the non-\nattacking replier in awry-turning conversations\n(e.g., second person pronouns, p < 0.001, Qs),\nfurther suggesting the dynamic of the replier push-\ning back at—and perhaps even escalating—the at-\ntacker’s initial hint of aggression. Among conver-\nsations initiated instead by the non-attacker (Fig-\nure 2C, 662 conversations), the non-attacker’s lin-\nguistic behavior in the first comment (Qs) is less\ndistinctive from that of initiators in the on-track\nsetting (i.e., log-odds ratios closer to 0); mark-\ners of future derailment are (unsurprisingly) more\npronounced once the eventual attacker (Vs) joins\nthe conversation in the second comment.”\n\nMore broadly, these results reveal how differ-\nent politeness strategies and rhetorical prompts de-\nployed in the initial stages of a conversation are\ntied to its future trajectory.\n\n6 Predicting Future Attacks\n\nWe now show that it is indeed feasible to predict\nwhether a conversation will turn awry based on\nlinguistic properties of its very first exchange, pro-\nviding several baselines for this new task. In do-\ning so, we demonstrate that the pragmatic devices\nexamined above encode signals about the future\ntrajectory of conversations, capturing some of the\nintuition humans are shown to have.\nWe consider the following balanced prediction\ntask: given a pair of conversations, which one\nwill eventually lead to a personal attack? We ex-\ntract all features from the very first exchange in\na conversation—i.e., a comment-reply pair, like\nthose illustrated in our introductory example (Fig-\nure 1). We use logistic regression and report ac-\ncuracies on a leave-one-page-out cross validation,\nsuch that in each fold, all conversation pairs from\na given talk page are held out as test data and pairs\nfrom all other pages are used as training data (thus\npreventing the use of page-specific information).\nPrediction results are summarized in Table 3.\nLanguage baselines. As baselines, we con-\nsider several straightforward features: word count\n(which performs at chance level), sentiment lexi-\ncon (Liu et al., 2005) and bag of words.\nPragmatic features. Next, we test the predic-\ntive power of the prompt types and politeness\n\nAs an interesting avenue for future work, we note that\nsome markers used by non-attacking initiators potentially still\nanticipate later attacks, suggested by, e.g., the relative preva-\nlence of sentence-initial you (p < 0.05, Os).\n\nFeature set #features Accuracy\nBag of words 5,000 56.7%\nSentiment lexicon 4 55.4%\nPoliteness strategies 38 60.5%\nPrompt types 12 59.2%\nPragmatic (all) 50 61.6%\nInterlocutor features 5 51.2%\nTrained toxicity 2 60.5%\nToxicity + Pragmatic 52 64.9%\nHumans 72.0%\nTable 3: Accuracies for the balanced future-\n\nprediction task. Features based on pragmatic de-\nvices are bolded, reference points are italicized.\n\nstrategies features introduced in Section 4. The\n12 prompt type features (6 features for each com-\nment in the initial exchange) achieve 59.2% accu-\nracy, and the 38 politeness strategies features (19\nper comment) achieve 60.5% accuracy. The prag-\nmatic features combine to reach 61.6% accuracy.\nReference points. To better contextualize the per-\nformance of our features, we compare their pre-\ndictive accuracy to the following reference points:\nInterlocutor features: Certain kinds of interlocu-\ntors are potentially more likely to be involved in\nawry-turning conversations. For example, perhaps\nnewcomers or anonymous participants are more\nlikely to derail interactions than more experienced\neditors. We consider a set of features representing\nparticipants’ experience on Wikipedia (i.e., num-\nber of edits) and whether the comment authors are\nanonymous. In our task, these features perform at\nthe level of random chance.\n\nTrained toxicity: We also compare with the tox-\nicity score of the exchange from the Perspective\nAPI classifier—a perhaps unfair reference point,\nsince this supervised system was trained on addi-\ntional human-labeled training examples from the\nsame domain and since it was used to create the\nvery data on which we evaluate. This results in\nan accuracy of 60.5%; combining trained toxicity\nwith our pragmatic features achieves 64.9%.\nHumans: A sample of 100 pairs were labeled by\n(non-author) volunteer human annotators. They\nwere asked to guess, from the initial exchange,\nwhich conversation in a pair will lead to a personal\nattack. Majority vote across three annotators was\nused to determine the human labels, resulting in an\naccuracy of 72%. This confirms that humans have\n\n1357\n", "vlm_text": "that tend to signal future attacks. Some of these markers are used particularly often by the  non- attacking replier  in awry-turning conversations (e.g.,  second person pronouns ,    $p<0.001$  ,    $\\mathrm{Cs})$  , further suggesting the dynamic of the replier push- ing back at—and perhaps even escalating—the at- tacker’s initial hint of aggression. Among conver- sations initiated instead by the  non-attacker  (Fig- ure  2 C, 662 conversations), the non-attacker’s lin- guistic behavior in the ﬁrst comment   $({\\bigcirc}\\mathbf{s})$   is less distinctive from that of initiators in the on-track setting (i.e., log-odds ratios closer to 0); mark- ers of future derailment are (unsurprisingly) more pronounced once the eventual attacker   $\\left(\\bigtriangledown\\mathbf{s}\\right)$   joins the conversation in the second comment. \nMore broadly, these results reveal how differ- ent politeness strategies and rhetorical prompts de- ployed in the initial stages of a conversation are tied to its future trajectory. \n6 Predicting Future Attacks \nWe now show that it is indeed feasible to predict whether a conversation will turn awry based on linguistic properties of its very ﬁrst exchange, pro- viding several baselines for this new task. In do- ing so, we demonstrate that the pragmatic devices examined above encode signals about the future trajectory of conversations, capturing some of the intuition humans are shown to have. \nWe consider the following balanced prediction task: given a pair of conversations, which one will eventually lead to a personal attack? We ex- tract all features from the very ﬁrst exchange in a conversation—i.e., a comment-reply pair, like those illustrated in our introductory example (Fig- ure  1 ). We use logistic regression and report ac- curacies on a leave-one-page-out cross validation, such that in each fold, all conversation pairs from a given talk page are held out as test data and pairs from all other pages are used as training data (thus preventing the use of page-speciﬁc information). Prediction results are summarized in Table  3 . \nLanguage baselines. As baselines, we con- sider several straightforward features: word count (which performs at chance level), sentiment lexi- con ( Liu et al. ,  2005 ) and bag of words. \nPragmatic features. Next, we test the predic- tive power of the  prompt types  and  politeness \nThe table outlines different feature sets, the number of features in each set, and their corresponding accuracy percentages. Here's a summary:\n\n- **Bag of words:** 5,000 features, 56.7% accuracy\n- **Sentiment lexicon:** 4 features, 55.4% accuracy\n- **Politeness strategies:** 38 features, 60.5% accuracy\n- **Prompt types:** 12 features, 59.2% accuracy\n- **Pragmatic (all):** 50 features, 61.6% accuracy\n- **Interlocutor features:** 5 features, 51.2% accuracy\n- **Trained toxicity:** 2 features, 60.5% accuracy\n- **Toxicity + Pragmatic:** 52 features, 64.9% accuracy\n- **Humans:** 72.0% accuracy\n\nIt looks like the table compares different methods or features for a task, showing how many features are used and how accurately they perform.\nstrategies  features introduced in Section  4 . The 12 prompt type features (6 features for each com- ment in the initial exchange) achieve   $59.2\\%$   accu- racy, and the 38 politeness strategies features (19 per comment) achieve   $60.5\\%$   accuracy. The  prag- matic  features combine to reach   $61.6\\%$   accuracy. Reference points.  To better contextualize the per- formance of our features, we compare their pre- dictive accuracy to the following reference points: Interlocutor features:  Certain kinds of interlocu- tors are potentially more likely to be involved in awry-turning conversations. For example, perhaps newcomers or anonymous participants are more likely to derail interactions than more experienced editors. We consider a set of features representing participants’ experience on Wikipedia (i.e., num- ber of edits) and whether the comment authors are anonymous. In our task, these features perform at the level of random chance. \nTrained toxicity:  We also compare with the tox- icity score of the exchange from the Perspective API classiﬁer—a perhaps unfair reference point, since this supervised system was trained on addi- tional human-labeled training examples from the same domain and since it was used to create the very data on which we evaluate. This results in an accuracy of   $60.5\\%$  ; combining trained toxicity with our pragmatic features achieves   $64.9\\%$  . \nHumans:  A sample of 100 pairs were labeled by (non-author) volunteer human annotators. They were asked to guess, from the initial exchange, which conversation in a pair will lead to a personal attack. Majority vote across three annotators was used to determine the human labels, resulting in an accuracy of  $72\\%$  . This conﬁrms that humans have some intuition about whether a conversation might be heading in a bad direction, which our features can partially capture. In fact, the classiﬁer using pragmatic features is accurate on  $80\\%$   of the ex- amples that humans also got right. "}
{"page": 8, "image_path": "doc_images/P18-1125_8.jpg", "ocr_text": "some intuition about whether a conversation might\nbe heading in a bad direction, which our features\ncan partially capture. In fact, the classifier using\npragmatic features is accurate on 80% of the ex-\namples that humans also got right.\nAttacks on the horizon. Finally, we seek to un-\nderstand whether cues extracted from the first ex-\nchange can predict future discussion trajectory be-\nyond the immediate next couple of comments. We\nthus repeat the prediction experiments on the sub-\nset of conversations in which the first personal at-\ntack happens after the fourth comment (282 pairs),\nand find that the pragmatic devices used in the first\nexchange maintain their predictive power (67.4%\naccuracy), while the sentiment and bag of words\nbaselines drop to the level of random chance.\nOverall, these initial results show the feasibil-\nity of reconstructing some of the human intuition\nabout the future trajectory of an ostensibly civil\nconversation in order to predict whether it will\neventually turn awry.\n\n7 Conclusions and Future Work\n\nIn this work, we started to examine the intriguing\nphenomenon of conversational derailment, study-\ning how the use of pragmatic and rhetorical de-\nvices relates to future conversational failure. Our\ninvestigation centers on the particularly perplex-\ning scenario in which one participant of a civil\ndiscussion later attacks another, and explores the\nnew task of predicting whether an initially healthy\nconversation will derail into such an attack. To\nthis end, we develop a computational framework\nfor analyzing how general politeness strategies\nand domain-specific rhetorical prompts deployed\nin the initial stages of a conversation are tied to its\nfuture trajectory.\n\nMaking use of machine learning and crowd-\nsourcing tools, we formulate a tightly-controlled\nsetting that enables us to meaningfully compare\nconversations that stay on track with those that go\nawry. The human accuracy on predicting future at-\ntacks in this setting (72%) suggests it is feasible at\nleast at the level of human intuition. We show that\nour computational framework can recover some of\nthat intuition, hinting at the potential of automated\nmethods to identify signals of the future trajecto-\nries of online conversations.\n\nOur approach has several limitations which\nopen avenues for future work. Our correlational\nanalyses do not provide any insights into causal\n\nmechanisms of derailment, which randomized ex-\nperiments could address. Additionally, since our\nprocedure for collecting and vetting data focused\non precision rather than recall, it might miss more\nsubtle attacks that are overlooked by the toxicity\nclassifier. Supplementing our investigation with\nother indicators of antisocial behavior, such as ed-\nitors blocking one another, could enrich the range\nof attacks we study. Noting that our framework\nis not specifically tied to Wikipedia, it would also\nbe valuable to explore the varied ways in which\nthis phenomenon arises in other (possibly non-\ncollaborative) public discussion venues, such as\nReddit and Facebook Pages.\n\nWhile our analysis focused on the very first ex-\nchange in a conversation for the sake of general-\nity, more complex modeling could extend its scope\nto account for conversational features that more\ncomprehensively span the interaction. Beyond the\npresent binary classification task, one could ex-\nplore a sequential formulation predicting whether\nthe next turn is likely to be an attack as a discus-\nsion unfolds, capturing conversational dynamics\nsuch as sustained escalation.\n\nFinally, our study of derailment offers only\none glimpse into the space of possible conversa-\nional trajectories. Indeed, a manual investiga-\nion of conversations whose eventual trajectories\nwere misclassified by our models—as well as by\nhe human annotators—suggests that interactions\nwhich initially seem prone to attacks can nonethe-\nless maintain civility, by way of level-headed in-\nerlocutors, as well as explicit acts of reparation.\nA promising line of future work could consider the\ncomplementary problem of identifying pragmatic\nstrategies that can help bring uncivil conversations\nback on track.\n\nAcknowledgements. We are grateful to the\nanonymous reviewers for their thoughtful com-\nments and suggestions, and to Maria Antoniak,\nValts Blukis, Liye Fu, Sam Havron, Jack Hessel,\nIshaan Jhaveri, Lillian Lee, Alex Niculescu-Mizil,\nAlexandra Schofield, Laure Thompson, Andrew\nWang, Leila Zia and the members of the Wikime-\ndia Foundation anti-harassment program for ex-\ntremely insightful (on-track) conversations and for\nassisting with data annotation efforts. This work\nis supported in part by NSF CAREER Award IIS-\n1750615, NSF Grant SES-1741441, a Google Fac-\nulty Award, a WMF gift and a CrowdFlower AI for\nEveryone Award.\n\n1358\n", "vlm_text": "\nAttacks on the horizon.  Finally, we seek to un- derstand whether cues extracted from the ﬁrst ex- change can predict future discussion trajectory be- yond the immediate next couple of comments. We thus repeat the prediction experiments on the sub- set of conversations in which the ﬁrst personal at- tack happens after the fourth comment (282 pairs), and ﬁnd that the pragmatic devices used in the ﬁrst exchange maintain their predictive power   $(67.4\\%$  accuracy), while the sentiment and bag of words baselines drop to the level of random chance. \nOverall, these initial results show the feasibil- ity of reconstructing some of the human intuition about the future trajectory of an ostensibly civil conversation in order to predict whether it will eventually turn awry. \n7 Conclusions and Future Work \nIn this work, we started to examine the intriguing phenomenon of conversational derailment, study- ing how the use of pragmatic and rhetorical de- vices relates to future conversational failure. Our investigation centers on the particularly perplex- ing scenario in which one participant of a civil discussion later attacks another, and explores the new task of predicting whether an initially healthy conversation will derail into such an attack. To this end, we develop a computational framework for analyzing how general politeness strategies and domain-speciﬁc rhetorical prompts deployed in the initial stages of a conversation are tied to its future trajectory. \nMaking use of machine learning and crowd- sourcing tools, we formulate a tightly-controlled setting that enables us to meaningfully compare conversations that stay on track with those that go awry. The human accuracy on predicting future at- tacks in this setting   $(72\\%)$   suggests it is feasible at least at the level of human intuition. We show that our computational framework can recover some of that intuition, hinting at the potential of automated methods to identify signals of the future trajecto- ries of online conversations. \nOur approach has several limitations which open avenues for future work. Our correlational analyses do not provide any insights into  causal mechanisms of derailment, which randomized ex- periments could address. Additionally, since our procedure for collecting and vetting data focused on precision rather than recall, it might miss more subtle attacks that are overlooked by the toxicity classiﬁer. Supplementing our investigation with other indicators of antisocial behavior, such as ed- itors blocking one another, could enrich the range of attacks we study. Noting that our framework is not speciﬁcally tied to Wikipedia, it would also be valuable to explore the varied ways in which this phenomenon arises in other (possibly non- collaborative) public discussion venues, such as Reddit and Facebook Pages. \n\nWhile our analysis focused on the very ﬁrst ex- change in a conversation for the sake of general- ity, more complex modeling could extend its scope to account for conversational features that more comprehensively span the interaction. Beyond the present binary classiﬁcation task, one could ex- plore a sequential formulation predicting whether the next turn is likely to be an attack as a discus- sion unfolds, capturing conversational dynamics such as sustained escalation. \nFinally, our study of derailment offers only one glimpse into the space of possible conversa- tional trajectories. Indeed, a manual investiga- tion of conversations whose eventual trajectories were misclassiﬁed by our models—as well as by the human annotators—suggests that interactions which initially seem prone to attacks can nonethe- less maintain civility, by way of level-headed in- terlocutors, as well as explicit acts of reparation. A promising line of future work could consider the complementary problem of identifying pragmatic strategies that can help bring uncivil conversations back on track. \nAcknowledgements. We are grateful to the anonymous reviewers for their thoughtful com- ments and suggestions, and to Maria Antoniak, Valts Blukis, Liye Fu, Sam Havron, Jack Hessel, Ishaan Jhaveri, Lillian Lee, Alex Niculescu-Mizil, Alexandra Schoﬁeld, Laure Thompson, Andrew Wang, Leila Zia and the members of the Wikime- dia Foundation anti-harassment program for ex- tremely insightful (on-track) conversations and for assisting with data annotation efforts. This work is supported in part by NSF CAREER Award IIS- 1750615, NSF Grant SES-1741441, a Google Fac- ulty Award, a WMF gift and a CrowdFlower AI for Everyone Award. "}
{"page": 9, "image_path": "doc_images/P18-1125_9.jpg", "ocr_text": "References\n\nYavuz Akbulut, Yusuf Levent Sahin, and Bahadir\nEristi. 2010. Cyberbullying victimization among\nTurkish online social utility members. Journal of\nEducational Technology & Society.\n\nKelsey Allen, Giuseppe Carenini, and Raymond T Ng.\n2014. Detecting disagreement in conversations us-\ning pseudo-monologic rhetorical structure. In Pro-\nceedings of EMNLP.\n\nTim Althoff, Cristian Danescu-Niculescu-Mizil, and\nDan Jurafsky. 2014. How to ask for a favor: A case\nstudy on the success of altruistic requests. In Pro-\nceedings of ICWSM.\n\nOfer Arazy, Lisa Yeo, and Oded Nov. 2013. Stay on the\nWikipedia task: When task-related disagreements\nslip into personal and procedural conflicts. Journal\nof the Association for Information Science and Tech-\nnology.\n\nMalika Aubakirova and Mohit Bansal. 2016. Interpret-\ning neural networks to improve politeness compre-\nhension. In Proceedings of EMNLP.\n\nLars Backstrom, Jon Kleinberg, Lillian Lee, and Cris-\ntian Danescu-Niculescu-Mizil. 2013. Characteriz-\ning and curating conversation threads: Expansion,\nfocus, volume, re-entry. In Proceedings of WSDM.\n\nPenelope Brown and Stephen Levinson. 1987. Polite-\nness: Some universals in language usage. Cam-\nbridge University Press.\n\nMoira Burke and Robert Kraut. 2008. Mind your Ps\nand Qs: The impact of politeness and rudeness in\nonline communities. In Proceedings of CSCW.\n\nEshwar Chandrasekharan, Umashanthi Pavalanathan,\nAnirudh Srinivasan, Adam Glynn, Jacob Eisenstein,\nand Eric Gilbert. 2017. You can’t stay here: The ef-\nficacy of Reddit’s 2015 ban examined through hate\nspeech. In Proceedings of CSCW.\n\nDespoina Chatzakou, Nicolas Kourtellis,\nBlackburn, Emiliano De Cristofaro, Gianluca\nStringhini, and Athena Vakali. 2017. Measuring\n#GamerGate: A tale of hate, sexism, and bullying.\nIn Proceedings of WWW.\n\nJeremy\n\nJustin Cheng, Michael Bernstein, Cristian Danescu-\nNiculescu-Mizil, and Jure Leskovec. 2017. Anyone\ncan become a troll: Causes of trolling behavior in\nonline discussions. In Proceedings of CSCW.\n\nJustin Cheng, Cristian Danescu-Niculescu-Mizil, and\nJure Leskovec. 2015. Antisocial behavior in online\ndiscussion communities. In Proceedings of ICWSM.\n\nElizabeth F Churchill and Sara Bly. 2000. Culture vul-\ntures: Considering culture and communication in\nvirtual environments. S/GGroup Bulletin.\n\nHerbert H Clark. 1979. Responding to indirect speech\nacts. Cognitive psychology.\n\nHerbert H Clark and Dale H Schunk. 1980. Polite re-\nsponses to polite requests. Cognition.\n\nBenjamin Collier and Julia Bear. 2012. Conflict, crit-\nicism, or confidence: An empirical examination of\nthe gender gap in Wikipedia contributions. In Pro-\nceedings of CSCW.\n\nLewis A Coser. 1956. The Functions of Social Conflict.\nRoutledge.\n\nCristian Danescu-Niculescu-Mizil, Lillian Lee,\nBo Pang, and Jon Kleinberg. 2012. Echoes of\npower: Language effects and power differences in\nsocial interaction. In Proceedings of WWW.\n\nCristian Danescu-Niculescu-Mizil, Moritz Sudhof,\nDan Jurafsky, Jure Leskovec, and Christopher Potts.\n2013. A computational approach to politeness with\napplication to social factors. In Proceedings of ACL.\n\nThomas Davidson, Dana Warmsley, Michael Macy,\nand Ingmar Weber. 2017. Automated hate speech\ndetection and the problem of offensive language. In\nProceedings of ICWSM.\n\nCarsten K De Dreu and Laurie R Weingart. 2003. Task\nversus relationship conflict, team performance, and\nteam member satisfaction: A meta-analysis. Jour-\nnal of Applied Psychology.\n\nBruce Fraser. 1980. Conversational mitigation. Jour-\nnal of Pragmatics.\n\nLiye Fu, Lillian Lee, and Cristian Danescu-Niculescu-\nMizil. 2017. When confidence and competence col-\nlide: Effects on online decision-making discussions.\nIn Proceedings of WWW.\n\nBjérn Gambick and Utpal Kumar Sikdar. 2017. Us-\ning convolutional neural networks to classify hate-\nspeech. In Proceedings of the Workshop on Abusive\nLanguage Online.\n\nAli Gheitasy, José Abdelnour-Nocera, and Bonnie\nNardi. 2015. Socio-technical gaps in online collabo-\nrative consumption (OCC): An example of the Etsy\ncommunity. In Proceedings of ICDC.\n\nErving Goffman. 1955. On face-work: An analysis of\nritual elements in social interaction. Psychiatry.\n\nChristophe Henner and Maria Sefidari. 2016. Wikime-\ndia Foundation Board on healthy Wikimedia com-\nmunity culture, inclusivity, and safe spaces. Wiki-\nmedia Blog.\n\nPamela J Hinds and Mark Mortensen. 2005. Un-\nderstanding conflict in geographically distributed\nteams: The moderating effects of shared identity,\nshared context, and spontaneous communication.\nOrganization Science.\n\nAxel Hiibler. 1983. Understatements and Hedges in\nEnglish. John Benjamins Publishing.\n\n1359\n", "vlm_text": "References \nYavuz Akbulut, Yusuf Levent Sahin, and Bahadir Eristi. 2010. Cyberbullying victimization among Turkish online social utility members. Journal of Educational Technology & Society . Kelsey Allen, Giuseppe Carenini, and Raymond T Ng. 2014. Detecting disagreement in conversations us- ing pseudo-monologic rhetorical structure. In  Pro- ceedings of EMNLP . Tim Althoff, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2014. How to ask for a favor: A case study on the success of altruistic requests. In  Pro- ceedings of ICWSM . Ofer Arazy, Lisa Yeo, and Oded Nov. 2013. Stay on the Wikipedia task: When task-related disagreements slip into personal and procedural conﬂicts.  Journal of the Association for Information Science and Tech- nology . Malika Aubakirova and Mohit Bansal. 2016. Interpret- ing neural networks to improve politeness compre- hension. In  Proceedings of EMNLP . Lars Backstrom, Jon Kleinberg, Lillian Lee, and Cris- tian Danescu-Niculescu-Mizil. 2013. Characteriz- ing and curating conversation threads: Expansion, focus, volume, re-entry. In  Proceedings of WSDM . Penelope Brown and Stephen Levinson. 1987.  Polite- ness: Some universals in language usage . Cam- bridge University Press. Moira Burke and Robert Kraut. 2008. Mind your Ps and Qs: The impact of politeness and rudeness in online communities. In  Proceedings of CSCW . Eshwar Chandrasekharan, Umashanthi Pavalanathan, Anirudh Srinivasan, Adam Glynn, Jacob Eisenstein, and Eric Gilbert. 2017. You can’t stay here: The ef- ﬁcacy of Reddit’s 2015 ban examined through hate speech. In  Proceedings of CSCW . Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, and Athena Vakali. 2017. Measuring #GamerGate: A tale of hate, sexism, and bullying. In  Proceedings of WWW . Justin Cheng, Michael Bernstein, Cristian Danescu- Niculescu-Mizil, and Jure Leskovec. 2017. Anyone can become a troll: Causes of trolling behavior in online discussions. In  Proceedings of CSCW . Justin Cheng, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec. 2015. Antisocial behavior in online discussion communities. In  Proceedings of ICWSM . Elizabeth F Churchill and Sara Bly. 2000. Culture vul- tures: Considering culture and communication in virtual environments.  SIGGroup Bulletin . Herbert H Clark. 1979. Responding to indirect speech acts.  Cognitive psychology . \nHerbert H Clark and Dale H Schunk. 1980. Polite re- \nsponses to polite requests.  Cognition . Benjamin Collier and Julia Bear. 2012. Conﬂict, crit- icism, or conﬁdence: An empirical examination of the gender gap in Wikipedia contributions. In  Pro- ceedings of CSCW . Lewis A Coser. 1956.  The Functions of Social Conﬂict . Routledge. Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg. 2012. Echoes of power: Language effects and power differences in social interaction. In  Proceedings of WWW . Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. 2013. A computational approach to politeness with application to social factors. In  Proceedings of ACL . Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings of ICWSM . Carsten K De Dreu and Laurie R Weingart. 2003. Task versus relationship conﬂict, team performance, and team member satisfaction: A meta-analysis.  Jour- nal of Applied Psychology . Bruce Fraser. 1980. Conversational mitigation.  Jour- nal of Pragmatics . Liye Fu, Lillian Lee, and Cristian Danescu-Niculescu- Mizil. 2017. When conﬁdence and competence col- lide: Effects on online decision-making discussions. In  Proceedings of WWW . Bj¨ orn Gamb¨ ack and Utpal Kumar Sikdar. 2017. Us- ing convolutional neural networks to classify hate- speech. In  Proceedings of the Workshop on Abusive Language Online . Ali Gheitasy, Jos´ e Abdelnour-Nocera, and Bonnie Nardi. 2015. Socio-technical gaps in online collabo- rative consumption (OCC): An example of the Etsy community. In  Proceedings of ICDC . Erving Goffman. 1955. On face-work: An analysis of ritual elements in social interaction.  Psychiatry . Christophe Henner and Maria Seﬁdari. 2016.  Wikime- dia Foundation Board on healthy Wikimedia com- munity culture, inclusivity, and safe spaces . Wiki- media Blog. Pamela J Hinds and Mark Mortensen. 2005. Un- derstanding conﬂict in geographically distributed teams: The moderating effects of shared identity, shared context, and spontaneous communication. Organization Science . Axel H¨ ubler. 1983.  Understatements and Hedges in English . John Benjamins Publishing. "}
{"page": 10, "image_path": "doc_images/P18-1125_10.jpg", "ocr_text": "Joseph M Kayany. 1998. Contexts of uninhibited on-\nline behavior: Flaming in social newsgroups on\nusenet. Journal of the Association for Information\nScience and Technology.\n\nAniket Kittur, Ed H Chi, and Bongwon Suh. 2009.\nWhat’s in Wikipedia?: Mapping topics and conflict\nusing socially annotated category structure. In Pro-\nceedings of CHI.\n\nAniket Kittur and Robert E Kraut. 2008. Harnessing\nthe wisdom of crowds in Wikipedia: Quality through\ncoordination. In Proceedings of CSCW.\n\nAniket Kittur, Bongwon Suh, Bryan A Pendleton, and\nEd H Chi. 2007. He says, she says: Conflict and\ncoordination in Wikipedia. In Proceedings of CHI.\n\nVinodh Krishnan and Jacob Eisenstein. 2015. “You’re\nMr. Lebowski, I’m the Dude”: Inducing address\nterm formality in signed social networks. In Pro-\nceedings of NAACL.\n\nRavi Kumar, Mohammad Mahdian, and Mary McGlo-\nhon. 2010. Dynamics of conversations. In Proceed-\nings of KDD.\n\nHaewoon Kwak, Jeremy Blackburn, and Seungyeop\nHan. 2015. Exploring cyberbullying and other toxic\nbehavior in team competition online games. In Pro-\nceedings of CHI.\n\nRobin T Lakoff. 1973. The logic of politeness: Mind-\ning your P’s and Q’s. In Proceedings of the Chicago\nLinguistic Society.\n\nBing Liu, Minging Hu, and Junsheng Cheng. 2005.\nOpinion observer: Analyzing and comparing opin-\nions on the web. In Proceedings of WWW.\n\nVlad Niculae and Cristian Danescu-Niculescu-Mizil.\n2016. Conversational markers of constructive dis-\ncussions. In Proceedings of NAACL.\n\nVlad Niculae, Srijan Kumar, Jordan Boyd-Graber, and\nCristian Danescu-Niculescu-Mizil. 2015. Linguistic\nharbingers of betrayal: A case study on an online\nstrategy game. In Proceedings of ACL.\n\nChikashi Nobata, Joel Tetreault, Achint Thomas,\nYashar Mehdad, and Yi Chang. 2016. Abusive lan-\nguage detection in online user content. In Proceed-\nings of WWW.\n\nMarco Ortu, Bram Adams, Giuseppe Destefanis, Paras-\ntou Tourani, Michele Marchesi, and Roberto Tonelli.\n2015. Are bullies more productive? Empirical study\nof affectiveness vs. issue fixing time. In Proceedings\nof MSR.\n\nJohn Pavlopoulos, Prodromos Malakasiotis, and Ion\nAndroutsopoulos. 2017a. Deep learning for user\ncomment moderation. In Proceedings of the Work-\nshop on Abusive Language Online.\n\nJohn Pavlopoulos, Prodromos Malakasiotis, and Ion\nAndroutsopoulos. 2017b. Deeper attention to abu-\nsive user content moderation. In Proceedings of\nEMNLP.\n\nAlan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-\npervised modeling of Twitter conversations. In Pro-\nceedings of NAACL.\n\nPaul R Rosenbaum. 2010. Design of observational\nstudies. Springer.\n\nDonald B Rubin. 2007. The design versus the analysis\nof observational studies for causal effects: Parallels\nwith the design of randomized trials. Statistics in\nMedicine.\n\nTamara Shepherd, Alison Harvey, Tim Jordan, Sam\nSrauy, and Kate Miltner. 2015. Histories of hating.\nSocial Media + Society.\n\nVivek K Singh, Marie L Radford, Qianjia Huang, and\nSusan Furrer. 2017. “They basically like destroyed\nthe school one day”: On newer app features and cy-\nberbullying in schools. In Proceedings of CSCW.\n\nSara Owsley Sood, Elizabeth F Churchill, and Judd\nAntin. 2012. Automatic identification of personal\ninsults on social news sites. Journal of the American\nSociety for Information Science and Technology.\n\nChenhao Tan, Vlad Niculae, Cristian Danescu-\nNiculescu-Mizil, and Lillian Lee. 2016. Winning\narguments: Interaction dynamics and persuasion\nstrategies in good-faith online discussions. In Pro-\nceedings of WWW.\n\nYla R Tausczik and James W Pennebaker. 2010. The\npsychological meaning of words: LIWC and com-\nputerized text analysis methods. Journal of Lan-\nguage and Social Psychology.\n\nKal Turnbull. 2018. “Thats Bullshit’ — Rude Enough\nfor Removal? A Multi-Mod Perspective. Change\nMy View Blog.\n\nJessica Vitak, Kalyani Chadha, Linda Steiner, and\nZahra Ashktorab. 2017. Identifying women’s ex-\nperiences with and strategies for mitigating nega-\ntive effects of online harassment. In Proceedings of\nCSCW.\n\nLu Wang and Claire Cardie. 2014. A piece of my mind:\nA sentiment analysis approach for online dispute de-\ntection. In Proceedings of ACL.\n\nWilliam Warner and Julia Hirschberg. 2012. Detecting\nhate speech on the World Wide Web. In Proceedings\nof the Workshop on Language in Social Media.\n\nWikimedia Support and Safety Team. 2015. Harass-\nment survey. Wikimedia Foundation.\n\nEllery Wulczyn, Nithum Thain, and Lucas Dixon.\n2016. Wikipedia talk labels: Toxicity.\n\n1360\n", "vlm_text": "Joseph M Kayany. 1998. Contexts of uninhibited on- line behavior: Flaming in social newsgroups on usenet.  Journal of the Association for Information Science and Technology . Aniket Kittur, Ed H Chi, and Bongwon Suh. 2009. What’s in Wikipedia?: Mapping topics and conﬂict using socially annotated category structure. In  Pro- ceedings of CHI . Aniket Kittur and Robert E Kraut. 2008. Harnessing the wisdom of crowds in Wikipedia: Quality through coordination. In  Proceedings of CSCW . Aniket Kittur, Bongwon Suh, Bryan A Pendleton, and Ed H Chi. 2007. He says, she says: Conﬂict and coordination in Wikipedia. In  Proceedings of CHI . Vinodh Krishnan and Jacob Eisenstein. 2015. “You’re Mr. Lebowski, I’m the Dude”: Inducing address term formality in signed social networks. In  Pro- ceedings of NAACL . Ravi Kumar, Mohammad Mahdian, and Mary McGlo- hon. 2010. Dynamics of conversations. In  Proceed- ings of KDD . Haewoon Kwak, Jeremy Blackburn, and Seungyeop Han. 2015. Exploring cyberbullying and other toxic behavior in team competition online games. In  Pro- ceedings of CHI . Robin T Lakoff. 1973. The logic of politeness: Mind- ing your P’s and Q’s. In  Proceedings of the Chicago Linguistic Society . Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: Analyzing and comparing opin- ions on the web. In  Proceedings of WWW . Vlad Niculae and Cristian Danescu-Niculescu-Mizil. 2016. Conversational markers of constructive dis- cussions. In  Proceedings of NAACL . Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, and Cristian Danescu-Niculescu-Mizil. 2015. Linguistic harbingers of betrayal: A case study on an online strategy game. In  Proceedings of ACL . Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. 2016. Abusive lan- guage detection in online user content. In  Proceed- ings of WWW . Marco Ortu, Bram Adams, Giuseppe Destefanis, Paras- tou Tourani, Michele Marchesi, and Roberto Tonelli. 2015. Are bullies more productive? Empirical study of affectiveness vs. issue ﬁxing time. In  Proceedings of MSR . John Pavlopoulos, Prodromos Malakasiotis, and Ion Androutsopoulos. 2017a. Deep learning for user comment moderation. In  Proceedings of the Work- shop on Abusive Language Online . \nJohn Pavlopoulos, Prodromos Malakasiotis, and Ion Androutsopoulos. 2017b. Deeper attention to abu- sive user content moderation. In  Proceedings of EMNLP . Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu- pervised modeling of Twitter conversations. In  Pro- ceedings of NAACL . Paul R Rosenbaum. 2010. Design of observational studies . Springer. Donald B Rubin. 2007. The design versus the analysis of observational studies for causal effects: Parallels with the design of randomized trials.  Statistics in Medicine . Tamara Shepherd, Alison Harvey, Tim Jordan, Sam Srauy, and Kate Miltner. 2015. Histories of hating. Social Media  $+$   Society . Vivek K Singh, Marie L Radford, Qianjia Huang, and Susan Furrer. 2017. “They basically like destroyed the school one day”: On newer app features and cy- berbullying in schools. In  Proceedings of CSCW . Sara Owsley Sood, Elizabeth F Churchill, and Judd Antin. 2012. Automatic identiﬁcation of personal insults on social news sites.  Journal of the American Society for Information Science and Technology . Chenhao Tan, Vlad Niculae, Cristian Danescu- Niculescu-Mizil, and Lillian Lee. 2016. Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. In  Pro- ceedings of WWW . Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: LIWC and com- puterized text analysis methods. Journal of Lan- guage and Social Psychology . Kal Turnbull. 2018.  “Thats Bullshit” – Rude Enough for Removal? A Multi-Mod Perspective . Change My View Blog. Jessica Vitak, Kalyani Chadha, Linda Steiner, and Zahra Ashktorab. 2017. Identifying women’s ex- periences with and strategies for mitigating nega- tive effects of online harassment. In  Proceedings of CSCW . Lu Wang and Claire Cardie. 2014. A piece of my mind: A sentiment analysis approach for online dispute de- tection. In  Proceedings of ACL . William Warner and Julia Hirschberg. 2012. Detecting hate speech on the World Wide Web. In  Proceedings of the Workshop on Language in Social Media . Wikimedia Support and Safety Team. 2015.  Harass- ment survey . Wikimedia Foundation. Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2016.  Wikipedia talk labels: Toxicity . "}
{"page": 11, "image_path": "doc_images/P18-1125_11.jpg", "ocr_text": "Ellery Wulczyn, Nithum Thain, and Lucas Dixon.\n2017. Ex machina: Personal attacks seen at scale.\nIn Proceedings of WWW.\n\nNaomi Yamashita and Toru Ishida. 2006. Auto-\nmatic prediction of misconceptions in multilingual\n\ncomputer-mediated communication. In Proceedings\nof IUI.\n\nDawei Yin, Zhenzhen Xue, Liangjie Hong, Brian D\nDavison, April Kontostathis, and Lynne Edwards.\n2009. Detection of harassment on Web 2.0. In Pro-\nceedings of the Workshop on Content Analysis in the\nWeb 2.0.\n\nAmy X Zhang, Bryan Culbertson, and Praveen Pari-\ntosh. 2017a. Characterizing online discussion us-\ning coarse discourse sequences. In Proceedings of\nICWSM.\n\nJustine Zhang, Ravi Kumar, Sujith Ravi, and Cris-\ntian Danescu-Niculescu-Mizil. 2016. | Conversa-\ntional flow in Oxford-style debates. In Proceedings\nof NAACL.\n\nJustine Zhang, Arthur Spirling, and Cristian Danescu-\nNiculescu-Mizil. 2017b. Asking too much? The\nrhetorical role of questions in political discourse. In\nProceedings of EMNLP.\n\n1361\n", "vlm_text": "Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In  Proceedings of WWW . Naomi Yamashita and Toru Ishida. 2006. Auto- matic prediction of misconceptions in multilingual computer-mediated communication. In  Proceedings of IUI . Dawei Yin, Zhenzhen Xue, Liangjie Hong, Brian D Davison, April Kontostathis, and Lynne Edwards. 2009. Detection of harassment on Web 2.0. In  Pro- ceedings of the Workshop on Content Analysis in the Web 2.0 . Amy X Zhang, Bryan Culbertson, and Praveen Pari- tosh. 2017a. Characterizing online discussion us- ing coarse discourse sequences. In  Proceedings of ICWSM . Justine Zhang, Ravi Kumar, Sujith Ravi, and Cris- tian Danescu-Niculescu-Mizil. 2016. Conversa- tional ﬂow in Oxford-style debates. In  Proceedings of NAACL . Justine Zhang, Arthur Spirling, and Cristian Danescu- Niculescu-Mizil. 2017b. Asking too much? The rhetorical role of questions in political discourse. In Proceedings of EMNLP . "}
