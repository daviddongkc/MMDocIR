{"page": 0, "image_path": "doc_images/P19-1033_0.jpg", "ocr_text": "Neural News Recommendation with Long- and Short-term\nUser Representations\n\nMingxiao An”, Fangzhao Wu’, Chuhan Wu*, Kun Zhang!, Zheng Liu’, Xing Xie”\n‘University of Science and Technology of China, Hefei 230026, China\nMicrosoft Research Asia, Beijing 100080, China\n3Department of Electronic Engineering, Tsinghua University, Beijing 100084, China\n{anmx, zhkun}@mail.ustc.edu.cn, wufangzhao@gmail.com\nwuch15@mails.tsinghua.edu.cn, {zhengliu, xingx}@microsoft.com\n\nAbstract\n\nPersonalized news recommendation is impor-\ntant to help users find their interested news and\nimprove reading experience. A key problem\nin news recommendation is learning accurate\nuser representations to capture their interests.\nUsers usually have both long-term preferences\nand short-term interests. However, existing\nnews recommendation methods usually learn\nsingle representations of users, which may be\ninsufficient. In this paper, we propose a neu-\nral news recommendation approach which can\nlearn both long- and short-term user represen-\ntations. The core of our approach is a news\nencoder and a user encoder. In the news en-\ncoder, we learn representations of news from\ntheir titles and topic categories, and use atten-\ntion network to select important words. In the\nuser encoder, we propose to learn long-term\nuser representations from the embeddings of\ntheir IDs. In addition, we propose to learn\nshort-term user representations from their re-\ncently browsed news via GRU network. Be-\nsides, we propose two methods to combine\nlong-term and short-term user representations.\nThe first one is using the long-term user repre-\nsentation to initialize the hidden state of the\nGRU network in short-term user representa-\ntion. The second one is concatenating both\nlong- and short-term user representations as\na unified user vector. Extensive experiments\non a real-world dataset show our approach can\neffectively improve the performance of neural\nnews recommendation.\n\n1 Introduction\n\nOnline news platforms such as MSN News! and\nGoogle News? which aggregate news from various\nsources and distribute them to users have gained\n\n*This work was done when the first author was an intern\nin Microsoft Research Asia.\n\n‘https://www.msn.com/news\n\n*https://news.google.com/\n\n336\n\n2017 NBA Championship\nCelebration From Warriors\n\nty\n\nRami Malek Wins\nthe 2019 Oscar\n\nliad\nti\n\nBohemian Rhapsody Is Highest- |\nGrossing Musician Biopic Ever\n\n5\nOklahoma City Thunder vs.\nGolden State Warriors\n\nFigure 1: An illustrative example of long-term and\nshort-term interests in news reading.\n\nhuge popularity and attracted hundreds of millions\nof users (Das et al., 2007; Wang et al., 2018).\nHowever, massive news are generated everyday,\nmaking it impossible for users to read through\nall news (Lian et al., 2018). Thus, personalized\nnews recommendation is very important for online\nnews platforms to help users find their interested\ncontents and alleviate information overload (Lavie\net al., 2010; Zheng et al., 2018).\n\nLearning accurate user representations is criti-\ncal for news recommendation (Okura et al., 2017).\nExisting news recommendation methods usually\nlearn a single representation for each user (Okura\net al., 2017; Lian et al., 2018; Wu et al., 2019).\nFor example, Okura et al. (2017) proposed to learn\nrepresentations of news using denoising autoen-\ncoder and learn representations of users from their\nbrowsed news using GRU network (Cho et al.,\n2014). However, it is very difficult for RNN net-\nworks such as GRU to capture the entire informa-\ntion of very long news browsing history. Wang\net al. (2018) proposed to learn the representa-\ntions of news using knowledge-aware convolu-\ntional neural network (CNN), and learn the repre-\nsentations of users from their browsed news based\non the similarities between the candidate news and\nthe browsed news. However, this method needs\nto store the entire browsing history of each user\nin the online news recommendation stage, which\nmay bring huge challenge to the storage and may\ncause heavy latency.\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 336-345\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Neural News Recommendation with Long- and Short-term User Representations \nMingxiao  $\\mathbf{A}\\mathbf{n}^{1,*}$  , Fangzhao   $\\mathbf{W}\\mathbf{u}^{2}$  , Chuhan   $\\mathbf{W}\\mathbf{u}^{3}$  , Kun Zhang 1 , Zheng Liu 2 , Xing Xie 2 \n1 University of Science and Technology of China, Hefei 230026, China 2 Microsoft Research Asia, Beijing 100080, China 3 Department of Electronic Engineering, Tsinghua University, Beijing 100084, China { anmx,zhkun } @mail.ustc.edu.cn ,  wufangzhao@gmail.com wuch15@mails.tsinghua.edu.cn ,  { zhengliu,xingx } @microsoft.com \nAbstract \nPersonalized news recommendation is impor- tant to help users ﬁnd their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufﬁcient. In this paper, we propose a neu- ral news recommendation approach which can learn both long- and short-term user represen- tations. The core of our approach is a news encoder and a user encoder. In the news en- coder, we learn representations of news from their titles and topic categories, and use atten- tion network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their re- cently browsed news via GRU network. Be- sides, we propose two methods to combine long-term and short-term user representations. The ﬁrst one is using the long-term user repre- sentation to initialize the hidden state of the GRU network in short-term user representa- tion. The second one is concatenating both long- and short-term user representations as a uniﬁed user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation. \n1 Introduction \nOnline news platforms such as MSN News 1   and Google News 2   which aggregate news from various sources and distribute them to users have gained \nThe image depicts a timeline that illustrates a person's news reading interests over time, highlighting both long-term and short-term interests. The timeline is anchored by different points in time (denoted as \\( t_1, t_i, t_{i+1}, t_j \\)) with corresponding news events associated with each point:\n\n1. At \\( t_1 \\), the person has an interest in an event about the \"2017 NBA Championship Celebration From Warriors.\"\n2. At a later time \\( t_i \\), the person reads about \"Bohemian Rhapsody Is Highest-Grossing Musician Biopic Ever.\"\n3. At \\( t_{i+1} \\), the interest is in \"Rami Malek Wins the 2019 Oscar.\"\n4. Finally, at \\( t_j \\), the focus is again on a sports-related event: \"Oklahoma City Thunder vs. Golden State Warriors.\"\n\nThe timeline suggests that the individual has sustained long-term interests in both basketball (specifically related to the Golden State Warriors) and movies/entertainment, particularly related to \"Bohemian Rhapsody\" and Rami Malek.\nhuge popularity and attracted hundreds of millions of users ( Das et al. ,  2007 ;  Wang et al. ,  2018 ). However, massive news are generated everyday, making it impossible for users to read through all news ( Lian et al. ,  2018 ). Thus, personalized news recommendation is very important for online news platforms to help users ﬁnd their interested contents and alleviate information overload ( Lavie et al. ,  2010 ;  Zheng et al. ,  2018 ). \nLearning accurate user representations is criti- cal for news recommendation ( Okura et al. ,  2017 ). Existing news recommendation methods usually learn a single representation for each user ( Okura et al. ,  2017 ;  Lian et al. ,  2018 ;  Wu et al. ,  2019 ). For example,  Okura et al.  ( 2017 ) proposed to learn representations of news using denoising autoen- coder and learn representations of users from their browsed news using GRU network ( Cho et al. , 2014 ). However, it is very difﬁcult for RNN net- works such as GRU to capture the entire informa- tion of very long news browsing history.  Wang et al.  ( 2018 ) proposed to learn the representa- tions of news using knowledge-aware convolu- tional neural network (CNN), and learn the repre- sentations of users from their browsed news based on the similarities between the candidate news and the browsed news. However, this method needs to store the entire browsing history of each user in the online news recommendation stage, which may bring huge challenge to the storage and may cause heavy latency. "}
{"page": 1, "image_path": "doc_images/P19-1033_1.jpg", "ocr_text": "Our work is motivated by the observation that\nthe interests of online users in news are very di-\nSome user interests may last for a long\ntime and are consistent for the same user (Li et al.,\n2014). For example, as shown in Fig. 1, if a\nuser is a fan of “Golden State Warriors”, this user\nmay tend to read many basketball news about this\nNBA team for several years. We call this kind of\nuser preferences as long-term interest. In addition,\nmany user interests may evolve with time and may\nbe triggered by specific contexts or temporal de-\nmands. For example, in Fig. 1, the browsing of the\nnews on movie “Bohemian Rhapsody” causes the\nuser reading several related news such as “Rami\nMalek Wins the 2019 Oscar” since “Rami Malek”\nis an important actor in this movie, although this\nuser may never read news about “Rami Malek” be-\nfore. We call this kind of user interests as short-\nterm interest. Thus, both long-term and short-\nterm user interests are important for personalized\nnews recommendation, and distinguishing long-\nterm user interests from short-term ones may help\nlearn more accurate user representations.\n\nverse.\n\nIn this paper, we propose a neural news rec-\nommendation approach with both long- and short-\nterm user representations (LSTUR). Our approach\ncontains two major components, i.e., a news en-\ncoder and a user encoder. The news encoder is\nused to learn representations of news articles from\ntheir titles and topic categories. We apply attention\nmechanism to the news encoder to learn informa-\ntive news representations by selecting important\nwords. The user encoder consists of two modules,\ni.e., a long-term user representation (LTUR) mod-\nule and a short-term user representation (STUR)\nmodule. In STUR, we use a GRU network to learn\nshort-term representations of users from their re-\ncently browsing news. In LTUR, we learn the\nlong-term representations of users from the em-\nbeddings of their IDs. In addition, we propose\ntwo methods to combine the short-term and long-\nterm user representations. The first one is using\nthe long-term user representations to initialize the\nhidden state of GRU network in the STUR model.\nThe second one is concatenating the long-tern and\nshort-term user representations as a unified user\nvector. We conducted extensive experiments on a\nreal-world dataset. The experimental results show\nour approach can effectively improve the perfor-\nmance of news recommendation and consistently\noutperform many baseline methods.\n\n337\n\n2 Related Works\n\nPersonalized news recommendation is an impor-\ntant task in natural language processing field and\nhas wide applications (Zheng et al., 2018). It is\ncritical for news recommendation methods to learn\naccurate news and user representations (Wang\net al., 2018). Many conventional news recommen-\ndation methods rely on manual feature engineer-\ning to build news and user representations (Phelan\net al., 2009; Liu et al., 2010; Li et al., 2010; Son\net al., 2013; Li et al., 2014; Bansal et al., 2015;\nLian et al., 2018). For example, Liu et al. (2010)\nproposed to use the topic categories and interests\nfeatures predicted by a Bayesian model to repre-\nsent news, and use the click distribution features of\nnews categories to represent users. Li et al. (2014)\nused a Latent Dirichlet Allocation (LDA) (Blei\net al., 2003) model to generate topic distribution\nfeatures as the news representations. They repre-\nsented a session by using the topic distribution of\nbrowsed news in this session, and the representa-\ntions of users were built from their session repre-\nsentations weighted by the time. However, these\nmethods heavily rely on manual feature engineer-\ning, which needs massive domain knowledge to\ncraft. In addition, the contexts and orders of words\nin news are not incorporated, which are important\nfor understanding the semantic meanings of news\nand learning representations of news and users.\n\nIn recent years, several deep learning meth-\nods were proposed for personalized news rec-\nommendation (Wang et al., 2018; Okura et al.,\n2017; Zheng et al., 2018). For example, Okura\net al. (2017) proposed to learn representations of\nnews from news bodies using denoising autoen-\ncoder, and learn representations of users from\nthe representations of their browsed news using\na GRU network. Wang et al. (2018) proposed to\nlearn representations of news from their titles via\na knowledge-aware CNN network, and learn rep-\nresentations of users from the representations of\ntheir browsed news articles weighted by their sim-\nilarities with the candidate news. Wu et al. (2019)\nproposed to learn news and user representations\nwith personalized word- and news-level attention\nnetworks, which exploits the embedding of user\nID to generate the query vector for the attentions.\nHowever, these methods usually learn a single rep-\nresentation vector for each user, and cannot dis-\ntinguish the long-term preferences and short-term\ninterests of users in reading news. Thus, the user\n\n", "vlm_text": "Our work is motivated by the observation that the interests of online users in news are very di- verse. Some user interests may last for a long time and are consistent for the same user ( Li et al. , 2014 ). For example, as shown in Fig.  1 , if a user is a fan of “Golden State Warriors”, this user may tend to read many basketball news about this NBA team for several years. We call this kind of user preferences as long-term interest. In addition, many user interests may evolve with time and may be triggered by speciﬁc contexts or temporal de- mands. For example, in Fig.  1 , the browsing of the news on movie “Bohemian Rhapsody” causes the user reading several related news such as “Rami Malek Wins the 2019 Oscar” since “Rami Malek” is an important actor in this movie, although this user may never read news about “Rami Malek” be- fore. We call this kind of user interests as short- term interest. Thus, both long-term and short- term user interests are important for personalized news recommendation, and distinguishing long- term user interests from short-term ones may help learn more accurate user representations. \nIn this paper, we propose a neural news rec- ommendation approach with both long- and short- term user representations (LSTUR). Our approach contains two major components, i.e., a news en- coder and a user encoder. The news encoder is used to learn representations of news articles from their titles and topic categories. We apply attention mechanism to the news encoder to learn informa- tive news representations by selecting important words. The user encoder consists of two modules, i.e., a long-term user representation (LTUR) mod- ule and a short-term user representation (STUR) module. In STUR, we use a GRU network to learn short-term representations of users from their re- cently browsing news. In LTUR, we learn the long-term representations of users from the em- beddings of their IDs. In addition, we propose two methods to combine the short-term and long- term user representations. The ﬁrst one is using the long-term user representations to initialize the hidden state of GRU network in the STUR model. The second one is concatenating the long-tern and short-term user representations as a uniﬁed user vector. We conducted extensive experiments on a real-world dataset. The experimental results show our approach can effectively improve the perfor- mance of news recommendation and consistently outperform many baseline methods. \n2 Related Works \nPersonalized news recommendation is an impor- tant task in natural language processing ﬁeld and has wide applications ( Zheng et al. ,  2018 ). It is critical for news recommendation methods to learn accurate news and user representations ( Wang et al. ,  2018 ). Many conventional news recommen- dation methods rely on manual feature engineer- ing to build news and user representations ( Phelan et al. ,  2009 ;  Liu et al. ,  2010 ;  Li et al. ,  2010 ;  Son et al. ,  2013 ;  Li et al. ,  2014 ;  Bansal et al. ,  2015 ; Lian et al. ,  2018 ). For example, Liu et al. ( 2010 ) proposed to use the topic categories and interests features predicted by a Bayesian model to repre- sent news, and use the click distribution features of news categories to represent users. Li et al. ( 2014 ) used a Latent Dirichlet Allocation (LDA) ( Blei et al. ,  2003 ) model to generate topic distribution features as the news representations. They repre- sented a session by using the topic distribution of browsed news in this session, and the representa- tions of users were built from their session repre- sentations weighted by the time. However, these methods heavily rely on manual feature engineer- ing, which needs massive domain knowledge to craft. In addition, the contexts and orders of words in news are not incorporated, which are important for understanding the semantic meanings of news and learning representations of news and users. \nIn recent years, several deep learning meth- ods were proposed for personalized news rec- ommendation ( Wang et al. ,  2018 ;  Okura et al. , 2017 ;  Zheng et al. ,  2018 ). For example, Okura et al. ( 2017 ) proposed to learn representations of news from news bodies using denoising autoen- coder, and learn representations of users from the representations of their browsed news using a GRU network. Wang et al. ( 2018 ) proposed to learn representations of news from their titles via a knowledge-aware CNN network, and learn rep- resentations of users from the representations of their browsed news articles weighted by their sim- ilarities with the candidate news. Wu et al. ( 2019 ) proposed to learn news and user representations with personalized word- and news-level attention networks, which exploits the embedding of user ID to generate the query vector for the attentions. However, these methods usually learn a single rep- resentation vector for each user, and cannot dis- tinguish the long-term preferences and short-term interests of users in reading news. Thus, the user representations learned in these methods may be insufﬁcient for news recommendation. Different from these methods, our approach can learn both long-term and short-term user representations in a uniﬁed framework to capture the diverse inter- ests of users for personalized neural new commen- dation. Extensive experiments on the real-world dataset validate the effectiveness of our approach and the advantage over many baseline methods. "}
{"page": 2, "image_path": "doc_images/P19-1033_2.jpg", "ocr_text": "representations learned in these methods may be\ninsufficient for news recommendation. Different\nfrom these methods, our approach can learn both\nlong-term and short-term user representations in\na unified framework to capture the diverse inter-\nests of users for personalized neural new commen-\ndation. Extensive experiments on the real-world\ndataset validate the effectiveness of our approach\nand the advantage over many baseline methods.\n\n3 Our Approach\n\nIn this section, we present our neural news rec-\nommendation approach with long- and short-term\nuser representations (LSTUR). Our approach con-\ntains two major components, i.e., a news encoder\nto learn representations of news and a user encoder\nto learn representations of users. Next, we intro-\nduce each component in detail.\n\n3.1 News Encoder\n\nThe news encoder is used to learn representations\nof news from their titles, topic and subtopic cat-\negories. The architecture of the news encoder in\nour approach is illustrated in Fig. 2. There are two\nsub-modules in the news encoder, i.e., a title en-\ncoder and a topic encoder.\n\nThe title encoder is used to learn news repre-\nsentations from titles. There are three layers in the\ntitle encoder. The first layer is word embedding,\nwhich is used to convert a news title from a word\nsequence into a sequence of dense semantic vec-\ntors. Denote the word sequence in a news title t\nast = [w1, w2,..., wa], where N is the length of\nthis title. It is transformed into [w , wo,..., wy]\nvia a word embedding matrix.\n\nThe second layer in title encoder is a convolu-\ntional neural network (CNN) (LeCun et al., 2015).\nLocal contexts are very useful for understanding\nthe semantic meaning of news titles. For exam-\nple, in the news title “Next season of super bowl\ngames”, the local contexts of “bowl” such as “su-\nper” and “games” are very important for inferring\nthat it belongs to a sports event name. Thus, we\napply a CNN network to learn contextual word\nrepresentations by capturing the local context in-\nformation. Denote the contextual representation\nof w; as c;, which is computed as follows:\n\nce = ReLU(C x wa_-mituj +), (1)\n\nwhere wi; 17::+.1] is the concatenation of the em-\nbeddings of words between position « — M and\n\n338\n\ne&\n\ney\n\nesy --\nSubtopic | Topic 4 *\ncy\nw, Ow, wy-1 4) w,\n\ni a ie\n\nTES Padding 3° padding\n= fof\n\nWr We Wy-1 Wy\nNews Title\n\nFigure 2: The framework of the news encoder.\n\ni+ M. C and bare the parameters of the convo-\nlutional filters in CNN, and / is the window size.\n\nThe third layer is an attention network (Bah-\ndanau et al., 2015). Different words in the same\nnews title may have different informativeness for\nrepresenting news. For instance, in the news ti-\ntle “The best NBA moments in 2018”, the word\n“NBA” is very informative for representing this\nnews since it is an important indication of sports\nnews, while the word “2018” is less informative.\nThus, we employ a word-level attention network\nto select important words in news titles to learn\nmore informative news representations. The at-\ntention weight a; of the i-th word is formulated\nas follows:\n\na; = tanh(v x ¢; + vp),\n\n(2)\n\na=\n\nwhere wv and vp are the trainable parameters.\nThe final representation of a news title ¢ is the\nsummation of its contextual word representations\nweighted by their attention weights as follows:\n\nN\n\ni=l\n\nThe topic encoder module is used to learn news\nrepresentations from its topics and subtopics. On\nmany online news platforms such as MSN news,\nnews articles are usually labeled with a topic cate-\ngory (e.g., “Sports”) and a subtopic category (e.g.,\n“Football_NFL’) to help target user interests. The\ntopic and subtopic categories of news are also in-\nformative for learning representations of news and\nusers. They can reveal the general and detailed\ntopics of the news, and reflect the preferences of\nusers. For example, if a user browsed many news\narticles with the “Sports” topic category, then we\n", "vlm_text": "\n3 Our Approach \nIn this section, we present our neural news rec- ommendation approach with long- and short-term user representations (LSTUR). Our approach con- tains two major components, i.e., a news encoder to learn representations of news and a user encoder to learn representations of users. Next, we intro- duce each component in detail. \n3.1 News Encoder \nThe  news encoder  is used to learn representations of news from their titles, topic and subtopic cat- egories. The architecture of the  news encoder  in our approach is illustrated in Fig.  2 . There are two sub-modules in the  news encoder , i.e., a title en- coder and a topic encoder. \nThe title encoder is used to learn news repre- sentations from titles. There are three layers in the title encoder. The ﬁrst layer is word embedding, which is used to convert a news title from a word sequence into a sequence of dense semantic vec- tors. Denote the word sequence in a news title    $t$  as    $t=[w_{1},w_{2},.\\,.\\,.\\,,w_{N}]$  , where    $N$   is the length of this title. It is transformed into    $[{\\pmb w}_{1},{\\pmb w}_{2},.\\,.\\,.\\,,{\\pmb w}_{N}]$  via a word embedding matrix. \nThe second layer in title encoder is a convolu- tional neural network (CNN) ( LeCun et al. ,  2015 ). Local contexts are very useful for understanding the semantic meaning of news titles. For exam- ple, in the news title “Next season of super bowl games”, the local contexts of “bowl” such as “su- per” and “games” are very important for inferring that it belongs to a sports event name. Thus, we apply a CNN network to learn contextual word representations by capturing the local context in- formation. Denote the contextual representation of    $w_{i}$   as  $c_{i}$  , which is computed as follows: \n\n$$\n\\pmb{c}_{i}=\\mathrm{ReLU}(\\pmb{C}\\times\\pmb{w}_{[i-M:i+M]}+\\pmb{b}),\n$$\n \nwhere    $\\pmb{w}_{[i-M:i+M]}$   is the concatenation of the em- beddings of words between position    $\\textit{i}-\\textit{M}$   and \nThe image illustrates a conceptual framework for a news encoder, often used in natural language processing tasks such as news article categorization or summarization. Here's a breakdown of the elements in the framework:\n\n1. **News Input**: The process begins with a news article, which is broken down into three components: News Title, News Subtopic, and News Topic.\n\n2. **Word Embedding**: The words in the News Title are represented visually as boxes labeled from \\( w_1 \\) to \\( w_N \\). These words are transformed into vectors using a word embedding layer, depicted in green. Padding is used to ensure uniform input size across different data instances.\n\n3. **Contextual Representation**: The embedded word vectors are processed to generate contextual representations \\( c_1 \\) through \\( c_N \\) (shown in green and blue), through mechanisms that likely involve attention or contextual transformation.\n\n4. **Attention Mechanism**: The framework utilizes an attention mechanism, depicted by the dotted arrows and coefficients \\( a_1, a_2, \\ldots, a_N \\). This mechanism calculates attention scores for each word to form a weighted context vector \\( v \\).\n\n5. **Title Encoding**: The context vector \\( v \\) is then used to produce the encoded title vector \\( e_t \\).\n\n6. **Subtopic and Topic Encoding**: Separate embedding layers are used to convert the News Subtopic and News Topic into their respective vector embeddings \\( e_{sv} \\) and \\( e_{t} \\).\n\n7. **Final Encoding**: The encoded subtopic \\( e_{sv} \\), topic \\( e_v \\), and title \\( e_t \\) vectors are combined, likely through vector addition or concatenation (represented by the ⊕ symbol), to form the final news encoding vector \\( e \\).\n\nThis framework leverages hierarchical embeddings and attention to efficiently synthesize multiple layers of news content information into a unified encoding.\n $i+M$  .    $C$   and    $^{b}$   are the parameters of the convo- lutional ﬁlters in CNN, and    $M$  is the window size. \nThe third layer is an attention network ( Bah- danau et al. ,  2015 ). Different words in the same news title may have different informativeness for representing news. For instance, in the news ti- tle “The best NBA moments in   $2018\"$  , the word “NBA” is very informative for representing this news since it is an important indication of sports news, while the word   $^{\\ast2018^{\\ast}}$   is less informative. Thus, we employ a word-level attention network to select important words in news titles to learn more informative news representations. The at- tention weight    $\\alpha_{i}$   of the    $i$  -th word is formulated as follows: \n\n$$\n\\begin{array}{l}{a_{i}=\\operatorname{tanh}({\\pmb v}\\times{\\pmb c}_{i}+{\\pmb v}_{b}),}\\\\ {\\alpha_{i}=\\displaystyle\\frac{\\exp(a_{i})}{\\sum_{j=1}^{N}\\exp(a_{j})},}\\end{array}\n$$\n \nwhere    $\\mathbfit{v}$   and    $v_{b}$   are the trainable parameters. The ﬁnal representation of a news title    $t$   is the summation of its contextual word representations weighted by their attention weights as follows: \n\n$$\ne_{t}=\\sum_{i=1}^{N}\\alpha_{i}\\pmb{c}_{i}.\n$$\n \nThe topic encoder module is used to learn news representations from its topics and subtopics. On many online news platforms such as MSN news, news articles are usually labeled with a topic cate- gory (e.g., “Sports”) and a subtopic category (e.g., “Football NFL”) to help target user interests. The topic and subtopic categories of news are also in- formative for learning representations of news and users. They can reveal the general and detailed topics of the news, and reﬂect the preferences of users. For example, if a user browsed many news articles with the “Sports” topic category, then we "}
{"page": 3, "image_path": "doc_images/P19-1033_3.jpg", "ocr_text": "u ex{\nuy —> GRU —» GRU — -—> GRU News\nNews News News aS\nCandidate\nCamas Cofaws Chews News\n= inl a\n[e) O. User Click History O\n\n(a) LSTUR-ini.\n\nGRU —®» GRU —> =>\n\nt t t\ner | 7 e2 1 ex 1\nNews News ~ News @) ° a\n=)\nCandidate\nC1 [news C2 news Chews’ News\nOo O User Click History O\n\n(b) LSTUR-con.\n\nFigure 3: The two frameworks of our LSTUR approach.\n\ncan infer this user is probably interested in sports,\nand it may be effective to recommend candidate\nnews in the “Sports” topic category to this user.\nTo incorporate the topic and subtopic information\ninto news representation, we propose to learn the\nrepresentations of topics and subtopics from the\nembeddings of their IDs, as shown in Fig. 2. De-\nnote e, and e€., as the representations of topic and\nsubtopic. The final representation of a news arti-\ncle is the concatenation of the representations of\nits title, topic and subtopic, i.e., e = [ez, ey, Esv].\n\n3.2 User Encoder\n\nThe user encoder is used to learn representations\nof users from the history of their browsed news. It\ncontains two modules, i.e., a short-term user repre-\nsentation model (STUR) to capture user’s tempo-\nral interests, and a long-term user representation\nmodel (LTUR) to capture user’s consistent prefer-\nences. Next, we introduce them in detail.\n\n3.2.1 Short-Term User Representation\n\nOnline users may have dynamic short-term inter-\nests in reading news articles, which may be influ-\nenced by specific contexts or temporal information\ndemands. For example, if a user just reads a news\narticle about “Mission: Impossible 6 — Fallout”,\nand she may want to know more about the actor\n“Tom Cruise” in this movie and click news arti-\ncles related to “Tom Cruise”, although she is not\nhis fan and may never read his news before. We\npropose to learn the short-term representations of\nusers from their recent browsing history to capture\ntheir temporal interests, and use gated recurrent\nnetworks (GRU) (Cho et al., 2014) network to cap-\nture the sequential news reading patterns (Okura\net al., 2017). Denote news browsing sequence\nfrom a user sorted by timestamp in ascending or-\n\nder as C = {c1,c2,...,¢c%}, where k is the length\nof this sequence. We apply the news encoder to\nobtain the representations of these browsed arti-\ncles, denoted as {e), €2,...,e,}. The short-term\nuser representation is computed as follows:\n\nr, = 0(W,,[hi-1, er]),\n21 = 0(W.[hi-1, e1]),\nhi = tanh(W;,[r; © hi-1, er]),\nhy = 2, Oy + (1— 24) Oy,\n\n(4)\n\nwhere o is the sigmoid function, © is the item-\nwise product, W,, W, and W;, are the param-\neters of the GRU network. The short-term user\nrepresentation is the last hidden state of the GRU\nnetwork, i.e., us = hy.\n\n3.2.2. Long-Term User Representations\n\nBesides the temporal interests, online users may\nalso have long-term interests in reading news. For\nexample, a basketball fan may tend to browse\nmany sports news related to NBA in several years.\nThus, we propose to learn long-term representa-\ntions of users to capture their consistent prefer-\nences. In our approach the long-term user repre-\nsentations are learned from the embeddings of the\nuser IDs, which are randomly initialized and fine-\ntuned during model training. Denote u as the ID of\na user and W,, as the look-up table for long-term\nuser representation, the long-term user representa-\ntion of this user is wu = W,,[u].\n\n3.2.3 Long- and Short-Term User\nRepresentation\n\nIn this section, we introduce two methods to com-\nbine the long-term and short-term user presenta-\ntions for unified user representation, which are\nshown in Fig. 3.\n\n339\n", "vlm_text": "The image depicts two frameworks of the \"LSTUR\" (Long- and Short-term User Representation) approach for personalized news recommendation systems. It compares two architectures, LSTUR-ini and LSTUR-con, used for processing user click history and candidate news to generate personalized news recommendations.\n\n1. **LSTUR-ini (left side of the image):**\n   - The framework starts with a user's click history, represented as a timeline.\n   - \"User Embedding\" is obtained from the user's past click history and initialized into the model.\n   - The Gru cells (Gated Recurrent Unit) sequentially process encoded news information (from the \"News Encoder\") of the clicked news articles (`c1`, `c2`, ..., `ck`).\n   - The candidate news (`cx`) is also processed by a similar \"News Encoder\" to obtain its vector representation (`ex`).\n   - A dot product operation computes the \"Score\" indicating the relevance of the candidate news to the user's interests.\n\n2. **LSTUR-con (right side of the image):**\n   - Similar to LSTUR-ini, this model also begins with the user's click history.\n   - Instead of initialization, the approach concatenates the user-level embedding (`us`) derived from click history, with a fixed user embedding (`ul`), forming a combined user vector (`u`).\n   - The combined vector (`u`) is processed in a similar manner using GRU cells alongside news encoders to handle click history (`c1`, `c2`, ..., `ck`).\n   - The \"Dot Product\" between the user's combined representation and the candidate news encoding (`ex`) generates a \"Score\" for the news recommendation fit.\n\nBoth frameworks aim to output a \"Score\" based on the comparison between user representation (derived from historical click data) and candidate news, facilitating personalized recommendations.\ncan infer this user is probably interested in sports, and it may be effective to recommend candidate news in the “Sports” topic category to this user. To incorporate the topic and subtopic information into news representation, we propose to learn the representations of topics and subtopics from the embeddings of their IDs, as shown in Fig.  2 . De- note  $e_{v}$   and    $e_{s v}$   as the representations of topic and subtopic. The ﬁnal representation of a news arti- cle is the concatenation of the representations of its title, topic and subtopic, i.e.,    $\\boldsymbol{e}=[e_{t},e_{v},e_{s v}]$  . \n3.2 User Encoder \nThe  user encoder  is used to learn representations of users from the history of their browsed news. It contains two modules, i.e., a short-term user repre- sentation model (STUR) to capture user’s tempo- ral interests, and a long-term user representation model (LTUR) to capture user’s consistent prefer- ences. Next, we introduce them in detail. \n3.2.1 Short-Term User Representation \nOnline users may have dynamic short-term inter- ests in reading news articles, which may be inﬂu- enced by speciﬁc contexts or temporal information demands. For example, if a user just reads a news article about “Mission: Impossible 6 – Fallout”, and she may want to know more about the actor “Tom Cruise” in this movie and click news arti- cles related to “Tom Cruise”, although she is not his fan and may never read his news before. We propose to learn the short-term representations of users from their recent browsing history to capture their temporal interests, and use gated recurrent networks (GRU) ( Cho et al. ,  2014 ) network to cap- ture the sequential news reading patterns ( Okura et al. ,  2017 ). Denote news browsing sequence from a user sorted by timestamp in ascending or- der as    ${\\mathcal{C}}=\\{c_{1},c_{2},.\\,.\\,.\\,,c_{k}\\}$  , where  $k$   is the length of this sequence. We apply the news encoder to obtain the representations of these browsed arti- cles, denoted as    $\\{e_{1},e_{2},.\\,.\\,.\\,,e_{k}\\}$  . The short-term user representation is computed as follows: \n\n\n$$\n\\begin{array}{r l}&{\\boldsymbol{r}_{t}=\\sigma\\big(\\boldsymbol{W}_{r}[\\boldsymbol{h}_{t-1},\\boldsymbol{e}_{t}]\\big),}\\\\ &{\\boldsymbol{z}_{t}=\\sigma\\big(\\boldsymbol{W}_{z}[\\boldsymbol{h}_{t-1},\\boldsymbol{e}_{t}]\\big),}\\\\ &{\\tilde{\\boldsymbol{h}}_{t}=\\operatorname{tanh}\\bigl(\\boldsymbol{W}_{\\tilde{h}}[\\boldsymbol{r}_{t}\\odot\\boldsymbol{h}_{t-1},\\boldsymbol{e}_{t}]\\bigr),}\\\\ &{\\boldsymbol{h}_{t}=\\boldsymbol{z}_{t}\\odot\\boldsymbol{h}_{t}+({\\bf1}-\\boldsymbol{z}_{t})\\odot\\tilde{\\boldsymbol{h}}_{t},}\\end{array}\n$$\n \nwhere    $\\sigma$   is the sigmoid function,    $\\odot$  is the item- wise product,    $W_{r},\\,W_{z}$   and    $W_{\\tilde{h}}$    are the param- eters of the GRU network. The short-term user representation is the last hidden state of the GRU network, i.e.,    ${\\pmb u}_{s}={\\pmb h}_{k}$  . \n3.2.2 Long-Term User Representations \nBesides the temporal interests, online users may also have long-term interests in reading news. For example, a basketball fan may tend to browse many sports news related to NBA in several years. Thus, we propose to learn long-term representa- tions of users to capture their consistent prefer- ences. In our approach the long-term user repre- sentations are learned from the embeddings of the user IDs, which are randomly initialized and ﬁne- tuned during model training. Denote  $u$   as the ID of a user and    $W_{u}$   as the look-up table for long-term user representation, the long-term user representa- tion of this user is    $\\pmb{u}_{l}=\\pmb{W}_{u}[u]$  . \n3.2.3 Long- and Short-Term User Representation \nIn this section, we introduce two methods to com- bine the long-term and short-term user presenta- tions for uniﬁed user representation, which are shown in Fig.  3 . "}
{"page": 4, "image_path": "doc_images/P19-1033_4.jpg", "ocr_text": "The first method is using the long-term user\nrepresentation to initialize the hidden state of the\nGRU network in the short-term user representation\nmodel, as shown in Fig. 3a. We denote this method\nas LSTUR-ini. We use the last hidden state of the\nGRU network as the final user representation. The\nsecond method is concatenating the long-term user\nrepresentation with the short-term user represen-\ntation as the final user representation, as shown in\nFig. 3b. We denote this method as LSTUR-con.\n\n3.3. Model Training\n\nFor online news recommendation services where\nuser and news representations can be computed in\nadvance, the scoring function should be as simple\nas possible to reduce latency. Motivated by (Okura\net al., 2017), we use the simple dot production to\ncompute the news click probability score. Denote\nthe representation of a user u as wu and the repre-\nsentation of a candidate news article e, as e,, the\nprobability score s(u, c,) of this user clicking this\nnews is computed as s(u, cr) = u! ep.\n\nMotivated by (Huang et al., 2013) and (Zhai\net al., 2016), we propose to use the negative sam-\npling technique for model training. For each news\nbrowsed by a user (regarded as a positive sam-\nple), we randomly sample / news articles from\nthe same impression which are not clicked by this\nuser as negative samples. Our model will jointly\npredict the click probability scores of the positive\nnews and the K negative news. In this way, the\nnews click prediction problem is reformulated as\na pseudo K + 1-way classification task. We mini-\nmize the summation of the negative log-likelihood\nof all positive samples during training, which can\nbe formulated as follows:\n\nP\n\n> log\n\ni=l\n\na. ce\nU, Cj\n\nexp(s(us2))\nexp(s(u, c?)) + an exp(s(u, cP) ,\n(5)\n\nwhere P is the number of positive training sam-\nples, and cj’, is the k-th negative sample in the\nsame session with the i-th positive sample.\n\nSince not all users can be incorporated in news\nrecommendation model training (e.g., the new\ncoming users), it is not appropriate to assume all\nusers have long-term representations in our mod-\nels in the prediction stage. In order to handle this\nproblem, in the model training stage, we randomly\nmask the long-term representations of users with\n\n340\n\na certain probability p. When we mask the long-\nterm representations, all the dimensions are set to\nzero. Thus, the long-term user representation in\nour LSTUR approach can be reformulated as:\nuw =M-W, ul, M~ Bil,1-p), (6)\nwhere B is Bernoulli distribution, and M is a ran-\ndom variable that subject to B(1, 1—p). We find in\nexperiments that this trick for model training can\nimprove the performance of our approach.\n\n4 Experiments\n\n4.1 Dataset and Experimental Settings\n\nSince there is no off-the-shelf dataset for news rec-\nommendation, we built one by ourselves through\ncollecting logs from MSN News? in four weeks\nfrom December 23rd, 2018 to January 19th, 2019.\nWe used the logs in the first three weeks for model\ntraining, and those in the last week for test. We\nalso randomly sampled 10% of logs from the train-\ning set as the validation data. For each sample,\nwe collected the browsing history in last 7 days to\nlearn short-term user representations. The detailed\ndataset statistics are summarized in Table 1.\n\n22,938\n9.98\n492,185\n9,224,537\n\n# of users\n# of news\n# of imprs\nNP ratio”\n\n25,000\n\n38,501\n\n393,191\n18.74\n\n# of users in training set\nAvg. # of words per title\n# of positive samples\n# of negative samples\n\nTable 1: Statistics of the dataset in our experiments.\n\nIn our experiments, we used the pretrained\nGloVe embedding? (Pennington et al., 2014) as the\ninitialization of word embeddings. The word em-\nbedding dimension is 200. The number of filters\nin CNN network is 300, and the window size of\nthe filters in CNN network is set to 3. We applied\ndropout (Srivastava et al., 2014) to each layer in\nour approach to mitigate overfitting. The dropout\nrate is 0.2. The default value of long-term user rep-\nresentation masking probability p for model train-\ning is 0.5. We used Adam (Kingma and Ba, 2014)\nto optimize the model, and the learning rate was\n0.01. The batch size is set to 400. The number\nof negative samples for each positive sample is\n4. These hyper-parameters were all selected ac-\ncording to the results on validation set. We used\n\n3https://www.msn.com/en-us/news\n\n“The ratio of the negative sample number to the positive\nsample number.\n\nShttp://nlp.stanford.edu/data/glove.840B.300d.zip\n", "vlm_text": "The ﬁrst method is using the long-term user representation to initialize the hidden state of the GRU network in the short-term user representation model, as shown in Fig.  3a . We denote this method as LSTUR-ini. We use the last hidden state of the GRU network as the ﬁnal user representation. The second method is concatenating the long-term user representation with the short-term user represen- tation as the ﬁnal user representation, as shown in Fig.  3b . We denote this method as LSTUR-con. \n3.3 Model Training \nFor online news recommendation services where user and news representations can be computed in advance, the scoring function should be as simple as possible to reduce latency. Motivated by ( Okura et al. ,  2017 ), we use the simple dot production to compute the news click probability score. Denote the representation of a user    $u$   as  $\\mathbfcal{U}$   and the repre- sentation of a candidate news article    $e_{x}$   as    $e_{x}$  , the probability score    $s(u,c_{x})$   of this user clicking this news is computed as  $\\boldsymbol{s}(u,c_{x})=\\mathbf{u}^{\\top}\\boldsymbol{e}_{x}$  . \nMotivated by ( Huang et al. ,  2013 ) and ( Zhai et al. ,  2016 ), we propose to use the negative sam- pling technique for model training. For each news browsed by a user (regarded as a positive sam- ple), we randomly sample    $K$   news articles from the same impression which are not clicked by this user as negative samples. Our model will jointly predict the click probability scores of the positive news and the    $K$   negative news. In this way, the news click prediction problem is reformulated as a pseudo    $K+1$  -way classiﬁcation task. We mini- mize the summation of the negative log-likelihood of all positive samples during training, which can be formulated as follows: \n\n$$\n-\\sum_{i=1}^{P}\\log\\frac{\\exp(s(u,c_{i}^{p}))}{\\exp(s(u,c_{i}^{p}))+\\sum_{k=1}^{K}\\exp(s(u,c_{i,k}^{n}))},\n$$\n \nwhere    $P$   is the number of positive training sam- ples, and    $c_{i,k}^{n}$    is the    $k$  -th negative sample in the same session with the  $i$  -th positive sample. \nSince not all users can be incorporated in news recommendation model training (e.g., the new coming users), it is not appropriate to assume all users have long-term representations in our mod- els in the prediction stage. In order to handle this problem, in the model training stage, we randomly mask the long-term representations of users with a certain probability    $p$  . When we mask the long- term representations, all the dimensions are set to zero. Thus, the long-term user representation in our LSTUR approach can be reformulated as: \n\n\n$$\n{\\mathbfit{u}}_{l}=M\\cdot W_{u}[u],M\\sim B(1,1-p),\n$$\n \nwhere  $B$   is Bernoulli distribution, and    $M$   is a ran- dom variable that subject to    $B(1,1{-}p)$  . We ﬁnd in experiments that this trick for model training can improve the performance of our approach. \n4 Experiments \n4.1 Dataset and Experimental Settings \nSince there is no off-the-shelf dataset for news rec- ommendation, we built one by ourselves through collecting logs from MSN News 3   in four weeks from December 23rd, 2018 to January 19th, 2019. We used the logs in the ﬁrst three weeks for model training, and those in the last week for test. We also randomly sampled   $10\\%$   of logs from the train- ing set as the validation data. For each sample, we collected the browsing history in last 7 days to learn short-term user representations. The detailed dataset statistics are summarized in Table  1 . \nThe table presents various statistics related to a dataset, presumably for a machine learning or data analysis context:\n\n- **# of users**: There are 25,000 users in total.\n- **# of news**: The dataset includes 38,501 news articles.\n- **# of imprs (impressions)**: There are 393,191 impressions recorded in the dataset.\n- **NP ratio**: The ratio of negative to positive samples is 18.74.\n- **# of users in training set**: There are 22,938 users in the training set.\n- **Avg. # of words per title**: On average, each title contains 9.98 words.\n- **# of positive samples**: There are 492,185 positive samples in the dataset.\n- **# of negative samples**: The dataset contains 9,224,537 negative samples.\nIn our experiments, we used the pretrained GloVe embedding 5   ( Pennington et al. ,  2014 ) as the initialization of word embeddings. The word em- bedding dimension is 200. The number of ﬁlters in CNN network is 300, and the window size of the ﬁlters in CNN network is set to 3. We applied dropout ( Srivastava et al. ,  2014 ) to each layer in our approach to mitigate overﬁtting. The dropout rate is 0.2. The default value of long-term user rep- resentation masking probability    $p$   for model train- ing is 0.5. We used Adam ( Kingma and Ba ,  2014 ) to optimize the model, and the learning rate was 0 . 01 . The batch size is set to 400. The number of negative samples for each positive sample is 4. These hyper-parameters were all selected ac- cording to the results on validation set. We used impression-based ranking metrics to evaluate the performance, including area under the ROC curve (AUC), mean reciprocal rank (MRR), and nor- malized discounted cumulative gain (nDCG). We repeated each experiment for 10 times indepen- dently, and reported the average results with 0.95 conﬁdence probability. "}
{"page": 5, "image_path": "doc_images/P19-1033_5.jpg", "ocr_text": "impression-based ranking metrics to evaluate the\nperformance, including area under the ROC curve\n(AUC), mean reciprocal rank (MRR), and nor-\nmalized discounted cumulative gain (nDCG). We\nrepeated each experiment for 10 times indepen-\ndently, and reported the average results with 0.95\nconfidence probability.\n\n4.2 Performance Evaluation\n\nWe evaluate the performance of our approach by\ncomparing it with several baseline methods, in-\ncluding:\n\n¢ LibFM (Rendle, 2012), a state-of-the-art ma-\ntrix factorization method which is widely\nused in recommendation. In our experiments,\nhe user features are the concatenation of\nTF-IDF features extracted from the browsed\nnews titles, and the normalized count features\nrom the topics and subtopics of the browsed\nnews. The features for news consists of TF-\nIDF features from its title, and one-hot vec-\nors of its topic and subtopic. The input to\nLibFM is the concatenation of user features\nand features of candidate news.\n\nDeepFM (Guo et al., 2017), a widely used\nmethod that combines factorization machines\nand deep neural networks. We use the same\nfeatures as LibFM.\n\nWide & Deep (Cheng et al., 2016), another\ndeep learning based recommendation method\nthat combines a wide channel and a deep\nchannel. Again, the same features with\nLibFM are used for both channels.\n\nDSSM (Huang et al., 2013), deep structured\nsemantic model. The inputs are hashed words\nvia character trigram, where all the browsed\nnews titles are merged as query document.\n\nCNN (Kim, 2014), using CNN with max\npooling to learn news representations from\nthe titles of browsed news by keeping the\nmost salient features.\n\nDKN (Wang et al., 2018), a deep news rec-\nommendation model which contains CNN\nand candidate-aware attention on the news\nbrowsing histories.\n\nGRU (Okura et al., 2017), learning news rep-\nresentations by a denoising autoencoder and\nuser representations by a GRU network.\n\n341\n\nThe results of comparing different methods are\nsummarized in Table 2.\n\nWe have obtained observations from Table 2.\nFirst, the news recommendation methods (e.g.\nCNN, DKN and LSTUR) which use neural net-\nworks to learn news and user representations can\nsignificantly outperform the methods using man-\nual feature engineering (e.g. LibFM, DeepFM,\nWide & Deep, and DSSM). This is probably be-\ncause handcrafted features are usually not optimal,\nand neural networks can capture both global and\nlocal semantic contexts in news, which are useful\nfor learning more accurate news and user repre-\nsentations for news recommendation.\n\nSecond, our LSTUR approach outperforms all\nbaseline methods compared here, including deep\nlearning models such as CNN, GRU and DKN. Our\nLSTUR approach can capture both the long-term\npreferences and short-term interests to capture the\ncomplex and diverse user interests in news read-\ning, while the baseline methods only learn a single\nrepresentation for each user, which is insufficient.\nIn addition, our LSTUR approach uses attention\nmechanism in the news encoder to select impor-\ntant words, which can help learn more informative\nnews representations.\n\nThird, our proposed two methods to learn long-\nand short-term user representations, i.e., LSTUR-\nini and LSTUR-con, can achieve comparable per-\n‘ormance and both outperform baseline methods,\nwhich validate the effectiveness of these meth-\nods. In addtion, the performance of LSTUR-con\nis more stable than LSTUR-ini, which indicates\nhat using the concatenation of both short-term\nand long-term user representations is capable of\nretaining all the information. We also conducted\nexperiments to explore the performance of com-\nbining both LSTUR-con and LSTUR-ini in the\nsame model, but the performance improvement is\nvery limited, implying that each of them can fully\ncapture the long- and short-term user interests for\nnews recommendation.\n\n4.3 Effectiveness of Long- and Short-Term\nUser Representation\n\nIn this section, we conducted several experiments\nto explore the effectiveness of our approach in\nlearning both long-term and short-term user rep-\nresentations. We compare the performance of our\nLSTUR methods with the long-term user represen-\ntation model LTUR and the short-term user rep-\n", "vlm_text": "\n4.2 Performance Evaluation \nWe evaluate the performance of our approach by comparing it with several baseline methods, in- cluding: \n•  LibFM  ( Rendle ,  2012 ), a state-of-the-art ma- trix factorization method which is widely used in recommendation. In our experiments, the user features are the concatenation of TF-IDF features extracted from the browsed news titles, and the normalized count features from the topics and subtopics of the browsed news. The features for news consists of TF- IDF features from its title, and one-hot vec- tors of its topic and subtopic. The input to LibFM  is the concatenation of user features and features of candidate news. •  DeepFM  ( Guo et al. ,  2017 ), a widely used method that combines factorization machines and deep neural networks. We use the same features as  LibFM . •  Wide & Deep  ( Cheng et al. ,  2016 ), another deep learning based recommendation method that combines a wide channel and a deep channel. Again, the same features with LibFM  are used for both channels. •  DSSM  ( Huang et al. ,  2013 ), deep structured semantic model. The inputs are hashed words via character trigram, where all the browsed news titles are merged as query document. •  CNN  ( Kim ,  2014 ), using CNN with max pooling to learn news representations from the titles of browsed news by keeping the most salient features. •  DKN  ( Wang et al. ,  2018 ), a deep news rec- ommendation model which contains CNN and candidate-aware attention on the news browsing histories. •  GRU  ( Okura et al. ,  2017 ), learning news rep- resentations by a denoising autoencoder and user representations by a GRU network. \nThe results of comparing different methods are summarized in Table  2 . \nWe have obtained observations from Table  2 . First, the news recommendation methods (e.g. CNN ,  DKN  and  LSTUR ) which use neural net- works to learn news and user representations can signiﬁcantly outperform the methods using man- ual feature engineering (e.g. LibFM ,  DeepFM , Wide & Deep , and  DSSM ). This is probably be- cause handcrafted features are usually not optimal, and neural networks can capture both global and local semantic contexts in news, which are useful for learning more accurate news and user repre- sentations for news recommendation. \nSecond, our LSTUR approach outperforms all baseline methods compared here, including deep learning models such as  CNN ,  GRU  and  DKN . Our LSTUR approach can capture both the long-term preferences and short-term interests to capture the complex and diverse user interests in news read- ing, while the baseline methods only learn a single representation for each user, which is insufﬁcient. In addition, our LSTUR approach uses attention mechanism in the news encoder to select impor- tant words, which can help learn more informative news representations. \nThird, our proposed two methods to learn long- and short-term user representations, i.e., LSTUR- ini and LSTUR-con, can achieve comparable per- formance and both outperform baseline methods, which validate the effectiveness of these meth- ods. In addtion, the performance of LSTUR-con is more stable than LSTUR-ini, which indicates that using the concatenation of both short-term and long-term user representations is capable of retaining all the information. We also conducted experiments to explore the performance of com- bining both LSTUR-con and LSTUR-ini in the same model, but the performance improvement is very limited, implying that each of them can fully capture the long- and short-term user interests for news recommendation. \n4.3 Effectiveness of Long- and Short-Term User Representation \nIn this section, we conducted several experiments to explore the effectiveness of our approach in learning both long-term and short-term user rep- resentations. We compare the performance of our LSTUR  methods with the long-term user represen- tation model LTUR and the short-term user rep- "}
{"page": 6, "image_path": "doc_images/P19-1033_6.jpg", "ocr_text": "Methods AUC MRR nDCG@5 nDCG@10\nLibFM 56.52 + 1.31 | 25.53 + 0.81 | 26.66 + 1.04 | 34.72 + 0.95\nDeepFM 58.13 + 1.69 | 27.01 + 0.20 | 28.37 + 0.57 | 36.78 + 0.62\nWide & Deep | 58.07 + 0.55 | 27.07 0.37 | 28.51 + 0.45 | 36.93 + 0.43\nDSSM 58.43 + 0.58 | 27.25 + 0.49 | 28.31 + 0.60 | 36.91 + 0.54\nCNN 61.13 £0.77 | 29.44 + 0.73 | 31.44 + 0.87 | 39.51 + 0.74\nDKN 61.25 + 0.78 | 29.47 + 0.64 | 31.54 + 0.79 | 39.59 + 0.67\nGRU 62.69 + 0.16 | 30.24 + 0.13 | 32.56 +0.17 | 40.55 + 0.13\nLSTUR-con 63.47 + 0.10 | 30.94 + 0.14 | 33.43 + 0.13 | 41.344 0.13\nLSTUR-ini | 63.56 + 0.42 | 30.98 + 0.32 | 33.45 + 0.39 | 41.37 + 0.36\nTable 2: The performance of different methods on news recommendation.\n\n0.640 0.420\n(= LTuR\neee 3 sTuR, 0.415\n0.625 Ga LSTUR-con |0.410\n0.620 Re 10.405\nDet 0.400\n0.600* AUC nDCG@10 ‘0.390\nFigure 4: The effectiveness of incorporating long-tern\n\nuser representations (LTUR) and short-term user rep-\nresentations (STUR).\n\nresentation model STUR. The results are summa-\nrized in Fig. 4.\n\nFrom the results we find both LTUR and STUR\nare useful for news recommendation, and the\nSTUR model can outperform the LTUR model.\nAccording to the statistics in Table 1, the long-\nterm representations of many users in test data\nare unavailable, which leads to relative weak per-\nformance of LTUR on these users. In addition,\ncombining STUR and LTUR using our two long-\nand short-term user representation methods, i.e.,\nLSTUR-ini and LSTUR-con, can effectively im-\nprove the performance. This result validates that\nincorporating both long-term and short-term user\nrepresentations is useful to capture the diverse\nuser interests more accurately and is beneficial for\nnews recommendation.\n\n4.4 Effectiveness of News Encoders in STUR\n\nIn our STUR model, GRU is used to learn short-\nterm user representations from the recent browsing\nnews. We explore the effectiveness of GRU in en-\ncoding news by replacing it with several other en-\ncoders, including: 1) Average: using the average\nof all the news representations in recent browsing\n\n0.638 0.418\n0.636 TS Atenton (0-416\n0.634 ezeism 10.414\n0.632 mm oR 412\n0.630 0.410\n0.628 0.408\n0.626! 0.406\n\nAUC\n\nnDCG@10\n\nFigure 5: The comparisons of different methods in\nlearning short-term user representations from recently\nbrowsed news articles.\n\nhistory; 2) Attention: the summation of news rep-\nresentations weighted by their attention weights;\n3) LSTM (Hochreiter and Schmidhuber, 1997), re-\nplacing GRU with LSTM. The results are summa-\nrized in Fig. 5.\n\nAccording to Fig. 5, the sequence-based en-\ncoders (e.g., GRU, LSTM) outperform the Aver-\nage and Attention based encoders. This is proba-\nbly because the sequence-based encoders can cap-\nture the sequential new reading patterns to learn\nshort-term representations of users, which is diffi-\ncult for Average and Attention based encoders. In\naddition, GRU achieves better performance than\nLSTM. This may be because GRU contains fewer\nparameters and has lower risk of overfitting . Thus,\nwe select GRU as the news encoder in STUR.\n\n4.5 Effectiveness of News Title Encoders\n\nIn this section, we conduct experiments to com-\npare different news title encoders. In our ap-\nproach, the news encoder is a combination of\nCNN network and an attention network (denoted\nas CNN+Att). We compare it with several vari-\nants, i.e., CNN, LSTM, and LSTM with attention\n(LSTM+Att), to validate the effectiveness of our\n\n342\n", "vlm_text": "The table compares the performance of various methods using four different evaluation metrics: AUC (Area Under the Curve), MRR (Mean Reciprocal Rank), nDCG@5 (Normalized Discounted Cumulative Gain at 5), and nDCG@10 (Normalized Discounted Cumulative Gain at 10). Each method has a corresponding value for each metric, expressed as a mean ± standard deviation. The methods listed are:\n\n1. LibFM\n2. DeepFM\n3. Wide & Deep\n4. DSSM\n5. CNN\n6. DKN\n7. GRU\n8. LSTUR-con\n9. LSTUR-ini\n\nLSTUR-ini has the highest values in all four metrics, suggesting it performs better than the other methods according to these evaluation criteria.\nThe image consists of two bar charts comparing different methods for representing user data in the context of a study on user representations. \n\nThe chart on the left assesses the effectiveness of incorporating long-term and short-term user representations: \n- It evaluates the methods labeled LTUR, STUR, LSTUR-con, and LSTUR-ini using two metrics: AUC and nDCG@10. \n- Each method's performance is represented by bars of different colors: yellow (LTUR), light green (STUR), medium green (LSTUR-con), and dark green (LSTUR-ini).\n- According to the bar chart, the LSTUR-ini method shows the highest performance in both metrics, followed by LSTUR-con, STUR, and LTUR.\n\nThe chart on the right compares different methods for learning short-term user representations from recently browsed news articles:\n- The methods compared include Average, Attention, LSTM, and GRU.\n- The evaluation is again based on AUC and nDCG@10 metrics, with each method represented by different colored bars: yellow (Average), light green (Attention), medium green (LSTM), and dark green (GRU).\n- The GRU method (dark green bar) outperforms others in both AUC and nDCG@10, followed by LSTM, Attention, and then Average.\n\nOverall, both charts are focused on evaluating the effectiveness of various methods for generating user representations using AUC and nDCG@10 as performance metrics.\nresentation model STUR. The results are summa- rized in Fig.  4 . \nFrom the results we ﬁnd both LTUR and STUR are useful for news recommendation, and the STUR model can outperform the LTUR model. According to the statistics in Table  1 , the long- term representations of many users in test data are unavailable, which leads to relative weak per- formance of LTUR on these users. In addition, combining STUR and LTUR using our two long- and short-term user representation methods, i.e., LSTUR-ini and LSTUR-con, can effectively im- prove the performance. This result validates that incorporating both long-term and short-term user representations is useful to capture the diverse user interests more accurately and is beneﬁcial for news recommendation. \n4.4 Effectiveness of News Encoders in STUR \nIn our STUR model, GRU is used to learn short- term user representations from the recent browsing news. We explore the effectiveness of GRU in en- coding news by replacing it with several other en- coders, including: 1) Average: using the average of all the news representations in recent browsing history; 2) Attention: the summation of news rep- resentations weighted by their attention weights; 3) LSTM ( Hochreiter and Schmidhuber ,  1997 ), re- placing GRU with LSTM. The results are summa- rized in Fig.  5 . \n\nAccording to Fig.  5 , the sequence-based en- coders (e.g., GRU, LSTM) outperform the Aver- age and Attention based encoders. This is proba- bly because the sequence-based encoders can cap- ture the sequential new reading patterns to learn short-term representations of users, which is difﬁ- cult for Average and Attention based encoders. In addition, GRU achieves better performance than LSTM. This may be because GRU contains fewer parameters and has lower risk of overﬁtting . Thus, we select GRU as the news encoder in STUR. \n4.5 Effectiveness of News Title Encoders \nIn this section, we conduct experiments to com- pare different news title encoders. In our ap- proach, the news encoder is a combination of CNN network and an attention network (denoted as   $\\mathbf{CNN+Aut}$  ). We compare it with several vari- ants, i.e., CNN, LSTM, and LSTM with attention  $(\\mathrm{LSTM+Aut})$  , to validate the effectiveness of our "}
{"page": 7, "image_path": "doc_images/P19-1033_7.jpg", "ocr_text": "0.640\n\n[= LSTM\n\n0.635 = LsTMratt\n3 CNN\n0.630 om\n\n0.625\n\n0.620\n\n0.615-\n\nLSTUR-ini LSTUR-con\n\n(a) AUC\n\n0.420\n[) LST\n0.415 3 LsTM+att\nGg CNN\n0.410 mmm NN\n\n0.405\n\n0.400\n\n0.395 -\n\nLSTUR-ini\n\nLSTUR-con\n(b) nDCG@ 10\n\nFigure 6: The comparisons of different methods in learning news title representations and the effectiveness of\n\nattention machenism in selecting important words.\n\n0.640\n0.636  +Subtopic\n0.634 =m\n0.632\n0.630\n0.628\n\n0.626 -\n\nLSTUR-ini\n\nLSTUR-con\n(a) AUC\n\n0.420\n\note Ss\n0.416 HG +Subtopic\n0.414 mm “Poth\n0.412\n\n0.410\n\n0.408\n\n0.406-\n\nLSTUR-ini LSTUR-con\n\n(b) nDCG@ 10\n\nFigure 7: The effectiveness of incorporating news topic and subtopic information for news recommendation.\n\napproach. The results are summarized in Fig. 6.\n\nAccording to Fig. 6, using attention mechanism\nin both encoders based on CNN and LSTM can\nachieve better performance. This is probably be-\ncause the attention network can select important\nwords, which can learn more informative news\nrepresentations. In addition, encoders using CNN\noutperform those using LSTM. This may be be-\ncause local contexts in news titles are more impor-\ntant for learning news representations.\n\n4.5.1 Effectiveness of News Topic\n\nIn this section, we conduct experiments to vali-\ndate the effectiveness of incorporating topic and\nsubtopic of news in the news encoder. We com-\npare the performance of our approach with its vari-\nants without topic and/or subtopics. The results\nare shown in Fig. 7.\n\nAccording to Fig. 7, incorporating either top-\nics or subtopics can effectively improve the per-\nformance of our approach. In addition, the news\nencoder with subtopics outperforms the news en-\ncoder with topics. This is probably because\nsubtopics can provide more fine-grained topic in-\n\n343\n\nformation which is more helpful for news rec-\nommendation. Thus, the model with subtopics\ncan achieve better news recommendation perfor-\nmance. Moreover, combining topics and subtopics\ncan further improve the performance of our ap-\nproach. These results validate the effectiveness of\nour approach in exploiting topic information for\nnews recommendation.\n\n4.5.2 Influence of Masking Probability\n\nIn this section, we explore the influence of the\nprobability p in Eq. (6) for randomly masking\nlong-term user representation in model training.\nWe vary the value of p from 0.0 to 0.9 with a step\nof 0.1 for both LSTUR-ini and LSTUR-con. The\nresults are summarized in Fig. 8.\n\nAccording to Fig. 8, the results of LSTUR-ini\nand LSTUR-con have similar patterns. The perfor-\nmance of both methods improves when p increases\nfrom 0. When p is too small, the model will tend\nto overfit on the LTUR, since LTUR has many pa-\nrameters. Thus, the performance is not optimal.\nHowever, when p is too large, the performance of\nboth methods starts to decline. This may be be-\n", "vlm_text": "The image is composed of two bar charts comparing different methods for learning news title representations. Here's a breakdown:\n\n1. **Methods Compared**:\n   - LSTM\n   - LSTM with Attention (LSTM+Att)\n   - CNN\n   - CNN with Attention (CNN+Att)\n\n2. **Metrics Used**:\n   - (a) AUC (Area Under the Curve)\n   - (b) nDCG@10 (Normalized Discounted Cumulative Gain)\n\n3. **Comparison**:\n   - **LSTUR-ini** and **LSTUR-con** are the two conditions or settings being compared across both metrics.\n   - For each condition, the performance of the methods is shown with bars of different colors, representing the performance differences and the impact of using the attention mechanism.\n\nThe charts illustrate that the methods incorporating attention generally perform better in both metrics.\nThe image consists of two bar charts showing the effectiveness of incorporating news topic and subtopic information into news recommendation systems. There are two conditions being compared: \"LSTUR-ini\" and \"LSTUR-con.\"\n\n1. The left chart, labeled \"(a) AUC,\" compares the Area Under the Curve (AUC) metric for four cases: \n\n   - Yellow bar: None (no additional topic or subtopic information)\n   - Light green bar: +Topic (with topic information)\n   - Dark green bar: +Subtopic (with subtopic information)\n   - Teal bar: +Both (with both topic and subtopic information)\n\n   For both LSTUR-ini and LSTUR-con, incorporating topic and/or subtopic information improves the AUC, with the combination of both (\"+Both\") resulting in the highest AUC values.\n\n2. The right chart, labeled \"(b) nDCG@10,\" compares the normalized Discounted Cumulative Gain at rank 10 (nDCG@10) for the same four cases. Similar to the AUC chart, the incorporation of topic and/or subtopic information improves the nDCG@10 scores, with \"+Both\" again showing the highest values for both LSTUR-ini and LSTUR-con.\n\nIn summary, adding topic and subtopic information enhances the performance metrics (AUC and nDCG@10) for news recommendation, with the greatest enhancement observed when both pieces of information are incorporated.\napproach. The results are summarized in Fig.  6 . \nAccording to Fig.  6 , using attention mechanism in both encoders based on CNN and LSTM can achieve better performance. This is probably be- cause the attention network can select important words, which can learn more informative news representations. In addition, encoders using CNN outperform those using LSTM. This may be be- cause local contexts in news titles are more impor- tant for learning news representations. \n4.5.1 Effectiveness of News Topic \nIn this section, we conduct experiments to vali- date the effectiveness of incorporating topic and subtopic of news in the news encoder. We com- pare the performance of our approach with its vari- ants without topic and/or subtopics. The results are shown in Fig.  7 . \nAccording to Fig.  7 , incorporating either top- ics or subtopics can effectively improve the per- formance of our approach. In addition, the news encoder with subtopics outperforms the news en- coder with topics. This is probably because subtopics can provide more ﬁne-grained topic in- formation which is more helpful for news rec- ommendation. Thus, the model with subtopics can achieve better news recommendation perfor- mance. Moreover, combining topics and subtopics can further improve the performance of our ap- proach. These results validate the effectiveness of our approach in exploiting topic information for news recommendation. \n\n4.5.2 Inﬂuence of Masking Probability \nIn this section, we explore the inﬂuence of the probability    $p$   in Eq. ( 6 ) for randomly masking long-term user representation in model training. We vary the value of  $p$   from 0.0 to 0.9 with a step of 0.1 for both LSTUR-ini and LSTUR-con. The results are summarized in Fig.  8 . \nAccording to Fig.  8 , the results of LSTUR-ini and LSTUR-con have similar patterns. The perfor- mance of both methods improves when    $p$   increases from 0. When  $p$   is too small, the model will tend to overﬁt on the LTUR, since LTUR has many pa- rameters. Thus, the performance is not optimal. However, when  $p$   is too large, the performance of both methods starts to decline. This may be be- "}
{"page": 8, "image_path": "doc_images/P19-1033_8.jpg", "ocr_text": "@— AUC —=—- MRR —e— nDCG@5—- nDCG@ 10\n64.0\n63.0 eet tte\nALS ;\npitts,\n40.5 E\n34.0 F-\n33.0 Cette,\n32.0\n31.0\n30.0\n0.0 0.3 0.6 0.9\nMask probability p\n(a) LSTUR-ini.\n\n@— AUC —m— MRR —e~ nDCG@5—— nDCG@ 10\n64.0\n\n63.0 ae\n41.5\n\n—*—E-f\n\n40.5 Ny\n34.0 =\"\n33.0 ane\n32.0\n\n31.0\n\n30.0 a\n\n0.0 0.3 0.6\n\nMask probability p\n\n0.9\n\n(b) LSTUR-con.\n\nFigure 8: The influence of mask probability p on the performance of our approach.\n\n2019 CES Highlights : Innovations in Enviro-Sensing for Robocars\nCalifornia dries off after storm batter state for days\n\n15 Recipes Inspired By Vintage Movies\n\nTexas State Rep . Dennis Bonnen Elected As House Speaker\nShould You Buy American Express Stock After Earnings ?\n\nHow Meghan Markle Has Changed Prince Harry Considerably\n\nFigure 9: Visualization of the word-level attentions.\n\ncause the useful information in LTUR cannot be\neffectively incorporated. Thus, the performance is\nalso not optimal. A moderate choice on p (e.g.,\n0.5) is most appropriate for both LSTUR-ini and\nLSTUR-con methods, which can properly balance\nthe learning of LTUR and STUR.\n\n5 Visualization of Attention Weights\n\nIn this section, we visually explore the effective-\nness of the word-level attention network in the\nnews encoder. The attention weights in several\nexample news titles are shown in Fig. 9. From\nthe results, we find our approach can effectively\nrecognize important words to learn more infor-\nmative news representations. For example, the\nwords “Enviro-Sensing” and “Robocars” in the\nfirst news title are assigned high attention weights\nbecause these words are indications of news on\ntechnologies, while the words “2019” and “for”\nare assigned low attention weights by our ap-\nproach since they are less informative. These re-\nsults validate the effectiveness of the attention net-\nwork in the news encoder.\n\n344\n\n6 Conclusion\n\nIn this paper, we propose a neural news recom-\nmendation approach which can learn both long-\nand short-term user representations. The core of\nour model is a news encoder and a user encoder.\nIn the news encoder, we learn representations of\nnews from their titles and topic categories, and use\nan attention network to highlight important words\nfor informative representation learning. In the user\nencoder, we propose to learn long-term represen-\ntations of users from the embeddings of their IDs.\nIn addition, we learn short-term representations of\nusers from their recently browsed news via a GRU\nnetwork. Besides, we propose two methods to fuse\nlong- and short-term user representations, i.e., us-\ning long-term user representation to initialize the\nhidden state of the GRU network in short-term\nuser representation, or concatenating both long-\nand short-term user representations as a unified\nuser vector. Extensive experiments on a real-world\ndataset collected from MSN news show our ap-\nproach can effecitively improve the performance\nof news recommendation.\n\nAcknowledgement\n\nThe authors would like to thank Microsoft News\nfor providing technical support and data in the ex-\nperiments, and Jiun-Hung Chen (Microsoft News)\nand Ying Qiao (Microsoft News) for their support\nand discussions. We also want to thank Jiangiang\nHuang for his help in the experiments.\n", "vlm_text": "The image consists of two line charts comparing the influence of mask probability \\( p \\) on the performance of two approaches: LSTUR-ini (on the left) and LSTUR-con (on the right). \n\nEach chart includes four metrics:\n\n- **AUC**: Area under the curve, shown with green circles.\n- **MRR**: Mean reciprocal rank, shown with orange squares.\n- **nDCG@5**: Normalized discounted cumulative gain at rank 5, shown with blue circles.\n- **nDCG@10**: Normalized discounted cumulative gain at rank 10, shown with red stars.\n\nThe x-axis represents the mask probability \\( p \\) ranging from 0.0 to 0.9. The y-axis represents percentage values for each metric. Both charts show how these metrics change as the mask probability increases.\n2019 CES Highlights  $:$   Innovations in Enviro-Sensing for Robocars California dries off after storm batter state for days 15 Recipes Inspired By Vintage Movies Texas State Rep . Dennis Bonnen Elected As House Speaker Should You Buy American Express Stock After Earnings ? How Meghan Markle Has Changed Prince Harry Considerably \nFigure 9: Visualization of the word-level attentions. \ncause the useful information in LTUR cannot be effectively incorporated. Thus, the performance is also not optimal. A moderate choice on    $p$   (e.g., 0.5) is most appropriate for both LSTUR-ini and LSTUR-con methods, which can properly balance the learning of LTUR and STUR. \n5 Visualization of Attention Weights \nIn this section, we visually explore the effective- ness of the word-level attention network in the news encoder. The attention weights in several example news titles are shown in Fig.  9 . From the results, we ﬁnd our approach can effectively recognize important words to learn more infor- mative news representations. For example, the words “Enviro-Sensing” and “Robocars” in the ﬁrst news title are assigned high attention weights because these words are indications of news on technologies, while the words “2019” and “for” are assigned low attention weights by our ap- proach since they are less informative. These re- sults validate the effectiveness of the attention net- work in the news encoder. \n6 Conclusion \nIn this paper, we propose a neural news recom- mendation approach which can learn both long- and short-term user representations. The core of our model is a news encoder and a user encoder. In the news encoder, we learn representations of news from their titles and topic categories, and use an attention network to highlight important words for informative representation learning. In the user encoder, we propose to learn long-term represen- tations of users from the embeddings of their IDs. In addition, we learn short-term representations of users from their recently browsed news via a GRU network. Besides, we propose two methods to fuse long- and short-term user representations, i.e., us- ing long-term user representation to initialize the hidden state of the GRU network in short-term user representation, or concatenating both long- and short-term user representations as a uniﬁed user vector. Extensive experiments on a real-world dataset collected from MSN news show our ap- proach can effecitively improve the performance of news recommendation. \nAcknowledgement \nThe authors would like to thank Microsoft News for providing technical support and data in the ex- periments, and Jiun-Hung Chen (Microsoft News) and Ying Qiao (Microsoft News) for their support and discussions. We also want to thank Jianqiang Huang for his help in the experiments. "}
{"page": 9, "image_path": "doc_images/P19-1033_9.jpg", "ocr_text": "References\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In JCLR.\n\nTrapit Bansal, Mrinal Das, and Chiranjib Bhat-\ntacharyya. 2015. Content driven user profiling\nfor comment-worthy recommendations of news and\nblog articles. In RecSys, pages 195-202.\n\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of Ma-\nchine Learning Research, 3(Jan):993-1022.\n\nHeng-Tze Cheng, Mustafa Ispir, Rohan Anil, Zakaria\nHaque, Lichan Hong, Vihan Jain, Xiaobing Liu,\nHemal Shah, Levent Koc, Jeremiah Harmsen, Tal\nShaked, Tushar Chandra, Hrishi Aradhye, Glen An-\nderson, Greg Corrado, and Wei Chai. 2016. Wide &\ndeep learning for recommender systems. In DLRS,\npages 7-10.\n\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder—decoder\nfor statistical machine translation. In EMNLP, pages\n1724-1734.\n\nAbhinandan S. Das, Mayur Datar, Ashutosh Garg,\nand Shyam Rajaram. 2007. Google news person-\nalization: scalable online collaborative filtering. In\nWWW, pages 271-280.\n\nHuifeng Guo, Ruiming TANG, Yunming Ye, Zhen-\nguo Li, and Xiuqiang He. 2017. DeepFM:\nA factorization-machine based neural network for\nCTR prediction. In JJCAI, pages 1725-1731.\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735-1780.\n\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In CIKM, pages 2333-2338.\n\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In EMNLP, pages 1746-\n1751.\n\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\n\nTalia Lavie, Michal Sela, [lit Oppenheim, Ohad Inbar,\nand Joachim Meyer. 2010. User attitudes towards\nnews content personalization. International Journal\nof Human-Computer Studies, 68(8):483-495.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton.\n2015. Deep learning. Nature, 521(7553):436-444.\n\n345\n\nLei Li, Li Zheng, Fan Yang, and Tao Li. 2014. Model-\ning and broadening temporal user interest in person-\nalized news recommendation. Expert Systems with\nApplications, 41(7):3 168-3177.\n\nLihong Li, Wei Chu, John Langford, and Robert E.\nSchapire. 2010. A contextual-bandit approach to\n\npersonalized news article recommendation. In\nWWW, pages 661-670.\nJianxun Lian, Fuzheng Zhang, Xing Xie, and\n\nGuangzhong Sun. 2018. Towards better represen-\ntation learning for personalized news recommenda-\ntion: a multi-channel deep fusion approach. In JJ-\nCAI, pages 3805-3811.\n\nJiahui Liu, Peter Dolan, and Elin Rgnby Pedersen.\n2010. Personalized news recommendation based on\nclick behavior. In JUI, pages 31-40.\n\nShumpei Okura, Yukihiro Tagami, Shingo Ono, and\nAkira Tajima. 2017. Embedding-based news rec-\nommendation for millions of users. In KDD, pages\n1933-1942.\n\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP, pages 1532-1543.\n\nOwen Phelan, Kevin McCarthy, and Barry Smyth.\n2009. Using twitter to recommend real-time topical\nnews. In RecSys, pages 385-388.\n\nSteffen Rendle. 2012. Factorization machines with\nlibFM. ACM Transactions on Intelligent Systems\nand Technology, 3(3):1—22.\n\nJeong-Woo Son, A-Yeong Kim, and Seong-Bae Park.\n2013. A location-based news article recommenda-\ntion with explicit localized semantic analysis. In SJ-\nGIR, pages 293-302.\n\nNitish Srivastava, Geoffrey E. Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. 2014. Dropout: a simple way to prevent neural\nnetworks from overfitting. Journal of Machine\nLearning Research, 15(1):1929-1958.\n\nHongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi\nGuo. 2018. DKN: Deep knowledge-aware network\nfor news recommendation. In WWW, pages 1835—\n1844.\n\nChuhan Wu, Fangzhao Wu, Mingxiao An, Jiangiang\nHuang, Yongfeng Huang, and Xing Xie. 2019.\nNPA: Neural news recommendation with personal-\nized attention. In KDD.\n\nShuangfei Zhai, Keng hao Chang, Ruofei Zhang, and\nZhongfei Mark Zhang. 2016. Deepintent: Learning\nattentions for online advertising with recurrent neu-\nral networks. In KDD, pages 1295-1304.\n\nGuanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang\nXiang, Nicholas Jing Yuan, Xing Xie, and Zhen-\nhui Li. 2018. DRN: A deep reinforcement learn-\ning framework for news recommendation. In WWW,\npages 167-176.\n", "vlm_text": "References \nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate . In  ICLR . Trapit Bansal, Mrinal Das, and Chiranjib Bhat- tacharyya. 2015. Content driven user proﬁling for comment-worthy recommendations of news and blog articles . In  RecSys , pages 195–202. David M Blei, Andrew   $\\mathrm{~Y~Ng~}$  , and Michael I Jordan. 2003.  Latent dirichlet allocation .  Journal of Ma- chine Learning Research , 3(Jan):993–1022. Heng-Tze Cheng, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen An- derson, Greg Corrado, and Wei Chai. 2016.  Wide & deep learning for recommender systems . In  DLRS , pages 7–10. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation . In  EMNLP , pages 1724–1734. Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.  Google news person- alization: scalable online collaborative ﬁltering . In WWW , pages 271–280. Huifeng Guo, Ruiming TANG, Yunming Ye, Zhen- guo Li, and Xiuqiang He. 2017. DeepFM: A factorization-machine based neural network for CTR prediction . In  IJCAI , pages 1725–1731. Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory . Neural Computation , 9(8):1735–1780. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013.  Learning deep structured semantic models for web search using clickthrough data . In  CIKM , pages 2333–2338. Yoon Kim. 2014.  Convolutional neural networks for sentence classiﬁcation . In  EMNLP , pages 1746– 1751. Diederik P Kingma and Jimmy Ba. 2014.  Adam: A method for stochastic optimization .  arXiv preprint arXiv:1412.6980.Talia Lavie, Michal Sela, Ilit Oppenheim, Ohad Inbar, and Joachim Meyer. 2010.  User attitudes towards news content personalization .  International Journal of Human-Computer Studies , 68(8):483–495. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015.  Deep learning .  Nature , 521(7553):436–444. \nLei Li, Li Zheng, Fan Yang, and Tao Li. 2014.  Model- ing and broadening temporal user interest in person- alized news recommendation .  Expert Systems with Applications , 41(7):3168–3177. Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A contextual-bandit approach to personalized news article recommendation . In WWW , pages 661–670. Jianxun Lian, Fuzheng Zhang, Xing Xie, and Guangzhong Sun. 2018.  Towards better represen- tation learning for personalized news recommenda- tion: a multi-channel deep fusion approach . In  IJ- CAI , pages 3805–3811. Jiahui Liu, Peter Dolan, and Elin Rønby Pedersen. 2010.  Personalized news recommendation based on click behavior . In  IUI , pages 31–40. Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. Embedding-based news rec- ommendation for millions of users . In  KDD , pages 1933–1942. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation . In  EMNLP , pages 1532–1543. Owen Phelan, Kevin McCarthy, and Barry Smyth. 2009.  Using twitter to recommend real-time topical news . In  RecSys , pages 385–388. Steffen Rendle. 2012. Factorization machines with libFM . ACM Transactions on Intelligent Systems and Technology , 3(3):1–22. Jeong-Woo Son, A-Yeong Kim, and Seong-Bae Park. 2013.  A location-based news article recommenda- tion with explicit localized semantic analysis . In  SI- GIR , pages 293–302. Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. 2014.  Dropout: a simple way to prevent neural networks from overﬁtting . Journal of Machine Learning Research , 15(1):1929–1958. Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018.  DKN: Deep knowledge-aware network for news recommendation . In  WWW , pages 1835– 1844. Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. NPA: Neural news recommendation with personal- ized attention. In  KDD . Shuangfei Zhai, Keng hao Chang, Ruofei Zhang, and Zhongfei Mark Zhang. 2016.  Deepintent: Learning attentions for online advertising with recurrent neu- ral networks . In  KDD , pages 1295–1304. Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhen- hui Li. 2018. DRN: A deep reinforcement learn- ing framework for news recommendation . In  WWW , pages 167–176. "}
