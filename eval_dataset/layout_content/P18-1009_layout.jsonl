{"layout": 0, "type": "text", "text": "Ultra-Fine Entity Typing ", "text_level": 1, "page_idx": 0, "bbox": [222, 68, 377, 86], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Eunsol Choi † Omer Levy † Yejin Choi † ♯ Luke Zettlemoyer † † Paul G. Allen School of Computer Science & Engineering, University of Washington ♯ Allen Institute for Artiﬁcial Intelligence, Seattle WA { eunsol,omerlevy,yejin,lsz } @cs.washington.edu ", "page_idx": 0, "bbox": [94.22900390625, 113.1820068359375, 506.3067626953125, 178.198974609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 224, 204, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation al- lows us to use a new type of distant super- vision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-ﬁne types can be crowd-sourced, and intro- duce new evaluation sets that are much more diverse and ﬁne-grained than exist- ing benchmarks. We present a model that can predict open types, and is trained using a multitask objective that pools our new head-word supervision with prior supervi- sion from entity linking. Experimental re- sults demonstrate that our model is effec- tive in predicting entity types at varying granularity; it achieves state of the art per- formance on an existing ﬁne-grained en- tity typing benchmark, and sets baselines for our newly-introduced datasets. ", "page_idx": 0, "bbox": [89, 246.2949981689453, 273, 571.0715942382812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 580, 155, 593], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "Entities can often be described by very ﬁne grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very speciﬁc types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “crimi- nal” due to the “robbing” and “arresting,” while\n\n “John” is more likely a “victim” because he was\n\n “robbed.” Such ﬁne-grained types (victim, crimi- nal) are important for context-sensitive tasks such ", "page_idx": 0, "bbox": [71, 601.9981079101562, 290, 737.0865478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "table", "page_idx": 0, "img_path": "layout_images/P18-1009_0.jpg", "bbox": [306, 221, 527, 337], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Sentence with Target Entity\n\nEntity Types\n\nDuring the Inca Empire, {the Inti\nRaymi} was the most important\nof four ceremonies celebrated in\nCusco.\n\nevent, festival, rit-\nual, custom, cere-\nmony, party, cele-\nbration\n\n{They} have been asked to appear\nin court to face the charge.\n\nperson, accused,\nsuspect, defendant\n\nBan praised Rwanda’s commit-\nment to the UN and its role in\n{peacemaking operations}.\n\nevent, plan, mis-\nsion, action\n\n", "vlm_text": "The table consists of two columns: \"Sentence with Target Entity\" and \"Entity Types.\"\n\n1. **Sentence with Target Entity**:\n   - The first sentence is: \"During the Inca Empire, {the Inti Raymi} was the most important of four ceremonies celebrated in Cusco.\"\n   - The second sentence is: \"{They} have been asked to appear in court to face the charge.\"\n   - The third sentence is: \"Ban praised Rwanda’s commitment to the UN and its role in {peacemaking operations}.\"\n\n2. **Entity Types**:\n   - For the target entity \"the Inti Raymi\" in the first sentence, the entity types are: event, festival, ritual, custom, ceremony, party, celebration.\n   - For the target entity \"They\" in the second sentence, the entity types are: person, accused, suspect, defendant.\n   - For the target entity \"peacemaking operations\" in the third sentence, the entity types are: event, plan, mission, action."}
{"layout": 7, "type": "text", "text": "Table 1: Examples of entity mentions and their an- notated types, as annotated in our dataset. The en- tity mentions are bold faced and in the curly brack- ets. The bold blue types do not appear in existing ﬁne-grained type ontologies. ", "page_idx": 0, "bbox": [307, 349.4150085449219, 525, 416.75750732421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "as coreference resolution and question answering (e.g. “Who was the victim?”). Inferring such types for each mention (John, he) is not possible given current typing models that only predict relatively coarse types and only consider named entities. ", "page_idx": 0, "bbox": [307, 427.3000183105469, 525, 494.64251708984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "To address this challenge, we present a new task: given a sentence with a target entity men- tion, predict free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Table  1  shows three examples that exhibit a rich variety of types at different granular- ities. Our task effectively subsumes existing ﬁne- grained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, includ- ing named entities, nominals, and pronouns. ", "page_idx": 0, "bbox": [307, 495.2490539550781, 525, 643.8855590820312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "Incorporating ﬁne-grained entity types has im- proved entity-focused downstream tasks, such as relation extraction ( Yaghoobzadeh et al. ,  2017a ), question answering ( Yavuz et al. ,  2016 ), query analysis ( Balog and Neumayer ,  2012 ), and coref- erence resolution ( Durrett and Klein ,  2014 ). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difﬁcult to cover all pos- ", "page_idx": 0, "bbox": [307, 644.4921264648438, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "image", "page_idx": 1, "img_path": "layout_images/P18-1009_1.jpg", "img_caption": "Figure 1: A visualization of all the labels that cover  $90\\%$   of the data, where a bubble’s size is proportional https://homes.cs.washington.edu/\\~eunsol/ne type visualization/onto_index.html 1/1 to the label’s frequency. Our dataset is much more diverse and ﬁne grained when compared to existing datasets (OntoNotes and FIGER), in which the top 5 types cover  $70.80\\%$   of the data. ", "bbox": [70, 73, 527, 297], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "a) Our Dataset\n\nb) OntoNotes\n\n_ - “s Aocation\niment .\n\nPerson organization\n\nc) FIGER\n", "vlm_text": "The image is a visual comparison of three datasets, demonstrating the diversity and granularity of labeled data types within each dataset. The caption describes this comparison, highlighting how these datasets cover different proportions of specific label types.\n\n1. **(a) Our Dataset**: This visualization shows a large central bubble labeled \"Person,\" surrounded by many smaller bubbles with labels such as \"leader,\" \"object,\" \"event,\" \"organization,\" and more. The abundance of different smaller bubbles indicates a diverse and fine-grained dataset, where the label \"Person\" covers a significant yet relatively smaller percentage of the dataset compared to the others.\n\n2. **(b) OntoNotes**: This shows a large bubble labeled \"Other\" taking up a significant portion of the space, with smaller bubbles labeled \"/company,\" \"/location,\" \"/person,\" \"/organization,\" \"/legal,\" and \"/country.\" This indicates that OntoNotes has broader, less specific categories, with \"Other\" being the most frequent category in the dataset.\n\n3. **(c) FIGER**: Here, the bubble labeled \"Person\" is the largest among various labeled categories such as \"/organization,\" \"/location,\" \"/event,\" and others. FIGER, similar to OntoNotes, has a few large categories, with \"Person\" being the most frequent label.\n\nOverall, the image illustrates that \"Our Dataset\" is more diverse and fine-grained than the other two, as indicated by the many different smaller bubbles representing a variety of specific labels. In contrast, OntoNotes and FIGER have fewer categories, with a significant portion of their data concentrated in a handful of broad labels."}
{"layout": 12, "type": "text", "text": "sible concepts even within a limited domain. This can be seen empirically in existing datasets, where the label distribution of ﬁne-grained entity typing datasets is heavily skewed toward coarse-grained types. For instance, annotators of the OntoNotes dataset ( Gillick et al. ,  2014 ) marked about half of the mentions as “other,” because they could not ﬁnd a suitable type in their ontology (see Figure  1 for a visualization and Section  2.2  for details). ", "page_idx": 1, "bbox": [71, 316.72894287109375, 290, 438.2674865722656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "Our more open, ultra-ﬁne vocabulary, where types are free-form noun phrases, alleviates the need for hand-crafted ontologies, thereby greatly increasing overall type coverage. To better un- derstand entity types in an unrestricted setting, we crowdsource a new dataset of 6,000 examples. Compared to previous ﬁne-grained entity typing datasets, the label distribution in our data is sub- stantially more  diverse  and  ﬁne-grained . Annota- tors easily generate a wide range of types and can determine with   $85\\%$   agreement if a type generated by another annotator is appropriate. Our evalu- ation data has over 2,500 unique types, posing a challenging learning problem. ", "page_idx": 1, "bbox": [71, 439.9630126953125, 290, 629.2474975585938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "While our types are harder to predict, they also allow for a new form of contextual distant super- vision. We observe that text often contains cues that explicitly match a mention to its type, in the form of the mention’s head word. For example, “the incumbent chairman of the African Union” is a type of “chairman.” This signal comple- ments the supervision derived from linking entities to knowledge bases, which is context-oblivious. For example, “Clint Eastwood” can be described with dozens of types, but context-sensitive typing would prefer “director” instead of “mayor” for the sentence “Clint Eastwood won ‘Best Director’ for Million Dollar Baby.” ", "page_idx": 1, "bbox": [71, 630.9430541992188, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "", "page_idx": 1, "bbox": [307, 316.72808837890625, 525, 370.5215759277344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "We combine head-word supervision, which pro- vides ultra-ﬁne type labels, with traditional sig- nals from entity linking. Although the problem is more challenging at ﬁner granularity, we ﬁnd that mixing ﬁne and coarse-grained supervision helps signiﬁcantly, and that our proposed model with a multitask objective exceeds the performance of existing entity typing models. Lastly, we show that head-word supervision can be used for previ- ous formulations of entity typing, setting the new state-of-the-art performance on an existing ﬁne- grained NER benchmark. ", "page_idx": 1, "bbox": [307, 370.92510986328125, 525, 533.1116333007812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "2 Task and Data ", "text_level": 1, "page_idx": 1, "bbox": [306, 542, 400, 555], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "Given a sentence and an entity mention    $e$   within it, the task is to predict a set of natural-language phrases    $T$   that describe the type of    $e$  . The selec- tion of  $T$  is context sensitive; for example, in “Bill Gates has donated billions to eradicate malaria,” Bill Gates should be typed as “philanthropist” and not “inventor.” This distinction is important for context-sensitive tasks such as coreference resolu- tion and question answering (e.g. “Which philan- thropist is trying to prevent malaria?”). ", "page_idx": 1, "bbox": [307, 563.1972045898438, 525, 698.2855834960938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "We annotate a dataset of about 6,000 mentions via crowdsourcing (Section  2.1 ), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section  2.2 ). ", "page_idx": 1, "bbox": [307, 698.6891479492188, 525, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "2.1 Crowdsourcing Entity Types ", "text_level": 1, "page_idx": 2, "bbox": [71, 65, 230, 77], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "To capture multiple domains, we sample sentences from Gigaword ( Parker et al. ,  2011 ), OntoNotes ( Hovy et al. ,  2006 ), and web articles ( Singh et al. , 2012 ). We select entity mentions by taking max- imal noun phrases from a constituency parser ( Manning et al. ,  2014 ) and mentions from a coref- erence resolution system ( Lee et al. ,  2017 ). ", "page_idx": 2, "bbox": [71, 82.31104278564453, 290, 176.75155639648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "We provide the sentence and the target entity mention to ﬁve crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate ﬁne-grained types, we require at least one general type (e.g. person, organization, location) and two speciﬁc types (e.g. doctor, ﬁsh, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet ( Miller ,  1995 ) to ex- pand these types automatically by generating all their synonyms and hypernyms based on the most common sense, and ask ﬁve different annotators to validate the generated types. Each pair of annota- tors agreed on   $85\\%$   of the binary validation deci- sions (i.e. whether a type is suitable or not) and 0.47 in Fleiss’s    $\\kappa$  . To further improve consistency, the ﬁnal type set contained only types selected by at least  $3/5$   annotators. Further crowdsourcing de- tails are available in the supplementary material. ", "page_idx": 2, "bbox": [71, 177.73207092285156, 290, 434.7636413574219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "Our collection process focuses on precision. Thus, the ﬁnal set is diverse but not comprehen- sive, making evaluation non-trivial (see Section  5 ). ", "page_idx": 2, "bbox": [71, 435.74517822265625, 290, 475.9886779785156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "2.2 Data Analysis ", "text_level": 1, "page_idx": 2, "bbox": [72, 487, 161, 499], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "We collected about 6,000 examples. For analysis, we classiﬁed each type into three disjoint bins:\n\n ", "page_idx": 2, "bbox": [71, 505.85321044921875, 290, 532.547607421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "•  9  general  types: person, location, object, orga- nization, place, entity, object, time, event\n\n •  121  ﬁne-grained  types, mapped to ﬁne-grained entity labels from prior work ( Ling and Weld , 2012 ;  Gillick et al. ,  2014 ) (e.g. ﬁlm, athlete)\n\n •  10,201  ultra-ﬁne  types, encompassing every other label in the type space (e.g. detective, law- suit, temple, weapon, composer) ", "page_idx": 2, "bbox": [71, 534.1325073242188, 290, 642.5146484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "On average, each example has 5 labels: 0.9 gen- eral, 0.6 ﬁne-grained, and 3.9 ultra-ﬁne types. Among the 10,000 ultra-ﬁne types, 2,300 unique types were actually found in the 6,000 crowd- sourced examples. Nevertheless, our distant su- pervision data (Section  3 ) provides positive train- ing examples for every type in the entire vocabu- lary, and our model (Section  4 ) can and does pre- dict from a 10K type vocabulary. For example, ", "page_idx": 2, "bbox": [71, 644.4921875, 290, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "image", "page_idx": 2, "img_path": "layout_images/P18-1009_2.jpg", "img_caption": "Figure 2: The label distribution across different evaluation datasets. In existing datasets, the top 4 or 7 labels cover over   $80\\%$   of the labels. In ours, the top 50 labels cover less than   $50\\%$   of the data. ", "bbox": [306, 64, 532, 287], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Cumulative Proportion of Covered labels\n\nas\n\nas\n\na4\n\na2\n\nao\n\n— Our Dataset\nsee FIGER:\nOntoNotes\n\nT T\n2 2 40\n\nNumber of Labels\n\n8+\n", "vlm_text": "The image is a line graph showing the cumulative proportion of covered labels as the number of labels increases, across three different evaluation datasets: \"Our Dataset,\" \"FIGER,\" and \"OntoNotes.\" The x-axis represents the number of labels, ranging from 0 to 50, while the y-axis represents the cumulative proportion of covered labels, ranging from 0.0 to 1.0.\n\nThe graph has three lines representing three datasets:\n1. A blue solid line for \"Our Dataset,\" which shows a gradual increase in the cumulative proportion of covered labels as the number of labels increases. Even with 50 labels, it covers less than 50% of the total data.\n2. A red dashed line for \"FIGER,\" which shows a steep increase initially, reaching over 80% coverage with fewer than 10 labels, and then plateauing.\n3. A green dotted line for \"OntoNotes,\" which also shows a rapid increase and reaches over 80% coverage with fewer than 7 labels, and then levels off.\n\nThe caption clarifies that the existing datasets (FIGER and OntoNotes) have a high concentration of coverage within the top few labels, whereas their dataset achieves broader coverage with a more diverse spread across 50 labels."}
{"layout": 29, "type": "text", "text": "the model correctly predicts “television network” and “archipelago” for some mentions, even though that type never appears in the 6,000 crowdsourced examples. ", "page_idx": 2, "bbox": [307, 302.70001220703125, 525, 356.4934997558594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "Improving Type Coverage We observe that prior ﬁne-grained entity typing datasets are heav- ily focused on coarse-grained types. To quan- tify our observation, we calculate the distribu- tion of types in FIGER ( Ling and Weld ,  2012 ), OntoNotes ( Gillick et al. ,  2014 ), and our data. For examples with multiple types   $(|T|\\,>\\,1)$  , we counted each type    $1/|T|$   times. ", "page_idx": 2, "bbox": [307, 364.59930419921875, 525, 473], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "Figure  2  shows the percentage of labels covered by the top    $N$   labels in each dataset. In previous enitity typing datasets, the distribution of labels is highly skewed towards the top few labels. To cover   $80\\%$   of the examples, FIGER requires only the top 7 types, while OntoNotes needs only 4; our dataset requires 429 different types. ", "page_idx": 2, "bbox": [307, 473.5960998535156, 525, 568.03662109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "Figure  1  takes a deeper look by visualizing the types that cover   $90\\%$   of the data, demonstrating the diversity of our dataset. It is also striking that more than half of the examples in OntoNotes are classiﬁed as “other,” perhaps because of the limi- tation of its predeﬁned ontology. ", "page_idx": 2, "bbox": [307, 568.6521606445312, 525, 649.5435791015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "Improving Mention Coverage Existing datasets focus mostly on named entity mentions, with the exception of OntoNotes, which contained nominal expressions. This has implications on the transferability of FIGER/OntoNotes-based models to tasks such as coreference resolution, which need to analyze all types of entity mentions (pronouns, nominal expressions, and named entity ", "page_idx": 2, "bbox": [307, 657.6484375, 525, 766.0315551757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "table", "page_idx": 3, "img_path": "layout_images/P18-1009_3.jpg", "table_footnote": "Table 2: Distant supervision examples and statistics. We extracted the headword and Wikipedia def- inition supervision from Gigaword and Wikilink corpora. KB-based supervision is mapped from prior work, which used Wikipedia and news corpora. ", "bbox": [70, 62, 526, 243], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "+KB\n\nSource Example Sentence Labels Size | Prec.\n. | Western powers that brokered the proposed deal in Vi- | power\nHead Words enna are likely to balk, said Valerie Lincy, a researcher\nwith the Wisconsin Project. 20M | 80.4%\nAlexis Kaniaris, CEO of the organizing company Eu- | radio, station, — ra-\nropartners, explained, speaking in a radio program in na- | dio-_station\ntional radio station NET.\nEntity Linking | Toyota recalled more than 8 million vehicles globally over | manufacturer 2.7M | 77.7%\n+ Definitions sticky pedals that can become entrapped in floor mats.\nEntity Linking | Iced Earth’s musical style is influenced by many traditional | person, artist, actor, | 2.5M | 77.6%\n\nheavy metal groups such as Black Sabbath.\n\nauthor, musician\n", "vlm_text": "The table provides information on data sources and their characteristics. It is divided into five columns: \"Source,\" \"Example Sentence,\" \"Labels,\" \"Size,\" and \"Prec. (Precision).\"\n\n1. **Source:** \n   - \"Head Words\"\n   - \"Entity Linking + Definitions\"\n   - \"Entity Linking + KB\"\n\n2. **Example Sentence:**\n   - For \"Head Words,\" examples include discussions about Western powers in Vienna and a Greek radio program.\n   - For \"Entity Linking + Definitions,\" the example sentence talks about Toyota recalling vehicles.\n   - For \"Entity Linking + KB,\" the example describes Iced Earth’s musical influence from Black Sabbath.\n\n3. **Labels:** \n   - \"Head Words\" has labels such as \"power\" and \"radio, station, radio_station.\"\n   - \"Entity Linking + Definitions\" includes the label \"manufacturer.\"\n   - \"Entity Linking + KB\" has labels like \"person, artist, actor, author, musician.\"\n\n4. **Size:** \n   - \"Head Words\" is 20 million.\n   - \"Entity Linking + Definitions\" is 2.7 million.\n   - \"Entity Linking + KB\" is 2.5 million.\n\n5. **Prec. (Precision):**\n   - \"Head Words\" has a precision of 80.4%.\n   - \"Entity Linking + Definitions\" has a precision of 77.7%.\n   - \"Entity Linking + KB\" has a precision of 77.6%."}
{"layout": 35, "type": "text", "text": "mentions). Our new dataset provides a well- rounded benchmark with roughly   $40\\%$   pronouns,  $38\\%$   nominal expressions, and  $22\\%$   named entity mentions. The case of pronouns is particularly interesting, since the mention itself provides little information. ", "page_idx": 3, "bbox": [71, 262.61798095703125, 290, 343.5094909667969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "3 Distant Supervision ", "text_level": 1, "page_idx": 3, "bbox": [71, 367, 191, 379], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "Training data for ﬁne-grained NER systems is typically obtained by linking entity mentions and drawing their types from knowledge bases (KBs). This approach has two limitations: recall can suf- fer due to KB incompleteness ( West et al. ,  2014 ), and precision can suffer when the selected types do not ﬁt the context ( Ritter et al. ,  2011 ). We al- leviate the recall problem by mining entity men- tions that were linked to Wikipedia in HTML, and extract relevant types from their encyclope- dic deﬁnitions (Section  3.1 ). To address the pre- cision issue (context-insensitive labeling), we pro- pose a new source of distant supervision: auto- matically extracted nominal head words from raw text (Section  3.2 ). Using head words as a form of distant supervision provides ﬁne-grained infor- mation about named entities and nominal men- tions. While a KB may link “the 44th president of the United States” to many types such as author, lawyer, and professor, head words provide only the type “president”, which is relevant in the context. ", "page_idx": 3, "bbox": [71, 396.92401123046875, 290, 681.0535278320312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "We experiment with the new distant supervi- sion sources as well as the traditional KB super- vision. Table  2  shows examples and statistics for each source of supervision. We annotate 100 ex- amples from each source to estimate the noise and usefulness in each signal (precision in Table 2). ", "page_idx": 3, "bbox": [71, 685.1400756835938, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "3.1 Entity Linking ", "text_level": 1, "page_idx": 3, "bbox": [307, 263, 402, 276], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "For KB supervision, we leveraged training data from prior work ( Ling and Weld ,  2012 ;  Gillick et al. ,  2014 ) by manually mapping their ontology to our 10,000 noun type vocabulary, which cov- ers 130 of our labels (general and ﬁne-grained). Section 6 deﬁnes this mapping in more detail. ", "page_idx": 3, "bbox": [307, 290.8570556640625, 525, 371.7485656738281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "To improve both entity and type coverage of KB supervision, we use deﬁnitions from Wikipedia. We follow Shnarch et al. () who observed that the ﬁrst sentence of a Wikipedia article often states the entity’s type via an “is a” relation; for exam- ple, “Roger Federer is a Swiss professional tennis player.” Since we are using a large type vocabu- lary, we can now mine this typing information. We extracted descriptions for 3.1M entities which contain 4,600 unique type labels such as “compe- tition,” “movement,” and “village.” ", "page_idx": 3, "bbox": [307, 377.57208251953125, 525, 526.2095947265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "We bypass the challenge of automatically link- ing entities to Wikipedia by exploiting existing hy- perlinks in web pages ( Singh et al. ,  2012 ), fol- lowing prior work ( Ling and Weld ,  2012 ;  Yosef et al. ,  2012 ). Since our heuristic extraction of types from the deﬁnition sentence is somewhat noisy, we use a more conservative entity linking policy 4   that yields a signal with similar overall ac- curacy to KB-linked data. ", "page_idx": 3, "bbox": [307, 532.0341186523438, 525, 653.5735473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "3.2 Contextualized Supervision ", "text_level": 1, "page_idx": 4, "bbox": [71, 64, 225, 76], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "Many nominal entity mentions include detailed type information within the mention itself. For example, when describing Titan  $\\mathrm{v}$   as “the newly- released graphics card”, the head words and phrases of this mention (“graphics card” and “card”) provide a somewhat noisy, but very easy to gather, context-sensitive type signal. ", "page_idx": 4, "bbox": [71, 81.16400909423828, 290, 175.60452270507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "We extract nominal head words with a depen- dency parser ( Manning et al. ,  2014 ) from the Gi- gaword corpus as well as the Wikilink dataset. To support multiword expressions, we included nouns that appear next to the head if they form a phrase in our type vocabulary. Finally, we lower- case all words and convert plural to singular. ", "page_idx": 4, "bbox": [71, 176.00807189941406, 290, 270.4485778808594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "Our analysis reveals that this signal has a com- parable accuracy to the types extracted from en- tity linking (around   $80\\%$  ). Many errors are from the parser, and some errors stem from idioms and transparent heads (e.g. “parts of capital” labeled as “part”). While the headword is given as an input to the model, with heavy regularization and multi- tasking with other supervision sources, this super- vision helps encode the context. ", "page_idx": 4, "bbox": [71, 270.85308837890625, 290, 392.3916320800781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "4 Model ", "text_level": 1, "page_idx": 4, "bbox": [71, 402, 124, 414], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "We design a model for predicting sets of types given a mention in context. The architec- ture resembles the recent neural AttentiveNER model ( Shimaoka et al. ,  2017 ), while improving the sentence and mention representations, and in- troducing a new multitask objective to handle mul- tiple sources of supervision. The hyperparameter settings are listed in the supplementary material. ", "page_idx": 4, "bbox": [71, 422.35015869140625, 290, 530.3396606445312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "Context Representation Given a sentence  $x_{1},\\ldots,x_{n}$  , we represent each token    $x_{i}$   using a pre-trained word embedding    $w_{i}$  . We concate- nate an additional location embedding    $l_{i}$   which indicates whether    $x_{i}$   is before, inside, or after the mention. We then use    $[x_{i};l_{i}]$   as an input to a bidirectional LSTM, producing a contextualized representation    $h_{i}$   for each token; this is different from the architecture of Shimaoka et al. 2017 , who used two separate bidirectional LSTMs on each side of the mention. Finally, we represent the context  $c$   as a weighted sum of the contextualized token representations using MLP-based attention: ", "page_idx": 4, "bbox": [71, 536.926513671875, 290, 713.0556640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "equation", "text": "\n$$\na_{i}=\\mathrm{SoftMax}_{i}(v_{a}\\cdot\\mathrm{relu}(W_{a}h_{i}))\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [108, 718, 253, 733], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "Where    $W_{a}$   and  $v_{a}$   are the parameters of the atten- tion mechanism’s MLP, which allows interaction between the forward and backward directions of the LSTM before computing the weight factors. ", "page_idx": 4, "bbox": [71, 739.336181640625, 290, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "", "page_idx": 4, "bbox": [307, 63.68720245361328, 525, 90.38168334960938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "Mention Representation We represent the mention    $m$   as the concatenation of two items: (a) a character-based representation produced by a CNN on the entire mention span, and (b) a weighted sum of the pre-trained word embeddings in the mention span computed by attention, similar to the mention representation in a recent coreference resolution model ( Lee et al. ,  2017 ). The ﬁnal representation is the concatenation of the context and mention representations:    $r=[c;m]$  . ", "page_idx": 4, "bbox": [307, 98.23448181152344, 525, 234], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "Label Prediction We learn a type label embed- ding matrix    $W_{t}\\in\\mathbb{R}^{n\\times d}$    where  $n$   i the number of labels in the prediction space and  d  is the dimen- sion of    $r$  . This matrix can be seen as a combination of three sub matrices,    $W_{g e n e r a l},W_{f i n e},W_{u l t r a},$  each of which contains the representations of the general, ﬁne, and ultra-ﬁne types respectively. We predict each type’s probability via the sigmoid of its inner product with  r :    $y=\\sigma(W_{t}r)$  . We predict every type    $t$   for which    $y_{t}\\,>\\,0.5$  , or  arg max    $y_{t}$   if there is no such type. ", "page_idx": 4, "bbox": [307, 241.5685272216797, 525, 390.5987854003906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "Multitask Objective The distant supervision sources provide partial supervision for ultra-ﬁne types; KBs often provide more general types, while head words usually provide only ultra-ﬁne types, without their generalizations. In other words, the absence of a type at a different level of abstraction does not imply a negative signal; e.g. when the head word is “inventor”, the model should not be discouraged to predict “person”. ", "page_idx": 4, "bbox": [307, 398.4515686035156, 525, 520.3828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "Prior work used a customized hinge loss ( Ab- hishek et al. ,  2017 ) or max margin loss ( Ren et al. , 2016a ) to improve robustness to noisy or incom- plete supervision. We propose a multitask objec- tive that reﬂects the characteristic of our training dataset. Instead of updating all labels for each ex- ample, we divide labels into three bins (general, ﬁne, and ultra-ﬁne), and update labels only in bin containing at least one positive label. Speciﬁcally, the training objective is to minimize    $J$   where    $t$   is the target vector at each granularity: ", "page_idx": 4, "bbox": [307, 520.9243774414062, 525, 669.5618286132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{J_{\\mathrm{all}}=J_{\\mathrm{general}}\\cdot\\mathbb{1}_{\\mathrm{general}}(t)}\\\\ &{~~~~+\\;J_{\\mathrm{fine}}\\cdot\\mathbb{1}_{\\mathrm{fine}}(t)}\\\\ &{~~~~+\\;J_{\\mathrm{intra}}\\cdot\\mathbb{1}_{\\mathrm{intra}}(t)}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [360, 679, 473, 729], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "Where    $\\mathbb{1}_{\\mathrm{cageogy}}(t)$   is an indicator function that checks if    $t$   contains a type in the category, and ", "page_idx": 4, "bbox": [307, 738.5072631835938, 525, 766.0308227539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "table", "page_idx": 5, "img_path": "layout_images/P18-1009_4.jpg", "table_caption": "Table 3: Performance of our model and AttentiveNER ( Shimaoka et al. ,  2017 ) on the new entity typing benchmark, using same training data. We show results for both development and test sets. ", "bbox": [70, 62, 527, 155], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Dev Test\n\nModel | MRR P R_ Fil | MRR PP R_ Fl\n\nAttentiveNER | 0.221 53.7) 15.0 23.5 | 0.223 54.2 15.2 23.7\nOur Model 0.229 48.1 23.2 31.3 | 0.234 47.1 24.2 32.0\n", "vlm_text": "The table compares the performance of two models, \"AttentiveNER\" and \"Our Model,\" across development (Dev) and test datasets. The performance metrics include Mean Reciprocal Rank (MRR), Precision (P), Recall (R), and F1-score (F1). For each dataset (Dev and Test), the table provides the MRR score followed by the precision, recall, and F1-score. \n\n- For the Dev dataset:\n  - AttentiveNER: MRR is 0.221, Precision is 53.7, Recall is 15.0, and F1-score is 23.5.\n  - Our Model: MRR is 0.229, Precision is 48.1, Recall is 23.2, and F1-score is 31.3.\n\n- For the Test dataset:\n  - AttentiveNER: MRR is 0.223, Precision is 54.2, Recall is 15.2, and F1-score is 23.7.\n  - Our Model: MRR is 0.234, Precision is 47.1, Recall is 24.2, and F1-score is 32.0.\n\n\"Our Model\" demonstrates slightly better MRR scores on both Dev and Test datasets compared to \"AttentiveNER,\" and significantly superior Recall and F1 scores, while \"AttentiveNER\" has higher Precision scores."}
{"layout": 60, "type": "table", "page_idx": 5, "img_path": "layout_images/P18-1009_5.jpg", "bbox": [86, 164, 512, 239], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Total General (1918) Fine (1289) Ultra-Fine (7594)\n\nTrain Data | vigR P RF P oR. Fl P RF P oR. Fi\nAll 0.229 48.1 232 313] 603 616 61.0] 404 384 394] 428 88 146\n— Crowd 0.173 40.1 148 216] 537 456 493] 208 185 196] 544 46 84\n— Head 0.220 50.3 19.6 282] 588 628 60.7] 444 298 356| 462 4.7 85\n\n-EL 0.225 48.4 22.3 30.6 | 62.2 60.1 61.2 | 403 261 31.7] 414 99 16.0\n", "vlm_text": "The table presents performance metrics across different datasets. It is structured to display various evaluation metrics for models trained on different portions of the dataset. Here's the breakdown:\n\n- **Columns:**\n  - The first column lists different training datasets or conditions: \"All\", \"– Crowd\", \"– Head\", and \"– EL\".\n  - The next set of columns (Total, General, Fine, Ultra-Fine) provides the evaluation metrics for each dataset category:\n    - **MRR (Mean Reciprocal Rank)** is shown in a standalone column.\n    - **Total, General, Fine, and Ultra-Fine** each include three sub-columns: Precision (P), Recall (R), and F1-score (F1). The numbers in parentheses next to General, Fine, and Ultra-Fine indicate the number of instances in each category.\n\n- **Rows:**\n  - **All**: Metrics when the model is trained on the entire dataset.\n  - **– Crowd**: Metrics when the crowd-sourced data is excluded from the training.\n  - **– Head**: Metrics when the head portion of the dataset is excluded.\n  - **– EL**: Metrics when entity linking data is excluded.\n\n**Notable Figures:**\n- The highest values in each column seem to be emphasized in bold.\n- The table provides detailed metrics on the impact of excluding parts of the data on the training performance, testing varied aspects of fine-grained and ultra-fine grained categorization tasks. \n\nThis type of table is commonly used in research papers to detail how different data compositions affect model performance."}
{"layout": 61, "type": "text", "text": "Table 4: Results on the development set for different type granularity and for different supervision data with our model. In each row, we remove a single source of supervision. Entity linking (EL) includes supervision from both KB and Wikipedia deﬁnitions. The numbers in the ﬁrst row are example counts for each type granularity. ", "page_idx": 5, "bbox": [72, 251.2899932861328, 525, 305.0834655761719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": " $J_{\\mathrm{theory}}$   is the category-speciﬁc logistic regression objective: ", "page_idx": 5, "bbox": [71, 325, 290, 351.8784484863281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "equation", "text": "\n$$\nJ=-\\sum_{i}t_{i}\\cdot\\log(y_{i})+(1-t_{i})\\cdot\\log(1-y_{i})\n$$\n ", "text_format": "latex", "page_idx": 5, "bbox": [78, 360, 283, 390], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "5 Evaluation ", "text_level": 1, "page_idx": 5, "bbox": [71, 400, 146, 414], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "Experiment Setup The crowdsourced dataset (Section  2.1 ) was randomly split into train, devel- opment, and test sets, each with about 2,000 ex- amples. We use this relatively small manually- annotated training set ( Crowd  in Table  4 ) along- side the two distant supervision sources: entity linking (KB and Wikipedia deﬁnitions) and head words. To combine supervision sources of differ- ent magnitudes (2K crowdsourced data, 4.7M en- tity linking data, and 20M head words), we sample a batch of equal size from each source at each it- eration. We reimplement the recent AttentiveNER model ( Shimaoka et al. ,  2017 ) for reference. ", "page_idx": 5, "bbox": [71, 422.37921142578125, 290, 598.5074462890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "We report macro-averaged precision, recall, and F1, and the average mean reciprocal rank (MRR). ", "page_idx": 5, "bbox": [71, 599.2260131835938, 290, 625.9204711914062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "Results Table  3  shows the performance of our model and our re implementation of Atten- tiveNER. Our model, which uses a multitask ob- jective to learn ﬁner types without punishing more general types, shows recall gains at the cost of drop in precision. The MRR score shows that our model is slightly better than the baseline at ranking correct types above incorrect ones. ", "page_idx": 5, "bbox": [71, 634.3862915039062, 290, 715.67041015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "", "page_idx": 5, "bbox": [307, 325.1829833984375, 525, 351.8784484863281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "Table  4  shows the performance breakdown for different type granularity and different supervi- sion. Overall, as seen in previous work on ﬁne- grained NER literature ( Gillick et al. ,  2014 ;  Ren et al. ,  2016a ), ﬁner labels were more challenging to predict than coarse grained labels, and this is- sue is exacerbated when dealing with ultra-ﬁne types. All sources of supervision appear to be useful, with crowdsourced examples making the biggest impact. Head word supervision is par- ticularly helpful for predicting ultra-ﬁne labels, while entity linking improves ﬁne label prediction. The low general type performance is partially be- cause of nominal/pronoun mentions (e.g. “it”), and because of the large type inventory (some- times “location” and “place” are annotated inter- changeably). ", "page_idx": 5, "bbox": [307, 352.3519592285156, 525, 582.2845458984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "Analysis We manually analyzed 50 examples from the development set, four of which we present in Table  5 . Overall, the model was able to generate accurate general types and a diverse set of type labels. Despite our efforts to annotate a com- prehensive type set, the gold labels still miss many potentially correct labels (example (a): “man” is reasonable but counted as incorrect). This makes the precision estimates lower than the actual per- formance level, with about half the precision er- rors belonging to this category. Real precision errors include predicting co-hyponyms (example (b): “accident” instead of “attack”), and types that ", "page_idx": 5, "bbox": [307, 589.9024047851562, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "table", "page_idx": 6, "img_path": "layout_images/P18-1009_6.jpg", "table_footnote": "Table 5: Example and predictions from our best model on the development set. Entity mentions are marked with curly brackets, the correct predictions are boldfaced, and the missing labels are italicized and written in red. ", "bbox": [70, 62, 530, 301], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "(a)\n\n(b)\n\n(c)\n\n(d)\n\nExample\nAnnotation\n\nPrediction\nExample\nAnnotation\nPrediction\nExample\nAnnotation\nPrediction\n\nContext\n\nAnnotation\n\nPrediction\n\nBruguera said {he} had problems with his left leg and had grown tired early during the match .\nperson, athlete, player, adult, male, contestant\n\nperson, athlete, player, adult, male, contestant, defendant, man\n\n{The explosions} occurred on the night of October 7 , against the Hilton Taba and campsites used by\nIsraelis in Ras al-Shitan.\nevent calamity, attack, disaster\n\nevent, accident\n\nSimilarly , Enterprise was considered for refit to replace Challenger after {the latter} was destroyed ,\nbut Endeavour was built from structural spares instead .\nobject, spacecraft, rocket, thing, vehicle, shuttle\n\nevent\n\n“ There is a wealth of good news in this report , and I’m particularly encouraged by the progress {we}\nare making against AIDS , ” HHS Secretary Donna Shalala said in a statement.\ngovernment, group, organization, hospital,administration,socialist\n\ngovernment, group, person\n", "vlm_text": "The table presents a comparison between human-generated annotations and predicted annotations for certain examples or contexts, as indicated in the left column. Each row represents a distinct example (labeled (a) to (d)) with associated annotations and predictions.\n\nIn more detail:\n- Column for Example/Context: This column contains a sentence or fragment with a highlighted word or phrase surrounded by curly braces (e.g., {he}, {The explosions}, {the latter}, {we}).\n- Annotation: This row lists the human-generated categories or labels for the highlighted words (e.g., \"person, athlete, player, adult, male, contestant\" for example (a)).\n- Prediction: This row lists the labels generated by a model for the highlighted words (e.g., \"person, athlete, player, adult, male, contestant, defendant, man\" for example (a)).\n\nThe table allows for the comparison of human annotations with model predictions, with correctly predicted terms typically colored in blue and incorrectly predicted or unmatched terms shown in red. Discrepancies and overlaps between annotations and predictions can be observed across different examples."}
{"layout": 72, "type": "text", "text": "may be true, but are not supported by the context. ", "page_idx": 6, "bbox": [72, 314.32196044921875, 290, 327.4674377441406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "We found that the model often abstained from predicting any ﬁne-grained types. Especially in challenging cases as in example (c), the model predicts only general types, explaining the low re- call numbers (  $28\\%$   of examples belong to this cat- egory). Even when the model generated correct ﬁne-grained types as in example (d), the recall was often fairly low since it did not generate a com- plete set of related ﬁne-grained labels. ", "page_idx": 6, "bbox": [72, 329.77996826171875, 290, 451.3185119628906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "Estimating the performance of a model in an in- complete label setting and expanding label cover- age are interesting areas for future work. Our task also poses a potential modeling challenge; some- times, the model predicts two incongruous types (e.g. “location” and “person”), which points to- wards modeling the task as a joint set prediction task, rather than predicting labels individually. We provide sample outputs on the project website. ", "page_idx": 6, "bbox": [72, 453.63104248046875, 290, 575.1705322265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "6 Improving Existing Fine-Grained NER with Better Distant Supervision ", "text_level": 1, "page_idx": 6, "bbox": [71, 592, 290, 619], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "We show that our model and distant supervision can improve performance on an existing ﬁne- grained NER task. We chose the widely-used OntoNotes ( Gillick et al. ,  2014 ) dataset which in- cludes nominal and named entity mentions. ", "page_idx": 6, "bbox": [72, 631.9910888671875, 290, 699.3335571289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "Augmenting the Training Data The original OntoNotes training set (O NTO  in Tables  6  and  7 ) is extracted by linking entities to a KB. We supple- ment this dataset with our two new sources of dis- tant supervision: Wikipedia deﬁnition sentences (W IKI ) and head word supervision (H EAD ) (see Section  3 ). To convert the label space, we manu- ally map a single noun from our natural-language vocabulary to each formal-language type in the OntoNotes ontology.  $77\\%$   of OntoNote’s types directly correspond to suitable noun labels (e.g. “doctor” to “/person/doctor”), whereas the other cases were mapped with minimal manual effort (e.g. “musician” to “person/artist/music”, “politi- cian” to “/person/political ﬁgure”). We then ex- pand these labels according to the ontology to in- clude their hypernyms (“/person/political ﬁgure” will also generate “/person”). Lastly, we create negative examples by assigning the “/other” label to examples that are not mapped to the ontology. The augmented dataset contains   $2.5\\mathbf{M}/0.6\\mathbf{M}$   new positive/negative examples, of which   $0.9\\mathbf{M}/0.1\\mathbf{M}$  are from Wikipedia deﬁnition sentences and  $1.6\\mathbf{M}/0.5\\mathbf{M}$   from head words. ", "page_idx": 6, "bbox": [306, 313.9292297363281, 527, 639.0994873046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "Experiment Setup We compare performance to other published results and to our reimplemen- tation of AttentiveNER ( Shimaoka et al. ,  2017 ). We also compare models trained with different sources of supervision. For this dataset, we did not use our multitask objective (Section  4 ), since ex- panding types to include their ontological hyper- nyms largely eliminates the partial supervision as- ", "page_idx": 6, "bbox": [306, 657.6483154296875, 527, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "table", "page_idx": 7, "img_path": "layout_images/P18-1009_7.jpg", "bbox": [71, 61, 291, 130], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Acc. Ma-F1 Mi-F1\nAttentiveNER++ 51.7 70.9 64.9\nAFET (Ren et al., 2016a) 55.1 711 64.7\nLNR (Ren et al., 2016b) 57.2 715 66.1\nOurs (ONTO+WIKI+HEAD) 59.5 76.8 71.8\n", "vlm_text": "The table presents comparative performance metrics for different Named Entity Recognition (NER) models. It includes the following columns:\n\n1. **Model**: Lists the models being compared. The table includes the following models:\n   - AttentiveNER++\n   - AFET (Ren et al., 2016a)\n   - LNR (Ren et al., 2016b)\n   - Ours (ONTO+WIKI+HEAD)\n\n2. **Acc.**: Refers to accuracy, which measures the proportion of correct predictions made by the model. The values for each model are as follows:\n   - AttentiveNER++: 51.7\n   - AFET: 55.1\n   - LNR: 57.2\n   - Ours: 59.5\n\n3. **Ma-F1**: Refers to the macro F1 score, which is the harmonic mean of precision and recall, calculated across multiple classes and averaged without taking class imbalance into account. The values are:\n   - AttentiveNER++: 70.9\n   - AFET: 71.1\n   - LNR: 71.5\n   - Ours: 76.8\n\n4. **Mi-F1**: Refers to the micro F1 score, which is calculated by considering the total true positives, false negatives, and false positives of all classes. This metric takes class imbalance into account. The values are:\n   - AttentiveNER++: 64.9\n   - AFET: 64.7\n   - LNR: 66.1\n   - Ours: 71.8\n\n\"Ours (ONTO+WIKI+HEAD)\" appears to be the proposed model or method in the context, which achieves the highest values in all three metrics."}
{"layout": 80, "type": "text", "text": "Table 6: Results on the OntoNotes ﬁne-grained entity typing test set. The ﬁrst two models (At- tentive  $\\tt N E R++$   and AFET) use only KB-based su- pervision. LNR uses a ﬁltered version of the KB- based training set. Our model uses all our distant supervision sources. ", "page_idx": 7, "bbox": [72, 144.72804260253906, 290, 225.61953735351562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "table", "page_idx": 7, "img_path": "layout_images/P18-1009_8.jpg", "bbox": [71, 236, 291, 344], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Training Data Performance\nModel oNTO wiki HEAD | Acc. MaFl  MiFl\nAttn. v 46.5 63.3 58.3\nNER v v v | 53.7 72.8 68.0\nv 41.7 64.2 595\nv v 48.5 67.6 63.6\nOurs v v | 57.9 3.0 66.9\nv v | 60.1 75.0 68.7\nv v v | 616 77.3 718\n", "vlm_text": "The table presents a comparison of performance metrics for different models and training data combinations on some task. Specifically, it compares the performance of an \"Attn. NER\" model to a model labeled \"Ours.\"\n\n1. **Models**:\n   - \"Attn. NER\" \n   - \"Ours\"\n\n2. **Training Data Types**:\n   - ONTO\n   - WIKI\n   - HEAD\n\n3. **Performance Metrics**:\n   - Accuracy (Acc.)\n   - Macro-averaged F1-score (MaF1)\n   - Micro-averaged F1-score (MiF1)\n\n4. **Attn. NER Performance**:\n   - Training on ONTO: Acc. 46.5, MaF1 63.3, MiF1 58.3\n   - Training on ONTO, WIKI, HEAD: Acc. 53.7, MaF1 72.8, MiF1 68.0\n\n5. **Our Model's Performance**:\n   - Training on ONTO: Acc. 41.7, MaF1 64.2, MiF1 59.5\n   - Training on WIKI: Acc. 48.5, MaF1 67.6, MiF1 63.6\n   - Training on HEAD: Acc. 57.9, MaF1 73.0, MiF1 66.9\n   - Training on ONTO, WIKI: Acc. 60.1, MaF1 75.0, MiF1 68.7\n   - Training on ONTO, WIKI, HEAD: Acc. 61.6, MaF1 77.3, MiF1 71.8\n\nOverall, the table indicates that for both models, training with a combination of ONTO, WIKI, and HEAD data yields the best performance across all three metrics, with \"Ours\" achieving the highest scores."}
{"layout": 82, "type": "text", "text": "Table 7: Ablation study on the OntoNotes ﬁne- grained entity typing development. The second row isolates dataset improvements, while the third row isolates the model. ", "page_idx": 7, "bbox": [72, 354.0069885253906, 290, 407.80047607421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "sumption. Following prior work, we report macro- and micro-averaged F1 score, as well as accuracy (exact set match). ", "page_idx": 7, "bbox": [72, 426.0400085449219, 290, 466.28448486328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "Results Table  6  shows the overall performance on the test set. Our combination of model and training data shows a clear improvement from prior work, setting a new state-of-the art result. ", "page_idx": 7, "bbox": [72, 474.26226806640625, 290, 528.448486328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "In Table  7 , we show an ablation study. Our new supervision sources improve the performance of both the AttentiveNER model and our own. We observe that every supervision source improves performance in its own right. Particularly, the naturally-occurring head-word supervision seems to be the prime source of improvement, increasing performance by about   $10\\%$   across all metrics. ", "page_idx": 7, "bbox": [72, 529.0260009765625, 290, 637.0164794921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "Predicting Miscellaneous Types While analyz- ing the data, we observed that over half of the men- tions in OntoNotes’ development set were anno- tated only with the miscellaneous type (“/other”). For both models in our evaluation, detecting the miscellaneous category is substantially easier than producing real types (  $94\\%$   F1 vs.   $58\\%$   F1 with our best model). We provide further details of this analysis in the supplementary material. ", "page_idx": 7, "bbox": [72, 644.9943237304688, 290, 726.2784423828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "", "page_idx": 7, "bbox": [307, 63.68701934814453, 525, 103.93148803710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 7, "bbox": [307, 116, 397, 131], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "Fine-grained NER has received growing atten- tion, and is used in many applications ( Gupta et al. ,  2017 ;  Ren et al. ,  2017 ;  Yaghoobzadeh et al. , 2017b ;  Raiman and Raiman ,  2018 ). Researchers studied typing in varied contexts, including men- tions in speciﬁc sentences (as we consider) ( Ling and Weld ,  2012 ;  Gillick et al. ,  2014 ;  Yogatama et al. ,  2015 ;  Dong et al. ,  2015 ;  Schutze et al. , 2017 ), corpus-level prediction ( Yaghoobzadeh and Sch¨ utze ,  2016 ), and lexicon level (given only a noun phrase with no context) ( Yao et al. ,  2013 ). ", "page_idx": 7, "bbox": [307, 140.5860137939453, 525, 289.2235412597656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "Recent work introduced ﬁne-grained type on- tologies ( Rabinovich and Klein ,  2017 ;  Murty et al. ,  2017 ;  Corro et al. ,  2015 ), deﬁned using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challeng- ing to gather, often approximating gold annota- tions with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pro- nouns), and (3) we provide crowdsourced annota- tions which provide context-sensitive, ﬁne grained type labels. ", "page_idx": 7, "bbox": [307, 290.48907470703125, 525, 479.7746276855469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "Contextualized ﬁne-grained entity typing is re- lated to selectional preference ( Resnik ,  1996 ;  Pan- tel et al. ,  2007 ;  Zapirain et al. ,  2013 ;  de Cruys , 2014 ), where the goal is to induce semantic gen- eralizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the ar- guments’ types, which allows us to capture more nuanced types. For example, not every type that ﬁts “ He  played the violin in his room” is also suitable for “ He  played the violin in the Carnegie Hall”. Entity typing here can be connected to ar- gument ﬁnding in semantic role labeling. ", "page_idx": 7, "bbox": [307, 481.0401611328125, 525, 656.775634765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "To deal with noisy distant supervision for KB population and entity typing, researchers used multi-instance multi-label learning ( Sur- deanu et al. ,  2012 ;  Yaghoobzadeh et al. ,  2017b ) or custom losses ( Abhishek et al. ,  2017 ;  Ren et al. , 2016a ). Our multitask objective handles noisy su- pervision by pooling different distant supervision sources across different levels of granularity. ", "page_idx": 7, "bbox": [307, 658.0411987304688, 525, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 8, "bbox": [71, 64, 147, 75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "Using virtually unrestricted types allows us to ex- pand the standard KB-based training methodol- ogy with typing information from Wikipedia deﬁ- nitions and naturally-occurring head-word super- vision. These new forms of distant supervision boost performance on our new dataset as well as on an existing ﬁne-grained entity typing bench- mark. These results set the ﬁrst performance lev- els for our evaluation dataset, and suggest that the data will support signiﬁcant future work. ", "page_idx": 8, "bbox": [72, 87.06702423095703, 290, 222.15554809570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 8, "bbox": [72, 236, 166, 248], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "The research was supported in part the ARO (W911NF-16-1-0121) the NSF (IIS-1252835, IIS- 1562364), and an Allen Distinguished Investigator Award. We would like to thank the reviewers for constructive feedback. Also thanks to Yotam Es- hel and Noam Cohen for providing the Wikilink dataset. Special thanks to the members of UW NLP for helpful discussions and feedback. ", "page_idx": 8, "bbox": [72, 259.60406494140625, 290, 367.5945739746094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [71, 394, 128, 406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Abhishek, Ashish Anand, and Amit Awekar. 2017. Fine-grained entity type classiﬁcation by jointly learning representations and label embeddings. In Proceedings of European Chapter of Association for Computational Linguistics . Krisztian Balog and Robert Neumayer. 2012. Hier- archical target type identiﬁcation for entity-oriented queries. In  Proceedings of the Conference on Infor- mation and Knowledge Management . Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. 2015. Finet: Context-aware ﬁne-grained named entity typing. In Proceedings of the conference on Empirical Meth- ods in Natural Language Processing . Tim Van de Cruys. 2014. A neural network approach to selectional preference acquisition. In  Proceedings of Empirical Methods in Natural Language Process- ing . Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. 2015. A hybrid neural model for type classiﬁcation of entity mentions. In  Proceedings of International Joint Conference on Artiﬁcial Intelligence . Greg Durrett and Dan Klein. 2014. A joint model for entity analysis: Coreference, typing, and linking. In Transactions of the Association for Computational Linguistics . ", "page_idx": 8, "bbox": [72, 415.044677734375, 290, 765.7655639648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "Daniel Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Context- dependent ﬁne-grained entity type tagging.  CoRR , abs/1412.1820. ", "page_idx": 8, "bbox": [307, 64.5616455078125, 525, 109.44353485107422], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En- tity linking via joint encoding of types, descriptions, and context. In  Proceedings of the Conference on Empirical Methods in Natural Language Process- ing , pages 2671–2680. ", "page_idx": 8, "bbox": [307, 117.4285888671875, 525, 173.2694549560547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the  $90\\%$   solution. In  Proceedings of the human lan- guage technology conference of the North American Chapter of the Association for Computational Lin- guistics, Companion Volume: Short Papers , pages 57–60. Association for Computational Linguistics. ", "page_idx": 8, "bbox": [307, 181.2545166015625, 525, 259.01239013671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference resolu- tion. In  Proceedings of the Conference on Empirical Methods in Natural Language Processing . ", "page_idx": 8, "bbox": [307, 266.99847412109375, 525, 311.87939453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "Xiao Ling and Daniel S Weld. 2012. Fine-grained en- tity recognition. In  Proceedings of Association for the Advancement of Artiﬁcial Intelligence . Citeseer. ", "page_idx": 8, "bbox": [307, 319.865478515625, 525, 353.78839111328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014.  The Stanford CoreNLP natural lan- guage processing toolkit . In  Association for Compu- tational Linguistics (ACL) System Demonstrations , pages 55–60. ", "page_idx": 8, "bbox": [307, 361.7734375, 525, 428.57232666015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "George A Miller. 1995. Wordnet: a lexical database for english.  Communications of the ACM , 38(11):39– 41. ", "page_idx": 8, "bbox": [307, 436.5583801269531, 525, 470.4803466796875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "Shikhar Murty, Patrick Verga, Luke Vilnis, and Andrew McCallum. 2017. Finer grained entity typing with typenet. In  AKBC Workshop . ", "page_idx": 8, "bbox": [307, 478.4664001464844, 525, 512.3893432617188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard H. Hovy. 2007. Isp: Learning inferential selectional preferences. In  Pro- ceedings of North American Chapter of the Associ- ation for Computational Linguistics . ", "page_idx": 8, "bbox": [307, 520.3743896484375, 525, 576.2142944335938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "Robert Parker, David Graff, David Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword ﬁfth edi- tion (ldc2011t07). In  Linguistic Data Consortium . ", "page_idx": 8, "bbox": [307, 584.2003173828125, 525, 618.123291015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "Maxim Rabinovich and Dan Klein. 2017. Fine-grained entity typing with high-multiplicity assignments. In Proceedings of Association for Computational Lin- guistics (ACL) . ", "page_idx": 8, "bbox": [307, 626.1083374023438, 525, 670.9902954101562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji, and Jiawei Han. 2016a. Afet: Automatic ﬁne- grained entity typing by hierarchical partial-label ", "page_idx": 8, "bbox": [307, 731.8423461914062, 525, 765.7653198242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "embedding. In  Proceedings Empirical Methods in Natural Language Processing . ", "page_idx": 9, "bbox": [82, 64.56158447265625, 290, 87.52550506591797], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, and Jiawei Han. 2016b. Label noise reduction in entity typing by heterogeneous partial-label embed- ding. In  Proceedings of Knowledge Discovery and Data Mining . Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek F. Abdelzaher, and Jiawei Han. 2017. Cotype: Joint extraction of typed entities and relations with knowledge bases. In  Proceedings of World Wide Web Conference . Philip Resnik. 1996. Selectional constraints: an information-theoretic model and its computational realization.  Cognition , 61 1-2:127–59. Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In  Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing , pages 1524–1534. Association for Computational Linguis- tics. Hinrich Schutze, Ulli Waltinger, and Sanjeev Karn. 2017. End-to-end trainable attentive decoder for hi- erarchical entity classiﬁcation. In  Proceedings of European Chapter of Association for Computational Linguistics . Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2017. An attentive neural archi- tecture for ﬁne-grained entity type classiﬁcation. In Proceedings of the European Chapter of Association for Computational Linguistics (ACL) . Eyal Shnarch, Libby Barak, and Ido Dagan. Extract- ing lexical reference rules from wikipedia. In  Pro- ceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP . Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. 2012. Wik- ilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia. Techni- cal Report UM-CS-2012-015, University of Mas- sachusetts, Amherst. Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap- ati, and Christopher D. Manning. 2012. Multi- instance multi-label learning for relation extraction. In  EMNLP-CoNLL . Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge base completion via search-based ques- tion answering . In  Proceedings of World Wide Web Conference . Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨ utze. 2017a. Noise mitigation for neural entity typing and relation extraction.  In Proceedings of the ", "page_idx": 9, "bbox": [71, 95.3355712890625, 290, 765.7650756835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Conference of the European Chapter of the Associa- tion for Computational Linguistics , abs/1612.07495. ", "page_idx": 9, "bbox": [318, 64.5611572265625, 525, 87.52507781982422], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨ utze. 2017b. Noise mitigation for neural entity typing and relation extraction. In  Proceedings of European Chapter of Association for Computational Linguistics . Yadollah Yaghoobzadeh and Hinrich Sch¨ utze. 2016. Corpus-level ﬁne-grained entity typing using con- textual information.  Proceedings of the Conference on Empirical Methods in Natural Language Pro- cessing . Limin Yao, Sebastian Riedel, and Andrew McCallum. 2013. Universal schema for entity type prediction. In  Automatic KnowledgeBase Construction Work- shop at the Conference on Information and Knowl- edge Management . Semih Yavuz, Izzeddin Gur, Yu Su, Mudhakar Srivatsa, and Xifeng Yan. 2016. Improving semantic parsing via answer type inference. In  Proceedings of Empir- ical Methods in Natural Language Processing . Dani Yogatama, Daniel Gillick, and Nevena Lazic. 2015. Embedding methods for ﬁne grained entity type classiﬁcation. In  Proceedings of Association for Computational Linguistics (ACL) . M Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. Hyena: Hier- archical type classiﬁcation for entity names. In  Pro- ceedings of the International Conference on Compu- tational Linguistics . Be˜ nat Zapirain, Eneko Agirre, Llu´ ıs M\\` arquez i Villo- dre, and Mihai Surdeanu. 2013. Selectional pref- erences for semantic role classiﬁcation.  Computa- tional Linguistics , 39:631–663. ", "page_idx": 9, "bbox": [307, 95.44512939453125, 525, 500.974609375], "page_size": [595.2760009765625, 841.8900146484375]}
