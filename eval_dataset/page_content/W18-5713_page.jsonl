{"page": 0, "image_path": "doc_images/W18-5713_0.jpg", "ocr_text": "Retrieve and Refine:\nImproved Sequence Generation Models For Dialogue\n\nJason Weston, Emily Dinan and Alexander H. Miller\nFacebook AI Research\n\njase@fb.com,\n\nAbstract\n\nSequence generation models for dialogue are\nknown to have several problems: they tend to\nproduce short, generic sentences that are un-\ninformative and unengaging. Retrieval models\non the other hand can surface interesting re-\nsponses, but are restricted to the given retrieval\nset leading to erroneous replies that cannot be\ntuned to the specific context. In this work we\ndevelop a model that combines the two ap-\nproaches to avoid both their deficiencies: first\nretrieve a response and then refine it — the final\nsequence generator treating the retrieval as ad-\nditional context. We show on the recent CON-\nvAI2 challenge task our approach produces\nresponses superior to both standard retrieval\nand generation models in human evaluations.\n\n1 Introduction\n\nSequence generation models like Seq2Seq\n(Sutskever et al., 2014) are increasingly popular\nfor tasks such as machine translation (MT) and\nsummarization, where generation is suitably\nconstrained by the source sentence. However,\nobtaining good performance on dialogue tasks,\nwhere the context still allows many interpreta-\ntions, remains an open problem despite much\nrecent work (Serban et al., 2016). Several au-\nthors report the issue that they produce short,\ngeneric sentences containing frequent words —\nthe so-called “I don’t know” problem — as that\nresponse can work as a reply in many instances,\nbut is uninformative and unengaging. Retrieval\nmodels (Ji et al., 2014) do not have this problem,\nbut instead either produce engaging responses or\nelse completely erroneous ones which they cannot\n\nProceedings of the 2018 EMNLP Workshop SCAI: The\n\n2nd International Workshop on Search-Oriented Conversa-\ntional AI 978-1-948087-75-9\n\nedinan@fb.com,\n\nahm@fb.com\n\ntune to the specific context, as they can only\nproduce a valid reply if it is in the retrieval set.\n\nIn this work we propose a Retrieve and Refine\nmodel to gain the advantages of both methods,\nand avoid both their disadvantages. Models that\nproduce an initial prediction and then refine it are\ngrowing in traction in NLP. They have been used\nin MT and summarization either for refinement of\ninitial predictions (Junczys-Dowmunt and Grund-\nkiewicz, 2017; Niehues et al., 2016; Novak et al.,\n2016; Xia et al., 2017; Grangier and Auli, 2017)\nor combining with retrieval (Gu et al., 2017; Cao\net al., 2018), as well as for sentence correction\nor refinement without context (Guu et al., 2017;\nSchmaltz et al., 2017). There is little work in ap-\nplying these methods to dialogue; one work we are\naware of has been done concurrently with ours is\nPandey et al. (2018). The usefulness of our ap-\nproach is shown with detailed experiments on the\nConvAI2 dataset! which is a chit-chat task to get\nto know the other speaker’s profile, obtaining gen-\nerations superior to both retrieval and sequence\ngeneration models in human evaluations.\n\n2 Retrieve and Refine\n\nThe model we propose in this work is remark-\nably straight-forward: we take a standard gener-\native model and concatenate the output of a re-\ntrieval model to its usual input, and then generate\nas usual, training the model under this setting.\nFor the generator, we use a standard Seq2Seq\nmodel: a 2-layer LSTM with attention. For the\nretriever, we use the Key-Value Memory Network\n(Miller et al., 2016) already shown to perform well\nfor this dataset (Zhang et al., 2018), which attends\nover the dialogue history, to learn input and candi-\ndate retrieval embeddings that match using cosine\nsimilarity. The top scoring utterance is provided\n\n‘http://convai.io/\n\nProceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational Al, pages 87-92\nBrussels, Belgium, October 31, 2018. ©2018 Association for Computational Linguistics\nISBN 978-1-948087-75-9\n", "vlm_text": "Retrieve and Reﬁne: Improved Sequence Generation Models For Dialogue \nJason Weston, Emily Dinan and Alexander H. Miller Facebook AI Research \njase@fb.com, edinan@fb.com, ahm@fb.com \nAbstract \nSequence generation models for dialogue are known to have several problems: they tend to produce short, generic sentences that are un- informative and unengaging. Retrieval models on the other hand can surface interesting re- sponses, but are restricted to the given retrieval set leading to erroneous replies that cannot be tuned to the speciﬁc context. In this work we develop a model that combines the two ap- proaches to avoid both their deﬁciencies: ﬁrst retrieve a response and then reﬁne it – the ﬁnal sequence generator treating the retrieval as ad- ditional context. We show on the recent C ON - V AI2 challenge task our approach produces responses superior to both standard retrieval and generation models in human evaluations. \n1 Introduction \nSequence generation models like Seq2Seq ( Sutskever et al. ,  2014 ) are increasingly popular for tasks such as machine translation (MT) and summarization, where generation is suitably constrained by the source sentence. However, obtaining good performance on dialogue tasks, where the context still allows many interpreta- tions, remains an open problem despite much recent work ( Serban et al. ,  2016 ). Several au- thors report the issue that they produce short, generic sentences containing frequent words – the so-called “I don’t know” problem – as that response can work as a reply in many instances, but is uninformative and unengaging. Retrieval models ( Ji et al. ,  2014 ) do not have this problem, but instead either produce engaging responses or else completely erroneous ones which they cannot tune to the speciﬁc context, as they can only produce a valid reply if it is in the retrieval set. \n\nIn this work we propose a Retrieve and Reﬁne model to gain the advantages of both methods, and avoid both their disadvantages. Models that produce an initial prediction and then reﬁne it are growing in traction in NLP. They have been used in MT and summarization either for reﬁnement of initial predictions ( Junczys-Dowmunt and Grund- kiewicz ,  2017 ;  Niehues et al. ,  2016 ;  Novak et al. , 2016 ;  Xia et al. ,  2017 ;  Grangier and Auli ,  2017 ) or combining with retrieval ( Gu et al. ,  2017 ;  Cao et al. ,  2018 ), as well as for sentence correction or reﬁnement without context ( Guu et al. ,  2017 ; Schmaltz et al. ,  2017 ). There is little work in ap- plying these methods to dialogue; one work we are aware of has been done concurrently with ours is Pandey et al.  ( 2018 ). The usefulness of our ap- proach is shown with detailed experiments on the ConvAI2 dataset 1   which is a chit-chat task to get to know the other speaker’s proﬁle, obtaining gen- erations superior to both retrieval and sequence generation models in human evaluations. \n2 Retrieve and Reﬁne \nThe model we propose in this work is remark- ably straight-forward: we take a standard gener- ative model and concatenate the output of a re- trieval model to its usual input, and then generate as usual, training the model under this setting. \nFor the generator, we use a standard Seq2Seq model: a 2-layer LSTM with attention. For the retriever, we use the Key-Value Memory Network ( Miller et al. ,  2016 ) already shown to perform well for this dataset ( Zhang et al. ,  2018 ), which attends over the dialogue history, to learn input and candi- date retrieval embeddings that match using cosine similarity. The top scoring utterance is provided as input to our Seq2Seq model in order to reﬁne it, prepended with a special separator token. For both models we use the code available in ParlAI 2 . At test time the retriever retrieves candidates from the training set. "}
{"page": 1, "image_path": "doc_images/W18-5713_1.jpg", "ocr_text": "as input to our Seq2Seq model in order to refine\nit, prepended with a special separator token. For\nboth models we use the code available in ParlAP’.\nAt test time the retriever retrieves candidates from\nthe training set.\n\nTo train our model we first precompute the re-\ntrieval result for every dialogue turn in the train-\ning set, but instead of using the top ranking results\nwe rerank the top 100 predictions of each by their\nsimilarity to the label (in embedding space). Fol-\nlowing Guu et al. (2017) this should help avoid the\nproblem of the refinement being too far away from\nthe original retrieval. We then append the cho-\nsen utterances to the input sequences used to train\nSeq2Seq. We refer to our model as RetrieveN-\nRefine, or RetNRef for short. We also consider\ntwo variants of the model in the following that we\nfound improve the results.\n\nUse Retriever More In our vanilla model, we\nnoticed there was not enough attention being paid\nto the retrieval utterance by the generator. As the\ninput to Seq2Seq is the dialogue history concate-\nnated with the retrieval utterance, truncating the\nhistory is one way to pay more attention to the re-\ntrieval. In particular for the ConvAI2 dataset we\nclip the initial profile sentences at the start of the\ndialogue, forcing the model to more strongly rely\nhe retriever which still has them.* We refer\n\non )\nthis modification as RetrieveNRefine*.\nFix Retrieval Copy Errors Our model learns to\n\nsometimes ignore the retrieval (when it is bad),\nsometimes use it partially, and other times sim\nply copy it. However, when it is mostly copied\nbut only changes a word or two, we observed it\nmade mistakes more often than not, leading to\nless meaningful utterances. We thus also con-\nsider a variant that exactly copies the retrieval\nif the model generates with large word over-\nlap (we chose >60%). Otherwise, we leave\nthe generation untouched.* We refer to this as\nRetrieveNRefine** .\n\n*http://parl.ai\n\n3Architectural changes might also deal with this issue,\ne.g. treating the two inputs as independent sources to do at-\ntention over, but we take the simplest possible approach here.\n\n4Other approaches might also help with this problem such\nas using an explicit copy mechanism or to use BPE tokeniza-\ntion (Fan et al., 2017), but we leave those for future work.\n\n88\n\nRetNRef Retrieval Method | PPL\nNone (Vanilla Seq2Seq) 31.4\nRandom label 32.0\nMemory Network 31.8\nTrue label’s neighbor 25.9\nTrue label 9.2\n\nTable 1: Perplexity on the ConvAI2 task test set with\ndifferent types of retriever for RetNRef, see text.\n\n3 Experiments\n\nWe conduct experiments on the recent ConvAI2\nchallenge dataset which uses a modified version\nof the PersonaChat dataset (Zhang et al., 2018)\n(larger, and with different processing). The dataset\nconsists of conversations between crowdworkers\nwho were randomly paired and asked to act the\npart of a given persona (randomly assigned from\n1155 possible personas, created by another set of\nworkers), chat naturally, and get to know each\nother during the conversation. There are around\n160,000 utterances in around 11,000 dialogues,\nwith 2000 dialogues for validation and test, which\nuse non-overlapping personas.\n\n3.1 Automatic Evaluation and Analysis\n\nPerplexity Dialogue is known to be notoriously\nhard to evaluate with automated metrics (Liu et al.,\n2016). In contrast to machine translation, there is\nmuch less constraint on the output with many valid\nanswers with little word overlap, e.g. there are\nmany answers to “what are you doing tonight?”.\nNevertheless many recent papers report perplexity\nresults in addition to human judgments. For the re-\ntrieve and refine case, perplexity evaluation is par-\nticularly flawed: if the retrieval points the model to\na response that is very different from (but equally\nvalid as) the true response, the model might focus\non refining that and get poor perplexity.\n\nWe therefore test our model by considering\nvarious types of retrieval methods: (i) the best\nperforming existing retriever model, the Memory\nNetwork approach from Zhang et al. (2018) (re-\ntrieving from the training set), (ii) a retriever that\nreturns a random utterance from the training set,\n(iii) the true label given in the test set, and (iv) the\nclosest nearest neighbor from the training set ut-\nerances to the true label, as measured by the em-\nbedding space of the Memory Network retriever\nmodel. While (iii) and (iv) cannot be used in a de-\nployed system as they are unknown, they can be\n\n", "vlm_text": "\nTo train our model we ﬁrst precompute the re- trieval result for every dialogue turn in the train- ing set, but instead of using the top ranking results we rerank the top 100 predictions of each by their similarity to the label (in embedding space). Fol- lowing  Guu et al.  ( 2017 ) this should help avoid the problem of the reﬁnement being too far away from the original retrieval. We then append the cho- sen utterances to the input sequences used to train Seq2Seq. We refer to our model as  RetrieveN- Reﬁne , or  RetNRef  for short. We also consider two variants of the model in the following that we found improve the results. \nUse Retriever More In our vanilla model, we noticed there was not enough attention being paid to the retrieval utterance by the generator. As the input to Seq2Seq is the dialogue history concate- nated with the retrieval utterance, truncating the history is one way to pay more attention to the re- trieval. In particular for the ConvAI2 dataset we clip the initial proﬁle sentences at the start of the dialogue, forcing the model to more strongly rely on the retriever which still has them.   We refer to this modiﬁcation as  RetrieveNReﬁne + . \nFix Retrieval Copy Errors Our model learns to sometimes ignore the retrieval (when it is bad), sometimes use it partially, and other times sim- ply copy it. However, when it is mostly copied but only changes a word or two, we observed it made mistakes more often than not, leading to less meaningful utterances. We thus also con- sider a variant that exactly copies the retrieval if the model generates with large word over- lap (we chose    ${>}60\\%$  ). Otherwise, we leave the generation untouched. We refer to this as RetrieveNReﬁne++.\nThe table presents a comparison of various retrieval methods used in conjunction with a sequence-to-sequence (Seq2Seq) model, by showing their associated perplexity (PPL) scores. The table has two columns: one for the \"RetNRef Retrieval Method\" and another for \"PPL\". \n\nThe retrieval methods listed are:\n1. None (Vanilla Seq2Seq) with a PPL of 31.4.\n2. Random label with a PPL of 32.0.\n3. Memory Network with a PPL of 31.8.\n4. True label’s neighbor with a PPL of 25.9.\n5. True label with a PPL of 9.2.\n\nLower PPL values indicate better model performance, with the \"True label\" method showing the best performance (lowest perplexity) among the methods tested.\n3 Experiments \nWe conduct experiments on the recent ConvAI2 challenge dataset which uses a modiﬁed version of the PersonaChat dataset ( Zhang et al. ,  2018 ) (larger, and with different processing). The dataset consists of conversations between crowdworkers who were randomly paired and asked to act the part of a given persona (randomly assigned from 1155 possible personas, created by another set of workers), chat naturally, and get to know each other during the conversation. There are around 160,000 utterances in around 11,000 dialogues, with 2000 dialogues for validation and test, which use non-overlapping personas. \n3.1 Automatic Evaluation and Analysis \nPerplexity Dialogue is known to be notoriously hard to evaluate with automated metrics ( Liu et al. , 2016 ). In contrast to machine translation, there is much less constraint on the output with many valid answers with little word overlap, e.g. there are many answers to “what are you doing tonight?”. Nevertheless many recent papers report perplexity results in addition to human judgments. For the re- trieve and reﬁne case, perplexity evaluation is par- ticularly ﬂawed: if the retrieval points the model to a response that is very different from (but equally valid as) the true response, the model might focus on reﬁning that and get poor perplexity. \nWe therefore test our model by considering various types of retrieval methods: (i) the best performing existing retriever model, the Memory Network approach from  Zhang et al.  ( 2018 ) (re- trieving from the training set), (ii) a retriever that returns a random utterance from the training set, (iii) the true label given in the test set, and (iv) the closest nearest neighbor from the training set ut- terances to the true label, as measured by the em- bedding space of the Memory Network retriever model. While (iii) and (iv) cannot be used in a de- ployed system as they are unknown, they can be "}
{"page": 2, "image_path": "doc_images/W18-5713_2.jpg", "ocr_text": "Word | Char | Rare Word %\nMethod cnt cnt | <100 | <1k\nSeq2Seq 11.7 | 40.5 | 0.4% | 5.8%\nRetNRef 11.8 | 40.4 | 1.1% | 6.9%\nRetNRef* 12.1 | 45.0 | 1.7% | 10.1%\nRetNRef*+ | 12.7. | 48.1 | 2.3% | 10.9%\nMemNet 13.1 | 54.5 | 4.0% | 15.3%\nHuman 13.0 | 54.6 | 3.0% | 11.5%\nTable 2: Output sequence statistics for the methods.\nSeq2Seq generates shorter sentences with more com-\nmon words than humans, which RetNRef alleviates.\n\nMethod <30% | 30-60% | 60-80% | >80%\nSeq2Seq 56% | 34% 1% 3%\nRetNRef 41% | 38% 13% 8%\nRetNRef* 26% | 20% 12% 42%\nRetNRef** | 26% | 20% 0% 53%\nTable 3: Word overlap between retrieved and gener-\n\nated utterances in RetNRef, and between Seq2Seq and\nthe Memory Network retriever (first row).\n\nused as a sanity check: a useful retrieve and refine\nshould improve perplexity if given these as input.\nWe also compare to a standard Seq2Seq model, i.e.\nno retrieval.\n\nThe results are given in Table 1. They show that\nthe RetNRef model can indeed improve perplexity\nwith label neighbors or the label itself. However,\nsurprisingly there is almost no difference between\nusing no retrieval, random labels or our best re-\ntriever. The RetNRef** model — that truncates\nthe dialogue history and focuses more on the re-\ntrieval utterance — does even worse in terms of\nperplexity: 48.4 using the Memory Network re-\ntriever. However, poor perplexity does not mean\nhuman judgments of the generated sequences will\nnot improve; in fact we will see that they do in the\nnext section. How to automatically evaluate these\nkinds of models still remains an open problem.\n\nWord Statistics Another way to measure the\nsalience of a generation model is to compare it to\nhuman utterances in terms of word statistics. We\nanalyze the word statistics of our models in Table\n2. Seq2Seq models are known to produce short\nsentences with more common words than humans.\nThe statistics on the ConvAI2 dataset bear this out,\nwhere the Seq2Seq model responses have lower\nword and character counts and use fewer rare\nwords than the human responses. The RetNRef\n\nmodel (using the Memory Network retriever, re-\ntrieving from the training set) makes some im-\nprovements in this regard, e.g. doubling the use\nof rare words (with frequency less than 100) and\nsmaller gains for words with frequency less than\n1000, but are still not close to human statistics.\nThe RetNReft* model which boosts the use of\nhe retrieval does better in this regard, making the\nstatistics much closer to human ones. Of course\nhese metrics do not measure whether the utter-\nances are semantically coherent, but it is encour-\naging to see a model using rare words as without\nhis we believe it is hard for it to be engaging.\n\nTable 3 compares the word overlap between re-\ntrieved and generated sentences in the RetNRef\nvariants in order to measure if RetNRef is either\nignoring the retriever, or else paying too much at-\nention to and copying it. As comparison, the first\nrow also shows the overlap between the retriever\nand vanilla Seq2Seq which does not retrieve at\nall. The results show that RetNReft* has >80%\nword overlap with the retriever output around half\n(53%) of the time, whereas Seq2Seq and RetNRef\nvery rarely overlap with the retriever (3% and 8%\nof the time respectively have >80% overlap). This\nshows that our improved model RetNReft+ does\nuse the retriever, but can also generate novel con-\ntent when it wants to, which a standard retriever\ncannot.\n\n3.2. Evaluation by Human Judgement Scores\n\nFollowing the protocol in Zhang et al. (2018), we\nasked humans to conduct short dialogues with our\nmodels (100 dialogues each of 10-20 turns, so 600\ndialogues in total), and then measure the engag-\ningness, consistency, and fluency (all scored out\nof 5) as well as to try to detect the persona that\nthe model is using, given the choice between that\nand a random persona. Consistency measures the\ncoherence of the dialogue, e.g. “I have a dog” fol-\nlowed by “T have no pets” is not consistent.\n\nThe results are given in Table 4. They show\nengagingness scores superior to Seq2Seq for all\nRetNRef variants, and with RetNRef** slightly\noutperforming the retriever which it conditions\non. Importantly however, it maintains this per-\nformance whilst still being able to generate text\nwhich a retrieval model cannot. It also performs\nwell in the other metrics, although like the Mem-\nory Network model, it is weaker at using the per-\nsona than Seq2Seq. Seq2Seq is inferior to the\n\n", "vlm_text": "The table shows a comparison of different methods based on four metrics: Word Count (cnt), Character Count (cnt), Rare Word Percentage for words appearing less than 100 times, and Rare Word Percentage for words appearing less than 1,000 times. Here is the data presented in the table:\n\n1. **Seq2Seq**\n   - Word Count: 11.7\n   - Character Count: 40.5\n   - Rare Word % (<100): 0.4%\n   - Rare Word % (<1k): 5.8%\n\n2. **RetNRef**\n   - Word Count: 11.8\n   - Character Count: 40.4\n   - Rare Word % (<100): 1.1%\n   - Rare Word % (<1k): 6.9%\n\n3. **RetNRef⁺**\n   - Word Count: 12.1\n   - Character Count: 45.0\n   - Rare Word % (<100): 1.7%\n   - Rare Word % (<1k): 10.1%\n\n4. **RetNRef⁺⁺**\n   - Word Count: 12.7\n   - Character Count: 48.1\n   - Rare Word % (<100): 2.3%\n   - Rare Word % (<1k): 10.9%\n\n5. **MemNet**\n   - Word Count: 13.1\n   - Character Count: 54.5\n   - Rare Word % (<100): 4.0%\n   - Rare Word % (<1k): 15.3%\n\n6. **Human**\n   - Word Count: 13.0\n   - Character Count: 54.6\n   - Rare Word % (<100): 3.0%\n   - Rare Word % (<1k): 11.5%\n\nThis table appears to be measuring the efficacy and complexity of different methods for a particular task, likely in the domain of natural language processing or text generation, as indicated by the focus on word and character counts, and the frequency of rare words.\nThe table presents a comparison of methods based on their performance categorized by different percentage ranges. The methods listed are Seq2Seq, RetNRef, RetNRef^+, and RetNRef^++. Each method's performance is shown across four percentage categories: <30%, 30-60%, 60-80%, and >80%.\n\n- **Seq2Seq method**: \n  - <30%: 56%\n  - 30-60%: 34%\n  - 60-80%: 7%\n  - >80%: 3%\n\n- **RetNRef method**:\n  - <30%: 41%\n  - 30-60%: 38%\n  - 60-80%: 13%\n  - >80%: 8%\n\n- **RetNRef^+ method**:\n  - <30%: 26%\n  - 30-60%: 20%\n  - 60-80%: 12%\n  - >80%: 42%\n\n- **RetNRef^++ method**:\n  - <30%: 26%\n  - 30-60%: 20%\n  - 60-80%: 0%\n  - >80%: 53%\n\nThis table likely demonstrates how each method performs in terms of certain criteria or metrics, with higher percentage values indicating better performance in specific ranges. RetNRef^++ has the most significant percentage of its performance in the >80% category, indicating it may be the most effective among the methods within this table's context.\nused as a sanity check: a useful retrieve and reﬁne should improve perplexity if given these as input. We also compare to a standard Seq2Seq model, i.e. no retrieval. \nThe results are given in Table  1 . They show that the RetNRef model can indeed improve perplexity with label neighbors or the label itself. However, surprisingly there is almost no difference between using no retrieval, random labels or our best re- triever. The RetNRef ++   model – that truncates the dialogue history and focuses more on the re- trieval utterance – does even worse in terms of perplexity: 48.4 using the Memory Network re- triever. However, poor perplexity does not mean human judgments of the generated sequences will not improve; in fact we will see that they do in the next section. How to automatically evaluate these kinds of models still remains an open problem. \nWord Statistics Another way to measure the salience of a generation model is to compare it to human utterances in terms of word statistics. We analyze the word statistics of our models in Table 2 . Seq2Seq models are known to produce short sentences with more common words than humans. The statistics on the ConvAI2 dataset bear this out, where the Seq2Seq model responses have lower word and character counts and use fewer rare words than the human responses. The RetNRef model (using the Memory Network retriever, re- trieving from the training set) makes some im- provements in this regard, e.g. doubling the use of rare words (with frequency less than 100) and smaller gains for words with frequency less than 1000, but are still not close to human statistics. The RetNRef ++   model which boosts the use of the retrieval does better in this regard, making the statistics much closer to human ones. Of course these metrics do not measure whether the utter- ances are semantically coherent, but it is encour- aging to see a model using rare words as without this we believe it is hard for it to be engaging. \n\nTable  3  compares the word overlap between re- trieved and generated sentences in the RetNRef variants in order to measure if RetNRef is either ignoring the retriever, or else paying too much at- tention to and copying it. As comparison, the ﬁrst row also shows the overlap between the retriever and vanilla Seq2Seq which does not retrieve at all. The results show that RetNRef ++   has  ${>}80\\%$  word overlap with the retriever output around half  $(53\\%)$   of the time, whereas Seq2Seq and RetNRef very rarely overlap with the retriever (  $3\\%$   and   $8\\%$  of the time respectively have  ${>}80\\%$   overlap). This shows that our improved model  $\\mathbf{Set}\\mathbf{NRef}^{++}$    does use the retriever, but can also generate novel con- tent when it wants to, which a standard retriever cannot. \n3.2 Evaluation by Human Judgement Scores \nFollowing the protocol in  Zhang et al.  ( 2018 ), we asked humans to conduct short dialogues with our models (100 dialogues each of 10-20 turns, so 600 dialogues in total), and then measure the engag- ingness, consistency, and ﬂuency (all scored out of 5) as well as to try to detect the persona that the model is using, given the choice between that and a random persona. Consistency measures the coherence of the dialogue, e.g. “I have a dog” fol- lowed by “I have no pets” is not consistent. \nThe results are given in Table  4 . They show engagingness scores superior to Seq2Seq for all RetNRef variants, and with   $\\mathbf{Set}\\mathbf{NRef}^{++}$    slightly outperforming the retriever which it conditions on. Importantly however, it maintains this per- formance whilst still being able to generate text which a retrieval model cannot. It also performs well in the other metrics, although like the Mem- ory Network model, it is weaker at using the per- sona than Seq2Seq. Seq2Seq is inferior to the "}
{"page": 3, "image_path": "doc_images/W18-5713_3.jpg", "ocr_text": "Method Engagingness Fluency Consistency Persona\nSeq2Seq (PPL) 2.70(1.17) 3.50(1.37) — 3.90(1.37) —-0.90(0.29)\nSeq2Seq (100 epochs) 2.76(1.15) 3.53(1.14) 3.84(1.38) —-0.85(0.35)\nMemory Network 3.66(1.26) 3.83(1.26)  3.61(1.36) —0.73(0.44)\nRetrieveNRefine 2.94(1.26) 3.65(1.28) — 3.72(1.32) —0.90(0.30)\nRetrieveNRefine* 3.50(1.33) 3.63(1.13)  3.55(1.33) 0.71.45)\nRetrieveNRefinett 3.80(1.18) 3.74(1.19) 3.80(1.40) —0.65(0.47)\n\nTable 4: Human Evaluation scores for the models,scoring fluency, engagingness, consistency and persona detec-\n\ntion, with standard deviation in parentheses. We consi\n\nler engagingness to be the most important metric.\n\nComparison (A vs. B) Win Rate A Wins BWins Tie p-value\nRetrieveNRefine*t vs. Memory Network | 54.5% 340 284 572 0.027\nRetrieveNRefinet* vs. Seq2Seq 53.7% 571 492 203 0.016\nRetrieveNRefine™* (retrieved) vs.Seq2Seq | 53.8% 290 249 87\nRetrieveNRefine** (generated) vs.Seq2Seq | 53.6% 281 243 116\nRetrieveNRefine* vs. Memory Network 51.63% 492 461 243\nRetrieveNRefine* vs. Human 30.13% 69 160 14\n\nSeq2Seq vs. Human 26.84% 62 169 22\n\nTable 5: A/B testing by humans comparing model pairs, reporting win rates for A over B (excluding ties).\n\nPerson | hey , george here . lets be friends yeah. well, i wish you luck there. we bought a cute 2 story house.\nPerson 2 hi george , i’m regina . do you enjoy dance ? nice , i don’t think i have ever left ohio.\nSeq2Seq i do not but i do have a lot of time to dance iam sorry. i am so scared of twins.\nMemNet hello george my name is pink iam from mississippi . just moved back here from austin texas\nRetNRef* | ido. i enjoy dance. i am from tennessee. just moved here from austin texas.\nPerson | drinking coffee? what time is it there? thank you for that , you are what you believe ?\nPerson 2 830 pm haha my dad is a construction worker ||| look energy isn’t physical , only spiritual\nso he works really late nights\nSeq2Seq iam a big guy soi can help you i am not sure what that means\nMemNet yep . moonlight is amazing but northern do you do any sports ? swimming helps me keep my energy up.\nlight grab you too\nRetNRef* | oh wow, that is a long time do you do any sports ? swimming helps me keep my energy up\n\nTable 6: Example dialogues (left) where RetNRef outperforms MemNet, and (right) where RetNRef outperforms\nSeq2Seq — by either paying attention to the retriever or not. The first two rows are the previous two dialogue turns\nby Person 1 & 2, the following rows are the predictions for the next utterance of Person | by the various models.\n\nMemory Network Retriever in terms of engage-\nment, in line with previous results. We also tried\novertraining the Seq2Seq for 100 epochs instead\nof early stopping by validation on perplexity as it\nmay overfit training sentences and act more as a\nretriever, but this did not help.\n\nSome example dialogues of the RetNReftt\nmodel performing well (as scored by the evalua-\ntors) are shown in Tables 7 and 8. Longer sen-\ntences from the bot (person 2) with more nu-\nanced entity information typically come from at-\ntending to the retriever, whereas the generator can\nalso produce shorter replies independent of the re-\ntriever that fit the context well. There are still is-\nsues however, such as repeated phrases by the gen-\nerator, and some tendency to copy the speaking\n\npartner’s phrases that could be improved.\n\n3.3 A/B Testing by Humans\n\nTo compute a statistically more meaningful test,\nand to evaluate models more clearly against each\nother, we also tested models by pairing them\nagainst each other (different to the protocol pre-\nviously used for this dataset). We took human-\nhuman paired dialogues from the test set, up to a\nrandomly chosen turn, and then computed the re-\nsponse of two rival models. The resulting dialogue\nhistory and the two choices are given to a third an-\nnotator to evaluate and asked which model is best;\nthey can also choose “unsure”. We collected over\n1000 such examples, with each conversation being\nevaluated twice. The results are given in Table 5.\n\n90\n", "vlm_text": "The table compares different methods based on four metrics: Engagingness, Fluency, Consistency, and Persona. Each metric includes a mean value with a standard deviation in parentheses. Here is a summary of the data:\n\n1. **Seq2Seq (PPL):**\n   - Engagingness: 2.70 (1.17)\n   - Fluency: 3.50 (1.37)\n   - Consistency: 3.90 (1.37)\n   - Persona: 0.90 (0.29)\n\n2. **Seq2Seq (100 epochs):**\n   - Engagingness: 2.76 (1.15)\n   - Fluency: 3.53 (1.14)\n   - Consistency: 3.84 (1.38)\n   - Persona: 0.85 (0.35)\n\n3. **Memory Network:**\n   - Engagingness: 3.66 (1.26)\n   - Fluency: 3.83 (1.26)\n   - Consistency: 3.61 (1.36)\n   - Persona: 0.73 (0.44)\n\n4. **RetrieveNRefine:**\n   - Engagingness: 2.94 (1.26)\n   - Fluency: 3.65 (1.28)\n   - Consistency: 3.72 (1.32)\n   - Persona: 0.90 (0.30)\n\n5. **RetrieveNRefine+:**\n   - Engagingness: 3.50 (1.33)\n   - Fluency: 3.63 (1.13)\n   - Consistency: 3.55 (1.33)\n   - Persona: 0.71 (0.45)\n\n6. **RetrieveNRefine++:**\n   - Engagingness: 3.80 (1.18)\n   - Fluency: 3.74 (1.19)\n   - Consistency: 3.80 (1.40)\n   - Persona: 0.65 (0.47)\n\nThese results may illustrate the performance of each method across the different metrics, providing insights into the strengths and weaknesses of each approach.\nThe table presents the results of several comparative evaluations between different methods or models. Each row shows a comparison between two approaches labeled \"A vs. B,\" along with several statistics:\n\n1. **Comparison (A vs. B):** Specifies the two methods being compared in each evaluation.\n2. **Win Rate:** The proportion of instances where method A is preferred or wins over method B, expressed as a percentage.\n3. **A Wins:** The number of instances where method A is favored over method B.\n4. **B Wins:** The number of instances where method B is favored over method A.\n5. **Tie:** The number of instances where the comparison resulted in a tie.\n6. **p-value:** The p-value indicating the statistical significance of the observed win rate difference, where a lower value suggests a more statistically significant difference.\n\nIn summary, different versions of the \"RetrieveNRefine\" model are compared against alternatives like \"Memory Network,\" \"Seq2Seq,\" and human performance, with calculated win rates and statistical significance for each comparison.\nThe table appears to show a conversation simulation between two people and three different response systems titled \"Seq2Seq,\" \"MemNet,\" and \"RetNRef⁺.\" The table is divided into two sections based on different conversation contexts. \n\nIn the first section:\n- Two persons (Person 1 and Person 2) are engaging in a conversation about introducing themselves and discussing dancing.\n- \"Seq2Seq,\" \"MemNet,\" and \"RetNRef⁺\" provide potential responses to the conversation. These responses vary in their relevance and coherence to the conversation context.\n\nIn the second section:\n- Two persons are engaged in a conversation about time zones, work, and energy.\n- \"Seq2Seq,\" \"MemNet,\" and \"RetNRef⁺\" again provide responses with varying degrees of relevance and appropriateness to this context.\n\nThe purpose of the table seems to be to compare the effectiveness and appropriateness of the responses from the different systems in conversational scenarios.\nMemory Network Retriever in terms of engage- ment, in line with previous results. We also tried overtraining the Seq2Seq for 100 epochs instead of early stopping by validation on perplexity as it may overﬁt training sentences and act more as a retriever, but this did not help. \nSome example dialogues of the RetNRef ++ model performing well (as scored by the evalua- tors) are shown in Tables  7  and  8 . Longer sen- tences from the bot (person 2) with more nu- anced entity information typically come from at- tending to the retriever, whereas the generator can also produce shorter replies independent of the re- triever that ﬁt the context well. There are still is- sues however, such as repeated phrases by the gen- erator, and some tendency to copy the speaking partner’s phrases that could be improved. \n\n3.3 A/B Testing by Humans \nTo compute a statistically more meaningful test, and to evaluate models more clearly against each other, we also tested models by pairing them against each other (different to the protocol pre- viously used for this dataset). We took human- human paired dialogues from the test set, up to a randomly chosen turn, and then computed the re- sponse of two rival models. The resulting dialogue history and the two choices are given to a third an- notator to evaluate and asked which model is best; they can also choose “unsure”. We collected over 1000 such examples, with each conversation being evaluated twice. The results are given in Table  5 . "}
{"page": 4, "image_path": "doc_images/W18-5713_4.jpg", "ocr_text": "Persona 1\n\nlam relatively tall.\n\nI like mexican food.\n\nI drive a subaru legacy.\n\nMy dog’s name is Drunky.\n\nIcan touch my toes with my nose.\n\nPersona 2\n\n[PERSON 1:\n[PERSON 2:\n[PERSON 1:\n[PERSON 2:\n[PERSON 1:\n[PERSON 2:\n[PERSON 1:\n[PERSON 2:\n[PERSON 1:\nL\n\nPERSON 2:] I have a Nissan Sentra. I love it.\n\nI like to play baseball.\n\nI love eating seafood.\n\nT enjoy learning new languages.\n\nI’m currently going for a marketing degree.\nI want to design ads in the future.\n\nI’m driving to the mexican place tonight for dinner, but my Subaru is at the rental shop.\nYummy. I love mexican food, but I never eat vegetables\n\nOh, I do, I think that’s why i’m so tall!\n\nhow tall are you? I do not... I enjoy studying languages.\n\nI’m 6 foot, a lot taller than my dog, Drunky.\n\nTam sorry to hear that. I want to be 6 foot tall.\n\nDo you have any talents? I can reach my toes using my nose.\n\nCool! I listen to audiobooks and speak one asian language. you like sushi? I do.\n\nOh sushi is my next favorite, after Mexican. What kind of car do you have?\n\nTable 7: Example dialog between a human (Person 1) and the RetNRef** model (Person 2). References to never\neating vegetables, listening to audiobooks and speaking one asian language and owning a Nissan Sentra appeared\nin the conditioned retrieved sentences, improving the overall engaging quality of the dialogue.\n\nPersona 1 Persona 2\nIam a lacto-ovo vegetarian. Ihave brown hair.\nI like to hike. Tam Latina.\n\nT have red hair.\nMy favorite drink is sprite.\nIhave a pet parrot named Tasha.\n\n[PERSON 1:\n[PERSON 2:\n[PERSON 1:\n[PERSON 2:] I bet. I love to go to the water\n[PERSON 1:\n[PERSON 2:\n[PERSON 1:\n[PERSON 2:\n[PERSON 1:\n[PERSON 2:\n\nMy mother works for special needs clients..\nTam a healer.\nI love my family.\n\nI dont know about you but I love sprite. Its my favorite.\nI like dr pepper a lot, but sprite is my second favorite.\nSprites great! I take it with me every day on my hikes.\n\nYa that sounds like a great time. I go with my pet parrot Tasha when I go\n\nthat is a good idea. I wish I had a parrot.\n\nYa its fun. As a pet owner you learn to love animals; im even a lactoovo vegetarian now\nT love animals. I am vegan so I eat vegan food.\n\nSounds like me :D im vegan, im ginger, and i love sprite\n\nT love animals. I am vegan so I eat a lot of it.\n\nTable 8: Example dialog between a human (Person 1) and the RetNReft+ model (Person 2). References to\nDr. Pepper and wishing to have a parrot appeared in the conditioned retrieved sentences, improving the overall\nengaging quality of the dialogue. There are still issues however, such as repeated phrases by the generator, failure\nto use the persona adequately, and some tendency to copy the speaking partner’s phrases that could be improved.\n\nRetrieveNRefine obtains statistically significant\nwins over the retriever Memory Network model\nand the generator Seq2Seq model using a binomial\ntwo-tailed test, with win rates ~54%. Breaking\ndown the wins between when RetNRef** exactly\ncopies the retrieval utterance vs. generates we see\nthat it chooses them about equally, with wins about\nequal in both cases. This shows it can effectively\nlearn when to choose the retrieval utterance (when\nit is good), and when to ignore it and generate in-\nstead (when it is bad). Table 6, which shows ex-\nample outputs of our model, illustrates this.\n\nRetNRef* sometimes loses out when making\nsmall changes to the retrieved text, for example\nit made changes to “i once broke my nose try-\ning to peak in on a jazz concert !” by replacing\n\n91\n\npeak with glacier. Recall that RetNReft* fixes\nthis problem by exactly copying the retrieved text\nwhen there is insignificant word overlap with the\ngenerated text; as such, it has a correspondingly\nlarger win rate against Memory Networks (54.5%\nversus 51.63%).\n\nWe also computed a small sample of A/B tests\ndirectly against humans rather than models, and\nagain see the win rate is higher for RetNRef.\n\n4 Conclusion\n\nIn conclusion, we showed that retrieval models\ncan be successfully used to improve generation\nmodels in dialogue, helping them avoid common\nissues such as producing short sentences with fre-\nquent words that ultimately are not engaging. Our\n", "vlm_text": "Persona 1 Persona 2 I am relatively tall. I like to play baseball. I like mexican food. I love eating seafood. I drive a subaru legacy. I enjoy learning new languages. My dog’s name is Drunky. I’m currently going for a marketing degree. I can touch my toes with my nose. I want to design ads in the future. \n[PERSON 1:] I’m driving to the mexican place tonight for dinner, but my Subaru is at the rental shop. [PERSON 2:] Yummy. I love mexican food, but I never eat vegetables [PERSON 1:] Oh, I do, I think that’s why i’m so tall! [PERSON 2:] how tall are you? I do not... I enjoy studying languages. [PERSON 1:] I’m 6 foot, a lot taller than my dog, Drunky. [PERSON 2:] I am sorry to hear that. I want to be 6 foot tall. [PERSON 1:] Do you have any talents? I can reach my toes using my nose. [PERSON 2:] Cool! I listen to audiobooks and speak one asian language. you like sushi? I do. [PERSON 1:] Oh sushi is my next favorite, after Mexican. What kind of car do you have? [PERSON 2:] I have a Nissan Sentra. I love it. \nTable 7: Example dialog between a human (Person 1) and the RetNRef ++   model (Person 2). References to never eating vegetables, listening to audiobooks and speaking one asian language and owning a Nissan Sentra appeared in the conditioned retrieved sentences, improving the overall engaging quality of the dialogue. \nThe table contains two columns, each labeled with \"Persona 1\" and \"Persona 2.\" It lists attributes or statements that describe each persona:\n\n**Persona 1:**\n- I am a lacto-ovo vegetarian.\n- I like to hike.\n- I have red hair.\n- My favorite drink is Sprite.\n- I have a pet parrot named Tasha.\n\n**Persona 2:**\n- I have brown hair.\n- I am Latina.\n- My mother works for special needs clients.\n- I am a healer.\n- I love my family.\n[PERSON 1:] I dont know about you but I love sprite. Its my favorite. [PERSON 2:] I like dr pepper a lot, but sprite is my second favorite. [PERSON 1:] Sprites great! I take it with me every day on my hikes. [PERSON 2:] I bet. I love to go to the water [PERSON 1:] Ya that sounds like a great time. I go with my pet parrot Tasha when I go [PERSON 2:] that is a good idea. I wish I had a parrot. [PERSON 1:] Ya its fun. As a pet owner you learn to love animals; im even a lactoovo vegetarian now [PERSON 2:] I love animals. I am vegan so I eat vegan food. [PERSON 1:] Sounds like me :D im vegan, im ginger, and i love sprite [PERSON 2:] I love animals. I am vegan so I eat a lot of it. \nTable 8: Example dialog between a human (Person 1) and the   $\\mathbf{Set}\\mathbf{NRef}^{++}$    model (Person 2). References to Dr. Pepper and wishing to have a parrot appeared in the conditioned retrieved sentences, improving the overall engaging quality of the dialogue. There are still issues however, such as repeated phrases by the generator, failure to use the persona adequately, and some tendency to copy the speaking partner’s phrases that could be improved. \nRetrieveNReﬁne obtains statistically signiﬁcant wins over the retriever Memory Network model and the generator Seq2Seq model using a binomial two-tailed test, with win rate  ${\\sim}54\\%$  reaking down the wins between when RetNRef   exactly copies the retrieval utterance vs. generates we see that it chooses them about equally, with wins about equal in both cases. This shows it can effectively learn when to choose the retrieval utterance (when it is good), and when to ignore it and generate in- stead (when it is bad). Table  6 , which shows ex- ample outputs of our model, illustrates this. \nRetNRef +   sometimes loses out when making small changes to the retrieved text, for example it made changes to “i once broke my nose try- ing to peak in on a jazz concert !” by replacing peak  with  glacier . Recall that   $\\mathbf{Set}\\mathbf{NRef}^{++}$    ﬁxes this problem by exactly copying the retrieved text when there is insigniﬁcant word overlap with the generated text; as such, it has a correspondingly larger win rate against Memory Networks   $(54.5\\%$  versus   $51.63\\%$  ). \n\nWe also computed a small sample of A/B tests directly against humans rather than models, and again see the win rate is higher for RetNRef. \n4 Conclusion \nIn conclusion, we showed that retrieval models can be successfully used to improve generation models in dialogue, helping them avoid common issues such as producing short sentences with fre- quent words that ultimately are not engaging. Our RetNRef ++   model has similar statistics to human utterances and provides more engaging conversa- tions according to human judgments. "}
{"page": 5, "image_path": "doc_images/W18-5713_5.jpg", "ocr_text": "RetNRef** model has similar statistics to human\nutterances and provides more engaging conversa-\ntions according to human judgments.\n\nFuture work should investigate improved ways\nto incorporate retrieval in generation, both avoid-\ning the heuristics we used here to improve per-\nformance, and seeing if more sophisticated ap-\nproaches than concatenation plus attention im-\nprove the results, for example by more clearly\ntreating the inputs as independent sources, or\ntraining the models jointly.\n\nReferences\n\nZigiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\n2018. Retrieve, rerank and rewrite: Soft template\nbased neural summarization. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 152-161.\n\nAngela Fan, David Grangier, and Michael Auli. 2017.\nControllable abstractive summarization. arXiv\npreprint arXiv:1711.05217.\n\nDavid Grangier and Michael Auli. 2017. Quickedit:\nEditing text & translations via simple delete actions.\narXiv preprint arXiv:1711.04805.\n\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\ntor OK Li. 2017. Search engine guided non-\n\nparametric neural machine translation. arXiv\npreprint arXiv: 1705.07267.\nKelvin Guu, Tatsunori B Hashimoto, Yonatan\n\nOren, and Percy Liang. 2017.\ntences by editing prototypes.\narXiv:1709.08878.\n\nGenerating sen-\narXiv preprint\n\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\ninformation retrieval approach to short text conver-\nsation. arXiv preprint arXiv: 1408.6988.\n\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2017. An exploration of neural sequence-to-\nsequence architectures for automatic post-editing.\narXiv preprint arXiv: 1706.04138.\n\nChia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael\nNoseworthy, Laurent Charlin, and Joelle Pineau.\n2016. How not to evaluate your dialogue system:\nAn empirical study of unsupervised evaluation met-\nrics for dialogue response generation. arXiv preprint\narXiv: 1603.08023.\n\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason We-\nston. 2016. Key-value memory networks for\ndirectly reading documents. arXiv preprint\narXiv: 1606.03126.\n\n92\n\nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\nWaibel. 2016. Pre-translation for neural machine\ntranslation. arXiv preprint arXiv: 1610.05243.\n\nRoman Novak, Michael Auli, and David Grangier.\n2016. Iterative refinement for machine translation.\narXiv preprint arXiv:1610.06602.\n\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\nSachindra Joshi. 2018. Exemplar encoder-decoder\nfor neural conversation generation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), volume 1, pages 1329-1338.\n\nAllen Schmaltz, Yoon Kim, Alexander M Rush,\nand Stuart M Shieber. 2017. Adapting sequence\nmodels for sentence correction. arXiv preprint\narXiv:1707.09067.\n\nJulian Vlad Serban, Ryan Lowe, Laurent Charlin, and\nJoelle Pineau. 2016. Generative deep neural net-\nworks for dialogue: A short review. arXiv preprint\narXiv:1611.06216.\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104-3112.\n\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In Advances in Neural Information Process-\ning Systems, pages 1782-1792.\n\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? arXiv preprint arXiv:1801.07243.\n", "vlm_text": "\nFuture work should investigate improved ways to incorporate retrieval in generation, both avoid- ing the heuristics we used here to improve per- formance, and seeing if more sophisticated ap- proaches than concatenation plus attention im- prove the results, for example by more clearly treating the inputs as independent sources, or training the models jointly. \nReferences \nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2018. Retrieve, rerank and rewrite: Soft template based neural summarization. In  Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , vol- ume 1, pages 152–161. Angela Fan, David Grangier, and Michael Auli. 2017. Controllable abstractive summarization. arXiv preprint arXiv:1711.05217 . David Grangier and Michael Auli. 2017. Quickedit: Editing text & translations via simple delete actions. arXiv preprint arXiv:1711.04805 . Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic- tor OK Li. 2017. Search engine guided non- parametric neural machine translation. arXiv preprint arXiv:1705.07267 . Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2017. Generating sen- tences by editing prototypes. arXiv preprint arXiv:1709.08878 . Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An information retrieval approach to short text conver- sation.  arXiv preprint arXiv:1408.6988 . Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2017. An exploration of neural sequence-to- sequence architectures for automatic post-editing. arXiv preprint arXiv:1706.04138 . Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation met- rics for dialogue response generation.  arXiv preprint arXiv:1603.08023 . Alexander Miller, Adam Fisch, Jesse Dodge, Amir- Hossein Karimi, Antoine Bordes, and Jason We- ston. 2016. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126 . \nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2016. Pre-translation for neural machine translation.  arXiv preprint arXiv:1610.05243 . Roman Novak, Michael Auli, and David Grangier. 2016. Iterative reﬁnement for machine translation. arXiv preprint arXiv:1610.06602 . Gaurav Pandey, Danish Contractor, Vineet Kumar, and Sachindra Joshi. 2018. Exemplar encoder-decoder for neural conversation generation. In  Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers) , volume 1, pages 1329–1338. Allen Schmaltz, Yoon Kim, Alexander M Rush, and Stuart M Shieber. 2017. Adapting sequence models for sentence correction. arXiv preprint arXiv:1707.09067 . Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016. Generative deep neural net- works for dialogue: A short review.  arXiv preprint arXiv:1611.06216 . Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In  Advances in neural information process- ing systems , pages 3104–3112. Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation networks: Sequence generation beyond one-pass de- coding. In  Advances in Neural Information Process- ing Systems , pages 1782–1792. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per- sonalizing dialogue agents: I have a dog, do you have pets too?  arXiv preprint arXiv:1801.07243 . "}
