{"layout": 0, "type": "text", "text": "NCRF++: An Open-source Neural Sequence Labeling Toolkit ", "text_level": 1, "page_idx": 0, "bbox": [107, 67, 491, 87], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Jie Yang  and  Yue Zhang Singapore University of Technology and Design jie yang@mymail.sutd.edu.sg yue zhang@sutd.edu.sg ", "page_idx": 0, "bbox": [184.90499877929688, 113.2550048828125, 415.62841796875, 169.4084014892578], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "This paper describes   $\\mathrm{NCRF++}$  , a toolkit for neural sequence labeling.  $\\mathrm{NCRF++}$  is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an inference for building the custom model structure through conﬁgu- ration ﬁle with ﬂexible neural feature de- sign and utilization. Built on PyTorch 1 , the core operations are calculated in batch, making the toolkit efﬁcient with the accel- eration of GPU. It also includes the imple- mentations of most state-of-the-art neural sequence labeling models such as LSTM- CRF, facilitating reproducing and reﬁne- ment on those methods. ", "page_idx": 0, "bbox": [89, 245, 274, 461.4345397949219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 470, 155, 483], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "Sequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statis- tical approaches ( Lafferty et al. ,  2001 ;  Ratinov and Roth ,  2009 ), where conditional random ﬁelds (CRF) ( Lafferty et al. ,  2001 ) has been proven as an effective framework, by taking discrete features as the representation of input sequence ( Sha and Pereira ,  2003 ;  Keerthi and Sundararajan ,  2007 ). ", "page_idx": 0, "bbox": [71, 490.89605712890625, 290, 639.5335693359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "With the advances of deep learning, neural se- quence labeling models have achieved state-of- the-art for many tasks ( Ling et al. ,  2015 ;  Ma and Hovy ,  2016 ;  Peters et al. ,  2017 ). Features are extracted automatically through network struc- tures including long short-term memory (LSTM) ( Hochreiter and Schmidhuber ,  1997 ) and convolu- tion neural network (CNN) ( LeCun et al. ,  1989 ), ", "page_idx": 0, "bbox": [71, 639.9370727539062, 290, 747.926513671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "##Network Configuration## use crf  $\\leftrightharpoons$  True word seq feature  $=$  LSTM word seq layer  $\\cdot{=}1$  char seq feature  $\\mathfrak{z}\\!=\\!\\mathrm{CNN}\\!\\mathrm{N}$  feature  $=$  [POS] emb dir  $=$  None emb size  $\\scriptstyle{:=10}$  feature  $=$  [Cap] emb dir  $\\mathbf{\\Sigma}=\\frac{\\circ}{\\circ}$  (cap emb dir) ##Hyperparameters## ... ", "page_idx": 0, "bbox": [314, 223.05099487304688, 509, 318.9115905761719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "Figure 1: Conﬁguration ﬁle segment ", "page_idx": 0, "bbox": [336.7690124511719, 328.41400146484375, 496, 341.5594787597656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "with distributed word representations. Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies ( Collobert et al. , 2011 ;  Lample et al. ,  2016 ;  Peters et al. ,  2017 ). ", "page_idx": 0, "bbox": [307, 360.92401123046875, 527, 428.2665100097656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "There exist several open-source statistical CRF sequence labeling toolkits, such as  $\\mathrm{CRF++}^{2}$  , CRF- Suite ( Okazaki ,  2007 ) and FlexCRFs ( Phan et al. , 2004 ), which provide users with ﬂexible means of feature extraction, various training settings and de- coding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural se- quence labeling toolkits. Although many authors released their code along with their sequence la- beling papers ( Lample et al. ,  2016 ;  Ma and Hovy , 2016 ;  Liu et al. ,  2018 ), the implementations are mostly focused on speciﬁc model structures and speciﬁc tasks. Modifying or extending can need enormous coding. ", "page_idx": 0, "bbox": [307, 428.6700439453125, 527, 631.5045166015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "In this paper, we present Neural   $\\mathrm{CRF++}$   $(\\mathrm{NCRF}++)^{3}$  , a neural sequence labeling toolkit based on PyTorch, which is designed for solv- ing general sequence labeling tasks with effective and efﬁcient neural models. It can be regarded as the neural version of   $\\mathrm{CRF++}$  , with both take the CoNLL data format as input and can add hand- ", "page_idx": 0, "bbox": [307, 631.9080810546875, 527, 726.3485107421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "image", "page_idx": 1, "img_path": "layout_images/P18-4013_0.jpg", "img_caption": "Figure 2:   $\\mathrm{NCRF++}$   for sentence “I love Bruce Lee”. Green, red, yellow and blue circles represent character embeddings, word embeddings, character sequence representations and word sequence repre- sentations, respectively. The grey circles represent the embeddings of sparse features. ", "bbox": [69, 64, 527, 292], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Char\nSequence\nLayer\n\nWord\nSequence\nLayer\n\nInference\nLayer\n\nInference Layer: Softmax or CRF\n", "vlm_text": "The image is a diagram representing the structure of the $\\mathrm{NCRF++}$ model applied to the sentence “I love Bruce Lee.” It consists of three layers:\n\n1. **Char Sequence Layer**: \n   - Green circles represent character embeddings for each character in the words of the sentence.\n   - These embeddings are processed through an RNN/CNN.\n\n2. **Word Sequence Layer**: \n   - The output from the Char Sequence Layer, along with additional features represented by grey circles, contributes to word embeddings (represented by red and yellow circles).\n   - These are combined and processed to form word sequence representations.\n\n3. **Inference Layer**:\n   - Blue circles represent the word sequence representations processed through the RNN/CNN.\n   - The final output is determined using either a Softmax or CRF (Conditional Random Field) function."}
{"layout": 13, "type": "text", "text": "crafted features to CRF framework conveniently. We take the layerwise implementation, which in- cludes character sequence layer, word sequence layer and inference layer.   $\\mathrm{NCRF++}$   is: ", "page_idx": 1, "bbox": [70, 310.322998046875, 290, 364.1164855957031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "•  Fully conﬁgurable : users can design their neural models only through a conﬁguration ﬁle without any code work. Figure  1  shows a seg- ment of the conﬁguration ﬁle. It builds a LSTM- CRF framework with CNN to encode character sequence (the same structure as  Ma and Hovy ( 2016 )), plus  POS  and  Cap  features, within 10 lines. This demonstrates the convenience of de- signing neural models using  $\\mathrm{NCRF++}$  . ", "page_idx": 1, "bbox": [70, 367.123291015625, 290, 489.0555419921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "•  Flexible with features : human-deﬁned fea- tures have been proved useful in neural se- quence labeling ( Collobert et al. ,  2011 ;  Chiu and Nichols ,  2016 ). Similar to the statistical toolkits,  $\\mathrm{NCRF++}$   supports user-deﬁned features but using distributed representations through lookup tables, which can be initialized randomly or from exter- nal pretrained embeddings (embedding directory: emb dir  in Figure  1 ). In addition,  $\\mathrm{NCRF++}$   in- tegrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many re- cent work ( Lample et al. ,  2016 ;  Chiu and Nichols , 2016 ;  Ma and Hovy ,  2016 ). ", "page_idx": 1, "bbox": [70, 492.0623474121094, 290, 681.7394409179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "•  Effective and efﬁcient : we reimplement sev- eral state-of-the-art neural models ( Lample et al. , 2016 ;  Ma and Hovy ,  2016 ) using  $\\mathrm{NCRF++}$  . Ex- periments show models built in   $\\mathrm{NCRF++}$   give comparable performance with reported results in the literature. Besides,   $\\mathrm{NCRF++}$   is implemented using batch calculation, which can be acceler- ated using GPU. Our experiments demonstrate that   $\\mathrm{NCRF++}$   as an effective and efﬁcient toolkit. •  Function enriched :  $\\mathrm{NCRF++}$   extends the Viterbi algorithm ( Viterbi ,  1967 ) to enable decod- ing  $n$   best sequence labels with their probabilities. Taking NER, Chunking and POS tagging as typ- ical examples, we investigate the performance of models built in   $\\mathrm{NCRF++}$  , the inﬂuence of human- deﬁned and automatic features, the performance of  nbest  decoding and the running speed with the batch size. Detail results are shown in Section  3 . ", "page_idx": 1, "bbox": [70, 684.7472534179688, 290, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "", "page_idx": 1, "bbox": [306, 310.322998046875, 525, 473.7585754394531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "2  $\\mathbf{NCHF}++$   Architecture ", "text_level": 1, "page_idx": 1, "bbox": [306, 486, 440, 500], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "The framework of  $\\mathrm{NCRF++}$   is shown in Figure  2 .  $\\mathrm{NCRF++}$   is designed with three layers: a character sequence layer; a word sequence layer and infer- ence layer. For each input word sequence, words are represented with word embeddings. The char- acter sequence layer can be used to automatically extract word level features by encoding the char- acter sequence within the word. Arbitrary hand- crafted features such as capitalization  [Cap] , POS tag  [POS] , preﬁxes  [Pre]  and sufﬁxes [Suf]  are also supported by   $\\mathrm{NCRF++}$  . Word representations are the concatenation of word em- beddings (red circles), character sequence encod- ing hidden vector (yellow circles) and handcrafted neural features (grey circles). Then the word se- quence layer takes the word representations as in- put and extracts the sentence level features, which are fed into the inference layer to assign a label to each word. When building the network, users only need to edit the conﬁguration ﬁle to conﬁg- ure the model structure, training settings and hy- perparameters. We use layer-wised encapsulation in our implementation. Users can extend  $\\mathrm{NCRF++}$  by deﬁning their own structure in any layer and in- tegrate it into  $\\mathrm{NCRF++}$   easily. ", "page_idx": 1, "bbox": [306, 509, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "", "page_idx": 2, "bbox": [71, 63.68701934814453, 291, 144.57852172851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "2.1 Layer Units ", "text_level": 1, "page_idx": 2, "bbox": [71, 154, 153, 167], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "2.1.1 Character Sequence Layer ", "text_level": 1, "page_idx": 2, "bbox": [71, 172, 230, 184], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "The character sequence layer integrates several typical neural encoders for character sequence in- formation, such as RNN and CNN. It is easy to se- lect our existing encoder through the conﬁguration ﬁle (by setting  char seq feature  in Figure 1 ). Characters are represented by character em- beddings (green circles in Figure  2 ), which serve as the input of character sequence layer. ", "page_idx": 2, "bbox": [71, 187.41004943847656, 291, 295.3995056152344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "•  Character RNN  and its variants Gated Re- current Unit (GRU) and LSTM are supported by  $\\mathrm{NCRF++}$  . The character sequence layer uses bidirectional RNN to capture the left-to-right and right-to-left sequence information, and concate- nates the ﬁnal hidden states of two RNNs as the encoder of the input character sequence. ", "page_idx": 2, "bbox": [71, 295.4253234863281, 291, 390.2585754394531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "•  Character CNN  takes a sliding window to cap- ture local features, and then uses a  max-pooling  for aggregated encoding of the character sequence. ", "page_idx": 2, "bbox": [71, 390.2853698730469, 291, 430.9216003417969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "2.1.2 Word Sequence Layer ", "text_level": 1, "page_idx": 2, "bbox": [71, 440, 209, 451], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "Similar to the character sequence layer,   $\\mathrm{NCRF++}$  supports both RNN and CNN as the word se- quence feature extractor. The selection can be con- ﬁgurated through  word seq feature  in Fig- ure  1 . The input of the word sequence layer is a word representation, which may include word em- beddings, character sequence representations and handcrafted neural features (the combination de- pends on the conﬁguration ﬁle). The word se- quence layer can be stacked, building a deeper fea- ture extractor. ", "page_idx": 2, "bbox": [71, 454.77410888671875, 291, 603.4114990234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "•  Word RNN  together with GRU and LSTM are available in   $\\mathrm{NCRF++}$  , which are popular struc- tures in the recent literature ( Huang et al. ,  2015 ; Lample et al. ,  2016 ;  Ma and Hovy ,  2016 ;  Yang et al. ,  2017 ). Bidirectional RNNs are supported to capture the left and right contexted information of each word. The hidden vectors for both direc- tions on each word are concatenated to represent the corresponding word. ", "page_idx": 2, "bbox": [71, 603.4373168945312, 291, 725.368408203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "•  Word CNN  utilizes the same sliding window as character CNN, while a nonlinear function ( Glo- rot et al. ,  2011 ) is attached with the extracted fea- tures. Batch normalization ( Ioffe and Szegedy , 2015 ) and dropout ( Srivastava et al. ,  2014 ) are also supported to follow the features. ", "page_idx": 2, "bbox": [71, 725.394287109375, 291, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "", "page_idx": 2, "bbox": [306, 63.68701934814453, 525, 103.93148803710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "2.1.3 Inference Layer ", "text_level": 1, "page_idx": 2, "bbox": [306, 113, 414, 126], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "The inference layer takes the extracted word se- quence representations as features and assigns la- bels to the word sequence.   $\\mathrm{NCRF++}$   supports both softmax and CRF as the output layer. A linear layer ﬁrstly maps the input sequence representa- tions to label vocabulary size scores, which are used to either model the label probabilities of each word through simple softmax or calculate the label score of the whole sequence. ", "page_idx": 2, "bbox": [306, 129.51502990722656, 525, 251.05453491210938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "•  Softmax  maps the label scores into a probabil- ity space. Due to the support of parallel decod- ing, softmax is much more efﬁcient than CRF and works well on some sequence labeling tasks ( Ling et al. ,  2015 ). In the training process, various loss functions such as negative likelihood loss, cross entropy loss are supported. ", "page_idx": 2, "bbox": [306, 251.3983612060547, 525, 346.2315979003906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "•  CRF  captures label dependencies by adding transition scores between neighboring labels.  $\\mathrm{NCRF++}$   supports CRF trained with the sentence- level maximum log-likelihood loss. During the decoding process, the Viterbi algorithm is used to search the label sequence with the highest proba- bility. In addition,  $\\mathrm{NCRF++}$   extends the decoding algorithm with the support of  nbest  output. ", "page_idx": 2, "bbox": [306, 346.57537841796875, 525, 454.9576416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "2.2 User Interface ", "text_level": 1, "page_idx": 2, "bbox": [307, 466, 399, 477], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": " $\\mathrm{NCRF++}$   provides users with abundant network conﬁguration interfaces, including the network structure, input and output directory setting, train- ing settings and hyperparameters. By editing a conﬁguration ﬁle, users can build most state-of- the-art neural sequence labeling models. On the other hand, all the layers above are designed as “plug-in” modules, where user-deﬁned layer can be integrated seamlessly. ", "page_idx": 2, "bbox": [306, 483, 525, 605.026611328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "2.2.1 Conﬁguration ", "text_level": 1, "page_idx": 2, "bbox": [306, 615, 406, 626], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "•  Networks  can be conﬁgurated in the three layers as described in Section  2.1 . It con- trols the choice of neural structures in character and word levels with  char seq feature  and word seq feature , respectively. The infer- ence layer is set by  use crf . It also deﬁnes the usage of handcrafted features and their properties in  feature . ", "page_idx": 2, "bbox": [306, 630.2174682617188, 525, 738.6004638671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "•  I/O  is the input and output ﬁle directory conﬁguration. It includes  training dir, ", "page_idx": 2, "bbox": [306, 738.9442749023438, 525, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "table", "page_idx": 3, "img_path": "layout_images/P18-4013_1.jpg", "table_caption": "Table 1: Results on three benchmarks. ", "bbox": [71, 61, 290, 229], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "NER chunking | POS\nModels Fi-value | Fi-value | Acc\nNochar+WCNN+CRF 88.90 94.23 96.99\nCLSTM+WCNN+CRF_ | 90.70 94.76 97.38\nCCNN+WCNN+CRF 90.43 94.77 97.33\nNochar+WLSTM+CRF | 89.45 94.49 97.20\nCLSTM+WLSTM+CREF | 91.20 95.00 97.49\nCCNN+WLSTM+CRF_ | 91.35 95.06 97.46\nLample et al. (2016) 90.94 - 97.51\nMa and Hovy (2016) 91.21 - 97.55\nYang et al. (2017) 91.20 94.66 97.55\nPeters et al. (2017) 90.87 95.00 -\n\n", "vlm_text": "The table presents the performance of various models across three different tasks: Named Entity Recognition (NER), Chunking, and Part-of-Speech (POS) tagging. Each model is evaluated using either the F1-value (for NER and Chunking) or Accuracy (for POS) as the metric.\n\n1. **Models**:\n    - The models are combinations of different components indicated by their names:\n        - Nochar+WCNN+CRF\n        - CLSTM+WCNN+CRF\n        - CCNN+WCNN+CRF\n        - Nochar+WLSTM+CRF\n        - CLSTM+WLSTM+CRF\n        - CCNN+WLSTM+CRF\n\n2. **NER (Named Entity Recognition)**:\n    - Performance is reported as the F1-value.\n    - Best performance in NER is 91.35, achieved by the model: CCNN+WLSTM+CRF.\n\n3. **Chunking**:\n    - Performance is reported as the F1-value.\n    - Best performance in Chunking is 95.06, achieved by the model: CCNN+WLSTM+CRF.\n\n4. **POS (Part-of-Speech) tagging**:\n    - Performance is reported as Accuracy.\n    - Highest accuracy is 97.55, achieved by Ma and Hovy (2016) and Yang et al. (2017).\n\n5. **Additional references**:\n    - Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), and Peters et al. (2017) are additional models cited for comparison.\n    - Note that some values for these references are missing or not applicable (e.g., Chunking F1-value is not provided for Lample et al. and Peters et al.).\n\nOverall, the table primarily compares the efficiency of different model architectures in processing NER, Chunking, and POS tasks, highlighting significant achievers in each category."}
{"layout": 41, "type": "text", "text": "dev dir, test dir, raw dir , pretrained character or word embedding ( char emb dim or  word emb dim ), and decode ﬁle directory\n\n ( decode dir ).\n\n ", "page_idx": 3, "bbox": [71, 245.75502014160156, 290, 299.5484924316406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "• ( loss function Training includes ), optimizer ( the optimizer loss function ) 4 shufﬂe training instances  train shuffle  and average batch loss  ave batch loss . ", "page_idx": 3, "bbox": [71, 299.5592956542969, 290, 353.7454833984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "•  Hyperparameter  includes most of the param- eters in the networks and training such as learn- ing rate   $(\\mathtt{L}\\mathtt{r})$   and its decay ( lr decay ), hidden layer size of word and character ( hidden dim and  char hidden dim ),  nbest  size ( nbest ), batch size ( batch size ), dropout ( dropout ), etc. Note that the embedding size of each hand- crafted feature is conﬁgured in the networks con- ﬁguration ( feature  $=$  [POS] emb dir  $=$  None emb si  $z\\!\\in\\!=\\!10$   in Figure  1 ). ", "page_idx": 3, "bbox": [71, 353.75628662109375, 290, 489.23748779296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "2.2.2 Extension ", "text_level": 1, "page_idx": 3, "bbox": [71, 497, 151, 509], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "Users can write their own custom modules on all three layers, and user-deﬁned layers can be inte- grated into the system easily. For example, if a user wants to deﬁne a custom character sequence layer with a speciﬁc neural structure, he/she only needs to implement the part between input char- acter sequence indexes to sequence representa- tions. All the other networks structures can be used and controlled through the conﬁguration ﬁle. A  README  ﬁle is given on this. ", "page_idx": 3, "bbox": [71, 512.9099731445312, 290, 647.9984741210938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "3 Evaluation ", "text_level": 1, "page_idx": 3, "bbox": [72, 658, 146, 671], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "3.1 Settings ", "text_level": 1, "page_idx": 3, "bbox": [71, 680, 134, 692], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "To evaluate the performance of our toolkit, we conduct the experiments on several datasets. For NER task, CoNLL 2003 data ( Tjong Kim Sang ", "page_idx": 3, "bbox": [71, 696.81103515625, 290, 737.054443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "table", "page_idx": 3, "img_path": "layout_images/P18-4013_2.jpg", "table_caption": "Table 2: Results using different features. ", "bbox": [307, 63, 524, 168], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Features P R F\nBaseline WLSTM+CRF | 80.44 | 87.88 | 89.15\n+POS 90.61 | 89.28 | 89.94\nHuman Feature +Cap 90.74 | 90.43 | 90.58\n+POS+Cap | 90.92 | 90.27 | 90.59\nAuto Feature +CLSTM 91.22 | 91.17 | 91.20\n+CCNN 91.66 | 91.04 | 91.35\n\n", "vlm_text": "The table presents a comparison of different features and their impact on model performance, measured by precision (P), recall (R), and F1 score (F). Below are the details:\n\n1. **Baseline:**\n   - **Features:** WLSTM+CRF\n   - **Performance:**\n     - Precision (P): 80.44\n     - Recall (R): 87.88\n     - F1 Score (F): 89.15\n\n2. **Human Feature:**\n   - **Features:**\n     - +POS\n       - Precision (P): 90.61\n       - Recall (R): 89.28\n       - F1 Score (F): 89.94\n     - +Cap\n       - Precision (P): 90.74\n       - Recall (R): 90.43\n       - F1 Score (F): 90.58\n     - +POS+Cap\n       - Precision (P): 90.92\n       - Recall (R): 90.27\n       - F1 Score (F): 90.59\n\n3. **Auto Feature:**\n   - **Features:**\n     - +CLSTM\n       - Precision (P): 91.22\n       - Recall (R): 91.17\n       - F1 Score (F): 91.20\n     - +CCNN\n       - Precision (P): 91.66\n       - Recall (R): 91.04\n       - F1 Score (F): 91.35\n\nThe table indicates that the use of automatic features (Auto Feature) such as CLSTM and CCNN results in better precision, recall, and F1 score compared to the baseline and human-engineered features."}
{"layout": 50, "type": "text", "text": "and De Meulder ,  2003 ) with the standard split is used. For the chunking task, we perform ex- periments on CoNLL 2000 shared task ( Tjong Kim Sang and Buchholz ,  2000 ), data split is fol- lowing  Reimers and Gurevych  ( 2017 ). For POS tagging, we use the same data and split with  Ma and Hovy  ( 2016 ). We test different combinations of character representations and word sequence representations on these three benchmarks. Hy- perparameters are mostly following  Ma and Hovy ( 2016 ) and almost keep the same in all these exper- iments 5 . Standard SGD with a decaying learning rate is used as the optimizer. ", "page_idx": 3, "bbox": [307, 189.2870635986328, 525, 365.0226135253906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "3.2 Results ", "text_level": 1, "page_idx": 3, "bbox": [307, 377, 366, 389], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "Table  1  shows the results of six CRF-based mod- els with different character sequence and word sequence representations on three benchmarks. State-of-the-art results are also listed. In this table, “Nochar” suggests a model without character se- quence information. “CLSTM” and “CCNN” rep- resent models using LSTM and CNN to encode character sequence, respectively. Similarly, “WL- STM” and “WCNN” indicate that the model uses LSTM and CNN to represent word sequence, re- spectively. ", "page_idx": 3, "bbox": [307, 395.2801208496094, 525, 543.9176025390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "As shown in Table  1 , “WCNN” based mod- els consistently underperform the “WLSTM” based models, showing the advantages of LSTM on capturing global features. Character in- formation can improve model performance sig- niﬁcantly, while using LSTM or CNN give similar improvement. Most of state-of-the-art models utilize the framework of word LSTM- CRF with character LSTM or CNN features\n\n (correspond to “CLSTM  $+$  WLSTM  $+$  CRF” and\n\n “CCNN  $^+$  WLSTM  $^+$  CRF” of our models) ( Lample et al. ,  2016 ;  Ma and Hovy ,  2016 ;  Yang et al. ,  2017 ; Peters et al. ,  2017 ). Our implementations can achieve comparable results, with better NER and ", "page_idx": 3, "bbox": [307, 544.97119140625, 525, 734.255615234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "image", "page_idx": 4, "img_path": "layout_images/P18-4013_3.jpg", "img_caption": "Figure 3: Oracle performance with nbest. ", "bbox": [75, 63, 288, 250], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "1.00\n\n0.98\n\n0.96\n\nOracle scores\n\n0.92\n\n—¥— Token Accuracy\n-a- Entity Fl-value\n\n3.44\n\n5 6\nN best\n\n7 8 9 10 11\n", "vlm_text": "The image is a line graph depicting \"Oracle performance with nbest.\" It shows the relationship between \"N best\" on the x-axis and \"Oracle scores\" on the y-axis. There are two lines representing different metrics: \n\n1. The blue line (with inverted triangle markers) represents \"Token Accuracy.\" This line starts near an Oracle score of 0.98 when N is 1 and gradually increases, maintaining a high level, and appears to slightly rise as N increases from 1 to 11.\n\n2. The red line (with triangle markers) represents \"Entity F1-value.\" This line starts around an Oracle score of 0.92 at N of 1 and rises more sharply compared to the blue line as N increases, eventually approaching about 0.97 as the N value reaches 11.\n\nThe graph includes a legend in the bottom right corner, labeling the two lines as \"Token Accuracy\" and \"Entity F1-value.\" The overall trend indicates that both Oracle scores improve as the N best increases, with Token Accuracy generally being higher than Entity F1-value at corresponding points."}
{"layout": 55, "type": "text", "text": "chunking performances and slightly lower POS tagging accuracy. Note that we use almost the same hyperparameters across all the experiments to achieve the results, which demonstrates the ro- bustness of our implementation. The full experi- mental results and analysis are published in  Yang et al.  ( 2018 ). ", "page_idx": 4, "bbox": [72, 271.2550048828125, 290, 365.6955261230469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "3.3 Inﬂuence of Features ", "text_level": 1, "page_idx": 4, "bbox": [71, 377, 195, 389], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "We also investigate the inﬂuence of different fea- tures on system performance. Table  2  shows the results on the NER task. POS tag and capital in- dicator are two common features on NER tasks ( Collobert et al. ,  2011 ;  Huang et al. ,  2015 ;  Strubell et al. ,  2017 ). In our implementation, each POS tag or capital indicator feature is mapped as 10- dimension feature embeddings through randomly initialized feature lookup table   6 . The feature em- beddings are concatenated with the word embed- dings as the representation of the corresponding word. Results show that both human features [POS]  and  [Cap]  can contribute the NER sys- tem, this is consistent with previous observations ( Collobert et al. ,  2011 ;  Chiu and Nichols ,  2016 ). By utilizing LSTM or CNN to encode character sequence automatically, the system can achieve better performance on NER task. ", "page_idx": 4, "bbox": [72, 395.7420654296875, 290, 639.224609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "3.4 N best Decoding ", "text_level": 1, "page_idx": 4, "bbox": [71, 651, 173, 663], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "We investigate nbest Viterbi decoding on NER dataset through the best model  $\\scriptstyle\\mathrm{\\\"CCNN+WLSTM+CRF^{\\prime}}$  . Figure 3 shows the oracle entity F1-values and token accuracies with different  nbest  sizes. The oracle F1-value ", "page_idx": 4, "bbox": [72, 669.2711181640625, 290, 736.6135864257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "image", "page_idx": 4, "img_path": "layout_images/P18-4013_4.jpg", "img_caption": "Figure 4: Speed with batch size. ", "bbox": [310, 64, 524, 250], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "2000\n\nHR\nu\n°\nfo)\n\n1000\n\nSpeed (sent/s)\n\n500\n\nlM Decoding Speed\nMI Training Speed\n\n10 15 20 30 50 100200\nBatch Size\n\n", "vlm_text": "The image is a bar chart titled \"Speed with batch size,\" which illustrates the relationship between batch size and speed in terms of sentences per second (sent/s). The x-axis represents the batch size, ranging from 1 to 200, while the y-axis represents the speed in sent/s, ranging from 0 to 2000.\n\nThere are two types of speed data represented in the chart:\n\n1. **Decoding Speed** - Shown in blue bars. It increases with the batch size, displaying a consistently higher speed compared to training speed for all batch sizes.\n\n2. **Training Speed** - Shown in red bars. It also increases with batch size but at a slower rate compared to decoding speed.\n\nAs the batch size increases, both decoding and training speeds increase, but decoding speed is significantly faster than training speed at all batch sizes, as indicated by the blue bars being taller than the red bars for each batch size."}
{"layout": 61, "type": "text", "text": "rises signiﬁcantly with the increasement of  nbest size, reaching  $97.47\\%$   at    $n=10$   from the baseline of   $91.35\\%$  . The token level accuracy increases from  $98.00\\%$   to   $99.39\\%$   in    $I O$  -best . Results show that the  nbest  outputs include the gold entities and labels in a large coverage, which greatly enlarges the performance of successor tasks. ", "page_idx": 4, "bbox": [307, 269.02496337890625, 526, 363.4654846191406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "3.5 Speed with Batch Size ", "text_level": 1, "page_idx": 4, "bbox": [306, 373, 435, 385], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "As  $\\mathrm{NCRF++}$   is implemented on batched calcula- tion, it can be greatly accelerated through paral- lel computing through GPU. We test the system speeds on both training and decoding process on NER dataset using a Nvidia GTX 1080 GPU. As shown in Figure  4 , both the training and the decod- ing speed can be signiﬁcantly accelerated through a large batch size. The decoding speed reaches sat- uration at batch size 100, while the training speed keeps growing. The decoding speed and training speed of   $\\mathrm{NCRF++}$   are over 2000 sentences/second and 1000 sentences/second, respectively, demon- strating the efﬁciency of our implementation. ", "page_idx": 4, "bbox": [307, 390, 526, 565.9185180664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 4, "bbox": [306, 575, 383, 589], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "We presented   $\\mathrm{NCRF++}$  , an open-source neural sequence labeling toolkit, which has a CRF ar- chitecture with conﬁgurable neural representation layers. Users can design custom neural models through the conﬁguration ﬁle.   $\\mathrm{NCRF++}$   supports ﬂexible feature utilization, including handcrafted features and automatically extracted features. It can also generate  nbest  label sequences rather than the best one. We conduct a series of experiments and the results show models built on   $\\mathrm{NCRF++}$  can achieve state-of-the-art results with an efﬁ- cient running speed. ", "page_idx": 4, "bbox": [307, 597, 526, 759.6384887695312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "References ", "text_level": 1, "page_idx": 5, "bbox": [71, 64, 128, 75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "Jason Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM- CNNs . Transactions of the Association for Computational Linguistics 4:357–370. https://transacl.org/ojs/index.php/tacl/article/view/792 Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12(Aug):2493–2537. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectiﬁer neural networks. In  In- ternational Conference on Artiﬁcial Intelligence and Statistics . pages 315–323. Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735–1780. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi- rectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv:1508.01991  . Sergey Ioffe and Christian Szegedy. 2015. Batch nor- malization: Accelerating deep network training by reducing internal covariate shift. In  International Conference on Machine Learning . pages 448–456. S Sathiya Keerthi and Sellamanickam Sundararajan. 2007. Crf versus svm-struct for sequence labeling. Yahoo Research Technical Report  . John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random ﬁelds: Prob- abilistic models for segmenting and labeling se- quence data. In  International Conference on Ma- chine Learning . volume 1, pages 282–289. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In  NAACL-HLT . pages 260–270. Yann LeCun, Bernhard Boser, John S Denker, Don- nie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation ap- plied to handwritten zip code recognition.  Neural computation  1(4):541–551. Wang Ling, Chris Dyer, Alan W Black, Isabel Tran- coso, Ramon Fermandez, Silvio Amir, Luis Marujo, and Tiago Luis. 2015. Finding function in form: Compositional character models for open vocabu- lary word representation. In  EMNLP . pages 1520– 1530. Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan Gui, Jian Peng, and Jiawei Han. 2018. Empower sequence labeling with task-aware neural language model. In  AAAI . ", "page_idx": 5, "bbox": [71, 83.50958251953125, 295, 765.7652587890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "Xuezhe Ma and Eduard Hovy. 2016. End-to-end se- quence labeling via Bi-directional LSTM-CNNs- CRF. In  ACL . volume 1, pages 1064–1074. Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random ﬁelds (crfs) . Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In  ACL . volume 1, pages 1756–1765. Xuan-Hieu Phan, Le-Minh Nguyen, and Cam-Tu Nguyen. 2004. Flexcrfs: Flexible conditional ran- dom ﬁelds. Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL . pages 147–155. Nils Reimers and Iryna Gurevych. 2017. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. In EMNLP . pages 338–348. Fei Sha and Fernando Pereira. 2003. Shallow pars- ing with conditional random ﬁelds. In  NAACL-HLT . pages 134–141. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overﬁtting.  Journal of Machine Learning Re- search  15(1):1929–1958. Emma Strubell, Patrick Verga, David Belanger, and Andrew McCallum. 2017. Fast and accurate entity recognition with iterated dilated convolutions. In EMNLP . pages 2670–2680. Erik F Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunk- ing. In  Proceedings of the 2nd workshop on Learn- ing language in logic and the 4th conference on Computational natural language learning-Volume 7 . pages 127–132. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In HLT-NAACL . pages 142–147. Andrew Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding al- gorithm.  IEEE transactions on Information Theory 13(2):260–269. Jie Yang, Shuailong Liang, and Yue Zhang. 2018. De- sign challenges and misconceptions in neural se- quence labeling. In  COLING . Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen. 2017. Transfer learning for sequence tag- ging with hierarchical recurrent networks. In  Inter- national Conference on Learning Representations . ", "page_idx": 5, "bbox": [307, 64.56134033203125, 525, 752.0331420898438], "page_size": [595.2760009765625, 841.8900146484375]}
