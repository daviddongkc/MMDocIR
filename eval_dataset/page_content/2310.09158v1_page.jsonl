{"page": 0, "image_path": "doc_images/2310.09158v1_0.jpg", "ocr_text": "023\n\n2\n\narXiv:2310.09158v1 [cs.AI] 13 Oct\n\nLearning To Teach Large Language Models Logical Reasoning\n\nMeiqi Chen Yubo Ma Kaitao Song\nPeking University S-Lab Microsoft Research Asia\nmeiqichen@stu.pku.edu.cn Nanyang Technological University\nYixin Cao Yan Zhang Dongsheng Li\nSingapore Management University Peking University Microsoft Research Asia\n\nABSTRACT\n\nLarge language models (LLMs) have gained enormous attention\nfrom both academia and industry, due to their exceptional ability in\nlanguage generation and extremely powerful generalization. How-\never, current LLMs still output unreliable content in practical reason-\ning tasks due to their inherent issues (e.g., hallucination). To better\ndisentangle this problem, in this paper, we conduct an in-depth\ninvestigation to systematically explore the capability of LLMs in\nlogical reasoning. More in detail, we first investigate the deficiency\nof LLMs in logical reasoning on different tasks, including event re-\nlation extraction and deductive reasoning. Our study demonstrates\nthat LLMs are not good reasoners in solving tasks with rigorous\nreasoning and will produce counterfactual answers, which require\nus to iteratively refine. Therefore, we comprehensively explore\ndifferent strategies to endow LLMs with logical reasoning ability,\nand thus enable them to generate more logically consistent an-\nswers across different scenarios. Based on our approach, we also\ncontribute a synthesized dataset (LLM-LR) involving multi-hop\nreasoning for evaluation and pre-training. Extensive quantitative\nand qualitative analyses on different tasks also validate the effec-\ntiveness and necessity of teaching LLMs with logic and provide\ninsights for solving practical tasks with LLMs in future work. Codes\nwill be available at https://github.com/chenmeiqii/Teach-LLM-LR.\n\nCCS CONCEPTS\n\n+ Computing methodologies — Knowledge representation\nand reasoning.\n\nKEYWORDS\n\nLarge Language Models, Event Relation Extraction, Logical Reason-\ning\n\n1 INTRODUCTION\n\nRecently, Large Language Models (LLMs) have made incredible\nprogress in many different downstream tasks, such as GPT-3 [3],\nChatGPT [32], and LLaMA [39]. These models are typically trained\non a combination of filtered web data and curated high-quality\ncorpora (e.g., social media conversations, books, or technical pub-\nlications) [34]. Studies have indicated that the emergent abilities\nof LLMs can exhibit promising reasoning capabilities [45] and the\ncuration process is necessary to produce their zero-shot generaliza-\ntion abilities [34].\n\nDespite these notable achievements, current LLMs still have\nsome issues in producing high-quality content with fluency and\nreliability. A good content generator should produce logically con-\nsistent answers that are reasonable for given or prior constraints.\n\n3S User\n\nIdentify the relations between events FIRE and collapsed ...\n\nText:\n\nA large FIRE broke out at the Waitrose supermarket in\nWellington’s High Street, half of the roof at the entrance of\nthe store collapsed during the blaze.\n\nChatGPT\nCoreference Relation: NO_COREFERENCE ---\n\nTemporal Relation ---1@ e Canes\n— Causal Relation: CAUSE “Jo je © Detection\n\nSubevent Relation: NO_LSUBEVENT\n©: conflicts ©: No Conflicts\n\nNumber of Conflicts (@)\n\nLogical Inconsistency (LI) = Number of Combinations (@+O)\n\n1\n=> = 16.7%\n6\n\nAnalysis\n\n'S is logically inconsistent.\n\nLogic:\n‘> © If event A causes event B, then event A must happen\neither before or overlap with event B.\n* If event A and B happens simultaneously, they won't have\na causal relation.\n\nFigure 1: An example of LLM in generating logically inconsis-\ntent answers. We let LLM (e.g., ChatGPT) answer the relations\nbetween events “FIRE” and “collapsed” from the given pas-\nsage. We can find that LLM predicts an incorrect answer (i.e.,\nSIMULTANEOUS) because it ignores some prior logic in this\nscenario, leading to logical inconsistency.\n\nHowever, LLMs sometimes output counterfactuals when dealing\nwith practical tasks that require rigorous logical reasoning. As\nshowcased in Figure 1, ChatGPT predicts the temporal and causal\nrelations between events “FIRE” and “collapsed” being “simulta-\nneous” and “cause”. According to the prior logical constraints, we\ncould readily claim the predictions are not fully correct even before\nreading the context, because \"simultaneous\" and \"cause\" conflict\nwith each other in terms of semantics. Some works [26, 33, 48]\nattribute these phenomena to their inherent deficiencies (e.g., hallu-\ncination, unfaithfulness), however, how to disentangle and improve\nthe capability of LLMs in these tasks is still an open problem.\n\nTo deeply understand the deficiencies of LLMs in logical rea-\nsoning and explore the corresponding solutions, in this paper, we\nconduct an in-depth investigation of LLMs in solving reasoning\ntasks from multiple dimensions. We first evaluate the capacity of\nLLMs in two practical scenarios including event relation extraction\n", "vlm_text": "Learning To Teach Large Language Models Logical Reasoning \nKaitao Song \nPeking University S-Lab meiqichen@stu.pku.edu.cn Nanyang Technological University \nDongsheng Li Microsoft Research Asia \nYan Zhang Peking University \nYixin Cao Singapore Management University \nABSTRACT \nUser \nLarge language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. How- ever, current LLMs still output unreliable content in practical reason- ing tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event re- lation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counter factual answers, which require us to iterative ly refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent an- swers across different scenarios. Based on our approach, we also contribute a synthesized dataset ( LLM-LR ) involving multi-hop reasoning for evaluation and pre-training. Extensive quantitative and qualitative analyses on different tasks also validate the effec- tiveness and necessity of teaching LLMs with logic and provide insights for solving practical tasks with LLMs in future work. Codes will be available at https://github.com/chenmeiqii/Teach-LLM-LR. \nText: A large  FIRE  broke out at the Waitrose supermarket in  Wellington’s High Street, half of the roof at the entrance of  the store  collapsed  during the blaze.  \nThe image appears to explain a method for detecting logical inconsistencies in relationships determined by ChatGPT. It involves the following:\n\n1. **Coreference Relation**: NO_COREFERENCE\n2. **Temporal Relation**: SIMULTANEOUS (with a conflict indicated)\n3. **Causal Relation**: CAUSE (no conflict)\n4. **Subevent Relation**: NO_SUBEVENT (no conflict)\n\nThe process uses \"Pairwise Conflicts Detection\" with indications for conflicts (red circles with exclamation marks) and no conflicts (green check marks).\n\nThe **Logical Inconsistency (LI)** is calculated as:\n- LI = (Number of Conflicts) / (Number of Combinations)\n- Example given: 1/6, which equals approximately 16.7%.\nThis answer  SIMULTANEOUS  is  logically inconsistent . \n\n•  If event A causes event B, then event A must happen either before or overlap with event B. •  If event A and B happens simultaneously, they won’t have a causal relation. \nFigure 1: An example of LLM in generating logically inconsis- tent answers. We let LLM (e.g., ChatGPT) answer the relations between events  “FIRE”  and  “collapsed”  from the given pas- sage. We can find that LLM predicts an incorrect answer (i.e., SIMULTANEOUS) because it ignores some prior logic in this scenario, leading to logical inconsistency. \nCCS CONCEPTS \n•  Computing methodologies  $\\rightarrow$  Knowledge representation and reasoning . \nKEYWORDS \nLarge Language Models, Event Relation Extraction, Logical Reason- ing \nHowever, LLMs sometimes output counter factual s when dealing with practical tasks that require rigorous logical reasoning. As showcased in Figure 1, ChatGPT predicts the temporal and causal relations between events    $\"F I R E\"$   and  “collapsed”  being  “simulta- neous”  and  “cause” . According to the prior logical constraints, we could readily claim the predictions are not fully correct even before reading the context, because  \"simultaneous\"  and  \"cause\"  conflict with each other in terms of semantics. Some works [ 26 ,  33 ,  48 ] attribute these phenomena to their inherent deficiencies (e.g., hallu- cination, unfaithfulness), however, how to disentangle and improve the capability of LLMs in these tasks is still an open problem. \n1 INTRODUCTION \nRecently, Large Language Models (LLMs) have made incredible progress in many different downstream tasks, such as GPT-3 [ 3 ], ChatGPT [ 32 ], and LLaMA [ 39 ]. These models are typically trained on a combination of filtered web data and curated high-quality corpora (e.g., social media conversations, books, or technical pub- lications) [ 34 ]. Studies have indicated that the emergent abilities of LLMs can exhibit promising reasoning capabilities [45] and the curation process is necessary to produce their zero-shot generaliza- tion abilities [34]. \nTo deeply understand the deficiencies of LLMs in logical rea- soning and explore the corresponding solutions, in this paper, we conduct an in-depth investigation of LLMs in solving reasoning tasks from multiple dimensions. We first evaluate the capacity of LLMs in two practical scenarios including event relation extraction \nDespite these notable achievements, current LLMs still have some issues in producing high-quality content with fluency and reliability. A good content generator should produce logically con- sistent answers that are reasonable for given or prior constraints. "}
{"page": 1, "image_path": "doc_images/2310.09158v1_1.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY\n\nand deductive reasoning tasks, both of which demand rigorous\nreasoning ability to infer [38, 42]. Our experimental results show\nthat: 1) Even the cutting-edge LLMs still generate large amounts\nof inconsistent answers, e.g., over 60% of the answers from Chat-\nGPT on the MAVEN-ERE [42] dataset are logically inconsistent\nas shown in Figure 2; 2) Chain-of-thought (CoT) prompting [47],\nlike “Let’s think step by step\" could stimulate the reasoning abili-\nties for LLMs. However, some inherent issues (e.g., hallucination,\nunfaithfulness) in the LLM will cause such generated rationale to\nbe unreliable or inconsistent; 3) Providing relevant logic to LLMs\nimproves performance, but injecting irrelevant logic introduces\nfluctuations in results. Therefore, how to obtain the relevant logic\nand inject its information into LLMs is a non-trivial problem, de-\nserving further exploration; 4) To verify the capacity of LLM for\nmore complex reasoning, we contribute a synthesized dataset (i.e.,\nLLM-LR) for evaluation, which involves multiple hops of logical\nreasoning. LLM-LR is automatically constructed by applying logic\nprogramming [11, 22] on our collected logical constraints, which\ncould provide logical reasoning instances with any number of hops.\nResults show that as the number of logical hops increases (2~10\nhops), LLMs struggle to output correct answers, and the propor-\ntion of logically inconsistent answers steadily rises. This indicates\nthat LLMs will perform worse when the reasoning becomes more\nabstract and complicated. Therefore, how to alleviate the afore-\nmentioned issues and enable LLMs with a more powerful ability of\nlogical reasoning is the critical point of our paper.\n\nBased on these findings, we put forward a series of solutions\nto teach LLMs to generate answers with better logical consistency.\nHere, we divide the procedure for teaching LLMs logical reasoning\ninto three different kinds of approaches according to the ways of\nlogic acquisition: 1) Generative-based approach, which encourages\nLLMs to generate reasoning rationale themselves, inspired by CoT\nprompting. In this paradigm, we find that incorporating logical\nconstraints into LLM instruction will bring substantial improve-\nments, but the uncertainty of the generated rationales may also\nbring some biases, leading to an incorrect subsequent answer; 2)\nRetrieval-based approach, which provides our manually designed\nlogical constraints, then retrieves relevant contents and adds them\nto the LLM instruction. This kind of approach ensures the correct-\nness of logical constraints and significantly improves performance,\nbut requires some hand-crafted engineering; 3) Pretraining-based\napproach, which uses our curated dataset LLM-LR introduced be-\nfore to train LLMs to perform complex logical reasoning. The pre-\ntraining dataset consists of 6776 instances containing 2~5 hops of\nlogical reasoning. This strategy encodes logic in model parameters\ninherently, while also requiring additional training time. Therefore,\nhow to choose the most suitable strategy can be a trade-off based\non the practical scenario.\n\nFurthermore, based on the above framework, we also conduct\nextensive quantitative and qualitative analyses on different tasks to\nvalidate the effectiveness of teaching LLMs with logic and provide\ninsights for future work: 1) We investigate whether to add logical\nconstraints before obtaining results or later, and find that directly\nconveying constraints to LLMs is more effective than adding post-\nprocessing operations based on the results; 2) Compared with the\nsetting that uses more demonstrations, incorporating logical con-\nstraints into prompts can achieve better performance with fewer\n\nChen, et al.\n\ndemonstrations. This phenomenon further indicates that it is im-\nportant to teach LLMs to balance demonstrations and logical con-\nstraints; 3) Benefits from LLMs’ powerful interactive ability, we can\nfurther improve the performance through multi-turn conversation\nenhanced by iterative retrievals. However, when there are too many\niterations, LLMs may have the problem of overthinking — more\nuseless and redundant information interferes with their predic-\ntions; 4) When trained on LLM-LR, LLMs such as LlaMA2-13B [39]\ncan achieve better performance, even surpassing that of greater\nLLMs (e.g., ChatGPT, 175B), which validates the effectiveness of\nour curated dataset.\n\nOverall, the contributions of our paper can be summarized as\nfollows:\n\ne We provide an in-depth investigation of the logical inconsistency\nproblems of current LLMs in solving practical tasks, and indicate\nthe deficiency of LLMs in utilizing logic.\n\ne To enhance the reliability of the content generated by LLMs, we\npropose several solutions to incorporate relevant logic. Based\non our approach, we construct a synthesized dataset (LLM—LR)\ninvolving multi-hop reasoning. By leveraging the LLM-LR, we\nendow specialized LLMs with logical reasoning ability, which\nenhances LLMs to generate more logically consistent answers.\n\ne Experimental results on different tasks with quantitative and\nqualitative analyses verify the importance of our investigation\nin empowering LLMs with logical reasoning.\n\n2 PRELIMINARIES\n\nIn this section, we first introduce two tasks that this paper mainly\nexplores.\n\n2.1 Event Relation Extraction\n\nEvent relation extraction (ERE) [21, 42] aims to identify relations\n(ie., Coreference, Temporal, Causal, and Subevent) between two\nevents in the text. Traditionally, it can be formulated as a multi-\nlabel classification problem, determining one label for each relation\ntype. Compared with other common tasks, ERE tasks should take\nmore considerations about the logical constraints between event\nrelations (e.g., the constraints in Figure 1), and guarantee the predic-\ntions should conform to those constraints to avoid counterfactuals.\nTherefore, we need to rigorously consider the logical constraints\nbetween each event pair during the prediction. To better evaluate\nthe capability of LLMs on the ERE task, we formulate the logical\nconsistency for evaluation.\n\nLogical consistency plays a crucial role in understanding the re-\nlations between events. To assess the logical consistency, we collect\na comprehensive set including 11 logical constraints for all relations\nbetween two events, as shown in Table 4. Based on these logical\nconstraints, we introduce a logical inconsistency metric (ie., LI) to\nmeasure LLMs’ ability on ERE tasks. Specifically, for the answers of\nLLMs, logical inconsistency is calculated as the ratio of the number\nof conflicts (i.e., the answers that conflict with the given logical\nconstraints) to the total number of combinations (i.e., all combina-\ntions of each two relations). To better illustrate the computation of\nlogical inconsistency, here we introduce an example (as shown in\nFigure 1): if an LLM outputs the relations between two events as\n“NO_COREFERENCE, SIMULTANEOUS, CAUSE, NO_SUBEVENT”.\n\n", "vlm_text": "and deductive reasoning tasks, both of which demand rigorous reasoning ability to infer [ 38 ,  42 ]. Our experimental results show that:  1)  Even the cutting-edge LLMs still generate large amounts of inconsistent answers, e.g., over  $60\\%$   of the answers from Chat- GPT on the MAVEN-ERE [ 42 ] dataset are logically inconsistent as shown in Figure 2;  2)  Chain-of-thought (CoT) prompting [ 47 ], like “ Let’s think step by step \" could stimulate the reasoning abili- ties for LLMs. However, some inherent issues (e.g., hallucination, unfaithfulness) in the LLM will cause such generated rationale to be unreliable or inconsistent;  3)  Providing relevant logic to LLMs improves performance, but injecting irrelevant logic introduces fluctuations in results. Therefore, how to obtain the relevant logic and inject its information into LLMs is a non-trivial problem, de- serving further exploration;  4)  To verify the capacity of LLM for more complex reasoning, we contribute a synthesized dataset (i.e., LLM-LR ) for evaluation, which involves multiple hops of logical reasoning.  LLM-LR  is automatically constructed by applying logic programming [ 11 ,  22 ] on our collected logical constraints, which could provide logical reasoning instances with any number of hops. Results show that as the number of logical hops increases   $(2{\\sim}10$  hops), LLMs struggle to output correct answers, and the propor- tion of logically inconsistent answers steadily rises. This indicates that LLMs will perform worse when the reasoning becomes more abstract and complicated. Therefore, how to alleviate the afore- mentioned issues and enable LLMs with a more powerful ability of logical reasoning is the critical point of our paper. \nBased on these findings, we put forward a series of solutions to teach LLMs to generate answers with better logical consistency. Here, we divide the procedure for teaching LLMs logical reasoning into three different kinds of approaches according to the ways of logic acquisition: 1)  Generative-based approach , which encourages LLMs to generate reasoning rationale themselves, inspired by CoT prompting. In this paradigm, we find that incorporating logical constraints into LLM instruction will bring substantial improve- ments, but the uncertainty of the generated rationales may also bring some biases, leading to an incorrect subsequent answer; 2) Retrieval-based approach , which provides our manually designed logical constraints, then retrieves relevant contents and adds them to the LLM instruction. This kind of approach ensures the correct- ness of logical constraints and significantly improves performance, but requires some hand-crafted engineering; 3)  Pre training-based approach , which uses our curated dataset  LLM-LR  introduced be- fore to train LLMs to perform complex logical reasoning. The pre- training dataset consists of 6776 instances containing  $2{\\sim}5$   hops of logical reasoning. This strategy encodes logic in model parameters inherently, while also requiring additional training time. Therefore, how to choose the most suitable strategy can be a trade-off based on the practical scenario. \nFurthermore, based on the above framework, we also conduct extensive quantitative and qualitative analyses on different tasks to validate the effectiveness of teaching LLMs with logic and provide insights for future work:  1)  We investigate whether to add logical constraints before obtaining results or later, and find that directly conveying constraints to LLMs is more effective than adding post- processing operations based on the results;  2)  Compared with the setting that uses more demonstrations, incorporating logical con- straints into prompts can achieve better performance with fewer demonstrations. This phenomenon further indicates that it is im- portant to teach LLMs to balance demonstrations and logical con- straints;  3)  Benefits from LLMs’ powerful interactive ability, we can further improve the performance through multi-turn conversation enhanced by iterative retrievals. However, when there are too many iterations, LLMs may have the problem of over thinking — more useless and redundant information interferes with their predic- tions;  4)  When trained on  LLM-LR , LLMs such as LlaMA2-13B [ 39 ] can achieve better performance, even surpassing that of greater LLMs (e.g., ChatGPT, 175B), which validates the effectiveness of our curated dataset. \n\nOverall, the contributions of our paper can be summarized as follows: \n We provide an in-depth investigation of the logical inconsistency\n\n • problems of current LLMs in solving practical tasks, and indicate the deficiency of LLMs in utilizing logic.  To enhance the reliability of the content generated by LLMs, we\n\n • propose several solutions to incorporate relevant logic. Based on our approach, we construct a synthesized dataset ( LLM-LR ) involving multi-hop reasoning. By leveraging the  LLM-LR , we endow specialized LLMs with logical reasoning ability, which enhances LLMs to generate more logically consistent answers.  Experimental results on different tasks with quantitative and\n\n • qualitative analyses verify the importance of our investigation in empowering LLMs with logical reasoning. \n2 PRELIMINARIES \nIn this section, we first introduce two tasks that this paper mainly explores. \n2.1 Event Relation Extraction \nEvent relation extraction (ERE) [ 21 ,  42 ] aims to identify relations (i.e., Co reference, Temporal, Causal, and Subevent) between two events in the text. Traditionally, it can be formulated as a multi- label classification problem, determining one label for each relation type. Compared with other common tasks, ERE tasks should take more considerations about the logical constraints between event relations (e.g., the constraints in Figure 1), and guarantee the predic- tions should conform to those constraints to avoid counter factual s. Therefore, we need to rigorously consider the logical constraints between each event pair during the prediction. To better evaluate the capability of LLMs on the ERE task, we formulate the logical consistency for evaluation. \nLogical consistency plays a crucial role in understanding the re- lations between events. To assess the logical consistency, we collect a comprehensive set including 11 logical constraints for all relations between two events, as shown in Table 4. Based on these logical constraints, we introduce a logical inconsistency metric (i.e., LI) to measure LLMs’ ability on ERE tasks. Specifically, for the answers of LLMs, logical inconsistency is calculated as the ratio of the number of conflicts (i.e., the answers that conflict with the given logical constraints) to the total number of combinations (i.e., all combina- tions of each two relations). To better illustrate the computation of logical inconsistency, here we introduce an example (as shown in Figure 1): if an LLM outputs the relations between two events as “NO CO REFERENCE, SIMULTANEOUS, CAUSE, NO SUB EVENT”. "}
{"page": 2, "image_path": "doc_images/2310.09158v1_2.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning\n\nMicro-F1  —e— Inconsistent Answers Micro-F1 —*— Inconsistent Answers\n\n8\n\neee\n& 8\nInconsistent Answers (%)\n\nInconsistent Answers (%)\n\n° -0\nVanilla ChatGPT + Irrelevant logic + Relevant logic\nProofWriter\n\no- -0\nVanilla ChatGPT + Irrelevant logic + Relevant logic\nMAVEN-ERE\n\nFigure 2: Performance of ChatGPT in the pilot study.\n\nAmong these, \"SIMULTANEOUS\" and \"CAUSE\" are identified as\nconflicting with each other based on the logical constraints we\nhave defined, resulting in a single conflict. Now, regarding the total\nnumber of combinations: for each pair of events, we have 4 types of\nrelations to determine. The total combinations between these rela-\ntions are calculated using the combinatorial formula: 4*(4—1)/2 = 6.\nSo, there are 6 possible combinations between the relations for two\nevents. Hence, the logical inconsistency in this example is com-\nputed as LI = 1/6 (or approximately 16.7%). Obviously, given the\nlogical constraints, an algorithm can be designed to automatically\ndetect conflicts and calculate the value of logical inconsistency.\nOverall, intuitively, the smaller the value of logical inconsistency\nis, the more self-consistent and reasonable answer that LLM can\nproduce. More descriptions about this task are in Appendix A.\n\n2.2 Deductive Reasoning\n\nDeductive reasoning typically begins with known facts and rules,\nthen iteratively makes new inferences until the desired statement\ncan be either confirmed or refuted [35]. To ensure the accuracy of\nthese inferences, each step in deductive reasoning must adhere to\nthe known logical constraints (rules). More specifically, the logical\nconstraints in deductive reasoning are usually specific to individ-\nual cases rather than being universally applicable like that in the\nERE task. Consequently, when engaging in deductive reasoning,\nit is essential to assess and apply logical constraints based on the\ndistinct circumstances and known facts of each example to arrive\nat accurate conclusions. For the calculation of logical inconsistency\nof deductive reasoning, we need to manually count the number of\nreasoning processes generated by LLMs that are inconsistent with\nknown facts or rules, and then calculate the proportion.\n\n3 UNVEILING LLMS IN LOGICAL REASONING\n\nIn this section, we conduct a pilot study to investigate how current\nLLMs exhibit in reasoning tasks and how logic benefits LLMs.\n\n3.1 How Is LLM Performing Practical\nReasoning Tasks?\n\n3.1.1. Data Source. We conduct a manual evaluation on MAVEN-\nERE [42] and Proof Writer [38]. MAVEN-ERE is a unified large-scale\ndataset for the ERE task, which needs to identify four types of\nrelations. Proof Writer is a commonly used dataset for deductive\nlogical reasoning, where each example is a pair of (problem, goal)\nand the label is selected from {Proved, Disproved, Unknown}. To\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nCE2 CE2\nVanilla ChatGPT Vanilla ChatGPT\n\ncE3 - + Irrelevant Logic CE3 + Irrelevant Logic\n+ Relevant Logic + +Relevant Logic\n60 7080\nno? », 5060\nio”? ~The\nCE1 * , (CE1\nhn\nFE1 FE1\nFE2 FE2\nMAVEN-ERE ProofWriter\n\nFigure 3: Error analysis of ChatGPT in the pilot study by\nhuman evaluation. CE and FE denote incorrectness and un-\nfaithfulness errors, respectively.\n\nemploy our investigation, we randomly choose 100 samples (50\nfrom MAVEN-ERE and 50 from Proof Writer).\n\n3.1.2 Experimental Setup. Our experiments are conducted as a\nmulti-turn conversation for zero-shot reasoning, to leverage LLM’s\ninteraction ability. Given a task input (X), we also write a prompt\n(T) describing the task, and let LLM generate output (Y) by an-\nswering the given query. We also add “Let’s think step by step”\nbefore each answer for prediction generation, which is a simple but\neffective trick to improve zero-shot reasoning for LLMs [19]. We\nadopt ChatGPT as the backbone and manually check its generated\nrationales under the following three settings:\n\ne Vanilla LLM (i.e., ChatGPT) without any additional information;\n\ne LLM (ie., ChatGPT) plus the most relevant (i.e., ground truth)\nlogic;\n\ne LLM (ie., ChatGPT) plus irrelevant logical constraints.\n\nThe prompt examples can be found in Figure 10~13.\n\n3.1.3. Analysis. As shown in Figure 2, we visualize the micro-F1\nand the proportion of logically inconsistent answers generated\nby ChatGPT. We find that no matter whether on MAVEN-ERE or\nProof Writer, Vanilla ChatGPT always achieves a bad result with\nlow micro-F1 scores and high inconsistency values (e.g., 15% micro-\nF1 and 63% inconsistent answers on MAVEN-ERE), which indicates\nthe deficiency of LLM in solving complex reasoning tasks. To inves-\ntigate this issue in depth, we conduct analyses from the following\ntwo aspects.\n\nWhat Is The Relation Between Logical Consistency And\nModel Performance? From Figure 2, we find that: 1) The model\ndirectly receives significant improvements on both MAVEN-ERE\nand Proof Writer when adding relevant logic; 2) When adding some\nirrelevant logic, the results show some fluctuations (exaltation in\nMAVEN-ERE and degeneration in Proof Writer). That means di-\nrectly adding logic without any constraints will bring some uncer-\ntainty; 3) Typically, a higher logical inconsistency corresponds to a\npoorer micro-F1, however, rectifying logical inconsistency does not\nnecessarily lead to the same degree of increase in micro-F1. Gen-\nerally, an intuitive observation is that incorporating relevant logic\ninto the LLM instruction will be very helpful in solving reasoning\n", "vlm_text": "The image contains two bar graphs with accompanying line plots showing the performance of ChatGPT in a pilot study across two different datasets: MAVEN-ERE and ProofWriter.\n\n1. MAVEN-ERE (left graph):\n   - The bar graph represents the Micro-F1 score in percentage (shown in purple).\n   - The line plot represents the percentage of inconsistent answers (shown with blue markers and line).\n   - Three conditions are compared: Vanilla ChatGPT, ChatGPT with Irrelevant Logic, and ChatGPT with Relevant Logic.\n     - For Vanilla ChatGPT, the Micro-F1 score is low, and the percentage of inconsistent answers is high.\n     - When irrelevant logic is applied, the Micro-F1 score remains low, but the inconsistent answers percentage decreases significantly.\n     - With relevant logic, the Micro-F1 score increases significantly, and the percentage of inconsistent answers remains low.\n\n2. ProofWriter (right graph):\n   - Similar to the MAVEN-ERE graph, the bar represents the Micro-F1 score and the line shows the inconsistent answers percentage.\n   - Conditions: Vanilla ChatGPT, ChatGPT with Irrelevant Logic, and ChatGPT with Relevant Logic.\n     - Vanilla ChatGPT shows a low Micro-F1 score and a moderate percentage of inconsistent answers.\n     - With irrelevant logic, the Micro-F1 score increases significantly, and inconsistent answers decrease.\n     - With relevant logic, the Micro-F1 score remains high, but the percentage of inconsistent answers increases slightly from the irrelevant logic condition.\n\nOverall, both graphs illustrate the impact of using relevant and irrelevant logic on the performance of ChatGPT, highlighting improvements in Micro-F1 scores and variations in inconsistent answer percentages.\nAmong these, \"SIMULTANEOUS\" and \"CAUSE\" are identified as conflicting with each other based on the logical constraints we have defined, resulting in a single conflict. Now, regarding the total number of combinations: for each pair of events, we have 4 types of relations to determine. The total combinations between these rela- tions are calculated using the combinatorial formula:  $4{*}(4{-}1)/2=6$  . So, there are 6 possible combinations between the relations for two events. Hence, the logical inconsistency in this example is com- puted as  $\\mathrm{{{LI}}}=1/6$   (or approximately   $16.7\\%$  ). Obviously, given the logical constraints, an algorithm can be designed to automatically detect conflicts and calculate the value of logical inconsistency. \nOverall, intuitively, the smaller the value of logical inconsistency is, the more self-consistent and reasonable answer that LLM can produce. More descriptions about this task are in Appendix A. \n2.2 Deductive Reasoning \nDeductive reasoning typically begins with known facts and rules, then iterative ly makes new inferences until the desired statement can be either confirmed or refuted [ 35 ]. To ensure the accuracy of these inferences, each step in deductive reasoning must adhere to the known logical constraints (rules). More specifically, the logical constraints in deductive reasoning are usually specific to individ- ual cases rather than being universally applicable like that in the ERE task. Consequently, when engaging in deductive reasoning, it is essential to assess and apply logical constraints based on the distinct circumstances and known facts of each example to arrive at accurate conclusions. For the calculation of logical inconsistency of deductive reasoning, we need to manually count the number of reasoning processes generated by LLMs that are inconsistent with known facts or rules, and then calculate the proportion. \n3 UNVEILING LLMS IN LOGICAL REASONING In this section, we conduct a pilot study to investigate how current LLMs exhibit in reasoning tasks and how logic benefits LLMs. \n3.1 How Is LLM Performing Practical Reasoning Tasks? \n3.1.1 Data Source .  We conduct a manual evaluation on MAVEN- ERE [ 42 ] and Proof Writer [ 38 ]. MAVEN-ERE is a unified large-scale dataset for the ERE task, which needs to identify four types of relations. Proof Writer is a commonly used dataset for deductive logical reasoning, where each example is a pair of (problem, goal) and the label is selected from {Proved, Disproved, Unknown}. To \nThe image contains two radar charts comparing three variations: \"Vanilla ChatGPT,\" \"+ Irrelevant Logic,\" and \"+ Relevant Logic.\" Each chart has labeled axes (CE1, CE2, CE3, FE1, FE2, FE3) representing different criteria, with varying scores indicated in concentric circles. The left chart is for \"MAVEN-ERE\" and the right is for \"ProofWriter.\" The charts visually compare how each variation performs across these criteria. The colors used for the variations are blue, orange, and green.\nFigure 3: Error analysis of ChatGPT in the pilot study by human evaluation. CE and FE denote incorrectness and un- faithfulness errors, respectively. \nemploy our investigation, we randomly choose 100 samples (50 from MAVEN-ERE and 50 from Proof Writer). \n3.1.2 Experimental Setup .  Our experiments are conducted as a multi-turn conversation for zero-shot reasoning, to leverage LLM’s interaction ability. Given a task input    $(X)$  , we also write a prompt\n\n  describing the task, and let LLM generate output  by an-\n\n ( 𝑇 )  ( 𝑌 ) swering the given query. We also add  “Let’s think step by step” before each answer for prediction generation, which is a simple but effective trick to improve zero-shot reasoning for LLMs [ 19 ]. We adopt ChatGPT as the backbone and manually check its generated rationales under the following three settings:\n\n \n Vanilla LLM (i.e., ChatGPT) without any additional information;\n\n •\n\n  LLM (i.e., ChatGPT) plus the most relevant (i.e., ground truth)\n\n • logic; \n\nThe prompt examples can be found in Figure  $10{\\sim}13$  \n3.1.3 Analysis .  As shown in Figure 2, we visualize the micro-F1 and the proportion of logically inconsistent answers generated by ChatGPT. We find that no matter whether on MAVEN-ERE or Proof Writer, Vanilla ChatGPT always achieves a bad result with low micro-F1 scores and high inconsistency values (e.g.,  $15\\%$   micro- F1 and  $63\\%$   inconsistent answers on MAVEN-ERE), which indicates the deficiency of LLM in solving complex reasoning tasks. To inves- tigate this issue in depth, we conduct analyses from the following two aspects. \nWhat Is The Relation Between Logical Consistency And Model Performance?  From Figure 2, we find that: 1) The model directly receives significant improvements on both MAVEN-ERE and Proof Writer when adding relevant logic; 2) When adding some irrelevant logic, the results show some fluctuations (exaltation in MAVEN-ERE and degeneration in Proof Writer). That means di- rectly adding logic without any constraints will bring some uncer- tainty; 3) Typically, a higher logical inconsistency corresponds to a poorer micro-F1, however, rectifying logical inconsistency does not necessarily lead to the same degree of increase in micro-F1. Gen- erally, an intuitive observation is that incorporating relevant logic into the LLM instruction will be very helpful in solving reasoning tasks. So, the challenges are how to obtain these relevant logic and how to utilize them for LLMs. "}
{"page": 3, "image_path": "doc_images/2310.09158v1_3.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY\n\ntasks. So, the challenges are how to obtain these relevant logic and\nhow to utilize them for LLMs.\n\nWhat Types of Errors Does LLM Usually Make? To delve into\na deep understanding of the failures that Vanilla LLM encounters in\nlogical reasoning, we also conduct a detailed error analysis for this.\nHere, we divide the error types into two aspects: 1) Incorrectness\nto the Constraint (CE): whether the rationale generated by LLM\nis wrong (CE1), incomplete (CE2), or redundant (CE3) compared\nwith the true logical constraints. 2) Unfaithfulness to the Reasoning\nProcess (FE): where LLM does not correctly use the constraints. We\ndefine two types of errors upon FE, i.e., i) Wrong start, LLM begins\nwith an irrelevant fact or focuses on an improper perspective for\nthe correct answer (FE1). ii) Wrong process, LLM starts from a\nproper point, but makes mistakes during the reasoning process\n(FE2). Annotators are asked to review 100 predictions generated\nby ChatGPT and mark the error types. Results in Figure 3 show\nthat: 1) The quality of constraints produced by the Vanilla ChatGPT\nis not high enough, which limits its subsequent reasoning ability.\n2) Incorporating relevant logical constraints could guarantee the\ncorrectness of constraints and thus greatly improve the generation\nquality of ChatGPT in faithfulness.\n\n3.2 How Is LLM Performing Abstract Multi-hop\nReasoning?\n\nBased on the above analyses, we can confirm the deficiency of\nLLMs in solving complex reasoning tasks and the effectiveness of\nincorporating logical constraints. Nevertheless, we also want to\nexplore how LLMs exhibit in more challenging settings.\n\n3.2.1. Data Source. Considering that existing datasets lack multi-\nhop instances, we construct a synthesized dataset (LLM-LR) to\nevaluate the ability of LLMs to perform multi-hop reasoning. Specif-\nically, we first collect 39 additional logical constraints for all the\nhigh-order relations among three events, as outlined in Table 7.\nThe collection is based on transitive dependency (i.e., one event\nmay affect another through an intermediate event). For example,\nBEFORE(A, B) A BEFORE(B, C) > BEFORE(A, C) means that “If\nevent A happens BEFORE event B, and event B happens BEFORE\nevent C, then event A happens BEFORE event C”. Thereby, we obtain\na comprehensive set containing a total of 50 logical constraints\n(along with the 11 constraints between two events we introduced\nin Section 2.1).\n\nAs the number of events further increases (i.e., >3), there are\nmore complex interactions involved, and it is inefficient to list all\nthe constraints manually at this time. To address this, we introduce\nlogic programming [11, 22] to automatically generate new event\nrelations by inputting the known constraints and relations. We em-\nploy a forward- and backward-chaining rule-based method utilizing\nProlog [10] as the foundation for our logic programming approach.\nFor instance, when dealing with temporal relations involving four\nevents (A, B, C, and D), given the known relations: “BEFORE(A, B)\nA SIMULTANEOUS(B, C) A OVERLAP(C, D)”, our logic program-\nming approach can deduce a “BEFORE(A, D)” conclusion according\nto the constraints in Table 7. Then, we provide a task description\nand use the given relations as the input case to let LLMs reason\nthe relation between events (A, D), i.e., a 3-hop query. We could\n\nChen, et al.\n\n—-— GPT-turbo\n06 + —— Text-davinci-003\na) — GPT-4\nVicuna-13B\n2 =: =a ¥\n2 Ne Llama2-13B\n\n~\n\n2\nFS\n\n©\n\n2\n\n—— GPT-turbo\n\nGPT4\n= Vicuna-13B\n—— Llama2-138\n\nInconsistent Answers(%)\n2\n\n0.0\n\n©\n\n2 3 4 5 6 7 8 9 10 23 4 5 6 7 8 9\nNumber of Hops Number of Hops\n\nFigure 4: Multi-hop reasoning performance of LLMs evalu-\nated on our LLM-LR dataset.\n\nuse the description text provided in Table 6 to convert the sym-\nbolic representation into natural language forms. The conclusion\ndeduced by our logic engine will serve as the ground truth to check\nLLMs’ answers. A pseudo-code can be found in Appendix D.1 and\na prompt example is in Figure 14.\n\n3.2.2. Experimental Setup. For evaluation, we randomly gen-\nerate 50 samples for each 2~10-hop reasoning. In addition to the\nthree variants of ChatGPT (gpt-turbo, text-davinci-003,\nand gpt 4), we employ another two open-source LLMs (Vicuna-\n13B-v1.3! and Llama2-13B) for evaluation. Note that: 1) for 2-hop\nreasoning (i.e., high-order relations among three events), there are\nonly 39 samples. 2) Our approach allows for the extension of the\nreasoning path, but we report results for clarity and due to the\nlength limits of LLMs, covering only 2 to 10 hops.\n\n3.2.3 Analysis. As shown in Figure 4, we visualize the micro-F1\nand the proportion of logically inconsistent answers generated by\nLLMs. We can see that: 1) When the number of hops is relatively\nsmall (i.e., 2 ~ 5 hops), the performance of GPT-4 is outstand-\ning compared with other models. 2) With the increase of hops,\nall the LLMs perform worse when the reasoning becomes more\ncomplicated, and the proportion of logically inconsistent answers\nis gradually increasing. Among them, Vicuna-13B fully fails after\n6 hops and could not output any correct answers. This further\ndemonstrates the necessity of teaching LLMs logical reasoning.\n\n4 TEACHING LLMS LOGICAL REASONING\n\nBased on the aforementioned analysis, we expect to explore how\nto empower LLMs’ capability with logical reasoning. Therefore, in\nthis section, we first introduce the instruction-following technique\nwe use in Section 4.1 and then propose three different approaches\nto instruct LLMs to generate answers with better logical consis-\ntency (Section 4.2 ~ 4.4).\n\n4.1 In-Context Learning for LLMs\n\nWe deploy LLMs for event relation extraction and deductive reason-\ning tasks via in-context learning (ICL, [3, 32]). Given a task input\n(X), we also write a prompt (T) describing the task, then further\nprovide several demonstrations D = {D,12! ., where Dj = (Xj, Yi)\nare used for few-shot learning. Then, the LLM generates output (Y)\n\nby completing the prompt (Y = M(T,D,X)), where M denotes\n\n‘https://Imsys.org/projects/\n\n—— Text-davinci-003\n\n10\n", "vlm_text": "\nWhat Types of Errors Does LLM Usually Make?  To delve into a deep understanding of the failures that Vanilla LLM encounters in logical reasoning, we also conduct a detailed error analysis for this. Here, we divide the error types into two aspects: 1)  Incorrectness to the Constraint  (CE): whether the rationale generated by LLM is wrong (CE1), incomplete (CE2), or redundant (CE3) compared with the true logical constraints. 2)  Unfaithfulness to the Reasoning Process  (FE): where LLM does not correctly use the constraints. We define two types of errors upon FE, i.e., i) Wrong start, LLM begins with an irrelevant fact or focuses on an improper perspective for the correct answer (FE1). ii) Wrong process, LLM starts from a proper point, but makes mistakes during the reasoning process (FE2). Annotators are asked to review 100 predictions generated by ChatGPT and mark the error types. Results in Figure 3 show that: 1) The quality of constraints produced by the Vanilla ChatGPT is not high enough, which limits its subsequent reasoning ability. 2) Incorporating relevant logical constraints could guarantee the correctness of constraints and thus greatly improve the generation quality of ChatGPT in faithfulness. \n3.2 How Is LLM Performing Abstract Multi-hop Reasoning? \nBased on the above analyses, we can confirm the deficiency of LLMs in solving complex reasoning tasks and the effectiveness of incorporating logical constraints. Nevertheless, we also want to explore how LLMs exhibit in more challenging settings. \n3.2.1 Data Source .  Considering that existing datasets lack multi- hop instances, we construct a synthesized dataset ( LLM-LR ) to evaluate the ability of LLMs to perform multi-hop reasoning. Specif- ically, we first collect 39 additional logical constraints for all the high-order relations among three events, as outlined in Table 7. The collection is based on transitive dependency (i.e., one event may affect another through an intermediate event). For example, BEFORE( 𝐴 ,  𝐵 )  ∧ BEF  $\\mathrm{DRE}(B,\\,C)\\rightarrow\\mathrm{BaFORE}(A,\\,C)$   means that “ If event  𝐴 happens BEFORE event    $B$  , and event  𝐵 happens BEFORE event  𝐶 , then event  𝐴 happens BEFORE event  $C^{\\ast}$  . Thereby, we obtain a comprehensive set containing a total of 50 logical constraints (along with the 11 constraints between two events we introduced in Section 2.1). \nAs the number of events further increases (i.e.,  ${>}3$  ), there are more complex interactions involved, and it is inefficient to list all the constraints manually at this time. To address this, we introduce logic programming [ 11 ,  22 ] to automatically generate new event relations by inputting the known constraints and relations. We em- ploy a forward- and backward-chaining rule-based method utilizing Prolog [ 10 ] as the foundation for our logic programming approach. For instance, when dealing with temporal relations involving four events   $(A,B,C$  , and  $D_{-}$  ), given the known relations: “BEFORE  $(A,B)$  ∧ SIMULTANEOUS  $(B,C)$     $\\setminus\\operatorname{OVERLAP}(C,D)^{\\ast}$  ”, our logic program- ming approach can deduce a   $\\mathrm{\"BEGRE}(A,D)\"$   conclusion according to the constraints in Table 7. Then, we provide a task description and use the given relations as the input case to let LLMs reason the relation between events  $(A,D)$  , i.e., a 3-hop query. We could \nThe image shows two line graphs comparing the performance of different language models (GPT-turbo, Text-davinci-003, GPT-4, Vicuna-13B, Llama2-13B) on multi-hop reasoning tasks. \n\n- **Left Graph: Micro-F1 (%) vs. Number of Hops**\n  - Displays how the Micro-F1 score changes as the number of hops increases from 2 to 10. Generally, the performance decreases as the number of hops increases.\n\n- **Right Graph: Inconsistent Answers (%) vs. Number of Hops**\n  - Shows the percentage of inconsistent answers as the number of hops increases from 2 to 10. The inconsistency tends to increase with more hops.\n\nEach line represents a different model, as indicated in the legend.\nuse the description text provided in Table 6 to convert the sym- bolic representation into natural language forms. The conclusion deduced by our logic engine will serve as the ground truth to check LLMs’ answers. A pseudo-code can be found in Appendix D.1 and a prompt example is in Figure 14. \n3.2.2 Experimental Setup .  For evaluation, we randomly gen- erate 50 samples for each  $2{\\sim}10$  -hop reasoning. In addition to the three variants of ChatGPT ( gpt-turbo ,  text-davinci-003 , and  gpt4 ), we employ another two open-source LLMs (Vicuna-  $13\\mathrm{B}–\\mathrm{v}1.3^{1}$    and Llama2-13B) for evaluation. Note that: 1) for 2-hop reasoning (i.e., high-order relations among three events), there are only 39 samples. 2) Our approach allows for the extension of the reasoning path, but we report results for clarity and due to the length limits of LLMs, covering only 2 to 10 hops. \n3.2.3 Analysis .  As shown in Figure 4, we visualize the micro-F1 and the proportion of logically inconsistent answers generated by LLMs. We can see that: 1) When the number of hops is relatively small (i.e.,   $2~\\sim~5$   hops), the performance of GPT-4 is outstand- ing compared with other models. 2) With the increase of hops, all the LLMs perform worse when the reasoning becomes more complicated, and the proportion of logically inconsistent answers is gradually increasing. Among them, Vicuna-13B fully fails after 6 hops and could not output any correct answers. This further demonstrates the necessity of teaching LLMs logical reasoning. \n4 TEACHING LLMS LOGICAL REASONING \nBased on the aforementioned analysis, we expect to explore how to empower LLMs’ capability with logical reasoning. Therefore, in this section, we first introduce the instruction-following technique we use in Section 4.1 and then propose three different approaches to instruct LLMs to generate answers with better logical consis- tency (Section  $4.2\\sim4.4$  ). \n4.1 In-Context Learning for LLMs \nWe deploy LLMs for event relation extraction and deductive reason- ing tasks via in-context learning (ICL, [ 3 ,  32 ]). Given a task input  $(X)$  , we also write a prompt    $(T)$   describing the task, then further provide several demonstrations    $D=\\left\\{D_{i}\\right\\}_{i=1}^{|\\bar{D}|}$    .  $D_{i}=(X_{i},Y_{i})$  are used for few-shot learning. Then, the LLM generates output  ( 𝑌 ) by completing the prompt    $\\mathcal{(Y}=\\mathcal{M}(T,D,X))$  ) , where  $\\mathcal{M}$   denotes "}
{"page": 4, "image_path": "doc_images/2310.09158v1_4.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning,\n\nUser\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nThere is a piece of text with two events, and your task is to identify four types of relations between them...\n\nText:\n\nA large FIRE broke out at the Waitrose supermarket in Wellington’s High Street, half of the roof at the entrance\n\nof the store collapsed during the blaze.\n\nLLM - Thought\n‘From the text, we could first get: “FIRE”\ncauses the store to 'collapsed”.\n\nLogical Set\n\nyour answers:\n\n| Due to the logic constraint: if event A CAUSEs (Qa) Iftwoe\n\nWe can deduce that the answers are:\n\nThere are some rules among the relations, you |\n“can select some of them to reason or check\n\nDataset\n\nLLM-LR\n\nconte:\n| Given that event A and event B are COREFERENCE,\n: | event B and event C are COREFERENCE.\n| Question:\n| Based on the above information, what are the four\n| types of relations between event A and C?\n| Answers:\n| COREFERENCE, NO_TEMPORAL, NO_CAUSAL,\n\n!+ Logical Constraints\n\nRetrieval y Pre-training\n\nCoreference Relation\nTemporal Relation ---| OVERLAP\nCausal Relation ~\nSubevent Relation\n\nOutput Answers\n\nOriginal Answers\n\n(a) Generative-based\n\n1 OVERLAP\n248 NERA\ncons foetaoves\n\n(b) Retrieval-based\n\noF\nPost-processing\n\nSpecialized LLMs\n\n8\n\nOutput Answers\n(c) Pretraining-based\n\nFigure 5: Incorporate logical constraints to LLMs by using generative, retrieval, and pretraining-based approaches. The dashed\nboxes indicate answers output by LLMs, and the underlined texts indicate the logical constraints.\n\nthe LLM. In such a setting, the LLM can follow the structure of the\nprovided demonstrates to output the expected format of answers\nfor subsequent automatic evaluation. Besides, the whole process\ndoes not require any gradient update, allowing LLMs to generate\npredictions without massive training data.\n\nCompared Models. We choose three variants of ChatGPT (gpt —\nturbo, text-davinci-003, and gpt 4), Vicuna-13B-v1.3, and\nLlama2-13B as the main experimental LLMs for evaluation. We also\nprovide two fine-tuning RoBERTa-large baselines (one-shot and\nfully fine-tuned) for comparison. The training details of ROBERTa-\nlarge can be found in Appendix B.2.\n\nDataset Construction. Our main experiments are evaluated on\nMAVEN-ERE, Causal-TimeBank [28], and Proof Writer. For the ERE\ntask, we focus on relations between two events and conduct sam-\npling at the sentence level. The samples of the two events that do\nnot have any relations will be excluded. Here, we randomly sample\n500 examples from the test set of MAVEN-ERE and 100 examples\nfrom the test set of Causal-TimeBank as our testbed. For the deduc-\ntive reasoning task, we use the OWA subset of Proof Writer, which\nis divided into five parts, each part requiring 0, 1, 2, 3, and 5 hops of\nreasoning, respectively. We evaluate the hardest 5-hop subset. To\nreduce the computation cost, we randomly sample 200 examples in\nthe test set and ensure a balanced label distribution. Other details\ncan be found in Appendix B.1.\n\nEvaluation Metrics. We adopt the averaged micro-F1 score as the\nevaluation metric and also report the logical inconsistency (defined\nin Section 2.1) on ERE datasets. The reported value is averaged by\nthe results of three runs to reduce random fluctuation.\n\n4.2 Generative-based Approaches\n\nGenerative-based approaches mean we let LLMs generate logic by\nusing a form of one-shot ICL. Here, we study three variants:\n\n(1) Vanilla ICL: which utilizes the common prompts consisting of a\ntask description, one demonstration, and the input case.\nVanilla ICL plus CoT: which first bootstraps rationales by using\nchain-of-thought as intermediate reasoning steps following the\nstyle of the given demonstration, then output answers. Ratio-\nnales here do not involve the content of logical constraints.\nCoT with self-generated logical constraints: which teaches LLMs\nto generate and utilize logical constraints based on CoT (Fig-\nure 5 (a)). Specifically, it will first extract the obvious relation-\ns/facts and generate the relevant logical constraints according\nto the extracted relations/facts, then we enforce LLMs to in-\nfer the remaining relations/facts based on the constraints and\nthe known relations/facts. The prompt example can be seen in\nAppendix H.2.\n\n(2\n\n(3\n\n4.2.1. Results. From Table 1, We could observe that: 1) Compared\nwith a smaller language model (SLM, i.e., ROBERTa-large), the gen-\neralization ability of vanilla LLMs on both two tasks under the\n", "vlm_text": "The image illustrates an approach to incorporating logical constraints into large language models (LLMs) using generative, retrieval, and pre-training methods, as labeled in sections (a), (b), and (c).\n\n1. **User Prompt**: A piece of text with two events is given, where the task is to identify four types of relations—Coreference, Temporal, Causal, and Subevent—between the two events described: a fire breaking out and a collapse occurring.\n\n2. **Sections**:\n   - **(a) Generative-based (LLM-Thought)**: This approach uses logical reasoning to derive answers. It starts by identifying that the \"FIRE\" causes the store to \"collapsed\". Logical constraints applied (e.g., causation implies certain ordering and lack of coreference/subevent) lead to deduced relations such as no coreference, overlap for temporal relation, cause, and no subevent. The output is aligned with logical constraints.\n   \n   - **(b) Retrieval-based (Logical Set)**: This method involves using predefined logic rules to detect and resolve conflicts in the answers retrieved. Here, the system detects a conflict between simultaneous and overlap, correcting it by replacing \"SIMULTANEOUS\" with \"OVERLAP\", resulting in conflict-free answers.\n   \n   - **(c) Pretraining-based (LLM-LR Dataset)**: Pre-trained specialized LLMs are used to generate answers based on a dataset context where known relationships serve as input. It outputs answers similar to the generative approach, complying with logical constraints.\n\nThe image highlights how logical constraints can refine LLM outputs to more accurately capture relationships between events based on the described approaches.\nthe LLM. In such a setting, the LLM can follow the structure of the provided demonstrates to output the expected format of answers for subsequent automatic evaluation. Besides, the whole process does not require any gradient update, allowing LLMs to generate predictions without massive training data. \nCompared Models.  We choose three variants of ChatGPT ( gpt- turbo ,  text-davinci-003 , and  gpt4 ), Vicuna-13B-v1.3, and Llama2-13B as the main experimental LLMs for evaluation. We also provide two fine-tuning RoBERTa-large baselines (one-shot and fully fine-tuned) for comparison. The training details of RoBERTa- large can be found in Appendix B.2. \nDataset Construction.  Our main experiments are evaluated on MAVEN-ERE, Causal-TimeBank [ 28 ], and Proof Writer. For the ERE task, we focus on relations between two events and conduct sam- pling at the sentence level. The samples of the two events that do not have any relations will be excluded. Here, we randomly sample 500 examples from the test set of MAVEN-ERE and 100 examples from the test set of Causal-TimeBank as our testbed. For the deduc- tive reasoning task, we use the OWA subset of Proof Writer, which is divided into five parts, each part requiring 0, 1, 2, 3, and 5 hops of reasoning, respectively. We evaluate the hardest 5-hop subset. To reduce the computation cost, we randomly sample 200 examples in the test set and ensure a balanced label distribution. Other details can be found in Appendix B.1. \nEvaluation Metrics.  We adopt the averaged micro-F1 score as the evaluation metric and also report the logical inconsistency (defined in Section 2.1) on ERE datasets. The reported value is averaged by the results of three runs to reduce random fluctuation. \n4.2 Generative-based Approaches \nGenerative-based approaches mean we let LLMs generate logic by using a form of one-shot ICL. Here, we study three variants:\n\n \n(1)  Vanilla ICL:  which utilizes the common prompts consisting of a task description, one demonstration, and the input case.\n\n (2)  Vanilla ICL plus CoT:  which first bootstraps rationales by using chain-of-thought as intermediate reasoning steps following the style of the given demonstration, then output answers. Ratio- nales here do not involve the content of logical constraints.\n\n (3)  CoT with self-generated logical constraints:  which teaches LLMs to generate and utilize logical constraints based on CoT (Fig- ure 5 (a)). Specifically, it will first extract the obvious relation-  $s/$  facts and generate the relevant logical constraints according to the extracted relations/facts, then we enforce LLMs to in- fer the remaining relations/facts based on the constraints and the known relations/facts. The prompt example can be seen in Appendix H.2.\n\n \n4.2.1 Results .  From Table 1, We could observe that: 1) Compared with a smaller language model (SLM, i.e., RoBERTa-large), the gen- era liz ation ability of vanilla LLMs on both two tasks under the "}
{"page": 5, "image_path": "doc_images/2310.09158v1_5.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY Chen, et al.\nModel MAVEN-ERE Causal-TimeBank Proof Writer\nMicro-F1 (%) LI (%) | Micro-F1 (%) LI (%) | Micro-F1 (%)\nRoBERTa-Large (one-shot) 17.4 54.8 - - 35\nRoBERTa-Large (fully fine-tuned) 56.8 6.4 22.2 36.2 63\n& vanilla ICL 18.0 53.3 19.0 54.0 39\nEj vanilla ICL w. CoT 18.8 49.3 17.0 30.3 42\nm CoT w. logical constraints 25.3 37.9 27.0 12.8 45\nFE vanilla ICL 21.6 49.1 18.0 58.8 30\n5 vanilla ICL w. CoT 20.5 60.5 21.0 64.7 40\n[a] CoT w. logical constraints 24.8 5.5 23.0 39.2 49\n+ vanilla ICL 29.3 50.7 22.5 30.5 47\n[ vanilla ICL w. CoT 30.3 36.7 23.0 35.0 67\n9 CoT w. logical constraints 32.3 13.7 24.5 24.0 70\n€ vanilla ICL 13.8 25.4 45 84.1 37\n& vanilla ICL w. CoT 11.6 47.4 6.0 57.6 40\n> CoT w. logical constraints 14.9 21.7 8.0 33.1 42\n3 vanilla ICL 17.0 54.6 115 26.7 29\nE vanilla ICL w. CoT 17.8 58.4 10.5 33.6 31\n= CoT w. logical constraints 21.5 18.9 13.0 18.1 40\n\nTable 1: ChatGPT (gpt-turbo, text-davinci-003, and gpt-4), Vicuna-13B, and Llama2-13B’s performance on MAVEN-ERE, Causal-\nTimeBank, and Proof Writer. We report averaged micro-F1 scores here and \"LI\" denotes the logical inconsistency metric.\nFor each dataset, the best result of each LLM is in bold. ROBERTa-Large (one-shot) fails to output any correct answers on\n\nCausal-TimeBank.\n\none-shot setting is remarkable, but there is still a gap with the fully-\nfinetuned baseline. 2) Directly using CoT to infer logic does not help\nmuch for ERE tasks, a possible reason is that the inherent issues\nmay lead to the failure of LLM in the precise rationale generation\n(i.e., a high ratio of logical inconsistency). We give a case study for\nthis in Appendix E. 3) When using generative-based approaches\nto encourage LLMs to produce logical constraints in the reasoning\nprocess, LLMs can significantly improve their performance on both\ntwo tasks. It is worth mentioning that the performance of GPT-4\n(CoT w. logical constraints) could even surpass that of the fully\nfine-tuned baseline on the Proofwriter dataset.\n\n4.3 Retrieval-based Approaches\n\nAlthough generative-based approaches enable models to automati-\ncally generate and utilize logic, the prediction of LLMs is usually un-\ncertain and inaccuracy. Therefore, we also provide retrieval-based\napproaches, which aim to obtain relevant logic from our pre-defined\nconstraints (Figure 5 (b)). We mainly conduct experiments on the\nERE task by utilizing the collected logical constraints. Specifically,\nWe take the collected 11 constraints in Section 2.1 as the retrieval\nset, and our solutions include:\n\n(1) with all logical constraints: which directly adds all the 11 logical\nconstraints in the set.\n\n(2) with retrieved logical constraints: which means that we first\ndetect logically inconsistent answers based on the prediction of\nLLMs, and then retrieve the corresponding information if we\nfind any conflict. Finally, we add it to the LLM instruction and\n\nlet LLMs re-generate the answers. Please see Appendix C.1 for\ndetails.\n\n(3) with post-processing: which first obtains the answers of LLMs,\nthen automatically generates some logically consistent candi-\ndates according to the constraints, and randomly selects one of\nthem as the final answer. This approach ensures that there is no\nlogical conflict (LI = 0%). Please see Appendix C.2 for details.\n\n4.3.1 Main Results. From Table 2, We could observe that: 1)\nWhen using retrieval-based approaches to obtain logic constraints\nand incorporate them into LLM instruction, the logical inconsis-\ntency of LLMs’ answers is greatly reduced and the overall perfor-\nmance on both two tasks is further improved. 2) Although our\npost-processing guarantees the absence of logical conflicts (result-\ning in LI of 0%), it may severely affect the quality of the whole\ngeneration. On one hand, the semantics of the post-processing an-\nswer may be far from the ground truth due to the random selection.\nOn the other hand, the size of the candidate set for each case will\nalso affect the performance. It may also need more operations at\nthe post-processing stage, which we leave as future work.\n\n4.3.2 Ablation Study. We conduct an ablation study using Chat-\nGPT (gpt-turbo) in this subsection.\n\nDemonstrations. Following previous experiences [3], we also ap-\npend demonstrations into the prompt to investigate how logical\nconstraints will affect when combined with different numbers of\ndemonstrations. Here, we select different numbers of demonstration\nsamples K from {1, 5, 10,20}. The experiments are tested on vanilla\n", "vlm_text": "The table presents the evaluation of different models on three tasks: MAVEN-ERE, Causal-TimeBank, and ProofWriter. For each task, it provides Micro-F1 (%) scores and LI (%) scores where applicable. The models include RoBERTa-Large (one-shot and fully fine-tuned), Turbo, Davinci, GPT-4, Vicuna, and Llama2, with variations in vanilla ICL, vanilla ICL with CoT, and CoT with logical constraints.\n\n### Columns:\n1. **Model** - The models and their configurations.\n2. **MAVEN-ERE** - Measures:\n   - Micro-F1 (%)\n   - LI (%)\n3. **Causal-TimeBank** - Measures:\n   - Micro-F1 (%)\n   - LI (%)\n4. **ProofWriter** - Measures:\n   - Micro-F1 (%)\n\n### Key Points:\n- RoBERTa-Large shows baseline scores for comparison.\n- Turbo, Davinci, GPT-4, Vicuna, and Llama2 are evaluated in various configurations.\n- The highest scores for each task and model variation are bolded.\none-shot setting is remarkable, but there is still a gap with the fully- finetuned baseline. 2) Directly using CoT to infer logic does not help much for ERE tasks, a possible reason is that the inherent issues may lead to the failure of LLM in the precise rationale generation (i.e., a high ratio of logical inconsistency). We give a case study for this in Appendix E. 3) When using generative-based approaches to encourage LLMs to produce logical constraints in the reasoning process, LLMs can significantly improve their performance on both two tasks. It is worth mentioning that the performance of GPT-4 (CoT w. logical constraints) could even surpass that of the fully fine-tuned baseline on the Proof writer dataset. \n4.3 Retrieval-based Approaches \nAlthough generative-based approaches enable models to automati- cally generate and utilize logic, the prediction of LLMs is usually un- certain and inaccuracy. Therefore, we also provide retrieval-based approaches, which aim to obtain relevant logic from our pre-defined constraints (Figure 5 (b)). We mainly conduct experiments on the ERE task by utilizing the collected logical constraints. Specifically, We take the collected 11 constraints in Section 2.1 as the retrieval set, and our solutions include:\n\n \n(1)  with all logical constraints:  which directly adds all the 11 logical constraints in the set.\n\n \n(2)  with retrieved logical constraints:  which means that we first detect logically inconsistent answers based on the prediction of LLMs, and then retrieve the corresponding information if we find any conflict. Finally, we add it to the LLM instruction and let LLMs re-generate the answers. Please see Appendix C.1 for details.\n\n \n(3)  with post-processing:  which first obtains the answers of LLMs, then automatically generates some logically consistent candi- dates according to the constraints, and randomly selects one of them as the final answer. This approach ensures that there is no logical conflict   $(\\mathrm{{LI}}=0\\%)$  ). Please see Appendix C.2 for details.\n\n \n4.3.1 Main Results .  From Table 2, We could observe that: 1) When using retrieval-based approaches to obtain logic constraints and incorporate them into LLM instruction, the logical inconsis- tency of LLMs’ answers is greatly reduced and the overall perfor- mance on both two tasks is further improved. 2) Although our post-processing guarantees the absence of logical conflicts (result- ing in LI of   $0\\%$  ), it may severely affect the quality of the whole generation. On one hand, the semantics of the post-processing an- swer may be far from the ground truth due to the random selection. On the other hand, the size of the candidate set for each case will also affect the performance. It may also need more operations at the post-processing stage, which we leave as future work. \n4.3.2 Ablation Study .  We conduct an ablation study using Chat- GPT ( gpt-turbo ) in this subsection. \nDemonstrations.  Following previous experiences [ 3 ], we also ap- pend demonstrations into the prompt to investigate how logical constraints will affect when combined with different numbers of demonstrations. Here, we select different numbers of demonstration samples  $K$  from  $\\{1,5,10,20\\}$  . The experiments are tested on vanilla "}
{"page": 6, "image_path": "doc_images/2310.09158v1_6.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nModel MAVEN-ERE Causal-TimeBank\nMicro-F1 (%) LI (%) | Micro-F1 (%) LI (%) |\nH w. all logical constraints 20.8 30.9 20.0 36.8\nEj w. retrieved logical constraints 22.3 30.2 22.0 11.3\na w. post-processing 14.0 0 15.0 0\n3g w. all logical constraints 27.0 25.6 31.0 21.8\n5 w. retrieved logical constraints 27.8 30.8 22.0 40.5\n[=] w. post-processing 14.8 0 19.0 0\n+ w. all logical constraints 37.3 8.3 26.0 20.0\noe w. retrieved logical constraints 33.5 28.8 24.0 13.5\n9 w. post-processing 17.0 0 19.0 0\n€ w. all logical constraints 15.2 37.6 11.0 23.5\ng w. retrieved logical constraints 15.7 33.2 10.0 26.7\nad w. post-processing 9.8 0 9.0 0\n3 w. all logical constraints 19.5 34.6 10.0 23.5\n5 w. retrieved logical constraints 18.3 38.2 9.5 26.7\n=| w. post-processing 12.0 0 9.5 0\n\nTable 2: Retrieval-based approaches’ performance on MAVEN-ERE and Causal-TimeBank. For each dataset, the best result of\n\neach LLM is in bold and the second-best result is underlined.\n\nICL and ICL plus all logical constraints. From Figure 6, we can ob-\nserve that: 1) When the number of demonstrations increases from\n1 to 5, we can observe an evident improvement, but the subsequent\nimprovements are limited when continue to increase the number of\ndemonstrations (e.g., > 10); 2) Adding logical constraints into LLM\ninstructions can provide stable improvements, especially with more\ndemonstrations. 3) The performance of incorporating logical con-\nstraints with a smaller number of demonstrations can even surpass\nthat of prompts with only a larger number of demonstrations (e.g.,\nthe performance of using 5 demonstrations on MAVEN-ERE w. log-\nical constraints, 25.7%, surpasses that of 10 demonstrations w/o.\nlogical constraints, 24.5%). This indicates that it is important to\ntell LLMs both \"What\" (demonstrations) and \"How\" (logical con-\nstraints). Overall, these studies further confirm the merits of using\nlogical constraints in solving reasoning tasks.\n\nIterative Retrieval. Considering the outstanding ability of LLMs\nin interaction, we further explore whether we can introduce log-\nical constraints into the multi-turn conversation (for the prompt\ndesign, please see Appendix H.3). Here, we adopt a retrieval-based\napproach to incorporate logical constraints iteratively and the re-\nsults are shown in Figure 6. We find that the logical inconsistency\nof answers will gradually decrease with the increase of iterations,\nbut the overall micro-F1 seems relatively stable. We guess the main\nreason for this phenomenon is the overthinking of LLMs, as al-\nthough it can bring more reasoning rationale, it possibly produces\ncorrect but more useless or abundant information when inferring\nmultiple iterations. Overall, instructing LLM with logic is beneficial\nfor conversation, but how to support longer information is still\nchallenging.\n\n4.4 Pretraining-based Approach\n\nAlthough the retrieval-based approach guarantees the correctness\nof logical constraints, it still needs to interact with an external set\n\n0s MAVEN-ERE Ss CTB 59\n20- >\n- _ Lae\nrr) = “3\n= Z45- 2\nr15 Ps we\n2 $10 1 8\nan MAVEN-ERE wio. le 5 10- -20£\n= MAVEN-ERE w. Ic = 3\n5 CTB wio. le 5- 10%\nCTBw. Ic MAVEN-ERE —— CTB 3\n\n° —_——— o- -0\n\n1 5 10 20 o 1 #2 3 4\n\nNumber of Demonstration Samples Number of iterations\n\nFigure 6: Ablation Study of ChatGPT for demonstrations and\niterative retrieval, where “Ic” denotes the logical constraints.\n\nconstantly. Therefore, we provide a pretraining-based approach to\nembed the logical constraints into LLMs themselves. We use the\nlogic programming approach introduced in Section 3.2 to automati-\ncally generate 6776 instances containing all the 2 ~ 5-hop reasoning\ndata. We do not generate longer hops for training here considering\nthe computation complexity and the length limitation of LLMs. The\ndataset statistic can be found in Table 5. Then, we train LLMs to\nperform complex logical reasoning based on the curated dataset\nLLM-LR. Finally, we conduct inference with the trained LLMs. An\nexample of the training data can be seen in Figure 5 (c) or Figure 14.\n\n4.4.1 Pretraining Details. We adopt Vicuna-13B-v1.3 and Llama2-\n13B as the base models and employ the LoRA [14] technique. During\npre-training, only LoRA parameters are optimized. Other Details\ncan be found in Appendix G.\n\n4.4.2 Results. As shown in Table 3, we find that: 1) Once trained\non LLM-LR, the performance of LlaMA2-13B and Vicuna-13B im-\nproves greatly compared with that of Table 1 and 2, especially on\nthe baselines without logical constraints. 2) The performance of\nLlaMA2-13B-PT could even surpass that of some greater LLMs (e.g.,\n", "vlm_text": "The table presents a comparison of model performance on two datasets: MAVEN-ERE and Causal-TimeBank. It reports the Micro-F1 percentages and Logical Inconsistency (LI) percentages for different models (Turbo, Davinci, GPT-4, Vicuna, and Llama2) under three conditions:\n\n1. With all logical constraints\n2. With retrieved logical constraints\n3. With post-processing\n\nKey columns include:\n- Micro-F1 (%) for MAVEN-ERE and Causal-TimeBank, indicating precision and recall balance.\n- LI (%) with a downward arrow, indicating lower is better for logical inconsistency.\n\nEach model has results for these metrics under the specified conditions.\nICL and ICL plus all logical constraints. From Figure 6, we can ob- serve that: 1) When the number of demonstrations increases from 1 to 5, we can observe an evident improvement, but the subsequent improvements are limited when continue to increase the number of demonstrations (e.g.,  $\\geq10.$  ); 2) Adding logical constraints into LLM instructions can provide stable improvements, especially with more demonstrations. 3) The performance of incorporating logical con- straints with a smaller number of demonstrations can even surpass that of prompts with only a larger number of demonstrations (e.g., the performance of using 5 demonstrations on MAVEN-ERE w. log- ical constraints,  $25.7\\%$  , surpasses that of 10 demonstrations w/o. logical constraints,  $24.5\\%$  ). This indicates that it is important to tell LLMs both \"What\" (demonstrations) and \"How\" (logical con- straints). Overall, these studies further confirm the merits of using logical constraints in solving reasoning tasks. \nIterative Retrieval.  Considering the outstanding ability of LLMs in interaction, we further explore whether we can introduce log- ical constraints into the multi-turn conversation (for the prompt design, please see Appendix H.3). Here, we adopt a retrieval-based approach to incorporate logical constraints iterative ly and the re- sults are shown in Figure 6. We find that the logical inconsistency of answers will gradually decrease with the increase of iterations, but the overall micro-F1 seems relatively stable. We guess the main reason for this phenomenon is the over thinking of LLMs, as al- though it can bring more reasoning rationale, it possibly produces correct but more useless or abundant information when inferring multiple iterations. Overall, instructing LLM with logic is beneficial for conversation, but how to support longer information is still challenging. \n4.4 Pre training-based Approach \nAlthough the retrieval-based approach guarantees the correctness of logical constraints, it still needs to interact with an external set \nThe image consists of two graphs.\n\n1. **Left Graph:**\n   - **Y-Axis:** Micro-F1 (%)\n   - **X-Axis:** Number of Demonstration Samples (1, 5, 10, 20)\n   - **Legend:**\n     - MAVEN-ERE without logical constraints (w/o. lc)\n     - MAVEN-ERE with logical constraints (w. lc)\n     - CTB without logical constraints (w/o. lc)\n     - CTB with logical constraints (w. lc)\n\n   The graph shows the performance (Micro-F1 score) of different models/datasets with and without logical constraints as the number of demonstration samples increases.\n\n2. **Right Graph:**\n   - **Left Y-Axis:** Micro-F1 (%), shown with orange bars\n   - **Right Y-Axis:** Logical Inconsistency (%), shown with a blue line\n   - **X-Axis:** Number of iterations (0, 1, 2, 3, 4)\n\n   The graph demonstrates how the Micro-F1 score and logical inconsistency change over multiple iterations for the MAVEN-ERE and CTB datasets. The MAVEN-ERE results are shown separately with a line, indicating a decreasing trend in logical inconsistency with iterations.\n\nOverall, the figures illustrate an ablation study of ChatGPT for demonstrations and iterative retrieval, focusing on the impact of logical constraints on performance and consistency.\nconstantly. Therefore, we provide a pre training-based approach to embed the logical constraints into LLMs themselves. We use the logic programming approach introduced in Section 3.2 to automati- cally generate 6776 instances containing all the  $2\\sim5$  -hop reasoning data. We do not generate longer hops for training here considering the computation complexity and the length limitation of LLMs. The dataset statistic can be found in Table 5. Then, we train LLMs to perform complex logical reasoning based on the curated dataset LLM-LR . Finally, we conduct inference with the trained LLMs. An example of the training data can be seen in Figure 5 (c) or Figure 14. \n4.4.1 Pre training Details .  We adopt Vicuna-13B-v1.3 and Llama2- 13B as the base models and employ the LoRA [ 14 ] technique. During pre-training, only LoRA parameters are optimized. Other Details can be found in Appendix G. \n4.4.2 Results .  As shown in Table 3, we find that: 1) Once trained on  LLM-LR , the performance of LlaMA2-13B and Vicuna-13B im- proves greatly compared with that of Table 1 and 2, especially on the baselines without logical constraints. 2) The performance of LlaMA2-13B-PT could even surpass that of some greater LLMs (e.g., "}
{"page": 7, "image_path": "doc_images/2310.09158v1_7.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY\n\nModel MAVEN-ERE Causal-TimeBank\nMicro-F1 (%) LI (%) | Micro-F1 (%) LI (%) |\nvanilla ICL 15.3 (+1.5%) 21.2 (-4.2%) 8.0 (+3.5%) 35.5 (-48.6%)\na vanilla ICL w. CoT 15.8 (+4.2%) 17.8 (-29.6%) 7.5 (+1.5%) 52.5 (-5.1%)\na CoT w. logical constraints 18.0 (+3.1%) 6.0 (-15.7%) 8.5 (+0.5%) 2.0 (-31.1%)\n4 oe\n5 w. all logical constraints 16.3 (+1.1%) 8.7 (-28.9%) 12.1 (+1.1%) 0 (-23.5%)\nFs w. retrieved logical constraints 16.1 (+0.4%) 19.0 (-14.2%) 10.7 (+0.7%) 9.5 (-17.2%)\nw. post-processing 11.0 (+1.2%) 0(-) 8.0 (-1.0%) 0(-)\n[ vanilla ICL 19.0 (+2.0%) 45.8 (-8.8%) 12.0 (+0.5%) 22.7 (-4.0%)\n& vanilla ICL w. CoT 22.1 (+4.3%) 42.9 (-15.5%) 11.5 (+1.0%) 3.0 (-30.6%)\nnl CoT w. logical constraints 26.4 (+4.9%) 15.7 (-3.2%) 13.3 (+0.3%) 13.0 (-5.1%)\ney eee ete\nBy w. all logical constraints 20.2 (+0.7%) 28.7 (-5.9%) 12.0 (+2.0%) 23.0 (-0.5%)\n4 w. retrieved logical constraints 18.7 (+0.4%) 34.2 (-4.0%) 11.0 (+1.5%) 19.4 (-7.3%)\nw. post-processing 11.0 (-1.0%) 0(-) 11.0 (+1.5%) 0(-)\n\nChen, et al.\n\nTable 3: Vicuna-13B and Llama2-13B’s performance on MAVEN-ERE and Causal-TimeBank after pre-training on LLM—LR.\n\nText: The exhibition < went on > to show at the Art Institute of\n\ni\ni\n\nf\n\ni\n\n| Chicago and then to The Copley Society of Art in Boston , where,\n| due toa lack of space, all the work by American artists was <\n\n\\ removed >.\n{\n\ni\n\n{\n\n{\n\n{\n\nt\n\ni\n\n{\n\n{\n\n1\n1\n\n1\n\n}\nEvent Pairs: {\n< went on >and < removed > 1\n< removed > and < went on > q\nAnswers: 1\nNO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT. 1\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT. |\nDemonstration ;\n\nNO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.\nCOREFERENCE, SIMULTANEOUS, NO_CAUSAL, SUBEVENT.\n\n1\nCOREFERENCE, NO_TEMPORAL, NO_CAUSAL, No_suBEVENT.~ ;\nLlama-2-13B-PT\n\nFigure 7: Case study on Llama-2-13B before and after pre-\ntraining (PT).\n\nvanilla ChatGPT, 175B), which further validates the importance of\nteaching LLM with logic in solving reasoning tasks.\n\n44.3 Case Study. In Figure 7, We conduct a case study of Llama-\n2-13B’s answers to the same input before and after pre-training.\nFrom Figure 7 we can see that LlaMA2-13B-PT could output the\ncorrect answers after pre-training on LLM-LR, which validates the\neffectiveness of our pre-training approach.\n\n5 RELATED WORK\n5.1 Large Language Models (LLMs)\n\nWe are fortunate to witness the surging development of Large\nLanguage Models (LLMs [3, 8, 9, 32]), and a series of work aiming\nto leverage the reasoning abilities of LLMs such as chain-of-thought\n\nprompting [19, 46, 51], self verification [18, 44], self learning [15,\n49], etc. However, recent studies show LLMs still stumble across\ngenerating hallucination and logic inconsistency [2, 13, 16, 17, 20].\nTo solve such challenges, our work explores teaching LLMs logical\nreasoning through various approaches.\n\n5.2 Event Relation Extraction (ERE)\n\nEvents play crucial roles in comprehending narratives, and under-\nstanding the complex relationships between events is essential to\nunderstanding the text [37]. Thus Event Relation Extraction (ERE)\ntasks are fundamental information extraction (IE) tasks and support\nvarious downstream applications [5, 36, 50]. Extensive studies have\nbeen carried out on ERE tasks, including different kinds of relations\nsuch as coreference relations [24, 25], temporal relations [30, 40],\ncausal relations [4, 6, 7], and subevent relations [1, 41].\n\nThere also have been some recent explorations on how to lever-\nage the power of LLMs on event-related information extraction\ntasks [12, 27, 43]. To the best of our knowledge, however, our work\nis the first to (1) design elaborate experiments to evaluate the per-\nformance of LLMs on the ERE task, and (2) analyze the logical\nreasoning abilities of LLMs using ERE as an intermediate task.\n\n6 CONCLUSION\n\nIn this paper, we conduct a detailed investigation on how to enhance\nLLMs to produce more logically consistent answers. Specifically,\nwe first investigate the existing issues of current LLMs in doing\n\nsome complex reasoning tasks (e.g., event relation extraction and\ndeductive reasoning). Then, we study multiple strategies to obtain\nand utilize logic for LLMs, including generative-based, retrieval-\nbased, and pretraining-based approaches. Based on our approach,\nwe also contribute a synthesized dataset (LLM-LR) involving multi-\nhop reasoning for evaluation and pre-training. We show that LLMs\nare not logically consistent reasoners, but their performance could\nbe improved if we explicitly teach them the logical constraints.\nComprehensive quantitative and qualitative analyses have been\nconducted to further provide insights.\n", "vlm_text": "The table presents the performance of two models, Vicuna-13B-PT and Llama2-13B-PT, on two datasets: MAVEN-ERE and Causal-TimeBank. The performance metrics shown are Micro-F1 percentage (%) and Logical Inconsistency (LI) percentage (%), with lower LI indicating better performance. The table reports these metrics under various conditions, including \"vanilla ICL,\" \"vanilla ICL with CoT,\" and \"CoT with logical constraints.\" Additional configurations like using all logical constraints, retrieved logical constraints, and post-processing effects are also evaluated.\n\nFor Vicuna-13B-PT on MAVEN-ERE:\n- The best Micro-F1 (18.0%) is achieved with CoT with logical constraints, while the lowest LI (0%) is achieved with post-processing.\n\nFor Vicuna-13B-PT on Causal-TimeBank:\n- The best Micro-F1 (12.1%) is under all logical constraints, and the lowest LI (0%) is obtained with post-processing.\n\nFor Llama2-13B-PT on MAVEN-ERE:\n- The highest Micro-F1 (26.4%) is for CoT with logical constraints, with the lowest LI (0%) from post-processing.\n\nFor Llama2-13B-PT on Causal-TimeBank:\n- The highest Micro-F1 (13.3%) is with logical constraints, while the lowest LI (0%) occurs with post-processing.\n\nEach performance metric shows an improvement indicator (e.g., +1.5%) relative to a baseline, providing a comparison of the relative benefit of different configurations.\nThe image shows a case study involving Llama-2-13B before and after pre-training (PT). It is structured as follows:\n\n- **Instruction**: An introductory statement explaining that text events are marked by \"<\" and \">\" symbols.\n\n- **Demonstration**: \n  - **Text**: An example sentence about an exhibition with marked events (\"went on\" and \"removed\").\n  - **Event Pairs**: Lists events and their sequences.\n  - **Answers**: Specifies relationships like NO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT, etc.\n\n- **Input**:\n  - **Text**: Describes an ice hockey tournament at the 1924 Winter Olympics, with marked events.\n  - **Event Pairs**: Lists sequences like \"<Championships>\" and \"<tournament>.\"\n  - **Answers**: Relations like NO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.\n\n- **Llama-2-13B and Llama-2-13B-PT Responses**:\n  - Shows the model's answers before and after pre-training.\n  - Llama-2-13B makes some errors, marked with an 'X.'\n  - Llama-2-13B-PT provides corrected answers, marked with a '✓.'\n\nThe color-coded sections are used to differentiate the stages and model responses.\nvanilla ChatGPT, 175B), which further validates the importance of teaching LLM with logic in solving reasoning tasks. \n4.4.3 Case Study .  In Figure 7, We conduct a case study of Llama- 2-13B’s answers to the same input before and after pre-training. From Figure 7 we can see that LlaMA2-13B-PT could output the correct answers after pre-training on  LLM-LR , which validates the effectiveness of our pre-training approach. \n5 RELATED WORK \n5.1 Large Language Models (LLMs) \nWe are fortunate to witness the surging development of Large Language Models (LLMs [ 3 ,  8 ,  9 ,  32 ]), and a series of work aiming to leverage the reasoning abilities of LLMs such as chain-of-thought prompting [ 19 ,  46 ,  51 ], self verification [ 18 ,  44 ], self learning [ 15 , 49 ], etc. However, recent studies show LLMs still stumble across generating hallucination and logic inconsistency [ 2 ,  13 ,  16 ,  17 ,  20 ]. To solve such challenges, our work explores teaching LLMs logical reasoning through various approaches. \n\n5.2 Event Relation Extraction (ERE) \nEvents play crucial roles in comprehending narratives, and under- standing the complex relationships between events is essential to understanding the text [ 37 ]. Thus Event Relation Extraction (ERE) tasks are fundamental information extraction (IE) tasks and support various downstream applications [ 5 ,  36 ,  50 ]. Extensive studies have been carried out on ERE tasks, including different kinds of relations such as co reference relations [ 24 ,  25 ], temporal relations [ 30 ,  40 ], causal relations [4, 6, 7], and subevent relations [1, 41]. \nThere also have been some recent explorations on how to lever- age the power of LLMs on event-related information extraction tasks [ 12 ,  27 ,  43 ]. To the best of our knowledge, however, our work is the first to (1) design elaborate experiments to evaluate the per- formance of LLMs on the ERE task, and (2) analyze the logical reasoning abilities of LLMs using ERE as an intermediate task. \n6 CONCLUSION \nIn this paper, we conduct a detailed investigation on how to enhance LLMs to produce more logically consistent answers. Specifically, we first investigate the existing issues of current LLMs in doing some complex reasoning tasks (e.g., event relation extraction and deductive reasoning). Then, we study multiple strategies to obtain and utilize logic for LLMs, including generative-based, retrieval- based, and pre training-based approaches. Based on our approach, we also contribute a synthesized dataset ( LLM-LR ) involving multi- hop reasoning for evaluation and pre-training. We show that LLMs are not logically consistent reasoners, but their performance could be improved if we explicitly teach them the logical constraints. Comprehensive quantitative and qualitative analyses have been conducted to further provide insights. "}
{"page": 8, "image_path": "doc_images/2310.09158v1_8.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning\n\nREFERENCES\n\n163)\n\nMohammed Aldawsari and Mark Finlayson. 2019. Detecting Subevents using\nDiscourse and Narrative Features. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics. Association for Computational\nLinguistics, Florence, Italy, 4780-4790. https://doi.org/10.18653/v1/P 19-1471\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan\nWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask,\nmultilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and\ninteractivity. arXiv preprint arXiv:2302.04023 (2023).\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nIn Advances in Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/\nhash/1457cOd6bfcb4967418bfb8ac1 42f64a- Abstract.html\n\nTommaso Caselli and Piek Vossen. 2017. The Event StoryLine Corpus: A New\nBenchmark for Causal and Temporal Relation Extraction. In Proceedings of the\nEvents and Stories in the News Workshop. Association for Computational Linguis-\ntics, Vancouver, Canada, 77-86. https://doi.org/10.18653/v1/W17-2711\nSnigdha Chaturvedi, Haoruo Peng, and Dan Roth. 2017. Story Comprehension\nfor Predicting What Happens Next. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computational\nLinguistics, Copenhagen, Denmark, 1603-1614. https://doi.org/10.18653/v1/D17-\n1168\n\nMeiqi Chen, Yixin Cao, Kunquan Deng, Mukai Li, Kun Wang, Jing Shao, and Yan\nZhang. 2022. ERGO: Event Relational Graph Transformer for Document-level\nEvent Causality Identification. In Proceedings of the 29th International Conference\non Computational Linguistics. International Committee on Computational Lin-\nguistics, Gyeongju, Republic of Korea, 2118-2128. https://aclanthology.org/2022.\ncoling-1.185\n\nMeiqi Chen, Yixin Cao, Yan Zhang, and Zhiwei Liu. 2023. CHEER: Centrality-\naware High-order Event Reasoning Network for Document-level Event Causality\nIdentification. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). 10804-10816.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.\nArXiv preprint abs/2204.02311 (2022). https://arxiv.org/abs/2204.02311\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\ninstruction-finetuned language models. ArXiv preprint abs/2210.11416 (2022).\nhttps://arxiv.org/abs/2210.11416\n\nWilliam F Clocksin and Christopher S$ Mellish. 2003. Programming in PROLOG.\nSpringer Science & Business Media.\n\nBruce Frederiksen. 2008. Applying expert system technology to code reuse with\nPyke. PyCon: Chicago (2008).\n\nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023. Exploring the Feasi-\nbility of ChatGPT for Event Extraction. _https://arxiv.org/abs/2303.03836\n\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer,\nMaryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. ROSCOE: A Suite of Metrics\nfor Scoring Step-by-Step Reasoning. arXiv:2212.07919 [cs.CL]\n\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large\nLanguage Models. In International Conference on Learning Representations. https:\n//openreview.net/forum?id=nZeVKeeFYf9\n\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun\nYu, and Jiawei Han. 2022. Large Language Models Can Self-Improve.\narXiv:2210.11610 [cs.CL]\n\nMyeongjun Jang and Thomas Lukasiewicz. 2023. Consistency Analysis of Chat-\nGPT. arXiv:2303.06273 [es.CL]\n\nFangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, Zhengyuan Liu,\nand Nancy F Chen. 2023. LogicLLM: Exploring Self-supervised Logic-enhanced\nTraining for Large Language Models. arXiv preprint arXiv:2305.13718 (2023).\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula,\nRonan Le Bras, and Yejin Choi. 2022. Maieutic Prompting: Logically Consistent\nReasoning with Recursive Explanations. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates, 1266-1279. https://aclanthology.\norg/2022.emnlp-main.82\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. ArXiv preprint\n\n20.\n\n21\n\n22,\n\n23,\n\n24\n\n25,\n\n26\n\n27\n\n28\n\n29\n\n30,\n\n31\n\n32)\n\n33,\n\n34\n\n35,\n\n36.\n\n(37]\n\n[38]\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nabs/2205.11916 (2022). https://arxiv.org/abs/2205.11916\n\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang.\n2023. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint\narXiv:2304,03439 (2023).\n\nKang Liu, Yubo Chen, Jian Liu, Xinyu Zuo, and Jun Zhao. 2020. Extracting Events\nand Their Relations from Texts: A Survey on Recent Research Progress and\nChallenges. AJ Open 1 (2020), 22-39. https://doi.org/10.1016/j.aiopen.2021.02.004\nJohn W Lloyd. 2012. Foundations of logic programming. Springer Science &\nBusiness Media.\n\nIlya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\nIn 7th International Conference on Learning Representations, ICLR 2019, New Or-\nleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?\nid=Bkg6RiCqY7\n\nJing Lu and Vincent Ng. 2021. Conundrums in Event Coreference Resolution:\nMaking Sense of the State of the Art. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computational\nLinguistics, Online and Punta Cana, Dominican Republic, 1368-1380. https:\n//doi.org/10.18653/v1/2021.emnlp-main.103\n\nYaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, and Le Sun. 2022. End-to-end\nneural event coreference resolution. Artificial Intelligence 303 (2022), 103632.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Mar-\nianna Apidianaki, and Chris Callison-Burch. 2023. Faithful Chain-of-Thought\nReasoning. ArXiv preprint abs/2301.13379 (2023). https://arxiv.org/abs/2301.13379\nYubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large Language\nModel Is Not a Good Few-shot Information Extractor, but a Good Reranker for\nHard Samples! arXiv:2303.08559 [cs.CL]\n\nParamita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014.\nAnnotating Causality in the TempEval-3 Corpus. In Proceedings of the EACL\n2014 Workshop on Computational Approaches to Causality in Language (CAtoCL).\nAssociation for Computational Linguistics, Gothenburg, Sweden, 10-19. https:\n//doi.org/10.3115/v1/W 14-0702\n\nParamita Mirza and Sara Tonelli. 2014. An Analysis of Causality between Events\nand its Relation to Temporal Information. In Proceedings of COLING 2014, the 25th\nInternational Conference on Computational Linguistics: Technical Papers. Dublin\nCity University and Association for Computational Linguistics, Dublin, Ireland,\n2097-2106. https://aclanthology.org/C14- 1198\n\nQiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018. Joint Reasoning for\nTemporal and Causal Relations. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Association for\nComputational Linguistics, Melbourne, Australia, 2278-2288. https://doi.org/10.\n18653/v1/P18-1212\n\nTim O'Gorman, Kristin Wright-Bettner, and Martha Palmer. 2016. Richer Event\nDescription: Integrating event coreference with temporal, causal and bridging\nannotation. In Proceedings of the 2nd Workshop on Computing News Storylines\n(CNS 2016). Association for Computational Linguistics, Austin, Texas, 47-56.\nhttps://doi.org/10.18653/v1/W16-5706\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35 (2022), 27730-27744.\n\nLiangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic-\nLM: Empowering Large Language Models with Symbolic Solvers for Faithful\nLogical Reasoning. ArXiv preprint abs/2305.12295 (2023). https://arxiv.org/abs/\n2305.12295\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116\n(2023).\n\nDavid L Poole and Alan K Mackworth. 2010. Artificial Intelligence: foundations of\ncomputational agents. Cambridge University Press.\n\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas\nLourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019.\nATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. In The\nThirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First\nInnovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth\nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019,\nHonolulu, Hawaii, USA, January 27 - February 1, 2019. AAAI Press, 3027-3035.\nhttps://doi.org/10.1609/aaai.v33i01.33013027\n\nBeth M. Sundheim. 1991. Evaluating Text Understanding Systems. In Speech and\nNatural Language: Proceedings of a Workshop Held at Pacific Grove, California,\nFebruary 19-22, 1991. https://aclanthology.org/H91- 1093\n\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Gener-\nating Implications, Proofs, and Abductive Statements over Natural Language.\nIn Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.\nAssociation for Computational Linguistics, Online, 3621-3634. https://doi.org/\n10.18653/v1/2021 findings-acl.317\n", "vlm_text": "REFERENCES \n[1]  Mohammed Aldawsari and Mark Finlayson. 2019. Detecting Subevents using Discourse and Narrative Features. In  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Florence, Italy, 4780–4790. https://doi.org/10.18653/v1/P19-1471\n\n [2]  Yejin Bang, Samuel Cah yaw i jaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al .  2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.  arXiv preprint arXiv:2302.04023  (2023).\n\n [3]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In  Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/ hash/1457 c 0 d 6 bfc b 4967418 b fb 8 ac 142 f 64 a-Abstract.html\n\n [4]  Tommaso Caselli and Piek Vossen. 2017. The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction. In  Proceedings of the Events and Stories in the News Workshop . Association for Computational Linguis- tics, Vancouver, Canada, 77–86. https://doi.org/10.18653/v1/W17-2711\n\n [5]  Snigdha Chaturvedi, Haoruo Peng, and Dan Roth. 2017. Story Comprehension for Predicting What Happens Next. In  Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Copenhagen, Denmark, 1603–1614. https://doi.org/10.18653/v1/D17- 1168\n\n [6]  Meiqi Chen, Yixin Cao, Kunquan Deng, Mukai Li, Kun Wang, Jing Shao, and Yan Zhang. 2022. ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification. In  Proceedings of the 29th International Conference on Computational Linguistics . International Committee on Computational Lin- guistics, Gyeongju, Republic of Korea, 2118–2128. https://a cl anthology.org/2022. coling-1.185\n\n [7]  Meiqi Chen, Yixin Cao, Yan Zhang, and Zhiwei Liu. 2023. CHEER: Centrality- aware High-order Event Reasoning Network for Document-level Event Causality Identification. In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 10804–10816.\n\n [8]  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se- bastian Gehrmann, et al .  2022. Palm: Scaling language modeling with pathways. ArXiv preprint  abs/2204.02311 (2022). https://arxiv.org/abs/2204.02311\n\n [9]  Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al .  2022. Scaling instruction-finetuned language models.  ArXiv preprint  abs/2210.11416 (2022). https://arxiv.org/abs/2210.11416\n\n [10]  William F Clocksin and Christopher S Mellish. 2003.  Programming in PROLOG . Springer Science & Business Media.\n\n [11]  Bruce Frederiksen. 2008. Applying expert system technology to code reuse with Pyke.  PyCon: Chicago  (2008).\n\n [12]  Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023. Exploring the Feasi- bility of ChatGPT for Event Extraction. https://arxiv.org/abs/2303.03836\n\n [13]  Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Z ett le moyer, Maryam Fazel-Zarandi, and Asli Cel i kyi l maz. 2022. ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. arXiv:2212.07919 [cs.CL]\n\n [14]  Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In  International Conference on Learning Representations . https: //openreview.net/forum?id=n ZeV Kee FY f 9\n\n [15]  Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large Language Models Can Self-Improve. arXiv:2210.11610 [cs.CL]\n\n [16]  Myeongjun Jang and Thomas L ukasiewicz. 2023. Consistency Analysis of Chat- GPT. arXiv:2303.06273 [cs.CL]\n\n [17]  Fangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, Zhengyuan Liu, and Nancy F Chen. 2023. LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models.  arXiv preprint arXiv:2305.13718  (2023).\n\n [18]  Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhaga va tula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 1266–1279. https://a cl anthology. org/2022.emnlp-main.82\n\n [19]  Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.  ArXiv preprint abs/2205.11916 (2022). https://arxiv.org/abs/2205.11916\n\n \n[20]  Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023. Evaluating the logical reasoning ability of chatgpt and gpt-4.  arXiv preprint arXiv:2304.03439  (2023).\n\n [21]  Kang Liu, Yubo Chen, Jian Liu, Xinyu Zuo, and Jun Zhao. 2020. Extracting Events and Their Relations from Texts: A Survey on Recent Research Progress and Challenges.  AI Open  1 (2020), 22–39. https://doi.org/10.1016/j.aiopen.2021.02.004\n\n [22]  John W Lloyd. 2012.  Foundations of logic programming . Springer Science & Business Media.\n\n [23]  Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regular iz ation. In  7th International Conference on Learning Representations, ICLR 2019, New Or- leans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum? id=Bkg6RiCqY7\n\n[24]  Jing Lu and Vincent Ng. 2021. Conundrums in Event Co reference Resolution: Making Sense of the State of the Art. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 1368–1380. https: //doi.org/10.18653/v1/2021.emnlp-main.103\n\n [25]  Yaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, and Le Sun. 2022. End-to-end neural event co reference resolution.  Artificial Intelligence  303 (2022), 103632.\n\n [26]  Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Mar- ianna Apidianaki, and Chris Callison-Burch. 2023. Faithful Chain-of-Thought Reasoning.  ArXiv preprint  abs/2301.13379 (2023). https://arxiv.org/abs/2301.13379\n\n [27]  Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples! arXiv:2303.08559 [cs.CL]\n\n [28]  Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating Causality in the TempEval-3 Corpus. In  Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL) . Association for Computational Linguistics, Gothenburg, Sweden, 10–19. https: //doi.org/10.3115/v1/W14-0702\n\n [29]  Paramita Mirza and Sara Tonelli. 2014. An Analysis of Causality between Events and its Relation to Temporal Information. In  Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers . Dublin City University and Association for Computational Linguistics, Dublin, Ireland, 2097–2106. https://aclanthology.org/C14-1198\n\n [30]  Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018. Joint Reasoning for Temporal and Causal Relations. In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Melbourne, Australia, 2278–2288. https://doi.org/10. 18653/v1/P18-1212\n\n [31]  Tim O’Gorman, Kristin Wright-Bettner, and Martha Palmer. 2016. Richer Event Description: Integrating event co reference with temporal, causal and bridging annotation. In  Proceedings of the 2nd Workshop on Computing News Storylines (CNS 2016) . Association for Computational Linguistics, Austin, Texas, 47–56. https://doi.org/10.18653/v1/W16-5706\n\n [32]  Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .  2022. Training language models to follow instructions with human feedback.  Advances in Neural Information Processing Systems  35 (2022), 27730–27744.\n\n [33]  Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic- LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning.  ArXiv preprint  abs/2305.12295 (2023). https://arxiv.org/abs/ 2305.12295\n\n [34]  Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only.  arXiv preprint arXiv:2306.01116 (2023).\n\n [35]  David L Poole and Alan K Mackworth. 2010.  Artificial Intelligence: foundations of computational agents . Cambridge University Press.\n\n [36]  Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhaga va tula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. In  The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 . AAAI Press, 3027–3035. https://doi.org/10.1609/aaai.v33i01.33013027\n\n [37]  Beth M. Sundheim. 1991. Evaluating Text Understanding Systems. In  Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, February 19-22, 1991 . https://a cl anthology.org/H91-1093\n\n [38]  Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. Proof Writer: Gener- ating Implications, Proofs, and Abductive Statements over Natural Language. In  Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 . Association for Computational Linguistics, Online, 3621–3634. https://doi.org/ 10.18653/v1/2021.findings-acl.317 "}
{"page": 9, "image_path": "doc_images/2310.09158v1_9.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY\n\n39\n\n40.\n\n41\n\n42\n\n43,\n\n44\n\n45,\n\n46\n\n47\n\n48\n\n49\n\n50.\n\n51\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n\nHaoyu Wang, Muhao Chen, Hongming Zhang, and Dan Roth. 2020. Joint Con-\nstrained Learning for Event-Event Relation Extraction. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP). Asso-\nciation for Computational Linguistics, Online, 696-706. https://doi-org/10.18653/\nv1/2020.emnlp-main.51\n\nHaoyu Wang, Hongming Zhang, Muhao Chen, and Dan Roth. 2021. Learning\nConstraints and Descriptive Segmentation for Subevent Detection. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, Online and Punta Cana, Dominican\nRepublic, 5216-5226. https://doi.org/10.18653/v1/2021.emnlp-main.423\nXiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han,\nLei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, and Jie Zhou. 2022. MAVEN-ERE: A Uni-\nfied Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent\nRelation Extraction. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing. Association for Computational Linguistics, Abu\nDhabi, United Arab Emirates, 926-941. https://aclanthology.org/2022.emnlp-\nmain.60\n\nXingyao Wang, Sha Li, and Heng Ji. 2022. Code4Struct: Code Generation for\nFew-Shot Structured Prediction from Natural Language. arXiv:2210.12810 [es.CL]\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\n2022. Self-consistency improves chain of thought reasoning in language models.\nArXiv preprint abs/2203.11171 (2022). https://arxiv.org/abs/2203.11171\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William\nFedus. 2022. Emergent Abilities of Large Language Models. Trans. Mach. Learn.\nRes. (2022).\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le,\nand Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large\nlanguage models. ArXiv preprint abs/2201.11903 (2022). https://arxiv.org/abs/\n2201.11903\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\nXia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. In NeurIPS.\n\nFangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023.\nAre Large Language Models Really Good Logical Reasoners? A Comprehensive\nEvaluation From Deductive, Inductive and Abductive Views. ArXiv preprint\nabs/2306.09841 (2023). https://arxiv.org/abs/2306.09841\n\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrap-\nping Reasoning With Reasoning. In Advances in Neural Information Processing\nSystems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho\n(Eds.). https://openreview.net/forum?id=_3ELRdg2sgl\n\nHongming Zhang, Daniel Khashabi, Yangqiu Song, and Dan Roth. 2020. Tran-\nsOMCS: From Linguistic Graphs to Commonsense Knowledge. In Proceedings of\nthe Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI\n2020, Christian Bessiere (Ed.). ijcai.org, 4004-4010. https://doi.org/10.24963/ijcai.\n2020/554\n\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain\nof thought prompting in large language models. ArXiv preprint abs/2210.03493\n(2022). https://arxiv.org/abs/2210.03493\n\nChen, et al.\n", "vlm_text": "[39]  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas- mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos- ale, et al .  2023. Llama 2: Open foundation and fine-tuned chat models.  arXiv preprint arXiv:2307.09288  (2023).\n\n [40]  Haoyu Wang, Muhao Chen, Hongming Zhang, and Dan Roth. 2020. Joint Con- strained Learning for Event-Event Relation Extraction. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Asso- ciation for Computational Linguistics, Online, 696–706. https://doi.org/10.18653/ v1/2020.emnlp-main.51\n\n [41]  Haoyu Wang, Hongming Zhang, Muhao Chen, and Dan Roth. 2021. Learning Constraints and Descriptive Segmentation for Subevent Detection. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 5216–5226. https://doi.org/10.18653/v1/2021.emnlp-main.423\n\n [42]  Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, and Jie Zhou. 2022. MAVEN-ERE: A Uni- fied Large-scale Dataset for Event Co reference, Temporal, Causal, and Subevent Relation Extraction. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 926–941. https://a cl anthology.org/2022.emnlp- main.60\n\n [43]  Xingyao Wang, Sha Li, and Heng Ji. 2022. Code 4 Struct: Code Generation for Few-Shot Structured Prediction from Natural Language. arXiv:2210.12810 [cs.CL]\n\n [44] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. ArXiv preprint  abs/2203.11171 (2022). https://arxiv.org/abs/2203.11171\n\n [45]  Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models.  Trans. Mach. Learn. Res.  (2022).\n\n [46]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models.  ArXiv preprint  abs/2201.11903 (2022). https://arxiv.org/abs/ 2201.11903\n\n [47]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In  NeurIPS .\n\n [48]  Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023. Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views.  ArXiv preprint abs/2306.09841 (2023). https://arxiv.org/abs/2306.09841\n\n [49]  Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrap- ping Reasoning With Reasoning. In  Advances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=3 EL Rd g 2 sgI\n\n [50]  Hongming Zhang, Daniel Khashabi, Yangqiu Song, and Dan Roth. 2020. Tran- sOMCS: From Linguistic Graphs to Commonsense Knowledge. In  Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020 , Christian Bessiere (Ed.). ijcai.org, 4004–4010. https://doi.org/10.24963/ijcai. 2020/554\n\n [51]  Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models.  ArXiv preprint  abs/2210.03493 (2022). https://arxiv.org/abs/2210.03493 "}
{"page": 10, "image_path": "doc_images/2310.09158v1_10.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning\n\nA UNDERSTANDING EVENT RELATIONS\n\nThere are four kinds of widely-used event relations: coreference,\ntemporal, causal, and subevent relations [31, 42].\n\n(1) Coreference relations between events occur when multiple event\nmentions in a text refer to the same underlying event. We call\nthese event mentions cluster.\nTemporal relations refer to the temporal ordering of events based\non their occurrence in time. In this paper, we consider seven\ndifferent types of temporal relations:\ne NO_TEMPORAL: if there is no clear temporal relation be-\ntween event A and B.\nBEFORE: if event A happened completely before event B.\nOVERLAP: if event A has an overlap with event B.\nCONTAINS: if event A’s time contains event B’s time.\nSIMULTANEOUS: if events A and B happen at the same time.\nENDS-ON: if event A ends when event B starts.\nBEGINS-ON: if event A and event B start at the same time,\nbut end at different times.\n\nIn Figure 8, we list all the types of temporal relations and illus-\n\ntrate their distinctions on a unified timeline.\n(3) Causal relations refer to that one event (the cause) brings about\nor influences the occurrence of another event (the effect). They\ncan be classified into two different types: CAUSE relation where\nthe tail event is inevitable given the head event, and PRECON-\nDITION where the tail event would not have happened if the\nhead event had not happened.\nSubevent relations refer to the connection where one event (the\nsubevent) is a component or a smaller part of another event (the\nmain event). Identifying and understanding subevent relations\nhelps to reveal the underlying hierarchy and organizational\nstructure of events in a given text.\n\n(2\n\n(4\n\nEvent Relation Extraction. Event Relation Extraction (ERE) in-\ncludes identifying coreference, temporal, causal, and subevent rela-\ntions between every two events in the text. We formulate ERE as a\nmulti-classification problem, determining one label (relation) for\neach of these four relation types. For coreference relations, the labels\n€{NO_COREFERENCE, COREFERENCE}; for temporal relations,\nthe labels € {NO_TEMPORAL, BEFORE, OVERLAP, CONTAINS,\nSIMULTANEOUS, ENDS-ON, BEGINS-ON}; for causal relations, the\nlabels € {NO_CAUSAL, PRECONDITION, CAUSE}; for subevent\nrelations, the labels € {NO_SUBEVENT, SUBEVENT}.\n\nB_ TRAINING DETAILS OF ROBERTA-LARGE\nON TWO TASKS\n\nB.1 Dataset Construction\n\nMAVEN-ERE. contains 4,480 documents, 103,193 events corefer-\nence chains, 1,216,217 temporal relations, 57,992 causal relations,\nand 15,841 subevent relations, which is larger than existing datasets\nof all the ERE tasks by at least an order of magnitude [42]. MAVEN-\nERE has released the train and valid set, but does not release the\nground-truth test set, so we randomly split its train set into train/-\nvalid sets with a ratio of 8:2, and then use its original valid set as\nthe new test set.\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nRelation(A, B)\n\nBEFORE\n\nOVERLAP\n\nCONTAINS\n\nSIMULTANEOUS\n\nENDS-ON\n\nBEGINS-ON\n\nFigure 8: Interpretations of the temporal relation between\ntwo events A and B. Brackets represent time intervals along\nthe time axis.\n\nCausal-TimeBank. contains 184 documents, 6,813 events, and\n7,608 event pairs [29]. Among them, 318 event pairs are annotated\nwith causal relations, and 6,115 event pairs are annotated with\ntemporal relations. Due to Causal-TimeBank does not split train/-\nvalid/test sets, we randomly split it to train/valid/test sets with a\nratio of 6:1:3. We do not evaluate coreference and subevent relations\nin the Causal-TimeBank dataset since there are no annotations for\nthese two relation types.\n\nProof Writer. We use the OWA subset of Proof Writer and con-\nsider the hardest 5-hop subset. The training, valid, and test sets\ncontain 3000, 600, and 600 samples, respectively.\n\nB.2 Experimental Setup\n\nOur experiments include two settings. (1) fully fine-tuned: we fine-\ntune SLMs with complete and abundant samples. This setting is for\nreference to see the performance limit of SLMs. (2) one-shot: we\nsample only one example for each label and construct a tiny train-\ning set. This setting is for direct comparison with our experiments\non LLMs (similar training/demonstration sample number).\n\nWe implement vanilla fine-tuning approaches on three datasets\nand use RoBERTa-Large as backbones. We run each experiment on\na single NVIDIA V100 GPU. We adopt the AdamW [23] optimizer\nwith a linear scheduler and 0.1 warm-up steps. We set the weight-\ndecay coefficient as 1e-5 and maximum gradient norms as 1.0. We\nset the batch size as 16 with 20 or 50 epochs. We set the maximum\ninput length as 256 and the learning rate as 2e-5.\n\nC LOGICAL CONSTRAINTS BETWEEN TWO\nEVENTS\n\nIn Table 4, we provide a comprehensive set of logical constraints for\nthe relations between two events to assess their logical consistency.\nWe also manually design description text for each constraint to let\nLLMs follow the prompt. As shown in Table 6, COREFERENCE(A,\nB) > “TEMPORAL(A, B), sCAUSAL(A, B), -SUBEVENT(A, B)\nindicates that \"if event A and event B have a coreference relation,\nthey will not have temporal, causal, and subevent relations\".\n", "vlm_text": "A UNDERSTANDING EVENT RELATIONS \nThere are four kinds of widely-used event relations: co reference, temporal, causal, and subevent relations [31, 42].\n\n \n(1)  Co reference relations  between events occur when multiple event mentions in a text refer to the same underlying event. We call these event mentions  cluster .\n\n (2)  Temporal relations  refer to the temporal ordering of events based on their occurrence in time. In this paper, we consider seven different types of temporal relations:  NO_TEMPO AL: i there is no clear temporal relation be- • tween event  𝐴 and  𝐵 . •  BEFORE: if event    $A$  appened completely before vent  $B$  . •  OVERLAP: if event    $A$  as an overlap with ev t  𝐵 .  CONTAINS: if event ’s time contains event ’s time. •  𝐴  𝐵 •  SIMULTANEOUS: i vents A and  $B$  hap en at the same time.  ENDS-ON: if event ends when e nt starts. •  𝐴  𝐵  BEGINS-ON: if event and event start at the same time, •  𝐴  𝐵 but end at different times. In Figure 8, we list all the types of temporal relations and illus- trate their distinctions on a unified timeline.\n\n (3)  Causal relations  refer to that one event (the cause) brings about or influences the occurrence of another event (the effect). They can be classified into two different types:  CAUSE  relation where the tail event is inevitable given the head event, and  PRECON- DITION  where the tail event would not have happened if the head event had not happened.\n\n (4)  Subevent relations  refer to the connection where one event (the subevent) is a component or a smaller part of another event (the main event). Identifying and understanding subevent relations helps to reveal the underlying hierarchy and organizational structure of events in a given text. \nEvent Relation Extraction.  Event Relation Extraction (ERE) in- cludes identifying co reference, temporal, causal, and subevent rela- tions between every two events in the text. We formulate ERE as a multi-classification problem, determining one label (relation) for each of these four relation types. For co reference relations, the labels {NO CO REFERENCE, CO REFERENCE}; for temporal relations, ∈ the labels  $\\in\\{\\mathrm{NO}_{-}$  TEMPORAL, BEFORE, OVERLAP, CONTAINS, SIMULTANEOUS, ENDS-ON, BEGINS-ON}; for causal relations, the labels  $\\in$  {NO_CAUSAL, PRECONDITION, CAUSE}; for subevent relations, the labels  $\\in\\{\\sf N O_{\\tau}}$  _SUBEVENT, SUBEVENT}. \nB TRAINING DETAILS OF ROBERTA-LARGE ON TWO TASKS \nB.1 Dataset Construction \nMAVEN-ERE.  contains 4,480 documents, 103,193 events corefer- ence chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude [ 42 ]. MAVEN- ERE has released the train and valid set, but does not release the ground-truth test set, so we randomly split its train set into train/- valid sets with a ratio of 8:2, and then use its original valid set as the new test set. \nThe table visually represents temporal relationships between two events or intervals, A and B, along a timeline. It shows different ways in which the interval B can relate to interval A in terms of timing. Here are the temporal relations depicted in the table:\n\n1. **BEFORE:** Interval B occurs completely before interval A on the timeline.\n2. **OVERLAP:** Interval B begins before interval A ends, creating an overlap between the two.\n3. **CONTAINS:** Interval A completely contains interval B, meaning B occurs entirely within the timeframe of A.\n4. **SIMULTANEOUS:** Intervals A and B occur simultaneously, starting and ending at the same time.\n5. **ENDS-ON:** Interval B ends at the same time as interval A but starts before A.\n6. **BEGINS-ON:** Interval B starts at the same time as interval A but ends after A.\n\nThese relationships help in understanding the sequencing and interaction of events over time.\nFigure 8: Interpretations of the temporal relation between two events A and B. Brackets represent time intervals along the time axis. \nCausal-TimeBank.  contains 184 documents, 6,813 events, and 7,608 event pairs [ 29 ]. Among them, 318 event pairs are annotated with causal relations, and 6,115 event pairs are annotated with temporal relations. Due to Causal-TimeBank does not split train/- valid/test sets, we randomly split it to train/valid/test sets with a ratio of 6:1:3. We do not evaluate co reference and subevent relations in the Causal-TimeBank dataset since there are no annotations for these two relation types. \nProof Writer.  We use the OWA subset of Proof Writer and con- sider the hardest 5-hop subset. The training, valid, and test sets contain 3000, 600, and 600 samples, respectively. \nB.2 Experimental Setup \nOur experiments include two settings. (1) fully fine-tuned: we fine- tune SLMs with complete and abundant samples. This setting is for reference to see the performance limit of SLMs. (2) one-shot: we sample only one example for each label and construct a tiny train- ing set. This setting is for direct comparison with our experiments on LLMs (similar training/demonstration sample number). \nWe implement vanilla fine-tuning approaches on three datasets and use RoBERTa-Large as backbones. We run each experiment on a single NVIDIA V100 GPU. We adopt the AdamW [ 23 ] optimizer with a linear scheduler and 0.1 warm-up steps. We set the weight- decay coefficient as 1e-5 and maximum gradient norms as 1.0. We set the batch size as 16 with 20 or 50 epochs. We set the maximum input length as 256 and the learning rate as 2e-5. \nC LOGICAL CONSTRAINTS BETWEEN TWO EVENTS \nIn Table 4, we provide a comprehensive set of logical constraints for the relations between two events to assess their logical consistency. We also manually design description text for each constraint to let LLMs follow the prompt. As shown in Table 6, CO REFERENCE( 𝐴 ,  $B)\\to\\lnot\\mathrm{EMPORAL}(A,\\,B)$  ,    $\\neg\\mathrm{{CASAL}}(A,\\,B)$  ,  ¬ SUBEVENT( 𝐴 ,  𝐵 ) indicates that \"if event  𝐴 and event  𝐵 have a co reference relation, they will not have temporal, causal, and subevent relations\". "}
{"page": 11, "image_path": "doc_images/2310.09158v1_11.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY\n\nChen, et al.\n\nIf Relation(A, B) Then Relation (A, B) Then Relation (B, A)\nCOREFERENCE =TEMPORAL, =CAUSAL, ~SUBEVENT COREFERENCE\naTEMPORAL =aCAUSAL, ~SUBEVENT /\nBEFORE =COREFERENCE, ~SUBEVENT aTEMPORAL\nOVERLAP =COREFERENCE, ~SUBEVENT aTEMPORAL\nCONTAINS —=COREFERENCE, ~CAUSAL aTEMPORAL\nSIMULTANEOUS “=COREFERENCE, =CAUSAL, -SUBEVENT SIMULTANEOUS\nENDS-ON “=COREFERENCE, =CAUSAL, -SUBEVENT aTEMPORAL\nBEGINS-ON “=COREFERENCE, =CAUSAL, -SUBEVENT BEGINS-ON\nCAUSE =COREFERENCE, BEFORE V OVERLAP, -SUBEVENT aTEMPORAL\nPRECONDITION =COREFERENCE, BEFORE V OVERLAP, -SUBEVENT aTEMPORAL\nSUBEVENT =COREFERENCE, CONTAINS =CAUSAL aTEMPORAL\n\nTable 4: Logical Constraints of relations between two events, where — denotes \"NOT\", V denotes \"OR\".\n\nC.1 An Example of Detecting Conflicts and\nRetrieving Relevant Constraints\n\nAs described above, for the ERE task, we meticulously collect 11\nlogical constraints covering all relations between two events. These\nconstraints serve as our benchmark to identify inconsistencies in\nthe predictions made by LLMs.\n\nLet us consider an illustrative example. If LLM produces an\nanswer such as “NO_COREFERENCE, SIMULTANEOUS, CAUSE,\nNO_SUBEVENT” (refer to Figure 5), we could detect the inconsis-\ntency between “SIMULTANEOUS” and “CAUSE”, as shown in Table\n4:\n\ne A “SIMULTANEOUS’ relation implies a “NO_CAUSAL” (>CAUSAL)\n\nrelation.\ne Conversely, a “CAUSE” relation suggests the presence of\neither a “BEFORE” or an “OVERLAP?” relation.\n\nGiven this, “SIMULTANEOUS” and “CAUSE” are inherently con-\ntradictory, and they cannot coexist in a consistent prediction. To\nrectify this, we retrieve the associated textual description from Ta-\nble 6. Specifically, the statements “If event A CAUSEs event B, then\nevent A happens BEFORE or OVERLAP event B ..” and “If event\nA and event B happen SIMULTANEOUSIy, then they won’t have\n\ncoreference, causal, and subevent relations ...” are integrated into\nthe LLM’s instruction.\n\nC.2 An Example of Post-processing\n\nAs shown in Figure 5, if LLMs predict the relations between two\nevents as “NO_COREFERENCE, SIMULTANEOUS, CAUSE,\nNO_SUBEVENT”, we can detect that “SIMULTANEOUS” and “CAUSE”\nare in conflict according to the logical constraints. In order to elim-\ninate conflicts, one relation can be fixed first, and then the other\nrelation can be randomly decided by the candidates that do not\nconflict with the current relation. For example, when the fixed tem-\nporal relation is “SIMULTANEOUS”, the causal relations can only\nbe “NO_CAUSAL”, while when the fixed causal relation is “CAUSE”,\nthe temporal relation can be either “BEFORE” or “OVERLAP”. We\nalso add a negative option “NO_COREFERENCE, NO_TEMPORAL,\nNO_CAUSAL, NO_SUBEVENT” to the candidate set because it is\npossible that neither relation exits. Finally, we randomly select one\noption from:\n\ne NO_COREFERENCE, SIMULTANEOUS, NO_CAUSAL,\n\nNO_SUBEVENT\n\nNO_COREFERENCE, OVERLAP, CAUSE, NO_SUBEVENT\n\nNO_COREFERENCE, BEFORE, CAUSE, NO_SUBEVENT\n\ne NO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL,\nNO_SUBEVENT\n\nas the ultimate answer, thus ensuring that the results must be\nlogically consistent (i.e., LI = 0).\n\nD LOGICAL CONSTRAINTS AMONG THREE\nEVENTS\n\nWe provide a comprehensive set of 39 logical constraints for the\nrelations among three events in Table 7. We also manually design\nprompt for each constraint, as shown in Table 8.\n\nD.1 Pseudo Code of Logic Programming\n\nOnce obtain 11 constraints between two events and 39 constraints\namong three events, we apply logic programming to automatically\nreason new event relations by inputting known constraints and\nrelations. The pseudo-code mentioned in the main context is shown\nin Algorithm 1.\n\nE CASE STUDY ON SELF-GENERATED\nLOGICAL CONSTRAINTS\n\nIn the main context, we have found that directly using CoT to infer\nlogic does not help much for ERE tasks. One possible reason is that\nthe inherent issues may lead to the failure of LLM in the precise\nrationale generation. To further illustrate an intuitive impression,\nwe conduct a case study on MAVEN-ERE and find that the logical\nconstraints generated by LLMs themselves are often inaccurate in\ncontent. As shown in Figure 9, ChatGPT could follow the logical\nconstraint provided in the demonstration to a certain extent. How-\never, it wrongly applies this to other relations — knowing that event\nA is event B’s precondition, it is wrong to think that event B will\ncause event A. Actually, according to the logical constraints in Ta-\nble 4, the relations between (B, A) should be “NO_COREFERENCE,\nNO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT”.\n", "vlm_text": "The table presents relational logic for events between entities A and B, organized into three columns:\n\n1. **If Relation(A, B)**: Lists initial conditions or event relationships between A and B, such as:\n   - COREFERENCE\n   - ¬TEMPORAL\n   - BEFORE\n   - OVERLAP\n   - CONTAINS\n   - SIMULTANEOUS\n   - ENDS-ON\n   - BEGINS-ON\n   - CAUSE\n   - PRECONDITION\n   - SUBEVENT\n\n2. **Then Relation (A, B)**: Lists resulting relations when the initial conditions are met, denoted by:\n   - Negatives of conditions (¬) like TEMPORAL, CAUSAL, SUBEVENT, COREFERENCE\n   - Combinations with logical operators like v (or), such as ¬CORE, BEFORE v OVERLAP\n\n3. **Then Relation (B, A)**: Represents the resultant relations from the perspective of B to A:\n   - Includes conditions like COREFERENCE, SIMULTANEOUS, BEGINS-ON\n   - Negatives like ¬TEMPORAL\n\nThis table essentially describes logical outcomes for certain types of event relations between two entities.\n NO CO REFERENCE, SIMULTANEOUS, NO_CAUSAL, • NO SUB EVENT  NO CO REFERENCE, OVERLAP, CAUSE, NO SUB EVENT •  NO CO REFERENCE, BEFORE, CAUSE, NO SUB EVENT •  NO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, • NO SUB EVENT \nC.1 An Example of Detecting Conflicts and Retrieving Relevant Constraints \nAs described above, for the ERE task, we meticulously collect 11 logical constraints covering all relations between two events. These constraints serve as our benchmark to identify inconsistencies in the predictions made by LLMs. \nLet us consider an illustrative example. If LLM produces an answer such as “NO CO REFERENCE, SIMULTANEOUS, CAUSE, NO SUB EVENT” (refer to Figure 5), we could detect the inconsis- tency between “SIMULTANEOUS” and “CAUSE”, as shown in Table 4: \nas the ultimate answer, thus ensuring that the results must be logically consistent (i.e.,  $\\mathrm{LI}=0)$  ). \nD LOGICAL CONSTRAINTS AMONG THREE EVENTS \n A “SIMULTANEOUS” relation implies a “NO_CAUSAL” (¬CAUSAL) • relation.  Conversely, a “CAUSE” relation suggests the presence of • either a “BEFORE” or an “OVERLAP” relation. \nWe provide a comprehensive set of 39 logical constraints for the relations among three events in Table 7. We also manually design prompt for each constraint, as shown in Table 8. \nGiven this, “SIMULTANEOUS” and “CAUSE” are inherently con- tradictory, and they cannot coexist in a consistent prediction. To rectify this, we retrieve the associated textual description from Ta- ble 6. Specifically, the statements “If event  𝐴 CAUSEs event  $B$  , then event  $A$  happens BEFORE or OVERLAP event  𝐵 ...” and “If event  $A$  and event  $B$  happen SIMULTANEOUSly, then they won’t have co reference, causal, and subevent relations ...” are integrated into the LLM’s instruction. \nD.1 Pseudo Code of Logic Programming \nOnce obtain 11 constraints between two events and 39 constraints among three events, we apply logic programming to automatically reason new event relations by inputting known constraints and relations. The pseudo-code mentioned in the main context is shown in Algorithm 1. \nE CASE STUDY ON SELF-GENERATED LOGICAL CONSTRAINTS \nC.2 An Example of Post-processing \nAs shown in Figure 5, if LLMs predict the relations between two events as “NO CO REFERENCE, SIMULTANEOUS, CAUSE, NO SUB EVENT”, we can detect that “SIMULTANEOUS” and “CAUSE” are in conflict according to the logical constraints. In order to elim- inate conflicts, one relation can be fixed first, and then the other relation can be randomly decided by the candidates that do not conflict with the current relation. For example, when the fixed tem- poral relation is “SIMULTANEOUS”, the causal relations can only be “NO_CAUSAL”, while when the fixed causal relation is “CAUSE”, the temporal relation can be either “BEFORE” or “OVERLAP”. We also add a negative option “NO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENT” to the candidate set because it is possible that neither relation exits. Finally, we randomly select one option from: \nIn the main context, we have found that directly using CoT to infer logic does not help much for ERE tasks. One possible reason is that the inherent issues may lead to the failure of LLM in the precise rationale generation. To further illustrate an intuitive impression, we conduct a case study on MAVEN-ERE and find that the logical constraints generated by LLMs themselves are often inaccurate in content. As shown in Figure 9, ChatGPT could follow the logical constraint provided in the demonstration to a certain extent. How- ever, it wrongly applies this to other relations — knowing that event 𝐴 is event    $B^{\\dagger}$  ’s precondition, it is wrong to think that event  $B$  will cause event  𝐴 . Actually, according to the logical constraints in Ta- ble 4, the relations between  $(B,A)$   should be “NO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENT”. "}
{"page": 12, "image_path": "doc_images/2310.09158v1_12.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning,\n\nAlgorithm 1 An Example of 3-hop Reasoning\n\nInitialize the knowledge base with facts and rules\nKnowledge Base:\nFact: BEFORE(A, B)\nFact: SIMULTANEOUS(B, C)\nFact: OVERLAP(C, D)\nRule: BEFORE «- BEFORE ( SIMULTANEOUS\n\nRule: BEFORE «- BEFORE \\ OVERLAP\nRule: OVERLAP <—- SIMULTANEOUS A OVERLAP\n\nInitialize the logic engine with the query\nQuery: BEFORE(A, D)?\n\nwhile obtain new facts do\nfor each rule r of the Knowledge Base do\nif r’s premise is satisfied by the current known facts then\nAdd r’s conclusion to the knowledge base\nend if\nend for\nend while\n\nQuery result: BEFORE(A, D) is satisfied with BEFORE(A, C) and\nOVERLAP(B, D)\n\n* Instruction\n\nHalloway was < retrieved > and <\nreattached >, she lived for several years more but eventually\n\nDemonstration\n\n[-\n\n1\n\nText: a\n| Text: the scalp }\nt '\n' became insane. i\nt '\nt\n\n| Event Pairs: ]\n| <retrieved > and < reattached > )\n| <reattached > and < retrieved > |\nt i\n| From the text, we could first get: !\n| <retrieved > happens BEFORE <reattached>, and <reattached > |\n| happens BEFORE < became >. '\n| Due to the logical constraints: f event A happens BEFORE event |\n| B, then they won't have coreference and subevent relations, ;\n' and event B has NO_TEMPORAL relation with event A. '\n| We can deduce that the answers are: q\n| NO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT. }\n| NO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\nInput\n\n\"<formed > happens before < or\n| PRECONDITION of < organise >.\n| Due to the logical constraints: Ifevent A happens BEFORE event 8, and\n\nise >, < formed >is the i\n1\n1\n1\n| event A is event B's PRECONDITION, then event B causes event A. |\n1\n1\n1\n1\n\nOutput\n\n| Answers:\n| NO. COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\n| NO_COREFERENCE, NO_TEMPORAL, CAUSE, NO_SUBEVENT. 2%\n\nFigure 9: A case study that ChatGPT generates inaccurate\nlogical constraints.\n\nF STATISTICS OF LLM-LR\n\nAs shown in Table 5, we provide the statistics of our LLM-LR\ndataset.\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nHop # Count # Average Tokens\n\n2 39 71\n3 179 83\n4 945 97\n5 5613 109\n\nTable 5: Statistics of LLM—LR.\n\nG IMPLEMENTATION DETAILS OF\nPRETRAINING-BASED APPROACH\n\nWe set the rank of LoORA modules to be 64. Our model is optimized\nwith a learning rate of 2e-4 and a linear warm-up for the first 3%\nsteps. We clip the gradients of model parameters to a max norm\nof 0.3. The batch size is 8 and the number of epochs is 3. All the\nLoRA parameters are trained on an NVIDIA A100 GPU with 80GB\nmemory.\n", "vlm_text": "The image shows \"Algorithm 1 An Example of 3-hop Reasoning,\" which outlines a process for logic-based reasoning using facts and rules. Here's a text breakdown:\n\n1. **Initialize the Knowledge Base with Facts and Rules:**\n\n   - **Facts:**\n     - BEFORE(A, B)\n     - SIMULTANEOUS(B, C)\n     - OVERLAP(C, D)\n\n   - **Rules:**\n     - BEFORE ← BEFORE ∧ SIMULTANEOUS\n     - BEFORE ← BEFORE ∧ OVERLAP\n     - OVERLAP ← SIMULTANEOUS ∧ OVERLAP\n\n2. **Initialize the Logic Engine with the Query:**\n\n   - Query: BEFORE(A, D)?\n\n3. **While Loop for Obtaining New Facts:**\n\n   - For each rule \\( r \\) of the knowledge base:\n     - If \\( r \\)'s premise is satisfied by the current known facts, then:\n       - Add \\( r \\)'s conclusion to the knowledge base.\n\nThis algorithm demonstrates the process of logic inference through a series of rules and queries in a knowledge base.\nThe table shows data across three columns:\n\n1. **Hop**: Lists the levels or steps from 2 to 5.\n2. **# Count**: Shows the number of instances or occurrences for each hop level:\n   - 2: 39\n   - 3: 179\n   - 4: 945\n   - 5: 5613\n3. **# Average Tokens**: Displays the average number of tokens for each hop level:\n   - 2: 71\n   - 3: 83\n   - 4: 97\n   - 5: 109\nG IMPLEMENTATION DETAILS OF PRE TRAINING-BASED APPROACH\nWe set the rank of LoRA modules to be 64. Our model is optimized with a learning rate of 2e-4 and a linear warm-up for the first  $3\\%$  steps. We clip the gradients of model parameters to a max norm of 0.3. The batch size is 8 and the number of epochs is 3. All the LoRA parameters are trained on an NVIDIA A100 GPU with 80GB memory. \nQuery result: BEFORE  $(A,D)$   is satisfied with BEFORE  $(A,C)$   and OVERLAP( 𝐵 ,  𝐷 ) \nThe image contains a case study illustrating how ChatGPT generates logical constraints inaccurately. It is divided into sections labeled Instruction, Demonstration, Input, and Output.\n\n1. **Instruction**: Describes marking events in a text with specific symbols.\n\n2. **Demonstration**: \n   - **Text**: Discusses the sequence of events involving Mrs. Halloway's scalp being retrieved and reattached.\n   - **Event Pairs**: Lists \"< retrieved > and < reattached >\" and \"< reattached > and < retrieved >\".\n   - **Analysis**: Concludes logical relations such as \"NO_COREFERENCE\" and \"BEFORE\".\n\n3. **Input**:\n   - **Text**: Refers to General Miaja and political leaders forming and organizing defense.\n   - **Event Pairs**: Lists \"< formed > and < organise >\" and \"< organise > and < formed >\".\n   - **Analysis**: States \"< formed > happens before < organise >\".\n\n4. **Output**:\n   - Lists supposed logical relations but with errors: the study indicates differences such as \"PRECONDITION\" instead of \"CAUSE\".\n\nThe case study highlights discrepancies in ChatGPT's logical deductions from given event sequences.\nF STATISTICS OF  LLM-LR \nAs shown in Table 5, we provide the statistics of our  LLM-LR dataset. "}
{"page": 13, "image_path": "doc_images/2310.09158v1_13.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY Chen, et al.\n\nIf Relation(A, B) Prompt Text\n\nIf event A and event B are COREFERENCE,\nCOREFERENCE then they won’t have temporal, causal, and subevent relations,\nand COREFERENCE relation is bidirectional.\n\nNO_TEMPORAL If event A and event B do not have a temporal relation,\nthen they won’t have causal and subevent relations.\n\nIf event A happens BEFORE event B,\nBEFORE then they won’t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nIf event A happens OVERLAP with event B,\nOVERLAP then they won’t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nIf event A’s time CONTAINS event B’s time,\nCONTAINS then they won’t have coreference and causal relations,\nand event B has NO_TEMPORAL relation with event A.\n\nIf event A and event B happen SIMULTANEOUSly,\nSIMULTANEOUS then they won’t have coreference, causal, and subevent relations,\nand SIMULTANEOUS relation is bidirectional.\n\nIf event A ENDS-ON event B,\nENDS-ON then they won’t have coreference, causal and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nIf event A BEGINS-ON event B,\nBEGINS-ON then they won’t have coreference, causal and subevent relations\nand BEGINS-ON relation is bidirectional.\n\nIf event A CAUSEs event B,\n\nthen event A happens BEFORE or OVERLAP event B,\nCAUSE , .\nand they won’t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nIf event A is event B’s PRECONDITION,\nthen event A happens BEFORE or OVERLAP event B,\n\nPRECONDITION and they won’t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\nIf event B is a SUBEVENT of event A,\nSUBEVENT then they won’t have coreference and causal relations,\n\nand event A’s time should CONTAINS event B’s time,\nand event B has NO_TEMPORAL relation with event A.\n\nTable 6: Prompt text of relations between two events.\n", "vlm_text": "The table presents different relational categories between two events, A and B, along with corresponding prompt texts detailing the implications and constraints associated with each type of relation. Here is a summary of the table content:\n\n1. **COREFERENCE**: If events A and B are coreferential, they won't have temporal, causal, or subevent relationships, and the coreference relation is bidirectional.\n\n2. **NO_TEMPORAL**: If events A and B do not have a temporal relationship, they won't have causal or subevent relationships either.\n\n3. **BEFORE**: If event A happens before event B, they won't have coreference or subevent relations, and event B has no temporal relation with event A.\n\n4. **OVERLAP**: If event A overlaps with event B, they won't have coreference or subevent relations, and event B has no temporal relation with event A.\n\n5. **CONTAINS**: If the time of event A contains event B's time, they won't have coreference or causal relations, and event B has no temporal relation with event A.\n\n6. **SIMULTANEOUS**: If events A and B happen simultaneously, they won't have coreference, causal, or subevent relations, and the simultaneous relation is bidirectional.\n\n7. **ENDS-ON**: If event A ends-on event B, they won't have coreference, causal, or subevent relations, and event B has no temporal relation with event A.\n\n8. **BEGINS-ON**: If event A begins-on event B, they won't have coreference, causal, or subevent relations, and the begins-on relation is bidirectional.\n\n9. **CAUSE**: If event A causes event B, event A happens before or overlaps with event B, and they won't have coreference or subevent relations. Event B has no temporal relation with event A.\n\n10. **PRECONDITION**: If event A is a precondition of event B, then event A happens before or overlaps with event B, they won't have coreference or subevent relations, and event B has no temporal relation with event A.\n\n11. **SUBEVENT**: If event B is a subevent of event A, they won't have coreference or causal relations, event A's time should contain event B's time, and event B has no temporal relation with event A.\n\nThe table effectively maps the potential interactions (or lack thereof) between events based on their relational context."}
{"page": 14, "image_path": "doc_images/2310.09158v1_14.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning\n\nIf Relation(A, B) A Relation(B, C)\n\nConference acronym \"XX, June 03-05, 2018, Woodstock, NY\n\nThen Relation (A, C)\n\nCOREFERENCE A COREFERENCE\nCOREFERENCE A BEFORE\nCOREFERENCE A OVERLAP\nCOREFERENCE A CONTAINS\nCOREFERENCE A SIMULTANEOUS\nCOREFERENCE A ENDS-ON\nCOREFERENCE A BEGINS-ON\nCOREFERENCE A CAUSE\nCOREFERENCE A PRECONDITION\nCOREFERENCE A SUBEVENT\nBEFORE A BEFORE\n\nBEFORE A OVERLAP\n\nBEFORE A CONTAINS\n\nBEFORE A SIMULTANEOUS\nBEFORE A ENDS-ON\n\nBEFORE A BEGINS-ON\n\nOVERLAP A BEFORE\n\nOVERLAP A SIMULTANEOUS\nCONTAINS A CONTAINS\nCONTAINS A SIMULTANEOUS\nSIMULTANEOUS A BEFORE\nSIMULTANEOUS A OVERLAP\nSIMULTANEOUS A CONTAINS\nSIMULTANEOUS A SIMULTANEOUS\nSIMULTANEOUS A ENDS-ON\nSIMULTANEOUS A BEGINS-ON,\nSIMULTANEOUS A COREFERENCE\nENDS-ON A CONTAINS\n\nENDS-ON A BEGINS-ON\nENDS-ON A SIMULTANEOUS\nBEGINS-ON A SIMULTANEOUS\nBEGINS-ON A BEGINS-ON\nBEGINS-ON A COREFERENCE\nCAUSE A CAUSE\n\nCAUSE A PRECONDITION\n\nCAUSE A SUBEVENT\nPRECONDITION A PRECONDITION\nPRECONDITION A SUBEVENT\nSUBEVENT A SUBEVENT\n\nCOREFERENCE, =TEMPORAL, ~CAUSAL, ~SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nOVERLAP, ~=COREFERENCE, ~SUBEVENT\nCONTAINS, ~COREFERENCE, ~CAUSAL\nSIMULTANEOUS, =COREFERENCE, =CAUSAL, -SUBEVENT\nENDS-ON, =CAUSAL, ~SUBEVENT\nBEGINS-ON, =CAUSAL, =SUBEVENT\nCAUSE, >COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\nPRECONDITION, ~COREFERENCE, BEFORE V OVERLAP, ~SUBEVENT\nSUBEVENT, ~COREFERENCE, CONTAINS =CAUSAL\nBEFORE, ~COREFERENCE, ~SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nOVERLAP, ~=COREFERENCE, ~SUBEVENT\nCONTAINS, ~COREFERENCE, ~CAUSAL\nCONTAINS, ~COREFERENCE, ~CAUSAL\nBEFORE, ~COREFERENCE, ~SUBEVENT\nOVERLAP, ~=COREFERENCE, ~SUBEVENT\nCONTAINS, ~COREFERENCE, ~CAUSAL\nSIMULTANEOUS, =COREFERENCE, =CAUSAL, -SUBEVENT\nENDS-ON, ~COREFERENCE, =SUBEVENT\nBEGINS-ON, =COREFERENCE, ~SUBEVENT\nSIMULTANEOUS, =COREFERENCE, =CAUSAL, -SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nENDS-ON, =COREFERENCE, =CAUSAL, =SUBEVENT\nENDS-ON, =COREFERENCE, =CAUSAL, =SUBEVENT\nBEGINS-ON, ~COREFERENCE, ~CAUSAL, ~SUBEVENT\nBEGINS-ON, ~COREFERENCE, ~CAUSAL, ~SUBEVENT\nBEGINS-ON, =CAUSAL, =SUBEVENT\nCAUSE, >COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\nPRECONDITION, ~COREFERENCE, BEFORE V OVERLAP, ~SUBEVENT\nCAUSE, >COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\nPRECONDITION, ~COREFERENCE, BEFORE V OVERLAP, ~SUBEVENT\nPRECONDITION, ~COREFERENCE, BEFORE V OVERLAP, ~SUBEVENT\nSUBEVENT, ~COREFERENCE, CONTAINS =CAUSAL\n\nTable 7: Logical Constraints of relations among three events, where ( denotes \"AND\", — denotes \"NOT\", V denotes \"OR\".\n", "vlm_text": "The table presents logical relationships between three entities \\( A \\), \\( B \\), and \\( C \\), denoted by various relations such as \"COREFERENCE,\" \"BEFORE,\" \"OVERLAP,\" \"CONTAINS,\" \"SIMULTANEOUS,\" \"ENDS-ON,\" \"BEGINS-ON,\" \"CAUSE,\" \"PRECONDITION,\" and \"SUBEVENT.\" The format of the table is structured as rules:\n\n- The left side of each row, under the column \"If Relation(A, B) ∧ Relation(B, C),\" specifies a conjunction of two relations: one between \\( A \\) and \\( B \\) and another between \\( B \\) and \\( C \\).\n\n- The right side of each row, under the column \"Then Relation (A, C),\" indicates the deduced relations between \\( A \\) and \\( C \\), often preceded by negations (¬), suggesting a lack of that relation or specific logical constraints.\n\nThe table functions as a logic-based system for deducing the relationships between three entities when given specific pairwise relationships between them. For example, if \\( A \\) and \\( B \\) have a \"COREFERENCE\" and \\( B \\) and \\( C \\) have a \"BEFORE\" relation, then the deduced relation between \\( A \\) and \\( C \\) is \"BEFORE, ¬COREFERENCE, ¬SUBEVENT.\""}
{"page": 15, "image_path": "doc_images/2310.09158v1_15.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY\n\nIf Relation(A, B) A Relation(B, C)\n\nPrompt Text\n\nCOREFERENCE ( COREFERENCE\nCOREFERENCE A BEFORE\nCOREFERENCE (A OVERLAP\nCOREFERENCE A CONTAINS\nCOREFERENCE A SIMULTANEOUS\nCOREFERENCE A ENDS-ON\nCOREFERENCE A BEGINS-ON\nCOREFERENCE A CAUSE\nCOREFERENCE A PRECONDITION\nCOREFERENCE A SUBEVENT\n\nIf event A and event B are COREFERENCE,\nthen the relations between event B and event C\nshould be the same as that between event A and event C.\n\nBEFORE ( BEFORE\nBEFORE A OVERLAP\nBEFORE A CONTAINS\nBEFORE A SIMULTANEOUS,\nBEFORE A ENDS-ON\nBEFORE A BEGINS-ON\n\nIf event A happens BEFORE event B, and Relation(B, C),\nthen event A happens BEFORE event C.\n\nOVERLAP A BEFORE\n\nIf event A happens OVERLAP with event B,\nand event B happens BEFORE event C,\nthen event A happens BEFORE event C.\n\nOVERLAP A SIMULTANEOUS\n\nIf event A happens OVERLAP with event B,\nand event B and event C happen SIMULTANEOUSly,\nthen event A happens BEFORE event C.\n\nCONTAINS A CONTAINS\n\nIf event A’s time CONTAINS event B’s time,\nand event B’s time CONTAINS event C’s time,\nthen event A’s time CONTAINS event C’s time.\n\nCONTAINS A SIMULTANEOUS\n\nIf event A’s time CONTAINS event B’s time,\nand event B and event C happen SIMULTANEOUSly,\nthen event A’s time CONTAINS event C’s time.\n\nSIMULTANEOUS A BEFORE\nSIMULTANEOUS A OVERLAP\nSIMULTANEOUS A CONTAINS\nSIMULTANEOUS A SIMULTANEOUS\nSIMULTANEOUS A ENDS-ON\nSIMULTANEOUS A BEGINS-ON\n\nIf events A and B happen SIMULTANEOUSIy, and Relation(B, C),\nthen event A’s time CONTAINS event C’s time.\n\nENDS-ON A CONTAINS\n\nIf event A ENDS-ON event B,\nand event B’s time CONTAINS event C’s time,\nthen event A happens BEFORE event C.\n\nENDS-ON A BEGINS-ON\nENDS-ON A SIMULTANEOUS\n\nIf event A ENDS-ON event B, and Relation(B, C),\nthen event A ENDS-ON event C.\n\nBEGINS-ON A SIMULTANEOUS,\nBEGINS-ON A BEGINS-ON\n\nIf event A BEGINS-ON event B, and Relation(B, C),\nthen event A BEGINS-ON event C.\n\nCAUSE A CAUSE\n\nIf event A CAUSEs event B,\nand event B CAUSEs event C,\nthen event A CAUSEs event C.\n\nIf event A CAUSEs event B,\n\nCAUSE A PRECONDITION and event B is event C’s PRECONDITION,\nthen event A is event C’s PRECONDITION.\nIf event A CAUSEs event B,\nCAUSE A SUBEVENT and event C is a SUBEVENT of event B,\n\nthen event A CAUSEs event C.\n\nPRECONDITION A PRECONDITION\n\nIf event A is event B’s PRECONDITION,\nand event B is event C’s PRECONDITION,\nthen event A is event C’s PRECONDITION.\n\nPRECONDITION A SUBEVENT\n\nIf event A is event B’s PRECONDITION,\nand event C is a SUBEVENT of event B,\nthen event A is event C’s PRECONDITION.\n\nSUBEVENT A SUBEVENT\n\nIf event B is a SUBEVENT of event A,\nand event C is a SUBEVENT of event B,\nthen event C is a SUBEVENT of event A.\n\nTable 8: Prompt text of relations among three events.\n\nChen, et al.\n", "vlm_text": "The table lists logical relationships between events A, B, and C, detailing different scenarios of temporal relationships. It includes columns for \"If Relation(A, B) ∧ Relation(B, C)\" and \"Prompt Text.\" The \"Prompt Text\" explains the outcome or condition based on the given relations.\n\n### Here’s a summary of some key rows:\n\n1. **COREFERENCE Relations**: If events A and B are COREFERENCE, then relations between B and C should mirror those between A and C.\n\n2. **BEFORE Relations**: If event A happens BEFORE event B, then event A also happens BEFORE event C.\n\n3. **OVERLAP and SIMULTANEOUS Relations**:\n   - If A OVERLAPS B, and B and C are SIMULTANEOUS, then A happens BEFORE C.\n   - If A's time contains B's, and B and C are SIMULTANEOUS, A's time contains C's time.\n\n4. **SIMULTANEOUS Relations**: If A and B are SIMULTANEOUS, and B relates to C, A’s time encompasses C’s time.\n\n5. **END-ON and BEGINS-ON Relations**:\n   - If A ENDS-ON B, and B contains C, A happens before C.\n   - If A BEGINS-ON B, and B relates to C, A BEGINS-ON C.\n\n6. **CAUSE and PRECONDITION Relations**:\n   - If A CAUSES B, and B causes C, A causes C.\n   - If A is B’s PRECONDITION, and B is C’s, A is C’s PRECONDITION.\n\n7. **SUBEVENT Relations**: If B is a SUBEVENT of A, and C is a subevent of B, then C is a subevent of A.\n\nThe table is designed to show how different event relations can determine the overall relationship between multiple events."}
{"page": 16, "image_path": "doc_images/2310.09158v1_16.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning, Conference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nH PROMPT EXAMPLES\n\nIn this section, we provide examples of prompts used for each task and approach.\n\nH.1 Pilot Case Study\n\nThis section includes prompt examples of:\n\nMAVEN-ERE w. relevant logic constraints (Figure 10);\nMAVEN-ERE w. irrelevant logic constraints (Figure 11);\nProof Writer w. relevant logic constraints (Figure 12);\nProof Writer w. irrelevant logic constraints (Figure 13).\nMulti-hop reasoning (Figure 14)\n\nMAVEN-ERE w. relevant logic constraints ¢Q>)\n\nTask Description:\n\ng\na There is a piece of text with two events marked by < and > symbols, and your task is\nUser\n\nto identify four types of relations between them\n\nText:\nThe 2013 Lahad Datu < standoff > ( also known as the Lahad Datu incursion ) was a\nmilitary < conflict > that started on 11 February 2013 and fully ended on 24 March\n2013 .\n\nEvent Pairs:\n< standoff > and < conflict >\n< conflict > and < standoff >\n\nAnswers:\n\nEvent Pair 1: < standoff > and < conflict > S\n(1) Coreference relation : COREFERENCE (2) Temporal relation: CONTAINS x«\n\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT ChatGPT\n\nEvent Pair 2: < conflict > and < standoff >\n(1) Coreference relation: NO_COREFERENCE x (2) Temporal relation: NO_TEMPORAL\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\nLet’s think step by step. You have identified that < standoff > and < conflict > are,\n\ncoreferences. The coreference should be bidirectional. And if two events are -@}\nUser COREFERENCE, they won’t have temporal relations. So please modify your Coreference\nSer) and Temporal answers. =\n\nApologies for the confusion. Based on the revised information, here are the corrected S\n\nanswers:\n\nEvent Pair 1: < standoff > and < conflict >\n\n(1) Coreference relation : COREFERENCE (2) Temporal relation: NO_TEMPORAL ChatGPT\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\nEvent Pair 2: < conflict > and < standoff >\n(1) Coreference relation: COREFERENCE YS (2) Temporal relation: NO_TEMPORAL\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\nFigure 10: MAVEN-ERE w. relevant logic constraints\n", "vlm_text": "H PROMPT EXAMPLES \nIn this section, we provide examples of prompts used for each task and approach. \nH.1 Pilot Case Study \nThis section includes prompt examples of: \n MAVEN-ERE w. relevant logic constraints (Figure 10); •  MAVEN-ERE w. irrelevant logic constraints (Figure 11); •  Proof Writer w. relevant logic constraints (Figure 12); •  Proof Writer w. irrelevant logic constraints (Figure 13). •  Multi-hop reasoning (Figure 14) • \nThe image is a simple, black and white icon representing a user. It features a stylized silhouette of a person, often used to signify a generic profile or user account, accompanied by the word \"User\" written below the icon.\nThis image shows an illustration of a lightbulb with lines around it, which often symbolizes an idea or inspiration. It has a green interior and is encased in a simple, stylized design.\nMAVEN-ERE w. relevant logic constraints ( \nTask Description: \nThere is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is  to identify four types of relations between them  (··· more context here ··· ) \nText:  \nThe 2013 Lahad Datu   $<$   standoff  $>$   ( also known as the Lahad Datu incursion ) was a  military  $<$   conflict  $>$   that started on 11 February 2013 and fully ended on 24 March  2013 .\n\n \nEvent Pairs:  \n $<$   standoff  $>$   and   $<$   conflict >\n\n  $<$   conflict  $>$   and   $<$   standoff  $>$  \nAnswers:  \nEvent Pair 1: < standoff  $>$   and < conflict >\n\n (1) Co reference relation : CO REFERENCE  (2) Temporal relation:  CONTAINS\n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT Event Pair 2: < conflict  $>$   and < standoff >\n\n (1) Co reference relation:  NO CO REFERENCE (2) Temporal relation: NO TEMPORAL \n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT \nUser \nLet’s think step by step. You have identified that   $<$   standoff  $>$   and   $<$   conflict  $>$   are  co references.  The co reference should be bidirectional. And if two events are  CO REFERENCE, they won’t have temporal relations.  So please modify your Co reference and Temporal answers. \nThe image shows text providing corrected answers for two event pairs regarding their relationships:\n\n**Event Pair 1:**  \n- Events: <standoff> and <conflict>\n  1. Coreference relation: COREFERENCE\n  2. Temporal relation: NO_TEMPORAL\n  3. Causal relation: NO_CAUSAL\n  4. Subevent relation: NO_SUBEVENT\n\n**Event Pair 2:**\n- Events: <conflict> and <standoff>\n  1. Coreference relation: COREFERENCE\n  2. Temporal relation: NO_TEMPORAL\n  3. Causal relation: NO_CAUSAL\n  4. Subevent relation: NO_SUBEVENT\n\nThere's a checkmark next to the COREFERENCE and NO_TEMPORAL relations in both pairs."}
{"page": 17, "image_path": "doc_images/2310.09158v1_17.jpg", "ocr_text": "Conference acronym \"XX, June 03-05, 2018, Woodstock, NY Chen, et al.\n\nMAVEN-ERE w. irrelevant logic constraints (\n\n@ | Task Description:\na There is a piece of text with two events marked by < and > symbols, and your task is\nUser to identify four types of relations between them\n\nText:\nThe 2013 Lahad Datu < standoff > ( also known as the Lahad Datu incursion ) was a\nmilitary < conflict > that started on 11 February 2013 and fully ended on 24 March\n2013 .\n\nEvent Pairs:\n< standoff > and < conflict >\n< conflict > and < standoff >\n\nAnswers:\nEvent Pair 1: < standoff > and < conflict > S\n(1) Coreference relation : COREFERENCE (2) Temporal relation: CONTAINS x\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT ChatGPT\nEvent Pair 2: < conflict > and < standoff >\n(1) Coreference relation: NO_COREFERENCE Pad (2) Temporal relation: NO_TEMPORAL\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\ngS 7 ,\n\n@ Let’s think step by step. If < standoff > happens BEFORE < conflict >, then they\n\nry won’t have coreference and subevent relations, and < conflict > will have NO_TEMPORAL\n\nUser | relation with < standoff > . (3)\nYes, you are correct. Based on the information, here are the answers:\nEvent Pair 1: < standoff > and < conflict > ChatGPT\n(1) Coreference relation : NO_COREFERENCE x“ (2) Temporal relation: BEFORE x“\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\nEvent Pair 2: < conflict > and < standoff >\n(1) Coreference relation: NO_COREFERENCE x“ (2) Temporal relation: NO_TEMPORAL\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\nFigure 11: MAVEN-ERE w. irrelevant logic constraints\n", "vlm_text": "The image shows a simple black silhouette icon of a person with the text \"User\" written below it.\nThe image shows a blue circle surrounding a light green light bulb icon that has a diagonal line crossing through it, possibly indicating \"no ideas\" or \"no power.\" The blue circle is flanked by two black curved lines.\nMAVEN-ERE w. irrelevant logic constraints ( \nTask Description: \nThere is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is  to identify four types of relations between them  (··· more context here ··· ) \nText:  \nThe 2013 Lahad Datu  $<$   standoff  $>$   ( also known as the Lahad Datu incursion ) was a  military  $<$   conflict   $>$   that started on 11 February 2013 and fully ended on 24 March  2013 .\n\n \nEvent Pairs:  \n< standoff  $>$   and   $<$   conflict >\n\n < conflict  $>$   and   $<$   standoff > \nAnswers:  \nEvent Pair 1: < standoff  $>$   and < conflict >\n\n (1) Co reference relation : CO REFERENCE  (2) Temporal relation:  CONTAINS\n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT \nEvent Pair 2: < conflict  $>$   and < standoff >\n\n (1) Co reference relation:  NO CO REFERENCE (2) Temporal relation: NO TEMPORAL \n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT \nThe image is an icon depicting a generic person or user symbol, often used to represent a user profile.\nThe image contains text that reads: \"Let's think step by step. If <standoff> happens BEFORE <conflict>, then they won't have coreference and subevent relations, and <conflict> will have NO_TEMPORAL relation with <standoff>.\" The text is written in a blue italic font. There is also a small icon in the image that resembles a green circular arrow surrounding the head of a green character that resembles Pac-Man, placed near the bottom center of the image. \nThe image appears to be a segment from a text-based interaction, possibly with an AI model like ChatGPT, providing information about various relationships between two pairs of events. The text indicates:\n\n- For Event Pair 1, `< standoff > and < conflict >`:\n  - Coreference relation: NO_COREFERENCE (highlighted in red and marked with 'X')\n  - Temporal relation: BEFORE (highlighted in red and marked with 'X')\n  - Causal relation: NO_CAUSAL\n  - Subevent relation: NO_SUBEVENT\n\n- For Event Pair 2, `< conflict > and < standoff >`:\n  - Coreference relation: NO_COREFERENCE (highlighted in red and marked with 'X')\n  - Temporal relation: NO_TEMPORAL\n  - Causal relation: NO_CAUSAL\n  - Subevent relation: NO_SUBEVENT\n\nThe red highlights with 'X' seem to indicate where a user might have made an incorrect assumption or selection. The ChatGPT logo is present on the right side of the image, suggesting that the feedback or evaluation comes from a ChatGPT interaction."}
{"page": 18, "image_path": "doc_images/2310.09158v1_18.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning, Conference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nProofWriter w. relevant logic constraints ¢Q>)\n\nS Task Description:\nrf Given a problem statement as contexts, the task is to answer a logical reasoning\nUser question.\n\nText:\n\nBob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white.\nCharlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry\nis white. All red people are round. Round, smart people are cold. If someone is cold\nand young then they are white. Red people are round. All round, young people are red.\nIf someone is smart then they are rough. Rough, young people are red. If Bob is round\nthen Bob is smart. If Charlie is white and Charlie is rough then Charlie is young.\n\nQuestion:\nBased on the above information, is the following statement true, false, or unknown?\nHarry is not round.\n\nBased on the given information, the statement \"Harry is not round\" is true.\n\nThe information states that all red people are round, and Harry is describe\n\nwhite, smart, and rough, but not round. Therefore, Harry does not fit the criteria of | ChatGPT\nbeing round, and the statement is true.\n\nLet’s think step by step. From the text, we could first get relevant facts:\n+ Harry is smart.\n+ Harry is white.\n\npo\n\n=\n8\n©\n\nAnd the relevant rule: -®-\n+ All red people are round. >\n\n+ Round, smart people are cold.\n\n+ All round, young people are red.\n\nGiven these statements, we can conclude the following:\nHarry is smart and white, but there is no direct statement about his being red.\n\nApologies for the confusion. You are correct, there is no direct statement in the\ngiven information that explicitly mentions whether Harry is round or not. Therefore,\nthe statement \"Harry is not round\" is unknown based solely on the provided\n\ninformation. Y ChatGPT\n\nFigure 12: Proof Writer w. relevant logic constraints\n", "vlm_text": "Proof Writer w. relevant logic constraints ( \nThe image is a simple black-and-white icon of a person with the word \"User\" written below it.\nTask Description: \nGiven a problem statement as contexts, the task is to answer a logical reasoning  question.  \nText:  \nBob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white.  Charlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry  is white. All red people are round. Round, smart people are cold. If someone is cold  and young then they are white. Red people are round. All round, young people are red.  If someone is smart then they are rough. Rough, young people are red. If Bob is round  then Bob is smart. If Charlie is white and Charlie is rough then Charlie is young. \nQuestion:  \nBased on the above information, is the following statement true, false, or unknown?  Harry is not round. \nBased on the given information, the statement \"Harry is not round\" is  true . The information states that all red people are round, and Harry is described as  white, smart, and rough,  but not round . Therefore, Harry does not fit the criteria of  being round, and the statement is true. \nThe image is a simple black and white icon of a person or user figure, which is typically used to represent a user profile or account. Below the figure, the word \"User\" is written.\nLet’s think step by step. From the text, we could first get relevant facts:\n\n • Harry is smart.\n\n • Harry is white. \nAnd  the relevant rule :\n\n • All red people are round.\n\n • Round, smart people are cold.\n\n • All round, young people are red. \nGiven these statements, we can conclude the following: Harry is smart and white, but there is no direct statement about his being red. \nThe image contains text that addresses a misunderstanding. The statement mentions that there's no direct information provided to determine whether \"Harry is round\" or not. It concludes that the assertion \"Harry is not round\" is unknown based on the available information. Certain words or phrases like \"there is no direct statement in the given information that explicitly mentions whether Harry is round or not\" and \"unknown\" are highlighted in green, and there is a green check mark at the bottom."}
{"page": 19, "image_path": "doc_images/2310.09158v1_19.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY Chen, et al.\n\nProofWriter w. irrelevant logic constraints (\n\n@ | Task Description:\n\nr Given a problem statement as contexts, the task is to answer a logical reasoning\ntion.\n\nUser | [USS\n\nText:\n\nBob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white.\nCharlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry\nis white. All red people are round. Round, smart people are cold. If someone is cold\nand young then they are white. Red people are round. All round, young people are red.\nIf someone is smart then they are rough. Rough, young people are red. If Bob is round\nthen Bob is smart. If Charlie is white and Charlie is rough then Charlie is young.\n\nQuestion:\nBased on the above information, is the following statement true, false, or unknown?\nHarry is not round.\n\nBased on the given information, the statement \"Harry is not round\" is true. x\nThe information states that all red people are round, and Harry is describe\n\nwhite, smart, and rough, but not round. Therefore, Harry does not fit the criteria of | ChatGPT\nbeing round, and the statement is true.\n\nLet’s think step by step. From the text, we could get some facts:\n+ Harry is smart.\n+ Harry is white.\n\npo\n\n=\n8\n©\n\nAnd some rules:\n\n+ If someone is cold and young then they are white.\n\n+ If someone is smart then they are rough.\n+ Rough, young people are red.\n\nGiven these statements, please output your answers.\n\nApologies for the confusion. Based on the information, the statement “Harry is not\nround” is False. 9\n\nChatGPT\n\nFigure 13: Proof Writer w. irrelevant logic constraints\n\nMulti-hop Reasoning\n\nTask Description:\nGiven a problem statement as contexts, your task\nis to induce the relations between two events.\n\nContext:\n\nGiven that event A happens BEFORE event B, event B\nand event C happen SIMULTANEOUSly, event C happens\nOVERLAP with event D.\n\nQuestion:\nBased on the above information, what are the four\ntypes of relations between event A and D?\n\nAnswers:\n\nFigure 14: Abstract Multi-hop Reasoning\n", "vlm_text": "The image shows a silhouette of a person with the text \"User\" below it. It's a simple icon-style representation.\nThe image shows a green light bulb with a diagonal line across it, inside a blue circle. There are also black parentheses on either side of the circle. This symbol might represent \"no ideas,\" \"idea off,\" or a similar concept.\nProof Writer w. irrelevant logic constraints ( \nTask Description: \nGiven a problem statement as contexts, the task is to answer a logical reasoning  question.  \nText:  \nBob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white.  Charlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry  is white. All red people are round. Round, smart people are cold. If someone is cold  and young then they are white. Red people are round. All round, young people are red.  If someone is smart then they are rough. Rough, young people are red. If Bob is round  then Bob is smart. If Charlie is white and Charlie is rough then Charlie is young. \nQuestion:  \nBased on the above information, is the following statement true, false, or unknown?  Harry is not round. \nBased on the given information, the statement \"Harry is not round\" is  true . The information states that all red people are round, and Harry is described as  white, smart, and rough,  but not round . Therefore, Harry does not fit the criteria of  being round, and the statement is true. \nThe image shows a simple black silhouette of a person, often used as a generic icon or avatar, with the word \"User\" written below it.\nLet’s think step by step. From the text, we could get some facts:\n\n • Harry is smart.\n\n • Harry is white.  \nAnd some rules:\n\n • If someone is cold and young then they are white. \n\n • If someone is smart then they are rough. \n\n • Rough, young people are red. \nThe image shows a blue circle with a green light bulb inside. There is a diagonal line crossing the bulb, likely indicating a prohibition or restriction, possibly related to energy conservation or \"no light bulb\" usage.\nGiven these statements, please output your answers. \nFigure 13: Proof Writer w. irrelevant logic constraints \nMulti-hop Reasoning \nThe image contains a problem statement about determining the relations between two events. Here's a summary:\n\n- **Task Description:** Induce the relations between two events based on the context.\n- **Context:** \n  - Event A happens before event B.\n  - Events B and C happen simultaneously.\n  - Event C overlaps with event D.\n- **Question:** Determine the four types of relations between event A and event D.\n\nThe image ends with \"Answers:\" but no answers are provided."}
{"page": 20, "image_path": "doc_images/2310.09158v1_20.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning\n\nH.2 Incoporating Logical Constraints\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nThe highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity.\n\nVanilla ICL\n\nTask Description:\n\nThere is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nAnswers:\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, NO_TEMPORAL, PRECONDITION, NO_SUBEVENT.\n\nw. all logical constraints\n\nVanilla ICL plus CoT\n\nTask Description:\n\nThere is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nReasoning:\n\n< bullied > happens before < tell >, and being <\nbullied > leads her to < tell >.\n\nAnswers:\n\nNO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\n\nTask Description:\n\nThere is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nLogical constraints:\n\nThere are some rules among the relations, you can\nselect some of them to reason or check your\nanswers:\n\n(1) If two events are COREFERENCE, then they won’t\nhave temporal, causal, and subevent relations, and\nCOREFERENCE relation is bidirectional.\n\n(11) If event B is a SUBEVENT of event A,\nthen they won’t have coreference and causal\nrelations, and event A’s time should CONTAINS\nevent B’s time, and event B has NO_TEMPORAL\nrelation with event A.\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nAnswers:\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\n\nw. retrieved logical constraints\n\nCoT w. self-generated logical constraints\n\nTask Description:\n\nThere is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nFrom the text, we could first get:\n\n< bullied > happens before < tell >, and being <\nbullied > leads her to < tell >\n\nDue to the logical constraint: If event A is event\nB’s PRECONDITION, then event A happens BEFORE or\nOVERLAP event B, and they won’t have coreference\nand subevent relations, and event B has\nNO_TEMPORAL relation with event A.\n\nWe can deduce that the answers are:\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\n\nTask Description:\n\nThere is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nLogical constraints:\n\nThere are some rules among the relations, you can\nselect some of them to reason or check your\nanswers:\n\n(1) If event A is event B’s PRECONDITION,\n\nthen event A happens BEFORE or OVERLAP event B,\nand they won’t have coreference and subevent\nrelations, and event B has NO_TEMPORAL relation\nwith event A.\n\n(2) If event A and event B do not have a temporal\nrelation, then they won’t have causal and subevent\nrelations.\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nAnswers:\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\n\n", "vlm_text": "H.2 In cop orating Logical Constraints \nThe highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity. \nVanilla ICL \nThe image contains a text analysis task relating to event pair relationships within a given passage. The task description outlines the objective: identify four types of relations (co-reference, temporal, causal, and subevent) between events marked by angle brackets \"<>\" in the text. The specific text to be analyzed describes a video posted by Todd using flash cards to recount experiences of blackmail and bullying. The event pairs given for analysis are \"<tell>\" and \"<bullied>\", in both \"tell, bullied\" and \"bullied, tell\" sequences. The corresponding answers indicate there are no relations such as co-reference, temporal, causal, or subevent for the \"<tell>\" and \"<bullied>\" pair, while for the \"<bullied>\" and \"<tell>\" pair, there is a precondition relationship; all other relations are absent. The task description is partially obscured with \"(... more context here ... )\" placeholder text.\nVanilla ICL plus CoT \nTask Description: There is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is to identify four types of relations between them  (··· more context  here ··· ) \nBefore her death , Todd posted a video on YouTube  in which she used a series of flash cards to  $<$    tell  $>$   her experience of being blackmailed into  exposing her breasts via webcam , and of being  $<$    bullied  $>$   and physically assaulted . \nThe image contains a textual excerpt displaying reasoning and answers related to an analysis of events. The \"Reasoning\" section discusses the sequence and causality of the events \"< bullied >\" and \"< tell >,\" noting that being bullied leads to telling, and the bullying occurs before telling. The \"Answers\" section lists possible relationships: \n- \"NO_COREFERENCE\" indicates no coreference relationship between entities.\n- \"BEFORE\" confirms the chronological order.\n- The two other options, \"NO_CAUSAL\" and \"PRECONDITION,\" suggest differing views on the causal relationship.\n- \"NO_SUBEVENT\" suggests no subevent relationship exists. \n\nThus, the text seems to be examining different logical relationships or event reasoning in a structured manner.\nCoT w. self-generated logical constraints \nThe image contains a text analysis task related to identifying the relationships between two marked events, \"<tell>\" and \"<bullied>\", in a given piece of text. The text describes an event where Todd posted a video before her death, telling about being blackmailed and bullied. The relationships considered are coreference, temporal, causal, and subevent. \n\nThe analysis in the image suggests that \"<bullied>\" happens before \"<tell>\", and that \"<bullied>\" leads her to \"<tell>\". It applies a logical constraint stating that if event A (bullied) is event B’s (tell) precondition, then event A happens before or overlaps event B, and there won't be coreference and subevent relations. It concludes with the relationship answers: \n\n- For \"<tell>\" and \"<bullied>\": NO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT\n- For \"<bullied>\" and \"<tell>\": NO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT\nw. all logical constraints \nThe image contains a task description about identifying types of relations between two events in a text marked by `<` and `>` symbols. It lists logical constraints to guide the identification of these relations, such as COREFFERENCE and SUBEVENT. The text for analysis describes a person using flashcards in a video to tell about being blackmailed and bullied. Two event pairs for analysis are given: `< tell > and < bullied >` and `< bullied > and < tell >`. The section for answers is blank.\nNO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENT. NO CO REFERENCE, BEFORE, PRECONDITION, NO SUB EVENT. \nw. retrieved logical constraints \nThere is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is to identify four types of relations between them  (··· more context  here ··· ) \nselect some of them to reason or check your  answers: (1) If event A is event B’s PRECONDITION, then event A happens BEFORE or OVERLAP event B,  and they won’t have co reference and subevent  relations, and event B has NO TEMPORAL relation  with event A.  (2) If event A and event B do not have a temporal  relation, then they won’t have causal and subevent  relations. \nBefore her death , Todd posted a video on YouTube  in which she used a series of flash cards to   $<$  tell  $>$   her experience of being blackmailed into  exposing her breasts via webcam , and of being   $<$    bullied  $>$   and physically assaulted . \nEvent Pairs:\n\n  $<$   tell  $>$   and  $<$   bullied  $>\n\n$   $<$   bullied  $>$   and  $<$   tell  $>$  \n\nNO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENT. NO CO REFERENCE, BEFORE, PRECONDITION, NO SUB EVENT. "}
{"page": 21, "image_path": "doc_images/2310.09158v1_21.jpg", "ocr_text": "Conference acronym ’XX, June 03-05, 2018, Woodstock, NY\n\n3S\nz_\n\nUser\n\npo\n\n=\no\n8\n\npo\n\n5\no\n8\n\nTask Description:\n\nThere is a piece of text with two events marked by <\nand > symbols, and your task is to identify four\ntypes of relations between them\n\nText:\n\nThis trade gradually < declined > during the early\n19th century as demand for furs in Europe <\ndiminished > .\n\nEvent Pairs:\n< diminished > and < declined >\n< declined > and < diminished >\n\nAnswers:\n\nNO_COREFERENCE, NO. TEMPORAL , CAUSE, SUBEVENT. x\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\nYour answers are logically inconsistent.\n\nThere are some rules among the relations, you can\nselect some of them to reason or check your answers:\n(1) If event A CAUSEs event B, then event A happens\nBEFORE or OVERLAP event B, and they won’t have\ncoreference and subevent relations ---\n\n(2) If event A and event B do not have a temporal\nrelation, then they won’t have causal and subevent\nrelations ---\n\n(3) If event B is a SUBEVENT of event A, then they\nwon’t have coreference and causal relations, and\nevent A’s time should CONTAINS event B’s time ---\n\nNO_COREFERENCE, CONTAINS, XCAUSE, NO_SUBEVENT.\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\nYour answers are logically inconsistent.\n\nThere are some rules among the relations, you can\nselect some of them to reason or check your answers:\n(1) If event A CAUSEs event B, then event A happens\nBEFORE or OVERLAP event B, and they won’t have\ncoreference and subevent relations ---\n\n(2) If event A’s time CONTAINS event B’s time, then\nthey won’t have coreference and causal relations ---\n\nNO_COREFERENCE, BEFORE CAUSE, NO_SUBEVENT . vo\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\n| Iteration 0\n\nChatGPT\n\nIteration 1\n\nChatGPT\n\nIteration 2\n\nChatGPT\n\nChen, et al.\n\nFigure 15: Multi-turn conversation with ChatGPT. We retrieve relevant logical constraints and provide them to ChatGPT.\n\nH.3 Iterative Retrievals\n\nIn this section, we present a prompt example used in Section 4.3.2.\nAs shown in Figure 15, with iterative prompting, ChatGPT finally\noutputs the correct answers.\n", "vlm_text": "The image depicts a multi-turn conversation with ChatGPT, focusing on identifying relationships between events in a text. It includes several iterations where the user provides logical rules to refine ChatGPT's answers. Here's a breakdown of each iteration:\n\n- **Iteration 0:**\n  - **Task Description:** Identify the relations between marked events in text.\n  - **Text:** Discusses the decline in trade and demand for furs in Europe.\n  - **Event Pairs Identified:** \"<diminished> and <declined>\", \"<declined> and <diminished>\"\n  - **ChatGPT's Initial Answer:** NO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, SUBEVENT (incorrect for first pair).\n\n- **Iteration 1:**\n  - **User Comments:** Logic rules are introduced for checking answers.\n  - **Rules Provided:** Relationships between causation, coreference, temporal relations, and subevents.\n  - **ChatGPT's Response:** Adjusts to NO_COREFERENCE, CONTAINS (incorrect), NO_CAUSAL, NO_SUBEVENT.\n\n- **Iteration 2:**\n  - **User Feedback:** Continues to highlight logical inconsistencies.\n  - **Refined Rules:** Further specified logic for temporal containment and causality.\n  - **ChatGPT's Final Answer:** NO_COREFERENCE, BEFORE (incorrect), NO_CAUSAL, NO_SUBEVENT.\n\nThe dialogue highlights how retrieving logical constraints assists in refining answers through iteration.\nH.3 Iterative Retrievals \nIn this section, we present a prompt example used in Section 4.3.2. As shown in Figure 15, with iterative prompting, ChatGPT finally outputs the correct answers. "}
{"page": 22, "image_path": "doc_images/2310.09158v1_22.jpg", "ocr_text": "Learning To Teach Large Language Models Logical Reasoning\n\nH.4 Deductive Reasoning\n\nConference acronym °XX, June 03-05, 2018, Woodstock, NY\n\nThe highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity.\n\nVanilla ICL\n\nTask Description:\n\nGiven a problem statement as contexts, the task is\nto answer a logical reasoning question. For each\nquestion, choose from the following options: A)\nTrue, B) False, or C) Unknown.\n\nContext:\nThe cow likes the lion. The cow visits the tiger.\n\nQuestion:\nBased on the above information, is the following\nstatement true, false, or unknown? The tiger is\nnot young.\n\nThe correct option is:\nC.\n\nVanilla ICL plus CoT\n\nTask Description:\n\nGiven a problem statement as contexts, the task is\nto answer a logical reasoning question. For each\nquestion, choose from the following options: A)\nTrue, B) False, or C) Unknown.\n\nContext:\nThe cow likes the lion. The cow visits the tiger.\n\nQuestion:\nBased on the above information, is the following\nstatement true, false, or unknown? The tiger is\nnot young.\n\nReasoning:\n\nThe tiger likes the cow. The tiger likes the\nsquirrel. If something likes the squirrel and it\nlikes the cow, then it visits the tiger. So the\ntiger visits the tiger. If something visits the\ntiger then it is nice. So the tiger is nice. If\nsomething is nice and it sees the tiger then it is\nyoung. So the tiger is young.\n\nThe correct option is: B.\n\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\n\nCoT w. self-generated logical constraints\n\nTask Description:\n\nGiven a problem statement as contexts, the task is\nto answer a logical reasoning question. For each\nquestion, choose from the following options: A)\nTrue, B) False, or C) Unknown.\n\nContext:\nThe cow likes the lion. The cow visits the tiger\n\nQuestion:\nBased on the above information, is the following\nstatement true, false, or unknown? The tiger is\nnot young.\n\nFrom the text, we could first get facts:\n\nThe tiger likes the cow. The tiger likes the\nsquirrel.\n\nDue to the mentioned rules: if something likes the\nsquirrel and it likes the cow, then it visits the\ntiger.\n\nWe can deduce the facts: the tiger visits the\ntiger.\n\nDue to the mentioned rules: if something visits\nthe tiger then it is nice.\n\nWe can deduce the facts: the tiger is nice.\n\nDue to the mentioned rules: if something is nice\nand it sees the tiger then it is young.\n\nWe can deduce the facts: the tiger is young.\nTherefore, the correct option is: B.\n\n", "vlm_text": "H.4 Deductive Reasoning \nThe highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity. \nVanilla ICL \nTask Description: \nGiven a problem statement as contexts, the task is  to answer a logical reasoning question. For each  question, choose from the following options: A)  True, B) False, or C) Unknown. \nContext: \nThe cow likes the lion. The cow visits the tiger. (··· more context here ··· ) \nQuestion:  \nBased on the above information, is the following  statement true, false, or unknown? The tiger is  not young. \nVanilla ICL plus CoT \nThe image contains a logical reasoning task description and an example. The task involves determining if a given statement is true, false, or unknown, based on provided context and reasoning.\n\nThe task description outlines that for each question, one must choose from the options: A) True, B) False, or C) Unknown.\n\nThe given context in the example includes: \"The cow likes the lion. The cow visits the tiger.\"\n\nThe specific question asks if the statement \"The tiger is not young\" is true, false, or unknown.\n\nIn the reasoning section, it explains that the tiger likes both the cow and the squirrel. It further states a conditional rule that something liking the squirrel and liking the cow would visit the tiger, therefore the tiger is visited by the cow. \n\nThe reasoning concludes with the statement: \"The tiger is nice. If something is nice and it sees the tiger then it is young. So the tiger is young.\"\n\nFinally, based on the reasoning provided, the correct option is stated as B, indicating that the statement \"The tiger is not young\" is false.\nCoT w. self-generated logical constraints \nTask Description: \nGiven a problem statement as contexts, the task is  to answer a logical reasoning question. For each  question, choose from the following options: A)  True, B) False, or C) Unknown. \nContext: \nThe cow likes the lion. The cow visits the tiger.  (··· more context here ··· ) \nQuestion:  \nBased on the above information, is the following  statement true, false, or unknown? The tiger is  not young. \nThe tiger likes the cow. The tiger likes the  squirrel. Due to the mentioned rules: if something likes the  squirrel and it likes the cow, then it visits the  tiger. We can deduce the facts: the tiger visits the  tiger. Due to the mentioned rules: if something visits  the tiger then it is nice. We can deduce the facts: the tiger is nice. Due to the mentioned rules: if something is nice  and it sees the tiger then it is young. We can deduce the facts: the tiger is young. Therefore, the correct option is: B. "}
