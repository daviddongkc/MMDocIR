{"page": 0, "image_path": "doc_images/SnapNTell_0.jpg", "ocr_text": "arX1v:2403.04735v1 [cs.CV] 7 Mar 2024\n\nSnapNTell: Enhancing Entity-Centric Visual Question Answering with\nRetrieval Augmented Multimodal LLM\n\nJielin Qiu':?; Andrea Madotto!, Zhaojiang Lin', Paul A. Crook!, Yifan Ethan Xu’,\nXin Luna Dong!, Christos Faloutsos’, Lei Li?, Babak Damavandi', Seungwhan Moon!\n' Meta Reality Labs & FAIR, Meta ?Carnegie Mellon University\n\n{jieling, leili,christos } @cs.cmu.edu, {andreamad8,zhaojiang,pacrook,ethanxu, lunadong,shanemoon} @meta.com\n\nAbstract\n\nVision-extended LLMs have made significant\nstrides in Visual Question Answering (VQA).\nDespite these advancements, VLLMs still\nencounter substantial difficulties in handling\nqueries involving long-tail entities, with a ten-\ndency to produce erroneous or hallucinated\nresponses. In this work, we introduce a\nnovel evaluative benchmark named SnapNTell,\nspecifically tailored for entity-centric VQA.\nThis task aims to test the models’ capabilities\nin identifying entities and providing detailed,\nentity-specific knowledge. We have developed\nthe SnapNTell Dataset, distinct from tradi-\ntional VQA datasets: (1) It encompasses a wide\nrange of categorized entities, each represented\nby images and explicitly named in the answers;\n(2) It features QA pairs that require extensive\nknowledge for accurate responses. The dataset\nis organized into 22 major categories, contain-\ning 7,568 unique entities in total. For each\nentity, we curated 10 illustrative images and\ncrafted 10 knowledge-intensive QA pairs. To\naddress this novel task, we devised a scalable,\nefficient, and transparent retrieval-augmented\nmultimodal LLM. Our approach markedly out-\nperforms existing methods on the SnapNTell\ndataset, achieving a 66.5% improvement in the\nBELURT score. We will soon make the dataset\nand the source code publicly accessible.\n\n1 Introduction\n\nVision-extended LLMs have shown significant ad-\nvancements, excelling at capturing complex seman-\ntics and context-aware attributes needed for intri-\ncate tasks. However, their abilities in factual VQA\ntasks, which demand accurate, concrete answers\nabout real-world entities and phenomena, expose\ncertain limitations. Particularly, torso-to-tail or\nlong-tail entities, which constitute a large propor-\ntion of real-world data but appear infrequently in\ntraining datasets, pose a challenge. This scarcity\n\nWork done while at Meta\n\nBaseline\n|__, | This is a tower. Not fine-grained\nentity-centric answer\nSnapNTell\n“What is the This is Eiffel Tower. It is a wrought-\nbuilding inthe |, iron lattice tower on the Champ de\n\nMars in Paris, France. It is named after\nthe engineer Gustave Eiffel, whose\ncompany designed and built the tower.\n\nimage?”\n\nFigure 1: Comparing SnapNTell with existing methods\nreveals a distinctive focus. In the SnapNTell benchmark,\nthe answers are predominantly entity-centric, charac-\nterized by a greater depth of knowledgeable information\npertaining to the specific entity depicted in the image as\nthe answer.\n\nin representation often leads to VLLMs resorting\nto generating plausible but incorrect or imaginative\ncontent in their outputs, a problem that manifests\nas “hallucinations\" within the context of model re-\nsponses. To ensure the confident deployment of\nVLLMs in practical scenarios, there is an urgent\nneed for dedicated research that not only recognizes\nbut actively strives to tackle and reduce instances of\nhallucinations, especially in the context of factual\nqueries involving these long-tail entities.\n\nThe lack of publicly available evaluation datasets\nspecifically tailored to assess models’ ability in rec-\nognizing real-world long-tailed entities presents a\nnotable gap in VQA. Existing datasets fall short\nin serving this purpose due to a narrow range of\nentity categories, the prevalence of overly simplis-\ntic yes/no QA pairs, and a general lack of entity\nspecificity, often using broad terms like “Tiger\"\ninstead of more specific ones like “Siberian Tiger\".\nTo address this gap, we introduce a novel eval-\nuation task called SnapNTell, which focuses on\nentity-centric knowledge-based VQA. The Snap-\nNTell benchmark has been designed to evaluate\nmodels’ abilities in accurately identifying entities\nand generating responses that showcase a deep un-\nderstanding of these entities. To support this task,\nwe have curated a new evaluation dataset that de-\nparts from existing datasets in two crucial ways: (1)\n", "vlm_text": "SnapNTell : Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM \nJielin  $\\mathbf{Q}\\mathbf{i}\\mathbf{u}^{1,2};$  , Andrea Madotto 1 , Zhaojiang  $\\mathbf{Li}\\mathbf{n}^{1}$  , Paul A. Crook 1 , Yifan Ethan   $\\mathbf{X}\\mathbf{u}^{1}$  , Xin Luna   $\\mathbf{Doneg^{1}}$  , Christos Faloutsos 2 , Lei  $\\mathbf{Li^{2}}$  , Babak Damavandi 1 , Seungwhan Moon 1 1  Meta Reality Labs & FAIR, Meta  2 Carnegie Mellon University \n{ jielinq,leili,christos } @cs.cmu.edu,  { andreamad8,zhaojiang,pacrook,ethanxu,lunadong,shanemoon } @meta.com \nAbstract \nVision-extended LLMs have made significant strides in Visual Question Answering (VQA). Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a ten- dency to produce erroneous or hallucinated responses. In this work, we introduce a novel evaluative benchmark named  SnapNTell , specifically tailored for entity-centric VQA. This task aims to test the models’ capabilities in identifying entities and providing detailed, entity-specific knowledge. We have developed the  SnapNTell Dataset , distinct from tradi- tional VQA datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features QA pairs that require extensive knowledge for accurate responses. The dataset is organized into 22 major categories, contain- ing 7,568 unique entities in total. For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive QA pairs. To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM. Our approach markedly out- performs existing methods on the SnapNTell dataset, achieving a  $66.5\\%$   improvement in the BELURT score. We will soon make the dataset and the source code publicly accessible. \n1 Introduction \nVision-extended LLMs have shown significant ad- vancements, excelling at capturing complex seman- tics and context-aware attributes needed for intri- cate tasks. However, their abilities in factual VQA tasks, which demand accurate, concrete answers about real-world entities and phenomena, expose certain limitations. Particularly, torso-to-tail or long-tail entities, which constitute a large propor- tion of real-world data but appear infrequently in training datasets, pose a challenge. This scarcity \nThe image shows a visual comparison of two systems answering the question \"What is the building in the image?\" with a picture of the Eiffel Tower.\n\n- The baseline system provides a simple answer: \"This is a tower.\"\n- The SnapNTell system offers a more detailed response: \"This is the Eiffel Tower. It is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\nFigure 1: Comparing SnapNTell with existing methods reveals a distinctive focus. In the SnapNTell benchmark, the answers are predominantly  entity-centric , charac- terized by a greater depth of knowledgeable information pertaining to the specific entity depicted in the image as the answer. \nin representation often leads to VLLMs resorting to generating plausible but incorrect or imaginative content in their outputs, a problem that manifests as “hallucinations\" within the context of model re- sponses. To ensure the confident deployment of VLLMs in practical scenarios, there is an urgent need for dedicated research that not only recognizes but actively strives to tackle and reduce instances of hallucinations, especially in the context of factual queries involving these long-tail entities. \nThe lack of publicly available evaluation datasets specifically tailored to assess models’ ability in rec- ognizing real-world long-tailed entities presents a notable gap in VQA. Existing datasets fall short in serving this purpose due to a narrow range of entity categories, the prevalence of overly simplis- tic yes/no QA pairs, and a general lack of entity specificity, often using broad terms like “Tiger\" instead of more specific ones like “Siberian Tiger\". To address this gap, we introduce a novel eval- uation task called  SnapNTell , which focuses on entity-centric knowledge-based VQA. The Snap- NTell benchmark has been designed to evaluate models’ abilities in accurately identifying entities and generating responses that showcase a deep un- der standing of these entities. To support this task, we have curated a new evaluation dataset that de- parts from existing datasets in two crucial ways: (1) "}
{"page": 1, "image_path": "doc_images/SnapNTell_1.jpg", "ocr_text": "It includes a wide range of fine-grained and catego-\nrized entities, each accompanied by corresponding\nimages and clear mention of the entity name within\nthe answer sets. (2) It features QA pairs designed\nto prompt knowledge-intensive responses, moving\nbeyond the binary yes/no format to challenge and\nassess the depth of the model’s comprehension.\nFurthermore, the limitations identified in factual\nquery generation underscore the need for new so-\nlutions to address the problem of hallucinations.\nRecent advancements suggest that retrieval-based\napproaches hold significant promise in this regard\n(Guu et al., 2020; Srinivasan et al., 2022; Yang\net al., 2023a,b). These methods enhance LLMs by\nintegrating external knowledge sources or incor-\nporating retrieval mechanisms to access relevant\ninformation from extensive knowledge bases. The\nsynergy between the advanced inference capabil-\nities of LLMs and the wealth of external knowl-\nedge has the potential to significantly reduce issues\nrelated to long-tail entities and, consequently, de-\ncrease the occurrence of hallucinatory responses.\n\nIn this work, we aim to propose an evaluation\ntask to investigate the model’s ability to recog-\nnize real-world long-tailed entities and provide\nknowledge-intensive answers. We also propose\na retrieval-augmented method to reduce hallucina-\ntions and enhance the precision and trustworthiness\nof generated responses.\n\nOur contribution is summarized as follows:\n\n¢ SnapNTell task. We propose a novel task for\nentity-centric VQA, specifically designed to\nassess the proficiency of models in accurately\nidentifying and generating responses that ex-\nhibit a deep comprehension of these identified\nentities.\n\nSnapNTell model. We proposed a retrieval-\naugmented multimodal LLM, devised as a\nbaseline model capable of undertaking the\nSnapNTell task, which is scalable, effective,\nand explainable.\n\nSnapNTell dataset. We collected a new eval-\nuation dataset with distinctive characteristics,\nwhich stands out for two key features: (1) It\nencompasses a diverse range of fine-grained\nentities, each accompanied by correspond-\ning representative images. (2) The question-\nanswer pairs contain knowledge-intensive re-\nsponses with entity names specifically men-\ntioned in the answer sets.\n\n* Our model demonstrates superior perfor-\nmance on the SnapNTell dataset, surpassing\ncurrent methodologies with a 66.5% improve-\nment in BELURT score.\n\n2 Related Works\n\nKnowledge-based VQA_ Research in vision-\nlanguage tasks, which necessitate understanding\nimage content to answer questions, has seen sig-\nnificant advancements over recent years. Begin-\nning with datasets like FVQA (Wang et al., 2016),\nwhich extracted facts from pre-established knowl-\nedge bases, the field has progressed to more chal-\nlenging ones like the OK-VQA dataset (Marino\net al., 2019), encompassing diverse knowledge cat-\negories. MultiModalQA (Talmor et al., 2021) intro-\nduced complexity with questions demanding cross-\nmodal reasoning over snippets, tables, and images.\nThe successor of OK-VQA, AOK-VQA (Schwenk\net al., 2022), raises the bar by providing ques-\ntions that transcend simple knowledge base queries.\nManyModalQA (Hannan et al., 2020) shifts the\nfocus to answer modality selection, MIMOQA\n(Singh et al., 2021) emphasizes multimodal answer\nextraction, and WebQA (Chang et al., 2021) in-\ntroduces real-world knowledge-seeking questions,\nalbeit with some limitations regarding entity catego-\nrization and granularity. More comparison details\ncan be found in Section 3.5.\n\nMultimodal LLMs Integrating visual under-\nstanding into text-based LLM typically combines\nthem with a visual encoder and uses image cap-\ntioning datasets for alignment (Koh et al., 2023;\nWt et al., 2023; Chowdhery et al., 2022). Tech-\nniques like adapter-based tuning (Alayrac et al.,\n2022) and prefix tuning (Tsimpoukelli et al., 2021)\nallow these models to process visual inputs while\nmaintaining their linguistic capabilities, without\nrequiring full model retraining (Yin et al., 2023).\n\nRetrieval-augmented LLM Previous studies\nhave explored retrieval augmentation in text-only\nsettings or image captioning tasks. Guu et al.\n(2020) introduced a retriever for language models\nto access large corpus during various stages. Srini-\nvasan et al. (2022) showed retrieval-augmented\nqueries enhance LLMs’ context understanding. Ya-\nsunaga et al. (2023) and Yang et al. (2023a) de-\nveloped methods for integrating multimodal doc-\numents and speeding up LLM inference, respec-\ntively. Yang et al. (2023b) created a visual lan-\nguage model, inspired by Flamingo (Alayrac et al.,\n", "vlm_text": "It includes a wide range of fine-grained and catego- rized entities, each accompanied by corresponding images and clear mention of the entity name within the answer sets. (2) It features QA pairs designed to prompt knowledge-intensive responses, moving beyond the binary yes/no format to challenge and assess the depth of the model’s comprehension. \nFurthermore, the limitations identified in factual query generation underscore the need for new so- lutions to address the problem of hallucinations. Recent advancements suggest that retrieval-based approaches hold significant promise in this regard ( Guu et al. ,  2020 ;  Srinivasan et al. ,  2022 ;  Yang et al. ,  2023a , b ). These methods enhance LLMs by integrating external knowledge sources or incor- porating retrieval mechanisms to access relevant information from extensive knowledge bases. The synergy between the advanced inference capabil- ities of LLMs and the wealth of external knowl- edge has the potential to significantly reduce issues related to long-tail entities and, consequently, de- crease the occurrence of hallucinatory responses. \nIn this work, we aim to propose an evaluation task to investigate the model’s ability to recog- nize real-world long-tailed entities and provide knowledge-intensive answers. We also propose a retrieval-augmented method to reduce hallucina- tions and enhance the precision and trustworthiness of generated responses. \nOur contribution is summarized as follows: \n•  SnapNTell task.  We propose a novel task for entity-centric  VQA, specifically designed to assess the proficiency of models in accurately identifying and generating responses that ex- hibit a deep comprehension of these identified entities. \n•  SnapNTell model.  We proposed a retrieval- augmented multimodal LLM, devised as a baseline model capable of undertaking the SnapNTell task, which is scalable, effective, and explain able. \n•  SnapNTell dataset.  We collected a new eval- uation dataset with distinctive characteristics, which stands out for two key features: (1) It encompasses a diverse range of fine-grained entities, each accompanied by correspond- ing representative images. (2) The question- answer pairs contain knowledge-intensive re- sponses with entity names specifically men- tioned in the answer sets. \n•  Our model demonstrates superior perfor- mance on the SnapNTell dataset, surpassing current methodologies with a  $66.5\\%$   improve- ment in BELURT score. \n2 Related Works \nKnowledge-based VQA Research in vision- language tasks, which necessitate understanding image content to answer questions, has seen sig- nificant advancements over recent years. Begin- ning with datasets like FVQA ( Wang et al. ,  2016 ), which extracted facts from pre-established knowl- edge bases, the field has progressed to more chal- lenging ones like the OK-VQA dataset ( Marino et al. ,  2019 ), encompassing diverse knowledge cat- egories. Multi Modal QA ( Talmor et al. ,  2021 ) intro- duced complexity with questions demanding cross- modal reasoning over snippets, tables, and images. The successor of OK-VQA, AOK-VQA ( Schwenk et al. ,  2022 ), raises the bar by providing ques- tions that transcend simple knowledge base queries. Many Modal QA ( Hannan et al. ,  2020 ) shifts the focus to answer modality selection, MIMOQA ( Singh et al. ,  2021 ) emphasizes multimodal answer extraction, and WebQA ( Chang et al. ,  2021 ) in- troduces real-world knowledge-seeking questions, albeit with some limitations regarding entity catego- rization and granularity. More comparison details can be found in Section  3.5 . \nMultimodal LLMs Integrating visual under- standing into text-based LLM typically combines them with a visual encoder and uses image cap- tioning datasets for alignment ( Koh et al. ,  2023 ; Wu et al. ,  2023 ;  Chowdhery et al. ,  2022 ). Tech- niques like adapter-based tuning ( Alayrac et al. , 2022 ) and prefix tuning ( Tsim po uk ell i et al. ,  2021 ) allow these models to process visual inputs while maintaining their linguistic capabilities, without requiring full model retraining ( Yin et al. ,  2023 ). \nRetrieval-augmented LLM Previous studies have explored retrieval augmentation in text-only settings or image captioning tasks. Guu et al. ( 2020 ) introduced a retriever for language models to access large corpus during various stages.  Srini- vasan et al.  ( 2022 ) showed retrieval-augmented queries enhance LLMs’ context understanding.  Ya- sunaga et al. (2023) and Yang et al. (2023a) de-veloped methods for integrating multimodal doc- uments and speeding up LLM inference, respec- tively.  Yang et al.  ( 2023b ) created a visual lan- guage model, inspired by Flamingo ( Alayrac et al. , 2022 ), for image captioning with external database retrieval. Similarly,  Gui et al.  ( 2021 ) combined im- plicit and explicit knowledge in an encoder-decoder setup to improve answer generation. "}
{"page": 2, "image_path": "doc_images/SnapNTell_2.jpg", "ocr_text": "2022), for image captioning with external database\nretrieval. Similarly, Gui et al. (2021) combined im-\nplicit and explicit knowledge in an encoder-decoder\nsetup to improve answer generation.\n\nOpen-domain visual entity recognition Hu et al.\n(2023) developed OVEN for associating images\nwith Wikipedia entities via text queries, while Chen\net al. (2023) introduced INFOSEEK, a dataset for\nVisual Question Answering focused on informa-\ntional queries. While OVEN is proficient in entity\nrecognition using a knowledge base, INFOSEEK\nmainly supplies factual responses. Our study seeks\nto merge these strengths, creating detailed para-\ngraphs that provide context for a more compre-\nhensive understanding beyond basic facts. More\nrelated work can be found in Appendix E.\n\n3 SnapNTell Dataset\n\n3.1 Entity Categorization\n\nTo tackle the challenge of the new SnapNTell task,\nthe first step involves creating a comprehensive\ndataset that represents a wide array of real-world\nentities. Our dataset creation methodology entails\nselecting a diverse set of entity names from vari-\nous categories that mirror the diversity of the real\nworld. This selection encompasses both commonly\nencountered entities and less frequently encoun-\ntered ones. We have identified 22 categories that\nadequately represent a cross-section of entities one\nmight encounter in daily life. These categories\ninclude landmark, painting, sculpture, food, fruit,\nvegetable, mammal, amphibian, insect, fish, bird,\nreptile, celebrity, instrument, plant, electronics,\ntool, transportation, sport, book, household, and\ncar. More details about the categories can be re-\nferred to Table 10 in the Appendix.\n\nTo populate each category with specific enti-\nties, we leveraged Wikipedia as a primary resource\ndue to its extensive and detailed entries. (See Ap-\npendix A for more details.) Our selection criteria\nare heavily biased towards specificity; for instance,\nin the category of mammals, we deliberately opted\nfor precise names such as “German Shepherd” or\n“Alaskan Malamute” instead of the generic “Dog”.\nThis level of specificity is critical as it enables the\nmodel to demonstrate its capacity for fine-grained\nrecognition and its ability to generate detailed, ac-\ncurate information about each entity. This dataset-\nbuilding approach is what distinguishes our dataset\nfrom existing VQA datasets, which often lack fine-\ngrained entities and specificity.\n\n3.2 Image collection\n\nThe dataset comprises 22 primary categories, en-\ncapsulating a total of 7,568 unique entities. For\neach individual entity, a set of 10 images has been\ncurated, where the statistic of the entity list is\nshown in Table 10 in the Appendix.\n\nFiltering Initially, a comprehensive list of enti-\nties, encompassing 22 primary categories, was com-\npiled, in a total of 14,910 diverse entities. Then the\nentity list underwent filtering by cross-referencing\neach entry with its corresponding Wikipedia page.\nEntities lacking valid Wikipedia pages were sub-\nsequently removed from the list. For each corre-\nsponding entity, images were sourced from Cre-\native Commons (CC). Further filtering was con-\nducted by removing entities that didn’t have a suffi-\ncient number of images obtained via Google Image\nSearch engine. The collected metadata was stored\nin a CSV file containing essential information such\nas image URLs, source page URLs, renamed im-\nage names, and the corresponding Wikipedia page\nURLs. After filtering, the final number of entities\nin the SnapNTell dataset is 7,568. (More filtering\ndetails can be found in Appendix B.)\n\n3.3 Knowledge-intensive Question-Answer\nPairs\n\nIn our SnapNTell dataset, we considered five types\nof questions:\n\n¢ Static facts (absolute facts, discrete facts).\nThese are objective facts that are concrete and\nare not contingent on other conditions. They\ncan usually be answered with a unique answer.\ni.e., “When was he (Barack Obama) born?\"\n\nNarrative facts. These facts encompass com-\nprehension of larger contexts (e.g., song lyrics,\nmovie plot). They are factual in the sense that\nthe content of the narrative should accurately\nreflect the source material or events, but a cor-\nrect answer is usually not unique, as they can\nvary in their level of detail and focus. i.e.,\n“What is the plot of that (“The Godfather’)?\"\n\n¢ Dynamic facts. These are facts that are sub-\nject to change over time. i.e., “What is the\nYelp customer rating of it (the Eleven Madi-\nson Park restaurant) in NYC?\"\n\nProcedural facts. These are usually answers\nto “how” questions, outlining a sequence of\nsteps to accomplish a task. While the steps\nmay not be unique and could be subjective,\n", "vlm_text": "\nOpen-domain visual entity recognition Hu et al. ( 2023 ) developed OVEN for associating images with Wikipedia entities via text queries, while  Chen et al.  ( 2023 ) introduced INFOSEEK, a dataset for Visual Question Answering focused on informa- tional queries. While OVEN is proficient in entity recognition using a knowledge base, INFOSEEK mainly supplies factual responses. Our study seeks to merge these strengths, creating detailed para- graphs that provide context for a more compre- hensive understanding beyond basic facts. More related work can be found in Appendix  E . \n3 SnapNTell Dataset \n3.1 Entity Categorization \nTo tackle the challenge of the new SnapNTell task, the first step involves creating a comprehensive dataset that represents a wide array of real-world entities. Our dataset creation methodology entails selecting a diverse set of entity names from vari- ous categories that mirror the diversity of the real world. This selection encompasses both commonly encountered entities and less frequently encoun- tered ones. We have identified 22 categories that adequately represent a cross-section of entities one might encounter in daily life. These categories include  landmark, painting, sculpture, food, fruit, vegetable, mammal, amphibian, insect, fish, bird, reptile, celebrity, instrument, plant, electronics, tool, transportation, sport, book, household, and car . More details about the categories can be re- ferred to Table  10  in the Appendix. \nTo populate each category with specific enti- ties, we leveraged Wikipedia as a primary resource due to its extensive and detailed entries. (See Ap- pendix  A  for more details.) Our selection criteria are heavily biased towards specificity; for instance, in the category of mammals, we deliberately opted for precise names such as “German Shepherd” or “Alaskan Malamute” instead of the generic “Dog”. This level of specificity is critical as it enables the model to demonstrate its capacity for fine-grained recognition and its ability to generate detailed, ac- curate information about each entity. This dataset- building approach is what distinguishes our dataset from existing VQA datasets, which often lack fine- grained entities and specificity. \n3.2 Image collection \nThe dataset comprises 22 primary categories, en- cap sula ting a total of 7,568 unique entities. For each individual entity, a set of 10 images has been curated, where the statistic of the entity list is shown in Table  10  in the Appendix. \nFiltering Initially, a comprehensive list of enti- ties, encompassing 22 primary categories, was com- piled, in a total of 14,910 diverse entities. Then the entity list underwent filtering by cross-referencing each entry with its corresponding Wikipedia page. Entities lacking valid Wikipedia pages were sub- sequently removed from the list. For each corre- sponding entity, images were sourced from Cre- ative Commons (CC). Further filtering was con- ducted by removing entities that didn’t have a suffi- cient number of images obtained via Google Image Search engine. The collected metadata was stored in a CSV file containing essential information such as image URLs, source page URLs, renamed im- age names, and the corresponding Wikipedia page URLs. After filtering, the final number of entities in the SnapNTell dataset is 7,568. (More filtering details can be found in Appendix  B .) \n3.3 Knowledge-intensive Question-Answer Pairs \nIn our SnapNTell dataset, we considered five types of questions: \n•  Static facts (absolute facts, discrete facts). These are objective facts that are concrete and are not contingent on other conditions. They can usually be answered with a unique answer. i.e., “When was he (Barack Obama) born?\" \n•  Narrative facts.  These facts encompass com- prehension of larger contexts (e.g., song lyrics, movie plot). They are factual in the sense that the content of the narrative should accurately reflect the source material or events, but a cor- rect answer is usually not unique, as they can vary in their level of detail and focus. i.e., “What is the plot of that (‘The Godfather’)?\" \n•  Dynamic facts.  These are facts that are sub- ject to change over time. i.e., “What is the Yelp customer rating of it (the Eleven Madi- son Park restaurant) in NYC?\" \n•  Procedural facts.  These are usually answers to “how” questions, outlining a sequence of steps to accomplish a task. While the steps may not be unique and could be subjective, "}
{"page": 3, "image_path": "doc_images/SnapNTell_3.jpg", "ocr_text": "the answer can still be classified as logical or\nnonsensical. Note that these facts may some-\ntimes overlap with dynamic facts or narrative\nfacts, i.e., “How do you check the battery level\nof my item (Ray-Ban Stories Glasses)?\"\n\nSubjective facts. (opinion-based facts).\nThese “facts” are not objective indisputable\nfacts, but based on individual perspectives\nor experience. Recommendations fall in this\ncategory. While there’s generally no single\ncorrect answer to questions seeking subjec-\ntive facts, it still requires the system to un-\nderstand the topic and provide reasonable an-\nswers grounded by world facts. i.e., “Why do\nyou like it (Niagara Falls)?\"\n\nTo construct a comprehensive and knowledge-\nintensive QA dataset, we employ a three-step\nprocess. Firstly, we extracted and condensed perti-\nnent information from Wikipedia for each entity,\nie., the summary of the introduction, the caption of\nthe image, etc. (See Appendix A for more details).\nFollowing similar approaches proposed by LLaVA\n(Liu et al., 2023b), Dettmers et al. (2023) is utilized\nto generate QA pairs for each entity automatically\nbased on five pre-defined question types, ensuring\ndiversity and informativeness. Then, we enlisted\nthree annotators (2 male and | female) from\nAmazon SageMaker to assess QA pair quality\nand make necessary revisions to meet specific\ncriteria. The responsibilities of these annotators\ninclude: (1) ensuring that the images and QA\npairs are semantically aligned, (2) validating the\naccuracy of the provided answers, (3) making sure\nthe questions are free of particular entity names\nbut demanding such specificity in the answers,\n(4) assessing if the modified QA pairs adhere\nto the criteria for knowledge-intensive content,\nand (5) removing specific entity-related details\nfrom the questions. This last step guarantees that\nthe question queries cannot be answered without\nunderstanding the accompanying visual context.\n\nQuality and consistency In order to verify the\nquality of the QA pairs, we conducted a quality\nevaluation by randomly choosing 1,000 QA pairs\nfrom our dataset. We assigned three independent\nhuman evaluators (1 male, 2 female) from Amazon\nSageMaker to review these pairs for accuracy [ac-\ncurate, inaccurate] and agreement on whether to\nsave the QA pair by Fleiss’ Kappa (Fleiss, 1971).\nThe outcome of this assessment revealed 98% ac-\n\ncuracy and k = 0.95 agreement rate among the\nevaluators, demonstrating a significant degree of\nuniformity in the quality of the QA pairs.\n\n3.4 Statistics and Analysis of Our Dataset\n\nEntity statistics To provide a clear summary of\nthis comprehensive dataset, we have condensed the\ndetails of the entity list into Table 10 and Figure 9\n(in Appendix F). Our analysis indicates that the\ndataset displays a well-balanced distribution across\ndifferent categories, enhancing its balanced and\ndiverse characteristics. Such a balanced and diverse\ncomposition enhances the representativeness of our\nproposed evaluation dataset.\n\nPopularity The importance of entity popularity\nin search engines is a key aspect to consider, simi-\nlar to examining the head, torso, and tail sections of\nknowledge bases within search engine frameworks.\nAs demonstrated in Figure 11 in Appendix F, we\nuse the average Wikipedia pageviews per entity\nover the last 60 days as the metric. This average is\ncalculated by summing up the pageviews and then\ndividing by the number of entities. The insights\nfrom Figure | 1 reveal that entities in the celebrity\ncategory have the highest average popularity. For\na broader comparison among different categories,\nwe also present a comprehensive analysis of total\npageviews for all categories in Figure 10 in Ap-\npendix F, which shows that the celebrity category\nremains at the forefront in terms of overall entity\npopularity. This is attributed to the combination of\na higher number of entities in this category and the\ngenerally higher popularity of each entity within it.\n\n3.5 Comparison with Existing VQA Datasets\n\nIn Table 2 and Figure 2, we present a compari-\nson with existing VQA datasets. It is evident that\nsome existing VQA datasets lack categorization,\nfine-grained entities, and knowledge-intensive an-\nswers, as observed in VQA 2.0 (Goyal et al., 2016)\nand GQA (Hudson and Manning, 2019). OK-VQA\n(Marino et al., 2019) contains images that may not\nbe sufficient to answer the questions, encouraging\nreliance on external knowledge resources. How-\never, the answers in OK-VQA are often simplistic\nbinary (yes/no) responses or selections from the\nquestions. A-OKVQA (Schwenk et al., 2022), the\nsuccessor of OK-VQA, aims to provide questions\nthat require commonsense reasoning about the de-\npicted scene but use general object names in the\nanswers. MultiModalQA (Talmor et al., 2021) fo-\n", "vlm_text": "the answer can still be classified as logical or nonsensical. Note that these facts may some- times overlap with dynamic facts or narrative facts, i.e., “How do you check the battery level of my item (Ray-Ban Stories Glasses)?\" \n•  Subjective facts. (opinion-based facts). These “facts” are not objective indisputable facts, but based on individual perspectives or experience. Recommendations fall in this category. While there’s generally no single correct answer to questions seeking subjec- tive facts, it still requires the system to un- derstand the topic and provide reasonable an- swers grounded by world facts. i.e., “Why do you like it (Niagara Falls)?\" \nTo construct a comprehensive and knowledge- intensive QA dataset, we employ a three-step process. Firstly, we extracted and condensed perti- nent information from Wikipedia for each entity, i.e., the summary of the introduction, the caption of the image, etc. (See Appendix  A  for more details). Following similar approaches proposed by LLaVA ( Liu et al. ,  2023b ),  Dettmers et al.  ( 2023 ) is utilized to generate QA pairs for each entity automatically based on five pre-defined question types, ensuring diversity and informative ness. Then, we enlisted three annotators (2 male and 1 female) from Amazon SageMaker to assess QA pair quality and make necessary revisions to meet specific criteria. The responsibilities of these annotators include: (1) ensuring that the images and QA pairs are semantically aligned, (2) validating the accuracy of the provided answers, (3) making sure the questions are free of particular entity names but demanding such specificity in the answers, (4) assessing if the modified QA pairs adhere to the criteria for knowledge-intensive content, and (5) removing specific entity-related details from the questions. This last step guarantees that the question queries cannot be answered without understanding the accompanying visual context. \nQuality and consistency In order to verify the quality of the QA pairs, we conducted a quality evaluation by randomly choosing   $1{,}000\\,\\mathrm{QA}$   pairs from our dataset. We assigned three independent human evaluators (1 male, 2 female) from Amazon SageMaker to review these pairs for accuracy [ ac- curate, inaccurate ] and agreement on whether to save the QA pair by Fleiss’ Kappa ( Fleiss ,  1971 ). The outcome of this assessment revealed   $98\\%$   ac- curacy and    $\\kappa\\,=\\,0.95$   agreement rate among the evaluators, demonstrating a significant degree of uniformity in the quality of the QA pairs. \n\n3.4 Statistics and Analysis of Our Dataset \nEntity statistics To provide a clear summary of this comprehensive dataset, we have condensed the details of the entity list into Table  10  and Figure  9 (in Appendix  F ). Our analysis indicates that the dataset displays a well-balanced distribution across different categories, enhancing its balanced and diverse characteristics. Such a balanced and diverse composition enhances the representative ness of our proposed evaluation dataset. \nPopularity The importance of entity popularity in search engines is a key aspect to consider, simi- lar to examining the head, torso, and tail sections of knowledge bases within search engine frameworks. As demonstrated in Figure  11  in Appendix  F , we use the average Wikipedia pageviews per entity over the last 60 days as the metric. This average is calculated by summing up the pageviews and then dividing by the number of entities. The insights from Figure  11  reveal that entities in the celebrity category have the highest average popularity. For a broader comparison among different categories, we also present a comprehensive analysis of total pageviews for all categories in Figure  10  in Ap- pendix  F , which shows that the celebrity category remains at the forefront in terms of overall entity popularity. This is attributed to the combination of a higher number of entities in this category and the generally higher popularity of each entity within it. \n3.5 Comparison with Existing VQA Datasets \nIn Table  2  and Figure  2 , we present a compari- son with existing VQA datasets. It is evident that some existing VQA datasets lack categorization, fine-grained entities, and knowledge-intensive an- swers, as observed in VQA 2.0 ( Goyal et al. ,  2016 ) and GQA ( Hudson and Manning ,  2019 ). OK-VQA ( Marino et al. ,  2019 ) contains images that may not be sufficient to answer the questions, encouraging reliance on external knowledge resources. How- ever, the answers in OK-VQA are often simplistic binary (yes/no) responses or selections from the questions. A-OKVQA ( Schwenk et al. ,  2022 ), the successor of OK-VQA, aims to provide questions that require commonsense reasoning about the de- picted scene but use general object names in the answers. Multi Modal QA ( Talmor et al. ,  2021 ) fo- "}
{"page": 4, "image_path": "doc_images/SnapNTell_4.jpg", "ocr_text": "Table 1: More detailed comparison with existing knowledge-based VQA datasets. Anonymity means whether the\nquestion already contains a knowledge clue related to the entity in question. (* Unclear)\n\nDataset | Categories | Unique Entity | QA Pairs | Images | Average Ans Length | Number of Images / Entity | Anonymity\nViQuAE 3 2,400 3,700 3,300 1.8 * x\nEncyclopedic VQA (test) 12 * 5,750 5,750 3.2 * x\n\nTable 2: Comparison with existing VQA datasets Knowl-\nedge means the QA pairs are knowledgeable, not simple\nyes/no answers or selection questions. Entities means\nwhether there are fine-grained entities specifically con-\ntained in answers. Categorization means the entities are\ncategorized, not randomly crawled online.\n\nDataset\n\nVQA 2.0 (Goyal et al., 2016)\n\nGQA (Hudson and Manning, 2019)\nOK-VQA (Marino et al., 2019)\nManyModalQA (Hannan et al., 2020)\nMultiModalQA (Talmor et al., 2021)\nMIMOQA (Singh et al., 2021)\nA-OKVQA (Schwenk et al., 2022)\nWebQA (Chang et al., 2021)\n\nViQUAE (Lerner et al., 2022)\nEncyclopedic VQA (Mensink et al., 2023)\n\n| Knowledge | Entities | Categorization\n\nVNAARASN\nSAN\nSAN\n\ncuses on cross-modal knowledge extraction but re-\nlies on question templates for question generation.\nManyModalQA (Hannan et al., 2020) focuses on\nanswer modality choice rather than knowledge ag-\ngregation or extraction. In MIMOQA (Singh et al.,\n2021), the task of extracting a multimodal answer\nis not necessarily knowledge-intensive. WebQA\n(Chang et al., 2021) does have categorization but\nlacks fine-grained entities in many QA pairs, result-\ning in more general questions and answers. Our\nproposed SnapNTell differs by including a wide\nrange of fine-grained entities with representative\nimages and explicit entity names in the answer sets.\nAdditionally, it incorporates question-answer pairs\nthat demand knowledge-intensive responses, going\nbeyond simplistic binary answers. Examples of our\ndataset can be found in Figure 8 in Appendix F.\n\nViQUAE (Lerner et al., 2022) and Encyclope-\ndic VQA (Mensink et al., 2023) both incorporate\nentity-level knowledge-based information along\nwith categorization. Therefore, we performed a\nmore in-depth analysis comparing them in Table 1.\nOur dataset surpasses these in terms of the vari-\nety of categories, the number of distinct entities,\nand the overall number of QA pairs. Additionally,\nour dataset boasts a higher count of images and a\nlonger average length for answers. Specifically, our\ndataset is structured to include 10 images for each\nentity, whereas the exact number of images per en-\ntity in ViQuAE and Encyclopedic VQA remains\nunspecified. Most notably, our dataset’s questions\nare highly anonymous, implying that they do not\n\nVOQA v2\n\nQ:Isthe umbrella Q: What animal _—_Q: Is the photo from\nupside down? is in the box? the 50’s or the 90's?\nA: No A: Bear A: 50's\n\nSnapNTell\n\nQ: What is the current status of it?\nA: The Mendenhall Glacier is\ncurrently experiencing a negative\nglacier mass balance and will likely\ncontinue to retreat.\n\nFigure 2: Comparison with existing datasets, where pre-\nvious VQA datasets mostly focus on freeform answers\n(such as yes/no for verification questions and choice for\nselection questions).\n\nreveal any knowledge hints about the entity. This\ndesign ensures that the questions cannot be straight-\nforwardly answered without interpreting the image\ndata, setting our dataset apart from both ViQuAE\nand Encyclopedic VQA.\n\n4 Method\n\nIn this section, we will introduce the details of our\nproposed retrieval-augmented multimodal LLM\nmodel. The architecture of our model is shown\nin Figure 3 (larger figure in Appendix D due\nto space limit). Our model can be considered\ntwofold: (1) Retrieval augmentation. Given the\ninput image-question pair, we retrieve useful entity-\ncentric information within knowledge sources. (2)\nEntity-centric knowledge-based answer genera-\ntion. The retrieved information will be combined\nwith the image and question together to generate a\nknowledgeable answer.\n\n4.1 Retrieval Augmentation\n\nThe retrieval augmentation process can be sub-\ndivided into: (i) Semantic region extraction via\nlanguage-guided object detection, (ii) Entity recog-\nnition via image retrieval, and (iii) Knowledge re-\ntrieval via multi-source aggregation.\n\nSemantic Region Extraction via Language-\nGuided Object Detection To improve recogni-\ntion performance, we focus on extracting specific\n", "vlm_text": "The table compares three datasets based on various attributes:\n\n1. **ViQuAE**\n   - Categories: 3\n   - Unique Entity: 2,400\n   - QA Pairs: 3,700\n   - Images: 3,300\n   - Average Answer Length: 1.8\n   - Number of Images / Entity: *\n   - Anonymity: ✗\n\n2. **Encyclopedic VQA (test)**\n   - Categories: 12\n   - Unique Entity: *\n   - QA Pairs: 5,750\n   - Images: 5,750\n   - Average Answer Length: 3.2\n   - Number of Images / Entity: *\n   - Anonymity: ✗\n\n3. **SnapNTell (Ours)**\n   - Categories: 22\n   - Unique Entity: 7,568\n   - QA Pairs: 75,680\n   - Images: 75,680\n   - Average Answer Length: 25.7\n   - Number of Images / Entity: 10\n   - Anonymity: ✓\n\nThe SnapNTell dataset is highlighted and indicates that it offers more categories, unique entities, QA pairs, images, and features anonymity with a significantly longer average answer length compared to the others.\nThe table lists various datasets related to visual question answering (VQA) and their attributes regarding knowledge, entities, and categorization. Here's a summary:\n\n1. **VQA 2.0 (Goyal et al., 2016)**: Knowledge ✓\n2. **GQA (Hudson and Manning, 2019)**: Entities ✓\n3. **OK-VQA (Marino et al., 2019)**: Knowledge ✓\n4. **ManyModalQA (Hannan et al., 2020)**: Entities ✓\n5. **MultiModalQA (Talmor et al., 2021)**: Entities ✓\n6. **MIMOQA (Singh et al., 2021)**: Knowledge ✓, Entities ✓\n7. **A-OKVQA (Schwenk et al., 2022)**: Knowledge ✓, Entities ✓\n8. **WebQA (Chang et al., 2021)**: Knowledge ✓, Entities ✓\n9. **ViQuAE (Lerner et al., 2022)**: Knowledge ✓, Entities ✓\n10. **Encyclopedic VQA (Mensink et al., 2023)**: Knowledge ✓, Entities ✓\n11. **SnapNTell (Ours)**: Knowledge ✓, Entities ✓, Categorization ✓\n\nThe checkmarks indicate the presence of these features in each dataset. SnapNTell includes all three features - knowledge, entities, and categorization.\ncuses on cross-modal knowledge extraction but re- lies on question templates for question generation. Many Modal QA ( Hannan et al. ,  2020 ) focuses on answer modality choice rather than knowledge ag- gregation or extraction. In MIMOQA ( Singh et al. , 2021 ), the task of extracting a multimodal answer is not necessarily knowledge-intensive. WebQA ( Chang et al. ,  2021 ) does have categorization but lacks fine-grained entities in many QA pairs, result- ing in more general questions and answers. Our proposed SnapNTell differs by including a wide range of fine-grained entities with representative images and explicit entity names in the answer sets. Additionally, it incorporates question-answer pairs that demand knowledge-intensive responses, going beyond simplistic binary answers. Examples of our dataset can be found in Figure  8  in Appendix  F . \nViQuAE ( Lerner et al. ,  2022 ) and Encyclope- dic VQA ( Mensink et al. ,  2023 ) both incorporate entity-level knowledge-based information along with categorization. Therefore, we performed a more in-depth analysis comparing them in Table  1 . Our dataset surpasses these in terms of the vari- ety of categories, the number of distinct entities, and the overall number of QA pairs. Additionally, our dataset boasts a higher count of images and a longer average length for answers. Specifically, our dataset is structured to include 10 images for each entity, whereas the exact number of images per en- tity in ViQuAE and Encyclopedic VQA remains unspecified. Most notably, our dataset’s questions are highly anonymous, implying that they do not \nThe image contains comparisons of different Visual Question Answering (VQA) datasets with a new dataset called \"SnapNTell.\" \n\n1. **VQA v2**:\n   - Image: A person with an umbrella.\n   - Question: \"Is the umbrella upside down?\"\n   - Answer: \"No\"\n\n2. **GQA**:\n   - Image: A box with a stuffed animal.\n   - Question: \"What animal is in the box?\"\n   - Answer: \"Bear\"\n\n3. **OK-VQA**:\n   - Image: A person in a kitchen.\n   - Question: \"Is the photo from the 50’s or the 90’s?\"\n   - Answer: \"50’s\"\n\n4. **SnapNTell**:\n   - Image: Mendenhall Glacier.\n   - Question: \"What is the current status of it?\"\n   - Answer: \"The Mendenhall Glacier is currently experiencing a negative glacier mass balance and will likely continue to retreat.\"\n\nThe caption notes that previous VQA datasets focus on freeform answers like yes/no or choice selection.\nreveal any knowledge hints about the entity. This design ensures that the questions cannot be straight- forwardly answered without interpreting the image data, setting our dataset apart from both ViQuAE and Encyclopedic VQA. \n4 Method \nIn this section, we will introduce the details of our proposed retrieval-augmented multimodal LLM model. The architecture of our model is shown in Figure  3  (larger figure in Appendix  D  due to space limit). Our model can be considered twofold: (1)  Retrieval augmentation . Given the input image-question pair, we retrieve useful entity- centric information within knowledge sources. (2) Entity-centric knowledge-based answer genera- tion . The retrieved information will be combined with the image and question together to generate a knowledgeable answer. \n4.1 Retrieval Augmentation \nThe retrieval augmentation process can be sub- divided into: (i) Semantic region extraction via language-guided object detection, (ii) Entity recog- nition via image retrieval, and (iii) Knowledge re- trieval via multi-source aggregation. \nSemantic Region Extraction via Language- Guided Object Detection To improve recogni- tion performance, we focus on extracting specific "}
{"page": 5, "image_path": "doc_images/SnapNTell_5.jpg", "ocr_text": "‘Word Embedding Layer\n\nRetrieval\n‘Augmentation\n\nFigure 3: Our SnapNTell model architecture takes an\nimage-question pair as input. It begins with retrieval\naugmentation to source relevant information about the\nentity in the image. This information, along with the\nquestion, feeds into the word embedding layer. Text\nembeddings merge with image-projected embeddings\nbefore entering the LLM, culminating in a knowledge-\nable answer as the output.\n\nimage regions containing the entity, rather than\ngeneral image-level recognition. We employ a\nlanguage-guided object detection model, i.e., GLIP\n(Li et al., 2021), for language-guided object detec-\ntion, extracting regions relevant to textual queries\nby understanding the query context. This targeted\napproach ensures precise region extraction, enhanc-\ning the system’s accuracy and contextual relevance.\n\nEntity Recognition via Image Retrieval We\nconstruct a similarity index using CLIP embed-\ndings (Radford et al., 2021) and Faiss (Johnson\net al., 2017) for indexing. Our database, built on\nthe WIT dataset (Srinivasan et al., 2021), maps\nCLIP image embeddings to their text descriptions,\nleveraging Faiss’s robust similarity search capa-\nbilities. After setting up the indexing database,\ngiven an input query image J, we perform a k-\nnearest neighbor retrieval based on cosine simi-\nlarity. The retrieval outcomes are represented as\nR(L) = {(41, €1) ,+++ (te, ch) }, where for each j\nwithin the range of 1 to k, i; and cj correspond\nto the retrieved image and its associated caption,\nrespectively. By comparing J with similar images\nfrom the database, we identify the entity in the\nimage region, which enables precise image-level\nentity recognition.\n\nKnowledge Retrieval via Multi-Source Aggrega-\ntion Facing diverse user queries, we gather extra\ninformation to compile resources for accurate re-\nsponses. Some queries require up-to-date informa-\ntion, not present in existing databases. We then turn\nto external sources to collect critical data like “year\nbuilt,\" “description,” and more. By using Knowl-\nedge Graph (KG) and web searches, we access rele-\n\nvant knowledge links, enriching our understanding\nof the specified image region, and improving our\nability to comprehend and contextualize the ex-\ntracted content. More details of the method can be\nfound in Appendix D.\n\n4.2 Entity-centric Knowledge-based Answer\nGeneration\n\nFollowing information collection, we enter the inte-\ngration phase, blending the input image, question,\nand retrieved data to generate a knowledgeable\nresponse, which is illustrated in Figure 3. Our\nmethod enhances multimodal understanding by\npre-training a LLM with image-text paired data.\nTaking cues from Moon et al. (2023), we employ\nlightweight adapters for each modality, converting\ninputs into the text token embedding space of the\nchosen LLM.\n\nIn our method, the LLM’s text token embedding\nspace morphs into a unified space, representing\nboth text and image content, with each modality\nassigned 64 to 256 token embeddings. We freeze\nthe LLM’s parameters during alignment training\nto quicken convergence and retain the LLM’s rea-\nsoning skills for inference. To ensure feature align-\nment, we use an image encoder, g(-), previously\nsynchronized with a text embedding space, like\nin CLIP (Radford et al., 2021; Schuhmann et al.,\n2022). For text-image pairs (Xyexr, Ximage), We\nalign them using specific objectives and a projec-\ntion module, like the Perceiver Resampler (Alayrac\net al., 2022), applied to the vision encoder as:\n\nL\nP(XreulXimage) = [] Po(Xeh|Zimages Zien!) — A)\n\ni=l\n\nZimage = Projg (Riatents, g(Ximage)) (2)\n\n5 Experiments and Results\n\n5.1 Experimental Setup\n\nEvaluation Metrics (1) In our evaluation pro-\ncess, the quality of the answers is first assessed\nusing established NLP metrics such as BLEU (Pa-\npineni et al., 2002), METEOR (Denkowski and\nLavie, 2014), ROUGE (Lin, 2004), and BLEURT\n(Sellam et al., 2020; Pu et al., 2021). (2) Addition-\nally, we incorporate accuracy and hallucination rate\nmetrics from (Sun et al., 2023). These metrics used\nGPT4 to automatically measure the proportion of\nquestions for which the model provides correct\nanswers or incorrect/partially incorrect answers,\nrespectively. (3) We conduct human evaluation\nfollowing Ye et al. (2023); Moon et al. (2023).\n", "vlm_text": "The image is a flowchart diagram illustrating a process for image and question processing involving a large language model (LLM).\n\n1. **Input**: An image and a question, e.g., \"What's the building in the image?\".\n2. **Image Encoder**: Processes the input image.\n3. **Entity Detection & Recognition Models**: Detect and recognize entities in the image, identifying the Eiffel Tower in this example.\n4. **Retrieval Augmentation**: Uses recognized entities to retrieve additional information from a database.\n5. **Projection Layers**: Process information from the image and combine it with textual data.\n6. **Word Embedding Layer and LLM**: The word embedding layer processes text, feeding it into the LLM.\n7. **Answer Generation**: The LLM combines image and text data to produce an answer.\n\nArrows indicate the data flow, showing forward and backward passes (gradient flow) across the layers.\nFigure 3: Our SnapNTell model architecture takes an image-question pair as input. It begins with retrieval augmentation to source relevant information about the entity in the image. This information, along with the question, feeds into the word embedding layer. Text embeddings merge with image-projected embeddings before entering the LLM, culminating in a knowledge- able answer as the output. \nimage regions containing the entity, rather than general image-level recognition. We employ a language-guided object detection model, i.e., GLIP ( Li et al. ,  2021 ), for language-guided object detec- tion, extracting regions relevant to textual queries by understanding the query context. This targeted approach ensures precise region extraction, enhanc- ing the system’s accuracy and contextual relevance. \nEntity Recognition via Image Retrieval We construct a similarity index using CLIP embed- dings ( Radford et al. ,  2021 ) and Faiss ( Johnson et al. ,  2017 ) for indexing. Our database, built on the WIT dataset ( Srinivasan et al. ,  2021 ), maps CLIP image embeddings to their text descriptions, leveraging Faiss’s robust similarity search capa- bilities. After setting up the indexing database, given an input query image    $I$  , we perform a    $k$  - nearest neighbor retrieval based on cosine simi- larity. The retrieval outcomes are represented as  $\\mathcal{R}(I)=\\{(i_{1},c_{1})\\,,\\cdot\\cdot\\cdot\\,,(i_{k},c_{k})\\}$  , where for each    $j$  within the range of  1  to    $k,\\,i_{j}$   and  $c_{j}$   correspond to the retrieved image and its associated caption, respectively. By comparing    $I$   with similar images from the database, we identify the entity in the image region, which enables precise image-level entity recognition. \nKnowledge Retrieval via Multi-Source Aggrega- tion Facing diverse user queries, we gather extra information to compile resources for accurate re- sponses. Some queries require up-to-date informa- tion, not present in existing databases. We then turn to external sources to collect critical data like “year built,\" “description,\" and more. By using Knowl- edge Graph (KG) and web searches, we access rele- vant knowledge links, enriching our understanding of the specified image region, and improving our ability to comprehend and contextual ize the ex- tracted content. More details of the method can be found in Appendix  D . \n\n4.2 Entity-centric Knowledge-based Answer Generation \nFollowing information collection, we enter the inte- gration phase, blending the input image, question, and retrieved data to generate a knowledgeable response, which is illustrated in Figure  3 . Our method enhances multimodal understanding by pre-training a LLM with image-text paired data. Taking cues from  Moon et al.  ( 2023 ), we employ lightweight adapters for each modality, converting inputs into the text token embedding space of the chosen LLM. \nIn our method, the LLM’s text token embedding space morphs into a unified space, representing both text and image content, with each modality assigned 64 to 256 token embeddings. We freeze the LLM’s parameters during alignment training to quicken convergence and retain the LLM’s rea- soning skills for inference. To ensure feature align- ment, we use an image encoder,    $g(\\cdot)$  , previously synchronized with a text embedding space, like in CLIP ( Radford et al. ,  2021 ;  Schuhmann et al. , 2022 ). For text-image pairs    $(\\mathbf{X}_{t e x t},\\mathbf{X}_{i m a g e})$  , we align them using specific objectives and a projec- tion module, like the Perceiver Resampler ( Alayrac et al. ,  2022 ), applied to the vision encoder as: \n\n$$\np(\\mathbf{X}_{t e x t}|\\mathbf{X}_{i m a g e})=\\prod_{i=1}^{L}p_{\\theta}(\\mathbf{X}_{t e x t}^{[i]}|\\mathbf{Z}_{i m a g e},\\mathbf{Z}_{t e x t}^{[1:i-1]})\n$$\n \n\n$$\n\\mathbf{Z}_{i m a g e}=P r o j_{\\theta}\\big(h_{l a t e n t s},g\\big(\\mathbf{X}_{i m a g e}\\big)\\big)\n$$\n \n5 Experiments and Results \n5.1 Experimental Setup \nEvaluation Metrics (1) In our evaluation pro- cess, the quality of the answers is first assessed using established NLP metrics such as BLEU ( Pa- pineni et al. ,  2002 ), METEOR ( Denkowski and Lavie ,  2014 ), ROUGE ( Lin ,  2004 ), and BLEURT ( Sellam et al. ,  2020 ;  Pu et al. ,  2021 ). (2) Addition- ally, we incorporate accuracy and hallucination rate metrics from ( Sun et al. ,  2023 ). These metrics used GPT4  to automatically measure the proportion of questions for which the model provides correct answers or incorrect/partially incorrect answers, respectively. (3) We conduct human evaluation following  Ye et al.  ( 2023 );  Moon et al.  ( 2023 ). "}
{"page": 6, "image_path": "doc_images/SnapNTell_6.jpg", "ocr_text": "Table 3: Performance comparison of different ap-\nproaches on the SnapNTell dataset.\n\nMethod ROUGE + BLEU? METEOR BLEURT t\n\nInstruct-BLIP (Dai et al., 2023) 10.72 0.95 1.59 0,09\nBLIP? (Li et al., 2023) 15.00 0.52 8.49 0.16\nMini-GPT4 (Zhu et al., 2023) 26.12 5.62 25.55 0.27\nLLaVA (Liu et al., 2023b) 26.86 6.03 26.97 0.31\nOpen-Flamingo (Awadalla et al., 2023) 30.57 6.52 22.53 0.32\n\nCOGVLM (Wang et al., 2023) 30.25 6.67 23.35 0.31\nmPLUG-Owl2 (Ye et al., 2023) 31.39 6.72 24.67 0.33\nLLaVA 1.5 (Liu et al., 2023a) 32.87 6.94 25.23 0.33\n\n_SnapNTell (ours) 582878 29.27 5S\nModel Setting We chose LLaMA2 (70B) (Tou-\nvron et al., 2023) as our LLM. For image encoding,\nthe CLIP image encoder (ViT-B/32) is employed\n(Radford et al., 2021; Schuhmann et al., 2022). Ad-\nditional configurations comprise a batch size of\n2,048, the integration of two resampler layers, and\nthe use of 64 modality tokens.\n\nModel Training We used a cleaned subset of the\nLAION-2B dataset, filtered using the CAT method\n(Radenovic et al., 2023b) and with any detectable\nfaces blurred (Radenovic et al., 2023a). Signifi-\ncant resources are essential to scale pre-training\nto 70 billion parameter models on a substantial\ndataset of over 200 million instances. Often, this\nnecessitates the utilization of an FSDP wrapper, as\noutlined in Dettmers et al. (2023), to distribute the\nmodel across multiple GPUs efficiently. To opti-\nmize our training process, we employ quantization\nstrategies, specifically 4-bit and 8-bit quantization\ntechniques (Dettmers et al., 2023), within our mul-\ntimodal framework. In this approach, we maintain\nthe LLM component of our model in a frozen state,\nallowing only the image modality tokenizers to\nbe trainable. This strategy drastically reduces the\nmemory requirements by an order of magnitude.\nAs a result of these optimizations, we can success-\nfully train a 70 billion parameter model on a single\nGPU with 80GB VRAM, using a batch size of 4.\n\n5.2. Results and Discussion\n\nTable 3 displays the comparative results between\nthe baseline models and our proposed method.\nAnalysis of this table indicates that for every met-\nric assessed, our retrieval-augmented multimodal\nLLM surpasses the performance of all existing\nbaseline models. This strong performance empha-\nsizes the efficiency of retrieval augmentation in\nproducing responses enriched with entity-centric\ninformation, thereby illustrating its substantial im-\npact on the task at hand.\n\nMoreover, to gain deeper insights into which\nevaluation metric more accurately reflects the out-\ncomes, we computed the Kendall correlation coef-\n\nTable 4: Effectiveness of evaluation metrics.\n\nROUGE BLEU\n\nT 0.999 0.799\nP_value 0.014 0.050\n\nMETEOR BELURT\n\n0.600 0.999\n0.142 0.014\n\nficient (Kendall, 1938; Knight, 1966; Kendall et al.,\n1995), comparing the results with those from the\nhuman evaluation in Section 5.4. Kendall’s 7 is a\nmeasure of the correspondence between two rank-\nings. Values close to 1 indicate strong agreement,\nvalues close to -1 indicate strong disagreement. Ta-\nble 4 revealed that both the ROUGE and BLEURT\nscores were more indicative in distinguishing the\ndifferences among various models. This finding\nsuggests that these two metrics are particularly sig-\nnificant in evaluating model performance in a way\nthat aligns closely with human judgment.\n\n5.3 Ablation Study\n\nFor a more in-depth understanding, we conducted\nseveral ablation studies to delve into the finer de-\ntails of our approach.\n\nEffectiveness of Entity Detection To assess the\nimpact of entity detection (ED) in our model, we\nperformed an ablation study. This involved com-\nparing the performance of our approach with and\nwithout the ED component. As indicated in Ta-\nble 5, our approach incorporating entity detection\nmarkedly surpasses the variant lacking this feature.\nThis highlights the significant contribution and ne-\ncessity of the entity detection step in our model’s\noverall effectiveness.\n\nTable 5: Ablation study on the effectiveness of entity\ndetection (ED).\n\nMethod ROUGE BLEUt METEOR*+ BELURT t\n\n26.26 0.45\n29.27 0.55\n\nw/o ED\nw/ ED\n\n28.02 3.73\n35.28 781\n\nHead/Torso/Tail Entities Head knowledge per-\ntains to well-established entities for which there is\na wealth of available training data. Ideally, LLMs\ncould be trained to possess this knowledge, fa-\ncilitating efficient retrieval. On the other hand,\ntorso-to-tail knowledge pertains to less-known or\nobscure entities, often characterized by scarce or\nnon-existent training data. Providing access to such\nknowledge involves effectively determining when\nexternal information is necessary, retrieving the\nrelevant knowledge efficiently, and seamlessly inte-\ngrating it into responses.\n\nTo assess the performance improvement for\nhead/torso/tail entities, we randomly selected 10%\n", "vlm_text": "The table shows a comparison of various methods using four evaluation metrics: ROUGE, BLEU, METEOR, and BLEURT. Each method is associated with a paper:\n\n- **Instruct-BLIP** (Dai et al., 2023)\n- **BLIP2** (Li et al., 2023)\n- **Mini-GPT4** (Zhu et al., 2023)\n- **LLAVA** (Liu et al., 2023b)\n- **Open-Flamingo** (Awadalla et al., 2023)\n- **COGVLM** (Wang et al., 2023)\n- **mPLUG-Owl2** (Ye et al., 2023)\n- **LLAVA 1.5** (Liu et al., 2023a)\n- **SnapNTell (ours)**\n\nThe metrics are given as follows:\n\n- **ROUGE** scores range from 10.72 to 35.28.\n- **BLEU** scores range from 0.52 to 7.81.\n- **METEOR** scores range from 7.59 to 29.27.\n- **BLEURT** scores range from 0.09 to 0.55.\n\nSnapNTell (ours) has the highest scores across all four metrics in this table.\nModel Setting We chose LLaMA2 (70B) ( Tou- vron et al. ,  2023 ) as our LLM. For image encoding, the CLIP image encoder (ViT-B/32) is employed ( Radford et al. ,  2021 ;  Schuhmann et al. ,  2022 ). Ad- ditional configurations comprise a batch size of 2,048, the integration of two resampler layers, and the use of 64 modality tokens. \nModel Training We used a cleaned subset of the LAION-2B dataset, filtered using the CAT method ( Radenovic et al. ,  2023b ) and with any detectable faces blurred ( Radenovic et al. ,  2023a ). Signifi- cant resources are essential to scale pre-training to 70 billion parameter models on a substantial dataset of over 200 million instances. Often, this necessitates the utilization of an FSDP wrapper, as outlined in  Dettmers et al.  ( 2023 ), to distribute the model across multiple GPUs efficiently. To opti- mize our training process, we employ quantization strategies, specifically 4-bit and 8-bit quantization techniques ( Dettmers et al. ,  2023 ), within our mul- timodal framework. In this approach, we maintain the LLM component of our model in a frozen state, allowing only the image modality tokenizers to be trainable. This strategy drastically reduces the memory requirements by an order of magnitude. As a result of these optimization s, we can success- fully train a 70 billion parameter model on a single GPU with 80GB VRAM, using a batch size of 4. \n5.2 Results and Discussion \nTable  3  displays the comparative results between the baseline models and our proposed method. Analysis of this table indicates that for every met- ric assessed, our retrieval-augmented multimodal LLM surpasses the performance of all existing baseline models. This strong performance empha- sizes the efficiency of retrieval augmentation in producing responses enriched with entity-centric information, thereby illustrating its substantial im- pact on the task at hand. \nMoreover, to gain deeper insights into which evaluation metric more accurately reflects the out- comes, we computed the Kendall correlation coef- \nThe table presents two metrics (\\(\\tau\\) and P_value) for four different evaluation measures: ROUGE, BLEU, METEOR, and BELURT.\n\n- **ROUGE**\n  - \\(\\tau\\): 0.999\n  - P_value: 0.014\n\n- **BLEU**\n  - \\(\\tau\\): 0.799\n  - P_value: 0.050\n\n- **METEOR**\n  - \\(\\tau\\): 0.600\n  - P_value: 0.142\n\n- **BELURT**\n  - \\(\\tau\\): 0.999\n  - P_value: 0.014\nficient ( Kendall ,  1938 ;  Knight ,  1966 ;  Kendall et al. , 1995 ), comparing the results with those from the human evaluation in Section  5.4 . Kendall’s  $\\tau$   is a measure of the correspondence between two rank- ings. Values close to 1 indicate strong agreement, values close to -1 indicate strong disagreement. Ta- ble  4  revealed that both the ROUGE and BLEURT scores were more indicative in distinguishing the differences among various models. This finding suggests that these two metrics are particularly sig- nificant in evaluating model performance in a way that aligns closely with human judgment. \n5.3 Ablation Study \nFor a more in-depth understanding, we conducted several ablation studies to delve into the finer de- tails of our approach. \nEffectiveness of Entity Detection To assess the impact of entity detection (ED) in our model, we performed an ablation study. This involved com- paring the performance of our approach with and without the ED component. As indicated in Ta- ble  5 , our approach incorporating entity detection markedly surpasses the variant lacking this feature. This highlights the significant contribution and ne- cessity of the entity detection step in our model’s overall effectiveness. \nThe table compares evaluation metrics for two methods, \"w/o ED\" (without ED) and \"w/ ED\" (with ED), across four metrics:\n\n- **ROUGE**: \n  - w/o ED: 28.02\n  - w/ ED: 35.28 (bold, indicating better performance)\n\n- **BLEU**: \n  - w/o ED: 3.73\n  - w/ ED: 7.81 (bold, indicating better performance)\n\n- **METEOR**: \n  - w/o ED: 26.26\n  - w/ ED: 29.27 (bold, indicating better performance)\n\n- **BELURT**: \n  - w/o ED: 0.45\n  - w/ ED: 0.55 (bold, indicating better performance)\n\nThe method \"w/ ED\" shows improved scores across all metrics. The arrows (↑) signify that higher values are better.\nHead/Torso/Tail Entities Head knowledge per- tains to well-established entities for which there is a wealth of available training data. Ideally, LLMs could be trained to possess this knowledge, fa- cilitating efficient retrieval. On the other hand, torso-to-tail knowledge pertains to less-known or obscure entities, often characterized by scarce or non-existent training data. Providing access to such knowledge involves effectively determining when external information is necessary, retrieving the relevant knowledge efficiently, and seamlessly inte- grating it into responses. \nTo assess the performance improvement for head/torso/tail entities, we randomly selected   $10\\%$  "}
{"page": 7, "image_path": "doc_images/SnapNTell_7.jpg", "ocr_text": "Table 6: Ablation study on head/torso/tail entities,\nwhere RA is short for Retrieval Augmentation and A is\nthe performance difference of with and without RA.\n\nAccuracy ¢ Hallucination |\n\nw/o RA 24.4 75.6\n\nHead w/ RA 27.1 72.9\nA (100%) 11.1% 3.6 % |\n\nw/o RA 19.1 80.9\n\nTorso w/ RA 22.7 77.3\nA (100%) 18.8 % + 44% |\n\nw/o RA 6.8 93.2\n\nTail w/ RA 12.6 87.4\nA (100%) 85.3 % tT 6.2% |\n\nentities for each category, where head/torso/tail\nentities are defined based on pageview statistics\n(popularity) in Section 3.4. The results presented\nin Table 6 clearly demonstrate that retrieval aug-\nmentation can significantly enhance performance\nacross various entity types. Notably, the perfor-\nmance improvement for torso-to-tail entities far\nexceeds that of head entities, effectively address-\ning the challenge of hallucinations in long-tailed\nentities through retrieval augmentation.\n\nPerformance of Different VQA Datasets To\ndemonstrate the uniqueness of our SnapNTell\ndataset compared to existing VQA datasets, we an-\nalyzed the performance of various baseline models\non both traditional VQA datasets and our SnapN-\nTell dataset. According to the findings presented in\nTable 7, the performance disparities among base-\nline models on existing datasets are not particularly\nmarked. In contrast, on the SnapNTell dataset,\nwe observed significantly larger differences and\nnotably lower performance. This indicates that\nour SnapNTell dataset is particularly effective in\nevaluating the capabilities of different models to\nrecognize entities and produce responses centered\naround these entities.\n\nTable 7: Ablation on the accuracy performance of dif-\nferent VQA datasets.\n\nMethod\n\nVQAv2 TextVQA OK-VQA | SnapNTell’)\nInstruct-BLIP (Dai et al., 2023) - 46.6 55.5 | 8.88)\n\nBLIP2 (Li et al., 2023) 52.6 43.1 54.7\nFlamingo (Alayrac et al., 2022) 56.3 37.9 57.8\n\n5.4 Human Evaluation Results\n\nIn alignment with the methodology presented in\nYe et al. (2023); Moon et al. (2023), we involved\na human evaluation process conducted by a panel\nof five human judges (3 male, 2 female). These\njudges were given specific instructions for their as-\nsessment, which encompassed three key aspects:\n\nLose © Tie ™ Win\n100%\n\n15%\n50%\n\n25%\n\n0% — om\n\n4 go UM ~ owl? 5 atell\nSPO FAME COGENT GOW? ya NY? gas\n\nFigure 4: Human evaluation results on pairwise com-\nparisons (% win, tie, lose) with baseline outputs against\nthe manually annotated ground-truth from SnapNTell.\n\n(1) Recognition Accuracy, where they evaluated\nwhether the model correctly identified the entity in\nthe image relevant to the question; (2) Response\nAccuracy, in which they assessed the factual cor-\nrectness of the model’s responses while checking\nfor any signs of hallucination (Rawte et al., 2023);\nand (3) Pairwise Comparison, where judges se-\nlected the response that better addressed the given\nquestion in terms of contextual appropriateness and\naccuracy, categorizing responses as winning, tying,\nor losing.\n\nIn our study, we conducted pairwise compar-\nisons for each baseline model against ground-truth\ndata across 1,000 samples. As depicted in Figure 4,\nour model outperforms the baselines by displaying\na significantly smaller difference when measured\nagainst manually annotated ground-truth samples,\nhighlighting its robustness.\n\n6 Conclusion\n\nIn this work, we tackle the significant challenge\nVLLMs face with long-tail entity queries, which of-\nten lead to inaccurate or hallucinated responses. To\naddress these issues, we introduce an entity-centric\nVQA task named SnapNTell. This task is designed\nto test models on entity recognition and their abil-\nity to provide detailed, entity-specific knowledge\nin their responses. We collected a unique eval-\nuation dataset for this task, which distinguishes\nitself from existing VQA datasets by including a\nwide array of fine-grained categorized entities, sup-\nported by images and explicit entity mentions in\nthe answers. This dataset emphasizes knowledge-\nintensive responses over simple binary answers. In\naddition, we propose a retrieval-augmented multi-\nmodal LLM solution for the SnapNTell task as an\neffective baseline. Our experimental results show\nthat our model outperforms existing approaches,\nproviding more accurate and coherent answers.\n", "vlm_text": "The table shows changes in accuracy and hallucination rates for different categories (Head, Torso, Tail) with and without RA. Here's a breakdown:\n\n- **Head**\n  - Accuracy: \n    - w/o RA: 24.4 \n    - w/ RA: 27.1 \n    - Increase: 11.1%\n  - Hallucination: \n    - w/o RA: 75.6 \n    - w/ RA: 72.9 \n    - Decrease: 3.6%\n\n- **Torso**\n  - Accuracy:\n    - w/o RA: 19.1 \n    - w/ RA: 22.7 \n    - Increase: 18.8%\n  - Hallucination:\n    - w/o RA: 80.9 \n    - w/ RA: 77.3 \n    - Decrease: 4.4%\n\n- **Tail**\n  - Accuracy:\n    - w/o RA: 6.8 \n    - w/ RA: 12.6 \n    - Increase: 85.3%\n  - Hallucination:\n    - w/o RA: 93.2 \n    - w/ RA: 87.4 \n    - Decrease: 6.2%\n\nThe arrows indicate the direction of change, with orange for increases and blue for decreases.\nentities for each category, where head/torso/tail entities are defined based on pageview statistics (popularity) in Section  3.4 . The results presented in Table  6  clearly demonstrate that retrieval aug- mentation can significantly enhance performance across various entity types. Notably, the perfor- mance improvement for torso-to-tail entities far exceeds that of head entities, effectively address- ing the challenge of hallucinations in long-tailed entities through retrieval augmentation. \nPerformance of Different VQA Datasets To demonstrate the uniqueness of our SnapNTell dataset compared to existing VQA datasets, we an- alyzed the performance of various baseline models on both traditional VQA datasets and our SnapN- Tell dataset. According to the findings presented in Table  7 , the performance disparities among base- line models on existing datasets are not particularly marked. In contrast, on the SnapNTell dataset, we observed significantly larger differences and notably lower performance. This indicates that our SnapNTell dataset is particularly effective in evaluating the capabilities of different models to recognize entities and produce responses centered around these entities. \nThe table compares different methods across four datasets or benchmarks: VQAv2, TextVQA, OK-VQA, and SnapNTell. It displays performance metrics for each method. Here's a breakdown:\n\n- **Methods:**\n  - Instruct-BLIP (Dai et al., 2023)\n  - BLIP2 (Li et al., 2023)\n  - Flamingo (Alayrac et al., 2022)\n\n- **Metrics:**\n  - VQAv2\n    - Instruct-BLIP: Not available\n    - BLIP2: 52.6\n    - Flamingo: 56.3\n  - TextVQA\n    - Instruct-BLIP: 46.6\n    - BLIP2: 43.1\n    - Flamingo: 37.9\n  - OK-VQA\n    - Instruct-BLIP: 55.5\n    - BLIP2: 54.7\n    - Flamingo: 57.8\n  - SnapNTell\n    - Instruct-BLIP: 8.88\n    - BLIP2: 16.16\n    - Flamingo: 32.17\n\nThe SnapNTell column is highlighted in orange.\n5.4 Human Evaluation Results \nIn alignment with the methodology presented in Ye et al.  ( 2023 );  Moon et al.  ( 2023 ), we involved a human evaluation process conducted by a panel of five human judges (3 male, 2 female). These judges were given specific instructions for their as- sessment, which encompassed three key aspects: \nThe image is a bar chart comparing human evaluation results of different models in terms of win, tie, and lose percentages against the manually annotated ground truth from SnapNTell. \n\n- Each bar represents a different model: MIni-GPT4, Open-Flamingo, COGVLM, mPLUG-Owl2, LLaVA 1.5, and SnapNTell.\n- The colors indicate different outcomes: blue for lose, yellow for tie, and red for win.\n- SnapNTell has the highest win percentage, while the other models predominantly have a high lose percentage.\n(1) Recognition Accuracy, where they evaluated whether the model correctly identified the entity in the image relevant to the question; (2) Response Accuracy, in which they assessed the factual cor- rectness of the model’s responses while checking for any signs of hallucination ( Rawte et al. ,  2023 ); and (3) Pairwise Comparison, where judges se- lected the response that better addressed the given question in terms of contextual appropriateness and accuracy, categorizing responses as winning, tying, or losing. \nIn our study, we conducted pairwise compar- isons for each baseline model against ground-truth data across 1,000 samples. As depicted in Figure  4 , our model outperforms the baselines by displaying a significantly smaller difference when measured against manually annotated ground-truth samples, highlighting its robustness. \n6 Conclusion \nIn this work, we tackle the significant challenge VLLMs face with long-tail entity queries, which of- ten lead to inaccurate or hallucinated responses. To address these issues, we introduce an entity-centric VQA task named SnapNTell. This task is designed to test models on entity recognition and their abil- ity to provide detailed, entity-specific knowledge in their responses. We collected a unique eval- uation dataset for this task, which distinguishes itself from existing VQA datasets by including a wide array of fine-grained categorized entities, sup- ported by images and explicit entity mentions in the answers. This dataset emphasizes knowledge- intensive responses over simple binary answers. In addition, we propose a retrieval-augmented multi- modal LLM solution for the SnapNTell task as an effective baseline. Our experimental results show that our model outperforms existing approaches, providing more accurate and coherent answers. "}
{"page": 8, "image_path": "doc_images/SnapNTell_8.jpg", "ocr_text": "Limitations\n\nIn this study, we introduce a novel SnapNTell task\nand its accompanying dataset, which features five\nunique types of questions, each paired with metic-\nulously formulated answers. It’s important to rec-\nognize that in cases involving human preferences,\nwhich are subjective by nature, the given answers\nmight not represent the only correct options. Fur-\nthermore, the relevancy of some answers may di-\nminish over time, highlighting the need for peri-\nodic updates to the dataset to ensure its ongoing\nrelevance and accuracy. Our proposed method ex-\nhibited superior performance over existing base-\nlines. However, human evaluation results suggest\nsignificant potential for further improvement. Al-\nthough our approach often neared human-level per-\nformance, it did not consistently outperform human\nannotations, showing opportunities for future ad-\nvancements.\n\nEthics Statement\n\nIn this study, the dataset was sourced from publicly\naccessible databases, and all author details remain\nanonymous. We conscientiously excluded any con-\ntent from our dataset that could be considered ethi-\ncally sensitive or related to personal privacy, such\nas images depicting human faces. To our under-\nstanding, and with careful consideration, we do\nnot anticipate any detrimental applications arising\nfrom the findings or methodologies presented in\nthis research.\n\nBroader Impact\n\nCurrent models have made commendable progress\nin grasping the nuanced semantics and context-\nsensitive aspects of Visual Question Answering\n(VQA). However, their efficacy in factual VQA\ntasks, which require precise and factual answers\nabout tangible entities and events, reveals certain\ndeficiencies. This is especially true for torso-to-tail\nor long-tail entities. Despite their prevalence in\nthe real world, these entities are underrepresented\nin training datasets, leading to a common issue\nwhere models produce plausible yet inaccurate or\ninvented responses, a phenomenon often termed\n“hallucinations\" in the realm of model-generated\ncontent. Tackling and minimizing these hallucina-\ntions is vital for enhancing the trustworthiness and\napplicability of these models in practical scenarios.\n\nThe existing VQA datasets, however, are inade-\nquate for evaluating a model’s ability to recognize\n\nentities, as they do not explicitly highlight these\nentities within the dataset. Our newly introduced\ndataset bridges this gap. It is designed to test mod-\nels’ capabilities not just in identifying entities but\nalso in generating informed and entity-aware re-\nsponses. Furthermore, our proposed dataset might\nserve as resources for either pre-training or fine-\ntuning existing models, to improve their ability in\nrecognizing entity-level real-world objects.\n\nReferences\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, Andy\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Miko-\nlaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a visual language model for few-shot\nlearning. ArXiv, abs/2204.14198.\n\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hes-\nsel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa,\nJenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt.\n2023. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models.\nArXiv, abs/2308.01390.\n\nYingshan Chang, Mridu Baldevraj Narang, Hisami\nSuzuki, Guihong Cao, Jianfeng Gao, and Yonatan\nBisk. 2021. Webqa: Multihop and multimodal qa.\n2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 16474-16483.\n\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit\nChangpinyo, Alan Ritter, and Ming-Wei Chang. 2023.\nCan pre-trained vision and language models answer\nvisual information-seeking questions? In EMNLP.\n\nAakanksha Chowdhery et al. 2022. Palm: Scaling lan-\nguage modeling with pathways. J. Mach. Learn. Res.,\n24:240:1-240:113.\n\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Albert Li, Pascale Fung, and Steven C. H.\nHoi. 2023. Instructblip: Towards general-purpose\nvision-language models with instruction tuning.\nArXiv, abs/2305.06500.\n\nMichael J. Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language specific translation evaluation\nfor any target language. In WMT@ACL.\n\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized Ilms. ArXiv, abs/2305.14314.\n", "vlm_text": "Limitations \nIn this study, we introduce a novel SnapNTell task and its accompanying dataset, which features five unique types of questions, each paired with metic- ulously formulated answers. It’s important to rec- ognize that in cases involving human preferences, which are subjective by nature, the given answers might not represent the only correct options. Fur- thermore, the relevancy of some answers may di- minish over time, highlighting the need for peri- odic updates to the dataset to ensure its ongoing relevance and accuracy. Our proposed method ex- hibited superior performance over existing base- lines. However, human evaluation results suggest significant potential for further improvement. Al- though our approach often neared human-level per- formance, it did not consistently outperform human annotations, showing opportunities for future ad- vancements. \nEthics Statement \nIn this study, the dataset was sourced from publicly accessible databases, and all author details remain anonymous. We conscientiously excluded any con- tent from our dataset that could be considered ethi- cally sensitive or related to personal privacy, such as images depicting human faces. To our under- standing, and with careful consideration, we do not anticipate any detrimental applications arising from the findings or methodologies presented in this research. \nBroader Impact \nCurrent models have made commendable progress in grasping the nuanced semantics and context- sensitive aspects of Visual Question Answering (VQA). However, their efficacy in factual VQA tasks, which require precise and factual answers about tangible entities and events, reveals certain deficiencies. This is especially true for torso-to-tail or long-tail entities. Despite their prevalence in the real world, these entities are underrepresented in training datasets, leading to a common issue where models produce plausible yet inaccurate or invented responses, a phenomenon often termed “hallucinations\" in the realm of model-generated content. Tackling and minimizing these hallucina- tions is vital for enhancing the trustworthiness and applicability of these models in practical scenarios. The existing VQA datasets, however, are inade- \nquate for evaluating a model’s ability to recognize entities, as they do not explicitly highlight these entities within the dataset. Our newly introduced dataset bridges this gap. It is designed to test mod- els’ capabilities not just in identifying entities but also in generating informed and entity-aware re- sponses. Furthermore, our proposed dataset might serve as resources for either pre-training or fine- tuning existing models, to improve their ability in recognizing entity-level real-world objects. \n\nReferences \nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An- toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharif za deh, Miko- laj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a visual language model for few-shot learning .  ArXiv , abs/2204.14198. \nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hes- sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. 2023.  Open flamingo: An open-source framework for training large auto regressive vision-language models . ArXiv , abs/2308.01390. \nYingshan Chang, Mridu Baldevraj Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2021.  Webqa: Multihop and multimodal qa . 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 16474–16483. \nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023. Can pre-trained vision and language models answer visual information-seeking questions? In  EMNLP . \nAakanksha Chowdhery et al. 2022.  Palm: Scaling lan- guage modeling with pathways .  J. Mach. Learn. Res. , 24:240:1–240:113.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. 2023.  Instruct blip: Towards general-purpose vision-language models with instruction tuning . ArXiv , abs/2305.06500. \nMichael J. Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In  WMT@ACL . \nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Z ett le moyer. 2023.  Qlora: Efficient finetuning of quantized llms .  ArXiv , abs/2305.14314. "}
{"page": 9, "image_path": "doc_images/SnapNTell_9.jpg", "ocr_text": "Joseph L. Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological Bulletin,\n76:378-382.\n\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2016. Making the v in vqa\nmatter: Elevating the role of image understanding in\nvisual question answering. International Journal of\nComputer Vision, 127:398 — 414.\n\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alexan-\nder G. Hauptmann, Yonatan Bisk, and Jianfeng Gao.\n2021. Kat: A knowledge augmented transformer for\nvision-and-language. In North American Chapter of\nthe Association for Computational Linguistics.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv,\nabs/2002.08909.\n\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\nManymodalga: Modality disambiguation and qa over\ndiverse inputs. In AAAI Conference on Artificial\nIntelligence.\n\nHexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-\nwal, Mandar Joshi, Kenton Lee, Kristina Toutanova,\nand Ming-Wei Chang. 2023. Open-domain visual\nentity recognition: Towards recognizing millions of\nwikipedia entities. ArXiv, abs/2302.11154.\n\nDrew A. Hudson and Christopher D. Manning. 2019.\nGqa: A new dataset for real-world visual reason-\ning and compositional question answering. 2019\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 6693-6702.\n\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with gpus. [EEE\nTransactions on Big Data, 7:535-547.\n\nM. G. Kendall. 1938. A new measure of rank correla-\ntion. Biometrika, 30:81—93.\n\nM. G. Kendall, Alan L. Stuart, and J. Keith Ord. 1995.\nKendall’s advanced theory of statistics. Journal of\nthe American Statistical Association, 90:398.\n\nWilliam Knight. 1966. A computer method for calcu-\nlating kendall’s tau with ungrouped data. Journal of\nthe American Statistical Association, 61:436-439.\n\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\n2023. Grounding language models to images for\nmultimodal inputs and outputs.\n\nPaul Lerner, Olivier Ferret, Camille Guinaudeau,\nHervé Le Borgne, Romaric Besancon, José G.\nMoreno, and Jestis Lovén-Melgarejo. 2022. Viquae,\na dataset for knowledge-based visual question an-\nswering about named entities. Proceedings of the\n45th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval.\n\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. ArXiv, abs/2301.12597.\n\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang,\nJianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan\nWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-\nWei Chang, and Jianfeng Gao. 2021. Grounded\nlanguage-image pre-training. 2022 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 10955-10965.\n\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In ACL 2004.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved baselines with visual instruc-\ntion tuning. ArXiv, abs/2310.03744.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual instruction tuning. ArXiv,\nabs/2304.08485.\n\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\nquestion answering benchmark requiring external\nknowledge. 20/9 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n3190-3199.\n\nThomas Mensink, Jasper R. R. Uijlings, Lluis Castrején,\nArushi Goel, Felipe Cadar, Howard Zhou, Fei Sha,\nAndre F. de Aratijo, and Vittorio Ferrari. 2023. Ency-\nclopedic vqa: Visual questions about detailed proper-\nties of fine-grained categories. 2023 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV),\npages 3090-3101.\n\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin,\nTushar Nagarajan, Matt Smith, Shashank Jain, Chun-\nFu Yeh, Prakash Murugesan, Peyman Heidari, Yue\nLiu, Kavya Srinet, Babak Damavandi, and Anuj\nKumar. 2023. Anymal: An efficient and scalable\nany-modality augmented language model. ArXiv,\nabs/2309.16058.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL.\n\nAmy Pu, Hyung Won Chung, Ankur P. Parikh, Sebastian\nGehrmann, and Thibault Sellam. 2021. Learning\ncompact metrics for mt. In Conference on Empirical\nMethods in Natural Language Processing.\n\nFilip Radenovic, Abhimanyu Dubey, Abhishek Kadian,\nTodor Mihaylov, Simon Vandenhende, Yash Patel,\nYi Wen, Vignesh Ramanathan, and Dhruv Mahajan.\n2023a. Filtering, distillation, and hard negatives for\nvision-language pre-training. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6967-6977.\n", "vlm_text": "Joseph L. Fleiss. 1971.  Measuring nominal scale agree- ment among many raters.  Psychological Bulletin , 76:378–382. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016.  Making the v in vqa matter: Elevating the role of image understanding in visual question answering .  International Journal of Computer Vision , 127:398 – 414. Liangke Gui, Borui Wang, Qiuyuan Huang, Alexan- der G. Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021.  Kat: A knowledge augmented transformer for vision-and-language . In  North American Chapter of the Association for Computational Linguistics . Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020.  Realm: Retrieval- augmented language model pre-training . ArXiv , abs/2002.08909. Darryl Hannan, Akshay Jain, and Mohit Bansal. 2020. Many modal qa: Modality disambiguation and qa over diverse inputs . In  AAAI Conference on Artificial Intelligence . Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel- wal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. 2023.  Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities .  ArXiv , abs/2302.11154. Drew A. Hudson and Christopher D. Manning. 2019. Gqa: A new dataset for real-world visual reason- ing and compositional question answering .  2019 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR) , pages 6693–6702. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus . IEEE Transactions on Big Data , 7:535–547. M. G. Kendall. 1938.  A new measure of rank correla- tion .  Biometrika , 30:81–93. M. G. Kendall, Alan L. Stuart, and J. Keith Ord. 1995. Kendall’s advanced theory of statistics .  Journal of the American Statistical Association , 90:398. William Knight. 1966.  A computer method for calcu- lating kendall’s tau with ungrouped data .  Journal of the American Statistical Association , 61:436–439. Jing Yu Koh, Ruslan Salak hut dino v, and Daniel Fried. 2023.  Grounding language models to images for multimodal inputs and outputs . Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G. Moreno, and Jesús Lovón-Melgarejo. 2022.  Viquae, a dataset for knowledge-based visual question an- swering about named entities .  Proceedings of the 45th International ACM SIGIR Conference on Re- search and Development in Information Retrieval . \nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023.  Blip-2: Boots trapping language-image pre-training with frozen image encoders and large language models .  ArXiv , abs/2301.12597. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai- Wei Chang, and Jianfeng Gao. 2021. Grounded language-image pre-training .  2022 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR) , pages 10955–10965. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In  ACL 2004 . Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.  Improved baselines with visual instruc- tion tuning .  ArXiv , abs/2310.03744. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning . ArXiv , abs/2304.08485. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.  Ok-vqa: A visual question answering benchmark requiring external knowledge .  2019 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 3190–3199. Thomas Mensink, Jasper R. R. Uijlings, Lluís Castrejón, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, Andre F. de Araújo, and Vittorio Ferrari. 2023.  Ency- clopedic vqa: Visual questions about detailed proper- ties of fine-grained categories .  2023 IEEE/CVF In- ter national Conference on Computer Vision (ICCV) , pages 3090–3101. Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun- Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, and Anuj Kumar. 2023.  Anymal: An efficient and scalable any-modality augmented language model .  ArXiv , abs/2309.16058. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In  ACL . Amy Pu, Hyung Won Chung, Ankur P. Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021.  Learning compact metrics for mt . In  Conference on Empirical Methods in Natural Language Processing . Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vanden he nde, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. 2023a. Filtering, distillation, and hard negatives for vision-language pre-training. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition , pages 6967–6977. "}
{"page": 10, "image_path": "doc_images/SnapNTell_10.jpg", "ocr_text": "Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian,\nTodor Mihaylov, Simon Vandenhende, Yash J. Patel,\nYi Wen, Vignesh Ramanathan, and Dhruv Kumar\nMahajan. 2023b. Filtering, distillation, and hard\nnegatives for vision-language pre-training. 2023\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 6967-6977.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Jnternational Conference on Machine\nLearning.\n\nVipula Rawte, A. Sheth, and Amitava Das. 2023. A\nsurvey of hallucination in large foundation models.\nArXiv, abs/2309.05922.\n\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, Patrick Schramowski, Srivatsa\nKundurthy, Katherine Crowson, Ludwig Schmidt,\nRobert Kaczmarczyk, and Jenia Jitsev. 2022. Laion-\n5b: An open large-scale dataset for training next gen-\neration image-text models. ArXiv, abs/2210.08402.\n\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answer-\ning using world knowledge. In European Conference\non Computer Vision.\n\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. Bleurt: Learning robust metrics for text gen-\neration. In Annual Meeting of the Association for\nComputational Linguistics.\n\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya\nAgarwal, Jatin Lamba, and Balaji Vasan Srinivasan.\n2021. Mimoga: Multimodal input multimodal output\nquestion answering. In North American Chapter of\nthe Association for Computational Linguistics.\n\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. 2021. Wit:\nWikipedia-based image text dataset for multimodal\nmultilingual machine learning. Proceedings of the\n44th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval.\n\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\nLing- Yen Liao, Luca Bertelli, and Michael Bender-\nsky. 2022. Quill: Query intent with large language\nmodels using retrieval augmentation and multi-stage\ndistillation. In Conference on Empirical Methods in\nNatural Language Processing.\n\nKai Sun, Y. Xu, Hanwen Zha, Yue Liu, and Xinhsuai\nDong. 2023. Head-to-tail: How knowledgeable are\nlarge language models (llm)? a.k.a. will llms replace\nknowledge graphs? ArXiv, abs/2308.10168.\n\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2021. Mul-\ntimodalqa: Complex question answering over text,\ntables and images. ArXiv, abs/2104.06039.\n\nHugo Touvron et al. 2023. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\n\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nS.M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. In Neural Information Processing Systems.\n\nPeng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick,\nand Anton van den Hengel. 2016. Fvqa: Fact-based\nvisual question answering. [EEE Transactions on\nPattern Analysis and Machine Intelligence, 40:2413-\n2427.\n\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi\nHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,\nLei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi\nLi, Yuxiao Dong, Ming Ding, and Jie Tang. 2023.\nCogvim: Visual expert for pretrained language mod-\nels. ArXiv, abs/2311.03079.\n\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-\nSeng Chua. 2023. Next-gpt: Any-to-any multimodal\nIlm. ArXiv, abs/2309.05519.\n\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin\nJiang, Linjun Yang, Rangan Majumder, and Furu\nWei. 2023a. Inference with reference: Lossless\nacceleration of large language models. ArXiv,\nabs/2304.04487.\n\nZhuolin Yang, Wei Ping, Zihan Liu, Vijay Anand\nKorthikanti, Weili Nie, De-An Huang, Linxi (Jim)\nFan, Zhiding Yu, Shiyi Lan, Bo Li, Mingyan Liu,\nYuke Zhu, Mohammad Shoeybi, Bryan Catanzaro,\nChaowei Xiao, and Anima Anandkumar. 2023b. Re-\nvilm: Retrieval-augmented visual language model\nfor zero and few-shot image captioning. ArXiv,\nabs/2302.04858.\n\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen tau Yih. 2023. Retrieval-\naugmented multimodal language modeling. ArXiv,\nabs/2211.12561.\n\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen\nHu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and\nJingren Zhou. 2023. mplug-owl2: Revolutionizing\nmulti-modal large language model with modality col-\nlaboration.\n\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing\nSun, Tong Xu, and Enhong Chen. 2023. A sur-\nvey on multimodal large language models. ArXiv,\nabs/2306.13549.\n\nDeyao Zhu, Jun Chen, Xiaogian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. ArXiv, abs/2304.10592.\n", "vlm_text": "Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vanden he nde, Yash J. Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Kumar Mahajan. 2023b. Filtering, distillation, and hard negatives for vision-language pre-training . 2023 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR) , pages 6967–6977. \nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.  Learn- ing transferable visual models from natural language supervision . In  International Conference on Machine Learning . \nVipula Rawte, A. Sheth, and Amitava Das. 2023.  A survey of hallucination in large foundation models . ArXiv , abs/2309.05922. \nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schr a mow ski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kac z marc zyk, and Jenia Jitsev. 2022.  Laion- 5b: An open large-scale dataset for training next gen- eration image-text models .  ArXiv , abs/2210.08402. \nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A benchmark for visual question answer- ing using world knowledge . In  European Conference on Computer Vision . \nThibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020.  Bleurt: Learning robust metrics for text gen- eration . In  Annual Meeting of the Association for Computational Linguistics . \nHrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.  Mimoqa: Multimodal input multimodal output question answering . In  North American Chapter of the Association for Computational Linguistics . \nKrishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. 2021.  Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning .  Proceedings of the 44th International ACM SIGIR Conference on Re- search and Development in Information Retrieval . \nKrishna Srinivasan, Karthik Raman, Anupam Samanta, Ling-Yen Liao, Luca Bertelli, and Michael Bender- sky. 2022.  Quill: Query intent with large language models using retrieval augmentation and multi-stage distillation . In  Conference on Empirical Methods in Natural Language Processing . \nKai Sun, Y. Xu, Hanwen Zha, Yue Liu, and Xinhsuai Dong. 2023.  Head-to-tail: How knowledgeable are large language models (llm)? a.k.a. will llms replace knowledge graphs?  ArXiv , abs/2308.10168. \nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Han- naneh Hajishirzi, and Jonathan Berant. 2021.  Mul- timodalqa: Complex question answering over text, tables and images .  ArXiv , abs/2104.06039. Hugo Touvron et al. 2023.  Llama 2: Open foundation and fine-tuned chat models .  ArXiv , abs/2307.09288. Maria Tsim po uk ell i, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models . In  Neural Information Processing Systems . Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel. 2016.  Fvqa: Fact-based visual question answering .  IEEE Transactions on Pattern Analysis and Machine Intelligence , 40:2413– 2427. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. Cogvlm: Visual expert for pretrained language mod- els .  ArXiv , abs/2311.03079. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat- Seng Chua. 2023.  Next-gpt: Any-to-any multimodal llm .  ArXiv , abs/2309.05519. Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023a. Inference with reference: Lossless acceleration of large language models . ArXiv , abs/2304.04487. Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Anand Kort hik anti, Weili Nie, De-An Huang, Linxi (Jim) Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mingyan Liu, Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023b.  Re- vilm: Retrieval-augmented visual language model for zero and few-shot image captioning . ArXiv , abs/2302.04858. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Z ett le moyer, and Wen tau Yih. 2023.  Retrieval- augmented multimodal language modeling .  ArXiv , abs/2211.12561. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023.  mplug-owl2: Revolutionizing multi-modal large language model with modality col- laboration . Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A sur- vey on multimodal large language models .  ArXiv , abs/2306.13549. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.  Minigpt-4: Enhancing vision-language understanding with advanced large language models .  ArXiv , abs/2304.10592. "}
{"page": 11, "image_path": "doc_images/SnapNTell_11.jpg", "ocr_text": "A More Details about the Dataset Building\n\nMore details about the dataset building process are shown in Figure 5.\n\nSource: Wikipedia\n\nQ Search Wikipedia Search\nEntity: Yosemite National Park\nYosemite National Park BA 73 languages v\nContents hide Asticle Talk Read Edit Viewhistory Tools v\n(Top) From Wikipedia, the free encyclopedia Coordinates: @ 37°4433\"N 1193215; @)\n*\nfoponym “Yosemite” redirects here. For other uses, see Yosemite (disambiguation)\n> Hi\nistory ‘Yosemite National Park (/jou’ seat yoh-SEM+ih-tee!) is a national park in California.\" tis Yosemite National Park\n> Geography bordered on the southeast by Sierra National Forest and on the northwest by Stanislaus National coon! conee ert 7\n> Geology Forest. The park is managed by the National Park Service and covers 769,620 acres (1,187 sq mi HGH calepory I atu pert\n> Ecology Knowledge: | 3.074 km2)!1in tour counties - centered in Tuolumne and Mariposa, extending north and east to Mono\nGeneral intro | and south to Madera. Designated a World Heritage Site in 1984, Yosemite is intemationally recognized\n> Activities\n\nfor its granite cliffs, waterfalls, clear streams, giant sequoia groves, lakes, mountains, meadows,\nIn popular culture Glaciers, and biological diversity\"! Almost 95 percent of the park is designated wilderness. Yosemite\nis one of the largest and least fragmented habitat blocks in the Sierra Nevada,\n\nSee also\n\nCitations Its geology Is characterized by granite and remnants of older rock. About 10 milion years ago, the\n\nGeneral references Sierra Nevada was uplifted and tilted to form its unique slopes, which increased the steepness of\nstream and river beds, forming deep, narrow canyons. About one milion years ago glaciers formed at\n\nExternal links\n\nhigher elevations. They moved downslope, cutting and sculpting the U-shaped Yosemite Valley.!®!\n\nEuropean American settlers first entered the valley in 1851. Other travelers entered eartier, but James\nD. Savage is credited with discovering the area that became Yosemite National Park.\" Native\n‘Americans had inhabited the region for nearly 4,000 years, although humans may have first visited as\nlong as 8,000 to 10,000 years ago.!\"tl\"2)\n\nYosemite was critical tothe development of the concept of national parks. Galen Clark and others\nlobbied to protect Yosemite Valley from development, ultimately leading to President Abraham Lincoln's ‘\nsigning of the Yosemite Grant of 1864 that deciared Yosemite as federally preserved land.!\"*! In 1890, ?\nJohn Muir led a successful movement to motivate Congress to establish Yosemite Valley and its\n\nsurrounding areas as a National Park. This helped pave the way for the National Park System.\"21\n\nYosemite draws about four million visitors annually.(“4! Most visitors spend the majority of their time in\n\nthe valley's seven square miles (18 km?).!®] The park set a visitation record in 2016, surpassing five oe g\nmillion visitors for the first time.!\"®) ote {\nwae\nToponym [eit Location in California\n© show map of California\n\nKnowledge: The word Yosemite (derived from yohhe'meti, “they are killers” in Miwok) historically referred to the O Show map of fhe United States\nToponym name that the Miwok gave to the Ahwahneechee People, the resident indigenous tribe !\"6°71721 Show a\n\nPreviously, the region had been called \"Ahwahnee' (\"big mouth\") by its only indigenous inhabitants, the | [Location Tolumne, Mariposa, Mono and\n\nfera Counties, California,\nnited States\n\n‘Ahwahneechee.|\"®! The term Yosemite in Miwok is easily confused with a similar term for \"grizzly\nbear’, ands stil a common misconception.\"\n\nNearest city lanposa calloma\n\nKnowledge:\nLocation\n\nFigure 5: The pertinent information collected during dataset building, i.e., from Wikipedia for each entity, which\nincludes the summary of the general introduction, toponym, lococation information, and so on.\n\nB_ More Details about the Filtering Process\n\nMore details about the filtering process are shown in Table 8.\n\nC_ Types of Questions\n\nMore introduction of different types of question in the SnapNTell dataset are shown Table 9.\nD Method\n\nIn this section, we will introduce the details of our proposed retrieval-augmented multimodal LLM model.\nThe architecture of our model is shown in Figure 7. Our model can be considered twofold: (1) Retrieval\naugmentation. Given the input image-question pair, we retrieve useful entity-centric information within\nknowledge sources. (2) Entity-centric knowledge-based answer generation. The retrieved information\nwill be combined with the image and question together to generate the answer. More details are introduced\nin the following sections.\n\nD.1_ Retrieval Augmentation\n\nThe retrieval augmentation process can be subdivided into three distinct steps: (i) Semantic region\nextraction via language-guided object detection, (ii) Entity recognition via image retrieval, and (iii)\n", "vlm_text": "A More Details about the Dataset Building \nMore details about the dataset building process are shown in Figure  5 . \nThe image is a screenshot of the Wikipedia page for Yosemite National Park. It includes various pieces of information:\n\n- **Entity**: The title \"Yosemite National Park\" is highlighted.\n- **Knowledge: General intro**: The introductory paragraph provides a general overview of Yosemite National Park, mentioning its location in California, management by the National Park Service, and notable geographic and geological features.\n- **Knowledge: Toponym**: A section on the origin of the park's name, noting its derivation from the Miwok language and its historical context.\n- **Knowledge: Location**: A map showing the location of Yosemite within California, and specific counties mentioned (Tuolumne, Mariposa, Mono, and Madera Counties).\n- **Source**: The source is identified as Wikipedia through its logo and formatting style.\n\nFigure 5: The pertinent information collected during dataset building, i.e., from Wikipedia for each entity, which includes the summary of the general introduction, toponym, lococation information, and so on. \nB More Details about the Filtering Process \nMore details about the filtering process are shown in Table  8 . \nC Types of Questions \nMore introduction of different types of question in the SnapNTell dataset are shown Table  9 . \nD Method \nIn this section, we will introduce the details of our proposed retrieval-augmented multimodal LLM model. The architecture of our model is shown in Figure  7 . Our model can be considered twofold: (1)  Retrieval augmentation . Given the input image-question pair, we retrieve useful entity-centric information within knowledge sources. (2)  Entity-centric knowledge-based answer generation . The retrieved information will be combined with the image and question together to generate the answer. More details are introduced in the following sections. \nD.1 Retrieval Augmentation \nThe retrieval augmentation process can be subdivided into three distinct steps: (i) Semantic region extraction via language-guided object detection, (ii) Entity recognition via image retrieval, and (iii) "}
{"page": 12, "image_path": "doc_images/SnapNTell_12.jpg", "ocr_text": "Table 8: Filtering statistics of the entity dataset. [1st Wiki filtering]: removing ones without wiki page. [2nd Google\nfiltering]: removing ones without enough images via google search API. [3rd Wiki filtering]: removing entity name\nwith ambiguous wiki pages.\n\nMain category _—_ Original Entity Ist Wiki filtering | 2nd Google filtering 3rd Wiki filtering\n\nlandmark 1595 1000 899 753\npainting 1057 367 358 288\nsculpture 300 164 164 134\nfood 883 338 337 271\nfruit 361 236 233 180\nvegetable 389 290 286 214\nmammal 778 633 619 434\nhibian 211 148 139 124\ninsect 366 179 176 145\nfish 1089 1054 987 722\nCategory bird 739 546 545 480\nreptile 279 232 231 210\ncelebrity 1514 1484 1466 732\ninstrument 477 375 368 277\nplant 606 601 593 489\nelectronics 432 354 342 269\ntool 801 213 209 150\ntransportation 334 296 290 227\nsport 694 478 464 395\nbook 1030 826 7171 645\nhousehold 475 319 299 221\ncar 500 320 320 208\nSummary 22 14910 10453 10102 7568\nGoogle __ tel tower\n< all images News Maps Videos More\n\nEnvy mame a a 7 ‘a d ad\nright A drawing france walipeper poris .) sketch\nquery\n\nEiffel Tower} >| Google image\nsearch API ~aet\n“\nae\n\n_,| Human\nFiltering\n\nFind the ones [=] i an\nwith CC license B Pixabey 8 Wiki © Wikimedia Bree sve\n\nEiffel Tower Pars France - Free photo FilecTour Eifel Wiki. Fileiffeltower pari... Detailed Eifel Tower Trace.\n\nPri\nthe eiffel tower, paris, france, eife\n\nFigure 6: Collecting images for building the evaluation dataset. Licenses: CC Publicdomain, CC Attribute, AA\nSharealike, CC Noncommercial, or CC Nonderived licenses. Metadata: image URLs, source page URLs, renamed\nimage names, and the corresponding Wikipedia page URL.\n\nKnowledge retrieval via multi-source aggregation.\n\nSemantic Region Extraction via Language-Guided Object Detection Due to the presence of entities\nwithin the image that occupy only a portion of the available space, employing a comprehensive image-level\nentity recognition approach may lead to a decrease in recognition performance. Instead, we opt to initially\nextract the image region containing the entity and utilize this specific region in subsequent recognition\nprocesses to enhance accuracy. During this phase, we leverage a language-guided object detection model,\nie., GLIP (Li et al., 2021), to extract meaningful regions from complex images. This approach helps\nprecisely identify and extract image regions directly relevant to specific textual queries. It accomplishes\nthis by understanding the context of the query and adjusting its object detection method to find the most\n", "vlm_text": "Table 8: Filtering statistics of the entity dataset. [1st Wiki filtering]: removing ones without wiki page. [2nd Google filtering]: removing ones without enough images via google search API. [3rd Wiki filtering]: removing entity name with ambiguous wiki pages. \nThe image consists of two main parts:\n\n1. **Table of Categories and Filtering:**\n   - It shows different main categories like landmarks, paintings, food, etc.\n   - Columns display the number of entities at different filtering stages: Original Entity, 1st Wiki filtering, 2nd Google filtering, and 3rd Wiki filtering.\n   - The table summarizes entities and their reductions through filtering processes, totaling 14,910 originally down to 7,568 after the third filtering.\n\n2. **Google Image Search Example:**\n   - Illustrates searching for \"Eiffel Tower\" using Google Image Search API.\n   - Emphasizes using the \"Creative Commons licenses\" filter to find images.\n   - Indicates a process involving entity queries, Creative Commons license filtering, and human filtering to select images.\n\nThis image appears to represent a workflow or study involving categorization, data filtering, and image selection procedures.\nKnowledge retrieval via multi-source aggregation. \nSemantic Region Extraction via Language-Guided Object Detection Due to the presence of entities within the image that occupy only a portion of the available space, employing a comprehensive image-level entity recognition approach may lead to a decrease in recognition performance. Instead, we opt to initially extract the image region containing the entity and utilize this specific region in subsequent recognition processes to enhance accuracy. During this phase, we leverage a language-guided object detection model, i.e., GLIP ( Li et al. ,  2021 ), to extract meaningful regions from complex images. This approach helps precisely identify and extract image regions directly relevant to specific textual queries. It accomplishes this by understanding the context of the query and adjusting its object detection method to find the most "}
{"page": 13, "image_path": "doc_images/SnapNTell_13.jpg", "ocr_text": "Table 9: Types of questions.\n\nTypes of questions Definition\n\nStatic facts (absolute | These are objective facts that are concrete and are not contingent on other conditions.\nfacts, discrete facts) They can usually be answered with a short, unique answer. For example: When was\nBarack Obama born?\n\nNarrative facts These facts encompass comprehension of larger contexts (e.g., song lyrics, movie plot,\nhistorical events). They are factual in the sense that the content of the narrative should\naccurately reflect the source material or events, but a correct answer is usually not unique,\nas they can vary in their level of detail and focus. For example: What is the plot of “The\nGodfather”?\n\nDynamic facts These are facts that are subject to change over time. For example: What is the Yelp\ncustomer rating of the Eleven Madison Park restaurant in NYC?\n\nProcedural facts These are usually answers to “how” questions, outlining a sequence of steps to accom-\nplish a task. While the steps may not be unique and could be subjective, in many cases,\nan answer can still be classified as logical (factual) or nonsensical (a hallucination). Note\nthat these facts can overlap with dynamic facts or narrative facts. For example, How do\nyou check the battery level of my Ray-Ban Stories Glasses?\n\nSubjective facts | These “facts” are not objective, indisputable facts, but are based on individual perspec-\n(opinion-based facts) tives or experiences. Recommendations fall in this category. While there’s generally no\nsingle correct answer to questions seeking subjective facts, it still requires the system\nto understand the topic and provide reasonable answers grounded by world facts. For\nexample: Where should I visit Tokyo next month?\n\nimportant image areas. This step enables the system to better understand the query’s context, resulting in\nmore accurate and contextually meaningful region extraction.\n\nEntity Recognition via Image Retrieval To accomplish this goal, we begin by constructing a similarity\nindex using CLIP embeddings, specifically employing Faiss (Johnson et al., 2017) as our indexing tool.\nOur indexing database is established based on the WIT dataset (Srinivasan et al., 2021). This database\nfollows a key-value mapping structure, where the keys represent CLIP ViT-B/32 image embeddings, and\nthe corresponding text descriptions serve as the values. Faiss, known for its efficiency in similarity search,\nis utilized for indexing (Johnson et al., 2017).\n\nOnce the indexing database is set up, we are ready to proceed with the query process. Given an input\nquery image, denoted as J (which is the entity image region extracted in the preceding step), we perform\na k-nearest neighbor retrieval based on cosine similarity between the embeddings of the query image and\nthose of the database images. The retrieval outcomes are represented as R(I) = {(i1,c1) +++ , (ix, cx) },\nwhere for each j within the range of 1 to k, 2; and c; correspond to the retrieved image and its associated\ncaption, respectively. Subsequently, by using the extracted image region as input for a search in the\nindexing database, we identify the entity within the extracted image region. This identification is achieved\nby comparing it with the most similar images retrieved from the indexing database, ultimately resulting in\nimage-level entity recognition.\n\nKnowledge Retrieval via Multi-Source Aggregation Given the wide array of questions users may pose,\nwe need to obtain additional information to compile the necessary resources for crafting accurate responses.\nFurthermore, certain queries may demand the latest information, which is not readily available within\npre-existing databases or knowledge graphs. In such cases, we rely on external sources of knowledge, such\nas online references, to gather essential data, encompassing elements like “year built,\" “description,” and\nother pertinent details. To accomplish this, we leverage Knowledge Graph (KG) and conduct web searches\nto access relevant knowledge connections. This approach enables us to acquire a wealth of information\nconcerning the specified image region, thereby bolstering our capacity to grasp and contextualize the\nextracted content effectively.\n", "vlm_text": "The table categorizes types of questions into five groups and provides definitions and examples for each:\n\n1. **Static facts (absolute facts, discrete facts):**\n   - **Definition:** Objective and concrete, with unique answers not contingent on other conditions.\n   - **Example:** \"When was Barack Obama born?\"\n\n2. **Narrative facts:**\n   - **Definition:** Comprehension of larger contexts like song lyrics, movie plots, or historical events. Answers may vary in detail and focus.\n   - **Example:** \"What is the plot of 'The Godfather'?\"\n\n3. **Dynamic facts:**\n   - **Definition:** Subject to change over time.\n   - **Example:** \"What is the Yelp customer rating of the Eleven Madison Park restaurant in NYC?\"\n\n4. **Procedural facts:**\n   - **Definition:** Answers to \"how\" questions, outlining steps to accomplish tasks. May overlap with other fact types.\n   - **Example:** \"How do you check the battery level of my Ray-Ban Stories Glasses?\"\n\n5. **Subjective facts (opinion-based facts):**\n   - **Definition:** Based on individual perspectives or experiences, not objective or indisputable.\n   - **Example:** \"Where should I visit in Tokyo next month?\"\nimportant image areas. This step enables the system to better understand the query’s context, resulting in more accurate and con textually meaningful region extraction. \nEntity Recognition via Image Retrieval To accomplish this goal, we begin by constructing a similarity index using CLIP embeddings, specifically employing Faiss ( Johnson et al. ,  2017 ) as our indexing tool. Our indexing database is established based on the WIT dataset ( Srinivasan et al. ,  2021 ). This database follows a key-value mapping structure, where the keys represent CLIP ViT-B/32 image embeddings, and the corresponding text descriptions serve as the values. Faiss, known for its efficiency in similarity search, is utilized for indexing ( Johnson et al. ,  2017 ). \nOnce the indexing database is set up, we are ready to proceed with the query process. Given an input query image, denoted as    $I$   (which is the entity image region extracted in the preceding step), we perform a    $k$  -nearest neighbor retrieval based on cosine similarity between the embeddings of the query image and those of the database images. The retriev  outcomes are represented as    $\\mathcal{R}(I)=\\{(i_{1},c_{1})\\,,\\cdot\\cdot\\cdot\\,,(i_{k},c_{k})\\}$  , where for each    $j$   within the range of  1  to  k ,  $i_{j}$   and  $c_{j}$   correspond to the retrieved image and its associated caption, respectively. Subsequently, by using the extracted image region as input for a search in the indexing database, we identify the entity within the extracted image region. This identification is achieved by comparing it with the most similar images retrieved from the indexing database, ultimately resulting in image-level entity recognition. \nKnowledge Retrieval via Multi-Source Aggregation Given the wide array of questions users may pose, we need to obtain additional information to compile the necessary resources for crafting accurate responses. Furthermore, certain queries may demand the latest information, which is not readily available within pre-existing databases or knowledge graphs. In such cases, we rely on external sources of knowledge, such as online references, to gather essential data, encompassing elements like “year built,\" “description,\" and other pertinent details. To accomplish this, we leverage Knowledge Graph (KG) and conduct web searches to access relevant knowledge connections. This approach enables us to acquire a wealth of information concerning the specified image region, thereby bolstering our capacity to grasp and contextual ize the extracted content effectively. "}
{"page": 14, "image_path": "doc_images/SnapNTell_14.jpg", "ocr_text": "tl tl tl tl tl tl tl\n\nbo ododaaa\n\nModel\n\nEntity Entity “Eiffel Tower” <>\nDetection |—> —— >} Recognition |__, Sus\n\nModel =\n\n» .\n| Projection ; Word Embedding Layer\nj Layers a, s\neee eee Pi t\n' Retrieval Retrieved '\nAugmentation Information | | as:\nt ryt NoGrad |\nImage t 1 ' '\nEncoder 1 ' ' t Forward Pass |\n\nInput Image Input Question\n\nthe image?\nFigure 7: The architecture of our SnapNTell model. The input to the model is an image-question pair, and our model\nfirst uses retrieval augmentation to retrieve useful information regarding the entity in the image. Then, the retrieved\ninformation is combined with the question as input to the word embedding layer, where the text embeddings will be\ncombined with image-projected embeddings as the input to LLM, which finally generates a knowledgeable answer\nas the output.\n\nD.2_ Entity-centric Knowledge-based Answer Generation\n\nFollowing the preceding step, where we’ve gathered insightful information from diverse sources, we now\nproceed to the second phase: determining how to integrate the input image, the question, and the retrieved\ninformation in order to produce a knowledge-driven response.\n\nOur approach is illustrated in Figure 7. Our strategy for improving the model’s multimodal compre-\nhension entails pre-training a LLM using paired multimodal data, which comprises images alongside\ncorresponding textual descriptions. To achieve this, we draw inspiration from Moon et al. (2023) and\ncreate lightweight adapters for each modality. These adapters facilitate the transformation of inputs into\nthe text token embedding space of a designated LLM.\n\nOur approach transforms the text token embedding space of the LLM into a unified token embedding\nspace, where tokens can represent either textual or image content. The number of token embeddings\nallocated to each input modality is predetermined for each adapter, ranging from 64 to 256. Throughout\nthe alignment training process, we keep the model parameters of the underlying LLM frozen. This\napproach not only accelerates convergence compared to training the model from scratch but also allows\nthe model to inherit the reasoning capabilities of the LLM during inference. Additionally, to maximize\nfeature compatibility, we employ an encoder denoted as g(-) for the image modality. This encoder has\npreviously been aligned with a text embedding space, for instance, in the case of CLIP (Radford et al.,\n2021; Schuhmann et al., 2022). For each pair of text and image, represented as (Ktext, Ximage)» We\nalign them using specific objectives along with a projection module, such as the Perceiver Resampler\n(Alayrac et al., 2022) for the vision encoder.\n\nL\np(Xeext|Ximase) = [ [ p(X! hve |Zinage Zh\") (3)\ni=1\n\nZimage = Projg(hiatents, 9(Ximage)) (4)\n", "vlm_text": "The image is a diagram of the SnapNTell model architecture. \n\n1. **Input**: It begins with an image-question pair.\n2. **Image Processing**: \n   - **Image Encoder**: Processes the input image.\n   - **Entity Detection Model**: Detects entities in the image.\n   - **Entity Recognition Model**: Identifies recognized entities (e.g., \"Eiffel Tower\").\n   - **Retrieval Augmentation**: Uses identified entities to retrieve additional information from a database.\n   \n3. **Text Processing**:\n   - The retrieved information and the question are combined.\n   - This combined input is processed by the **Word Embedding Layer**.\n\n4. **Image and Text Fusion**:\n   - **Projection Layers**: Combine image features as embeddings.\n   - These embeddings are integrated with the text embeddings.\n\n5. **Language Model Processing**:\n   - A **Large Language Model (LLM)** takes the embeddings to generate an answer to the input question.\n\nThe diagram includes pathways for forward and backward passes indicating model training and inference flow.\nD.2 Entity-centric Knowledge-based Answer Generation \nFollowing the preceding step, where we’ve gathered insightful information from diverse sources, we now proceed to the second phase: determining how to integrate the input image, the question, and the retrieved information in order to produce a knowledge-driven response. \nOur approach is illustrated in Figure  7 . Our strategy for improving the model’s multimodal compre- hension entails pre-training a LLM using paired multimodal data, which comprises images alongside corresponding textual descriptions. To achieve this, we draw inspiration from  Moon et al.  ( 2023 ) and create lightweight adapters for each modality. These adapters facilitate the transformation of inputs into the text token embedding space of a designated LLM. \nOur approach transforms the text token embedding space of the LLM into a unified token embedding space, where tokens can represent either textual or image content. The number of token embeddings allocated to each input modality is predetermined for each adapter, ranging from 64 to 256. Throughout the alignment training process, we keep the model parameters of the underlying LLM frozen. This approach not only accelerates convergence compared to training the model from scratch but also allows the model to inherit the reasoning capabilities of the LLM during inference. Additionally, to maximize feature compatibility, we employ an encoder denoted as    $g(\\cdot)$   for the image modality. This encoder has previously been aligned with a text embedding space, for instance, in the case of CLIP ( Radford et al. , 2021 ;  Schuhmann et al. ,  2022 ). For each pair of text and image, represented as    $(\\mathbf{X}_{\\mathsf{t e x t}},\\mathbf{X}_{\\mathsf{i m a g e}})$  , we align them using specific objectives along with a projection module, such as the Perceiver Resampler ( Alayrac et al. ,  2022 ) for the vision encoder. \n\n$$\np(\\mathbf{X}_{\\mathrm{text}}|\\mathbf{X}_{\\mathrm{image}})=\\prod_{i=1}^{L}p_{\\theta}(\\mathbf{X}_{\\mathrm{text}}^{[i]}|\\mathbf{Z}_{\\mathrm{image}},\\mathbf{Z}_{\\mathrm{text}}^{[1:i-1]})\n$$\n \n\n$$\n\\mathbf{Z}_{\\mathsf{i m a g e}}={\\tt P r o j}_{\\theta}\\big(h_{\\tt l a t e n t s},g(\\mathbf{X}_{\\mathsf{i m a g e}})\\big)\n$$\n "}
{"page": 15, "image_path": "doc_images/SnapNTell_15.jpg", "ocr_text": "E More Related Works\n\nKnowledge-based VQA_ Various vision-language tasks often require knowledge to answer questions\nbased on image content and have evolved in recent years. Beginning with datasets like FVQA (Wang\net al., 2016), which extracted facts from pre-established knowledge bases, the field has progressed to\nmore challenging ones like the OK-VQA dataset (Marino et al., 2019), encompassing diverse knowledge\ncategories. MultiModalQA (Talmor et al., 2021) introduced complexity with questions demanding\ncross-modal reasoning over snippets, tables, and images. The successor of OK-VQA, AOK-VQA\n(Schwenk et al., 2022), raises the bar by providing questions that transcend simple knowledge base queries.\nManyModalQA (Hannan et al., 2020) shifts the focus to answer modality selection, MIMOQA (Singh\net al., 2021) emphasizes multimodal answer extraction, and WebQA (Chang et al., 2021) introduces\nreal-world knowledge-seeking questions, albeit with some limitations regarding entity categorization and\ngranularity. More comparison details are introduced in Section 3.5.\n\nMultimodal LLMs Expanding text-only LLMs to interpret visual information typically involves in-\ntegrating a visual encoder with a frozen LLM, using extensive image captioning datasets for alignment\n(Koh et al., 2023; Wu et al., 2023; Chowdhery et al., 2022). This integration can be accomplished through\nmethods such as adapter-based tuning (Alayrac et al., 2022), which fine-tunes a small portion of the model\nto process visual inputs, or prefix tuning (Tsimpoukelli et al., 2021), where trained prefixed vectors are\ninputted to guide the frozen LLM towards contextually relevant text outputs based on the visual data.\nThese techniques allow LLMs to maintain their linguistic prowess while gaining visual understanding\nwithout full model retraining (Yin et al., 2023).\n\nRetrieval augmented LLM _ Several prior approaches have investigated retrieval-augmented in the\ntext-only setting or image captioning tasks. Guu et al. (2020) augmented language model pretraining\nwith a latent knowledge retriever, which allows the model to retrieve and attend over documents from\na large corpus such as Wikipedia, used during pretraining, fine-tuning, and inference. Srinivasan et al.\n(2022) demonstrated that retrieval augmentation of queries provides LLMs with valuable additional\ncontext, enabling improved understanding. Yasunaga et al. (2023) proposed a retriever to retrieve relevant\nmultimodal documents from external memory and use the generator to make predictions for the input.\nYang et al. (2023a) proposed an accelerator to losslessly speed up LLM inference with references through\nretrieval. Yang et al. (2023b) introduced a retrieval-augmented visual language model, built upon the\nFlamingo (Alayrac et al., 2022), which supports retrieving the relevant knowledge from the external\ndatabase for zero and in-context few-shot image captioning. Another related work by Gui et al. (2021)\nintegrated implicit and explicit knowledge in an encoder-decoder architecture for jointly reasoning over\nboth knowledge sources during answer generation.\n\nOpen-domain visual entity recognition Hu et al. (2023) introduced Open-domain Visual Entity\nRecognition (OVEN) for linking images to Wikipedia entities through text queries. Chen et al. (2023)\npresented INFOSEEK, a Visual Question Answering dataset designed for information-seeking queries.\nOVEN excels at entity recognition but relies on a knowledge base for entity names, while INFOSEEK\nprimarily provides factual answers. Our research aims to bridge these gaps by generating informative\nparagraphs that offer context, enabling a deeper understanding beyond mere facts.\n\nF More Statistics of the SnapNTell Dataset\n\nIn Table 10 and Figure 9,10,11, we show more statistics of the SnapNTell dataset.\n", "vlm_text": "E More Related Works \nKnowledge-based VQA Various vision-language tasks often require knowledge to answer questions based on image content and have evolved in recent years. Beginning with datasets like FVQA ( Wang et al. ,  2016 ), which extracted facts from pre-established knowledge bases, the field has progressed to more challenging ones like the OK-VQA dataset ( Marino et al. ,  2019 ), encompassing diverse knowledge categories. Multi Modal QA ( Talmor et al. ,  2021 ) introduced complexity with questions demanding cross-modal reasoning over snippets, tables, and images. The successor of OK-VQA, AOK-VQA ( Schwenk et al. ,  2022 ), raises the bar by providing questions that transcend simple knowledge base queries. Many Modal QA ( Hannan et al. ,  2020 ) shifts the focus to answer modality selection, MIMOQA ( Singh et al. ,  2021 ) emphasizes multimodal answer extraction, and WebQA ( Chang et al. ,  2021 ) introduces real-world knowledge-seeking questions, albeit with some limitations regarding entity categorization and granularity. More comparison details are introduced in Section  3.5 . \nMultimodal LLMs Expanding text-only LLMs to interpret visual information typically involves in- tegrating a visual encoder with a frozen LLM, using extensive image captioning datasets for alignment ( Koh et al. ,  2023 ;  Wu et al. ,  2023 ;  Chowdhery et al. ,  2022 ). This integration can be accomplished through methods such as adapter-based tuning ( Alayrac et al. ,  2022 ), which fine-tunes a small portion of the model to process visual inputs, or prefix tuning ( Tsim po uk ell i et al. ,  2021 ), where trained prefixed vectors are inputted to guide the frozen LLM towards con textually relevant text outputs based on the visual data. These techniques allow LLMs to maintain their linguistic prowess while gaining visual understanding without full model retraining ( Yin et al. ,  2023 ). \nRetrieval augmented LLM Several prior approaches have investigated retrieval-augmented in the text-only setting or image captioning tasks.  Guu et al.  ( 2020 ) augmented language model pre training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre training, fine-tuning, and inference.  Srinivasan et al. ( 2022 ) demonstrated that retrieval augmentation of queries provides LLMs with valuable additional context, enabling improved understanding.  Yasunaga et al.  ( 2023 ) proposed a retriever to retrieve relevant multimodal documents from external memory and use the generator to make predictions for the input. Yang et al.  ( 2023a ) proposed an accelerator to losslessly speed up LLM inference with references through retrieval.  Yang et al.  ( 2023b ) introduced a retrieval-augmented visual language model, built upon the Flamingo ( Alayrac et al. ,  2022 ), which supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image captioning. Another related work by  Gui et al.  ( 2021 ) integrated implicit and explicit knowledge in an encoder-decoder architecture for jointly reasoning over both knowledge sources during answer generation. \nOpen-domain visual entity recognition Hu et al.  ( 2023 ) introduced Open-domain Visual Entity Recognition (OVEN) for linking images to Wikipedia entities through text queries.  Chen et al.  ( 2023 ) presented INFOSEEK, a Visual Question Answering dataset designed for information-seeking queries. OVEN excels at entity recognition but relies on a knowledge base for entity names, while INFOSEEK primarily provides factual answers. Our research aims to bridge these gaps by generating informative paragraphs that offer context, enabling a deeper understanding beyond mere facts. \nF More Statistics of the SnapNTell Dataset \nIn Table  10  and Figure  9 , 10 , 11 , we show more statistics of the SnapNTell dataset. "}
{"page": 16, "image_path": "doc_images/SnapNTell_16.jpg", "ocr_text": "Image\n\nQuestion\n\nAnswer\n\nWhere is it located?\n\nWhat date did the it\nopen to the public?\n\nWhat is the\narchitectural style of it?\n\nAbel Tasman National Park is\nlocated at the northern tip of the\nSouth Island of New Zealand\nbetween Golden Bay and Tasman\nBay\n\nThe Acropolis Museum was\ninaugurated on June 20, 2009, after\nmany years of planning and\nconstruction\n\nThe Saint Alexander Nevsky\nCathedral has been built in the\nNeo-Byzantine style.\n\nFigure 8: Examples from our SnapNTell dataset.\n\nTable 10: Category statistics of the SnapNTell dataset.\n\nCategory Number of entities\n\nlandmark\npainting\nsculpture\nfood\nfruit\nvegetable\nmammal\nfish\nbird\nreptile\namphibian\ninsect\ncelebrity\ninstrument\nplant\nelectronics\ntool\ntransportation\nsport\nbook\nhousehold\ncar\n\nCategory\n\n753\n288\n134\n271\n180\n214\n434\n124\n145\n722\n480\n210\n732\n277\n489\n269\n150\n227\n395\n645\n221\n208\n\n", "vlm_text": "The table contains three rows, each with an image, a question, and an answer:\n\n1. **Row 1:**\n   - **Image:** Landscape with coast.\n   - **Question:** \"Where is it located?\"\n   - **Answer:** \"Abel Tasman National Park is located at the northern tip of the South Island of New Zealand between Golden Bay and Tasman Bay.\"\n\n2. **Row 2:**\n   - **Image:** Interior of a museum with visitors.\n   - **Question:** \"What date did it open to the public?\"\n   - **Answer:** \"The Acropolis Museum was inaugurated on June 20, 2009, after many years of planning and construction.\"\n\n3. **Row 3:**\n   - **Image:** Cityscape with a cathedral.\n   - **Question:** \"What is the architectural style of it?\"\n   - **Answer:** \"The Saint Alexander Nevsky Cathedral has been built in the Neo-Byzantine style.\"\nThe table lists various categories with the corresponding number of entities in each category. Here's the summary:\n\n- **Category**: Number of Entities\n  - Landmark: 753\n  - Painting: 288\n  - Sculpture: 134\n  - Food: 271\n  - Fruit: 180\n  - Vegetable: 214\n  - Mammal: 434\n  - Fish: 124\n  - Bird: 145\n  - Reptile: 722\n  - Amphibian: 480\n  - Insect: 210\n  - Celebrity: 732\n  - Instrument: 277\n  - Plant: 489\n  - Electronics: 269\n  - Tool: 150\n  - Transportation: 227\n  - Sport: 395\n  - Book: 645\n  - Household: 221\n  - Car: 208\n\n- **Summary**: 22 categories with a total of 7,568 entities."}
{"page": 17, "image_path": "doc_images/SnapNTell_17.jpg", "ocr_text": "household landmark\n2.9% 9.9%\nbook _—_—— painting\n8.5%\n\ntransportation\n24%\n\nvegetable\nelectronics\n5.7%\n\n6.59\ninstrument\n3.7\ncelebrity\n9.7\nreptile\n\n2.8%\n\ninsect\n\nFigure 9: Statistics of number of entities in each category.\n\nlandmark\n9.1%\n\ncelebrity\n49.3%\n\nFigure 10: Statistics of all pageviews for all categories.\n\nlandmark\npainting\nsculpture\nfood\n\nfruit\nvegetable\nmammal\namphibian\ninsect\n\nfish\n\nbird\nreptile\ncelebrity\ninstrument\nplant\nelectronics\ntool\ntransportation\nsport\nbook\nhousehold\ncar\n\n0 1000 2000 3000 4000 5000\n\nFigure 11: Average pageview per entity within each category, where average pageview is defined as the sum of\npageviews/ number of entity.\n", "vlm_text": "The image is a donut chart representing statistics of the number of entities in each category. Here are the categories and their corresponding percentages:\n\n- Household: 2.9%\n- Book: 8.5%\n- Sport: 5.2%\n- Transportation: 3.0%\n- Electronics: 3.6%\n- Plant: 6.5%\n- Instrument: 3.7%\n- Celebrity: 9.7%\n- Reptile: 2.8%\n- Landmark: 9.9%\n- Painting: 3.8%\n- Food: 3.6%\n- Fruit: 2.4%\n- Vegetable: 2.8%\n- Mammal: 5.7%\n- Insect: 1.9%\n- Fish: 9.5%\n- Bird: 6.3%\nThe image is a donut chart showing the percentage distribution of pageviews across various categories. Here's the breakdown:\n\n- Celebrity: 49.3%\n- Landmark: 9.1%\n- Book: 5.7%\n- Mammal: 5.8%\n- Sport: 2.9%\n- Car: 2.7%\n- Food: 2.6%\n- Vegetable: 2.5%\n- Sculpture: 2.5%\n- Fruit: 2.3%\n- Fish: 2.2%\n- Plant: 1.9%\n- Transportation: 1.5%\nThe image is a horizontal bar chart with various categories on the y-axis and numbers on the x-axis. The bars represent the quantity or frequency of each category. The categories listed include:\n\n- Landmark\n- Painting\n- Sculpture\n- Food\n- Fruit\n- Vegetable\n- Mammal\n- Amphibian\n- Insect\n- Fish\n- Bird\n- Reptile\n- Celebrity\n- Instrument\n- Plant\n- Electronics\n- Tool\n- Transportation\n- Sport\n- Book\n- Household\n- Car\n\nThe \"celebrity\" category has the highest value, with \"sculpture\" and \"vegetable\" also having notable values.\nFigure 11: Average pageview per entity within each category, where average pageview is defined as the sum of pageviews/ number of entity. "}
{"page": 18, "image_path": "doc_images/SnapNTell_18.jpg", "ocr_text": "G_ Some Result Examples and Human Evaluation\n\nIn Table 11, we showed several example result by different models, and the corresponding human\nevaluation results.\n\nTable 11: Examples of answers generated by different models, where Ground-truth, BLIP2, MiniGPT4, Open-\nFlamingo, InstructBLIP, LLaVA, SnapNTell are assigned as MO, M1, ..., M6 in rating.\n\nImage\n\nQuestion Where is the attraction located?\n\nGround-truth (MO) Abel Tasman National Park is located at the northern tip of the South Island of\nNew Zealand between Golden Bay and Tasman Bay\n\nBLIP-2 (M1) california\n\nMini-GPT4 (M2) The image shows a beach with a body of water in the foreground and trees and\nmountains in the background.\n\nOpen-Flamingo (M3) __ It is located in New Zealand.\nInstruct-BLIP (M4) beach 1\n\nLLaVA (M5) The image is located at the beach, which is surrounded by a forest of trees.\n\nSnapNTell (Ours) The image is not located in the Republic of Altai, but rather in the South Island\n(M6) of New Zealand. The location is the Abel Tasman National Park, specifically\nthe area around Awaroa Beach.\n\nHuman Rating MO = M6 > M3 > MI = M2= M5 > M4\n\nImage\n\nQuestion\n\nGround-truth (MO) The Acropolis Museum was inaugurated on June 20, 2009 after many years of\nplanning and construction\n\nBLIP-2 (M1) 1 January 2009\n\nMini-GPT4 (M2) © museum opened to the public on January 1, 2018\n\nT\nOpen-Flamingo (M3) The museum opened to the public on December 12, 1987.\nInstruct-BLIP (M4) 2007\n\nLLaVA (M5) The museum opened to the public on April 13, 2019.\nSnapNTell (Ours) The Acropolis Museum opened to the public on June 20, 2009.\n(M6)\n\nHuman Rating MO = M6 > M1 > M2= M3 = M4=M5\n\n", "vlm_text": "G Some Result Examples and Human Evaluation \nIn Table  11 , we showed several example result by different models, and the corresponding human evaluation results. \nTable 11: Examples of answers generated by different models, where Ground-truth, BLIP2, MiniGPT4, Open- Flamingo, Instruct BLIP, LLaVA, SnapNTell are assigned as M0, M1, ..., M6 in rating. \nThe image depicts a scenic landscape with a coastal view. It features lush green vegetation in the foreground, a sandy beach area, and water. In the background, there are hills or low mountains under a partly cloudy sky.\nThe table contains two sections, each with an image, a question, model responses (M0-M6), and human ratings. \n\n### First Section:\n- **Image**: A landscape with trees, water, and mountains.\n- **Question**: \"Where is the attraction located?\"\n- **Responses**:\n  - Ground-truth (M0): Abel Tasman National Park, South Island of New Zealand.\n  - BLIP-2 (M1): California.\n  - Mini-GPT4 (M2): General description of a beach.\n  - Open-Flamingo (M3): New Zealand.\n  - Instruct-BLIP (M4): beach 1.\n  - LLaVA (M5): A beach surrounded by trees.\n  - SnapNTell (M6): Abel Tasman National Park, South Island of New Zealand, Awaroa Beach.\n- **Human Rating**: M0 = M6 > M3 > M1 = M2 = M5 > M4.\n\n### Second Section:\n- **Image**: Interior of a museum with modern design.\n- **Question**: \"What date did it open to the public?\"\n- **Responses**:\n  - Ground-truth (M0): Acropolis Museum, June 20, 2009.\n  - BLIP-2 (M1): January 1, 2009.\n  - Mini-GPT4 (M2): January 1, 2018.\n  - Open-Flamingo (M3): December 12, 1987.\n  - Instruct-BLIP (M4): 2007.\n  - LLaVA (M5): April 13, 2019.\n  - SnapNTell (M6): June 20, 2009.\n- **Human Rating**: M0 = M6 > M1 > M2 = M3 = M4 = M5.\n\nEach section evaluates how accurately different models can answer questions about images compared to the provided ground-truth, with ratings indicating the most accurate responses."}
{"page": 19, "image_path": "doc_images/SnapNTell_19.jpg", "ocr_text": "Image\n\nQuestion\n\nWhat is the name of the view in this picture?\n\nGround-truth (MO)\n\nBLIP-2 (M1)\n\nMini-GPT4 (M2)\nOpen-Flamingo (M3)\nInstruct-BLIP (M4)\n\nLLaVA (M5)\n\nSnapNTell\n(M6)\n\n(Ours)\n\nLake Titicaca is a large freshwater lake located in the Andes Mountains spanning\nthe borders of both Bolivia and Peru\n\na town in Peru\n\nIt is a small town with colorful buildings and boats in the water.\n\nThis is the city of Puerto Natales in Chile.\n\nharbor 2\n\nIt is a cityscape with boats in the water, located near a mountainous area.\n\nIt’s Lake Titicaca in the Andes Mountains.\n\nHuman Rating\n\nMO = M6 > M1 > M3 > M2=M5 > M4\n\nImage\n\nQuestion\n\nWhat is the age of the building now?\n\nGround-truth (MO)\n\nBLIP-2 (M1)\n\nMini-GPT4 (M2)\nOpen-Flamingo (M3)\nInstruct-BLIP (M4)\n\nLLaVA (M5)\n\nSnapNTell\n(M6)\n\n(Ours)\n\nThe Chapel Bridge is over 700 years old having been built in 1333\n\n16th century\n\nThe age of the building in the image is not known.\n\nThe age of the bridge is over 100 years old.\n\nold\n\nThe age of the building in the image is not specified. However, it is described\nas an old building, which suggests that it could be quite old or historic.\n\nThe Chapel Bridge (Chapel Bridge or Kapellbriicke) in Lucerne, Switzerland,\nis a medieval wooden bridge that dates back to the 14th century. It was built\nin 1333 and is one of the oldest covered bridges in Europe, which is over 700\nyears old now.\n\nHuman Rating\n\nM6 >= MO > M1 = M2 = M3 = M4= M5\n\n", "vlm_text": "Image \nThe image shows a picturesque lakeside town with colorful buildings nestled against a backdrop of hills or mountains. Several boats are floating on the water in the foreground. The sky is clear with a few clouds, providing a bright and vibrant atmosphere.\nThe table compares different models' responses to the question \"What is the name of the view in this picture?\" regarding an image possibly of Lake Titicaca.\n\n- **Ground-truth (M0):** Describes Lake Titicaca as a large freshwater lake in the Andes, spanning Bolivia and Peru.\n- **BLIP-2 (M1):** Identifies it as a town in Peru.\n- **Mini-GPT4 (M2):** Mentions a small town with colorful buildings and boats.\n- **Open-Flamingo (M3):** Describes it as the city of Puerto Natales in Chile.\n- **Instruct-BLIP (M4):** Refers to it as \"harbor 2\".\n- **LLaVA (M5):** States it is a cityscape with boats near a mountainous area.\n- **SnapNTell (Ours, M6):** Identifies it accurately as Lake Titicaca in the Andes Mountains.\n\n**Human Rating:** M0 = M6 > M1 > M3 > M2 = M5 > M4, indicating M0 and M6 provided the best answers.\nImage \nThe image shows the Kapellbrücke, a famous covered wooden footbridge in Lucerne, Switzerland. It features a distinctive tower and is adorned with flowers along the sides. The bridge spans a body of water, and there are buildings and hills in the background.\nThe table presents different models' responses about the age of the Chapel Bridge and how they compare to the ground truth:\n\n- **Ground-truth (M0):** The Chapel Bridge is over 700 years old, built in 1333.\n- **BLIP-2 (M1):** 16th century.\n- **Mini-GPT4 (M2):** The age of the building in the image is not known.\n- **Open-Flamingo (M3):** The age of the bridge is over 100 years old.\n- **Instruct-BLIP (M4):** Old.\n- **LLAVA (M5):** The age is not specified but is described as old.\n- **SnapNTell (M6):** Detailed description matching the ground truth, built in 1333, over 700 years old.\n\n**Human Rating:** M6 ≥ M0 > M1 = M2 = M3 = M4 = M5, indicating M6 and M0 provide the most accurate information."}
