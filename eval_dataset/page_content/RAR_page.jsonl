{"page": 0, "image_path": "doc_images/RAR_0.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented\nMLLMs for Visual Recognition\n\nZiyu Liu*!4, Zeyi Sun*?-4, Yuhang Zang*, Wei Li®, Pan Zhang‘,\nXiaoyi Dong*, Yuanjun Xiong®, Dahua Lin, Jiagi Wang!*\nlWuhan University ?Shanghai Jiao Tong University *The Chinese\nUniversity of Hong Kong “Shanghai AI Laboratory °MThreads, Inc. °\nNanyang Technological University\n2020302121195@whu.edu.cn, szy2023@sjtu.edu.cn, {zangyuhang, zhangpan,\ndongxiaoyi, wangjiaqi}@pjlab.org.cn\nhttps: //github.com/Liuziyu77/RAR\n\nAbstract. CLIP (Contrastive Language-Image Pre-training) uses con-\ntrastive learning from noise image-text pairs to excel at recognizing a\nwide array of candidates, yet its focus on broad associations hinders the\nprecision in distinguishing subtle differences among fine-grained items.\nConversely, Multimodal Large Language Models (MLLMs) excel at clas-\nsifying fine-grained categories, thanks to their substantial knowledge\nfrom pre-training on web-level corpora. However, the performance of\nMLLMs declines with an increase in category numbers, primarily due\nto growing complexity and constraints of limited context window size.\nTo synergize the strengths of both approaches and enhance the few-\nshot /zero-shot recognition abilities for datasets characterized by exten-\nsive and fine-grained vocabularies, this paper introduces 2.\\R, a Retrieving\nAnd Ranking augmented method for MLLMs. We initially establish a\nmulti-modal retriever based on CLIP to create and store explicit memory\nfor different categories beyond the immediate context window. During in-\nference, R.\\R retrieves the top-k similar results from the memory and\nuses MLLMs to rank and make the final predictions. Our proposed ap-\nproach not only addresses the inherent limitations in fine-grained recog-\nnition but also preserves the model’s comprehensive knowledge base,\nsignificantly boosting accuracy across a range of vision-language recog-\nnition tasks. Notably, our approach demonstrates a significant improve-\nment in performance on 5 fine-grained visual recognition benchmarks, 11\nfew-shot image recognition datasets, and the 2 object detection datasets\nunder the zero-shot recognition setting.\n\nKeywords: MLLM - Fine-Grained - Few-shot - Zero-shot Recognition\n\n1 Introduction\n\nThe CLIP (Contrastive Language—Image Pre-training) [41] model and its diverse\nvariants [8, 26,45] provide flexible and robust performance across a wide array\nof visual-language understanding tasks. Despite its successes, we observe that\n", "vlm_text": "RAR :  R etrieving    $\\mathbf{A}$  nd    $\\mathrm{R}$  anking Augmented MLLMs for Visual Recognition \nZiyu Liu  $^{*1,4}$  , Zeyi Sun  $^{*2,4}$  , Yuhang Zang  $^4$  , Wei Li  $^6$  , Pan Zhang  $^4$  , Xiaoyi Dong  $^4$  , Yuanjun Xiong  $^5$  , Dahua Lin  $^{3,4}$  , Jiaqi Wang † 4  $^{1}$  Wuhan University  $^2$  Shanghai Jiao Tong University 3 The Chinese University of Hong Kong 4 Shanghai AI Laboratory 5 MThreads, Inc. 6 Nanyang Technological University 2020302121195@whu.edu.cn, szy2023@sjtu.edu.cn, {zangyuhang, zhangpan, dongxiaoyi, wangjiaqi}@pjlab.org.cn https://github.com/Liuziyu77/RAR \nAbstract.  CLIP (Contrastive Language–Image Pre-training) uses con- trastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at clas- sifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few- shot/zero-shot recognition abilities for datasets characterized by exten- sive and fine-grained vocabularies, this paper introduces  RAR , a  R etrieving A nd  R anking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During in- ference,  RAR  retrieves the top-  $k$   similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed ap- proach not only addresses the inherent limitations in fine-grained recog- nition but also preserves the model’s comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recog- nition tasks. Notably, our approach demonstrates a significant improve- ment in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting. \nKeywords:  MLLM · Fine-Grained · Few-shot · Zero-shot Recognition \n1 Introduction \nThe CLIP (Contrastive Language–Image Pre-training) [ 41 ] model and its diverse variants [ 8 ,  26 ,  45 ] provide flexible and robust performance across a wide array of visual-language understanding tasks. Despite its successes, we observe that "}
{"page": 1, "image_path": "doc_images/RAR_1.jpg", "ocr_text": "2 Ziyu Liu et al.\n\n7 Motivation ClassificationtRAR\n\nBAE 146-300 Common\n\nATR-A2\n\n+ |\n\nCaltech101— UCF101 Imagenet\n\nLarge V y\nDetection Datasets top2: pinwheel\n\ntop3: doughnut\n\ntopl: doughnut\ntop2: pastry\ntop3: pinwheel\n\nFig. 1: Upper left: our motivation about the drawbacks of CLIP and MLLM. Our\nRAR can seamlessly integrate into MLLMs to improve the few-shot /zero-shot abilities\non classification (upper right) and detection (bottom) datasets.\n\nCLIP’s performance begins to wane when faced with datasets characterized by\nvast vocabularies or fine-grained categories. As shown in the upper left of Fig. 1,\nthe decline is largely attributable to the inherent ambiguity of language descrip-\ntions and the challenges posed by synonyms, which can confound the model’s\nability to distinguish between closely related but distinct classes.\n\nParallel to these developments, Multi-modal Large Language Models (MLLMs)\nhave emerged as a powerful class of generative models, exemplified by the likes of\nGPT-4V [38] and analogous advancements [1—3, 6,29, 40, 49, 55,56, 60]. MLLMs,\npre-trained on extensive corpora with substantial knowledge, demonstrate re-\nmarkable proficiency in identifying fine-grained categories when the total num-\nber of candidates remains manageable. Nevertheless, MLLMs’ efficacy is simi-\nlarly compromised in scenarios involving extensive vocabularies and fine-grained\ncategorizations (upper left of Fig. 1). The core of the issue lies in MLLMs fac-\ning significant challenges in managing large context windows (e.g., maximum\n2k tokens for LLaVA1.5 [28]), a critical requirement for accurate processing and\ninterpreting tasks that demand a nuanced understanding of vast vocabularies\nand subtle distinctions.\n\nTo address these challenges, we propose augmenting standard MLLMs with\nour RAR, a retrieving-and-ranking augmented technique. Our [\\ ii enables\nmodels to dynamically incorporate external knowledge into the processing and\ngeneration workflows. By augmenting MLLMs with external knowledge sources,\nwe address challenges related to language ambiguity, synonym handling, and the\nlimitations imposed by limited context windows when dealing with vast vocab-\nularies. Our method uses the inherent strength of MLLMs in generalizing from\n\n", "vlm_text": "The image presents a visual summary of the research study on enhancing the performance of CLIP and MLLM using RAR. Here's an overview:\n\n- **Motivation**: Highlights the challenges of using VLMs like CLIP for fine-grained classification and MLLM for large vocabulary classification. It displays the performance issues with particular aircraft models and classification datasets.\n\n- **Classification+RAR**: Shows improved accuracy on classification datasets, both fine-grained (like Food101 and Flowers102) and common (like Caltech101). It illustrates the correction of a misclassification from \"Azalea\" to \"Clematis\" using RAR, and overall improved accuracy across datasets.\n\n- **Detection+RAR**: Demonstrates improvements in detection on large vocabulary datasets such as LVIS and V3Det. The RAR approach enhances detection by reranking and correcting initial predictions, evidenced by higher AP scores.\n\nOverall, the image emphasizes the seamless integration of RAR into MLLMs to improve few-shot/zero-shot abilities in both classification and detection tasks.\nCLIP’s performance begins to wane when faced with datasets characterized by vast vocabularies or fine-grained categories. As shown in the upper left of Fig.  1 , the decline is largely attributable to the inherent ambiguity of language descrip- tions and the challenges posed by synonyms, which can confound the model’s ability to distinguish between closely related but distinct classes. \nParallel to these developments, Multi-modal Large Language Models (MLLMs) have emerged as a powerful class of generative models, exemplified by the likes of GPT-4V [ 38 ] and analogous advancements [ 1 – 3 , 6 , 29 , 40 , 49 , 55 , 56 , 60 ]. MLLMs, pre-trained on extensive corpora with substantial knowledge, demonstrate re- markable proficiency in identifying fine-grained categories when the total num- ber of candidates remains manageable. Nevertheless, MLLMs’ efficacy is simi- larly compromised in scenarios involving extensive vocabularies and fine-grained categorizations (upper left of Fig.  1 ). The core of the issue lies in MLLMs fac- ing significant challenges in managing large context windows ( e.g ., maximum 2k tokens for LLaVA1.5 [ 28 ]), a critical requirement for accurate processing and interpreting tasks that demand a nuanced understanding of vast vocabularies and subtle distinctions. \nTo address these challenges, we propose augmenting standard MLLMs with our  RAR , a retrieving-and-ranking augmented technique. Our  RAR  enables models to dynamically incorporate external knowledge into the processing and generation workflows. By augmenting MLLMs with external knowledge sources, we address challenges related to language ambiguity, synonym handling, and the limitations imposed by limited context windows when dealing with vast vocab- ularies. Our method uses the inherent strength of MLLMs in generalizing from "}
{"page": 2, "image_path": "doc_images/RAR_2.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 3\n\nexisting knowledge while addressing their limitations in visual recognition. We\nfirst construct a multi-modal retriever that creates and stores multimodal em-\nbeddings for visual images and text descriptions. As shown in Fig. 1, upon receiv-\ning an input image at the inference stage, our approach retrieves the top-k class\nnames most similar to the image. Subsequently, the MLLMs rank these retrieved\ncandidate results as the final prediction results. To bolster the MLLMs’ rank-\ning performance, we explore fine-tuning with ranking format data or in-context\nlearning examples without training. By integrating our retrieval-augmented de-\nsign, our approach seeks to bridge the gap between the broad generalization\ncapabilities of MLLMs and the need for precise, fine-grained categorization, of-\n\nfering a path forward that preserves the model’s extensive knowledge base while\nsignificantly boosting its performance on downstream tasks.\n\nTo evaluate our method’s efficacy, we conducted benchmarks in three ar-\neas: (1) fine-grained visual recognition across 5 benchmarks, (2) few-shot image\nrecognition across 11 datasets, and (3) zero-shot object recognition on 2 object\ndetection datasets with vast vocabularies (e.g., 13204 classes of V3Det [48]). As\npresented in the right part of Fig. 1, our findings reveal that our approach no-\ntably enhances few-shot learning abilities, yielding an average improvement of\n6.2% over 11 image classification datasets under the 4-shot setting. Furthermore,\nour method achieves a 6.4% improvement on the LVIS dataset and a 1.5% gain\non the V3Det dataset in zero-shot object recognition performance.\n\nIn summary, our key contributions are outlined as follows: (1) We conduct\nan in-depth analysis of the strengths and weaknesses of VLMs and MLLMs in\nprocessing fine-grained datasets. (2) To enhance the fine-grained few-shot and\nzero-shot perception capabilities of MLLMs, we introduce 2. AR with a multi-\nmodal retriever and the inference pipeline based on retrieving and ranking. (3)\nOur 2. AR can be seamlessly integrated into various MLLMs in a plug-and-play\nmanner. (4) Through rigorous testing across 11 classification datasets and 2 ob-\nject detection datasets, we demonstrate that our method outperforms baselines\non a variety of visual recognition tasks.\n\n2 Related Work\n\nContrastive Language-Image Pre-training (CLIP) [41] understands im-\nages and texts by contrastive learning from a vast amount of visual data paire\nwith natural language descriptions. CLIP has robust capabilities in downstream\ntasks including image-text retrieval [54], zero-shot classification [12,58], an\nopen-vocabulary perception [13,59]. Following CLIP, many subsequent vision-\nlanguage models [8, 10,18, 23,24, 26,33,46,53,57] are proposed to further improve\nthe vision-language understanding abilities. There are also works done to improve\nCLIP in zero-shot perception tasks [27,42, 44,52]. However, simple dot-produc\nbetween two unimodality features can lead to sub-optimal results for fine-graine:\nclassification. In this paper, we demonstrate that CLIP faces challenges in mak-\ning accurate zero-shot predictions for fine-grained classes, and how our propose:\nmethod can effectively re-rank these predictions to improve the accuracy.\n\n", "vlm_text": "existing knowledge while addressing their limitations in visual recognition. We first construct a multi-modal retriever that creates and stores multimodal em- beddings for visual images and text descriptions. As shown in Fig.  1 , upon receiv- ing an input image at the inference stage, our approach retrieves the top-  $k$   class names most similar to the image. Subsequently, the MLLMs rank these retrieved candidate results as the final prediction results. To bolster the MLLMs’ rank- ing performance, we explore fine-tuning with ranking format data or in-context learning examples without training. By integrating our retrieval-augmented de- sign, our approach seeks to bridge the gap between the broad generalization capabilities of MLLMs and the need for precise, fine-grained categorization, of- fering a path forward that preserves the model’s extensive knowledge base while significantly boosting its performance on downstream tasks. \nTo evaluate our method’s efficacy, we conducted benchmarks in three ar- eas: (1) fine-grained visual recognition across 5 benchmarks, (2) few-shot image recognition across 11 datasets, and (3) zero-shot object recognition on 2 object detection datasets with vast vocabularies ( e.g ., 13204 classes of V3Det [ 48 ]). As presented in the right part of Fig.  1 , our findings reveal that our approach no- tably enhances few-shot learning abilities, yielding an average improvement of  $6.2\\%$   over 11 image classification datasets under the 4-shot setting. Furthermore, our method achieves a    $6.4\\%$   improvement on the LVIS dataset and a    $1.5\\%$   gain on the V3Det dataset in zero-shot object recognition performance. \nIn summary, our key contributions are outlined as follows: (1) We conduct an in-depth analysis of the strengths and weaknesses of VLMs and MLLMs in processing fine-grained datasets. (2) To enhance the fine-grained few-shot and zero-shot perception capabilities of MLLMs, we introduce  RAR  with a multi- modal retriever and the inference pipeline based on retrieving and ranking. (3) Our  RAR  can be seamlessly integrated into various MLLMs in a plug-and-play manner. (4) Through rigorous testing across 11 classification datasets and 2 ob- ject detection datasets, we demonstrate that our method outperforms baselines on a variety of visual recognition tasks. \n2 Related Work \nContrastive Language-Image Pre-training (CLIP)  [ 41 ] understands im- ages and texts by contrastive learning from a vast amount of visual data paired with natural language descriptions. CLIP has robust capabilities in downstream tasks including image-text retrieval [ 54 ], zero-shot classification [ 12 ,  58 ], and open-vocabulary perception [ 13 ,  59 ]. Following CLIP, many subsequent vision- language models [ 8 , 10 , 18 , 23 , 24 , 26 , 33 , 46 , 53 , 57 ] are proposed to further improve the vision-language understanding abilities. There are also works done to improve CLIP in zero-shot perception tasks [ 27 , 42 , 44 , 52 ]. However, simple dot-product between two uni modality features can lead to sub-optimal results for fine-grained classification. In this paper, we demonstrate that CLIP faces challenges in mak- ing accurate zero-shot predictions for fine-grained classes, and how our proposed method can effectively re-rank these predictions to improve the accuracy. "}
{"page": 3, "image_path": "doc_images/RAR_3.jpg", "ocr_text": "4 Ziyu Liu et al.\n\nMultimodal Large Language Models (MLLMs) such as GPT4V [38], repre-\nsent a significant evolution in the landscape of Large Language Models (LLMs)\nby integrating visual images as input tokens alongside textual information. The\nintegration is facilitated through the use of an additional vision encoder [41\nand a bridging mechanism [1—3, 6, 29, 40, 49,55, 56,60]. MLLMs significantly en-\nhance the interaction between humans and AI in more natural and intuitive\nways and demonstrate remarkable capabilities in understanding and generating\nmulti-modal content. Despite their prowess, our research uncovers a nuance:\nlimitation: MLLMs tend to underperform in tasks requiring vast vocabularies,\nwhere distinguishing subtle differences among different categories is crucial. How-\never, we prove that MLLMs exhibit a strong ability to excel in the re-ranking of\ntop results obtained through vision-language models such as CLIP. Fine-R [31\nfirst delves into leveraging MLLMs for fine-grained perception tasks by promp\ndesign for better descriptions and attributes. We find a new way to prompt i\nwith possible candidates to help screening and achieve better performance.\n\nRetrieval-Augmented Generation (RAG) [21] refers to the solution of in-\ncorporating knowledge from external databases for LLMs, which helps reduce\nhallucination, continuous knowledge updates, and integration of domain-specific\ninformation. Specifically, RAG models first retrieve the relevant knowledge to the\ngiven text query from the external knowledge base and then augment the LLMs\nwith the retrieved knowledge. In computer vision, some previous works explore\nretrieval-augmented approaches with VLMs for long-tailed classification [17,32],\nimage-text retrieval [30] or image generation [54]. Different from previous works,\nour paper first designs a retrieval-augmented solution for MLLMs. Our research\ninvestigates how incorporating image-image retrieval, image-text retrieval, and\n\nan explicit memory component can enhance the zero-shot and few-shot capabil-\nities of MLLMs.\n\n3 Methodology\n\nWe first provide the background information on CLIP, MLLMs, and retrieval-\naugmentation in LLMs (Sec. 3.1). Then we present the multi-modal retriever\n(Sec. 3.2) module of 2. AR and how to apply 2A on downstream tasks via\nretrieving and ranking (Sec. 3.3).\n\n3.1 Preliminaries\n\nCLIP is a model combining an image encoder ®img and a text encoder Px that:\nuses contrastive learning to understand and align images and text by training\non a vast dataset gathered from the web. The core mechanism of CLIP involves\nmapping an input image Z to its most semantically similar category c € C:\n\nPly = elx) = arg max cos(Pimg(Z), Prxt(c)) (1)\n\nwhere y represents the predicted category, C refers to the whole categories list\nand cos(-,-) denotes to the cosine similarity.\n", "vlm_text": "Multimodal Large Language Models  (MLLMs) such as GPT4V [ 38 ], repre- sent a significant evolution in the landscape of Large Language Models (LLMs) by integrating visual images as input tokens alongside textual information. The integration is facilitated through the use of an additional vision encoder [ 41 ] and a bridging mechanism [ 1 – 3 , 6 , 29 , 40 , 49 , 55 , 56 , 60 ]. MLLMs significantly en- hance the interaction between humans and AI in more natural and intuitive ways and demonstrate remarkable capabilities in understanding and generating multi-modal content. Despite their prowess, our research uncovers a nuanced limitation: MLLMs tend to under perform in tasks requiring vast vocabularies, where distinguishing subtle differences among different categories is crucial. How- ever, we prove that MLLMs exhibit a strong ability to excel in the re-ranking of top results obtained through vision-language models such as CLIP. Fine-R [ 31 ] first delves into leveraging MLLMs for fine-grained perception tasks by prompt design for better descriptions and attributes. We find a new way to prompt it with possible candidates to help screening and achieve better performance. \nRetrieval-Augmented Generation  (RAG) [ 21 ] refers to the solution of in- corpora ting knowledge from external databases for LLMs, which helps reduce hallucination, continuous knowledge updates, and integration of domain-specific information. Specifically, RAG models first retrieve the relevant knowledge to the given text query from the external knowledge base and then augment the LLMs with the retrieved knowledge. In computer vision, some previous works explore retrieval-augmented approaches with VLMs for long-tailed classification [ 17 , 32 ], image-text retrieval [ 30 ] or image generation [ 54 ]. Different from previous works, our paper first designs a retrieval-augmented solution for MLLMs. Our research investigates how incorporating image-image retrieval, image-text retrieval, and an explicit memory component can enhance the zero-shot and few-shot capabil- ities of MLLMs. \n3 Methodology \nWe first provide the background information on CLIP, MLLMs, and retrieval- augmentation in LLMs (Sec.  3.1 ). Then we present the multi-modal retriever (Sec.  3.2 ) module of  RAR  and how to apply  RAR  on downstream tasks via retrieving and ranking (Sec.  3.3 ). \n3.1 Preliminaries \nCLIP  is a model combining an image encoder    $\\varPhi_{\\mathrm{img}}$   and a text encoder    $\\varPhi_{\\mathrm{{ext}}}$   that uses contrastive learning to understand and align images and text by training on a vast dataset gathered from the web. The core mechanism of CLIP involves mapping an input image  $\\mathcal{L}$   to its most semantically similar category    $c\\in\\mathcal C$  : \n\n$$\np(y=c|\\mathbf{x})=\\arg\\operatorname*{max}_{c\\in\\mathcal{C}}\\cos(\\varPhi_{\\mathrm{img}}(\\mathcal{Z}),\\varPhi_{\\mathrm{stat}}(c))\\,,\n$$\n \nwhere    represents the predicted category,    $\\mathcal{C}$   refers to the whole categories list  $y$  and    $\\cos(\\cdot,\\cdot)$   denotes to the cosine similarity. "}
{"page": 4, "image_path": "doc_images/RAR_4.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 5\n\nMultimodal Large Language Models such as GPT4V [38] learning to gener-\nate predictions over sequences of tokens that span both image and text modali-\nties. The MLLM model f, parameterized by weights 0, conditioned on the input\nsequences x = (21,...,%z,,) of length Lin, which consist of both text tokens\nXtxt and visual tokens Ximg. The Ximg are extracted from the input image Z\nvia the image encoder ;,,. MLLM model forecast a sequence of output tokens\nY =(Y1.---5YLouw) Of length Lou, as follows:\n\nLout\n\npotylx) = TT], potuixsyera) = [2 softmax(fe,yeias®))y+ 2)\n\nwhere y<i-1 := (y1,...,yi—1) refers to the mechanism that predicts the distri-\nbution of the next token considering all previously generated tokens.\nRetrieval-Augmentation in Large Language Models introduces a retrieval\nmodule R with the LLM parameterized by @ for generation. The retrieval module\nR is designed to process an input sequence x against an external memory of\ndocuments M, efficiently selecting a subset of documents M C M. The subset\nM is then fed along with the original input sequence x into the LLM @, which\nuses both the input and the context provided by retrieved results to generate\nthe target output y:\n\nLout 4\npe(y|x, M) = IL po(y|x, M, y<i-1). (3)\n\n3.2. Multimodal Retriever\n\nThe multimodal retriever is essentially responsible for querying a large multi-\nmodal external memory or database to find information relevant to the input\nquery or context. In the process of multimodal retriever, the main challenge lies\nin efficiently encoding and storing a large volume of images/text embeddings for\nquick, accurate retrieval. Recognizing the main challenge, as shown in Fig. 2, we\nhave developed a multi-modal retriever that creates and stores multimodal em-\nbeddings, with a focus on optimizing retrieval speed through index construction\ntechniques.\n\nExtracting the Multi-modal Embeddings. We use the CLIP model dis-\ncussed in Sec. 3.1 to extract the multi-modal embeddings. Given a data sample\n(x;,c;) from the dataset D containing the image x; and class name c;, we use the\nCLIP image encoder ®ing to extract the image embedding eimg € R¢ and the\nCLIP text encoder ®.4 to extract the text embedding etx, € R?. The symbol\nd refers to the feature dimension (e.g., d = 576 for CLIP ViT-B/16). The image\nand text embeddings are stored in the memory M for retrieval (will discuss in\nSec. 3.3). In some zero-shot settings, the image embedding is not available and\nwe merely store the text embedding into the memory.\n\nFast Retrieval Optimization. The brute force search is the common method\nfor designing the retriever, which requires iteration over all vectors in the memory\nM to compute similarity scores (e.g., cosine similarity) and subsequently identify\nthe top-k results. Although the brute force method is inherently straightforward,\n", "vlm_text": "Multimodal Large Language Models  such as GPT4V [ 38 ] learning to gener- ate predictions over sequences of tokens that span both image and text modali- ties. The MLLM model    $f$  , parameterized by weights    $\\theta$  , conditioned on the input sequences    $\\mathbf{x}=\\left(x_{1},\\cdot\\cdot\\cdot,x_{L_{i n}}\\right)$   of length    $L_{i n}$  , which consist of both text tokens  $\\mathbf{x}_{\\mathrm{{tot}}}$   and visual tokens    $\\mathbf{x}_{\\mathrm{img}}$  . The    $\\mathbf{x}_{\\mathrm{img}}$   are extracted from the input image    $\\mathcal{L}$  via the image encoder    $\\varPhi_{\\mathrm{img}}$  . MLLM model forecast a sequence of output tokens  $\\mathbf{y}=(y_{1},.\\,.\\,.\\,,y_{L_{o u t}})$   of length    $L_{o u t}$   as follows: \n\n$$\np_{\\theta}(\\mathbf{y}|\\mathbf{x})=\\prod_{l=1}^{L_{o u t}}\\,p_{\\theta}(y_{l}|\\mathbf{x},\\mathbf{y}_{\\leq l-1})=\\prod_{l=1}^{L_{o u t}}\\,\\mathrm{softmax}(f(\\mathbf{x},\\mathbf{y}_{\\leq l-1};\\theta))_{y_{l}}\\,,\n$$\n \nwhere    $\\mathbf{y}_{\\leq l-1}:=(y_{1},.\\,.\\,.\\,,y_{l-1})$   refers to the mechanism that predicts the distri- bution of the next token considering all previously generated tokens. \nRetrieval-Augmentation in Large Language Models  introduces a retrieval module    $R$   with the LLM parameterized by    $\\theta$   for generation. The retrieval module  $R$   is designed to process an input sequence    $\\mathbf{x}$   against an external memory of documents    $\\mathcal{M}$  , efficiently selecting a subset of documents    $M\\subseteq\\mathcal{M}$  . Th  subset  $M$   is then fed along with the original input sequence    $\\mathbf{x}$   into the LLM  θ , which uses both the input and the context provided by retrieved results to generate the target output    $\\mathbf{y}$  : \n\n$$\np_{\\theta}(\\mathbf{y}\\vert\\mathbf{x},M)=\\prod_{l=1}^{L_{o u t}}p_{\\theta}(y_{l}\\vert\\mathbf{x},M,\\mathbf{y}_{\\le l-1}).\n$$\n \n3.2 Multimodal Retriever \nThe multimodal retriever is essentially responsible for querying a large multi- modal external memory or database to find information relevant to the input query or context. In the process of multimodal retriever, the main challenge lies in efficiently encoding and storing a large volume of images/text embeddings for quick, accurate retrieval. Recognizing the main challenge, as shown in Fig.  2 , we have developed a multi-modal retriever that creates and stores multimodal em- beddings, with a focus on optimizing retrieval speed through index construction techniques. \nExtracting the Multi-modal Embeddings.  We use the CLIP model dis- cussed in Sec.  3.1  to extract the multi-modal embeddings. Given a data sample  $(x_{i},c_{i})$   from the dataset    $\\mathcal{D}$   containing the image    and class name   , we use the  $x_{i}$   $c_{i}$  CLIP image encod  $\\varPhi_{\\mathrm{img}}$   to extract the image embedd  $e_{\\mathrm{img}}\\in\\mathbb{R}^{d}$    and the LIP text encoder  $\\varPhi_{\\mathrm{text}}$   to extract the mbedding  e  $e_{\\mathrm{test}}\\in\\mathbb{R}^{d}$   ∈ . The symbol d  refers to the feature dimension ( e.g .,  d  = 576  for CLIP ViT-B/16). The image and text embeddings are stored in the memory    $\\mathcal{M}$   for retrieval (will discuss in Sec.  3.3 ). In some zero-shot settings, the image embedding is not available and we merely store the text embedding into the memory. \nFast Retrieval Optimization.  The brute force search is the common method for designing the retriever, which requires iteration over all vectors in the memory  $\\mathcal{M}$   to co pute similarity scores ( e.g ., cosine similarity) and subsequently identify the top- k  results. Although the brute force method is inherently straightforward, "}
{"page": 5, "image_path": "doc_images/RAR_5.jpg", "ocr_text": "6 Ziyu Liu et al.\n\n(a) Multimodal Retriever\n\nDatabase Image Feature Index 4\n\nt 4\n\nImage-Image k-NN\n\n———_ Labels\n\nRetrieving TLL tmage-Text k-NN\n\n(b) Retrieving & Ranking\nPredicted\nLabel! Monarch\n\nCy Cy Cy & | butterfly\n\n: I ranine 3)\n\n\\—\" MLLM\n\nRetrieved Top-K Categories\n\nCe\n\nEmbeddings Combining |\n\nFig. 2: Pipeline of RAR. (a) We design a multimodal retriever that extracts\nthe image or text embeddings and stores embeddings in an external memory M. (b)\nFor the inference stage of downstream recognition tasks, we retrieve top-k categories\nfrom the memory and use MLLMs to refine the retrieved results as the final prediction\nthrough ranking.\n\nits efficiency markedly diminishes as the dataset escalates to the magnitude of\nmillions of embeddings. To enhance the speed of retrieval, we implement an index\nsystem that uses the HNSW (Hierarchical Navigable Small World) algorithm [35].\nThe adoption of the HNSW methodology facilitates a significant dimensionality\nreduction, thereby enabling the construction of a more condensed index. Specif-\nically, vectors in a R¢ space of dimension d are transformed into a reduced d\ndimensional space. This reduction in dimensionality plays a pivotal role in en-\nhancing the speed of the retrieval process.\n\nPre-processing for Detection Datasets. In object detection datasets, our\nmethodology for extracting image embeddings €jmg is slightly different from the\napproach discussed previously. As presented in Fig. 3, we apply two additional\npre-processing steps: cropping and blurring. Some previous works have proposed\nsimilar methods in CLIP like [33,53]. In the object detection dataset, an image\ntypically contains multiple objects of varying sizes. Some objects may dominate\na large portion of the image, whereas others occupy minimal space. Accordingly,\nour object detection procedure begins with cropping the image regions based on\nproposal bounding box coordinates, subsequently resizing the cropped region\nto a fixed proportion. Moreover, unlike image classification tasks the objects\nof interest generally appear large and centrally positioned, the objects within\nobject detection datasets are smaller and their positions more varied. To help the\nMLLMs understand the objects to be detected, we employ a blurring technique\non the non-target areas surrounding the objects of interest. The blurring strategy\nis designed to direct the MLLMs’ focus toward the relevant objects, thereby\nfacilitating their identification in object detection tasks.\n", "vlm_text": "The image depicts a two-part pipeline for a process labeled \"RAR\":\n\n1. **Multimodal Retriever (a)**:\n   - **Image Encoder**: Extracts image feature embeddings from a dataset.\n   - **Feature Index**: Stores the embeddings and indexes them for retrieval.\n   - **Memory ($\\mathcal{M}$)**: External storage for embeddings.\n   - **Retrieving Process**: Utilizes k-nearest neighbors (k-NN) for image-image and image-text retrieval.\n\n2. **Retrieving & Ranking (b)**:\n   - **Inference Stage**: An image is encoded into embeddings.\n   - **Top-K Categories**: Retrieved from memory based on similarity.\n   - **Ranking**: Multimodal Large Language Models (MLLMs) are used to refine and rank these categories.\n   - **Final Prediction**: Outputs the predicted label, e.g., \"Monarch butterfly.\"\n\nThe pipeline aims to enhance recognition tasks by combining multi-modal data retrieval and ranking processes.\nits efficiency markedly diminishes as the dataset escalates to the magnitude of millions of embeddings. To enhance the speed of retrieval, we implement an index system that uses the HNSW(Hierarchical Navigable Small World) algorithm [ 35 ]. The adoption of the HNSW methodology facilitates a significant dimensionality reduction, thereby enabling the construction of a more condensed index. Specif- ically, vectors in a    $\\mathbb{R}^{d}$    space of dimension    $d$   are transformed into a reduced    $\\frac{d}{9}$  dimensional space. This reduction in dimensionality plays a pivotal role in en- hancing the speed of the retrieval process. \nPre-processing for Detection Datasets.  In object detection datasets, our methodology for extracting image embeddings    $e_{\\mathrm{img}}$   is slightly different from the approach discussed previously. As presented in Fig.  3 , we apply two additional pre-processing steps: cropping and blurring. Some previous works have proposed similar methods in CLIP like [ 33 , 53 ]. In the object detection dataset, an image typically contains multiple objects of varying sizes. Some objects may dominate a large portion of the image, whereas others occupy minimal space. Accordingly, our object detection procedure begins with  cropping  the image regions based on proposal bounding box coordinates, subsequently  resizing  the cropped region to a fixed proportion. Moreover, unlike image classification tasks the objects of interest generally appear large and centrally positioned, the objects within object detection datasets are smaller and their positions more varied. To help the MLLMs understand the objects to be detected, we employ a  blurring  technique on the non-target areas surrounding the objects of interest. The blurring strategy is designed to direct the MLLMs’ focus toward the relevant objects, thereby facilitating their identification in object detection tasks. "}
{"page": 6, "image_path": "doc_images/RAR_6.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 7\n\n(a) Pre-process (b) Embedding & Retrieve\n\n3 —[] SS Gv\n\nBbox!: carnation, bouquet, flower_arrangement,\nBbox2: pepper_mill, saltshaker, chopping_board,\nBbox3: flowerpot, vase, glass_(drink_container),\n\n@® Bbox!: flower_arrangement\n® Bbox?: saltshaker\nD\n® Bbox3: vase\n\nFig. 3: Extending our multimodal retriever to zero-shot recognition on object detec-\ntion datasets such as LVIS [14] and V3Det [48]. Compared to the classification datasets,\nwe apply the additional pre-processing techniques such as cropping and resizing to\nextract the image embeddings.\n\n3.3 Inference with Retrieving And Ranking\n\nAfter successfully constructing memory M by using our multimodal retriever,\nour next step is to integrate the memory with the retrieval process and use\nMLLMs to rank the retrieval results and enhance the performance in few-shot /zero-\nshot perception tasks.\n\nFor example, in the inference stage of the few-shot image classification task,\n\nwe first use the visual encoder ®img to process the input image and obtain the\ncorresponding image embedding é. The visual encoder is identical to the encoder\nused in our multi-modal retriever. The image embedding é is then navigate\nthrough the previously constructed memory index and ranked by similarity to\nidentify the top-k related images. Consequently, memory M yields the names of\nhe retrieved top-k categories, denoted as {c1,c2,¢3,...,ck}. The top-k retrieve\nresults serve as a preliminary filter, narrowing down the vast possibilities to\nthose most likely relevant, based on historical data and the semantic closeness\nof stored labels to the image content.\nSince these cropped sub-images are usually small, CLIP’s ability to extrac\nfeatures from these low-resolution images is limited. Therefore, in the object de-\ntection task, we do not perform image-to-image retrieval but use CLIP’s inherent\nimage-text interaction capabilities to conduct image-to-text retrieval. Finally, we\nalso obtain the top-k category information with the highest similarity.\n\nFollowing the retrieval phase, the retrieved category labels alongside image\nembedding é are integrated and sent to the MLLMs through our ranking prompt.\nThe MLLMs, combining the internal knowledge and the retrieved information,\nmake the final prediction of the image category. Our proposed inference process,\nusing both the retrieval results from our memory bank and subsequent ranking\nby the MLLM, ensures a more accurate and contextually aware classification\n\n+\n\nprediction. Our design represents a significant advancement in few-shot image\nclassification, enabling our system to handle a wide variety of images and cate-\ngories with high precision and flexibility.\n\nRanking Prompt Format. Fig. 4 presents our ranking prompt format. The\nprocess begins with the prompt ‘Sort the optional categories: [class a,\n", "vlm_text": "The image is a diagram illustrating a system for object recognition on detection datasets, featuring two main sections:\n\n1. **(a) Pre-process**: \n   - An image with multiple bounding boxes is shown. Each bounding box highlights different objects (e.g., flowers and a vase).\n   - These objects are cropped and resized to create individual embeddings using an Image Encoder.\n\n2. **(b) Embedding & Retrieve**:\n   - The image embeddings are used in a k-nearest neighbors (k-NN) search.\n   - An index is created for these embeddings, linked to a memory storage (Memory M).\n   - The retrieval process associates each bounding box with possible labels: \n     - Bbox1 with objects like \"flower arrangement\"\n     - Bbox2 with \"saltshaker\"\n     - Bbox3 with \"vase\".\n\nThe diagram illustrates how pre-processing techniques like cropping and resizing assist in zero-shot recognition for object detection tasks using multimodal retrieval methods.\n3.3 Inference with    $\\mathbf{R}$  etrieving  A nd    $\\mathbf{R}$  anking \nAfter successfully constructing memory    $\\mathcal{M}$   by using our multimodal retriever, our next step is to integrate the memory with the retrieval process and use MLLMs to rank the retrieval results and enhance the performance in few-shot/zero- shot perception tasks. \nFor example, in the inference stage of the few-shot image classification task, we first use the visual encoder    $\\varPhi_{\\mathrm{img}}$   to process the input image and obtain the corresponding image embedding   e . The visual encoder is identical to the encoder used in our multi-modal retriever. The image embedding   e  is then navigated through the previously constructed memory index and ranked by similarity to identify the top-  $k$  elated images. Consequently, memory    $\\mathcal{M}$   yields th  names of the retrieved top- k  categories, denoted as    $\\{c_{1},c_{2},c_{3},...,c_{k}\\}$  . The top-k retrieved results serve as a preliminary filter, narrowing down the vast possibilities to those most likely relevant, based on historical data and the semantic closeness of stored labels to the image content. \nSince these cropped sub-images are usually small, CLIP’s ability to extract features from these low-resolution images is limited. Therefore, in the object de- tection task, we do not perform image-to-image retrieval but use CLIP’s inherent image-text interaction capabilities to conduct image-to-text retrieval. Finally, we also obtain the top-  $k$   category information with the highest similarity. \nFollowing the retrieval phase, the retrieved category labels alongside image embedding e  are integrated and sent to the MLLMs through our ranking prompt. The MLLMs, combining the internal knowledge and the retrieved information, make the final prediction of the image category. Our proposed inference process, using both the retrieval results from our memory bank and subsequent ranking by the MLLM, ensures a more accurate and con textually aware classification prediction. Our design represents a significant advancement in few-shot image classification, enabling our system to handle a wide variety of images and cate- gories with high precision and flexibility. \nRanking Prompt Format.  Fig.  4  presents our ranking prompt format. The process begins with the prompt  ‘Sort the optional categories: [class a, "}
{"page": 7, "image_path": "doc_images/RAR_7.jpg", "ocr_text": "8 Ziyu Liu et al.\n\nRanking Prompt Example\n\nI =p | Retrieve\n\nMercedes-Benz Mercedes-Benz Mercedes-Benz, Mercedes-Benz, 2010BMW Mercedes-Benz\nE-Class Sedan S-Class Sedan % C-Class Sedan % E-Class Sedan “@ MS Sedan ® SL-Class Coupe\n\na Sorted these categories:[ Mercedes-Benz S-Class Sedan, Mercedes-Benz C-Class Sedan, Mercedes-Benz E-Class\nSedan, 2010 BMW MS Sedan, Mercedes-Benz SL-Class Coupe ]\n\nEh\n\nTop-k for high to low: [ Mercedes-Benz E-Class Sedan, Mercedes-Benz S-Class Sedan,\nMercedes-Benz C-Class Sedan, Mercedes-Benz SL-Class Coupe, 2010 BMW MS Sedan ] J\n\nFig. 4: Ranking Prompt examples for few-shot image classification. The fine-\ngrained image examples are from Stanford Cars [20]. We incorporate the initial top-k\nretrieved results (e.g., k = 5) into our ranking prompts and use the MLLMs to rank\nthe retrieved results and make the final prediction.\n\nclass b, class c, class d, class e]’, which is dynamically generated to\ninclude the top-k class names retrieved from our multimodal retriever. Our\nmethod uses the MLLM’s ability to rank these retrieved class names. Unlike\ntraditional approaches that might rely solely on the initial retrieval order, our\nMLLM employs advanced linguistic and semantic analysis to assess the contex-\ntual appropriateness of each class name with the input image.\n\nFine-tuning for Ranking. When directly applying MLLMs to ranking the\nretrieved results, MLLMs may predict some errors such as beyond the given list\nor occasional misalignment. To fully exploit the ranking potential of MLLMs for\ndownstream tasks, while avoiding the consumption of extensive computational\nresources for training MLLMs, we selected a small-scale classification dataset to\nfine-tune the MLLMs. The primary goal of fine-tuning was to enable MLLMs\nto improve their ranking ability such as following the format of prompts and\nreturning results as required.\n\nTo create our fine-tuning data, we use the CLIP image encoder Pimg to\nextract the embeddings of two disjoint subsets of images D, and Dp, both drawn\nfrom the FGVC-Aircraft dataset. We provide the ablation studies in Sec. 4.5\nabout using different datasets to construct the fine-tuning data. Our observation\nreveals that the MLLM demonstrates robustness to the choice of fine-tuning\ndatasets, with only marginal differences in performance outcomes.\n\nFor each image in D,, we apply the k-NN clustering algorithm to find the top\n20 most similar images in D, including their categories. Afterward, we select 16\nsets from these 20 images, each set comprising k images, and retain those groups\nthat contain images of the same category as Dy. We then shuffled the category\nlabels for these sets. Using the prompts shown in Fig. 4, we create a dataset\ncomprising roughly 30,000 entries, with the original sequence of categories serv-\ning as the ground-truth label. In summary, we build the fine-tuning data aiming\nto bolster the MLLM’s ranking performance.\n\nIn-Context Learning for Ranking. In-context learning presents a valuable\nalternative to fine-tuning with ranking examples, particularly due to its flexibility\nand lower requirement for specialized data preparation. While fine-tuning with\nranking examples has proven to be highly effective, it necessitates a substantial\n", "vlm_text": "The image is a visual example of a ranking prompt for few-shot image classification. It includes:\n\n1. An image labeled \"Mercedes-Benz E-Class Sedan\" on the left.\n2. A series of retrieved car images on the right, each labeled with their respective model names:\n   - Mercedes-Benz S-Class Sedan (incorrect)\n   - Mercedes-Benz E-Class Sedan (correct)\n   - 2010 BMW M5 Sedan (incorrect)\n   - Mercedes-Benz SL-Class Coupe (incorrect)\n3. An explanation showing the sorted categories based on their relevance.\n4. A top-k list (in descending order of likelihood for correct match) is shown, highlighting the Mercedes-Benz E-Class Sedan as the top result.\n\nThis illustrates the process of using few-shot learning and ranking prompts to classify images.\nclass b, class c, class d, class e]’ , which is dynamically generated to include the top-k class names retrieved from our multimodal retriever. Our method uses the MLLM’s ability to rank these retrieved class names. Unlike traditional approaches that might rely solely on the initial retrieval order, our MLLM employs advanced linguistic and semantic analysis to assess the contex- tual appropriateness of each class name with the input image. \nFine-tuning for Ranking.  When directly applying MLLMs to ranking the retrieved results, MLLMs may predict some errors such as beyond the given list or occasional misalignment. To fully exploit the ranking potential of MLLMs for downstream tasks, while avoiding the consumption of extensive computational resources for training MLLMs, we selected a small-scale classification dataset to fine-tune the MLLMs. The primary goal of fine-tuning was to enable MLLMs to improve their ranking ability such as following the format of prompts and returning results as required. \nTo create our fine-tuning data, we use the CLIP image encoder    $\\varPhi_{\\mathrm{img}}$   to extract the embeddings of two disjoint subsets of images    $\\mathcal{D}_{a}$   and    $\\mathcal{D}_{b}$  , both drawn from the FGVC-Aircraft dataset. We provide the ablation studies in Sec.  4.5 about using different datasets to construct the fine-tuning data. Our observation reveals that the MLLM demonstrates robustness to the choice of fine-tuning datasets, with only marginal differences in performance outcomes. \nFor each image in    $\\mathcal{D}_{b}$  , we apply the    $k$  -NN clustering algorithm to find the top 20 most similar images in    $\\mathcal{D}_{a}$   including their c tegories. Afterward, we select 16 sets from these 20 images, each set comprising  k  images, and retain those groups that contain images of the same category as    $\\mathcal{D}_{b}$  . We then shuffled the category labels for these sets. Using the prompts shown in Fig.  4 , we create a dataset comprising roughly 30,000 entries, with the original sequence of categories serv- ing as the ground-truth label. In summary, we build the fine-tuning data aiming to bolster the MLLM’s ranking performance. \nIn-Context Learning for Ranking.  In-context learning presents a valuable alternative to fine-tuning with ranking examples, particularly due to its flexibility and lower requirement for specialized data preparation. While fine-tuning with ranking examples has proven to be highly effective, it necessitates a substantial amount of curated data and computational resources for training. In contrast, in- context learning uses the model’s existing knowledge by providing it with specific examples directly within the input prompt, guiding the model to understand and execute the task of ranking without the need for explicit re-training. Here we elaborate on the application of in-context learning with MLLMs to rank the retrieved results. To effectively guide the MLLMs in comprehending the ranking task, we use the prompt format similar to Fig.  4  and integrate a specific ranking example into the prompts. Please refer to the Appendix  B  for our structured in-context learning prompt. Please refer to Sec.  4.5  for the ablation studies of discussing the difference between using fine-tuning or in-context learning for ranking. "}
{"page": 8, "image_path": "doc_images/RAR_8.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 9\n\namount of curated data and computational resources for training. In contrast, in-\ncontext learning uses the model’s existing knowledge by providing it with specific\nexamples directly within the input prompt, guiding the model to understand and\nexecute the task of ranking without the need for explicit re-training. Here we\nelaborate on the application of in-context learning with MLLMs to rank the\nretrieved results. To effectively guide the MLLMs in comprehending the ranking\ntask, we use the prompt format similar to Fig. 4 and integrate a specific ranking\nexample into the prompts. Please refer to the Appendix B for our structured\nin-context learning prompt. Please refer to Sec. 4.5 for the ablation studies of\ndiscussing the difference between using fine-tuning or in-context learning for\nranking.\n\n4 Experiments\n\nIn this section, we present our experiment step (Sec. 4.1) and conduct experi-\nments on different tasks such as fine-grained visual recognition (Sec. 4.2), few-\nshot image recognition (Sec. 4.3) and zero-shot object recognition (Sec. 4.4). We\nalso provide the ablation studies about our design choices (Sec. 4.5).\n\n4.1 Experimental Setup\n\nDatasets and Evaluation Metrics. We follow previous work [31] to choose\n5 datasets for fine-grained visual recognition (Bird-200 [47], Cars-196 [20],\nDog-120 [19], Flower-102 [37], and Pet-37 [39]) and report the clustering accuracy\n(cACC) and semantic similarity accuracy (sACC) as evaluation metrics.\n\nFor few-shot image recognition, we select 11 datasets including gen-\neral objects (ImageNet [7], Caltech101 [11]), textual (DTD [4]), scene objects\n(SUN397 [51]), satellite images (EuroSAT [15]), facial expressions (RAF-DB [25]),\ncar types (Stanford Cars [20]) and fine-grained datasets (FGVC-Aircraft [34],\nOxford Flowers [37], Food101 [37] and Oxford Pets [39]). We report the top-1\naccuracy (%) for all these classification datasets.\n\nAdditionally, we also select two benchmarks for our zero-shot object recog-\nnition setting: (1) The LVIS [14] dataset that encompasses over 164,000 images\nand 1,203 categories. We report the AP,, AP., APs, and AP metrics for rare,\ncommon, frequent, and all categories. (2) V3Det [48] dataset encompasses an\nimmense number of 13204 categories of real-world images. For V3Det, we report\nthe standard mAP metric of the object detection task.\n\nImplementation Details. We employ a frozen CLIP ViT B/16 model as the\nvisual encoder img to encode the input images and extract the corresponding\nimage embeddings. For the retrieval process, we search the stored embeddings in\nmemory M using the HNSW algorithm [35]. We use k = 5 for the top-k results,\nwith a solo exception k = 4 in the 4-shot few-shot setting. To improve the ranking\nability of MLLMs, we prepare 30k fine-tuning data from the FGVC-Aircraft\ndataset. In the fine-tuning process, we train the model with one epoch with\na learning rate of le~° on our fine-tuning data and subsequently evaluate the\n\n", "vlm_text": "\n4 Experiments \nIn this section, we present our experiment step (Sec.  4.1 ) and conduct experi- ments on different tasks such as fine-grained visual recognition (Sec.  4.2 ), few- shot image recognition (Sec.  4.3 ) and zero-shot object recognition (Sec.  4.4 ). We also provide the ablation studies about our design choices (Sec.  4.5 ). \n4.1 Experimental Setup \nDatasets and Evaluation Metrics.  We follow previous work [ 31 ] to choose 5 datasets for  fine-grained visual recognition  (Bird-200 [ 47 ], Cars-196 [ 20 ], Dog-120 [ 19 ], Flower-102 [ 37 ], and Pet-37 [ 39 ]) and report the clustering accuracy (cACC) and semantic similarity accuracy (sACC) as evaluation metrics. \nFor  few-shot image recognition , we select 11 datasets including gen- eral objects (ImageNet [ 7 ], Caltech101 [ 11 ]), textual (DTD [ 4 ]), scene objects (SUN397 [ 51 ]), satellite images (EuroSAT [ 15 ]), facial expressions (RAF-DB [ 25 ]), car types (Stanford Cars [ 20 ]) and fine-grained datasets (FGVC-Aircraft [ 34 ], Oxford Flowers [ 37 ], Food101 [ 37 ] and Oxford Pets [ 39 ]). We report the top-1 accuracy (  $\\%$  ) for all these classification datasets. \nAdditionally, we also select two benchmarks for our  zero-shot object recog- nition  setting: (1) The LVIS [ 14 ] dataset that encompasses over 164,000 images and 1,203 categories. We report the AP  $\\mathbf{r}$  ,   $\\mathrm{AP_{c}}$  , AP  $\\mathrm{f}$  , and   $\\mathrm{AP_{all}}$   metrics for rare, common, frequent, and all categories. (2) V3Det [ 48 ] dataset encompasses an immense number of 13204 categories of real-world images. For V3Det, we report the standard mAP metric of the object detection task. \nImplementation Details.  We employ a frozen CLIP ViT B/16 model as the visual encoder    $\\varPhi_{\\mathrm{img}}$   to encode the input images and extract the corresponding image embeddings. For the retrieval process, we search the stored embeddings in memory    $\\mathcal{M}$   using the W algorithm [ 35 ]. We use    $k=5$   for the top-  $k$   results, with a solo exception  k  = 4  in the  4 -shot few-shot setting. To improve the ranking ability of MLLMs, we prepare  30 k fine-tuning data from the FGVC-Aircraft dataset. In the fine-tuning process, we train the model with one epoch with a learning rate of    $1e^{-5}$    on our fine-tuning data and subsequently evaluate the "}
{"page": 9, "image_path": "doc_images/RAR_9.jpg", "ocr_text": "10 Ziyu Liu et al.\n\nTable 1: Fine-grained visual recognition across 5 datasets. We follow [31] to report the\naveraged clustering accuracy (cACC, %) and semantic similarity accuracy (sACC, %)\nresults over 10 runs. The best and second-best results are colored Green and Red ,\nrespectively.\n\nBird-200 Car-196 Dog-120 Flower-102 Pet-37 Average\ncACC sACC cACC sACC cACC’sACC cACC sACC cACC sACC |cACC sACC\nWordNet+CLIP_ 39.3 57.7 183 33.3 53.9 70.6 42.1 49.8 5.4 61.9 41.8 54.7\nBLIP-2 30.9 56.8 43.1 57.9 39.0 58.6 619 59.1 60.5 47.2 58.6\nCaSED 25.6 50.1 269 414 380 55.9 67.2 52.3 60.9 63.6 43.7 52.6\nFineR 51.1 69.5 49.2 63.5 48.1 64.9 63.8 72.9 72.4 57.0, 64.3\nRAR (Ours) | 51.6 69.5 53.2 63.6 50.0 65.2 63.7 TAL 74.8 58.5, 65.3\n\nperformance across additional datasets. We present the ablation studies about\nthe hyper-parameters such as the value of k and the fine-tuning data source in\nthe Sec. 4.5.\n\n4.2 Fine-Grained Visual Recognition\n\nWe first evaluate our RAR on the fine-grained visual recognition setting defined\nin previous work [31]. We use only 3 unlabelled images per category to build our\nmemory MM for retrieving. Please refer to Appendix C for more implementation\ndetails.\n\nBaselines. We follow [31] to select four representative methods as our baselines\nto compare with: WordNet [36]4+CLIP, BLIP-2 [22], CaSED [5], and FineR [31].\nAveraged Results over 5 Datasets. Tab. 1 summarizes the results and our\nRAR achieves the top performance on both the cACC (58.5%) and sACC (65.3%)\nmetrics. The WordNet+CLIP and CaSED baselines rely solely on CLIP for class\nname retrieval, yet often yield inaccurate predictions. In contrast, our method\nadds the additional ranking process with MLLMs, which increases the likelihood\nof correctly predicting those accurate yet initially lower-ranked candidates and\nthereby boosting the performance. Besides, FineR uses MLLM (e.g., BLIP-2) for\nfine-grained recognition via multi-round questioning-answering processes, which\nmay demand more computational resources and struggle to scale efficiently with\nlarge vocabulary datasets. Conversely, our approach first retrieves candidates\nand then lets MLLMs make predictions on the candidates, optimizing both ac-\ncuracy and efficiency.\n\n4.3. Few-Shot Image Recognition\n\nThe few-shot setting aims to enable a model to recognize new objects with only\na few examples for each new category. Few-shot learning faces substantial chal-\nlenges when applied to fine-grained datasets, which consist of numerous highly\nsimilar classes yet are accompanied by only a minimal amount of training data.\nBaselines. For few-shot image recognition, we introduce two baselines including\nCLIP and MLLMs. The first is the CLIP [41] model combined with k-NN to\n", "vlm_text": "This table compares the performance of different models on various datasets using two metrics: cACC and sACC. The datasets are Bird-200, Car-196, Dog-120, Flower-102, and Pet-37. An average score is also provided.\n\nThe models compared are:\n- WordNet+CLIP\n- BLIP-2\n- CaSED\n- FineR\n- RAR (Ours)\n\nEach cell contains a performance score, with higher scores generally highlighted in green. The RAR model appears to have competitive scores, showing improvement in several categories.\nperformance across additional datasets. We present the ablation studies about the hyper-parameters such as the value of    $k$   and the fine-tuning data source in the Sec.  4.5 . \n4.2 Fine-Grained Visual Recognition \nWe first evaluate our  RAR  on the  fine-grained visual recognition  setting defined in previous work [ 31 ]. We use only 3 unlabelled images per category to build our memory    $\\mathcal{M}$   for retrieving. Please refer to Appendix  C  for more implementation details. \nBaselines.  We follow [ 31 ] to select four representative methods as our baselines to compare with: WordNet [ 36  $^+$  CLIP, BLIP-2 [ 22 ], CaSED [ 5 ], and FineR [ 31 ]. Averaged Results over 5 Datasets.  Tab.  1  summarizes the results and our RAR  achieves the top performance on both the cACC   $(58.5\\%)$  ) and sACC   $(65.3\\%$  ) metrics. The WordNet+CLIP and CaSED baselines rely solely on CLIP for class name retrieval, yet often yield inaccurate predictions. In contrast, our method adds the additional ranking process with MLLMs, which increases the likelihood of correctly predicting those accurate yet initially lower-ranked candidates and thereby boosting the performance. Besides, FineR uses MLLM ( e.g ., BLIP-2) for fine-grained recognition via multi-round questioning-answering processes, which may demand more computational resources and struggle to scale efficiently with large vocabulary datasets. Conversely, our approach first retrieves candidates and then lets MLLMs make predictions on the candidates, optimizing both ac- curacy and efficiency. \n4.3 Few-Shot Image Recognition \nThe few-shot setting aims to enable a model to recognize new objects with only a few examples for each new category. Few-shot learning faces substantial chal- lenges when applied to fine-grained datasets, which consist of numerous highly similar classes yet are accompanied by only a minimal amount of training data. Baselines.  For  few-shot image recognition , we introduce two baselines including CLIP and MLLMs. The first is the CLIP [ 41 ] model combined with    $k$  -NN to "}
{"page": 10, "image_path": "doc_images/RAR_10.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 11\n\nTable 2: Few-shot image classification across 11 datasets. We report the top-1 accuracy\n(%) under the 4-shot and 8-shot settings. Here our .\\R uses the LLaVA1.5 [28] as the\nMLLM to rank the retrieved results. The symbol ‘-’ denotes to the LLaVA model fails\nto make the predictions due to the limited window size.\n\nMethod | Common Fine-Grained\n2\n\nS S = =) S 2\n\n® 2 « £F $ oa & 2 = =F &]| 3\n\n8 “3 < D> 3 q o B=} & 8 K >\n\n4 iS} Ey B a a i & GB & ie) <\n4-shot\nCLIP+KNN 421 87.9 142 514 676 475 646 84.5 49.2 626 55.6 | 57.0\nLLaVAL.5 Finetuning] - = 88.4 24.9 - 482 466 589 132 - 664 289] -\nRAR (LLaVA1.5) 51.0 92.1 27.7 58.8 74.8 53.9 69.6 804 54.4 71.4 60.9 | 63.2\nA +9.9 44.2 413.5 +74 +7.2 +6.4 45.0 -4.1 45.2 +8.8 +5.3| +6.2\n8-shot\nCLIP+KNN 47.6 90.6 28.2 568 728 532 683 89.5 56.1 683 618 | 63.0\nLLaVAL.5 Finetuning] - = 92.1 24.9 - 482 547 665 301 - 725 461] -\nRAR (LLaVA1.5) 56.5 93.5 46.9 63.4 81.5 59.3 74.3 87.3 61.2 76.6 67.7 | 69.8\nA +8.9 +2.9 +18.7 46.6 +8.7 +6.1 46.0 -2.2 45.1 +8.3 +5.9|+6.8\n\nretrieve predictions based on few-shot examples. The second is the LLaVA model\ndirectly fine-tuning with LoRA [16] on few-shot examples.\n\nAveraged Results on 11 Datasets. Tab. 2 summarizes the few-shot results\non 11 datasets, including 4 fine-grained datasets. Compared to the CLIP initial\nretrieval results (top row), our 2. \\R (third row) with ranking facilitates a notable\nincrease in classification accuracy. On average, our approach boosts the top-l\naccuracy from 57.0 to 63.2 (%) on the 4-shot setting, and from 63.0 to 69.8 (%) on\nthe 8-shot setting. Such improvements illustrate the ranking process of MLLMs\neffectively uses a nuanced understanding of context and detail to better align\npredictions with ground truth. Additionally, we observe that LLaVA1.5 + fine-\ntuning (second row) baseline underperforms in datasets with large vocabularies\nsuch as ImageNet due to the constraint of LLMs’ context window. Thanks to the\nretrieved candidates, our AR works for datasets with a vast of categories and\nis a potent tool in refining classification decisions, proving particularly useful in\nhandling the diverse and challenging landscape of image classification tasks.\n\n4.4 Zero-Shot Object Recognition\n\nGiven the pre-existing object proposals such as ground-truth box annotations,\nthe zero-shot object recognition task measures the model’s capability of aligning\nregions with textual class descriptions.\n\nBaselines. We select two representative papers CLIP [41] and RegionCLIP [57]\nand report their performances as the baseline results. Besides, we apply our\nmethod on a range of cutting-edge open-source MLLMs, including LLaVAL.5 [28],\nQWen-VL [2] and InternLM-XC2 [9].\n\nMain Results on LVIS. Tab. 3 presents the results that reveal notable im-\nprovements in all the metrics when applying our 2. Al. Specifically, when comb-\ning with the recent InternLM-XC2 [9] model, our approach yielded an 8.4 (%)\n\n", "vlm_text": "The table compares the performance of different methods across various datasets. There are two main methods compared: \"CLIP+KNN\" and \"RAR (LLaVA1.5)\" with \"LLaVA1.5 Finetuning\" as a reference. The datasets are divided into \"Common\" and \"Fine-Grained\" categories. \n\n### Categories and Datasets:\n- **Common**: ImageNet, Caltech101, RAF-DB, SUN397, EuroSAT, DTD, UCF-101\n- **Fine-Grained**: Flower102, StanfordCars, Food101, OxfordPets\n\n### Rows:\n1. **4-shot**: Evaluates performance using four samples.\n   - \"CLIP+KNN\" and \"RAR (LLaVA1.5)\" performances are compared.\n   - The \"Δ\" row shows the difference or improvement of \"RAR (LLaVA1.5)\" over the other methods.\n\n2. **8-shot**: Evaluates performance using eight samples.\n   - Similar structure as the 4-shot with comparisons and differences highlighted.\n\n### Highlights:\n- The numbers represent performance metrics (e.g., accuracy) for each method on each dataset.\n- Improvements or changes are highlighted in green and blue, showing how \"RAR (LLaVA1.5)\" performs relative to the others.\n- Average performances for each method across datasets are included.\n\nOverall, \"RAR (LLaVA1.5)\" shows generally better performance (highlighted in green) compared to \"CLIP+KNN\".\nretrieve predictions based on few-shot examples. The second is the LLaVA model directly fine-tuning with LoRA [ 16 ] on few-shot examples. \nAveraged Results on 11 Datasets.  Tab.  2  summarizes the few-shot results on 11 datasets, including 4 fine-grained datasets. Compared to the CLIP initial retrieval results (top row), our  RAR  (third row) with ranking facilitates a notable increase in classification accuracy. On average, our approach boosts the top-1 accuracy from 57.0 to 63.2 (  ${\\%}$  ) on the 4-shot setting, and from 63.0 to 69.8 (  $\\%$  ) on the 8-shot setting. Such improvements illustrate the ranking process of MLLMs effectively uses a nuanced understanding of context and detail to better align predictions with ground truth. Additionally, we observe that LLaVA1.5 + fine- tuning (second row) baseline under performs in datasets with large vocabularies such as ImageNet due to the constraint of LLMs’ context window. Thanks to the retrieved candidates, our  RAR  works for datasets with a vast of categories and is a potent tool in refining classification decisions, proving particularly useful in handling the diverse and challenging landscape of image classification tasks. \n4.4 Zero-Shot Object Recognition \nGiven the pre-existing object proposals such as ground-truth box annotations, the zero-shot object recognition task measures the model’s capability of aligning regions with textual class descriptions. \nBaselines.  We select two representative papers CLIP [ 41 ] and RegionCLIP [ 57 ] and report their performances as the baseline results. Besides, we apply our method on a range of cutting-edge open-source MLLMs, including LLaVA1.5 [ 28 ], QWen-VL [ 2 ] and InternLM-XC2 [ 9 ]. \nMain Results on LVIS.  Tab.  3  presents the results that reveal notable im- provements in all the metrics when applying our  RAR . Specifically, when comb- ing with the recent InternLM-XC2 [ 9 ] model, our approach yielded an 8.4 (  $\\%$  ) "}
{"page": 11, "image_path": "doc_images/RAR_11.jpg", "ocr_text": "12 Ziyu Liu et al.\n\nTable 3: Zero-shot object recognition on Table 4: Zero-shot object recognition on\n\nLVIS [14] v1.0 validation set. V3Det [48] validation set with 13,204 cat-\nJAP, AP. AP; APan egories.\n\nCLIP w/ box 40.6 59.2 48.7 TAP, AP AP; APan\nCLIP w/ mask 40.8 59.6 49.2 - Tr ;\neenig 408 53.5 [5061 192 CLIP w/ box 72. 129 1228 98\nSAR (LLaV) RAR (LLaVAL.5) Ez 13.2 13.9 111\nRAR (LLaVALS 58.7 57.9 54.4 56.2\nA «ha ) 8.6 P78 42.7 15.5 A +2.7 +03 +11 +1.3\ni eos es RAR (Qwen-VL) 96 12.7 137 108\nRAR (Qwen-VL) 59.6 57.5 53.7 56.4 1\nA 49.5 +7.4 42.0 45.7 A Sa 0.2 +09 +1.0\n\n60.2 58.0 54.3 B71 RAR (InternLM-XC2)} 10.1 13.1 14.5 11.3\n\nRAR (InternLM-XC2)\nA 410.1 +7.9 42.6 16.4 A +2.9 40.2 41.7 $1.5\n\npoint increase over the CLIP baseline and a 6.4 (%) enhancement relative to Re-\ngionCLIP [57]. These advancements underscore the efficacy of using an external\nmemory for retrieval assistance coupled with the ranking prowess of MLLMs.\nComparison with Rare Classes Results (AP,). We find an interesting ob-\nservation from the experimental results presented in Tab. 3. For the CLIP model,\nwe observe a progressive increase in performance from AP, through AP, to APr,\nwhich indicates a gradation in precision across varying class frequencies. How-\never, employing our method yields a different trend, where the peak performance\nis achieved on AP,, surpassing the CLIP model by as much as 19.6 percentage\npoints. This significant leap in performance suggests a substantial advantage\nof our method when it comes to rare categories. The integration of our RAR\nto MLLMs plays a pivotal role here, as it demonstrates a heightened ability\nto discriminate among the rare classes. Our observation could be attributed to\nthe fact that our retrieving and reranking mechanism effectively pools relevant\ninformation from the external memory, providing the MLLMs with a richer con-\ntext for rare class identification. Moreover, the ranking capability of MLLMs\nensures that even the lesser-represented classes receive adequate attention dur-\ning the classification process. Our 1.\\lt achieves a robust enhancement in the\nmodel’s ability to discern and accurately classify objects that are infrequently en-\ncountered, addressing one of the significant challenges in long-tailed distribution\ndatasets.\n\nMain Results on V3Det. To further test the effectiveness of using MLLMs for\nranking in scenarios with an extremely large number of fine-grained categories,\nwe conducted additional experiments on V3Det [49]. The experimental results in\nTab. 4 reveal that our 2 AR has achieved a commendable improvement in perfor-\nmance, surpassing the CLIP baseline by 1.5 percentage points in overall average\nprecision (AP,) with InternLM-XC2. Such an improvement is particularly sig-\nnificant given the complexity of the V3Det dataset, which presents a challenging\narray of 13,204 distinct classes. The MLLMs, with the aid of our retrieving and\nranking mechanisms, have once again demonstrated their robust performance in\nthe domain of object detection datasets. Using our retrieval-augmented approach\nallows MLLMs to navigate the extensive and fine-grained category landscape of\nV3Det effectively.\nQualitative Results. Fig. 5 presents the visualization results about ranking\n\n", "vlm_text": "The table compares performance metrics (AP_r, AP_c, AP_f, AP_all) for different models and configurations. Here's a breakdown:\n\n- **Models/Configurations:**\n  - CLIP with box\n  - CLIP with mask\n  - RegionCLIP\n  - RAR (LLaVA1.5)\n  - RAR (Qwen-VL)\n  - RAR (InternLM-XC2)\n\n- **Metrics:**\n  - **AP_r**: Average Precision for region\n  - **AP_c**: Average Precision for classification\n  - **AP_f**: Average Precision for function\n  - **AP_all**: Overall Average Precision\n\n- **Key Observations:**\n  - The RAR models show improvements (indicated by the delta symbol and green highlights) over the baseline models (CLIP variants).\n  - RAR (InternLM-XC2) displays the highest improvements across most metrics.\n\nThe delta values indicate improvements compared to the RegionCLIP baseline.\nThe table presents performance metrics for various models in object detection or recognition, possibly using the CLIP model with bounding boxes. Here's a breakdown:\n\n- **Columns**:\n  - **APs, APm, AP1, APall**: These are likely Average Precision metrics, indicating performance at different scales or conditions (small, medium, certain threshold, and overall).\n\n- **Rows**:\n  - **CLIP w/ box**: Baseline model with performance scores of 7.2, 12.9, 12.8, and 9.8 respectively.\n  - **RAR (LLaVA1.5)**: Achieves scores of 9.9, 13.2, 13.9, and 11.1, with improvements of +2.7, +0.3, +1.1, and +1.3.\n  - **RAR (Qwen-VL)**: Achieves scores of 9.6, 12.7, 13.7, and 10.8, with improvements of +2.4, -0.2, +0.9, and +1.0.\n  - **RAR (InternLM-XC2)**: Achieves scores of 10.1, 13.1, 14.5, and 11.3, with improvements of +2.9, +0.2, +1.7, and +1.5.\n\nThe numbers in green highlight improvements over the baseline, while red numbers likely indicate no improvement or a decrease.\npoint increase over the CLIP baseline and a 6.4   $(\\%)$  ) enhancement relative to Re- gionCLIP [ 57 ]. These advancements underscore the efficacy of using an external memory for retrieval assistance coupled with the ranking prowess of MLLMs. \nComparison with Rare Classes Results   $\\left(\\mathbf{A}\\mathbf{P_{r}}\\right)$  .  We find an interesting ob- servation from the experimental results presented in Tab.  3 . For the CLIP model, we observe a progressive increase in performance from   $\\mathrm{AP_{r}}$   through AP  $\\mathrm{c}$   to AP  $^\\dag$  , which indicates a gradation in precision across varying class frequencies. How- ever, employing our method yields a different trend, where  the peak performance is achieved on   $A P_{r}$  , surpassing the CLIP model by as much as  19 . 6  percentage points. This significant leap in performance suggests a substantial advantage of our method when it comes to rare categories. The integration of our  RAR to MLLMs plays a pivotal role here, as it demonstrates a heightened ability to discriminate among the rare classes. Our observation could be attributed to the fact that our retrieving and reranking mechanism effectively pools relevant information from the external memory, providing the MLLMs with a richer con- text for rare class identification. Moreover, the ranking capability of MLLMs ensures that even the lesser-represented classes receive adequate attention dur- ing the classification process. Our  RAR  achieves a robust enhancement in the model’s ability to discern and accurately classify objects that are infrequently en- countered, addressing one of the significant challenges in long-tailed distribution datasets. \nMain Results on V3Det.  To further test the effectiveness of using MLLMs for ranking in scenarios with an extremely large number of fine-grained categories, we conducted additional experiments on V3Det [ 49 ]. The experimental results in Tab.  4  reveal that our  RAR  has achieved a commendable improvement in perfor- mance, surpassing the CLIP baseline by 1.5 percentage points in overall average precision (  $.A P_{a l l.}$  ) with InternLM-XC2. Such an improvement is particularly sig- nificant given the complexity of the V3Det dataset, which presents a challenging array of 13,204 distinct classes. The MLLMs, with the aid of our retrieving and ranking mechanisms, have once again demonstrated their robust performance in the domain of object detection datasets. Using our retrieval-augmented approach allows MLLMs to navigate the extensive and fine-grained category landscape of V3Det effectively. \nQualitative Results.  Fig.  5  presents the visualization results about ranking "}
{"page": 12, "image_path": "doc_images/RAR_12.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 13\n\nObjects Retrieved Reranked\n\nearring\n\nglove\n\npolo_shirt\n\nshort_pants\n\nFig. 5: Visualization of the ranking examples for zero-shot object recognition on\nLVIS [14] validation set. Given the top retrieved predictions, our }.\\ uses MLLMs to\nselect the correct class names accurately.\n\nexamples of our approach on LVIS validation set. The CLIP&K-NN approach\nprovides an extensive list of object predictions, albeit with the caveat that the\nmost accurate label might not always emerge as the top-1 choice. The incor-\nporation of MLLMs in our RAR significantly streamlines the prediction pro-\ncess, yielding more precise and relevant object labels. The visualization results\ndemonstrate that our 2 AR meets the need for fine-grained and large vocabulary\nrecognition.\n\n4.5 Ablation Experiments\n\nEffects of the parameter k. We delve into the impact of the hyper-parameter\nk on few-shot image recognition setting, as detailed in Tab. 5. We report the\nresults of AR with the LLaVA1.5 as the MLLM. Our findings reveal tha\nour {.\\} demonstrates a remarkable robustness to variations in k, with only\nminor differences observed across a broad spectrum of values from 3 to 7. Such a\nconsistency suggests that 2 ARs ability to generalize from a few examples is no\nsignificantly influenced by the choice of k. Consequently, based on the average\nresults, we select k = 5 as the default choice.\nDifferent Fine-tuning data. We study the importance of using different fine-\ntuning datasets for ranking. We select two representative datasets: FGVC-Aircraft:\nand Stanford-Cars as the data sources for constructing the fine-tuning data. Our\nselection is motivated by their diverse characteristics and relevance in visual\nrecognition tasks, providing a comprehensive basis for fine-tuning. Subsequently,\nwe fine-tune the 2. \\R with different MLLMs (QWen-VL and InternLM-XC2) on\nthese two datasets, aiming to investigate how different data sources influence per-\nformance. To thoroughly assess the impact of using different fine-tuning datasets,\nwe evaluate the fine-tuned 1t.\\ 1 across a diverse set of 10 additional datasets.\nTab. 6 presents the results. We observe that 1 A) is not sensitive to changes\nin the fine-tuning dataset for ranking, thereby confirming its viability as a gen-\neralizable and reliable method for enhancing the performance of MLLMs. The\nconsistency in results, irrespective of the fine-tuning data source, underlines the\nrobustness of our fine-tuning strategy. Despite these minor variations, the over-\nall performance of using FGVC-Aircrafts (82.7%, top row) is higher than using\nStanfordCars (82.0%, second row) for QWen-VL, and we observe the same trend\n\n", "vlm_text": "The image is a table showcasing the process of reranking class names for zero-shot object recognition. It includes three columns:\n\n1. **Objects**: Displays images with highlighted objects.\n2. **Retrieved**: Lists the initially retrieved class names for each highlighted object.\n3. **Reranked**: Shows the correctly identified class names after reranking.\n\n- The first row highlights an object and retrieves multiple names, with \"earring\" being the correct class.\n- The second row identifies a \"glove\".\n- The third row initially includes \"polo_shirt\" as a correct retrieval for the object.\n- The fourth row correctly reranks \"short_pants\".\n\nThe table demonstrates how MLLMs (multi-label learning models) are used for accurate label selection.\nexamples of our approach on LVIS  validation  set. The CLIP &  $K$  -NN approach provides an extensive list of object predictions, albeit with the caveat that the most accurate label might not always emerge as the top-1 choice. The incor- poration of MLLMs in our  RAR  significantly streamlines the prediction pro- cess, yielding more precise and relevant object labels. The visualization results demonstrate that our  RAR  meets the need for fine-grained and large vocabulary recognition. \n4.5 Ablation Experiments \nEffects of the parameter    $k$  .  We delve into the impact of the hyper-parameter  $k$   on few-shot image recognition setting, as detailed in Tab.  5 . We report the results of  RAR  with the LLaVA1.5 as the MLLM. Our findings reveal that our  RAR  demonstrates a remarkable robustness to variations in    $k$  , with only minor differences observed across a broad spectrum of values from  3  to  7 . Such a consistency suggests that  RAR ’s ability to generalize from a few examples is not significantly influenced by the choice of    $k$  . Consequently, based on the averaged results, we select    $k=5$   as the default choice. \nDifferent Fine-tuning data.  We study the importance of using different fine- tuning datasets for ranking. We select two representative datasets: FGVC-Aircraft and Stanford-Cars as the data sources for constructing the fine-tuning data. Our selection is motivated by their diverse characteristics and relevance in visual recognition tasks, providing a comprehensive basis for fine-tuning. Subsequently, we fine-tune the  RAR  with different MLLMs (QWen-VL and InternLM-XC2) on these two datasets, aiming to investigate how different data sources influence per- formance. To thoroughly assess the impact of using different fine-tuning datasets, we evaluate the fine-tuned  RAR  across a diverse set of 10 additional datasets. \nTab.  6  presents the results. We observe that  RAR  is not sensitive to changes in the fine-tuning dataset for ranking, thereby confirming its viability as a gen- eralizable and reliable method for enhancing the performance of MLLMs. The consistency in results, irrespective of the fine-tuning data source, underlines the robustness of our fine-tuning strategy. Despite these minor variations, the over- all performance of using FGVC-Aircrafts (82.7 % , top row) is higher than using Stanford Cars (82.0 % , second row) for QWen-VL, and we observe the same trend "}
{"page": 13, "image_path": "doc_images/RAR_13.jpg", "ocr_text": "14 Ziyu Liu et al.\n\n|k=3 k=4 k=5 k=6 k=7\nDTD 70.27 71.34 71.93 71.93 71.99 Table 5: Ablation studies\n\nFlowersl02 | 96.18 95.57 95.62 95.66 95.57 about the selection of the hyper-\nOxford-pets| 80.21 80.38 79.91 79.72 79.42 parameter k.\nEurosat 92.38 92.48 98.28 92.52 92.59\n\nAverage | 84.76 84.96 85.19 84.96 84.90\n\nTable 6: Ablation studies about (1) using different datasets for fine-tuning and (2)\nfine-tuning vs. in-context learning. The symbols ‘F’ and ‘S’ stand for fine-tuning on\nthe FGVC-Aircraft or Stanford-Cars datasets.\n\nMethod ] Strategy ] Common Fine-Grained |\na a z\n\n3 Ss is} Ss a < g\n\n2 2 8 Z 2 2 8 &| ®\n\n® 2 @ 2 8 8 ¢ ¢ &) 8\n\nA a 464 5 & So 3 8 Ed 5\n\nFine-tune _In-Context | 4 iS] a a a = & 4 fe} <\nban F x 75.8 95.5 66.0 72.7 90.7 72.5 81.4 97.5 88.1 87.2 | 82.7\ns x 75.3 949 65.1 73.1 881 710 811 95.8 88.3 87.0 | 82.0\n(QWen-VL) x v 72.0 63.6 65.6 86.2 668 76.5 95.6 84.7 82.3 | 78.7\n: F x 71.5 944 72.7 69.7 917 69.9 77.6 93.2 83.9 79.3 | 80.4\nnan s x 71.5 94.7 712 69.7 90.3 69.9 77.5 92.0 83.6 79.7 | 80.0\n(InternLM-XC2) | X v 69.2 94.1 66.0 69.7 91.8 689 66.1 95.7 85.7 79.2 | 78.6\n\nfor InternLM-XC2. Based on our findings, we adopt the FGVC-Aircraft dataset\nas our preferred choice for fine-tuning.\n\nFine-tuning vs. In-Context Learning. We validate the effectiveness of fine-\ntuning the MLLM or just in-context learning (training-free) for ranking. The\nresults are illustrated in Tab. 6. We select two distinct groups for comparison.\nThe first group (top and fourth rows) involves models that are fine-tuned using\nthe FGVC-Aircraft dataset, while the second group (third and bottom rows)\nconsists of models with in-context learning prompts for ranking. The results show\na consistent improvement in accuracy for the fine-tuned model across almost all\ndatasets for both QWen-VL and InternLM-XC2. The notable enhancement in\nperformance across a diverse range of datasets highlights the efficacy of our\nfine-tuning strategy. The results substantiate that fine-tuning the MLLM with\ntarget datasets like FGVC-Aircraft significantly bolsters the model’s ranking\ncapabilities.\n\n5 Conclusion\n\nIn this paper, we highlight the potential of combining retrieving and ranking\nwith multi-modal large language models to revolutionize perception tasks such as\nfine-grained recognition, zero-shot image recognition, and few-shot object recog-\nnition. Motivated by the limited zero-shot/few-shot of CLIP and MLLMs on\nfine-grained datasets, our AR designs the pipeline that uses MLLM to rank\nthe retrieved results. Our proposed approach can be seamlessly integrated into\nvarious MLLMs for real-world applications where the variety and volume of cat-\negories continuously expand. Our method opens up new avenues for research in\naugmenting the MLLM’s abilities with the retrieving-augmented solution and\n", "vlm_text": "This table presents a comparison of values across different datasets (DTD, Flowers102, Oxford-pets, Eurosat) and parameters labeled \\(k = 3\\) to \\(k = 7\\). The table shows specific numerical results for each combination, with averages calculated for each parameter \\(k\\).\n\nHighlighted in green are the highest values within each row and column, indicating the best performance or result for that particular dataset or parameter value. Highlights in red typically indicate the lowest values, but they seem absent here, suggesting a focus on top results.\n\nThe 'Average' row provides the mean of values for each \\(k\\), helping identify overall performance trends across datasets.\nThe table presents performance metrics for different methods evaluated on several datasets. Here's a breakdown:\n\n- **Columns:**\n  - **Method**: Lists the methods used, e.g., RAR with specific models (QWen-VL, InternLM-XC2).\n  - **Strategy**: Indicates whether fine-tuning (F) or in-context learning (S) strategies were applied.\n  - **Common and Fine-Grained**: Categories of datasets used for evaluation, which include:\n    - Common: ImageNet, Caltech101, RAF-DB, SUN397, EuroSAT, DTD, UCF101\n    - Fine-Grained: Flower102, Food101, OxfordPets\n  - **Average**: Represents the average performance across all datasets.\n\n- **Values**: The cells contain numeric performance values, where green-highlighted numbers indicate the highest performance for that specific dataset and strategy combination.\nfor InternLM-XC2. Based on our findings, we adopt the FGVC-Aircraft dataset as our preferred choice for fine-tuning. \nFine-tuning  vs . In-Context Learning.  We validate the effectiveness of fine- tuning the MLLM or just in-context learning (training-free) for ranking. The results are illustrated in Tab.  6 . We select two distinct groups for comparison. The first group (top and fourth rows) involves models that are fine-tuned using the FGVC-Aircraft dataset, while the second group (third and bottom rows) consists of models with in-context learning prompts for ranking. The results show a consistent improvement in accuracy for the fine-tuned model across almost all datasets for both QWen-VL and InternLM-XC2. The notable enhancement in performance across a diverse range of datasets highlights the efficacy of our fine-tuning strategy. The results substantiate that fine-tuning the MLLM with target datasets like FGVC-Aircraft significantly bolsters the model’s ranking capabilities. \n5 Conclusion \nIn this paper, we highlight the potential of combining retrieving and ranking with multi-modal large language models to revolutionize perception tasks such as fine-grained recognition, zero-shot image recognition, and few-shot object recog- nition. Motivated by the limited zero-shot/few-shot of CLIP and MLLMs on fine-grained datasets, our  RAR  designs the pipeline that uses MLLM to rank the retrieved results. Our proposed approach can be seamlessly integrated into various MLLMs for real-world applications where the variety and volume of cat- egories continuously expand. Our method opens up new avenues for research in augmenting the MLLM’s abilities with the retrieving-augmented solution and could be beneficial for other tasks such as reasoning and generation in future works.\n\n "}
{"page": 14, "image_path": "doc_images/RAR_14.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 15\n\ncould be beneficial for other tasks such as reasoning and generation in future\nworks.\n\nReferences\n\nou\n\n. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K.,\n\nBitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, $., Koh, P.W., Ilharco, G.,\nWortsman, M., Schmidt, L.: Openflamingo: An open-source framework for training\nlarge autoregressive vision-language models. arXiv.org (2023) 2, 4\n\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.:\nQwen-VL: A frontier large vision-language model with versatile abilities. arXiv.org\n(2023) 2, 4, 11\n\nChen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.:\nSharegpt4v: Improving large multi-modal models with better captions. arXiv\npreprint arXiv:2311.12793 (2023) 2, 4\n\nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., Vedaldi, A.: Describing textures\nin the wild. In: CVPR (2014) 9, 19\n\nConti, A., Fini, E., Mancini, M., Rota, P., Wang, Y., Ricci, E.: Vocabulary-free\nimage classification. In: NeurIPS (2024) 10\n\nDai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,\nS.: Instructblip: Towards general-purpose vision-language models with instruction\ntuning (2023) 2, 4\nDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A large-scale\nhierarchical image database. In: CVPR (2009) 9, 19\n\nDong, X., Bao, J., Zheng, Y., Zhang, T., Chen, D., Yang, H., Zeng, M., Zhang, W.,\nYuan, L., Chen, D., Wen, F., Yu, N.: Maskclip: Masked self-distillation advances\ncontrastive language-image pretraining. In: Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR). pp. 10995-11005\n(June 2023) 1, 3\nDong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S.,\nDuan, H., Cao, M., et al.: InternLM-XComposer2: Mastering free-form text-image\ncomposition and comprehension in vision-language large model. arXiv preprint\narXiv:2401.16420 (2024) 11\nFang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X.,\nCao, Y.: EVA: Exploring the limits of masked visual representation learning at\nscale. In: CVPR (2023) 3\nFei-Fei, L., Fergus, R., Perona, P.: Learning generative visual models from few\ntraining examples: An incremental bayesian approach tested on 101 object cate-\ngories. In: CVPR workshop (2004) 9, 19\nGao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip-\nAdapter: Better vision-language models with feature adapters. IJCV (2023) 3\nGu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision\nand language knowledge distillation. In: ICLR (2022) 3\nGupta, A., Dollar, P., Girshick, R.: LVIS: A dataset for large vocabulary instance\nsegmentation. In: CVPR (2019) 7, 9, 12, 13, 26, 27, 28\nHelber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep\nlearning benchmark for land use and land cover classification. IEEE J. Sel. Top.\nAppl. Earth Obs. Remote Sens. (2019) 9, 19\n\n", "vlm_text": "\nReferences \n1. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P.W., Ilharco, G., Wortsman, M., Schmidt, L.: Open flamingo: An open-source framework for training large auto regressive vision-language models. arXiv.org (2023)  2 ,  4\n\n 2. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-VL: A frontier large vision-language model with versatile abilities. arXiv.org (2023)  2 ,  4 ,  11\n\n 3. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023)  2 ,  4\n\n 4. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., Vedaldi, A.: Describing textures in the wild. In: CVPR (2014)  9 ,  19\n\n 5. Conti, A., Fini, E., Mancini, M., Rota, P., Wang, Y., Ricci, E.: Vocabulary-free image classification. In: NeurIPS (2024)  10\n\n 6. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instruct blip: Towards general-purpose vision-language models with instruction tuning (2023)  2 ,  4\n\n 7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A large-scale hierarchical image database. In: CVPR (2009)  9 ,  19\n\n 8. Dong, X., Bao, J., Zheng, Y., Zhang, T., Chen, D., Yang, H., Zeng, M., Zhang, W., Yuan, L., Chen, D., Wen, F., Yu, N.: Maskclip: Masked self-distillation advances contrastive language-image pre training. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR). pp. 10995–11005 (June 2023)  1 ,  3\n\n 9. Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., et al.: InternLM-XComposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420 (2024)  11\n\n 10. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., Cao, Y.: EVA: Exploring the limits of masked visual representation learning at scale. In: CVPR (2023)  3\n\n 11. Fei-Fei, L., Fergus, R., Perona, P.: Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object cate- gories. In: CVPR workshop (2004)  9 ,  19\n\n 12. Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip- Adapter: Better vision-language models with feature adapters. IJCV (2023)  3\n\n 13. Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision and language knowledge distillation. In: ICLR (2022)  3\n\n 14. Gupta, A., Dollar, P., Girshick, R.: LVIS: A dataset for large vocabulary instance segmentation. In: CVPR (2019)  7 ,  9 ,  12 ,  13 ,  26 ,  27 ,  28\n\n 15. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. (2019)  9 ,  19 "}
{"page": 15, "image_path": "doc_images/RAR_15.jpg", "ocr_text": "20.\n\n21.\n\n22.\n\n23.\n\n24.\n\n26.\n\n27.\n\n28.\n\n29.\n\n30.\n\n31.\n\n32.\n\n33.\n\n34,\n\n36.\n\nZiyu Liu et al.\n\nHu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: LoRA: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021) 11\nIscen, A., Fathi, A., Schmid, C.: Improving image recognition by retrieving from\nweb-scale image-text data. In: CVPR (2023) 4\nJia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,\nLi, Z., Duerig, T.: Scaling up visual and vision-language representation learning\nwith noisy text supervision. In: ICML (2021) 3\nKhosla, A., Jayadevaprakash, N., Yao, B., Li, F.F.: Novel dataset for fine-grained\nimage categorization: Stanford dogs. In: CVPR workshop (2011) 9, 19\n\nKrause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine-\ngrained categorization. In: ICCV workshops (2013) 8, 9, 19\n\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kiittler,\nH., Lewis, M., Yih, W.t., Rocktaschel, T., et al.: Retrieval-Augmented generation\nfor knowledge-intensive nlp tasks. NeurIPS (2020) 4\n\nLi, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. In: ICML (2023)\n10\n\nLi, J., Li, D., Xiong, C., Hoi, S.: BLIP: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In: ICML (2022) 3\n\nLi, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L.,\nZhang, L., Hwang, J.N., et al.: Grounded language-image pre-training. In: CVPR\n(2022) 3\n\nLi, S., Deng, W., Du, J.: Reliable crowdsourcing and deep locality-preserving learn-\ning for expression recognition in the wild. In: CVPR (2017) 9, 19\n\nLi, Y., Fan, H., Hu, R., Feichtenhofer, C., He, K.: Scaling language-image pre-\ntraining via masking. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). pp. 23390-23400 (June 2023) 1, 3\nLiang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P.,\nMarculescu, D.: Open-vocabulary semantic segmentation with mask-adapted clip.\nIn: CVPR (2023) 3\n\nLiu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning.\narXiv preprint arXiv:2310.03744 (2023) 2, 11, 24\n\nLiu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2024)\n2,4\n\nLiu, H., Son, K., Yang, J., Liu, C., Gao, J., Lee, Y.J., Li, C.: Learning customize\nvisual models with retrieval-augmented knowledge. In: CVPR (2023) 4\n\nLiu, M., Roy, S., Li, W., Zhong, Z., Sebe, N., Ricci, E.: Democratizing fine-graine\nvisual recognition with large language models. In: ICLR (2024) 4, 9, 10, 22, 23\nLong, A., Yin, W., Ajanthan, T., Nguyen, V., Purkait, P., Garg, R., Blair, A.,\nShen, C., van den Hengel, A.: Retrieval augmented classification for long-tail visua’\nrecognition. In: CVPR (2022) 4\n\nLiiddecke, T., Ecker, A.: Image segmentation using text and image prompts. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR). pp. 7086-7096 (June 2022) 3, 6\n\nMaji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visua’\nclassification of aircraft. arXiv preprint arXiv:1306.5151 (2013) 9, 19\n\nMalkov, Y.A., Yashunin, D.A.: Efficient and robust approximate nearest neighbor\nsearch using hierarchical navigable small world graphs. TPAMI (2018) 6, 9\nMiller, G.A.: WordNet: a lexical database for english. Communications of the ACM\n(1995) 10\n\n", "vlm_text": "16. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)  11\n\n 17. Iscen, A., Fathi, A., Schmid, C.: Improving image recognition by retrieving from web-scale image-text data. In: CVPR (2023)  4\n\n 18. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: ICML (2021)  3\n\n 19. Khosla, A., Jaya deva prakash, N., Yao, B., Li, F.F.: Novel dataset for fine-grained image categorization: Stanford dogs. In: CVPR workshop (2011)  9 ,  19\n\n 20. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine- grained categorization. In: ICCV workshops (2013)  8 ,  9 ,  19\n\n 21. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rock t s chel, T., et al.: Retrieval-Augmented generation for knowledge-intensive nlp tasks. NeurIPS (2020)  4\n\n 22. Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Boots trapping language-image pre- training with frozen image encoders and large language models. In: ICML (2023) 10\n\n 23. Li, J., Li, D., Xiong, C., Hoi, S.: BLIP: Boots trapping language-image pre-training for unified vision-language understanding and generation. In: ICML (2022)  3\n\n 24. Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.N., et al.: Grounded language-image pre-training. In: CVPR (2022)  3\n\n 25. Li, S., Deng, W., Du, J.: Reliable crowd sourcing and deep locality-preserving learn- ing for expression recognition in the wild. In: CVPR (2017)  9 ,  19\n\n 26. Li, Y., Fan, H., Hu, R., Fei chten hofer, C., He, K.: Scaling language-image pre- training via masking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 23390–23400 (June 2023)  1 ,  3\n\n 27. Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P., Marculescu, D.: Open-vocabulary semantic segmentation with mask-adapted clip. In: CVPR (2023)  3\n\n 28. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023)  2 ,  11 ,  24\n\n 29. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2024) 2 ,  4\n\n 30. Liu, H., Son, K., Yang, J., Liu, C., Gao, J., Lee, Y.J., Li, C.: Learning customized visual models with retrieval-augmented knowledge. In: CVPR (2023)  4\n\n 31. Liu, M., Roy, S., Li, W., Zhong, Z., Sebe, N., Ricci, E.: Democratizing fine-grained visual recognition with large language models. In: ICLR (2024)  4 ,  9 ,  10 ,  22 ,  23\n\n 32. Long, A., Yin, W., Ajanthan, T., Nguyen, V., Purkait, P., Garg, R., Blair, A., Shen, C., van den Hengel, A.: Retrieval augmented classification for long-tail visual recognition. In: CVPR (2022)  4\n\n 33. Lüddecke, T., Ecker, A.: Image segmentation using text and image prompts. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR). pp. 7086–7096 (June 2022)  3 ,  6\n\n 34. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151 (2013)  9 ,  19\n\n 35. Malkov, Y.A., Yashunin, D.A.: Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. TPAMI (2018)  6 ,  9\n\n 36. Miller, G.A.: WordNet: a lexical database for english. Communications of the ACM (1995)  10 "}
{"page": 16, "image_path": "doc_images/RAR_16.jpg", "ocr_text": "37.\n\n38.\n\n39.\n\n40.\n\nAl.\n\n42.\n\n43.\n\n44.\n\n46.\n\n47.\n\n48.\n\n49.\n\nRAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 17\n\nNilsback, M.E., Zisserman, A.: Automated flower classification over a large number\nof classes. In: ICVGIP (2008) 9, 19\n\nOpenAI: GPT-4V(ision) system card (2023), https: //openai.com/research/\ngpt-4v-system-card 2, 4, 5, 25\n\nParkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: CVPR\n(2012) 9, 19\n\nPeng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2:\nGrounding multimodal large language models to the world. arXiv.org (2023) 2, 4\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models\nfrom natural language supervision. In: ICML (2021) 1, 3, 4, 10, 11, 26, 27\nShtedritski, A., Rupprecht, C., Vedaldi, A.: What does clip know about a red circle?\nvisual prompt engineering for vims. arXiv preprint arXiv:2304.06712 (2023) 3\nSoomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402 (2012) 19\nSubramanian, S., Merrill, W., Darrell, T., Gardner, M., Singh, S., Rohrbach, A.:\nReclip: A strong zero-shot baseline for referring expression comprehension. In: Pro-\nceedings of the 60th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers). pp. 5198-5215 (2022) 3\n\n. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training tech-\n\nniques for clip at scale. arXiv preprint arXiv:2303.15389 (2023) 1\n\nSun, Z., Fang, Y., Wu, T., Zhang, P., Zang, Y., Kong, S., Xiong, Y., Lin, D.,\nWang, J.: Alpha-CLIP: A clip model focusing on wherever you want. arXiv preprint\narXiv:2312.03818 (2023) 3\n\nWah, C., Branson, S., Welinder, P., Perona, P., Belongie, 8.: Caltech-ucsd birds-\n200-2011 (2011) 9, 19\n\nWang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., Lin,\nD.: V3Det: Vast vocabulary visual detection dataset. In: ICCV (2023) 3, 7, 9, 12,\n28\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,\nL., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual\nexpert for pretrained language models (2023) 2, 4, 12\n\n50. Wu, W., Yao, H., Zhang, M., Song, Y., Ouyang, W., Wang, J.: GPT4Vis: What can\n\ngpt-4 do for zero-shot visual recognition? arXiv preprint arXiv:2311.15732 (2023)\n25\n\n. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: SUN database: Large-\n\nscale scene recognition from abbey to zoo. In: CVPR (2010) 9, 19\n\n52. Xu, X., Xiong, T., Ding, Z., Tu, Z.: Masqclip for open-vocabulary universal im-\n\nage segmentation. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 887-898 (2023) 3\n\n. Yang, L., Wang, Y., Li, X., Wang, X., Yang, J.: Fine-grained visual prompting\n\n(2023) 3, 6\n\nYasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis,\nM., Zettlemoyer, L., Yih, W.t.: Retrieval-augmented multimodal language model-\ning. In: ICML (2023) 3, 4\n\n. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,\n\nShi, Y., et al.: mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv.org (2023) 2, 4\n\n. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S.,\n\nZhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large\n", "vlm_text": "37. Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number of classes. In: ICVGIP (2008)  9 ,  19\n\n 38. OpenAI: GPT-4V(ision) system card (2023),  https://openai.com/research/ gpt-4v-system-card  2 ,  4 ,  5 ,  25\n\n 39. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: CVPR (2012)  9 ,  19\n\n 40. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding multimodal large language models to the world. arXiv.org (2023)  2 ,  4\n\n 41. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)  1 ,  3 ,  4 ,  10 ,  11 ,  26 ,  27\n\n 42. Sh ted rit ski, A., Rupprecht, C., Vedaldi, A.: What does clip know about a red circle? visual prompt engineering for vlms. arXiv preprint arXiv:2304.06712 (2023)  3\n\n 43. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)  19\n\n 44. Subramania n, S., Merrill, W., Darrell, T., Gardner, M., Singh, S., Rohrbach, A.: Reclip: A strong zero-shot baseline for referring expression comprehension. In: Pro- ceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers). pp. 5198–5215 (2022)  3\n\n 45. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training tech- niques for clip at scale. arXiv preprint arXiv:2303.15389 (2023)  1\n\n 46. Sun, Z., Fang, Y., Wu, T., Zhang, P., Zang, Y., Kong, S., Xiong, Y., Lin, D., Wang, J.: Alpha-CLIP: A clip model focusing on wherever you want. arXiv preprint arXiv:2312.03818 (2023)  3\n\n 47. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: Caltech-ucsd birds- 200-2011 (2011)  9 ,  19\n\n 48. Wang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., Lin, D.: V3Det: Vast vocabulary visual detection dataset. In: ICCV (2023)  3 ,  7 ,  9 ,  12 , 28\n\n 49. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual expert for pretrained language models (2023)  2 ,  4 ,  12\n\n 50. Wu, W., Yao, H., Zhang, M., Song, Y., Ouyang, W., Wang, J.: GPT4Vis: What can gpt-4 do for zero-shot visual recognition? arXiv preprint arXiv:2311.15732 (2023) 25\n\n 51. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: SUN database: Large- scale scene recognition from abbey to zoo. In: CVPR (2010)  9 ,  19\n\n 52. Xu, X., Xiong, T., Ding, Z., Tu, Z.: Masqclip for open-vocabulary universal im- age segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 887–898 (2023)  3\n\n 53. Yang, L., Wang, Y., Li, X., Wang, X., Yang, J.: Fine-grained visual prompting (2023)  3 ,  6\n\n 54. Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Z ett le moyer, L., Yih, W.t.: Retrieval-augmented multimodal language model- ing. In: ICML (2023)  3 ,  4\n\n 55. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modular iz ation empowers large language models with multi modality. arXiv.org (2023)  2 ,  4\n\n 56. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112 (2023)  2 ,  4\n\n "}
{"page": 17, "image_path": "doc_images/RAR_17.jpg", "ocr_text": "18\n\nZiyu Liu et al.\n\nmodel for advanced text-image comprehension and composition. arXiv preprint\narXiv:2309.15112 (2023) 2, 4\n\n57. Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L-H., Zhou, L., Dai, X.,\n\nYuan, L., Li, Y., et al.: RegionCLIP: Region-based language-image pretraining. In:\nCVPR (2022) 3, 11, 12\n\n58. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language\n\nmodels. IJCV (2022) 3\n\n. Zhou, X., Girdhar, R., Joulin, A., Krahenbiihl, P., Misra, I.: Detecting twenty-\n\nthousand classes using image-level supervision. In: ECCV (2022) 3\n\n. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision-\n\nlanguage understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 (2023) 2, 4\n", "vlm_text": "57. Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L.H., Zhou, L., Dai, X., Yuan, L., Li, Y., et al.: RegionCLIP: Region-based language-image pre training. In: CVPR (2022)  3 ,  11 ,  12\n\n 58. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. IJCV (2022)  3\n\n 59. Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty- thousand classes using image-level supervision. In: ECCV (2022)  3\n\n 60. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision- language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)  2 ,  4 In this appendix, we provide a series of detailed supporting materials to aid "}
{"page": 18, "image_path": "doc_images/RAR_18.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 19\n\nIn this appendix, we provide a series of detailed supporting materials to aid\nin a deeper understanding of our work. Firstly, in Appendix A, we introduce\nthe fourteen image classification datasets involved in our experiments, including\nseven common datasets and seven fine-grained datasets, as well as two large-\nscale vocabulary detection datasets. Following that, in Appendix B, we provide\ndetailed information on the prompts used in our |} AR, as well as the prompts\nused in corresponding ablation studies. In Appendix C, we supplement details\non the structure and experimental aspects of AR, dividing the content into\nthree sections: Fine-Grained Visual Recognition, Few-Shot Image Classification,\nand Zero-Shot Region Recognition.\n\nA Dataset Statistics\n\nIn this section, we delve deeper into the specifics of the fourteen classification\nand two detection datasets employed in our research. The classification datasets\nencompass a wide range, from general categories that cover a broad spectrum\nof common objects to fine-grained types that focus on more specific, detailed\ndistinctions within a particular category. The detection datasets, on the other\nhand, are extensive, encompassing tens of thousands of object categories. These\ndatasets are designed to challenge the model’s ability to identify and categorize\nobjects from a vast array of possible classes. The long-tail nature of these datasets\nposes a significant challenge for our 2 AR model.\n\nA.1 Classification Datasets\n\nIn the experimental part, we use a total of fourteen image classification datasets,\nincluding seven fine-grained classification datasets and seven common classifica-\ntion datasets. Fine-grained image classification datasets include: Bird-200 [47],\nStanford Cars [20], Dog-120 [19], Oxford Flowers [37], Oxford Pets [39], FGVC-\nAircraft [34], and Food101 [37]. Common image classification datasets include:\nImageNet [7], Caltech101 [11], RAF-DB [25], Sun397 [51], Eurosat [15], DTD [4],\nand UCF-101 [43]. We present all the utilized datasets in Figure 1. And in Tab. 7,\nwe list the statistics and sources of these datasets in detail.\n\nIn our fine-grained visual recognition experiments, we employed the follow-\ning datasets: Bird-200, Stanford Cars, Dog-120, Flowers-102, and Oxford pets.\nIn each dataset, we selected 3 images from the training set to construct our mem-\nory and conducted tests on the corresponding validation sets. In our few-shot\nimage classification experiments, we used the FGVC-Aircraft dataset to build\nfine-tune data and tested our AR model across eleven classification datasets:\nStanford Cars, Flower-102, Oxford Pets, Food101, ImageNet, Caltech101, RAF-\nDB, Sun397, Eurosat, DTD, and UCF-101. We selected either 4 or 8 images from\nthe training set of each dataset to place into memory, corresponding to 4-shot\nand 8-shot settings, respectively, and conducted tests across all validation sets.\n", "vlm_text": "in a deeper understanding of our work. Firstly, in Appendix  A , we introduce the fourteen image classification datasets involved in our experiments, including seven common datasets and seven fine-grained datasets, as well as two large- scale vocabulary detection datasets. Following that, in Appendix  B , we provide detailed information on the prompts used in our  RAR , as well as the prompts used in corresponding ablation studies. In Appendix  C , we supplement details on the structure and experimental aspects of  RAR , dividing the content into three sections: Fine-Grained Visual Recognition, Few-Shot Image Classification, and Zero-Shot Region Recognition. \nA Dataset Statistics \nIn this section, we delve deeper into the specifics of the fourteen classification and two detection datasets employed in our research. The classification datasets encompass a wide range, from general categories that cover a broad spectrum of common objects to fine-grained types that focus on more specific, detailed distinctions within a particular category. The detection datasets, on the other hand, are extensive, encompassing tens of thousands of object categories. These datasets are designed to challenge the model’s ability to identify and categorize objects from a vast array of possible classes. The long-tail nature of these datasets poses a significant challenge for our  RAR  model. \nA.1 Classification Datasets \nIn the experimental part, we use a total of fourteen image classification datasets, including seven fine-grained classification datasets and seven common classifica- tion datasets. Fine-grained image classification datasets include: Bird-200 [ 47 ], Stanford Cars [ 20 ], Dog-120 [ 19 ], Oxford Flowers [ 37 ], Oxford Pets [ 39 ], FGVC- Aircraft [ 34 ], and Food101 [ 37 ]. Common image classification datasets include: ImageNet [ 7 ], Caltech101 [ 11 ], RAF-DB [ 25 ], Sun397 [ 51 ], Eurosat [ 15 ], DTD [ 4 ], and UCF-101 [ 43 ]. We present all the utilized datasets in Figure 1. And in Tab.  7 , we list the statistics and sources of these datasets in detail. \nIn our fine-grained visual recognition experiments, we employed the follow- ing datasets: Bird-200, Stanford Cars, Dog-120, Flowers-102, and Oxford pets. In each dataset, we selected 3 images from the training set to construct our mem- ory and conducted tests on the corresponding validation sets. In our few-shot image classification experiments, we used the FGVC-Aircraft dataset to build fine-tune data and tested our  RAR  model across eleven classification datasets: Stanford Cars, Flower-102, Oxford Pets, Food101, ImageNet, Caltech101, RAF- DB, Sun397, Eurosat, DTD, and UCF-101. We selected either 4 or 8 images from the training set of each dataset to place into memory, corresponding to 4-shot and 8-shot settings, respectively, and conducted tests across all validation sets. "}
{"page": 19, "image_path": "doc_images/RAR_19.jpg", "ocr_text": "20 Ziyu Liu et al.\n\nDatasets\n\nFine-Grained Classification Dataset\n\nFood101 Stanford Cars Flowerl02._ FGVC-Aircraft Oxford pets\n\n—_~\n\nCommon Classification Dataset\n\nFig. 6: Datsets used in our experiments. We select 14 classification datasets (7 fine-\ngrained and 7 common) and 2 object detection datasets as our benchmarks.\n\nTable 7: Statistics for the classification and detection datasets used in our three set-\ntings: fine-grained visual recognition, few-shot image recognition, and zero-shot region\nrecognition.\n\nSettings Dataset Categories Evaluation Metrics Source link\n: ; Bird-200 200 cAGC, sACC Bird website\nFine-Grained Car-196 196 cACC, sACC Kaggle\nVisual Recog. Dog-120 120 cACC, sACC Tensorflow\nFlower-102 102 cACC, sACC Tensorflow\nPet-37 37 cACC, sACC Tensorflow\nRAF-DB 7 Accuracy RAF-DB website\nEurosat 10 Accuracy Tensorflow\nFGVC Aireraft 100 Accuracy FGVC webs\n- x ircraft ccurac. 3VC website\nFew-Shot Image Caltech101 101 ‘Accuracy Tensorflow\nRecog. Food101 101 Accuracy Tensorflow\nUCF-101 101 Accuracy Tensorflow\nSUN397 397 Accuracy Tensorflow\nImageNet 1000 Accuracy Tensorflow\nZero-Shot LVIS 1203 mAP LVIS website\nRegion Recog. V3Det 13204 mAP Github\n\nA.2 Detection Datasets\n\nIn our Zero-Shot Region Recognition experiments, we utilized two large-scale\nvocabulary detection datasets, namely LVIS and V3Det. The LVIS dataset, de-\nveloped by Facebook AI researchers, stands out with its extensive coverage,\nincluding 164,000 images and about 2,000,000 high-quality instance segmenta-\ntion annotations that span over 1,000 object classes. This dataset is particularly\nnotable for its long-tail distribution, which means it includes a large number of\ninfrequent or rare object classes in addition to the common ones. This diver-\nsity challenges our model to recognize and differentiate between a wide array\nof objects, including those that are less common and hence more challenging to\nidentify accurately.\n\nThe V3Det dataset complements LVIS by offering an even broader scope.\nWith its 245,000 images distributed across an impressive 13,204 categories,\n", "vlm_text": "The image lists datasets used in experiments, divided into three categories:\n\n1. **Fine-Grained Classification Datasets:**\n   - Food101\n   - Stanford Cars\n   - Flower102\n   - FGVC-Aircraft\n   - Oxford Pets\n   - Dog-120\n   - Bird-200\n\n2. **Common Classification Datasets:**\n   - SUN397\n   - Eurosat\n   - Caltech101\n   - UCF101\n   - DOTD\n   - Imagenet\n   - RAF-DB\n\n3. **Detection Datasets:**\n   - LVIS\n   - V3Det\n\nThese serve as benchmarks for classification and object detection tasks.\nThe table presents various datasets and their details categorized under different settings for visual recognition tasks:\n\n1. **Settings**: The tasks are categorized into Fine-Grained Visual Recognition, Few-Shot Image Recognition, and Zero-Shot Region Recognition.\n\n2. **Dataset**: Lists the specific datasets used for each setting.\n\n3. **Categories**: Indicates the number of categories within each dataset.\n\n4. **Evaluation Metrics**: Specifies the metrics used to evaluate performance:\n   - For Fine-Grained Visual Recognition: cACC, sACC\n   - For Few-Shot Image Recognition: Accuracy\n   - For Zero-Shot Region Recognition: mAP\n\n5. **Source Link**: Provides the source links for accessing the datasets, with references to specific websites and platforms like Tensorflow, Kaggle, FGVC website, RAF-DB website, LVIS website, and Github.\nA.2 Detection Datasets \nIn our Zero-Shot Region Recognition experiments, we utilized two large-scale vocabulary detection datasets, namely LVIS and V3Det. The LVIS dataset, de- veloped by Facebook AI researchers, stands out with its extensive coverage, including 164,000 images and about 2,000,000 high-quality instance segmenta- tion annotations that span over 1,000 object classes. This dataset is particularly notable for its long-tail distribution, which means it includes a large number of infrequent or rare object classes in addition to the common ones. This diver- sity challenges our model to recognize and differentiate between a wide array of objects, including those that are less common and hence more challenging to identify accurately. \nThe V3Det dataset complements LVIS by offering an even broader scope. With its 245,000 images distributed across an impressive 13,204 categories, "}
{"page": 20, "image_path": "doc_images/RAR_20.jpg", "ocr_text": "2AR: Retrieving And Ranking Augmented MLLMs for Visual Recognition\n\n21\n\n(D)\n\nPlease play the role of a classification expert, and\nsort the provided categories from high to low\naccording to the top 5 similarity with the input\nimage. Here are the optional categories:{'2012 Aston\nMartin V8 Vantage Convertible’, '2012 Aston Martin\nVirage Coupe’, '2012 Aston Martin Virage\nConvertible’, 2012 Audi TTS Coupe’, 2012 Aston\nMartin V8 Vantage Coupe’)\n\nPlease play the role of a classification expert, and\nsort the provided categories from high to low\naccording to the top 5 similarity with the input\nimage. Here are the optional categories: ['737-800',\n'757-300', '767-200', 'A340-300', 'MD-90']\n\nBased on the input image and the provided\n\n3. 2012 Aston Martin V8 Vantage Convertible\n4, 2012 Aston Martin Virage Convertible\n5.2012 Audi TTS Coupe\n\nBased on the input image and the provided\ncategories, the sorting from high to low similarity\nwith the aircraft in the image is as follows:\n\n1. 757-300\n\ncategories, the sorting from high to low similarity 2. 767-200\nwith the car in the image is as follows: 3. MD-90\n\n1, 2012 Aston Martin V8 Vantage Coupe 4. 737-800\n2, 2012 Aston Martin Virage Coupe 5. A340-300\n\nWhy did you give this order?\n\nWhy did you give this order?\n\nThe car in the image is an Aston Martin, and its\nstyling most closely resembles the Vantage model\nline. It's a coupe, not a convertible, so coupes have\n\nThe aircraft in the image is a Boeing 757, which is\nidentifiable by its long fuselage, narrow-body, and\ncharacteristic nose shape. The -300 series has an\nextended fuselage compared to other variants, which\nappears to match the aircraft pictured. The 767-200\nand MD-90 have some similarities but differ in\n\nbeen ranked higher than convertibles. The Audi TTS\nCoupe, while also a coupe, is from a different\nmanufacturer and thus has distinet styling cues that\nare less similar to the vehicle in the image.\n\nspecific design elements such as winglets and\nengine placement, The 737-800 is a shorter aircraft,\nwith a different wing design. The A340-300 is a\nwide-body aircraft with four engines, making it\nquite distinct from the 757-300.\n\nFig. 7: GPT4V Example for Stanford Cars and FGVC Aircraft. Green for ground\ntruth, blue for characteristics analyzed by GPT-4V.\n\nV3Det brings an unprecedented level of diversity to the table. The dataset in-\ncludes 1,753,000 meticulously annotated bounding boxes, making it an invaluable\nresource for developing and testing detection algorithms capable of handling a\nwide variety of object types. Its large number of categories ensures that the\ndataset has a comprehensive representation of the visual world, making it an\nideal testing ground for our Zero-Shot Region Recognition experiments.\n\nB- Prompt Formats\n\nIn this section, we delve into the detailed design of our prompts. We have crafted\ndistinct prompts for various tasks to test the capabilities of the baseline model\nand our .\\ model in visual recognition.\n\nn our |.\\R pipeline, the prompt primarily serves to merge the input image\nwith the category information retrieved from memory. It guides MLLMs to rank\nthe retrieved candidate object categories based on similarity. Our prompt format\nis as follows:\n\nPlease play the role of aclassification expert, and sort the provided\ncategories from high to low according to the {top-k} similarity with the\ninput image. Here are the optional categories: {categories}.\n\nHere, ‘{top-k}’ is replaced with the number of categories input. And ‘{cate-\ngories}’ is replaced with the top-k categories retrieved from memory.\n\n", "vlm_text": "The image showcases two examples of GPT-4V analyzing and classifying vehicles based on images. \n\n**Left Side: Car Analysis**\n- A red car is pictured, and GPT-4V is tasked with sorting provided car categories by similarity.\n- Categories include various Aston Martin models and an Audi.\n- The model identified with the highest similarity is the \"2012 Aston Martin V8 Vantage Coupe.\"\n\n**Right Side: Aircraft Analysis**\n- An aircraft is pictured, and GPT-4V is tasked with sorting provided aircraft categories by similarity.\n- Categories include various Boeing and other aircraft models.\n- The model identified with the highest similarity is the \"757-300.\"\n\nThe green text indicates ground truth, and the blue text highlights the characteristics analyzed by GPT-4V.\nV3Det brings an unprecedented level of diversity to the table. The dataset in- cludes 1,753,000 meticulously annotated bounding boxes, making it an invaluable resource for developing and testing detection algorithms capable of handling a wide variety of object types. Its large number of categories ensures that the dataset has a comprehensive representation of the visual world, making it an ideal testing ground for our Zero-Shot Region Recognition experiments. \nB Prompt Formats \nIn this section, we delve into the detailed design of our prompts. We have crafted distinct prompts for various tasks to test the capabilities of the baseline model and our  RAR  model in visual recognition. \nIn our  RAR  pipeline, the prompt primarily serves to merge the input image with the category information retrieved from memory. It guides MLLMs to rank the retrieved candidate object categories based on similarity. Our prompt format is as follows: \nPlease play the role of a classification expert, and sort the provided categories from high to low according to the {top-k} similarity with the input image. Here are the optional categories:{categories}. \nHere,   $^{\\circ}\\{\\mathrm{top-k}\\}^{\\prime}$   is replaced with the number of categories input. And ‘{cate- gories}’ is replaced with the top-  $\\cdot\\mathrm{k}$   categories retrieved from memory. "}
{"page": 21, "image_path": "doc_images/RAR_21.jpg", "ocr_text": "22 Ziyu Liu et al.\n\n(D)\n(D)\n\n.)\n\na\nPlease play the role of a Please play the role of a Please play the role of a\nclassification expert, and sort classification expert, and sort classification expert, and sort\nthe provided categories from the provided categories from the provided categories from\nhigh to low according to the high to low according to the high to low according to the top\ntop 5 similarity with the input top 5 similarity with the input 5 similarity with the input\nimage, Here are the optional image, Here are the optional image, Here are the optional\ncategories:('lenten rose’, categories: ['yorkshire_terrier’, categories:\n\n‘japanese anemone’, tree ‘chihuahua’miniature_pinsche [‘donuts’, breakfast_burrito’,\npoppy’, ‘columbine’, ‘camellia'] shavanese' beagle’) ‘falafel’, fnummus',‘beet_salad’]\n\nBased on the input image and\nthe provided categories, here\nis the sorting of the categories\n\nBased on the input image and\nthe provided categories, the\nsorting from high to low\n\nBased on the input image and\nthe provided categories, the\nsorting from high to low\n\nfrom high to low according to similarity with the dog in the similarity with the food items\ntheir similarity with the flower image is: in the image is:\n\nin the image: 1. Chihuahua 1. Falafel\n\n1. Japanese anemone 2. Miniature Pinscher 2. Hummus\n\n2. Tree poppy 3. Yorkshire Terrier 3. Beet Salad\n\n3. Camellia 4, Havanese 4, Breakfast Burrito\n\n4, Lenten rose 5. Beagle 5, Donuts\n\n5. Columbine\n\nWhy did you give this order? Why did you give this order? Why did you give this order?\n\n(D)\n\nThe flower in the image\nappears to be a Japanese\nanemone, which typically has\n\nThe dog in the image is a\nChihuahua, identifiable by its\nsmall size, prominent ears, and\n\nThe dish in the image appears\nto include falafel balls and a\nside of hummus, which are\n\nwhite petals surrounding a facial features. Miniature typically found in Middle\nyellow center of stamens and a nschers are somewhat similar Eastem cuisine. The other\ngreenish pistil, matching the in size and ear shape, while items, such as beet salad,\n\none shown. The other flowers Yorkshire Terriers, Havanese, breakfast burrito, and donuts,\nlisted have different and Beagles have distinct do not seem to be present in the\ncharacteristics that do not differences in coat, size, and image or resemble the food\nmatch as closely. facial structure compared to the shown.\n\nChihuahua.\n\nFig. 8: GPT4V Example for Flowers102, Pets37 and Food101. Green for ground\ntruth, blue for characteristics analyzed by GPT-4V.\n\nAdditionally, to assess the visual recognition and ranking capabilities of\nMLLMs themselves, we have prepared a prompt with examples to serve as input\nfor the model. Our structured in-context learning prompt is as follows:\n\n‘Please play the role of aclassification expert, and sort the provided\ncategories from high to low according to the top 5 similarity with\nthe input image. Here are the optional categories: {categories}. Your\nanswer should follow the following format, like: [‘category A’, ‘category\nB’, ‘category C’, ‘categoryD’, ‘category E’]. Only choose five categories,\nand no further information.’’\n\nWhen testing the R.\\R pipeline with MLLMs, ‘{categories\nall the category names of each dataset.\n\nis replaced with\n\nC More Implemented Details and Experiments\n\nC.1 Fine-Grained Visual Recognition\n\nIn the fine-grained visual recognition section, we first evaluate our AR on the\nsetting defined in previous work [31]. For each category in the five datasets,\n", "vlm_text": "The image displays three panels, each with a category classification task involving different types of items (flowers, dogs, and foods). Each panel includes:\n\n1. **Image and Task Description**:\n   - The left panel shows a flower, the middle panel shows a dog, and the right panel shows food.\n   - Each task involves sorting provided categories based on similarity to the image.\n\n2. **GPT-4V Analysis**:\n   - Below each image, the AI provides an ordered list based on similarity to the image.\n   - For the flower, \"Japanese anemone\" is most similar.\n   - For the dog, \"Chihuahua\" is most similar.\n   - For the food, \"Falafel\" is most similar.\n\n3. **Rationale Behind the Analysis**:\n   - Explanations are given for each ordered list, detailing specific characteristics in the images that led to the classifications, such as petal color for flowers, physical features for the dog, and identifiable food components.\n\nThe panels highlight GPT-4V’s ability to perform image classification and provide detailed reasoning for its choices.\nAdditionally, to assess the visual recognition and ranking capabilities of MLLMs themselves, we have prepared a prompt with examples to serve as input for the model. Our structured in-context learning prompt is as follows: \n“Please play the role of a classification expert, and sort the provided categories from high to low according to the top 5 similarity with the input image. Here are the optional categories:{categories}. Your answer should follow the following format, like:[‘category A’, ‘category B’, ‘category C’, ‘category D’, ‘category E’]. Only choose five categories, and no further information.” \nWhen testing the  RAR  pipeline with MLLMs, ‘{categories}’ is replaced with all the category names of each dataset. \nC More Implemented Details and Experiments \nC.1 Fine-Grained Visual Recognition \nIn the fine-grained visual recognition section, we first evaluate our  RAR  on the setting defined in previous work [ 31 ]. For each category in the five datasets, "}
{"page": 22, "image_path": "doc_images/RAR_22.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition\n\nTop-k Accuracy on Caltech101\n\nv2 3\n\nTop-k Accuracy on DTD\n\n23\n\nTop-k Accuracy on Flowers102\n\n98 \"gt 80 oe “ oon\nE 6 / Brs ot z\n/ oss = oa /\n> 1 of 2\nge g” / Fa\nFA i 5 FA a\nZool / 3 / Zo, /\n2 ge) fag\n90) 3) Of /\net = Tepe Accuracy 50750 eso == Tepe Accuracy\ni 3 i Fr % i 3 ry Fa 7 i 5 70 Fr 7\nTop-k Top-k Top-k\n‘Top-k Accuracy on Pets37 ‘Top-k Accuracy on RAF-DB Top-k Accuracy on UCF-101\n20) ep accuracy yyy MRRP 300) aap accuracy IN 95) ap accuracy ,, 4, Sut ~ 78°\n-\" sau 90. we\n_ 90 ago = > > -\nz Va z op\" Bes oe\ne” 7 go a“ B00 /\ng / é val gis, /\ngm os Z g /\nsso 20 1426 os tito\ni 70 ro ED iS 10 5 % nr) i % 7\nTop-k Top-k Top-k\n\nFig. 9: Evaluation on CLIP+KNN for Caltech101, Flowers102, RAF-DB, Pets37,\nDTD and UCF101. We report the top-1, 5, 10, 15, 20 accuracy (%) under the 4-shot\n\nsettings.\n\nMLLMs tested on Caltech101\n\nMLLMs tested on Flowers102\n\nous 39\na. == Top-k Accuracy aso) =\" Top-k Accuracy\naif :\nNS 425 te\n90 89.70 ‘.\n> 5 ano sag\nz _ g a\na a 337. S\nB as Ny a SS\ng #724 ero 2 3g\nS57 woe 2325 ce\n‘S. 529.72\n\" a“\n86 oe Me\nory .\n“ 22 5 a\n$3 | a\nTop-k Top-k\n\nFig.10: Evaluation on MLLMs for Caltech101, Flowers102. We report the test\nresults using 10, 15, 20, 25, and 30 category names as inputs.\n\nwe select three unlabeled images to form a 3-shot setting. Then, we extract\nembeddings using the CLIP B/16 model and store them in memory. The labels\nfor each image correspond to the predictions in [31]. We then test the validation\nset using the [.\\Q pipeline and measure the results with Clustering Accuracy\n\n(cACC) and Semantic Similarity (sACC).\n\nEvaluation Metrics. In the fine-grained visual recognition section, we use\ntwo synergistic metrics: Clustering Accuracy (CACC) and Semantic Similarity\n(sACC) to evaluate our method, following [31]. Clustering Accuracy (cACC)\nmainly assesses the accuracy of clustering images within the same category,\nwithout considering the semantic relatedness of category labels. Complementing\nthis, Semantic Similarity (sACC) measures the similarity between the names of\ncategories in the clusters and the ground truth.\n", "vlm_text": "The image contains six line graphs showing the top-k accuracy percentages for different datasets under 4-shot settings. The datasets are Caltech101, DTD (Describable Textures Dataset), Flowers102, Pets37, RAF-DB, and UCF101. Each graph depicts how accuracy increases as the value of k in top-k increases from 1 to 20.\n\n- **Caltech101**: Starts at 87.91% for top-1 and reaches 99.51% for top-20.\n- **DTD**: Starts at 50.45% for top-1 and reaches 87.53% for top-20.\n- **Flowers102**: Starts at 84.50% for top-1 and reaches 99.76% for top-20.\n- **Pets37**: Starts at 55.60% for top-1 and reaches 98.34% for top-20.\n- **RAF-DB**: Starts at 14.26% for top-1 and reaches 89.99% for top-20.\n- **UCF101**: Starts at 64.60% for top-1 and reaches 94.90% for top-20.\n\nThe overall trend indicates that accuracy improves with higher k values across all datasets.\nThe image contains two line graphs evaluating the performance of Multimodal Large Language Models (MLLMs) on two different datasets: Caltech101 and Flowers102. Each graph shows how the Top-k accuracy changes as the number of category names used as inputs increases.\n\n1. **Left Graph (Caltech101):**\n   - The y-axis represents accuracy in percentage.\n   - The x-axis represents the number of category names used as inputs, ranging from 10 to 30.\n   - The trend shows a general decrease in accuracy from 91.56% at 10 category names to 85.23% at 30 category names.\n\n2. **Right Graph (Flowers102):**\n   - The y-axis represents accuracy in percentage.\n   - The x-axis represents the number of category names used as inputs, also ranging from 10 to 30.\n   - The trend also shows a decrease in accuracy, from 45.59% at 10 category names to 26.72% at 30 category names.\n\nBoth graphs use a dashed line to connect data points, illustrating how accuracy decreases as more category names are used as input in the model testing for both datasets.\nwe select three unlabeled images to form a 3-shot setting. Then, we extract embeddings using the CLIP B/16 model and store them in memory. The labels for each image correspond to the predictions in [31]. We then test the validation set using the  RAR  pipeline and measure the results with Clustering Accuracy (cACC) and Semantic Similarity (sACC). \nEvaluation Metrics.  In the fine-grained visual recognition section, we use two synergistic metrics: Clustering Accuracy (cACC) and Semantic Similarity (sACC) to evaluate our method, following [ 31 ]. Clustering Accuracy (cACC) mainly assesses the accuracy of clustering images within the same category, without considering the semantic relatedness of category labels. Complementing this, Semantic Similarity (sACC) measures the similarity between the names of categories in the clusters and the ground truth. "}
{"page": 23, "image_path": "doc_images/RAR_23.jpg", "ocr_text": "24 Ziyu Liu et al.\n\nTable 8: Few-shot image classification across 11 datasets. We report the top-1 accuracy\n(%) under the 1-shot, 2-shot, 4-shot, 8-shot and 16-shot settings. The CLIP+KNN\nmethod does not utilize the text encoder of CLIP. Instead, we employ the visual encoder\nto extract image features, and then apply the KNN algorithm to these features. Here\nour RAR uses the LLaVA1.5 [28] as the MLLM to rank the retrieved results. The\nsymbol ‘-’ denotes to the LLaVA model fails to make the predictions due to the limited\nwindow size.\n\nMethod Common Fine-Grained\nes & 7 ~~ a & z] 4\n2 = §& § & 2 ¢ EF 5s §]| ®\n& £ &¢£ 2 $€ @ &£ € 2 3 8/6\nEI I =) 3 & iS) PS] & § K By\n4 ie) + a [<3] a i=) & a q fo} <\n1-shot\nCLIP+KNN 29.2 75.9 11.3 37.7 53.9 35.1 47.8 66.7 32.6 45.3, 41.3 43.3,\nLLaVAL.5 Finetuning - 84.1 24.9 - 48.2 22.3 35.4 4.59 - 39.2 16.3 -\nRAR (LLaVA1.5) 40.3 85.2 34.8 46.5 62.4 38.1 57.4 504 38.3 57.6 47.0 | 50.7\nA +10.5 +9.3 +23.5 48.8 +8.5 43.0 49.6 -16.3 +5.7 +12.3 45.7] +7.4\n2-shot\nCLIP+KNN 36.1 82.9 11.7 446 58.7 41.2 58.5 78.9 40.9 541 49.0 50.6,\nLLaVAL.5 Finetuning - 53.1 24.9 - 48.2 22.3 38.7 10.03 - 38.2 16.3 -\nRAR (LLaVA1.5) 46.8 89.2 27.9 53.1 68.6 47.9 66.5 54.7 45.9 65.4 54.7 | 57.4\nA +10.7 +6.3 +16.2 48.5 +9.9 46.7 48.0 -24.2 45.0 +11.3 45.7] +6.8\n4-shot\nCLIP+KNN 42.1 87.9 14.2 51.4 67.6 47.5, 64.6 84.5 49.2 62.6 55.6 57.0\nLLaVAL.5 Finetuning - 88.4 24.9 - 48.2 46.6. 58.9. 13.2 - 66.4 28.9 -\nRAR (LLaVA1.5) 51.0 92.1 27.7 58.8 74.8 53.9 69.6 804 544 71.4 60.9 | 63.2\nA +9.9 44.2 413.5 47.4 47.2 46.4 45.0 -4.1 45.2 48.8 45.3 | +6.2\n8-shot\nCLIP+KNN 47.6 90.6 28.2 56.8 72.8 53.2 68.3 89.5 56.1 68.3, 61.8 63.0\nLLaVAL.5 Finetuning - 92.1 24.9 - 48.2 54.7 66.5, 30.1 - 72.5 46.1 -\nRAR (LLaVA1.5) 56.5 93.5 46.9 63.4 81.5 59.3 74.3 87.3 61.2 76.6 67.7 | 69.8\nA +8.9 42.9 418.7 +6.6 +8.7 +6.1 46.0 -2.2 45.1 48.3 +45.9| +6.8\n16-shot\nCLIP+KNN 52.0 92.4 35.0 61.2 78.7, 57.5 70.6 92.1 63.2 718 68.3, 67.5\nLLaVAL.5 Finetuning - 94.1 24.9 - 50.6 63 TA.7 59.0 - 62.4 -\nRAR (LLaVA1.5) 60.3 94.1 53.1 68.0 84.8 63.7 75.9 92.1 67.8 79.4 72.7 | 73.8\nA +8.3 41.7 +18.1 46.8 46.1 46.2 45.3 40.0 446 +476 {4.4 | +6.3\n\nC.2  Few-Shot Image Classification\n\nIn this section, we delve deeper into some intriguing observations and motivations\nbehind our study. Additionally, we have included an array of expanded test\nresults in this part, encompassing classification tests from l-shot to 16-shot,\ntests for top-5 accuracy, and we have further expanded our memory to explore\nthe potential capabilities of RAR.\nMore Discussion about Motivation. In the field of image classification, es-\npecially when facing the challenges of fine-grained image categorization, can\nMLLMs prove competent and effective? To further explore the potential of\nMLLMs in image classification tasks, we employed the GPT-4V model to test se-\nlected images from our fine-grained datasets. Initially, we used the CLIP+KNN\nmethod to select 5 candidate images and their categories for a single image, en-\nsuring that these candidates are at the top-5 in similarity among all images in\nmemory, thus guaranteeing minimal differences between the chosen categories.\nAdditionally, we intentionally selected examples that CLIP failed to classify cor-\n\n", "vlm_text": "The table compares the performance results of different methods on image recognition tasks across various datasets. The table is divided into two main categories: \"Common\" and \"Fine-Grained,\" each containing several datasets. The methods evaluated include:\n\n1. CLIP + KNN\n2. LLaVA1.5 Finetuning\n3. RAR (LLaVA1.5)\n\nEach method's performance is displayed as a percentage. The table is further subdivided based on the number of shots (or examples) provided for training: 1-shot, 2-shot, 4-shot, 8-shot, and 16-shot. \n\nFor each method, the \"RAR (LLaVA1.5)\" row shows performance metrics, and the \"Δ\" row indicates the improvement in performance over the previous best approach – \"LLaVA1.5 Finetuning\" – for that particular shot setting. Improvements are highlighted with the corresponding positive difference from the previous method.\n\nPerformance average across all datasets for each method is also presented in the last column. The \"RAR (LLaVA1.5)\" consistently shows improvements over \"LLaVA1.5 Finetuning\" as denoted by the values in green.\nC.2 Few-Shot Image Classification \nIn this section, we delve deeper into some intriguing observations and motivations behind our study. Additionally, we have included an array of expanded test results in this part, encompassing classification tests from 1-shot to 16-shot, tests for top-5 accuracy, and we have further expanded our memory to explore the potential capabilities of  RAR . \nMore Discussion about Motivation.  In the field of image classification, es- pecially when facing the challenges of fine-grained image categorization, can MLLMs prove competent and effective? To further explore the potential of MLLMs in image classification tasks, we employed the GPT-4V model to test se- lected images from our fine-grained datasets. Initially, we used the CLIP+KNN method to select 5 candidate images and their categories for a single image, en- suring that these candidates are at the top-5 in similarity among all images in memory, thus guaranteeing minimal differences between the chosen categories. Additionally, we intentionally selected examples that CLIP failed to classify cor- "}
{"page": 24, "image_path": "doc_images/RAR_24.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 25\n\nTable 9: Evaluation on 11 datasets, reporting the top-5 accuracy. We use the 4-shot\nsetting.\n\nMethod Common Fine-Grained\n2\n= a» é 2\n\n3 g a is] 2 s = = [a g\n\nS 2 a z a fa) EA 2 ‘g 3 s ¢\n\nI a sz 5 3 ml 1S) 8 & g ¥ >\n\n4 is) ej RB a a p i} a 4 le} <\nCLIP+KNN 67.1 976 48.0 789 915 70.5 854 96.5 79.1 862 87.6 | 80.8\nRAR (LLaVAL.5)| 69.7 97.7 53.8 80.1 92.5 71.9 86.2 96.5 79.1 87.7 88.1 | 82.1\nA +2.6 40.1 45.8 41.2 41.0 41.4 40.8 +0.0 40.0 41.5 +0.5| +1.3\n\nTable 10: Evaluation on 11 datasets, reporting the top-1 accuracy. The GPT4V [38]\nresults are copied from [50].\n\nMethod | Common Fine-Grained\n- . & 2\n8 Ss i = 3 a 9g\n2 = 8 5 ¢ E = =F 3s §] &\nEI a <q > EI a 9 pS} s 5 EY 2\n4 iS) Ea B a a = & G & ie) <\nGPT-4V 62.0 95.5 585 57.7 36.2 591 8. 70.6 58.3 80.1 92.6 | 68.4\n\n6\nRAR (LLaVA1.5) 73.4 94.6 73.8 70.6 93.3 71.9 79.1 95.6 726 86.2 79.9 81.0\nA +11.4 -0.9 415.3 +12.9 +57.1 412.8 -2.5 +25.0 +14.3 +6.1 -12.7|+12.6\n6\n0\n\nRAR (Intern-IXC2)} 71.5 94.4 72.7 69.7, 91.7 69.9 77. 93.2 65.4 83.9 79.3\nA\n\n79.0\n\n+9.5  -1.1 414.2 +12.0 +55.5 +10.8 -4.1 +22.6 +7.1 43.8 -13.3|+10.6\nRAR (Qwen-VL) 75.8 95.5 66.0 72.7 90.7 72.5 814 97.5 81.6 87.2 88.1 | 82.6\nA +13.8 +0.0 +7.5 +5.0 454.5 +13.4 -0.2 +26.9 +23.3 47.1 -4.5 |4+14.2\n\nrectly, increasing the complexity of the task. Subsequently, we presented these\nimages and categories to GPT-4V, utilizing the prompt described in Appendix B,\nprompting GPT-4V to rank all categories by similarity. During this process, we\nalso requested GPT-4V to provide the rationale for its classifications, allowing\nus to analyze the specific role of MLLMs in classification tasks based on the\nreasons provided by GPT-4V. Fig. 7 and Fig. 8 presents several examples of\nfive fine-grained classification datasets.\n\nFrom the examples in Fig. 7 and Fig. 8, it is evident that GPT-4V is ca-\npable of effectively analyzing the main feature information of objects in images\nduring fine-grained image classification tasks. For instance, it identifies key char-\nacteristics such as “coupe” (a two-door car), “long fuselage” (long body of an\naircraft), and “prominent ears” (noticeably protruding ears), which are crucial\nfor distinguishing between similar categories. Sometimes, these detailed aspects\nmay be overlooked by the CLIP model, leading to classification errors. Therefore,\nadopting a method of initial retrieval followed by deeper analysis, firstly filter-\ning through the numerous fine-grained categories and then using MLLMs for\nfurther examination to select the most accurate answer, proves to be an effective\napproach for fine-grained image classification tasks.\n\nSimultaneously, we assessed CLIP’s accuracy in handling a variety of clas-\nsification datasets. We selected six datasets: Caltech101, Flowerl02, RAF-DB,\n\n", "vlm_text": "The table compares the performance of two methods, CLIP+KNN and RAR (LLaVA1.5), across various datasets. The datasets are divided into two categories: Common and Fine-Grained. The table displays the following information:\n\n- CLIP+KNN performance scores for each dataset, as well as an average score.\n- RAR (LLaVA1.5) performance scores for each dataset and an average score.\n- The improvement (Δ) of RAR (LLaVA1.5) over CLIP+KNN for each dataset and on average.\n\nThe datasets listed are:\n\n**Common:**\n- ImageNet\n- Caltech101\n- RAF-DB\n- SUN397\n- EuroSAT\n- DTD\n- UCF-101\n\n**Fine-Grained:**\n- Flower102\n- Stanford Cars\n- Food101\n- Oxford Pets\n\nRAR (LLaVA1.5) shows improvements over CLIP+KNN in most datasets, as indicated by the positive values in the Δ row.\nThis table compares different methods for performing tasks across various datasets. Here's a breakdown of the table structure:\n\n- **Methods Compared**: GPT-4V, RAR (LLaVA1.5), RAR (Intern-IXC2), and RAR (Qwen-VL).\n\n- **Datasets**: It includes results for both \"Common\" datasets (ImageNet, Caltech101, RAF-DB, SUN397, EuroSAT, DTD, UCF-101) and \"Fine-Grained\" datasets (Flower102, StanfordCars, Food101, OxfordPets).\n\n- **Results**: Each cell contains:\n  - The score achieved by the method.\n  - Δ (Delta) represents the improvement or change in the score compared to GPT-4V.\n\n- **Average Column**: Presents the average score across all datasets for each method.\n\n- **Color Coding**: Green text indicates improvement (positive change) compared to GPT-4V.\n\nKey Insights:\n- RAR methods generally improve upon GPT-4V, especially in \"EuroSAT,\" \"Flower102,\" and \"Average.\"\n- RAR (Qwen-VL) tends to have the highest scores and biggest improvements (Δ) across several datasets.\nrectly, increasing the complexity of the task. Subsequently, we presented these images and categories to GPT-4V, utilizing the prompt described in Appendix  B , prompting GPT-4V to rank all categories by similarity. During this process, we also requested GPT-4V to provide the rationale for its classifications, allowing us to analyze the specific role of MLLMs in classification tasks based on the reasons provided by GPT-4V. Fig.  7  and Fig.  8  presents several examples of five fine-grained classification datasets. \nFrom the examples in Fig.  7  and Fig.  8 , it is evident that GPT-4V is ca- pable of effectively analyzing the main feature information of objects in images during fine-grained image classification tasks. For instance, it identifies key char- act eris tics such as “ coupe ” (a two-door car), “ long fuselage ” (long body of an aircraft), and “ prominent ears ” (noticeably protruding ears), which are crucial for distinguishing between similar categories. Sometimes, these detailed aspects may be overlooked by the CLIP model, leading to classification errors. Therefore, adopting a method of initial retrieval followed by deeper analysis, firstly filter- ing through the numerous fine-grained categories and then using MLLMs for further examination to select the most accurate answer, proves to be an effective approach for fine-grained image classification tasks. \nSimultaneously, we assessed CLIP’s accuracy in handling a variety of clas- sification datasets. We selected six datasets: Caltech101, Flower102, RAF-DB, "}
{"page": 25, "image_path": "doc_images/RAR_25.jpg", "ocr_text": "26 Ziyu Liu et al.\n\nTable 11: Cropping ablation of CLIP [41] zero-shot classification on LVIS [14]\nwith ground truth proposals. Different behaviors can be seen before and after blurring\nwith respect to different object scales.\n\nxX | 46.7 47.0 46.6 46.4 43.4 43.0 40.9 40.7 37.7 37.1 36.2\nAP Y |479 51.3 52.2 53.9 53.3 52.952.651.8 51.2 50.3 49.8\nxX | 39.5 40.9 44.6 44.8 44.4 44.2 42.9 43.3 41.2 40.5 39.8\nAs Y | 33.6 35.2 41.4 43.2 45.6 46.3 46.7 46.9 47.4 47.4 47.3\nxX |61.5 61.3 564 55.2 49.5 48.6 44.4 43.7 39.9 39.0 38.5\nAP v 63.5 64.2 66.1 68.3 65.2 64.2 63.4 62.2 61.0 59.2 58.6\nX |59.1 57.2 51.1 50.1 45.6 44.4 41.440.9 38.0 37.8 37.2\nAP, Y |72.4 71.3 69.5 69.6 67.0 65.2 62.9 60.7 59.6 57.4 55.2\n\nPets37, DTD, and UCF101, and tested the CLIP+KNN method for top 1, 5, 10,\n15, and 20 accuracy, with results presented in Fig. 9. We observed that as the\ntop-k value increased, the classification accuracy improved rapidly, reaching over\n90% in four of the six datasets when top-k reached 10. This indicates that CLIP\nshows significant advantages as the number of predicted categories increases,\ncomplementing MLLMs’ ability to discern among similar categories.\n\nFollowing the experimental design in Fig. 9, we used MLLMs to rank cate-\ngories when expanding the number of categories. We chose two datasets, Cal-\ntech101 and Flowersl02, and used 10, 15, 20, 25, 30 categories as input to\nMLLMs, ensuring these included the correct category. As shown in Fig. 10, the\ndistinction ability of MLLMs gradually decreased as the number of categories\ninput into MLLMs increased.\n\nHence, we found that MLLMs and CLIP have complementary advantages in\nclassification tasks. CLIP initially narrows down the correct answer to a smaller\nset through preliminary screening, while MLLMs can finely select the correct an-\nswer from this set. Our }.\\ i combines the strengths of both CLIP and MLLMs,\nfirst finding likely correct candidates through CLIP and retrieval, and then ac-\ncurately selecting the correct answer through MLLMs’ ranking, thus achieving\noutstanding results across multiple classification datasets.\nMore Evaluation Results. In our few-shot image classification experiments,\nwe employed the CLIP B/16 model to extract embeddings from n images in each\ncategory, which were then stored in memory for testing the accuracy of n-shot\nexperiments. To accelerate retrieval speed, we initially use the HNSW algorithm\nto transform the original 576-dimensional vectors into 64-dimensional indices\nbefore storing the image embeddings in memory. HNSW is a commonly used\nApproximate Nearest Neighbor (ANN) algorithm, primarily aimed at quickly\nfinding the k nearest elements to a query in a large set of candidates. To demon-\nstrate the effectiveness of our method, we included results from 1-shot, 2-shot,\n\n", "vlm_text": "The table shows the results of some performance metrics (AP, APs, APm, AP1) under different crop scales and blurring conditions.\n\n- Crop scale values range from 1.0 to 3.0.\n- There are two blurring conditions, indicated by symbols: ✔️ and ✘.\n- AP metrics are evaluated: \n  - AP\n  - APs (small objects)\n  - APm (medium objects)\n  - AP1\n- Each cell contains numeric scores corresponding to the performance of each metric under different conditions.\n- Certain scores are highlighted in green or red.\n\nThis table likely pertains to a comparison of model performance under varying image preprocessing conditions.\nPets37, DTD, and UCF101, and tested the CLIP+KNN method for top 1, 5, 10, 15, and 20 accuracy, with results presented in Fig.  9 . We observed that as the top-k value increased, the classification accuracy improved rapidly, reaching over  $90\\%$   in four of the six datasets when top-k reached 10. This indicates that CLIP shows significant advantages as the number of predicted categories increases, complementing MLLMs’ ability to discern among similar categories. \nFollowing the experimental design in Fig.  9 , we used MLLMs to rank cate- gories when expanding the number of categories. We chose two datasets, Cal- tech101 and Flowers102, and used 10, 15, 20, 25, 30 categories as input to MLLMs, ensuring these included the correct category. As shown in Fig.  10 , the distinction ability of MLLMs gradually decreased as the number of categories input into MLLMs increased. \nHence, we found that MLLMs and CLIP have complementary advantages in classification tasks. CLIP initially narrows down the correct answer to a smaller set through preliminary screening, while MLLMs can finely select the correct an- swer from this set. Our  RAR  combines the strengths of both CLIP and MLLMs, first finding likely correct candidates through CLIP and retrieval, and then ac- curately selecting the correct answer through MLLMs’ ranking, thus achieving outstanding results across multiple classification datasets. \nMore Evaluation Results.  In our few-shot image classification experiments, we employed the CLIP B/16 model to extract embeddings from n images in each category, which were then stored in memory for testing the accuracy of n-shot experiments. To accelerate retrieval speed, we initially use the HNSW algorithm to transform the original 576-dimensional vectors into 64-dimensional indices before storing the image embeddings in memory. HNSW is a commonly used Approximate Nearest Neighbor (ANN) algorithm, primarily aimed at quickly finding the k nearest elements to a query in a large set of candidates. To demon- strate the effectiveness of our method, we included results from 1-shot, 2-shot, "}
{"page": 26, "image_path": "doc_images/RAR_26.jpg", "ocr_text": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition 27\n\ni)\n65\n60.\n\n55\n\nAP\n\n50.\n\n45\n\n40\n\n35\n\n1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00\nCrop Scale\n\nFig. 11: Metric curve visualization of CLIP [41] zero-shot classification on\nLVIS [14] with ground truth proposals. Different behaviors can be seen before and after\nblurring with respect to different object’s scales.\n\nand 16-shot experiments in the supplementary materials, alongside the results\nof 4-shot and 8-shot experiments, all of which are presented in Tab. 8.\n\nFrom the 1-shot to 16-shot experiments, |t.\\|t’s results showed an improve-\nment over the CLIP+KNN method by 7.4%, 6.8%, 6.2%, 6.8%, and 6.3% re-\nspectively, averaging a 6.7% percentage point increase, and significantly outper-\nforming the performance of the LLaVa model itself. This outcome demonstrates\nthe excellence of [i \\ 1 in image classification tasks (including fine-grained image\nclassification), achieved by integrating the strengths of MLLMs and retrieval\ntechniques.\n\nTop-5 Accuracy Results. Moreover, in the experiments conducted for our\npaper, we selected the top 5 retrieved results for ranking. To test the scalability\nof this method, we conducted a new experiment using the top 10 retrieved results,\nranking these ten categories and then assessing the accuracy of the top 5. In this\nexperiment, we utilized a 4-shot setting, the result is shown in Tab. 9.\n\nThe final results demonstrate that although the top 5 accuracy achieved by\nCLIP+KNN was already high, our [.\\ method still managed to make compre-\nhensive improvements on this basis. The average top 5 accuracy across eleven\ndatasets increased by 1.3%.\n\nExtension to the whole Training Set. To further explore the potential of\nRAR, we expanded the memory size to include all images from the training set\nstored in memory. We then compared the performance of A under this setup\n", "vlm_text": "The image is a graph showing the metric curve visualization of CLIP's zero-shot classification performance on the LVIS dataset using ground truth proposals. The graph plots Average Precision (AP) against various crop scales. It includes different colored and styled lines representing various metrics:\n\n- **APs (small objects)** and **APs_c** (blurred): Blue lines with triangles\n- **APm (medium objects)** and **APm_c** (blurred): Green lines with crosses\n- **APl (large objects)** and **APl_c** (blurred): Red lines with squares\n\nThe graph illustrates how performance varies with changes in object scale and the effect of blurring on classification accuracy.\nand 16-shot experiments in the supplementary materials, alongside the results of 4-shot and 8-shot experiments, all of which are presented in Tab.  8 . \nFrom the 1-shot to 16-shot experiments,  RAR ’s results showed an improve- ment over the CLIP+KNN method by    $7.4\\%$  ,    $6.8\\%$  ,    $6.2\\%$  ,    $6.8\\%$  , and    $6.3\\%$   re- spectively, averaging a  6.7%  percentage point increase, and significantly outper- forming the performance of the LLaVa model itself. This outcome demonstrates the excellence of  RAR  in image classification tasks (including fine-grained image classification), achieved by integrating the strengths of MLLMs and retrieval techniques. \nTop-5 Accuracy Results.  Moreover, in the experiments conducted for our paper, we selected the top 5 retrieved results for ranking. To test the s cal ability of this method, we conducted a new experiment using the top 10 retrieved results, ranking these ten categories and then assessing the accuracy of the top 5. In this experiment, we utilized a 4-shot setting, the result is shown in Tab.  9 . \nThe final results demonstrate that although the top 5 accuracy achieved by CLIP+KNN was already high, our  RAR  method still managed to make compre- hensive improvements on this basis. The average top 5 accuracy across eleven datasets increased by    $1.3\\%$  . \nExtension to the whole Training Set.  To further explore the potential of RAR , we expanded the memory size to include all images from the training set stored in memory. We then compared the performance of  RAR  under this setup with that of GPT-4V across multiple image classification datasets. The results are presented in Tab.  10 . "}
{"page": 27, "image_path": "doc_images/RAR_27.jpg", "ocr_text": "28 Ziyu Liu et al.\n\nwith that of GPT-4V across multiple image classification datasets. The results\nare presented in Tab. 10.\n\nThe results in Tab. 10 show that, regardless of whether the base model is\nLLaVa, Intern-IXC2, or Qwen-VL, 2A significantly outperforms GPT-4V in\nterms of accuracy. Across eleven datasets, the average precision of } AR exceeds\nthat of GPT-4V by 12.5 percentage points. It is observed that even 7B MLLMs,\nwhen integrated into the 2 A} pipeline, far surpass the classification capabilities\nof GPT-4V across multiple image classification datasets.\n\nC.3  Zero-Shot Region Recognition\n\nWe carefully study how to adapt CLIP and MLLMs pretrained on full images\nto region-level recognition tasks. Zero-shot LVIS [14] AP metric under different\ncrop scales and object scales are reported in Fig. 11 and Tab. 11. Based on this\nexperiment, we conclude with two major observations: Firstly, a proper amount\nof blurring can significantly improve classification accuracy. This trick can help\nleave enough context information while keeping the foreground object promi-\nnent. Secondly, for objects with different scales, different crop scales should be\nadapted to maximize classification accuracy. As shown in Fig. 11, after blur-\nring, Different object scale AP curves behave differently with respect to crop\nscale. We contribute this phenomenon to the resolution shift of CLIP input im-\nages. Therefore, we make two adaptations for CLIP and MLLMs for region-level\nrecognition: Gaussian blurring and adaptive crop scale. We adopt the hyperpa-\nrameters of these two tricks on the LVIS training set and find these adaptions\nnot only fit for the LVIS validation set but also other detection datasets like\nV2Det [48].\n", "vlm_text": "\nThe results in Tab.  10  show that, regardless of whether the base model is LLaVa, Intern-IXC2, or Qwen-VL,  RAR  significantly outperforms GPT-4V in terms of accuracy. Across eleven datasets, the average precision of  RAR  exceeds that of GPT-4V by 12.5 percentage points. It is observed that even 7B MLLMs, when integrated into the  RAR  pipeline, far surpass the classification capabilities of GPT-4V across multiple image classification datasets. \nC.3 Zero-Shot Region Recognition \nWe carefully study how to adapt CLIP and MLLMs pretrained on full images to region-level recognition tasks. Zero-shot LVIS [ 14 ] AP metric under different crop scales and object scales are reported in Fig.  11  and Tab.  11 . Based on this experiment, we conclude with two major observations: Firstly, a proper amount of blurring can significantly improve classification accuracy. This trick can help leave enough context information while keeping the foreground object promi- nent. Secondly, for objects with different scales, different crop scales should be adapted to maximize classification accuracy. As shown in Fig.  11 , after blur- ring, Different object scale AP curves behave differently with respect to crop scale. We contribute this phenomenon to the resolution shift of CLIP input im- ages. Therefore, we make two adaptations for CLIP and MLLMs for region-level recognition: Gaussian blurring and adaptive crop scale. We adopt the hyperpa- rameters of these two tricks on the LVIS training set and find these adaptions not only fit for the LVIS validation set but also other detection datasets like V3Det [ 48 ]. "}
