{"page": 0, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_0.jpg", "ocr_text": "MMDetection\nRelease 2.18.0\n\nMMDetection Authors\n\nOct 28, 2021\n", "vlm_text": "MM Detection Release 2.18.0 \nMM Detection Authors "}
{"page": 1, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_1.jpg", "ocr_text": "", "vlm_text": ""}
{"page": 2, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_2.jpg", "ocr_text": "GET STARTED\n\nPrerequisites 1\nInstallation 3\n2.1 Prepareenvironment .. 2... ee ee 3\n2.2 Install MMDetection . 2... ee ee 3\n2.3 Install without GPU support... 6. ee ee 5\n2.4 Another option: Docker Image... 6... ee 5\n2.5 A from-scratch setup script ©... ee ee 5\n2.6 Developing with multiple MMDetection versions... 6. 0. ee 6\nVerification 7\nBenchmark and Model Zoo 9\n4.1 Mirrorsites 2. ee ee 9\n4.2 Common settings. 2... ee eee 9\n4.3 ImageNet Pretrained Models... 1 6. ee ee 9\n44 Baselines... ee eee 10\n4.5 Speedbenchmark. . 2... ee ee 15\n4.6 Comparison with Detectron2. 2... eee 16\n1: Inference and train with existing models and standard datasets 17\n5.1 Inference with existing models... 2... 2 ee 17\n5.2 Test existing models on standard datasets . 6... 0 ee 20\n5.3. Train predefined models on standard datasets... 0. ee 26\n2: Train with customized datasets 29\n6.1 Prepare the customized dataset... 6. ee ee 29\n6.2 Prepareaconfig 2... ee 33\n6.3 Trainanewmodel .. 2... ee ee 34\n6.4 Testandinference ... 2... . ee 34\n3: Train with customized models and standard datasets 35\n7.1 Prepare the standard dataset... ee ee 35\n7.2 Prepare your own customized model... 2... ee ee 36\n7.3 Prepareaconfig 2... ee 37\n74  Trainanewmodel .. 2... ee 40\n7.5 Testandinference 2.2... ee ee 40\nTutorial 1: Learn about Configs 41\n8.1 Modify config through script arguments... 2... 2. ee 41\n8.2 Config File Structure... 2. 41\n\n", "vlm_text": "1 Prerequisites \n2 Installation 3 \n2.1 Prepare environment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n 2.2 Install MM Detection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n 2.3 Install without GPU support  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n 2.4 Another option: Docker Image  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n 2.5 A from-scratch setup script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n 2.6 Developing with multiple MM Detection versions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n\n \n3 Verification \n4 Benchmark and Model Zoo 9 \n4.1 Mirror sites  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n 4.2 Common settings  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n 4.3 ImageNet Pretrained Models  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n 4.4 Baselines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n 4.5 Speed benchmark  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n 4.6 Comparison with Detectron2  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n \n5 1: Inference and train with existing models and standard datasets 17 \n5.1 Inference with existing models  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n 5.2 Test existing models on standard datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n 5.3 Train predefined models on standard datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n \n6 2: Train with customized datasets 29 \n6.1 Prepare the customized dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n 6.2 Prepare a config . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n 6.3 Train a new model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\n 6.4 Test and inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\n \n7 3: Train with customized models and standard datasets 35 \n7.1 Prepare the standard dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n 7.2 Prepare your own customized model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n 7.3 Prepare a config . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n 7.4 Train a new model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\n 7.5 Test and inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\n \n8 Tutorial 1: Learn about Configs 41 \n8.2 Config File Structure  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 8.3 Config Name Style  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n 8.4 Deprecated train_cfg/test_cfg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n 8.5 An Example of Mask R-CNN  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\n 8.6 FAQ  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n\n "}
{"page": 3, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_3.jpg", "ocr_text": "10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n8.3 ConfigName Style... 2... 2\n\n8.4 Deprecated train_cfg/test_cfg 2...\n8.5 AnExample of Mask R-CNN 1... 2. 2. ee\n8.6 FAQ...\nTutorial 2: Customize Datasets\n\n9.1 Support new dataformat... 2.2... ee ee\n9.2 Customize datasets by dataset wrappers .. 6... ee\n9.3. Modify Dataset Classes 2.2... 2 ee\n9.4 COCO Panoptic Dataset... 2. ee\nTutorial 3: Customize Data Pipelines\n\n10.1 Design of Data pipelines... . 2... 2. ee\n10.2 Extend and use custom pipelines... 2... 2. eee\n\nTutorial 4: Customize Models\n11.1 Develop newcomponents ..... 2... 0.00.0. 0. ee ee\n\nTutorial 5: Customize Runtime Settings\n\n12.1 Customize optimization settings... 2... eee\n12.2 Customize training schedules . 2... 2. eee\n12.3. Customize workflow . 2... ee\n12.4 Customize hooks... 6. ee ee\n\nTutorial 6: Customize Losses\n\n13.1 Computation pipeline ofaloss.. 2... ee\n13.2. Set sampling method (step 1) 2... 2... 2 ee\n13.3. Tweaking loss 2...\n13.4 Weighting loss (step3). 2...\n\nTutorial 7: Finetuning Models\n\n14.1 Inherit base configs... 2... ee\n14.2 Modifyhead .. 2... ee\n14.3. Modify dataset... ee\n14.4 Modify training schedule... 2... ee\n14.5 Use pre-trained model 2... 2 ee\n\nTutorial 8: Pytorch to ONNX (Experimental)\n\n15.1 How to convert models from PytorchtoONNX .. 2... ee\n15.2 How to evaluate the exported models . 2... 2... 2. eee\n15.3. List of supported models exportabletoONNX .. 0... 0.02 2. ee eee\n15.4 The Parameters of Non-Maximum Suppression in ONNX Export ...............0.00-\n15.5 Reminders 2... ee\n15.6 FAQs 2... ee\n\nTutorial 9: ONNX to TensorRT (Experimental)\n\n16.1 How to convert models from ONNX to TensorRT. 2 2 2 ee\n16.2 How to evaluate the exported models . 2... 2... 2. ee\n16.3. List of supported models convertible to TensorRT .. 2... 0.0... . 00.0000 000 0000.5\n16.4 Reminders .. 2... 2... ee\n16.5 FAQS 2. eee\n\nTutorial 10: Weight initialization\n17.1 Description. 2... ee\n17.2 Initialize parameters .. 2... ee\n\n", "vlm_text": "\n9 Tutorial 2: Customize Datasets 55 \n9.1 Support new data format  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n\n 9.2 Customize datasets by dataset wrappers  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n\n 9.3 Modify Dataset Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n\n 9.4 COCO Panoptic Dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n\n \n10 Tutorial 3: Customize Data Pipelines 67 \n10.1 Design of Data pipelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\n 10.2 Extend and use custom pipelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\n \n11.1 Develop new components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n\n \n12 Tutorial 5: Customize Runtime Settings 79 \n12.1 Customize optimization settings  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n 12.2 Customize training schedules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n\n 12.3 Customize workflow  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\n 12.4 Customize hooks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\n \n13 Tutorial 6: Customize Losses 87 \n13.1 Computation pipeline of a loss  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n\n 13.2 Set sampling method (step 1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n\n 13.3 Tweaking loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n\n 13.4 Weighting loss (step 3)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n\n \n14 Tutorial 7: Finetuning Models 91 \n14.1 Inherit base configs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n\n 14.2 Modify head  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n\n 14.3 Modify dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\n 14.4 Modify training schedule  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\n 14.5 Use pre-trained model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n\n \n15 Tutorial 8: Pytorch to ONNX (Experimental) \n15.1 How to convert models from Pytorch to ONNX  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n\n 15.2 How to evaluate the exported models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n\n 15.3 List of supported models exportable to ONNX  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n\n 15.4 The Parameters of Non-Maximum Suppression in ONNX Export . . . . . . . . . . . . . . . . . . . 99\n\n 15.5 Reminders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n 15.6 FAQs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n \n16 Tutorial 9: ONNX to TensorRT (Experimental) 101 \n16.1 How to convert models from ONNX to TensorRT  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n\n 16.2 How to evaluate the exported models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n\n 16.3 List of supported models convertible to TensorRT . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n\n 16.4 Reminders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n\n 16.5 FAQs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n\n \n17 Tutorial 10: Weight initialization 105 \n17.1 Description  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n\n 17.2 Initialize parameters  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 17.3 Usage of init_cfg  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 "}
{"page": 4, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_4.jpg", "ocr_text": "18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n17.3. Usage of init_cfg. 2. ee\nLog Analysis\nResult Analysis\n\nVisualization\n\n20.1 Visualize Datasets 2... ee\n20.2 Visualize Models... 1... ee\n20.3 Visualize Predictions... 1... ee\n\nError Analysis\n\nModel Serving\n\n22.1 1. Convert model from MMDetection to TorchServe ... 2... 2. 2 eee\n22.2 2. Build mmdet-serve dockerimage .... 2... 0... 0. 2. ee ee\n22.3 3.Runmmdet-serve ...... ee\n22.4 4. Testdeployment... 2... . 0.000.000.0000 02 22 eee ee\n\nModel Complexity\n\nModel conversion\n\n24.1 MMDetection model to ONNX (experimental) .. 2... ee eee\n24.2. MMDetection 1.x model to MMDetection2.x... 2... 0... 2 eee\n24.3 RegNet model to MMDetection .. 2... 2 ee\n24.4 Detectron ResNet to Pytorch. 2... ee ee\n24.5 Prepare a model for publishing... 2.2... ee ee\n\nDataset Conversion\n\nBenchmark\n26.1 Robust Detection Benchmark .. 2.2... 2. ee\n26.2 FPS Benchmark .. 2.2... 0. 2 ee\n\nMiscellaneous\n27.1 Evaluatinga metric. ©... ee\n27.2 Print the entireconfig ...... 2... 2. ee\n\nHyper-parameter Optimization\n28.1 YOLO Anchor Optimization... 2... ee ee\n\nConventions\n\n29.1 LOSS. ee\n29.2 Empty Proposals .. 2... ee\n29.3 Coco Panoptic Dataset... ee\n\nCompatibility of MMDetection 2.x\n\n30.1 MMDetection2.18.0 .. 0.0.0.0... 00 ee ee\n30.2 MMDetection2.14.0 2.0... 0.0 00 ee ee\n30.3. MMDetection 2.12.0... 0.0... ee ee\n30.4 Compatibility with MMDetection lx 2... 2... ee\n30.5 pycocotools compatibility 2... 2.2...\n\nProjects based on MMDetection\n31.1 Projectsasanextension .. 2...\n31.2 Projectsofpapers ... 2... ee\n\n109\n111\n\n113\n113\n113\n113\n\n115\n\n117\n117\n117\n117\n118\n\n121\n\n123\n123\n123\n123\n124\n124\n\n125\n\n127\n127\n127\n\n129\n129\n129\n\n", "vlm_text": "\n18 Log Analysis 109 \n19 Result Analysis 111 \n20 Visualization 113 \n20.1 Visualize Datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 20.2 Visualize Models  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 20.3 Visualize Predictions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 \n21 Error Analysis 115 \n22 Model Serving 117 \n22.1 1. Convert model from MM Detection to TorchServe  . . . . . . . . . . . . . . . . . . . . . . . . . . 117 22.2 2. Build  mmdet-serve  docker image  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 22.3 3. Run  mmdet-serve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 22.4 4. Test deployment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 \n23 Model Complexity 121 \n24 Model conversion 123 \n24.1 MM Detection model to ONNX (experimental)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 24.2 MM Detection 1.x model to MM Detection 2.x  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 24.3 RegNet model to MM Detection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 24.4 Detectron ResNet to Pytorch  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 24.5 Prepare a model for publishing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 \n25 Dataset Conversion 125 \n26 Benchmark 127 26.1 Robust Detection Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 26.2 FPS Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 \n27 Miscellaneous 129 27.1 Evaluating a metric  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 27.2 Print the entire config . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 \n28 Hyper-parameter Optimization 131 28.1 YOLO Anchor Optimization  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 \n29 Conventions 133 \n29.1 Loss  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 29.2 Empty Proposals  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 29.3 Coco Panoptic Dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 \n30 Compatibility of MM Detection 2.x 135 \n30.1 MM Detection 2.18.0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 30.2 MM Detection 2.14.0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 30.3 MM Detection 2.12.0  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 30.4 Compatibility with MM Detection 1.x  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 30.5 py coco tools compatibility  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 \n31 Projects based on MM Detection 139 \n31.1 Projects as an extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 31.2 Projects of papers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 "}
{"page": 5, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_5.jpg", "ocr_text": "32\n\n33\n\n34\n\n35\n\n36\n\n37\n\nChangelog\n\n32.1 v2.18.0 (27/10/2021)... ee\n32.2 v2.17.0 (28/9/2021) 2. ee ee\n32.3. v2.16.0 (30/8/2021) 2...\n32.4 v2.15.1 (11/8/2021) 2.\n32.5 v2.15.0 (02/8/2021) 2...\n32.6 v2.14.0 (29/6/2021) 2... ee\n32.7 v2.13.0 (01/6/2021)...\n32.8 v2.12.0 (01/5/2021) 2...\n32.9 v2.11.0 (01/4/2021) 2. ee\n32.10 v2.10.0 (01/03/2021)...\n32.11 v2.9.0 (01/02/2021) 2. ee ee\n32.12 v2.8.0 (04/01/2021) 2... ee\n32.13 v2.7.0 (30/11/2020) 2... ee\n32.14 v2.6.0 (1/11/2020) 2...\n32.15 v2.5.0 (5/10/2020) 2...\n32.16 v2.4.0 (5/9/2020)...\n32.17 v2.3.0 (5/8/2020)...\n32.18 v2.2.0 (1/7/2020)...\n32.19 v2.1.0 (8/6/2020)... ee ee\n32.20 v2.0.0 (6/5/2020)...\n32.21 v1.1.0 (24/2/2020) 2...\n32.22 v1.0.0 (30/1/2020) 2... ee\n32.23 vl.0rel (13/12/2019)...\n32.24 v1.0rcO (27/07/2019). 6\n32.25 v0.6.0 (14/04/2019) 2. eee\n32.26 v0.6rc0(06/02/2019) . ee\n32.27 v0.5.7 (06/02/2019) 2. eee\n32.28 v0.5.6 (17/01/2019) 2 eee\n32.29 v0.5.5 (22/12/2018) 6. ee\n32.30 v0.5.4 (27/11/2018) 2.\n32.31 v0.5.3 (26/11/2018) 2...\n32.32 v0.5.2 (21/10/2018) 2...\n32.33 v0.5.1 (20/10/2018) 2... ee\n\nFrequently Asked Questions\n\n33.1 MMCV Installation 2.2... eee\n33.2 PyTorch/CUDA Environment ... 2... 0... 000002 eee ee\n33.3 Training. 2...\n33.4 Evaluation . 2... ee\n\nEnglish\n\nmmdet.apis\n\nmmdet.core\n\n37.1 anchor 2...\n37.2 bDbDOX 2. ee\n37.3 export...\n374 mask 2... ee\n37.5 evaluation...\n37.6 post_processing 2.2...\n37.7 utils. ee\n\n141\n141\n142\n144\n145\n146\n147\n148\n150\n151\n152\n153\n154\n156\n157\n158\n159\n161\n162\n163\n165\n166\n167\n168\n171\n171\n171\n171\n171\n171\n172\n172\n172\n172\n\n173\n173\n173\n174\n175\n\n177\n\n179\n\n", "vlm_text": "32 Changelog \n32.1 v2.18.0 (27/10/2021)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n\n 32.2 v2.17.0 (28/9/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n\n 32.3 v2.16.0 (30/8/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n\n 32.4 v2.15.1 (11/8/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n\n 32.5 v2.15.0 (02/8/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n\n 32.6 v2.14.0 (29/6/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n\n 32.7 v2.13.0 (01/6/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n\n 32.8 v2.12.0 (01/5/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n\n 32.9 v2.11.0 (01/4/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n\n 32.10 v2.10.0 (01/03/2021)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 32.11 v2.9.0 (01/02/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n\n 32.12 v2.8.0 (04/01/2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n\n 32.13 v2.7.0 (30/11/2020) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\n 32.14 v2.6.0 (1/11/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n\n 32.15 v2.5.0 (5/10/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n\n 32.16 v2.4.0 (5/9/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n\n 32.17 v2.3.0 (5/8/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n\n 32.18 v2.2.0 (1/7/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n\n 32.19 v2.1.0 (8/6/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n\n 32.20 v2.0.0 (6/5/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n\n 32.21 v1.1.0 (24/2/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n\n 32.22 v1.0.0 (30/1/2020)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n\n 32.23 v1.0rc1 (13/12/2019)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n\n 32.24 v1.0rc0 (27/07/2019)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.25 v0.6.0 (14/04/2019) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.26 v0.6rc0(06/02/2019)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.27 v0.5.7 (06/02/2019) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.28 v0.5.6 (17/01/2019) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.29 v0.5.5 (22/12/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\n 32.30 v0.5.4 (27/11/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n 32.31 v0.5.3 (26/11/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n 32.32 v0.5.2 (21/10/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n 32.33 v0.5.1 (20/10/2018) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n \n33.1 MMCV Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n\n 33.2 PyTorch/CUDA Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n\n 33.3 Training  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n\n 33.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n\n \n34 English \n36 mmdet.apis \n37 mmdet.core 183 \n37.1 anchor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n\n 37.2 bbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n\n 37.3 export  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n\n 37.4 mask  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n\n 37.5 evaluation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n\n 37.6 post processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n\n 37.7 utils  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 38.1 datasets  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 38.2 pipelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 38.3 samplers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 38.4 api wrappers  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 "}
{"page": 6, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_6.jpg", "ocr_text": "38 mmdet.datasets 227\n\n38.1 datasets... ee 227\n38.2 pipelines 2... ee 239\n38.3. samplers 2... ee 256\n38.4 api_wrappers . 2... 257\n39 mmdet.models 259\n39.1 detectors 2... 259\n39.2 backbones .. 2... 273\n39.3 necks 2... ee 291\n39.4 dense_heads 2... 1... ee 301\n39.5 roi_heads . 11. ee 383\n39.6 losses... 412\n39.7 utils... 425\n40 mmdet.utils 437\n41 Indices and tables 439\nPython Module Index 441\nIndex 443\n\n", "vlm_text": "\n39 mmdet.models 259 39.1 detectors  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259 39.2 backbones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273 39.3 necks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 39.4 dense heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301 39.5 roi_heads  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 39.6 losses  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412 39.7 utils  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425 \n439 \n441 \n443 "}
{"page": 7, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_7.jpg", "ocr_text": "vi\n", "vlm_text": "vi\n"}
{"page": 8, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_8.jpg", "ocr_text": "CHAPTER\nONE\n\nPREREQUISITES\n\n« Linux or macOS (Windows is in experimental support)\n¢ Python 3.6+\n¢ PyTorch 1.3+\n\nCUDA 9.2+ (If you build PyTorch from source, CUDA 9.0 is also compatible)\n* GCC 5+\n* MMCV\n\nCompatible MMDetection and MMCV versions are shown as below. Please install the correct version of MMCV to\navoid installation issues.\n\nNote: You need to run pip uninstall mmcv first if you have mmcv installed. If mmcv and mmcy-full are both\ninstalled, there will be ModuleNotFoundError.\n\n", "vlm_text": "PREREQUISITES \n\n• Python  $^{3.6+}$  • PyTorch   $1.3+$  • CUDA  $9.2+$   (If you build PyTorch from source, CUDA 9.0 is also compatible) • GCC  $^{5+}$  •  MMCV \nCompatible MM Detection and MMCV versions are shown as below. Please install the correct version of MMCV to avoid installation issues. \nNote:  You need to run  pip uninstall mmcv  first if you have mmcv installed. If mmcv and mmcv-full are both installed, there will be  Module Not Found Error . "}
{"page": 9, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_9.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n2 Chapter 1. Prerequisites\n", "vlm_text": "MMDetection, Release 2.18.0\n\n2 Chapter 1. Prerequisites\n"}
{"page": 10, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_10.jpg", "ocr_text": "CHAPTER\nTwo\n\nINSTALLATION\n\n2.1 Prepare environment\n\n1. Create a conda virtual environment and activate it.\n\nconda create -n openmmlab python=3.7 -y\nconda activate openmmlab\n\n2. Install PyTorch and torchvision following the official instructions, e.g.,\n\nconda install pytorch torchvision -c pytorch\n\nNote: Make sure that your compilation CUDA version and runtime CUDA version match. You can check the\nsupported CUDA version for precompiled packages on the PyTorch website.\n\nE.g.1 If you have CUDA 10.1 installed under /usr/local/cuda and would like to install PyTorch 1.5, you\nneed to install the prebuilt PyTorch with CUDA 10.1.\n\nconda install pytorch cudatoolkit=10.1 torchvision -c pytorch\n\nE.g. 2 If you have CUDA 9.2 installed under /usr/local/cuda and would like to install PyTorch 1.3.1., you\nneed to install the prebuilt PyTorch with CUDA 9.2.\n\nconda install pytorch=1.3.1 cudatoolkit=9.2 torchvision=0.4.2 -c pytorch\n\nIf you build PyTorch from source instead of installing the prebuilt package, you can use more CUDA versions\nsuch as 9.0.\n\n2.2 Install MMDetection\n\nIt is recommended to install MMDetection with MIM, which automatically handle the dependencies of OpenMMLab\nprojects, including mmcv and other python packages.\n\npip install openmim\nmim install mmdet\n\nOr you can still install MMDetection manually:\n\n1. Install mmcev-full.\n\npip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/{cu_version}/\n\n= 3 (continues on next page)\n\n", "vlm_text": "INSTALLATION \n2.1 Prepare environment \n1. Create a conda virtual environment and activate it. \nThe image provides a step-by-step guide for setting up a Python environment using conda, specifically for using PyTorch and torchvision with CUDA support. Here's a breakdown of the steps shown:\n\n1. **Create and activate a conda environment:**\n   - Use the command `conda create -n openmmlab python=3.7 -y` to create a new environment named `openmmlab` with Python version 3.7.\n   - Activate the environment using `conda activate openmmlab`.\n\n2. **Install PyTorch and torchvision:**\n   - Follow the official instructions, which are not entirely detailed in the image, but generally involve using the command:\n     ```\n     conda install pytorch torchvision -c pytorch\n     ```\n   - It is important to ensure that your compilation CUDA version and runtime CUDA version match. Check compatibility on the PyTorch website.\n\n3. **Examples of CUDA installation:**\n   - **Example 1:** For a setup with CUDA 10.1:\n     ```\n     conda install pytorch cudatoolkit=10.1 torchvision -c pytorch\n     ```\n   - **Example 2:** For a setup with CUDA 9.2 and PyTorch 1.3.1:\n     ```\n     conda install pytorch=1.3.1 cudatoolkit=9.2 torchvision=0.4.2 -c pytorch\n     ```\n\nThese instructions highlight the importance of matching the version of PyTorch and CUDA toolkit to ensure compatibility when setting up the environment for machine learning purposes.\nIf you build PyTorch from source instead of installing the prebuilt package, you can use more CUDA versions such as 9.0. \n2.2 Install MM Detection \nIt is recommended to install MM Detection with  MIM , which automatically handle the dependencies of OpenMMLab projects, including mmcv and other python packages. \nThe table contains two command lines:\n\n1. `pip install openmim` \n2. `mim install mmdet`\n\nThese commands are typically used in a software development context to install Python packages. The first command uses pip (a package manager for Python) to install a package named `openmim`, and the second command uses the `mim` tool to install a package called `mmdet`. The exact purpose of these packages would be determined by their respective functionalities in the context they are used.\nOr you can still install MM Detection manually: "}
{"page": 11, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_11.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nPlease replace {cu_version} and {torch_version} in the url to your desired one. For example, to install the\nlatest mmcv-full with CUDA 11.0 and PyTorch 1.7.90, use the following command:\n\npip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/\nindex.html\n\nSee here for different versions of MMCV compatible to different PyTorch and CUDA versions.\n\nOptionally you can compile mmcv from source if you need to develop both mmcv and mmdet. Refer to the guide\nfor details.\n\n2. Install MMDetection.\n\nYou can simply install mmdetection with the following command:\n\npip install mmdet\n\nor clone the repository and then install it:\n\ngit clone https://github.com/open-mmlab/mmdetection. git\ncd mmdetection\n\npip install -r requirements/build. txt\n\npip install -v -e . # or \"python setup.py develop\"\n\n3. Install extra dependencies for Instaboost, Panoptic Segmentation, LVIS dataset, or Albumentations.\n\n# for instaboost\n\npip install instaboostfast\n\n# for panoptic segmentation\n\npip install git+https://github.com/cocodataset/panopticapi.git\n\n# for LVIS dataset\n\npip install git+https://github.com/lvis-dataset/lvis-api.git\n\n# for albumentations\n\npip install albumentations>=0.3.2 --no-binary imgaug,albumentations\n\nNote:\n\na. When specifying -e or develop, MMDetection is installed on dev mode , any local modifications made to the code\nwill take effect without reinstallation.\n\nb. If you would like to use opencv-python-headless instead of opencv-python, you can install it before installing\nMMCV.\n\nc. Some dependencies are optional. Simply running pip install -v -e . will only install the minimum runtime\nrequirements. To use optional dependencies like albumentations and imagecorruptions either install them man-\nually with pip install -r requirements/optional.txt or specify desired extras when calling pip (e.g. pip\ninstall -v -e .[optional]). Valid keys for the extras field are: all, tests, build, and optional.\n\nd. If you would like to use albumentations, we suggest using pip install albumentations>=0.3.2\n--no-binary imgaug,albumentations. If you simply use pip install albumentations>=0. 3.2, it will in-\nstall opencv-python-headless simultaneously (even though you have already installed opencv-python). We\nshould not allow opencv-python and opencv-python-head1ess installed at the same time, because it might cause\nunexpected issues. Please refer to official documentation for more details.\n\n4 Chapter 2. Installation\n", "vlm_text": "(continued from previous page) \nPlease replace  {cu_version}  and  {torch version}  in the url to your desired one. For example, to install the latest  mmcv-full  with  CUDA 11.0  and  PyTorch 1.7.0 , use the following command: \npip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/  $\\hookrightarrow$  index.html → \nSee  here  for different versions of MMCV compatible to different PyTorch and CUDA versions. \nOptionally you can compile mmcv from source if you need to develop both mmcv and mmdet. Refer to the  guide for details. \n2. Install MM Detection. \nYou can simply install mm detection with the following command: \nThe table provides two methods for installing the `mmdet` library:\n\n1. Install `mmdet` using pip:\n   ```\n   pip install mmdet\n   ```\n\n2. Clone the repository and install it manually. The steps are:\n   - Clone the repository from GitHub:\n     ```\n     git clone https://github.com/open-mmlab/mmdetection.git\n     ```\n   - Change the directory to the cloned repository:\n     ```\n     cd mmdetection\n     ```\n   - Install the requirements from `requirements/build.txt`:\n     ```\n     pip install -r requirements/build.txt\n     ```\n   - Install the package in editable mode:\n     ```\n     pip install -v -e .  # or \"python setup.py develop\"\n     ```\n3. Install extra dependencies for Instaboost, Panoptic Segmentation, LVIS dataset, or Albumen tat ions. \n# for instaboost pip install insta boost fast # for panoptic segmentation pip install git+https://github.com/coco data set/pan optic api.git # for LVIS dataset pip install git+https://github.com/lvis-dataset/lvis-api.git # for albumen tat ions pip install albumen tat ions>  $\\scriptstyle\\cdot=\\!0$  .3.2 --no-binary imgaug,albumen tat ions \nNote: \na. When specifying  -e  or  develop , MM Detection is installed on dev mode , any local modifications made to the code will take effect without re installation. \nb. If you would like to use  opencv-python-headless  instead of  opencv-python , you can install it before installing MMCV. \nc. Some dependencies are optional. Simply running  pip install -v -e .  will only install the minimum runtime requirements. To use optional dependencies like  albumen tat ions  and  image corruptions  either install them man- ually with  pip install -r requirements/optional.txt  or specify desired extras when calling  pip  (e.g.  pip install -v -e .[optional] ). Valid keys for the extras field are:  all ,  tests ,  build , and  optional . \nd. If you would like to use  albumen tat ions , we suggest using  pip install albumen tat ions>  $\\scriptstyle{\\sum=0}$  .3.2 --no-binary imgaug,albumen tat ions . If you simply use  pip install albumen tat ions  $\\scriptstyle:>=\\varnothing\\,.\\,3\\,.\\,2$  , it will in- stall  opencv-python-headless  simultaneously (even though you have already installed  opencv-python ). We should not allow  opencv-python  and  opencv-python-headless  installed at the same time, because it might cause unexpected issues. Please refer to  official documentation  for more details. "}
{"page": 12, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_12.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n2.3 Install without GPU support\n\nMMbDetection can be built for CPU only environment (where CUDA isn’t available).\nIn CPU mode you can run the demo/webcam_demo.py for example. However some functionality is gone in this mode:\n\n* Deformable Convolution\n\n* Modulated Deformable Convolution\n\n* ROI pooling\n\n* Deformable ROI pooling\n\n* CARAFE: Content-Aware ReAssembly of FEatures\n\n¢ SyncBatchNorm\n\n* CrissCrossAttention: Criss-Cross Attention\n\n* MaskedConv2d\n\n* Temporal Interlace Shift\n\n* nms_cuda\n\n* sigmoid_focal_loss_cuda\n\n* bbox_overlaps\n\nIf you try to run inference with a model containing above ops, an error will be raised. The following table lists affected\nalgorithms.\n\nNotice: MMDetection does not support training with CPU for now.\n\n2.4 Another option: Docker Image\n\nWe provide a Dockerfile to build an image. Ensure that you are using docker version >=19.03.\n\n# build an image with PyTorch 1.6, CUDA 10.1\ndocker build -t mmdetection docker/\n\nRun it with\n\ndocker run --gpus all --shm-size=8g -it -v {DATA_DIR}:/mmdetection/data mmdetection\n\n2.5 A from-scratch setup script\n\nAssuming that you already have CUDA 10.1 installed, here is a full script for setting up MMDetection with conda.\n\nconda create -n openmmlab python=3.7 -y\nconda activate openmmlab\n\nconda install pytorch==1.6.0 torchvision==0.7.9 cudatoolkit=10.1 -c pytorch -y\n\n# install the latest mmcv\npip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cul01/torch1.6.0/index.\n\nhem\n\n(continues on next page)\n\n2.3. Install without GPU support 5\n\n", "vlm_text": "2.3 Install without GPU support \nMM Detection can be built for CPU only environment (where CUDA isn’t available). \nIn CPU mode you can run the demo/webcam demo.py for example. However some functionality is gone in this mode: \n• Deformable Convolution • Modulated Deformable Convolution • ROI pooling • Deformable ROI pooling • CARAFE: Content-Aware ReAssembly of FEatures • Sync Batch Norm • CrissCross Attention: Criss-Cross Attention • Masked Con v 2 d • Temporal Interlace Shift • nms_cuda • s igm oid focal loss cuda • b box overlaps \nIf you try to run inference with a model containing above ops, an error will be raised. The following table lists affected algorithms. \nNotice:  MM Detection does not support training with CPU for now. \n2.4 Another option: Docker Image \nWe provide a  Dockerfile  to build an image. Ensure that you are using  docker version  $>=19.03$  . \n# build an image with PyTorch 1.6, CUDA 10.1 docker build -t mm detection docker/ \nRun it with \ndocker run --gpus all --shm-size = 8g -it -v  { DATA_DIR } :/mm detection/data mm detection \n2.5 A from-scratch setup script \nAssuming that you already have CUDA 10.1 installed, here is a full script for setting up MM Detection with conda. \nconda create -n openmmlab  python = 3 .7 -y conda activate openmmlab conda install  pytorch  $\\scriptstyle{\\mathcal{S}}=1.6.0$   torch vision  $\\scriptstyle{1\\equiv=0}$  .7.0  cuda toolkit = 10 .1 -c pytorch -y # install the latest mmcv pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index. ˓ → html (continues on next page) "}
{"page": 13, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_13.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n# install mmdetection\n\ngit clone https://github.com/open-mmlab/mmdetection.git\ncd mmdetection\n\npip install -r requirements/build. txt\n\npip install -v -e .\n\n2.6 Developing with multiple MMDetection versions\n\nThe train and test scripts already modify the PYTHONPATH to ensure the script use the MMDetection in the current\ndirectory.\n\nTo use the default MMDetection installed in the environment rather than that you are working with, you can remove\nthe following line in those scripts\n\nPYTHONPATH=\"$(dirname $0)/..\":$PYTHONPATH\n\n6 Chapter 2. Installation\n\n", "vlm_text": "2.6 Developing with multiple MM Detection versions \nThe train and test scripts already modify the  PYTHONPATH  to ensure the script use the MM Detection in the current directory. \nTo use the default MM Detection installed in the environment rather than that you are working with, you can remove the following line in those scripts "}
{"page": 14, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_14.jpg", "ocr_text": "CHAPTER\nTHREE\n\nVERIFICATION\n\nTo verify whether MMDetection is installed correctly, we can run the following sample code to initialize a detector and\ninference a demo image.\n\nfrom mmdet.apis import init_detector, inference_detector\n\nconfig_file = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n\n# download the checkpoint from model zoo and put it in ‘checkpoints/\n\n# url: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_\n«+ 1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n\ncheckpoint_file = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\ndevice = 'cuda:Q'\n\n# init a detector\n\nmodel = init_detector(config_file, checkpoint_file, device=device)\n\n# inference the demo image\n\ninference_detector(model, 'demo/demo.jpg')\n\nThe above code is supposed to run successfully upon you finish the installation.\n\n", "vlm_text": "VERIFICATION \nTo verify whether MM Detection is installed correctly, we can run the following sample code to initialize a detector and inference a demo image. \nfrom  mmdet.apis  import  in it detector, inference detector \nconfig file  $=$   ' configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py '\n\n # download the checkpoint from model zoo and put it in  \\` checkpoints/ \\`\n\n # url: https://download.openmmlab.com/mm detection/v2.0/faster r cnn/faster r cnn r 50 fp n\n\n  $\\hookrightarrow$  1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n\n → checkpoint file  $=$   ' checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth ' device  $=$   ' cuda:0 ' # init a detector model  $=$   in it detector(config file, checkpoint file, device  $=$  device)  $\\#$   inference the demo image inference detector(model,  ' demo/demo.jpg ' ) \nThe above code is supposed to run successfully upon you finish the installation. "}
{"page": 15, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_15.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n8 Chapter 3. Verification\n", "vlm_text": "MMDetection, Release 2.18.0\n\n8 Chapter 3. Verification\n"}
{"page": 16, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_16.jpg", "ocr_text": "4.1\n\nCHAPTER\nFOUR\n\nBENCHMARK AND MODEL ZOO\n\nMirror sites\n\nWe only use aliyun to maintain the model zoo since MMDetection V2.0. The model zoo of V1.x has been deprecated.\n\n4.2\n\n4.3\n\nCommon settings\n\nAll models were trained on coco_2017_train, and tested on the coco_2017_val.\nWe use distributed training.\n\nAll pytorch-style pretrained backbones on ImageNet are from PyTorch model zoo, caffe-style pretrained back-\nbones are converted from the newly released model from detectron2.\n\nFor fair comparison with other codebases, we report the GPU memory as the maximum value of torch. cuda.\nmax_memory_allocated() for all 8 GPUs. Note that this value is usually less than what nvidia-smi shows.\n\nWe report the inference time as the total time of network forwarding and post-processing, excluding the data\nloading time. Results are obtained with the script benchmark.py which computes the average time on 2000\nimages.\n\nImageNet Pretrained Models\n\nIt is common to initialize from backbone models pre-trained on ImageNet classification task. All pre-trained model\nlinks can be found at open_mmlab. According to img_norm_cfg and source of weight, we can divide all the ImageNet\npre-trained model weights into some cases:\n\nTorchVision: Corresponding to torchvision weight, including ResNet50, ResNetl101. The img_norm_cfg is\ndict (mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True).\n\nPycls: Corresponding to pycls weight, including RegNetX. The img_norm_cfg is dict( mean=[103.530,\n116.280, 123.675], std=[57.375, 57.12, 58.395], to_rgb=False).\n\nMSRA styles: Corresponding to MSRA weights, including ResNet50_Caffe and ResNet101_Caffe.\nThe img_norm_cfg is dict( mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0],\nto_rgb=False).\n\nCaffe2 styles: Currently only contains ResNext101_32x8d. The img_norm_cfg is dict (mean=[103.530,\n116.280, 123.675], std=[57.375, 57.120, 58.395], to_rgb=False).\n\n", "vlm_text": "BENCHMARK AND MODEL ZOO \n4.1 Mirror sites \nWe only use aliyun to maintain the model zoo since MM Detection V2.0. The model zoo of V1.x has been deprecated. \n4.2 Common settings \n• All models were trained on  coco 2017 train , and tested on the  coco 2017 val . \n• We use distributed training. • All pytorch-style pretrained backbones on ImageNet are from PyTorch model zoo, caffe-style pretrained back- bones are converted from the newly released model from detectron2. • For fair comparison with other codebases, we report the GPU memory as the maximum value of  torch.cuda. max memory allocated()  for all 8 GPUs. Note that this value is usually less than what  nvidia-smi  shows. • We report the inference time as the total time of network forwarding and post-processing, excluding the data loading time. Results are obtained with the script  benchmark.py  which computes the average time on 2000 images. \n4.3 ImageNet Pretrained Models \nIt is common to initialize from backbone models pre-trained on ImageNet classification task. All pre-trained model links can be found at  open_mmlab . According to  img norm cf g  and source of weight, we can divide all the ImageNet pre-trained model weights into some cases: \n• Torch Vision: Corresponding to torch vision weight, including ResNet50, ResNet101. The  img norm cf g  is dict(mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], std  $=$  [58.395, 57.12, 57.375], to_rgb  $=$  True) . • Pycls: Corresponding to  pycls  weight, including RegNetX. The  img norm cf g  is  dict( mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [103.530, 116.280, 123.675], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [57.375, 57.12, 58.395], to_rgb  $=$  False) . • MSRA styles: Corresponding to  MSRA  weights, including Res Net 50 Caff e and Res Net 101 Caff e. The img norm cf g is dict( mean  $\\risingdotseq$  [103.530, 116.280, 123.675], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [1.0, 1.0, 1.0], to_rgb  $\\leftrightharpoons$  False) . • Caffe2 styles: Currently only contains Res Next 101 32 x 8 d. The  img norm cf g  is  dict(mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [103.530, 116.280, 123.675], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [57.375, 57.120, 58.395], to_rgb  $\\mathrm{=}$  False) . "}
{"page": 17, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_17.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Other styles: E.g SSD which corresponds to img_norm_cfg is dict (mean=[123.675, 116.28, 103.53],\nstd=[1, 1, 1], to_rgb=True) and YOLOv3 which corresponds to img_norm_cfg is dict(mean=[0, 0,\n\n0], std=[255., 255., 255.], to_rgb=True).\n\nThe detailed table of the commonly used backbone models in MMDetection is listed below :\n\n4.4 Baselines\n\n4.4.1 RPN\n\nPlease refer to RPN for details.\n\n4.4.2 Faster R-CNN\n\nPlease refer to Faster R-CNN for details.\n\n4.4.3 Mask R-CNN\n\nPlease refer to Mask R-CNN for details.\n\n4.4.4 Fast R-CNN (with pre-computed proposals)\n\nPlease refer to Fast R-CNN for details.\n\n4.4.5 RetinaNet\n\nPlease refer to RetinaNet for details.\n\n4.4.6 Cascade R-CNN and Cascade Mask R-CNN\n\nPlease refer to Cascade R-CNN for details.\n\n4.4.7 Hybrid Task Cascade (HTC)\n\nPlease refer to HTC for details.\n\n4.4.8 SSD\n\nPlease refer to SSD for details.\n\n10 Chapter 4.\n\nBenchmark and Model Zoo\n", "vlm_text": "• Other styles: E.g SSD which corresponds to  img norm cf g  is  dict(mean  $\\risingdotseq$  [123.675, 116.28, 103.53], std  $\\scriptstyle=\\left[1\\right]$  , 1, 1], to_rgb  $\\leftrightharpoons$  True)  and YOLOv3 which corresponds to  img norm cf g  is  dict(mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [0, 0, 0], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [255., 255., 255.], to_rgb  $\\risingdotseq$  True) . \nThe detailed table of the commonly used backbone models in MM Detection is listed below : \n4.4 Baselines \n4.4.1 RPN \nPlease refer to  RPN  for details. \n4.4.2 Faster R-CNN \nPlease refer to  Faster R-CNN  for details. \n4.4.3 Mask R-CNN \nPlease refer to  Mask R-CNN  for details. \n4.4.4 Fast R-CNN (with pre-computed proposals) \nPlease refer to  Fast R-CNN  for details. \n4.4.5 RetinaNet \nPlease refer to  RetinaNet  for details. \n4.4.6 Cascade R-CNN and Cascade Mask R-CNN \nPlease refer to  Cascade R-CNN  for details. \n4.4.7 Hybrid Task Cascade (HTC) \nPlease refer to  HTC  for details. \n4.4.8 SSD \nPlease refer to  SSD  for details. "}
{"page": 18, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_18.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n4.4.9 Group Normalization (GN)\n\nPlease refer to Group Normalization for details.\n\n4.4.10 Weight Standardization\n\nPlease refer to Weight Standardization for details.\n\n4.4.11 Deformable Convolution v2\n\nPlease refer to Deformable Convolutional Networks for details.\n\n4.4.12 CARAFE: Content-Aware ReAssembly of FEatures\n\nPlease refer to CARAFE for details.\n\n4.4.13 Instaboost\n\nPlease refer to Instaboost for details.\n\n4.4.14 Libra R-CNN\n\nPlease refer to Libra R-CNN for details.\n\n4.4.15 Guided Anchoring\n\nPlease refer to Guided Anchoring for details.\n\n4.4.16 FCOS\n\nPlease refer to FCOS for details.\n\n4.4.17 FoveaBox\n\nPlease refer to FoveaBox for details.\n\n4.4.18 RepPoints\n\nPlease refer to RepPoints for details.\n\n4.4. Baselines\n\n11\n", "vlm_text": "4.4.9 Group Normalization (GN) \nPlease refer to  Group Normalization  for details. \n4.4.10 Weight Standardization Please refer to  Weight Standardization  for details. \n4.4.11 Deformable Convolution v2 \nPlease refer to  Deformable Convolutional Networks  for details. \n4.4.12 CARAFE: Content-Aware ReAssembly of FEatures Please refer to  CARAFE  for details. \n4.4.13 Instaboost Please refer to  Instaboost  for details. \n4.4.14 Libra R-CNN \nPlease refer to  Libra R-CNN  for details. \n4.4.15 Guided Anchoring \nPlease refer to  Guided Anchoring  for details. \n4.4.16 FCOS Please refer to  FCOS  for details. \n4.4.17 FoveaBox Please refer to  FoveaBox  for details. \n4.4.18 RepPoints \nPlease refer to  RepPoints  for details. "}
{"page": 19, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_19.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n4.4.19 FreeAnchor\n\nPlease refer to FreeAnchor for details.\n\n4.4.20 Grid R-CNN (plus)\n\nPlease refer to Grid R-CNN for details.\n\n4.4.21 GHM\n\nPlease refer to GHM for details.\n\n4.4.22 GCNet\n\nPlease refer to GCNet for details.\n\n4.4.23 HRNet\n\nPlease refer to HRNet for details.\n\n4.4.24 Mask Scoring R-CNN\n\nPlease refer to Mask Scoring R-CNN for details.\n\n4.4.25 Train from Scratch\n\nPlease refer to Rethinking ImageNet Pre-training for details.\n\n4.4.26 NAS-FPN\n\nPlease refer to NAS-FPN for details.\n\n4.4.27 ATSS\n\nPlease refer to ATSS for details.\n\n4.4.28 FSAF\n\nPlease refer to FSAF for details.\n\n12 Chapter 4.\n\nBenchmark and Model Zoo\n", "vlm_text": "4.4.19 FreeAnchor Please refer to  FreeAnchor  for details. \n4.4.20 Grid R-CNN (plus) Please refer to  Grid R-CNN  for details. \n4.4.21 GHM \nPlease refer to  GHM  for details. \n4.4.22 GCNet Please refer to  GCNet  for details. \n4.4.23 HRNet Please refer to  HRNet  for details. \n4.4.24 Mask Scoring R-CNN Please refer to  Mask Scoring R-CNN  for details. \n4.4.25 Train from Scratch \nPlease refer to  Rethinking ImageNet Pre-training  for details. \n4.4.26 NAS-FPN Please refer to  NAS-FPN  for details. \n4.4.27 ATSS Please refer to  ATSS  for details. \n4.4.28 FSAF \nPlease refer to  FSAF  for details. "}
{"page": 20, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_20.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n4.4.29 RegNetX\n\nPlease refer to RegNet for details.\n\n4.4.30 Res2Net\n\nPlease refer to Res2Net for details.\n\n4.4.31 GRolE\n\nPlease refer to GRolE for details.\n\n4.4.32 Dynamic R-CNN\n\nPlease refer to Dynamic R-CNN for details.\n\n4.4.33 PointRend\n\nPlease refer to PointRend for details.\n\n4.4.34 DetectoRS\n\nPlease refer to DetectoRS for details.\n\n4.4.35 Generalized Focal Loss\n\nPlease refer to Generalized Focal Loss for details.\n\n4.4.36 CornerNet\n\nPlease refer to CornerNet for details.\n\n4.4.37 YOLOv3\n\nPlease refer to YOLOVv3 for details.\n\n4.4.38 PAA\n\nPlease refer to PAA for details.\n\n4.4. Baselines\n\n13\n", "vlm_text": "4.4.29 RegNetX Please refer to  RegNet  for details. \n4.4.30 Res2Net \nPlease refer to  Res2Net  for details. \n4.4.31 GRoIE \nPlease refer to  GRoIE  for details. \n4.4.32 Dynamic R-CNN Please refer to  Dynamic R-CNN  for details. \n4.4.33 PointRend Please refer to  PointRend  for details. \n4.4.34 DetectoRS \nPlease refer to  DetectoRS  for details. \n4.4.35 Generalized Focal Loss \nPlease refer to  Generalized Focal Loss  for details. \n4.4.36 CornerNetPlease refer to  CornerNet  for details. \n4.4.37 YOLOv3 Please refer to  YOLOv3  for details. \n4.4.38 PAA \nPlease refer to  PAA  for details. "}
{"page": 21, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_21.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n4.4.39 SABL\n\nPlease refer to SABL for details.\n\n4.4.40 CentripetalNet\n\nPlease refer to CentripetalNet for details.\n\n4.4.41 ResNeSt\n\nPlease refer to ResNeSt for details.\n\n4.4.42 DETR\n\nPlease refer to DETR for details.\n\n4.4.43 Deformable DETR\n\nPlease refer to Deformable DETR for details.\n\n4.4.44 AutoAssign\n\nPlease refer to AutoAssign for details.\n\n4.4.45 YOLOF\n\nPlease refer to YOLOF for details.\n\n4.4.46 Seesaw Loss\n\nPlease refer to Seesaw Loss for details.\n\n4.4.47 CenterNet\n\nPlease refer to CenterNet for details.\n\n4.4.48 YOLOX\n\nPlease refer to YOLOX for details.\n\n14 Chapter 4.\n\nBenchmark and Model Zoo\n", "vlm_text": "4.4.39 SABL \nPlease refer to  SABL  for details. \n4.4.40 Centripetal Net Please refer to  Centripetal Net  for details. \n4.4.41 ResNeSt \nPlease refer to  ResNeSt  for details. \n4.4.42 DETR \nPlease refer to  DETR  for details. \n4.4.43 Deformable DETR Please refer to  Deformable DETR  for details. \n4.4.44 AutoAssign Please refer to  AutoAssign  for details. \n4.4.45 YOLOF Please refer to  YOLOF  for details. \n4.4.46 Seesaw Loss Please refer to  Seesaw Loss  for details. \n4.4.47 CenterNet Please refer to  CenterNet  for details. \n4.4.48 YOLOX \nPlease refer to  YOLOX  for details. "}
{"page": 22, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_22.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n4.4.49 PVT\n\nPlease refer to PVT for details.\n\n4.4.50 SOLO\n\nPlease refer to SOLO for details.\n\n4.4.51 Querylinst\n\nPlease refer to QueryInst for details.\n\n4.4.52 Other datasets\n\nWe also benchmark some methods on PASCAL VOC, Cityscapes and WIDER FACE.\n\n4.4.53 Pre-trained Models\n\nWe also train Faster R-CNN and Mask R-CNN using ResNet-50 and RegNetX-3.2G with multi-scale training and longer\nschedules. These models serve as strong pre-trained models for downstream tasks for convenience.\n\n4.5 Speed benchmark\n\n4.5.1 Training Speed benchmark\n\nWe provide analyze_logs.py to get average time of iteration in training. You can find examples in Log Analysis.\n\nWe compare the training speed of Mask R-CNN with some other popular frameworks (The data is copied from detec-\ntron2). For mmdetection, we benchmark with mask_rcnn_r50_caffe_fpn_poly_1x_coco_v1.py, which should have the\nsame setting with mask_rcnn_R_50_FPN_noaug_1x.yaml of detectron2. We also provide the checkpoint and training\nlog for reference. The throughput is computed as the average throughput in iterations 100-500 to skip GPU warmup\ntime.\n\n4.5.2 Inference Speed Benchmark\n\nWe provide benchmark.py to benchmark the inference latency. The script benchmarkes the model with 2000 images\nand calculates the average time ignoring first 5 times. You can change the output log interval (defaults: 50) by setting\nLOG- INTERVAL.\n\npython toools/benchmark.py ${CONFIG} ${CHECKPOINT} [--log-interval $[LOG-INTERVAL]] [--\n«fuse-conv-bn]\n\nThe latency of all models in our model zoo is benchmarked without setting fuse-conv-bn, you can get a lower latency\nby setting it.\n\n4.5. Speed benchmark 15\n\n", "vlm_text": "4.4.49 PVT \nPlease refer to  PVT  for details. \n4.4.50 SOLO \nPlease refer to  SOLO  for details. \n4.4.51 QueryInst \nPlease refer to  QueryInst  for details. \n4.4.52 Other datasets \nWe also benchmark some methods on  PASCAL VOC ,  Cityscapes  and  WIDER FACE . \n4.4.53 Pre-trained Models \nWe also train  Faster R-CNN  and  Mask R-CNN  using ResNet-50 and  RegNetX-3.2G  with multi-scale training and longer schedules. These models serve as strong pre-trained models for downstream tasks for convenience. \n4.5 Speed benchmark \n4.5.1 Training Speed benchmark \nWe provide  analyze logs.py  to get average time of iteration in training. You can find examples in  Log Analysis . \nWe compare the training speed of Mask R-CNN with some other popular frameworks (The data is copied from  detec- tron2 ). For mm detection, we benchmark with  mask r cnn r 50 caff e fp n poly 1 x coco v 1.py , which should have the same setting with  mask r cnn R 50 FP N no aug 1 x.yaml  of detectron2. We also provide the  checkpoint  and  training log  for reference. The throughput is computed as the average throughput in iterations 100-500 to skip GPU warmup time. \n4.5.2 Inference Speed Benchmark \nWe provide  benchmark.py  to benchmark the inference latency. The script benchmark es the model with 2000 images and calculates the average time ignoring first 5 times. You can change the output log interval (defaults: 50) by setting LOG-INTERVAL . \npython toools/benchmark.py  \\${ CONFIG } \\${ CHECKPOINT }  [ --log-interval \\$ [ LOG-INTERVAL ]] [ --\n\n  $\\hookrightarrow$  fuse-conv-bn ]\n\n → \nThe latency of all models in our model zoo is benchmarked without setting  fuse-conv-bn , you can get a lower latency by setting it. "}
{"page": 23, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_23.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n4.6 Comparison with Detectron2\n\nWe compare mmdetection with Detectron2 in terms of speed and performance. We use the commit id\n185c27e(30/4/2020) of detectron. For fair comparison, we install and run both frameworks on the same machine.\n\n4.6.1 Hardware\n\n* 8 NVIDIA Tesla V100 (32G) GPUs\n¢ Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz\n\n4.6.2 Software environment\n\n¢ Python 3.7\n¢ PyTorch 1.4\nCUDA 10.1\n\nCUDNN 7.6.03\n\nNCCL 2.4.08\n\n4.6.3 Performance\n\n4.6.4 Training Speed\n\nThe training speed is measure with s/iter. The lower, the better.\n\n4.6.5 Inference Speed\n\nThe inference speed is measured with fps (img/s) on a single GPU, the higher, the better. To be consistent with Detec-\ntron2, we report the pure inference speed (without the time of data loading). For Mask R-CNN, we exclude the time\nof RLE encoding in post-processing. We also include the officially reported speed in the parentheses, which is slightly\nhigher than the results tested on our server due to differences of hardwares.\n\n4.6.6 Training memory\n\n16 Chapter 4. Benchmark and Model Zoo\n", "vlm_text": "4.6 Comparison with Detectron2 \nWe compare mm detection with  Detectron2  in terms of speed and performance. We use the commit id 185c27e (30/4/2020) of detectron. For fair comparison, we install and run both frameworks on the same machine. \n4.6.1 Hardware \n• 8 NVIDIA Tesla V100 (32G) GPUs • Intel(R) Xeon(R) Gold 6148 CPU   $@$   2.40GHz \n4.6.2 Software environment \n• Python 3.7 • PyTorch 1.4 • CUDA 10.1 • CUDNN 7.6.03 • NCCL 2.4.08 \n4.6.3 Performance \n4.6.4 Training Speed \nThe training speed is measure with s/iter. The lower, the better. \n4.6.5 Inference Speed \nThe inference speed is measured with fps   $\\mathrm{(img/s)}$   on a single GPU, the higher, the better. To be consistent with Detec- tron2, we report the pure inference speed (without the time of data loading). For Mask R-CNN, we exclude the time of RLE encoding in post-processing. We also include the officially reported speed in the parentheses, which is slightly higher than the results tested on our server due to differences of hardwares. \n4.6.6 Training memory "}
{"page": 24, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_24.jpg", "ocr_text": "CHAPTER\nFIVE\n\n1: INFERENCE AND TRAIN WITH EXISTING MODELS AND\nSTANDARD DATASETS\n\nMMDetection provides hundreds of existing and existing detection models in Model Zoo), and supports multiple stan-\ndard datasets, including Pascal VOC, COCO, CityScapes, LVIS, etc. This note will show how to perform common\ntasks on these existing models and standard datasets, including:\n\n* Use existing models to inference on given images.\n* Test existing models on standard datasets.\n\n* Train predefined models on standard datasets.\n\n5.1 Inference with existing models\n\nBy inference, we mean using trained models to detect objects on images. In MMDetection, a model is defined by a\nconfiguration file and existing model parameters are save in a checkpoint file.\n\nTo start with, we recommend Faster RCNN with this configuration file and this checkpoint file. It is recommended to\ndownload the checkpoint file to checkpoints directory.\n\n5.1.1 High-level APIs for inference\n\nMMDetection provide high-level Python APIs for inference on images. Here is an example of building the model and\ninference on given images or videos.\n\nfrom mmdet.apis import init_detector, inference_detector\nimport mmcv\n\n# Specify the path to model config and checkpoint file\nconfig_file = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n\n# build the model from a config file and a checkpoint file\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')\n\n# test a single image and show the results\n\nimg = 'test.jpg' # or img = mmcv.imread(img), which will only load it once\nresult = inference_detector(model, img)\n\n# visualize the results in a new window\n\nmodel.show_result(img, result)\n\n(continues on next page)\n\n17\n\n", "vlm_text": "1: INFERENCE AND TRAIN WITH EXISTING MODELS AND STANDARD DATASETS \nMM Detection provides hundreds of existing and existing detection models in  Model Zoo ), and supports multiple stan- dard datasets, including Pascal VOC, COCO, CityScapes, LVIS, etc. This note will show how to perform common tasks on these existing models and standard datasets, including: \n• Use existing models to inference on given images. • Test existing models on standard datasets. • Train predefined models on standard datasets. \n5.1 Inference with existing models \nBy inference, we mean using trained models to detect objects on images. In MM Detection, a model is defined by a configuration file and existing model parameters are save in a checkpoint file. \nTo start with, we recommend  Faster RCNN  with this  configuration file  and this  checkpoint file . It is recommended to download the checkpoint file to  checkpoints  directory. \n5.1.1 High-level APIs for inference \nMM Detection provide high-level Python APIs for inference on images. Here is an example of building the model and inference on given images or videos. \nfrom  mmdet.apis  import  in it detector, inference detector import  mmcv # Specify the path to model config and checkpoint file config file  $=$   ' configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py ' checkpoint file  $=$   ' checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth ' # build the model from a config file and a checkpoint file model  $=$   in it detector(config file, checkpoint file, device  $\\equiv^{\\dagger}$  cuda:0 ' ) # test a single image and show the results img  $=$  ' test.jpg ' # or img  $=$   mmcv.imread(img), which will only load it once result  $=$   inference detector(model, img) # visualize the results in a new window model . show result(img, result) "}
{"page": 25, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_25.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n# or save the visualization results to image files\nmodel.show_result(img, result, out_file='result.jpg')\n\n# test a video and show the results\n\nvideo = mmcv.VideoReader('video.mp4')\n\nfor frame in video:\nresult = inference_detector(model, frame)\nmodel.show_result(frame, result, wait_time=1)\n\nA notebook demo can be found in demo/inference_demo.ipynb.\n\nNote: inference_detector only supports single-image inference for now.\n\n5.1.2 Asynchronous interface - supported for Python 3.7+\n\nFor Python 3.7+, MMDetection also supports async interfaces. By utilizing CUDA streams, it allows not to block\nCPU on GPU bound inference code and enables better CPU/GPU utilization for single-threaded application. Inference\ncan be done concurrently either between different input data samples or between different models of some inference\npipeline.\n\nSee tests/async_benchmark . py to compare the speed of synchronous and asynchronous interfaces.\n\nimport asyncio\n\nimport torch\n\nfrom mmdet.apis import init_detector, async_inference_detector\nfrom mmdet.utils.contextmanagers import concurrent\n\nasync def main():\n\nconfig_file = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\ndevice = 'cuda:Q'\n\nmodel = init_detector(config_file, checkpoint=checkpoint_file, device=device)\n\n# queue is used for concurrent inference of multiple images\nstreamqueue = asyncio.Queue()\n\n# queue size defines concurrency level\n\nstreamqueue_size = 3\n\nfor _ in range(streamqueue_size):\nstreamqueue. put_nowait (torch. cuda.Stream(device=device))\n\n# test a single image and show the results\nimg = 'test.jpg' # or img = mmcv.imread(img), which will only load it once\n\nasync with concurrent (streamqueue) :\nresult = await async_inference_detector(model, img)\n\n# visualize the results in a new window\nmodel.show_result(img, result)\n\n# or save the visualization results to image files\nmodel.show_result(img, result, out_file='result.jpg')\n\n(continues on next page)\n\n18 Chapter 5. 1: Inference and train with existing models and standard datasets\n\n", "vlm_text": "# or save the visualization results to image files model . show result(img, result, out_file  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}^{\\dagger}$  ' result.jpg ' ) # test a video and show the results video  $=$   mmcv . Video Reader( ' video.mp4 ' ) for  frame  in  video: result  $=$   inference detector(model, frame) model . show result(frame, result, wait_time  $\\mathbf{\\omega}=\\!1.$  ) \nA notebook demo can be found in  demo/inference demo.ipynb . Note:  inference detector  only supports single-image inference for now. \n5.1.2 Asynchronous interface - supported for Python  $\\mathbf{3.7+}$  \nFor Python   $3.7+$  , MM Detection also supports async interfaces. By utilizing CUDA streams, it allows not to block CPU on GPU bound inference code and enables better CPU/GPU utilization for single-threaded application. Inference can be done concurrently either between different input data samples or between different models of some inference pipeline. \nSee  tests/a sync benchmark.py  to compare the speed of synchronous and asynchronous interfaces. \nThis image depicts a Python script that uses libraries like `asyncio`, `torch`, and MMDetection to perform asynchronous inference with a Faster R-CNN model. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `asyncio` for handling asynchronous operations.\n   - `torch` presumably for PyTorch functionalities.\n   - Functions from `mmdet` for initializing the detector and performing async inference.\n\n2. **Function `main`**:\n   - Defines config and checkpoint files for the Faster R-CNN model.\n   - Initializes the model using `init_detector`.\n\n3. **Asynchronous Operations**:\n   - Uses `asyncio.Queue` to manage concurrency.\n   - The queue size is set to 3 for concurrent processing.\n\n4. **Loading and Processing**:\n   - Adds streams to the queue for GPU processing.\n   - Loads an image `test.jpg` for testing.\n\n5. **Inference and Result Display**:\n   - Asynchronously runs inference using `async_inference_detector`.\n   - Visualizes the results using `model.show_result`, either in a window or saved to `result.jpg`.\n(continues on next page) "}
{"page": 26, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_26.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nasyncio.run(main())\n\n5.1.3 Demos\n\nWe also provide three demo scripts, implemented with high-level APIs and supporting functionality codes. Source\ncodes are available here.\n\nImage demo\n\nThis script performs inference on a single image.\n\npython demo/image_demo.py \\\n${IMAGE_FILE} \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n[--device §${GPU_ID}] \\\n[--score-thr ${SCORE_THR}]\n\nExamples:\n\npython demo/image_demo.py demo/demo.jpg \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n--device cpu\n\nWebcam demo\n\nThis is a live demo from a webcam.\n\npython demo/webcam_demo.py \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n[--device §${GPU_ID}] \\\n[--camera-id §{CAMERA-ID}] \\\n[--score-thr ${SCORE_THR}]\n\nExamples:\n\npython demo/webcam_demo.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n\n5.1. Inference with existing models 19\n\n", "vlm_text": "The table contains the line of Python code: `asyncio.run(main())`.\n5.1.3 Demos \nWe also provide three demo scripts, implemented with high-level APIs and supporting functionality codes. Source codes are available  here . \nImage demo \nThis script performs inference on a single image. \nThe table contains a command line script for running a Python demo. Here’s the breakdown:\n\n```\npython demo/image_demo.py \\\n    ${IMAGE_FILE} \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--device ${GPU_ID}] \\\n    [--score-thr ${SCORE_THR}]\n```\n\n- `${IMAGE_FILE}`: Placeholder for the path to the image file.\n- `${CONFIG_FILE}`: Placeholder for the path to the configuration file.\n- `${CHECKPOINT_FILE}`: Placeholder for the checkpoint file.\n- `[--device ${GPU_ID}]`: Optional argument to specify the GPU ID.\n- `[--score-thr ${SCORE_THR}]`: Optional argument for setting the score threshold.\nExamples: \npython demo/image_demo.py demo/demo.jpg  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --device cpu \nWebcam demo \nThis is a live demo from a webcam. \nThe image shows a code snippet for running a Python script called `webcam_demo.py`, which appears to be part of a demonstration or application that uses a webcam. The script is run with the following command-line arguments:\n\n1. `${CONFIG_FILE}` - This placeholder is meant to be replaced with the path to the configuration file needed by the script.\n\n2. `${CHECKPOINT_FILE}` - This placeholder is meant to be replaced with the path to the checkpoint file, which is likely a model file needed for the script to execute.\n\nOptional arguments include:\n\n- `--device ${GPU_ID}` - This specifies the device to be used, with `${GPU_ID}` being an optional placeholder for the GPU ID if the script should run on a specific GPU.\n\n- `--camera-id ${CAMERA-ID}` - This specifies the camera ID, which likely selects which webcam to use if more than one is available.\n\n- `--score-thr ${SCORE_THR}` - This specifies a score threshold for the operation, with `${SCORE_THR}` being a placeholder for the value.\n\nThe placeholders (denoted by `${...}`) need to be replaced with actual values when executing the command.\nExamples: \npython demo/webcam demo.py  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth "}
{"page": 27, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_27.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nVideo demo\n\nThis script performs inference on a video.\n\npython demo/video_demo.py \\\n${VIDEO_FILE} \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n[--device §${GPU_ID}] \\\n[--score-thr ${SCORE_THR}] \\\n[--out ${OUT_FILE}] \\\n[--show] \\\n[--wait-time ${WAIT_TIME}]\n\nExamples:\n\npython demo/video_demo.py demo/demo.mp4 \\.\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n--out result.mp4\n\n5.2 Test existing models on standard datasets\n\nTo evaluate a model’s accuracy, one usually tests the model on some standard datasets. MMDetection supports multiple\npublic datasets including COCO, Pascal VOC, CityScapes, and more. This section will show how to test existing models\non supported datasets.\n\n5.2.1 Prepare datasets\n\nPublic datasets like Pascal VOC or mirror and COCO are available from official websites or mirrors. Note: In the\ndetection task, Pascal VOC 2012 is an extension of Pascal VOC 2007 without overlap, and we usually use them together.\nIt is recommended to download and extract the dataset somewhere outside the project directory and symlink the dataset\nroot to $MMDETECTION/data as below. If your folder structure is different, you may need to change the corresponding\npaths in config files.\n\nmmdetection\nt— mmdet\nt— tools\nt— configs\nt— data\nt— coco\nt— annotations\n+— train2017\nt— val2017\nfr— test2017\nt— cityscapes\nt— annotations\nt— leftImg8bit\n— train\nL— val\nt— gtFine\n\n(continues on next page)\n\n20 Chapter 5. 1: Inference and train with existing models and standard datasets\n\n", "vlm_text": "Video demo \nThis script performs inference on a video. \npython demo/video_demo.py  \\ \\${ VIDEO_FILE }  \\ \\${ CONFIG FILE }  \\ \\${ CHECKPOINT FILE }  \\ [ --device  \\${ GPU_ID } ]  \\ [ --score-thr  \\${ SCORE_THR } ]  \\ [ --out  \\${ OUT_FILE } ]  \\ [ --show ]  \\ [ --wait-time  \\${ WAIT_TIME } ] \nExamples: \npython demo/video_demo.py demo/demo.mp4  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --out result.mp4 \n5.2 Test existing models on standard datasets \nTo evaluate a model’s accuracy, one usually tests the model on some standard datasets. MM Detection supports multiple public datasets including COCO, Pascal VOC, CityScapes, and  more . This section will show how to test existing models on supported datasets. \n5.2.1 Prepare datasets \nPublic datasets like  Pascal VOC  or mirror and  COCO  are available from official websites or mirrors. Note: In the detection task, Pascal VOC 2012 is an extension of Pascal VOC 2007 without overlap, and we usually use them together. It is recommended to download and extract the dataset somewhere outside the project directory and symlink the dataset root to  \\$MM DETECTION/data  as below. If your folder structure is different, you may need to change the corresponding paths in config files. \nThe image displays a directory structure, likely related to a machine learning or computer vision project using the MMDetection framework. Here’s a breakdown of the directory structure:\n\n- **mmdetection**: The root directory of the project.\n  - **mmdet**: Presumably a directory for core MMDetection functionalities or library code.\n  - **tools**: Likely contains scripts or tools related to running experiments or processing data.\n  - **configs**: Probably holds configuration files for various models and experiments.\n  - **data**: A directory designated for datasets.\n    - **coco**: A sub-directory for the COCO dataset.\n      - **annotations**: Contains annotation files for COCO data.\n      - **train2017**: Training data from the 2017 COCO dataset.\n      - **val2017**: Validation data from the 2017 COCO dataset.\n      - **test2017**: Test data from the 2017 COCO dataset.\n    - **cityscapes**: A sub-directory for the Cityscapes dataset.\n      - **annotations**: Contains annotation files for Cityscapes data.\n      - **leftImg8bit**: A folder likely containing Cityscapes images processed in 8-bit format.\n        - **train**: Training images for Cityscapes.\n        - **val**: Validation images for Cityscapes.\n      - **gtFine**: Likely contains the finely annotated ground truth data for Cityscapes.\n\nThis hierarchical structure organizes the project into folders for core modules, tools, configurations, and datasets, specifically COCO and Cityscapes.\n(continues on next page) "}
{"page": 28, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_28.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nt— train\nt— val\nvoCdevkit\n\nt— VOC2007\nt— VOC2012\n\nSome models require additional COCO-stuff datasets, such as HTC, DetectoRS and SCNet, you can download and\nunzip then move to the coco folder. The directory should be like this.\n\nmmdetection\ndata\ncoco\nt— annotations\n+— train2017\nt— val2017\n-— test2017\nt— stuffthingmaps\n\nPanoptic segmentation models like PanopticFPN require additional COCO Panoptic datasets, you can download and\nunzip then move to the coco annotation folder. The directory should be like this.\n\nmmdetection\n\nt— data\n\ncoco\n\nt— annotations\n\nt— panoptic_train2017.json\nt— panoptic_train2017\nt— panoptic_val2017.json\nt— panoptic_val2017\n\n+— train2017\n\nt— val2017\n\nfr— test2017\n\nThe cityscapes annotations need to be converted into the coco format using tools/dataset_converters/\ncityscapes.py:\n\npip install cityscapesscripts\n\npython tools/dataset_converters/cityscapes.py \\\n./data/cityscapes \\\n--nproc 8 \\\n--out-dir ./data/cityscapes/annotations\n\nTODO: CHANGE TO THE NEW PATH\n\n5.2. Test existing models on standard datasets 21\n\n", "vlm_text": "The image shows a directory structure, likely related to a dataset. There is a main directory named `VOCdevkit`, which contains two subdirectories named `VOC2007` and `VOC2012`. Additionally, there are two folders `train` and `val` which are possibly subdirectories under each of `VOC2007` and `VOC2012`. This structure suggests it might be related to a dataset used for training and validation purposes, possibly the PASCAL VOC dataset.\nSome models require additional  COCO-stuff  datasets, such as HTC, DetectoRS and SCNet, you can download and unzip then move to the coco folder. The directory should be like this. \nThe image shows a directory structure related to a machine learning project using the COCO dataset. It looks like this:\n\n```\nmmdetection\n│\n└─── data\n     └─── coco\n          ├─── annotations\n          ├─── train2017\n          ├─── val2017\n          ├─── test2017\n          └─── stuffthingmaps\n```\n\nThis structure is typically used in computer vision tasks for object detection or segmentation, containing annotations and splits for training, validation, and testing.\nPanoptic segmentation models like Pan optic FP N require additional  COCO Panoptic  datasets, you can download and unzip then move to the coco annotation folder. The directory should be like this. \nThe image shows a directory structure for a dataset setup in MMDetection. It includes the following folders and files:\n\n- `mmdetection`\n  - `data`\n    - `coco`\n      - `annotations`\n        - `panoptic_train2017.json`\n        - `panoptic_train2017`\n        - `panoptic_val2017.json`\n        - `panoptic_val2017`\n      - `train2017`\n      - `val2017`\n      - `test2017`\nThe  cityscapes  annotations need to be converted into the coco format using  tools/data set converters/ cityscapes.py : \npip install cityscape s scripts python tools/data set converters/cityscapes.py  \\ ./data/cityscapes  \\ --nproc  8  \\ --out-dir ./data/cityscapes/annotations \nTODO: CHANGE TO THE NEW PATH "}
{"page": 29, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_29.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n5.2.2 Test existing models\n\nWe provide testing scripts for evaluating an existing model on the whole dataset (COCO, PASCAL VOC, Cityscapes,\n\netc.).\n\nThe following testing environments are supported:\nsingle GPU\nsingle node multiple GPUs\n\nmultiple nodes\n\nChoose the proper script to perform testing depending on the testing environment.\n\n# single-gpu testing\npython tools/test.py \\\n\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n[--out ${RESULT_FILE}] \\\n[--eval §{EVAL_METRICS}] \\\n[--show]\n\n# multi-gpu testing\nbash tools/dist_test.sh \\\n\nS${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n${GPU_NUM} \\\n\n[--out ${RESULT_FILE}] \\\n[--eval ${EVAL_METRICS }]\n\ntools/dist_test.sh also supports multi-node testing, but relies on PyTorch’s launch utility.\n\nOptional arguments:\n\nRESULT_FILE: Filename of the output results in pickle format. If not specified, the results will not be saved to a\nfile.\n\nEVAL_METRICS: Items to be evaluated on the results. Allowed values depend on the dataset, e.g.,\nproposal_fast, proposal, bbox, segm are available for COCO, mAP, recall for PASCAL VOC. Cityscapes\ncould be evaluated by cityscapes as well as all COCO metrics.\n\n--show: If specified, detection results will be plotted on the images and shown in a new window. It is only\napplicable to single GPU testing and used for debugging and visualization. Please make sure that GUI is available\nin your environment. Otherwise, you may encounter an error like cannot connect to X server.\n\n--show-dir: If specified, detection results will be plotted on the images and saved to the specified directory.\nIt is only applicable to single GPU testing and used for debugging and visualization. You do NOT need a GUI\navailable in your environment for using this option.\n\n--show-score-thr: If specified, detections with scores below this threshold will be removed.\n--cfg-options: if specified, the key-value pair optional cfg will be merged into config file\n\n--eval-options: if specified, the key-value pair optional eval cfg will be kwargs for dataset.evaluate() function,\nit’s only for evaluation\n\n22\n\nChapter 5. 1: Inference and train with existing models and standard datasets\n\n", "vlm_text": "5.2.2 Test existing models \nWe provide testing scripts for evaluating an existing model on the whole dataset (COCO, PASCAL VOC, Cityscapes, etc.). The following testing environments are supported: \n• single GPU • single node multiple GPUs • multiple nodes \nChoose the proper script to perform testing depending on the testing environment. \nThe image contains text that appears to be a script or command line instructions for testing some software or model on a computer system. It includes commands for both single-GPU and multi-GPU testing.\n\n1. **Single-GPU Testing**:\n    - The command used is a Python script: `python tools/test.py`\n    - It takes several arguments in the form of placeholders:\n        - `${CONFIG_FILE}`: The configuration file required for testing.\n        - `${CHECKPOINT_FILE}`: The checkpoint file of the model.\n        - `[--out ${RESULT_FILE}]`: An optional argument to specify the output result file.\n        - `[--eval ${EVAL_METRICS}]`: An optional argument for evaluation metrics.\n        - `[--show]`: An optional flag to display additional information.\n\n2. **Multi-GPU Testing**:\n    - The command used is a bash script: `bash tools/dist_test.sh`\n    - It similarly takes several arguments:\n        - `${CONFIG_FILE}`: The configuration file required for testing.\n        - `${CHECKPOINT_FILE}`: The checkpoint file of the model.\n        - `${GPU_NUM}`: The number of GPUs to use.\n        - `[--out ${RESULT_FILE}]`: An optional argument to specify the output result file.\n        - `[--eval ${EVAL_METRICS}]`: An optional argument for evaluation metrics.\n\nThese commands are likely used in the context of machine learning or deep learning experiments for testing models.\ntools/dist_test.sh  also supports multi-node testing, but relies on PyTorch’s  launch utility . \nOptional arguments: \n•  RESULT FILE : Filename of the output results in pickle format. If not specified, the results will not be saved to a file. \n•  E VAL METRICS : Items to be evaluated on the results. Allowed values depend on the dataset, e.g., proposal fast ,  proposal ,  bbox ,  segm  are available for COCO,  mAP ,  recall  for PASCAL VOC. Cityscapes could be evaluated by  cityscapes  as well as all COCO metrics. •  --show : If specified, detection results will be plotted on the images and shown in a new window. It is only applicable to single GPU testing and used for debugging and visualization. Please make sure that GUI is available in your environment. Otherwise, you may encounter an error like  cannot connect to X server . •  --show-dir : If specified, detection results will be plotted on the images and saved to the specified directory. It is only applicable to single GPU testing and used for debugging and visualization. You do NOT need a GUI available in your environment for using this option. •  --show-score-thr : If specified, detections with scores below this threshold will be removed. •  --cfg-options : if specified, the key-value pair optional cfg will be merged into config file \n•  --eval-options : if specified, the key-value pair optional eval cfg will be kwargs for dataset.evaluate() function, it’s only for evaluation "}
{"page": 30, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_30.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n5.2.3 Examples\n\nAssuming that you have already downloaded the checkpoints to the directory checkpoints/.\n\n1. Test Faster R-CNN and visualize the results. Press any key for the next image. Config and checkpoint files are\navailable here.\n\npython tools/test.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n--show\n\n2. Test Faster R-CNN and save the painted images for future visualization. Config and checkpoint files are available\nhere.\n\npython tools/test.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n--show-dir faster_rcnn_r50_fpn_1x_results\n\n3. Test Faster R-CNN on PASCAL VOC (without saving the test results) and evaluate the mAP. Config and check-\npoint files are available here.\n\npython tools/test.py \\\nconfigs/pascal_voc/faster_rcnn_r50_fpn_1x_voc.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_voc0712_20200624-c9895d40.pth \\\n--eval mAP\n\n4. Test Mask R-CNN with 8 GPUs, and evaluate the bbox and mask AP. Config and checkpoint files are available\nhere.\n\n./tools/dist_test.sh \\\nconfigs/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n8 \\\n--out results.pkl \\\n--eval bbox segm\n\n5. Test Mask R-CNN with 8 GPUs, and evaluate the classwise bbox and mask AP. Config and checkpoint files are\navailable here.\n\n./tools/dist_test.sh \\\nconfigs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n8 \\\n\n--out results.pkl \\\n--eval bbox segm \\\n--options \"classwise=True\"\n\n6. Test Mask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files for submitting to the official evalu-\nation server. Config and checkpoint files are available here.\n\n./tools/dist_test.sh \\\nconfigs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n\n(continues on next page)\n\n5.2. Test existing models on standard datasets 23\n", "vlm_text": "5.2.3 Examples \nAssuming that you have already downloaded the checkpoints to the directory  checkpoints/ . \n1. Test Faster R-CNN and visualize the results. Press any key for the next image. Config and checkpoint files are available  here . \npython tools/test.py  \\ configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --show \n2. Test Faster R-CNN and save the painted images for future visualization. Config and checkpoint files are available here . \nThe image displays a command being executed in a terminal. This command is used to run a Python script from a set of machine learning tools, likely related to object detection using a model such as Faster R-CNN. Here's a breakdown of the command:\n\n1. `python tools/test.py \\`: This is the command to run a Python script named `test.py`, which is located in the `tools` directory.\n\n2. `configs/faster_rcnn/faster_rcnn_r50_fpn_1x.py \\`: This refers to a configuration file likely used to set up the Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN) with a 1x schedule.\n\n3. `checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\`: This specifies the path to a pre-trained model checkpoint file. The file is named in such a way that it indicates a Faster R-CNN model with ResNet-50 and FPN, pre-trained on the COCO dataset, with a date stamp of 2020-01-30.\n\n4. `--show-dir faster_rcnn_r50_fpn_1x_results`: This specifies the directory where the results of this test or evaluation will be saved or displayed.\n3. Test Faster R-CNN on PASCAL VOC (without saving the test results) and evaluate the mAP. Config and check- point files are available  here . \npython tools/test.py  \\ configs/pascal_voc/faster r cnn r 50 fp n 1 x voc.py  \\ checkpoints/faster r cnn r 50 fp n 1 x voc 0712 20200624-c9895d40.pth  \\ --eval mAP \n4. Test Mask R-CNN with 8 GPUs, and evaluate the bbox and mask AP. Config and checkpoint files are available here . \nThe image shows a command line input for testing a model using a script. The command is structured as follows:\n\n- `./tools/dist_test.sh`: This is the test script being executed.\n- `configs/mask_rcnn_r50_fpn_1x_coco.py`: This is the configuration file for testing, specifying the Mask R-CNN model with ResNet-50 and FPN on the COCO dataset.\n- `checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth`: This is the file path to the pre-trained model checkpoint.\n- `8`: Indicates the number of GPUs to be used for testing.\n- `--out results.pkl`: This flag specifies the output file where the results will be saved, in this case, `results.pkl`.\n- `--eval bbox segm`: This flag specifies the evaluation metrics, which in this case are bounding box and segmentation.\n5. Test Mask R-CNN with 8 GPUs, and evaluate the  classwise  bbox and mask AP. Config and checkpoint files are available  here . \nThe image displays a command-line script used to run a distributed test for object detection and segmentation using Mask R-CNN. The command is executed using the `dist_test.sh` script located in the `tools` directory. The configuration file used is `mask_rcnn_r50_fpn_1x_coco.py` from the `configs/mask_rcnn` directory, and the checkpoint file is `mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth` from the `checkpoints` directory. It specifies 8 GPUs for distributed testing (`8 \\`). The output will be saved to `results.pkl`. The evaluation metrics are bounding box (bbox) and segmentation (segm). An additional option to evaluate results class-wise is set as `\"classwise=True\"`.\n6. Test Mask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files for submitting to the official evalu- ation server. Config and checkpoint files are available  here . \nThe image shows a command-line script being executed. It runs a shell script `dist_test.sh` from the `tools` directory with the following arguments:\n\n1. `configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py`: A configuration file for Mask R-CNN using a ResNet-50 backbone with FPN on the COCO dataset.\n2. `checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth`: A checkpoint file for the Mask R-CNN model, which likely contains pre-trained model weights."}
{"page": 31, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_31.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n8 \\\n--format-only \\\n--options \"jsonfile_prefix=./mask_rcnn_test-dev_results\"\n\nThis command generates two JSON files mask_rcnn_test-dev_results.bbox.json and\nmask_rcnn_test-dev_results.segm. json.\n\n7. Test Mask R-CNN on Cityscapes test with 8 GPUs, and generate txt and png files for submitting to the official\nevaluation server. Config and checkpoint files are available here.\n\n./tools/dist_test.sh \\\nconfigs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227-afe51d5a.pth \\\n8 \\\n\n--format-only \\\n--options \"txtfile_prefix=./mask_rcnn_cityscapes_test_results\"\n\nThe generated png and txt would be under . /mask_rcnn_cityscapes_test_results directory.\n\n5.2.4 Test without Ground Truth Annotations\n\nMMbDetection supports to test models without ground-truth annotations using CocoDataset. If your dataset format\nis not in COCO format, please convert them to COCO format. For example, if your dataset format is VOC, you can\ndirectly convert it to COCO format by the script in tools. If your dataset format is Cityscapes, you can directly convert\nit to COCO format by the script in tools. The rest of the formats can be converted using this script.\n\npython tools/dataset_converters/images2coco.py \\\n${IMG_PATH} \\\n${CLASSES} \\\n${OUT} \\\n\n[--exclude-extensions]\n\narguments\n* IMG_PATH: The root path of images.\n* CLASSES: The text file with a list of categories.\n* OUT: The output annotation json file name. The save dir is in the same directory as IMG_PATH.\n* exclude-extensions: The suffix of images to be excluded, such as ‘png’ and ‘bmp’.\n\nAfter the conversion is complete, you can use the following command to test\n\n# single-gpu testing\n\npython tools/test.py \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n--format-only \\\n--options §{JSONFILE_PREFIX} \\\n[--show]\n\n# multi-gpu testing\nbash tools/dist_test.sh \\\n${CONFIG_FILE} \\\n\n(continues on next page)\n\n24 Chapter 5. 1: Inference and train with existing models and standard datasets\n\n", "vlm_text": "The image shows a snippet of a command or script involving some options and parameters. It includes:\n\n- A line number: `8 \\`\n- Command line arguments:\n  - `--format-only \\`\n  - `--options \"jsonfile_prefix=./mask_rcnn_test-dev_results\"`\n\nThis command appears to be related to processing or configuring results for a task, possibly in data processing or machine learning, indicated by the mention of `mask_rcnn`.\nThis command generates two JSON files mask r cnn test-dev results.bbox.json and mask r cnn test-dev results.segm.json . \n7. Test Mask R-CNN on Cityscapes test with 8 GPUs, and generate txt and png files for submitting to the official evaluation server. Config and checkpoint files are available  here . \nThis image contains a command-line script that is used for testing a model using a distributed testing script. The command provides the paths to both the model configuration file and the checkpoint file. Here’s a breakdown of the command:\n\n- `./tools/dist_test.sh`: This script is used for executing distributed testing.\n\n- `configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py`: This is the path to the configuration file for the Mask R-CNN model, which is configured for use on the Cityscapes dataset with a ResNet-50 backbone and a Feature Pyramid Network (FPN).\n\n- `checkpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227-afe51d5a.pth`: This is the path to the checkpoint file containing the pre-trained weights for the model.\n\n- `8`: This number likely indicates the number of GPUs to be used for the distributed test.\n\n- `--format-only`: A flag indicating that the script should run in a format-only mode, typically used to evaluate detection results without computing evaluation metrics.\n\n- `--options \"txtfile_prefix=./mask_rcnn_cityscapes_test_results\"`: This option specifies output options, such as the prefix for the text files generated as results. In this case, the results will be prefixed with `./mask_rcnn_cityscapes_test_results`.\n\nThe command is structured and formatted for ease of reading when extended over multiple lines using backslashes (`\\`).\nThe generated png and txt would be under  ./mask r cnn cityscape s test results  directory. \n5.2.4 Test without Ground Truth Annotations \nMM Detection supports to test models without ground-truth annotations using  Coco Data set . If your dataset format is not in COCO format, please convert them to COCO format. For example, if your dataset format is VOC, you can directly convert it to COCO format by the  script in tools.  If your dataset format is Cityscapes, you can directly convert it to COCO format by the  script in tools.  The rest of the formats can be converted using  this script . \nThe table contains a command for running a Python script, likely used for converting images to COCO dataset format. Here's the command:\n\n```bash\npython tools/dataset_converters/images2coco.py \\\n    ${IMG_PATH} \\\n    ${CLASSES} \\\n    ${OUT} \\\n    [--exclude-extensions]\n```\n\n- `${IMG_PATH}`: Placeholder for the path to the images.\n- `${CLASSES}`: Placeholder for the classes or labels.\n- `${OUT}`: Placeholder for the output directory or file.\n- `[--exclude-extensions]`: An optional argument to exclude certain file extensions.\narguments \n•  IMG_PATH : The root path of images. •  CLASSES : The text file with a list of categories. •  OUT : The output annotation json file name. The save dir is in the same directory as  IMG_PATH . •  exclude-extensions : The suffix of images to be excluded, such as ‘png’ and ‘bmp’. \nAfter the conversion is complete, you can use the following command to test \nThe image displays a snippet of code that provides commands for testing a machine learning model using Python and shell scripts. It covers two scenarios: single-GPU and multi-GPU testing.\n\n1. **Single-GPU Testing**:\n   - The command begins with `python tools/test.py`, indicating the usage of a Python script located in the `tools` directory.\n   - It takes in several arguments:\n     - `${CONFIG_FILE}`: A placeholder for the configuration file.\n     - `${CHECKPOINT_FILE}`: A placeholder for the model's checkpoint file.\n     - `--format-only`: A flag indicating that the script should run in a mode where no evaluations are conducted, possibly just formatting.\n     - `--options ${JSONFILE_PREFIX}`: An option to specify JSON file-related settings.\n     - `[--show]`: An optional flag for displaying results.\n\n2. **Multi-GPU Testing**:\n   - The command begins with `bash tools/dist_test.sh`, indicating the usage of a shell script for distributed testing.\n   - It appears to take in one argument, `${CONFIG_FILE}`, which is a placeholder for the configuration file.\n\nThe code snippets are accompanied by comments that describe their purpose. The image's content is visually separated, with blue text denoting the actual command elements and purple text for placeholders or variable parts of the commands."}
{"page": 32, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_32.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n${CHECKPOINT_FILE} \\\n${GPU_NUM} \\\n\n--format-only \\\n\n--options §{JSONFILE_PREFIX} \\\n[--show]\n\nAssuming that the checkpoints in the model zoo have been downloaded to the directory checkpoints/, we can test\nMask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files using the following command.\n\n./tools/dist_test.sh \\\nconfigs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n8\\\n\n-format-only \\\n--options \"jsonfile_prefix=./mask_rcnn_test-dev_results\"\n\nThis command — generates two JSON files mask_rcnn_test-dev_results.bbox.json and\nmask_rcnn_test-dev_results.segm. json.\n\n5.2.5 Batch Inference\n\nMMDetection supports inference with a single image or batched images in test mode. By default, we use single-image\ninference and you can use batch inference by modifying samples_per_gpu in the config of test data. You can do that\neither by modifying the config as below.\n\ndata = dict(train=dict(...), val=dict(...), test=dict(samples_per_gpu=2, ...))\n\nOr you can set it through --cfg-options as --cfg-options data.test.samples_per_gpu=2\n\n5.2.6 Deprecated ImageToTensor\n\nIn test mode, ImageToTensor pipeline is deprecated, it’s replaced by DefaultFormatBund1le that recommended to\nmanually replace it in the test data pipeline in your config file. examples:\n\n# use ImageToTensor (deprecated)\npipelines = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True) ,\ndict(type='RandomFlip'),\ndict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n)\n\n(continues on next page)\n\n5.2. Test existing models on standard datasets 25\n\n", "vlm_text": "The image is a screenshot of a command-line snippet or script. It seems to be a part of a larger command related to some computational task, possibly involving machine learning or data processing. The placeholders or variables include `${CHECKPOINT_FILE}`, `${GPU_NUM}`, and `${JSONFILE_PREFIX}`, suggesting these values need to be substituted with actual file paths or numbers before running the command. The options `--format-only`, `--options`, and optionally `--show` indicate selectable flags or parameters to control the behavior of whatever process this command runs. The `\\` at the end of each line signifies line continuation, likely in a shell or script that spans multiple lines for better readability.\nAssuming that the checkpoints in the  model zoo  have been downloaded to the directory  checkpoints/ , we can test Mask R-CNN on COCO test-dev with 8 GPUs, and generate JSON files using the following command. \n./tools/dist_test.sh  \\ configs/mask_rcnn/mask r cnn r 50 fp n 1 x coco.py  \\ checkpoints/mask r cnn r 50 fp n 1 x coco 20200205-d4b0c5d6.pth  \\ 8  \\ -format-only \\--options  \"json file prefix  $=$  ./mask r cnn test-dev results\" \nThis command generates two JSON files mask r cnn test-dev results.bbox.json and mask r cnn test-dev results.segm.json . \n5.2.5 Batch Inference \nMM Detection supports inference with a single image or batched images in test mode. By default, we use single-image inference and you can use batch inference by modifying  samples per gpu  in the config of test data. You can do that either by modifying the config as below. \nThe image contains a snippet of Python code. It defines a dictionary called `data` with three keys: `train`, `val`, and `test`. Each key is associated with another dictionary. The `test` dictionary includes a parameter `samples_per_gpu` set to 2. The ellipses (`...`) indicate that additional content may be included in the actual code.\nOr you can set it through  --cfg-options  as  --cfg-options data.test.samples per gpu  $^{=2}$  \n5.2.6 Deprecated Image To Tensor \nIn test mode,  Image To Tensor  pipeline is deprecated, it’s replaced by  Default Format Bundle  that recommended to manually replace it in the test data pipeline in your config file. examples: \nThe table contains Python code for image processing using pipelines. Here's a breakdown of the code:\n\n1. **LoadImageFromFile**: Loads an image from a file.\n\n2. **MultiScaleFlipAug**: Handles multi-scale and flip augmentation.\n   - `img_scale=(1333, 800)`: The target image scale.\n   - `flip=False`: Disables image flipping.\n   - `transforms`: A list of transformations:\n     - **Resize**: Resizes the image while keeping the aspect ratio.\n     - **RandomFlip**: Randomly flips the image.\n     - **Normalize**: Normalizes the image using the given mean and std deviation (`mean=[0, 0, 0]`, `std=[1, 1, 1]`).\n     - **Pad**: Pads the image to a specified size divisor (`size_divisor=32`).\n     - **ImageToTensor**: Converts the image to a tensor.\n     - **Collect**: Collects the image into a batch.\n\nThe pipeline is setup to load, augment, and preprocess images for model training or inference."}
{"page": 33, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_33.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n# manually replace ImageToTensor to DefaultFormatBundle (recommended)\npipelines = [\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True) ,\ndict(type='RandomFlip'),\ndict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict (type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img']),\n)\n\n5.3 Train predefined models on standard datasets\n\nMMDetection also provides out-of-the-box tools for training detection models. This section will show how to train\npredefined models (under configs) on standard datasets i.e. COCO.\n\nImportant: The default learning rate in config files is for 8 GPUs and 2 img/gpu (batch size = 8*2 = 16). According\nto the linear scaling rule, you need to set the learning rate proportional to the batch size if you use different GPUs or\nimages per GPU, e.g., 1r=0.01 for 4 GPUs * 2 imgs/gpu and 1r=0.08 for 16 GPUs * 4 imgs/gpu.\n\n5.3.1 Prepare datasets\n\nTraining requires preparing datasets too. See section Prepare datasets above for details.\n\nNote: Currently, the config files under configs/cityscapes use COCO pretrained weights to initialize. You could\ndownload the existing models in advance if the network connection is unavailable or slow. Otherwise, it would cause\nerrors at the beginning of training.\n\n5.3.2 Training on a single GPU\n\nWe provide tools/train. py to launch training jobs on a single GPU. The basic usage is as follows.\n\npython tools/train.py \\\n${CONFIG_FILE} \\\n{optional arguments]\n\nDuring training, log files and checkpoints will be saved to the working directory, which is specified by work_dir in\nthe config file or via CLI argument --work-dir.\n\nBy default, the model is evaluated on the validation set every epoch, the evaluation interval can be specified in the\nconfig file as shown below.\n\n# evaluate the model every 12 epoch.\nevaluation = dict(interval=12)\n\n26 Chapter 5. 1: Inference and train with existing models and standard datasets\n\n", "vlm_text": "The image shows a snippet of Python code configuring a data processing pipeline, likely for deep learning purposes. Here's a breakdown of the code:\n\n- **pipelines** is a list of dictionaries, each representing a different stage in the data processing pipeline.\n\n1. **LoadImageFromFile**: Loads an image from a file.\n\n2. **MultiScaleFlipAug**: Applies multiple augmentations:\n   - `img_scale`: Sets the image scale to (1333, 800).\n   - `flip`: Sets the flip operation to False.\n   - `transforms`: Contains a list of transformations to apply:\n     - **Resize**: Resizes the image, keeping the aspect ratio.\n     - **RandomFlip**: Randomly flips the image.\n     - **Normalize**: Normalizes the image using mean [0, 0, 0] and std [1, 1, 1].\n     - **Pad**: Pads the image to a size divisible by 32.\n     - **DefaultFormatBundle**: Formats data for training.\n     - **Collect**: Collects the keys, with 'img' specified here.\n\nThe comment suggests replacing `ImageToTensor` with `DefaultFormatBundle`.\n5.3 Train predefined models on standard datasets \nMM Detection also provides out-of-the-box tools for training detection models. This section will show how to train predefined  models (under  configs ) on standard datasets i.e. COCO. \nImportant : The default learning rate in config files is for 8 GPUs and  $2{\\mathrm{~img/gmu}}$   (batch   ${\\mathrm{size}}=8^{*}2=16$  ). According to the  linear scaling rule , you need to set the learning rate proportional to the batch size if you use different GPUs or images per GPU, e.g.,  $\\scriptstyle{1\\!\\mathrm{r}=\\!\\varnothing.\\varnothing1}$   for 4 GPUs   $^{*}\\,2$   imgs/gpu and  $\\scriptstyle1r=0.08$   for 16 GPUs  $^{\\ast}\\,4$   imgs/gpu. \n5.3.1 Prepare datasets \nTraining requires preparing datasets too. See section  Prepare datasets  above for details. \nNote : Currently, the config files under  configs/cityscapes  use COCO pretrained weights to initialize. You could download the existing models in advance if the network connection is unavailable or slow. Otherwise, it would cause errors at the beginning of training. \n5.3.2 Training on a single GPU \nWe provide  tools/train.py  to launch training jobs on a single GPU. The basic usage is as follows. \nThe image contains a command line snippet for running a Python script. The command is:\n\n```\npython tools/train.py \\\n    ${CONFIG_FILE} \\\n    [optional arguments]\n```\n\nThis command is likely used to execute a training script named `train.py` located in the `tools` directory. It requires a configuration file specified by the placeholder `${CONFIG_FILE}` and can accept optional arguments indicated by `[optional arguments]`. The use of backslashes (`\\`) at the end of the lines suggests that the command can be written across multiple lines for readability.\nDuring training, log files and checkpoints will be saved to the working directory, which is specified by  work_dir  in the config file or via CLI argument  --work-dir . \nBy default, the model is evaluated on the validation set every epoch, the evaluation interval can be specified in the config file as shown below. "}
{"page": 34, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_34.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nThis tool accepts several optional arguments, including:\n* --no-validate (not suggested): Disable evaluation during training.\n« --work-dir ${WORK_DIR}: Override the working directory.\n* --resume-from ${CHECKPOINT_FILE}: Resume from a previous checkpoint file.\n* --options 'Key=value': Overrides other settings in the used config.\nNote:\nDifference between resume-from and load-from:\n\nresume-from loads both the model weights and optimizer status, and the epoch is also inherited from the specified\ncheckpoint. It is usually used for resuming the training process that is interrupted accidentally. load-from only loads\nthe model weights and the training epoch starts from 0. It is usually used for finetuning.\n\n5.3.3 Training on multiple GPUs\n\nWe provide tools/dist_train. sh to launch training on multiple GPUs. The basic usage is as follows.\n\nbash ./tools/dist_train.sh \\\n${CONFIG_FILE} \\\n${GPU_NUM} \\\n{optional arguments]\n\nOptional arguments remain the same as stated above.\n\nLaunch multiple jobs simultaneously\n\nIf you would like to launch multiple jobs on a single machine, e.g., 2 jobs of 4-GPU training on a machine with 8 GPUs,\nyou need to specify different ports (29500 by default) for each job to avoid communication conflict.\n\nIf you use dist_train. sh to launch training jobs, you can set the port in commands.\n\nCUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh ${CONFIG_FILE} 4\nCUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 ./tools/dist_train.sh ${CONFIG_FILE} 4\n\n5.3.4 Training on multiple nodes\n\nMMbDetection relies on torch. distributed package for distributed training. Thus, as a basic usage, one can launch\ndistributed training via PyTorch’s launch utility.\n\n5.3.5 Manage jobs with Slurm\n\nSlurm is a good job scheduling system for computing clusters. On a cluster managed by Slurm, you can use\nslurm_train. sh to spawn training jobs. It supports both single-node and multi-node training.\n\nThe basic usage is as follows.\n\n[GPUS=${GPUS}] ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR}\n\nBelow is an example of using 16 GPUs to train Mask R-CNN on a Slurm partition named dev, and set the work-dir to\nsome shared file systems.\n\n5.3. Train predefined models on standard datasets 27\n\n", "vlm_text": "This tool accepts several optional arguments, including: \n•  --no-validate  ( not suggested ): Disable evaluation during training. •  --work-dir \\${WORK_DIR} : Override the working directory. •  --resume-from \\${CHECKPOINT FILE} : Resume from a previous checkpoint file. •  --options  ' Key  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  value ' : Overrides other settings in the used config. \nNote : \nDifference between  resume-from  and  load-from : \nresume-from  loads both the model weights and optimizer status, and the epoch is also inherited from the specified checkpoint. It is usually used for resuming the training process that is interrupted accidentally.  load-from  only loads the model weights and the training epoch starts from 0. It is usually used for finetuning. \n5.3.3 Training on multiple GPUs \nWe provide  tools/dist_train.sh  to launch training on multiple GPUs. The basic usage is as follows. \nThe table contains a shell command for running a distributed training script. Here's the command:\n\n```bash\nbash ./tools/dist_train.sh \\\n${CONFIG_FILE} \\\n${GPU_NUM} \\\n[optional arguments]\n```\n\n- `CONFIG_FILE`: Placeholder for the configuration file.\n- `GPU_NUM`: Placeholder for the number of GPUs to use.\n- `[optional arguments]`: Placeholder for any optional arguments.\nOptional arguments remain the same as stated  above . \nLaunch multiple jobs simultaneously \nIf you would like to launch multiple jobs on a single machine, e.g., 2 jobs of 4-GPU training on a machine with 8 GPUs, you need to specify different ports (29500 by default) for each job to avoid communication conflict. \nIf you use  dist_train.sh  to launch training jobs, you can set the port in commands. \nThe table contains two command-line entries, each for executing a distributed training script (`dist_train.sh`). The configurations specified in each line are:\n\n1. The `CUDA_VISIBLE_DEVICES` environment variable is set to specific GPU device IDs:\n   - In the first line, devices 0, 1, 2, and 3 are used.\n   - In the second line, devices 4, 5, 6, and 7 are used.\n\n2. The `PORT` environment variable is set with unique port numbers for each line:\n   - In the first line, the port is set to 29500.\n   - In the second line, the port is set to 29501.\n\n3. Both lines execute the script `./tools/dist_train.sh` with the same command-line arguments:\n   - They both use the `${CONFIG_FILE}` placeholder, likely representing a configuration file for the training.\n   - '4' is likely the number of GPUs or some other parameter related to training.\n\nThis setup seems to be for running parallel distributed training jobs with different subsets of GPUs and port configurations.\n5.3.4 Training on multiple nodes \nMM Detection relies on  torch.distributed  package for distributed training. Thus, as a basic usage, one can launch distributed training via PyTorch’s  launch utility . \n5.3.5 Manage jobs with Slurm \nSlurm  is a good job scheduling system for computing clusters. On a cluster managed by Slurm, you can use slur m train.sh  to spawn training jobs. It supports both single-node and multi-node training. \nThe basic usage is as follows. \nThe image shows a command line script for executing a shell script using SLURM, a job scheduler for Linux. The command is:\n\n```\nGPUS=${GPUS} ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR}\n```\n\nVariables such as `${GPUS}`, `${PARTITION}`, `${JOB_NAME}`, `${CONFIG_FILE}`, and `${WORK_DIR}` are placeholders likely meant to be replaced with actual values when running the script.\nBelow is an example of using 16 GPUs to train Mask R-CNN on a Slurm partition named  dev , and set the work-dir to some shared file systems. "}
{"page": 35, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_35.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nGPUS=16 ./tools/slurm_train.sh dev mask_r50_1x configs/mask_rcnn_r50_fpn_1x_coco.py /nfs/\n> Xxxx/mask_rcnn_r50_fpn_1x\n\nYou can check the source code to review full arguments and environment variables.\nWhen using Slurm, the port option need to be set in one of the following ways:\n\n1. Set the port through --options. This is more recommended since it does not change the original configs.\n\nCUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME}.,\n—configl.py ${WORK_DIR} --options 'dist_params.port=29500'\nCUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME}.,\n—config2.py ${WORK_DIR} --options 'dist_params.port=29501'\n\n2. Modify the config files to set different communication ports.\n\nIn config1.py, set\n\ndist_params = dict(backend='nccl', port=29500)\n\nIn config2.py, set\n\ndist_params = dict(backend='nccl', port=29501)\n\nThen you can launch two jobs with config1.py and config2.py.\n\nCUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME}.,\nsconfigl.py ${WORK_DIR}\nCUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME}.,\n—config2.py ${WORK_DIR}\n\n28 Chapter 5. 1: Inference and train with existing models and standard datasets\n", "vlm_text": "GPUS = 16  ./tools/slur m train.sh dev mask r 50 1 x configs/mask r cnn r 50 fp n 1 x coco.py /nfs/ xxxx/mask r cnn r 50 fp n 1 x ˓ → \nYou can check  the source code  to review full arguments and environment variables. When using Slurm, the port option need to be set in one of the following ways: 1. Set the port through  --options . This is more recommended since it does not change the original configs. \n\n\nThe image shows two command lines for running scripts with specified GPU devices using SLURM and CUDA:\n\n1. **First Command:**\n   - **CUDA_VISIBLE_DEVICES=0,1,2,3**: Specifies which GPUs to use.\n   - **GPUS=4**: Number of GPUs allocated.\n   - **Script**: `./tools/slurm_train.sh`\n   - **Parameters**: `${PARTITION}` and `${JOB_NAME}`\n   - **Configuration**: `config1.py`\n   - **Options**: `--options 'dist_params.port=29500'`\n\n2. **Second Command:**\n   - **CUDA_VISIBLE_DEVICES=4,5,6,7**: Specifies a different set of GPUs.\n   - **GPUS=4**: Number of GPUs allocated.\n   - **Script**: `./tools/slurm_train.sh`\n   - **Parameters**: `${PARTITION}` and `${JOB_NAME}`\n   - **Configuration**: `config2.py`\n   - **Options**: `--options 'dist_params.port=29501'`\n2. Modify the config files to set different communication ports. \nIn  config1.py , set \nThe image shows a piece of Python code that initializes a dictionary named `dist_params` with two key-value pairs. The `backend` key is assigned the value `'nccl'`, and the `port` key is assigned the value `29500`.\nIn  config2.py , set \ndist params  $=$   dict (backend = ' nccl ' , port = 29501 ) \nThen you can launch two jobs with  config1.py  and  config2.py \nCUDA VISIBLE DEVICES  $\\scriptstyle=\\0$  ,1,2,3  GPUS  $_{:=4}$   ./tools/slur m train.sh    $\\mathcal{S}$  { PARTITION } \\${ JOB_NAME } ␣  $\\hookrightarrow$  config1.py  \\${ WORK_DIR } → CUDA VISIBLE DEVICES  ${=}4$  ,5,6,7  GPUS  $_{:=4}$   ./tools/slur m train.sh  \\${ PARTITION } \\${ JOB_NAME } ␣  $\\hookrightarrow$  config2.py  \\${ WORK_DIR } → "}
{"page": 36, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_36.jpg", "ocr_text": "CHAPTER\nSIX\n\n2: TRAIN WITH CUSTOMIZED DATASETS\n\nIn this note, you will know how to inference, test, and train predefined models with customized datasets. We use the\nballoon dataset as an example to describe the whole process.\n\nThe basic steps are as below:\n1. Prepare the customized dataset\n2. Prepare a config\n\n3. Train, test, inference models on the customized dataset.\n\n6.1 Prepare the customized dataset\n\nThere are three ways to support a new dataset in MMDetection:\n\n1. reorganize the dataset into COCO format.\n\n2. reorganize the dataset into a middle format.\n\n3. implement a new dataset.\nUsually we recommend to use the first two methods which are usually easier than the third.\nIn this note, we give an example for converting the data into COCO format.\n\nNote: MMDetection only supports evaluating mask AP of dataset in COCO format for now. So for instance segmen-\ntation task users should convert the data into coco format.\n\n6.1.1 COCO annotation format\n\nThe necessary keys of COCO format for instance segmentation is as below, for the complete details, please refer here.\n\n{\n\"images\": [image],\n\"annotations\": [annotation],\n\"categories\": [category]\n\nimage = {\n\"id\": int,\n\"width\": int,\n\"height\": int,\n\n(continues on next page)\n\n29\n\n", "vlm_text": "2: TRAIN WITH CUSTOMIZED DATASETS \nIn this note, you will know how to inference, test, and train predefined models with customized datasets. We use the balloon dataset  as an example to describe the whole process. \nThe basic steps are as below: \n1. Prepare the customized dataset 2. Prepare a config 3. Train, test, inference models on the customized dataset. \n6.1 Prepare the customized dataset \nThere are three ways to support a new dataset in MM Detection: \n1. reorganize the dataset into COCO format. 2. reorganize the dataset into a middle format. 3. implement a new dataset. \nUsually we recommend to use the first two methods which are usually easier than the third. \nIn this note, we give an example for converting the data into COCO format. \nNote : MM Detection only supports evaluating mask AP of dataset in COCO format for now. So for instance segmen- tation task users should convert the data into coco format. \n6.1.1 COCO annotation format \nThe necessary keys of COCO format for instance segmentation is as below, for the complete details, please refer  here . \n\"images\": [image], \"annotations\": [annotation], \"categories\": [category] \nimage = { \"id\": int, \"width\": int, \"height\": int, "}
{"page": 37, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_37.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n“file_name\": str,\n\n}\n\nannotation = {\n\"id\": int,\n“image_id\": int,\n\"“category_id\": int,\n\"segmentation\": RLE or [polygon],\n\"area\": float,\n“pbox\": [x,y,width,height],\n“iscrowd\": 0 or 1,\n\n}\n\ncategories = [{\n\"id\": int,\n\"name\": str,\n“supercategory\": str,\n\n3]\n\nAssume we use the balloon dataset. After downloading the data, we need to implement a function to convert the\nannotation format into the COCO format. Then we can use implemented COCODataset to load the data and perform\ntraining and evaluation.\n\nIf you take a look at the dataset, you will find the dataset format is as below:\n\n{'base64_img_data': ;\n\n\"file_attributes': {},\n\n\"filename': '34020010494_e5cb88e1c4_k.jpg',\n\n\"fileref': '',\n\n'regions': {'0': {'region_attributes': {},\n\n‘shape_attributes': {'all_points_x': [1020,\n\n1000,\n994,\n1003,\n1023,\n1050,\n1089,\n1134,\n1190,\n1265,\n1321,\n1361,\n1403,\n1428,\n1442,\n1445,\n1441,\n1427,\n1400,\n1361,\n1316,\n1269,\n1228,\n\n(continues on next page)\n\n30 Chapter 6. 2: Train with customized datasets\n\n", "vlm_text": "The table contains a data structure typically used in image annotations within datasets.\n\n1. **Image Information**:\n   - `\"file_name\": str`: Represents the file name as a string.\n\n2. **Annotation**:\n   - `\"id\": int`: An integer identifier for the annotation.\n   - `\"image_id\": int`: An integer identifier linking the annotation to a specific image.\n   - `\"category_id\": int`: An integer identifier linking to a specific category.\n   - `\"segmentation\": RLE or [polygon]`: Defines segmentation, either as Run-Length Encoding (RLE) or polygon coordinates.\n   - `\"area\": float`: A floating-point number representing the area of the annotation.\n   - `\"bbox\": [x, y, width, height]`: A list defining the bounding box with x and y coordinates, along with width and height.\n   - `\"iscrowd\": 0 or 1`: A binary value indicating if the annotation is for a crowd.\n\n3. **Categories**:\n   - `categories`: A list containing category information.\n   - `\"id\": int`: An integer identifier for the category.\n   - `\"name\": str`: The name of the category as a string.\n   - `\"supercategory\": str`: The supercategory name as a string.\nAssume we use the balloon dataset. After downloading the data, we need to implement a function to convert the annotation format into the COCO format. Then we can use implemented COCO Data set to load the data and perform training and evaluation. \nIf you take a look at the dataset, you will find the dataset format is as below: \nThis table appears to contain a snippet of JSON data, likely used for image annotation or analysis purposes. Here's a breakdown of the contents:\n\n- `'base64_img_data'`: An empty string, which might be intended to hold base64 encoded image data.\n- `'file_attributes'`: An empty object, probably meant for storing additional metadata about the file.\n- `'filename'`: The name of the image file, `'34020010494_e5cb88e1c4_k.jpg'`.\n- `'fileref'`: An empty string, possibly meant to reference the file location.\n- `'regions'`: An object that contains regions within the image. In this case, there is one region (`'0'`), which includes:\n  - `'region_attributes'`: An empty object that could store specific attributes of the region.\n  - `'shape_attributes'`: Contains the key `'all_points_x'`, an array of x-coordinates that likely define a shape or path within the image.\n\nThe table essentially represents a structure that could be used in image annotation software for defining shapes or regions of interest in an image file."}
{"page": 38, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_38.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n1198,\n1207,\n1210,\n1190,\n77,\n72,\n74,\n70,\n53,\n27,\n04,\n1061,\n1032,\n1020],\nall_points_y': [963,\n899,\n841,\n787,\n738,\n700,\n663,\n638,\n621,\n619,\n643,\n672,\n720,\n765,\n800,\n860,\n896,\n942,\n990,\n1035,\n1079,\n12,\n29,\n34,\n44,\n53,\n66,\n66,\n50,\n36,\n29,\n22,\n12,\n1084,\n1037,\n989,\n963],\n\"‘name': 'polygon'}}},\n\nPRPRPRPRP RR\n\nPPP PRP PRP RP RRP RR\n\n(continues on next page)\n\n6.1. Prepare the customized dataset\n\n31\n\n", "vlm_text": "1207, 1210, 1190, 1177, 1172, 1174, 1170, 1153, 1127, 1104, 1061, 1032, 1020],\n\n ' all points y ' : [963, 899, 841, 787, 738, 700, 663, 638, 621, 619, 643, 672, 720, 765, 800, 860, 896, 942, 990, 1035, 1079, 1112, 1129, 1134, 1144, 1153, 1166, 1166, 1150, 1136, 1129, 1122, 1112, 1084, 1037, 989, 963],\n\n ' name ' :  ' polygon ' }}}, ' size ' : 1115004} "}
{"page": 39, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_39.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n\"size': 1115004}\n\nThe annotation is a JSON file where each key indicates an image’s all annotations. The code to convert the balloon\ndataset into coco format is as below.\n\nimport os.path as osp\n\ndef convert_balloon_to_coco(ann_file, out_file, image_prefix):\n\ndata_infos = mmcv.load(ann_file)\n\nannotations = []\n\nimages = []\n\nobj_count = 0\n\nfor idx, v in enumerate(mmcv.track_iter_progress(data_infos.values())):\n\nfilename = v['filename']\nimg_path = osp.join(image_prefix, filename)\nheight, width = mmcv.imread(img_path) .shape[:2]\n\nimages .append(dict(\nid=idx,\nfile_name=filename,\nheight=height,\nwidth=width))\n\nbboxes = []\nlabels = []\nmasks = []\nfor _, obj in v['regions'].items(Q):\nassert not obj['region_attributes']\nobj = obj['shape_attributes']\npx = obj['all_points_x']\npy = obj['all_points_y']\npoly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\npoly = [p for x in poly for p in x]\n\nX_min, y_min, x_max, y_max = (\nmin(px), min(py), max(px), max(py))\n\ndata_anno = dict(\nimage_id=idx,\nid=obj_count,\ncategory_id=0,\nbbox=[x_min, y_min, x_max - x_min, y_max - y_min],\narea=(x_max - x_min) * (y_max - y_min),\nsegmentation=[poly],\niscrowd=0)\nannotations. append(data_anno)\nobj_count += 1\n\ncoco_format_json = dict(\n\nimages=images,\n\n(continues on next page)\n\n32\n\nChapter 6. 2: Train with customized datasets\n\n", "vlm_text": "\nThe annotation is a JSON file where each key indicates an image’s all annotations. The code to convert the balloon dataset into coco format is as below. \nimport  os.path  as  osp def  convert balloon to coco (ann_file, out_file, image prefix): data_infos  $=$   mmcv . load(ann_file) annotations  $=$   [] images    $=$   [] obj_count  $\\mathbf{\\varepsilon}=\\mathbf{\\varepsilon}\\,\\Updownarrow$  for  idx, v  in  enumerate (mmcv . track it er progress(data_infos . values())): filename  $=$   v[ ' filename ' ] img_path  $=$   osp . join(image prefix, filename) height, width  $=$   mmcv . imread(img_path) . shape[: 2 ] images . append( dict ( id = idx, file_name  $\\scriptstyle{\\varepsilon}$  filename, height  $=$  height, width = width)) bboxes    $\\begin{array}{l l}{\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}[\\mathbf{\\Sigma}]}\\end{array}$  labels  $\\begin{array}{l l}{\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}[\\mathbf{\\Sigma}]}\\end{array}$  masks  $\\begin{array}{l l}{\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}}\\\\ {\\mathbf{\\Sigma}}&{\\mathbf{\\Sigma}=\\mathbf{\\Sigma}}\\end{array}\\left[\\begin{array}{l l l}{\\mathbf{\\Sigma}}\\end{array}\\right]$  for  _, obj  in  v[ ' regions ' ] . items(): assert not  obj[ ' region attributes ' ] obj  $=$   obj[ ' shape attributes ' ] p  $\\tt{\\textbf{x}=}$   obj[ ' all points x ' ] py  $=$   obj[ ' all points y ' ]  $\\mathtt{p o l y}\\ =\\ \\left[\\left(\\mathbf{x}\\ +\\ \\mathbb{0}\\,,5\\,,\\textbf{y}+\\,\\mathbb{0}\\,,5\\right)\\right.$   for  x, y  in  zip (px, py)] poly  $=$   [p  for  x  in  poly  for    $\\mathtt{p}$   in  x] x_min, y_min, x_max, y_max  $=$   ( min (px),  min (py),  max (px),  max (py)) data_anno  $=$   dict ( image_  $\\scriptstyle{\\mathrm{i}}{\\mathsf{d}}={\\mathrm{i}}{\\mathsf{d}}\\mathbf{x}$  , id = obj_count, category_  $\\scriptstyle{\\dot{\\mathbf{\\tau}}}{\\mathsf{d}}=\\mathbb{0}$  , bbox  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  [x_min, y_min, x_max  -  x_min, y_max  -  y_min], area  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  (x_max  -  x_min)  \\*  (y_max  -  y_min), segmentation  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  [poly], iscrowd  $\\scriptstyle\\cdot=\\!0$  ) annotations . append(data_anno) obj_count    $\\mathbf{\\varepsilon}+=\\mathbf{\\varepsilon}^{1}$  coco format json  $=$   dict ( images  $=$  images, "}
{"page": 40, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_40.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nannotations=annotations,\ncategories=[{'id':0, ‘name': 'balloon'}])\nmmcv.dump(coco_format_json, out_file)\n\nUsing the function above, users can successfully convert the annotation file into json format, then we can use\nCocoDataset to train and evaluate the model.\n\n6.2 Prepare a config\n\nThe second step is to prepare a config thus the dataset could be successfully loaded. Assume that we want to use Mask\nR-CNN with FPN, the config to train the detector on balloon dataset is as below. Assume the config is under directory\nconfigs/balloon/ and named as mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_balloon.py, the config is as\nbelow.\n\n# The new config inherits a base config to highlight the necessary modification\n_base_ = 'mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py'\n\n# We also need to change the num_classes in head to match the dataset's annotation\nmodel = dict(\nroi_head=dict(\nbbox_head=dict (num_classes=1),\nmask_head=dict (num_classes=1)))\n\n# Modify dataset related settings\ndataset_type = 'COCODataset'\nclasses = ('balloon',)\ndata = dict(\ntrain=dict(\nimg_prefix='balloon/train/',\nclasses=classes,\nann_file='balloon/train/annotation_coco.json'),\nval=dict(\nimg_prefix='balloon/val/',\nclasses=classes,\nann_file='balloon/val/annotation_coco.json'),\ntest=dict(\nimg_prefix='balloon/val/',\nclasses=classes,\nann_file='balloon/val/annotation_coco.json'))\n\n# We can use the pre-trained Mask RCNN model to obtain higher performance\nload_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__\n+ segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n\n6.2. Prepare a config 33\n\n", "vlm_text": "The image shows a snippet of Python code formatted to appear as if it is within a table. The code processes data into the COCO (Common Objects in Context) format, which is commonly used for object detection tasks in computer vision. Here's a breakdown of the contents:\n\n1. `annotations=annotations,` - A parameter setting where the variable `annotations` is assigned to the key `annotations`.\n\n2. `categories=[{'id':0, 'name': 'balloon'}]` - This line defines a list with a dictionary describing a category for the dataset. The category has an ID of 0 and a name 'balloon'.\n\n3. `mmcv.dump(coco_format_json, out_file)` - This method likely writes or saves the JSON content formatted in COCO structure to an output file.\n\nThis snippet seems to be part of a larger script that converts or manipulates data related to object detection, specifically involving a category for 'balloon'.\nUsing the function above, users can successfully convert the annotation file into json format, then we can use Coco Data set  to train and evaluate the model. \n6.2 Prepare a config \nThe second step is to prepare a config thus the dataset could be successfully loaded. Assume that we want to use Mask R-CNN with FPN, the config to train the detector on balloon dataset is as below. Assume the config is under directory configs/balloon/  and named as  mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py , the config is as below.\n\n \n# The new config inherits a base config to highlight the necessary modification\n\n _base_  $=$   ' mask_rcnn/mask r cnn r 50 caff e fp n m strain-poly 1 x coco.py '\n\n # We also need to change the num classes in head to match the dataset ' s annotation model  $=$   dict ( roi_head  $\\overbar{\\;-\\;}$  dict ( bbox_head = dict (num classes  $_{:=1}$  ), mask_head = dict (num classes  $^{=1}$  ))) # Modify dataset related settings data set type  $=$   ' COCO Data set ' classes $=$  ('balloon',)data  $=$   dict ( train  $\\overbar{\\ }$  dict ( img_prefix  $\\c=\"$  ' balloon/train/ ' , classes  $,=$  classes, ann_file  $\\equiv^{1}$  ' balloon/train/annotation coco.json ' ), val  $\\mathbf{\\beta}\\!=$  dict ( img_prefix  $\\c=\"$  ' balloon/val/ ' , classes  $,=$  classes, ann_file  $\\equiv^{\\dagger}$  balloon/val/annotation coco.json ' ), test  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  dict ( img_prefix  $\\c=\"$  ' balloon/val/ ' , classes  $,=$  classes, ann_file  $\\equiv$  ' balloon/val/annotation coco.json ' )) # We can use the pre-trained Mask RCNN model to obtain higher performance \nload_from  $=$   ' checkpoints/mask r cnn r 50 caff e fp n m strain-poly 3 x coco b box mAP-0.408__\n\n  $\\hookrightarrow$  segm_mAP-0.37_20200504_163245-42aa3d00.pth '\n\n → "}
{"page": 41, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_41.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n6.3 Train a new model\n\nTo train a model with the new config, you can simply run\npython tools/train.py configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_balloon.py\n\nFor more detailed usages, please refer to the Case /\n\n6.4 Test and inference\n\nTo test the trained model, you can simply run\npython tools/test.py configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_balloon.py.\n\nwork_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_balloon.py/latest.pth --eval bbox.,\n\n—segm\n\nFor more detailed usages, please refer to the Case /.\n\nChapter 6. 2: Train with customized datasets\n\n34\n", "vlm_text": "6.3 Train a new model \nTo train a model with the new config, you can simply run \npython tools/train.py configs/balloon/mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py \nFor more detailed usages, please refer to the  Case 1 . \n6.4 Test and inference \nTo test the trained model, you can simply run \npython tools/test.py configs/balloon/mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py ␣\n\n  $\\hookrightarrow$  work_dirs/mask r cnn r 50 caff e fp n m strain-poly 1 x balloon.py/latest.pth --eval bbox ␣\n\n →\n\n  $\\hookrightarrow$  segm\n\n → \nFor more detailed usages, please refer to the  Case 1 . "}
{"page": 42, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_42.jpg", "ocr_text": "CHAPTER\nSEVEN\n\n3: TRAIN WITH CUSTOMIZED MODELS AND STANDARD DATASETS\n\nIn this note, you will know how to train, test and inference your own customized models under standard datasets. We\nuse the cityscapes dataset to train a customized Cascade Mask R-CNN R50 model as an example to demonstrate the\nwhole process, which using AugFPN to replace the default FPN as neck, and add Rotate or Translate as training-time\nauto augmentation.\n\nThe basic steps are as below:\n1. Prepare the standard dataset\n2. Prepare your own customized model\n3. Prepare a config\n\n4. Train, test, and inference models on the standard dataset.\n\n7.1 Prepare the standard dataset\n\nIn this note, as we use the standard cityscapes dataset as an example.\n\nIt is recommended to symlink the dataset root to $MMDETECTION/data. If your folder structure is different, you may\nneed to change the corresponding paths in config files.\n\nmmdetection\nt— mmdet\nt— tools\nt— configs\nt— data\nt— coco\nt— annotations\n+— train2017\nt— val2017\n-— test2017\nt— cityscapes\nt— annotations\nt— leftImg8bit\n}— train\nt— val\nt— gtFine\n-— train\nt— val\nL— VOCdevkit\n\n(continues on next page)\n\n35\n\n", "vlm_text": "3: TRAIN WITH CUSTOMIZED MODELS AND STANDARD DATASETS \nIn this note, you will know how to train, test and inference your own customized models under standard datasets. We use the cityscapes dataset to train a customized Cascade Mask R-CNN R50 model as an example to demonstrate the whole process, which using  AugFPN  to replace the default  FPN  as neck, and add  Rotate  or  Translate  as training-time auto augmentation. \nThe basic steps are as below: \n1. Prepare the standard dataset 2. Prepare your own customized model 3. Prepare a config 4. Train, test, and inference models on the standard dataset. \n7.1 Prepare the standard dataset \nIn this note, as we use the standard cityscapes dataset as an example. \nIt is recommended to symlink the dataset root to    $\\S$  MM DETECTION/data . If your folder structure is different, you may need to change the corresponding paths in config files. \nThe table represents a directory structure related to a project, likely involving deep learning and computer vision, particularly for object detection tasks. The hierarchy is displayed as follows:\n\n- `mmdetection`: The root directory, likely referring to the MMDetection open-source toolbox for object detection.\n\n  - `mmdet`: A directory that may contain MMDetection-related modules or code.\n  \n  - `tools`: A directory likely containing scripts or tools for facilitating various tasks such as training, testing, or data processing.\n  \n  - `configs`: A directory likely containing configuration files for different experiments or model configurations.\n  \n  - `data`: A directory designated for storing datasets.\n  \n    - `coco`: A subdirectory for the COCO dataset. Within this, there are further subdirectories for:\n    \n      - `annotations`: This likely contains annotation files for the COCO dataset.\n      \n      - `train2017`: This likely contains the training images for the 2017 version of the COCO dataset.\n      \n      - `val2017`: This likely contains the validation images for the COCO dataset, 2017 version.\n      \n      - `test2017`: This likely contains test images for the 2017 version of the COCO dataset.\n\n    - `cityscapes`: A subdirectory for the Cityscapes dataset. It contains:\n    \n      - `annotations`: Likely contains annotation files for the Cityscapes dataset.\n      \n      - `leftImg8bit`: Subdirectory containing 8-bit left view images, with its own subdirectories for:\n      \n        - `train`: Likely contains training images.\n        \n        - `val`: Likely contains validation images.\n        \n      - `gtFine`: A subdirectory likely containing fine-grained annotations, with:\n      \n        - `train`: Likely contains training annotations.\n        \n        - `val`: Likely contains validation annotations.\n\n    - `VOCdevkit`: A directory that likely contains data related to the PASCAL VOC dataset, commonly used for object detection tasks.\n\nThis structure is useful for organizing data and configurations needed for training and evaluating object detection models."}
{"page": 43, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_43.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nt— VOC2007\nr— Vvoc2012\n\nThe cityscapes annotations have to be converted into the coco format using tools/dataset_converters/\ncityscapes.py:\n\npip install cityscapesscripts\npython tools/dataset_converters/cityscapes.py ./data/cityscapes --nproc 8 --out-dir ./\n—data/cityscapes/annotations\n\nCurrently the config files in cityscapes use COCO pre-trained weights to initialize. You could download the pre-\ntrained models in advance if network is unavailable or slow, otherwise it would cause errors at the beginning of training.\n\n7.2 Prepare your own customized model\n\nThe second step is to use your own module or training setting. Assume that we want to implement a new neck called\nAugFPN to replace with the default FPN under the existing detector Cascade Mask R-CNN R50. The following imple-\nmentsAugFPN under MMDetection.\n\n7.2.1 1. Define a new neck (e.g. AugFPN)\n\nFirstly create a new file mmdet/models/necks/augfpn. py.\n\nfrom ..builder import NECKS\n\n@NECKS .register_module()\nclass AugFPN(nn.Module):\n\ndef __init__(self,\nin_channels,\nout_channels,\nnum_outs,\nstart_level=0,\nend_level=-1,\nadd_extra_convs=False) :\n\npass\n\ndef forward(self, inputs):\n# implementation is ignored\npass\n\n36 Chapter 7. 3: Train with customized models and standard datasets\n\n", "vlm_text": "The table includes two entries: VOC2007 and VOC2012, arranged in a hierarchical or tree-like structure.\nThe cityscapes annotations have to be converted into the coco format using  tools/data set converters/ cityscapes.py : \nThe image is not a table, but rather shows code snippets.\n\n1. The first line is a command to install a Python package:\n   ```\n   pip install cityscapescripts\n   ```\n\n2. The second line is a Python command to run a script that converts dataset formats:\n   ```\n   python tools/dataset_converters/cityscapes.py ./data/cityscapes --nproc 8 --out-dir ./data/cityscapes/annotations\n   ```\n\n   - `python tools/dataset_converters/cityscapes.py` is the script being executed.\n   - `./data/cityscapes` specifies the dataset directory.\n   - `--nproc 8` sets the number of processing threads to 8.\n   - `--out-dir ./data/cityscapes/annotations` specifies the output directory for annotations.\nCurrently the config files in  cityscapes  use COCO pre-trained weights to initialize. You could download the pre- trained models in advance if network is unavailable or slow, otherwise it would cause errors at the beginning of training. \n7.2 Prepare your own customized model \nThe second step is to use your own module or training setting. Assume that we want to implement a new neck called AugFPN  to replace with the default  FPN  under the existing detector Cascade Mask R-CNN R50. The following imple- ments AugFPN  under MM Detection. \n7.2.1 1. Define a new neck (e.g. AugFPN) \nFirstly create a new file  mmdet/models/necks/augfpn.py \nThe image you provided is not a table; it is a snippet of Python code. Here's a summary of the code:\n\n1. The code snippet imports `NECKS` from a module named `builder` that is located in the parent directory.\n\n2. It defines a new class named `AugFPN`, which inherits from `nn.Module`. This suggests that `AugFPN` is part of a neural network implementation, possibly related to feature pyramid networks (FPN).\n\n3. The `AugFPN` class is decorated with `@NECKS.register_module()`, which likely means that it is being registered as a module in the NECKS registry for some framework.\n\n4. The `__init__` method initializes the class with the following parameters:\n   - `in_channels`\n   - `out_channels`\n   - `num_outs`\n   - `start_level` with a default value of 0\n   - `end_level` with a default value of -1\n   - `add_extra_convs` with a default value of `False`\n   This method currently contains a `pass` statement, indicating that the body of the method is not implemented.\n\n5. The `forward` method defines the forward pass of the network, accepting `inputs` as an argument. It also contains a `pass` statement and a comment saying \"# implementation is ignored\", suggesting that the actual implementation of the forward pass is missing.\n\nOverall, the code defines the structure for a class used in neural network modeling, but it does not implement any functionality yet."}
{"page": 44, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_44.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n7.2.2 2. Import the module\n\nYou can either add the following line to mmdet/models/necks/__init__.py,\n\nfrom .augfpn import AugFPN\n\nor alternatively add\n\ncustom_imports = dict(\nimports=['mmdet.models.necks.augfpn.py'],\nallow_failed_imports=False)\n\nto the config file and avoid modifying the original code.\n\n7.2.3 3. Modify the config file\n\nneck=dict(\ntype='AugFPN',\nin_channels=[256, 512, 1024, 2048],\nout_channels=256,\nnum_outs=5)\n\nFor more detailed usages about customize your own models (e.g. implement a new backbone, head, loss, etc) and\nruntime training settings (e.g. define a new optimizer, use gradient clip, customize training schedules and hooks, etc),\nplease refer to the guideline Customize Models and Customize Runtime Settings respectively.\n\n7.3 Prepare a config\n\nThe third step is to prepare a config for your own training setting. | Assume that we want to add\nAugFPN and Rotate or Translate augmentation to existing Cascade Mask R-CNN RS5O to train the\ncityscapes dataset, and assume the config is under directory configs/cityscapes/ and named as\ncascade_mask_rcnn_r50_augfpn_autoaug_10e_cityscapes. py, the config is as below.\n\n# The new config inherits the base configs to highlight the necessary modification\n_base_ = [\n',./_base_/models/cascade_mask_rcnn_r50_fpn.py',\n'../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py'\n\n]\n\nmodel = dict(\n# set None to avoid loading ImageNet pretrained backbone,\n# instead here we set ‘load_from’ to load from COCO pretrained detectors.\nbackbone=dict (init_cfg=None) ,\n# replace neck from defaultly “FPN. to our new implemented module ‘AugFPN*\nneck=dict(\ntype='AugFPN',\nin_channels=[256, 512, 1024, 2048],\nout_channels=256,\nnum_outs=5),\n# We also need to change the num_classes in head from 80 to 8, to match the\n# cityscapes dataset's annotation. This modification involves “‘bbox_head* and “mask_\nhead’.\n\n(continues on next page)\n\n7.3. Prepare a config 37\n\n", "vlm_text": "7.2.2 2. Import the module \nYou can either add the following line to  mmdet/models/necks/__init__.py , \nThe table contains Python code snippets related to importing a module named `AugFPN`. The first snippet shows a direct import statement:\n\n```python\nfrom .augfpn import AugFPN\n```\n\nThe second snippet provides an alternative method for importing by adding entries to a `custom_imports` dictionary. This dictionary specifies a path to import from and an option to not allow failed imports:\n\n```python\ncustom_imports = dict(\n    imports=['mmdet.models.necks.augfpn.py'],\n    allow_failed_imports=False\n)\n```\nto the config file and avoid modifying the original code. \n7.2.3 3. Modify the config file \nThe table contains a Python dictionary setup for a neural network component named `neck`. Here's the breakdown:\n\n- `type`: 'AugFPN', indicating the type of feature pyramid network.\n- `in_channels`: [256, 512, 1024, 2048], specifying input channel sizes.\n- `out_channels`: 256, defining the output channel size.\n- `num_outs`: 5, indicating the number of output feature maps.\nFor more detailed usages about customize your own models (e.g. implement a new backbone, head, loss, etc) and runtime training settings (e.g. define a new optimizer, use gradient clip, customize training schedules and hooks, etc), please refer to the guideline  Customize Models  and  Customize Runtime Settings  respectively. \n7.3 Prepare a config \nThe third step is to prepare a config for your own training setting. Assume that we want to add AugFPN  and  Rotate  or  Translate  augmentation to existing Cascade Mask R-CNN R50 to train the cityscapes dataset, and assume the config is under directory configs/cityscapes/ and named as cascade mask r cnn r 50 aug fp n auto aug 10 e cityscape s.py , the config is as below.\n\n \n# The new config inherits the base configs to highlight the necessary modification\n\n _base_  $\\begin{array}{r l}{\\mathbf{\\eta}=}&{{}\\mathsf{[}\\mathbf{\\eta}}\\end{array}$  ' ../_base_/models/cascade mask r cnn r 50 fp n.py ' , ' ../_base_/datasets/cityscape s instance.py ' ,  ' ../_base_/default runtime.py '\n\n ] model  $=$   dict ( # set None to avoid loading ImageNet pretrained backbone, # instead here we set  \\` load_from \\`  to load from COCO pretrained detectors. backbone  $=$  dict (init_cfg  $=$  None ), # replace neck from defaultly  \\` FPN \\`  to our new implemented module  \\` AugFPN \\` neck  $\\fallingdotseq$  dict ( type $\\equiv^{\\dagger}$ AugFPN',in channels  $=$  [ 256 ,  512 ,  1024 ,  2048 ], out channel  $\\mathsf{s}\\!\\!=\\!\\!256$  , num_outs  ${=}5$  ), # We also need to change the num classes in head from 80 to 8, to match the # cityscapes dataset ' s annotation. This modification involves  \\` bbox_head \\`  and  \\` mask_ head \\` . ˓ → "}
{"page": 45, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_45.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nroi_head=dict(\nbbox_head=[\ndict(\ntype='Shared2FCBBoxHead',\nin_channels=256,\nfc_out_channels=1024,\nroi_feat_size=7,\n# change the number of classes from defaultly COCO to cityscapes\nnum_classes=8,\nbbox_coder=dict(\ntype='DeltaXYWHBBoxCoder' ,\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.1, 0.1, 0.2, 0.2]),\nreg_class_agnostic=True,\nloss_cls=dict(\ntype='CrossEntropyLoss',\nuse_sigmoid=False,\nloss_weight=1.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0,\nloss_weight=1.0)),\ndict(\ntype='Shared2FCBBoxHead',\nin_channels=256,\nfc_out_channels=1024,\nroi_feat_size=7,\n# change the number of classes from defaultly COCO to cityscapes\nnum_classes=8,\nbbox_coder=dict(\ntype='DeltaXYWHBBoxCoder',\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.05, 0.05, 0.1, 0.1]),\nreg_class_agnostic=True,\nloss_cls=dict(\ntype='CrossEntropyLoss',\nuse_sigmoid=False,\nloss_weight=1.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0,\nloss_weight=1.0)),\ndict(\ntype='Shared2FCBBoxHead',\nin_channels=256,\nfc_out_channels=1024,\nroi_feat_size=7,\n# change the number of classes from defaultly COCO to cityscapes\nnum_classes=8,\nbbox_coder=dict(\ntype='DeltaXYWHBBoxCoder',\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.033, 0.033, 0.067, 0.067]),\nreg_class_agnostic=True,\nloss_cls=dict(\ntype='CrossEntropyLoss',\n\n(continues on next page)\n\n38 Chapter 7. 3: Train with customized models and standard datasets\n\n", "vlm_text": "The table contains code snippets for setting up configuration dictionaries, likely for a machine learning model. Here are the main components:\n\n1. **Model Settings**:\n   - **type**: 'Shared2FCBBoxHead'\n   - **in_channels**: 256\n   - **fc_out_channels**: 1024\n   - **roi_feat_size**: 7\n   - **num_classes**: 8 (indicating a change from default COCO to Cityscapes)\n\n2. **Bounding Box Coder Settings**:\n   - **type**: 'DeltaXYWHBBoxCoder'\n   - **target_means**: [0.0, 0.0, 0.0, 0.0]\n   - **target_stds**: Different sets for different configurations:\n     - [0.1, 0.1, 0.2, 0.2]\n     - [0.05, 0.05, 0.1, 0.1]\n     - [0.033, 0.033, 0.067, 0.067]\n\n3. **Classification and Bounding Box Loss**:\n   - **reg_class_agnostic**: True\n   - **loss_cls**: CrossEntropyLoss\n     - **use_sigmoid**: False\n     - **loss_weight**: 1.0\n   - **loss_bbox**: SmoothL1Loss\n     - **beta**: 1.0\n     - **loss_weight**: 1.0\n\nThese configurations are used in object detection models to define the architecture and loss functions for training.\n(continues on next page) "}
{"page": 46, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_46.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nuse_sigmoid=False,\nloss_weight=1.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0, loss_weight=1.0))\n],\nmask_head=dict(\ntype='FCNMaskHead',\nnum_convs=4,\nin_channels=256,\nconv_out_channels=256,\n# change the number of classes from defaultly COCO to cityscapes\nnum_classes=8,\nloss_mask=dict (\ntype='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n\n# over-write ‘train_pipeline’ for new added “AutoAugment* training setting\nimg_norm_cfg = dict(\nmean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\ndict (type='LoadImageFromFile'),\ndict(type='LoadAnnotations', with_bbox=True, with_mask=True) ,\ndict¢\ntype='AutoAugment',\npolicies=[\n[dict¢\ntype='Rotate',\nlevel=5,\nimg_fill_val=(124, 116, 104),\nprob=0.5,\nscale=1)\n1,\n[dict(type='Rotate', level=7, img_fill_val=(124, 116, 104)),\ndict¢\ntype='Translate',\nlevel=5,\nprob=0.5,\nimg_fill_val=(124, 116, 104))\n1,\n)D,\ndict¢\ntype='Resize', img_scale=[(2048, 800), (2048, 1024)], keep_ratio=True),\ndict(type='RandomFlip', flip_ratio=0.5),\ndict(type='Normalize', **img_norm_cfg),\ndict(type='Pad', size_divisor=32),\ndict (type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n\n]\n\n# set batch_size per gpu, and set new training pipeline\ndata = dict(\nsamples_per_gpu=1,\nworkers_per_gpu=3,\n# over-write ‘pipeline’ with new training pipeline setting\n\n(continues on next page)\n\n7.3. Prepare a config 39\n\n", "vlm_text": "use s igm oid = False , loss weight  $\\scriptstyle=1.\\,\\mathbb{O}.$  ), loss_bbox  $\\fallingdotseq$  dict ( type  $\\equiv^{\\dagger}$  Smooth L 1 Loss ' , beta  $\\scriptstyle=1.\\,\\mathbb{0}$  , loss weight  $\\,=\\!1\\,.\\,\\mathbb{O}.$  )) ], mask_head = dict ( type  $\\equiv^{\\dagger}$  FC N Mask Head ' , num_convs  ${=}4$  , in channels  $\\bullet{=}256$  , con v out channel  $s{=}256$  , # change the number of classes from defaultly COCO to cityscapes num classes  $^{=8}$  , loss_mask  $\\fallingdotseq$  dict ( type  $\\equiv^{\\dagger}$  Cross Entropy Loss ' , use_mask  $\\risingdotseq$  True , loss weight  $\\scriptstyle=1.\\,\\mathbb{O}$  )))) # over-write  \\` train pipeline \\`  for new added  \\` Auto Augment \\`  training setting img norm cf g  $=$   dict ( mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ 123.675 ,  116.28 ,  103.53 ], std  $\\overbar{\\;-\\;}$  [ 58.395 ,  57.12 ,  57.375 ], to_rgb  $\\risingdotseq$  True ) train pipeline  $=$   [ dict ( type  $=\"$  Load Image From File ' ), dict ( type  $\\risingdotseq$  Load Annotations ' , with_bbox  $\\fallingdotseq$  True , with_mask  $\\risingdotseq$  True ), dict ( type  $\\equiv^{1}$  ' Auto Augment ' , policies  $=$  [ [ dict ( type  $\\risingdotseq$  Rotate ' , level  ${\\tt=}5$  , img fill val  $\\mathbf{\\beta}\\!=$  ( 124 ,  116 ,  104 ), prob  $\\scriptstyle{\\mathfrak{s}}=\\mathbb{0}\\,.\\,5$  , scal  ${\\mathsf{e}}{\\mathsf{=}}{\\mathsf{1}}.$  ) ], [ dict ( type  $\\equiv^{\\dagger}$  Rotate ' , level  ${}^{=7}$  , img fill val  $\\mathbf{\\beta}\\!=$  ( 124 ,  116 ,  104 )), dict ( type  $\\risingdotseq$  Translate ' , level  ${\\tt=}5$  , prob  $\\scriptstyle{\\mathfrak{s}}=\\mathbb{0}\\,.\\,5$  , img fill val  $\\mathbf{\\beta}\\!=$  ( 124 ,  116 ,  104 )) ], ]), dict ( type  $\\equiv^{\\dagger}$  Resize ' , img_scale  $=$  [( 2048 ,  800 ), ( 2048 ,  1024 )], keep_ratio  $\\risingdotseq$  True ), dict ( type  $\\risingdotseq$  RandomFlip ' , flip_ratio  $\\scriptstyle{\\mathfrak{s}}=\\emptyset$  .5 ), dict ( type  $\\risingdotseq$  Normalize ' ,  \\*\\* img norm cf g), dict ( type  $\\scriptstyle{\\varepsilon}$  ' Pad ' , size divisor  $=\\!32$  ), dict ( type  $\\risingdotseq$  Default Format Bundle ' ), dict ( type  $\\circeq$  ' Collect ' , keys  $,=$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ,  ' gt_masks ' ]),\n\n ]\n\n # set batch_size per gpu, and set new training pipeline data  $=$   dict ( samples per gpu  $_{=1}$  , workers per gpu  $^{=3}$  , # over-write  \\` pipeline \\`  with new training pipeline setting "}
{"page": 47, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_47.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ntrain=dict (dataset=dict (pipeline=train_pipeline)))\n\n# Set optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# Set customized learning policy\nlr_config = dict(\npolicy='step',\nwarmup='linear',\nwarmup_iters=500,\nwarmup_ratio=0.001,\nstep=[8])\nrunner = dict(type='EpochBasedRunner', max_epochs=10)\n\n# We can use the COCO pretrained Cascade Mask R-CNN R50 model for more stable,\nperformance initialization\n\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_\norcnn_r50_fpn_1x_coco/cascade_mask_rcnn_r50_fpn_1x_coco_20200203-9d4dcb24.pth'\n\n7.4 Train a new model\n\nTo train a model with the new config, you can simply run\n\npython tools/train.py configs/cityscapes/cascade_mask_rcnn_r50_augfpn_autoaug_10e_\n—cityscapes.py\n\nFor more detailed usages, please refer to the Case /.\n\n7.5 Test and inference\n\nTo test the trained model, you can simply run\n\npython tools/test.py configs/cityscapes/cascade_mask_rcnn_r50_augfpn_autoaug_10e_\n—cityscapes.py work_dirs/cascade_mask_rcnn_r50_augfpn_autoaug_10e_cityscapes.py/latest.\npth --eval bbox segm\n\nFor more detailed usages, please refer to the Case /.\n\n40 Chapter 7. 3: Train with customized models and standard datasets\n\n", "vlm_text": "The image contains a snippet of Python code, which appears to be a configuration setup for training a machine learning model, likely in a deep learning framework such as PyTorch or TensorFlow.\n\nKey elements of the code:\n\n1. **Optimizer Configuration:**\n   - The optimizer used is SGD (Stochastic Gradient Descent) with a learning rate (`lr`) of 0.01, a momentum of 0.9, and a weight decay of 0.0001.\n   - `optimizer_config` sets `grad_clip` to `None`.\n\n2. **Learning Rate Policy:**\n   - A step policy is employed for learning rate adjustment.\n   - Specifies a linear warmup strategy with 500 iterations (`warmup_iters`) and a warmup ratio of 0.001.\n   - The learning rate decays at step [8].\n\n3. **Runner Configuration:**\n   - Uses an `EpochBasedRunner` with a maximum of 10 epochs.\n\n4. **Loading Pretrained Models:**\n   - A pretrained Cascade Mask R-CNN R50 model is loaded from a COCO dataset URL for stable performance initialization. The model file seems to be stored at the specified URL.\n\n5. **Training Pipeline:**\n   - A dictionary setup for training data input, referred to as `train_pipeline`.\n\nThe code seems to be part of a larger script or configuration for training an object detection model.\n7.4 Train a new model \nTo train a model with the new config, you can simply run \npython tools/train.py configs/cityscapes/cascade mask r cnn r 50 aug fp n auto aug 10 e\n\n  $\\hookrightarrow$  cityscapes.py\n\n → \nFor more detailed usages, please refer to the  Case 1 . \n7.5 Test and inference \nTo test the trained model, you can simply run \npython tools/test.py configs/cityscapes/cascade mask r cnn r 50 aug fp n auto aug 10 e\n\n  $\\hookrightarrow$  cityscapes.py work_dirs/cascade mask r cnn r 50 aug fp n auto aug 10 e cityscape s.py/latest.\n\n →\n\n  $\\hookrightarrow$  pth --eval bbox segm\n\n → "}
{"page": 48, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_48.jpg", "ocr_text": "CHAPTER\nEIGHT\n\nTUTORIAL 1: LEARN ABOUT CONFIGS\n\nWe incorporate modular and inheritance design into our config system, which is convenient to conduct various exper-\niments. If you wish to inspect the config file, you may run python tools/misc/print_config.py /PATH/TO/\nCONFIG to see the complete config.\n\n8.1 Modify config through script argumenis\n\nWhen submitting jobs using “tools/train.py” or “‘tools/test.py”, you may specify --cfg-options to in-place modify\nthe config.\n\n* Update config keys of dict chains.\n\nThe config options can be specified following the order of the dict keys in the original config. For example,\n--cfg-options model. backbone.norm_eval=False changes the all BN modules in model backbones to\ntrain mode.\n\n* Update keys inside a list of configs.\n\nSome config dicts are composed as a list in your config. For example, the training pipeline data.train.\npipeline is normally a list e.g. [dict(type='LoadImageFromFile'), ...]. If you want to change\n\"LoadImageFromFile' to 'LoadImageFromWebcam' in the pipeline, you may specify --cfg-options\ndata.train.pipeline.0.type=LoadImageFromWebcam.\n\n* Update values of list/tuples.\n\nIf the value to be updated is a list or a tuple. For example, the config file normally sets workflow=[('train',\n1)]. If you want to change this key, you may specify --cfg-options workflow=\"[(train,1),(val,1)]\".\nNote that the quotation mark “ is necessary to support list/tuple data types, and that NO white space is allowed\ninside the quotation marks in the specified value.\n\n8.2 Config File Structure\n\nThere are 4 basic component types under config/_base_, dataset, model, schedule, default_runtime. Many methods\ncould be easily constructed with one of each like Faster R-CNN, Mask R-CNN, Cascade R-CNN, RPN, SSD. The\nconfigs that are composed by components from _base_ are called primitive.\n\nFor all configs under the same folder, it is recommended to have only one primitive config. All other configs should\ninherit from the primitive config. In this way, the maximum of inheritance level is 3.\n\nFor easy understanding, we recommend contributors to inherit from existing methods. For example, if some modifica-\ntion is made base on Faster R-CNN, user may first inherit the basic Faster R-CNN structure by specifying _base_ =\n. /faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py, then modify the necessary fields in the config files.\n\n41\n", "vlm_text": "TUTORIAL 1: LEARN ABOUT CONFIGS \nWe incorporate modular and inheritance design into our config system, which is convenient to conduct various exper- iments. If you wish to inspect the config file, you may run  python tools/misc/print config.py /PATH/TO/ CONFIG  to see the complete config. \n8.1 Modify config through script arguments \nWhen submitting jobs using “tools/train.py” or “tools/test.py”, you may specify  --cfg-options  to in-place modify the config. \n• Update config keys of dict chains. \nThe config options can be specified following the order of the dict keys in the original config. For example, --cfg-options model.backbone.norm_eval  $\\b=$  False  changes the all BN modules in model backbones to train  mode. \n• Update keys inside a list of configs. \nSome config dicts are composed as a list in your config. For example, the training pipeline  data.train. pipeline  is normally a list e.g. [dict(type  $:=$  ' Load Image From File ' ), ...] . If you want to change ' Load Image From File '  to  ' Load Image From Webcam '  in the pipeline, you may specify  --cfg-options data.train.pipeline.0.type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  Load Image From Webcam . \n• Update values of list/tuples. \nIf the value to be updated is a list or a tuple. For example, the config file normally sets  workflow  $\\leftleftarrows$  [( ' train ' , 1)] . If you want to change this key, you may specify  --cfg-options workflow=\"[(train,1),(val,1)]\" . Note that the quotation mark “ is necessary to support list/tuple data types, and that  NO  white space is allowed inside the quotation marks in the specified value. \n8.2 Config File Structure \nThere are 4 basic component types under  config/_base_ , dataset, model, schedule, default runtime. Many methods could be easily constructed with one of each like Faster R-CNN, Mask R-CNN, Cascade R-CNN, RPN, SSD. The configs that are composed by components from  _base_  are called  primitive . \nFor all configs under the same folder, it is recommended to have only  one  primitive  config. All other configs should inherit from the  primitive  config. In this way, the maximum of inheritance level is 3. \nFor easy understanding, we recommend contributors to inherit from existing methods. For example, if some modifica- tion is made base on Faster R-CNN, user may first inherit the basic Faster R-CNN structure by specifying  _base_  $=$  ../faster r cnn/faster r cnn r 50 fp n 1 x coco.py , then modify the necessary fields in the config files. "}
{"page": 49, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_49.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nIf you are building an entirely new method that does not share the structure with any of the existing methods, you may\ncreate a folder xxx_rcnn under configs,\n\nPlease refer to mmcv for detailed documentation.\n\n8.3 Config Name Style\n\nWe follow the below style to name config files. Contributors are advised to follow the same style.\n\n{model}_[model setting]_{backbone}_{neck}_[norm setting]_[misc]_[gpu x batch_per_gpu]_\n{schedule}_{dataset}\n\n{xxx} is required field and [yyy] is optional.\n* {model}: model type like faster_rcnn, mask_rcnn, etc.\n\n¢ [model setting]: specific setting for some model, like without_semantic for htc, moment for reppoints,\netc.\n\n* {backbone}: backbone type like r50 (ResNet-50), x101 (ResNeXt-101).\n* {neck}: neck type like fpn, pafpn, nasfpn, c4.\n\n* [morm_setting]: bn (Batch Normalization) is used unless specified, other norm layer type could be gn (Group\nNormalization), syncbn (Synchronized Batch Normalization). gn-head/gn-neck indicates GN is applied in\nhead/neck only, while gn-al1 means GN is applied in the entire model, e.g. backbone, neck, head.\n\n* [misc]: miscellaneous setting/plugins of model, e.g. dconv, gcb, attention, albu, mstrain.\n¢ [gpu x batch_per_gpu]: GPUs and samples per GPU, 8x2 is used by default.\n\n* {schedule}: training schedule, options are 1x, 2x, 20e, etc. 1x and 2x means 12 epochs and 24 epochs\nrespectively. 20e is adopted in cascade models, which denotes 20 epochs. For 1x/2x, initial learning rate decays\nby a factor of 10 at the 8/16th and 11/22th epochs. For 20e, initial learning rate decays by a factor of 10 at the\n6th and 19th epochs.\n\n{dataset}: dataset like coco, cityscapes, voc_0712, wider_face.\n\n8.4 Deprecated train_cfg/test_cfg\n\nThe train_cfg and test_cfg are deprecated in config file, please specify them in the model config. The original\nconfig structure is as below.\n\n# deprecated\nmodel = dict(\ntype=...,\n\n)\ntrain_cfg=dict(...)\ntest_cfg=dict(...)\n\nThe migration example is as below.\n\n42 Chapter 8. Tutorial 1: Learn about Configs\n\n", "vlm_text": "If you are building an entirely new method that does not share the structure with any of the existing methods, you may create a folder  xxx_rcnn  under  configs , \nPlease refer to  mmcv  for detailed documentation. \n8.3 Config Name Style \nWe follow the below style to name config files. Contributors are advised to follow the same style.\n\n \n{model}_[model setting]_{backbone}_{neck}_[norm setting]_[misc]_[gpu x batch per gpu]_\n\n  $\\hookrightarrow$  {schedule}_{dataset}\n\n →\n\n \n $\\{{\\bf x x x}\\}$   is required field and  [yyy]  is optional. \n•  {model} : model type like  faster r cnn ,  mask_rcnn , etc. •  [model setting] : specific setting for some model, like  without semantic  for  htc ,  moment  for  reppoints , etc. •  {backbone} : backbone type like  r50  (ResNet-50),  x101  (ResNeXt-101). •  {neck} : neck type like  fpn ,  pafpn ,  nasfpn ,  c4 . •  [norm setting] :  bn  (Batch Normalization) is used unless specified, other norm layer type could be  gn  (Group Normalization),  syncbn  (Synchronized Batch Normalization).  gn-head / gn-neck  indicates GN is applied in head/neck only, while  gn-all  means GN is applied in the entire model, e.g. backbone, neck, head. •  [misc] : miscellaneous setting/plugins of model, e.g.  dconv ,  gcb ,  attention ,  albu ,  mstrain . •  [gpu x batch per gpu] : GPUs and samples per GPU,  8x2  is used by default. •  {schedule} : training schedule, options are  1x ,  2x ,  20e , etc.  1x  and    $2\\mathbf{x}$   means 12 epochs and 24 epochs respectively.    $2\\mathbb{o}$   is adopted in cascade models, which denotes 20 epochs. For    $1\\mathbf{x}/2\\mathbf{x}$  , initial learning rate decays by a factor of 10 at the 8/16th and 11/22th epochs. For  20e , initial learning rate decays by a factor of 10 at the 16th and 19th epochs. •  {dataset} : dataset like  coco ,  cityscapes ,  voc_0712 ,  wider_face . \n8.4 Deprecated train_cfg/test_cfg \nThe  train_cfg  and  test_cfg  are deprecated in config file, please specify them in the model config. The original config structure is as below. \nThe image appears to contain a code snippet, not a table. The snippet shows the setup of configuration dictionaries for a model, where:\n\n- `model` is assigned a dictionary with a `type` key. The contents are marked as deprecated.\n- `train_cfg` is assigned a dictionary for training configuration.\n- `test_cfg` is assigned a dictionary for testing configuration.\n\nSpecific keys and values in these configurations are not fully visible.\nThe migration example is as below. "}
{"page": 50, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_50.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n# recommended\nmodel = dict(\ntype=...,\n\ntrain_cfg=dict(..\ntest_cfg=dict(...\n\n8.5 An Example of Mask R-CNN\n\nTo help the users have a basic idea of a complete config and the modules in a modern detection system, we make brief\ncomments on the config of Mask R-CNN using ResNet50 and FPN as the following. For more detailed usage and the\ncorresponding alternative for each modules, please refer to the API documentation.\n\nmodel = dict(\ntype='MaskRCNN', # The name of detector\nbackbone=dict( # The config of backbone\ntype='ResNet', # The type of the backbone, refer to https://github.com/open-\n—mmlab/mmdetection/blob/master/mmdet /models/backbones/resnet .py#L308 for more details.\ndepth=50, # The depth of backbone, usually it is 50 or 101 for ResNet and.\nResNext backbones.\nnum_stages=4, # Number of stages of the backbone.\nout_indices=(0, 1, 2, 3), # The index of output feature maps produced in each.\nstages\nfrozen_stages=1, # The weights in the first 1 stage are fronzen\nnorm_cfg=dict( # The config of normalization layers.\ntype='BN', # Type of norm layer, usually it is BN or GN\nrequires_grad=True), # Whether to train the gamma and beta in BN\nnorm_eval=True, # Whether to freeze the statistics in BN\nstyle='pytorch' # The style of backbone, 'pytorch' means that stride 2 layers...\nsare in 3x3 conv, 'caffe' means stride 2 layers are in 1x1 convs.\ninit_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')), #.\n<The ImageNet pretrained backbone to be loaded\nneck=dict(\ntype='FPN', # The neck of detector is FPN. We also support 'NASFPN', 'PAFPN',..\netc. Refer to https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/necks/\nfpn.py#L10 for more details.\nin_channels=[256, 512, 1024, 2048], # The input channels, this is consistent.\nwith the output channels of backbone\nout_channels=256, # The output channels of each level of the pyramid feature map\nnum_outs=5), # The number of output scales\nrpn_head=dict(\ntype='RPNHead', # The type of RPN head is 'RPNHead', we also support 'GARPNHead\n=', etc. Refer to https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/\ndense_heads/rpn_head.py#L12 for more details.\nin_channels=256, # The input channels of each input feature map, this is.\nconsistent with the output channels of neck\nfeat_channels=256, # Feature channels of convolutional layers in the head.\nanchor_generator=dict( # The config of anchor generator\ntype='AnchorGenerator', # Most of methods use AnchorGenerator, SSD.\nDetectors uses ‘SSDAnchorGenerator’. Refer to https://github.com/open-mmlab/\n\n—mmdetection/blob/master/mmdet/core/anchor/anchor_generator.py#L10 for moxecniatea Di sext page)\n\n8.5. An Example of Mask R-CNN 43\n\n", "vlm_text": "# recommended model  $=$   dict ( type =... , ... train_cfg  $|=$  dict ( ... ), test_cfg  $\\leftrightharpoons$  dict ( ... ), ) \n8.5 An Example of Mask R-CNN \nTo help the users have a basic idea of a complete config and the modules in a modern detection system, we make brief comments on the config of Mask R-CNN using ResNet50 and FPN as the following. For more detailed usage and the corresponding alternative for each modules, please refer to the API documentation.\n\n \n $\\mathtt{m o d e l\\;=\\;d i c t(}$  type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' MaskRCNN ' , # The name of detector backbone  $=$  dict( # The config of backbone type  $=$  ' ResNet ' , # The type of the backbone, refer to https://github.com/open-\n\n  $\\hookrightarrow$  mmlab/mm detection/blob/master/mmdet/models/backbones/resnet.py#L308 for more details.\n\n → depth  $\\scriptstyle{=50}$  , # The depth of backbone, usually it is 50 or 101 for ResNet and ␣\n\n  $\\hookrightarrow$  ResNext backbones.\n\n → num_stages  ${=}4$  , # Number of stages of the backbone. out indices  $=$  (0, 1, 2, 3), # The index of output feature maps produced in each ␣\n\n  $\\hookrightarrow$  stages\n\n → frozen stages  $_{=1}$  , # The weights in the first 1 stage are fronzen norm_cfg  $=$  dict( # The config of normalization layers. type  $=$  ' BN ' , # Type of norm layer, usually it is BN or GN requires grad  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  True), # Whether to train the gamma and beta in BN norm_eval  $=$  True, # Whether to freeze the statistics in BN style  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' pytorch '  # The style of backbone,  ' pytorch '  means that stride 2 layers ␣\n\n  $\\hookrightarrow$  are in 3x3 conv,  ' caffe '  means stride 2 layers are in 1x1 convs.\n\n → init_cfg  $=$  dict(type  $=$  ' Pretrained ' , checkpoint  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' torch vision://resnet50 ' )), # ␣\n\n  $\\hookrightarrow$  The ImageNet pretrained backbone to be loaded\n\n → neck  $\\fallingdotseq$  dict( type  $=$  ' FPN ' , # The neck of detector is FPN. We also support  ' NASFPN ' ,  ' PAFPN ' , ␣\n\n  $\\hookrightarrow$  etc. Refer to https://github.com/open-mmlab/mm detection/blob/master/mmdet/models/necks/\n\n →\n\n  $\\hookrightarrow$  fpn.py#L10 for more details.\n\n → in channels  $=$  [256, 512, 1024, 2048], # The input channels, this is consistent ␣\n\n  $\\hookrightarrow$  with the output channels of backbone\n\n → out channel  $\\mathfrak{s}{=}256$  , # The output channels of each level of the pyramid feature map num_outs  $_{:=5}$  ), # The number of output scales rpn_head  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( type  $=$  ' RPNHead ' , # The type of RPN head is  ' RPNHead ' , we also support  ' GARPNHead\n\n  $\\hookrightarrow{}^{\\bullet}$  ' , etc. Refer to https://github.com/open-mmlab/mm detection/blob/master/mmdet/models/\n\n  $\\hookrightarrow$  dense heads/rpn_head.py#L12 for more details.\n\n → in_channel  $s{=}256$  , # The input channels of each input feature map, this is ␣\n\n  $\\hookrightarrow$  consistent with the output channels of neck\n\n → feat channel  $s{=}256$  , # Feature channels of convolutional layers in the head. anchor generator  $=$  dict( # The config of anchor generator type  $=$  ' Anchor Generator ' , # Most of methods use Anchor Generator, SSD ␣\n\n  $\\hookrightarrow$  Detectors uses  \\` SSD Anchor Generator \\` . Refer to https://github.com/open-mmlab/\n\n → \nmm detection/blob/master/mmdet/core/anchor/anchor generator.py#L10 for more details (continues on next page) "}
{"page": 51, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_51.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nscales=[8], # Basic scale of the anchor, the area of the anchor in one,\nposition of a feature map will be scale * base_sizes\nratios=[0.5, 1.0, 2.0], # The ratio between height and width.\nstrides=[4, 8, 16, 32, 64]), # The strides of the anchor generator. This is.\nconsistent with the FPN feature strides. The strides will be taken as base_sizes if.\nbase_sizes is not set.\nbbox_coder=dict( # Config of box coder to encode and decode the boxes during..\ntraining and testing\ntype='DeltaXYWHBBoxCoder', # Type of box coder. 'DeltaXYWHBBoxCoder' is..\napplied for most of methods. Refer to https://github.com/open-mmlab/mmdetection/blob/\n—master/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py#L9 for more details.\ntarget_means=[0.0, 0.0, 0.0, 0.0], # The target means used to encode and.\ndecode boxes\ntarget_stds=[1.0, 1.0, 1.0, 1.0]), # The standard variance used to encode.\nand decode boxes\nloss_cls=dict( # Config of loss function for the classification branch\ntype='CrossEntropyLoss', # Type of loss for classification branch, we also.\nsupport FocalLoss etc.\nuse_sigmoid=True, # RPN usually perform two-class classification, so itu\nusually uses sigmoid function.\nloss_weight=1.0), # Loss weight of the classification branch.\nloss_bbox=dict( # Config of loss function for the regression branch.\ntype='LiLoss', # Type of loss, we also support many IoU Losses and smooth.\n=Li-loss, etc. Refer to https://github.com/open-mmlab/mmdetection/blob/master/mmdet/\n—models/losses/smooth_11_loss.py#L56 for implementation.\nloss_weight=1.0)), # Loss weight of the regression branch.\nroi_head=dict( # RoIHead encapsulates the second stage of two-stage/cascade.,\ndetectors.\ntype='StandardRoIHead', # Type of the RoI head. Refer to https://github.com/\n—open-mmlab/mmdetection/blob/master/mmdet/models/roi_heads/standard_roi_head.py#L10 for.\nimplementation.\nbbox_roi_extractor=dict( # RoI feature extractor for bbox regression.\ntype='SingleRoIExtractor', # Type of the RoI feature extractor, most of.\nmethods uses SingleRoIExtractor. Refer to https://github.com/open-mmlab/mmdetection/\n—blob/master/mmdet/models/roi_heads/roi_extractors/single_level.py#L10 for details.\nroi_layer=dict( # Config of RoI Layer\ntype='RoIAlign', # Type of RoI Layer, DeformRoIPoolingPack and..\n-sModulatedDeformRoIPoolingPack are also supported. Refer to https://github.com/open-\n—mmlab/mmdetection/blob/master/mmdet/ops/roi_align/roi_align.py#L79 for details.\noutput_size=7, # The output size of feature maps.\nsampling_ratio=0), # Sampling ratio when extracting the RoI features. 0.\nmeans adaptive ratio.\nout_channels=256, # output channels of the extracted feature.\nfeatmap_strides=[4, 8, 16, 32]), # Strides of multi-scale feature maps. It.\nshould be consistent to the architecture of the backbone.\nbbox_head=dict( # Config of box head in the RolIHead.\ntype='Shared2FCBBoxHead', # Type of the bbox head, Refer to https://github.\n—com/open-mmlab/mmdetection/blob/master/mmdet/models/roi_heads/bbox_heads/convfc_bbox_\nhead.py#L177 for implementation details.\nin_channels=256, # Input channels for bbox head. This is consistent with.\nthe out_channels in roi_extractor\nfc_out_channels=1024, # Output feature channels of FC layers.\n\n(continues on next page)\n\n44 Chapter 8. Tutorial 1: Learn about Configs\n\n", "vlm_text": " $\\mathtt{s c a l e s}\\!\\!=\\!\\![8]$  , # Basic scale of the anchor, the area of the anchor in one ␣ position of a feature map will be scale \\* base_sizes \nratios  $\\overleftarrow{}$  [0.5, 1.0, 2.0], # The ratio between height and width. \nstrides  $=$  [4, 8, 16, 32, 64]), # The strides of the anchor generator. This is ␣ consistent with the FPN feature strides. The strides will be taken as base_sizes if ␣ base_sizes is not set. \nbbox_coder  $\\leftleftarrows$  dict( # Config of box coder to encode and decode the boxes during ␣ training and testing \ntype  $=$  ' Delta XY WH B Box Code r ' , # Type of box coder.  ' Delta XY WH B Box Code r '  is ␣ applied for most of methods. Refer to https://github.com/open-mmlab/mm detection/blob/ master/mmdet/core/bbox/coder/delta xy wh b box code r.py#L9 for more details. \ntarget means  $=$  [0.0, 0.0, 0.0, 0.0], # The target means used to encode and ␣ decode boxes → target stds  $=$  [1.0, 1.0, 1.0, 1.0]), # The standard variance used to encode ␣ and decode boxes \nloss_cls  $=$  dict( # Config of loss function for the classification branch \ntype  $={}^{1}$  ' Cross Entropy Loss ' , # Type of loss for classification branch, we also ␣ support FocalLoss etc. \nuse s igm oid  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  True, # RPN usually perform two-class classification, so it ␣ usually uses sigmoid function. \nloss weight  $_{=1}$  .0), # Loss weight of the classification branch. loss_bbox  $=$  dict( # Config of loss function for the regression branch. \ntype  $=$  ' L1Loss ' , # Type of loss, we also support many IoU Losses and smooth ␣ L1-loss, etc. Refer to https://github.com/open-mmlab/mm detection/blob/master/mmdet/ models/losses/smooth l 1 loss.py#L56 for implementation. \nloss weight  $\\scriptstyle=1.\\,\\emptyset)$  ), # Loss weight of the regression branch. \nroi_head  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # RoIHead encapsulates the second stage of two-stage/cascade ␣ detectors. \ntype  $=$  ' Standard RoI Head ' , # Type of the RoI head. Refer to https://github.com/ open-mmlab/mm detection/blob/master/mmdet/models/roi_heads/standard roi head.py#L10 for ␣ implementation. \nb box roi extractor  $=$  dict( # RoI feature extractor for bbox regression. \ntype  $=$  ' Single RoI Extractor ' , # Type of the RoI feature extractor, most of ␣ methods uses Single RoI Extractor. Refer to https://github.com/open-mmlab/mm detection/ blob/master/mmdet/models/roi_heads/roi extractors/single level.py#L10 for details. \nroi_layer  $\\leftleftarrows$  dict( # Config of RoI Layer type  $=$  ' RoIAlign ' , # Type of RoI Layer, Deform RoI Pooling Pack and \n\nModulated Deform RoI Pooling Pack are also supported. Refer to https://github.com/open- mmlab/mm detection/blob/master/mmdet/ops/roi_align/roi_align.py#L79 for details. \nsampling ratio  $\\scriptstyle{\\mathfrak{\\varphi}}=\\varnothing$  ), # Sampling ratio when extracting the RoI features.   $\\smash{\\mathfrak{O}_{\\perp}}$  means adaptive ratio. \nout channel  $s{=}256$  , # output channels of the extracted feature. \nfeat map strides  $=$  [4, 8, 16, 32]), # Strides of multi-scale feature maps. It ␣ should be consistent to the architecture of the backbone. \nbbox_head=dict( # Config of box head in the RoIHead. \ntype  $=$  ' Shared 2 FCB Box Head ' , # Type of the bbox head, Refer to https://github. com/open-mmlab/mm detection/blob/master/mmdet/models/roi_heads/bbox_heads/con v fc b box head.py#L177 for implementation details. \nin_channel  $s{=}256$  , # Input channels for bbox head. This is consistent with ␣ the out channels in roi extractor \nfc out channel  $\\scriptstyle{.s=1024}$  , # Output feature channels of FC layers. num classes  $\\scriptstyle=80$  , # Number of classes for classification bbox_coder  $=$  dict( # Box coder used in the second stage. "}
{"page": 52, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_52.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nroi_feat_size=7, # Size of RoI features\nnum_classes=80, # Number of classes for classification\nbbox_coder=dict( # Box coder used in the second stage.\ntype='DeltaXYWHBBoxCoder', # Type of box coder. 'DeltaXYWHBBoxCoder' is..\napplied for most of methods.\ntarget_means=[0.0, 0.0, 0.0, 0.0], # Means used to encode and decode box\ntarget_stds=[0.1, 0.1, 0.2, 0.2]), # Standard variance for encoding and.\ndecoding. It is smaller since the boxes are more accurate. [0.1, 0.1, 0.2, 0.2] is a.\nconventional setting.\nreg_class_agnostic=False, # Whether the regression is class agnostic.\nloss_cls=dict( # Config of loss function for the classification branch\ntype='CrossEntropyLoss', # Type of loss for classification branch, we.\nalso support FocalLoss etc.\nuse_sigmoid=False, # Whether to use sigmoid.\nloss_weight=1.0), # Loss weight of the classification branch.\nloss_bbox=dict( # Config of loss function for the regression branch.\ntype='LiLoss', # Type of loss, we also support many IoU Losses and,\nsmooth Li-loss, etc.\nloss_weight=1.0)), # Loss weight of the regression branch.\nmask_roi_extractor=dict( # RoI feature extractor for mask generation.\ntype='SingleRoIExtractor', # Type of the RoI feature extractor, most of.\nmethods uses SingleRoIExtractor.\nroi_layer=dict( # Config of RoI Layer that extracts features for instance.\nsegmentation\ntype='RoIAlign', # Type of RoI Layer, DeformRoIPoolingPack and..\n;ModulatedDeformRoIPoolingPack are also supported\noutput_size=14, # The output size of feature maps.\nsampling_ratio=0), # Sampling ratio when extracting the RoI features.\nout_channels=256, # Output channels of the extracted feature.\nfeatmap_strides=[4, 8, 16, 32]), # Strides of multi-scale feature maps.\nmask_head=dict( # Mask prediction head\ntype='FCNMaskHead', # Type of mask head, refer to https://github.com/open-\n—mmlab/mmdetection/blob/master/mmdet /models/roi_heads/mask_heads/fcn_mask_head.py#L21.\nfor implementation details.\nnum_convs=4, # Number of convolutional layers in mask head.\nin_channels=256, # Input channels, should be consistent with the output.\nchannels of mask roi extractor.\nconv_out_channels=256, # Output channels of the convolutional layer.\nnum_classes=80, # Number of class to be segmented.\nloss_mask=dict( # Config of loss function for the mask branch.\ntype='CrossEntropyLoss', # Type of loss used for segmentation\nuse_mask=True, # Whether to only train the mask in the correct class.\nloss_weight=1.0)))) # Loss weight of mask branch.\ntrain_cfg = dict( # Config of training hyperparameters for rpn and rcnn\nrpn=dict( # Training config of rpn\nassigner=dict( # Config of assigner\ntype='MaxIoUAssigner', # Type of assigner, MaxIoUAssigner is used for.\nmany common detectors. Refer to https://github.com/open-mmlab/mmdetection/blob/master/\n—mmdet/core/bbox/assigners/max_iou_assigner.py#L10 for more details.\npos_iou_thr=0.7, # IoU >= threshold 0.7 will be taken as positive.\nsamples\nneg_iou_thr=0.3, # IoU < threshold 0.3 will be taken as negative samples\n\n(continues on next page)\n\n8.5. An Example of Mask R-CNN 45\n\n", "vlm_text": "\ntype  $=$  ' Delta XY WH B Box Code r ' , # Type of box coder.  ' Delta XY WH B Box Code r '  is ␣ applied for most of methods. \nuse s igm oid  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  False, # Whether to use sigmoid. loss weight  $\\scriptstyle=1.\\,\\emptyset.$  ), # Loss weight of the classification branch. loss_bbox  $=$  dict( # Config of loss function for the regression branch. \ntype  $=\"$  L1Loss ' , # Type of loss, we also support many IoU Losses and ␣ smooth L1-loss, etc. \nloss weight  $\\scriptstyle=1\\,.\\,\\emptyset)$  ), # Loss weight of the regression branch. \n\nmethods uses Single RoI Extractor. \nroi_layer  $\\leftleftarrows$  dict( # Config of RoI Layer that extracts features for instance ␣ segmentation \ntype  $:=$  ' RoIAlign ' , # Type of RoI Layer, Deform RoI Pooling Pack and ␣ Modulated Deform RoI Pooling Pack are also supported \n\noutput size  $\\scriptstyle=14$  , # The output size of feature maps. sampling ratio  $\\scriptstyle{\\mathfrak{\\varphi}}=\\varnothing$  ), # Sampling ratio when extracting the RoI features. out channel  $s{=}256$  , # Output channels of the extracted feature. \nfeat map strides  $=$  [4, 8, 16, 32]), # Strides of multi-scale feature maps. mask_head  $=$  dict( # Mask prediction head \ntype  $=$  ' FC N Mask Head ' , # Type of mask head, refer to https://github.com/open- mmlab/mm detection/blob/master/mmdet/models/roi_heads/mask_heads/fc n mask head.py#L21 ␣ for implementation details. \nnum_convs  ${\\it\\Delta\\phi}_{\\mathrm{7}}=\\!4$  , # Number of convolutional layers in mask head. \nin_channel  $s{=}256$  , # Input channels, should be consistent with the output ␣ channels of mask roi extractor. \ncon v out channel  $\\mathfrak{s}{=}256$  , # Output channels of the convolutional layer. \nnum classes  $\\scriptstyle=80$  , # Number of class to be segmented. loss_mask  $\\fallingdotseq$  dict( # Config of loss function for the mask branch. type  $={}^{1}$  ' Cross Entropy Loss ' , # Type of loss used for segmentation use_mask  $\\fallingdotseq$  True, # Whether to only train the mask in the correct class. loss weight  $\\scriptstyle=1\\,.\\,\\emptyset)$  ))) # Loss weight of mask branch. \ntrain_cfg  $=$   dict( # Config of training hyper parameters for rpn and rcnn \nrpn  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # Training config of rpn assigner  $=$  dict( # Config of assigner \ntype  $=$  ' MaxI oU As signer ' , # Type of assigner, MaxI oU As signer is used for ␣ many common detectors. Refer to https://github.com/open-mmlab/mm detection/blob/master/ → mmdet/core/bbox/assigners/max i ou as signer.py#L10 for more details. \npos i ou thr  $\\scriptstyle{\\ =0.7}$  , # IoU  $>=$   threshold 0.7 will be taken as positive ␣ \nsamples \nne g i ou thr  $\\scriptstyle{\\;=0}$  .3, # IoU   $<$   threshold 0.3 will be taken as negative samples "}
{"page": 53, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_53.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nmin_pos_iou=0.3, # The minimal IoU threshold to take boxes as positive.\nsamples\nmatch_low_quality=True, # Whether to match the boxes under low quality.\n(see API doc for more details).\nignore_iof_thr=-1), # IoF threshold for ignoring bboxes\nsampler=dict( # Config of positive/negative sampler\ntype='RandomSampler', # Type of sampler, PseudoSampler and other,,\nsamplers are also supported. Refer to https://github.com/open-mmlab/mmdetection/blob/\nmaster/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details.\nnum=256, # Number of samples\npos_fraction=0.5, # The ratio of positive samples in the total samples.\nneg_pos_ub=-1, # The upper bound of negative samples based on the.\nnumber of positive samples.\nadd_gt_as_proposals=False), # Whether add GT as proposals after.\nsampling.\nallowed_border=-1, # The border allowed after padding for valid anchors.\npos_weight=-1, # The weight of positive samples during training.\ndebug=False), # Whether to set the debug mode\nrpn_proposal=dict( # The config to generate proposals during training\nnms_across_levels=False, # Whether to do NMS for boxes across levels. Only.\nwork in “GARPNHead’, naive rpn does not support do nms cross levels.\nnms_pre=2000, # The number of boxes before NMS\nnms_post=1000, # The number of boxes to be kept by NMS, Only work in,\n..°GARPNHead’.\nmax_per_img=1000, # The number of boxes to be kept after NMS.\nnms=dict( # Config of NMS\ntype='nms', # Type of NMS\niou_threshold=0.7 # NMS threshold\n5\nmin_bbox_size=0), # The allowed minimal box size\nrcenn=dict( # The config for the roi heads.\nassigner=dict( # Config of assigner for second stage, this is different for.\nthat in rpn\ntype='MaxIoUAssigner', # Type of assigner, MaxIoUAssigner is used for.\nall roi_heads for now. Refer to https://github.com/open-mmlab/mmdetection/blob/master/\n—mmdet/core/bbox/assigners/max_iou_assigner.py#L10 for more details.\npos_iou_thr=0.5, # IoU >= threshold 0.5 will be taken as positive.\nsamples\nneg_iou_thr=0.5, # IoU < threshold 9.5 will be taken as negative samples\nmin_pos_iou=0.5, # The minimal IoU threshold to take boxes as positive.\nsamples\nmatch_low_quality=False, # Whether to match the boxes under low quality.\n(see API doc for more details).\nignore_iof_thr=-1), # IoF threshold for ignoring bboxes\nsampler=dict(\ntype='RandomSampler', # Type of sampler, PseudoSampler and other,,\nsamplers are also supported. Refer to https://github.com/open-mmlab/mmdetection/blob/\nmaster/mmdet/core/bbox/samplers/random_sampler.py#L8 for implementation details.\nnum=512, # Number of samples\npos_fraction=0.25, # The ratio of positive samples in the total samples.\nneg_pos_ub=-1, # The upper bound of negative samples based on the.\nnumber of positive samples.\n\n(continues on next page)\n\n46 Chapter 8. Tutorial 1: Learn about Configs\n\n", "vlm_text": "The table provides a configuration for a machine learning model, specifically detailing settings related to region proposal networks (RPN) and region of interest (ROI) heads, often used in object detection tasks. Here's a summary of the table contents:\n\n1. **IoU and Sampling Configuration:**\n   - `min_pos_iou=0.3`: Minimum Intersection over Union (IoU) threshold for considering a bounding box as positive.\n   - `match_low_quality=True`: Option to match boxes under low quality conditions.\n   - `ignore_iof_thr=-1`: Ignore IoU threshold for ignoring bounding boxes.\n\n2. **Sampler Configuration:**\n   - `sampler=dict(...)`: Configuration for positive/negative sampler with a `type` of `RandomSampler`.\n   - `num=256` or `num=512`: Number of samples.\n   - `pos_fraction=0.5` or `pos_fraction=0.25`: Ratio of positive samples in total samples.\n   - `neg_pos_ub=-1`: Upper bound of negative samples.\n\n3. **Proposal Configuration:**\n   - `add_gt_as_proposals=False`: Whether to add ground truth as proposals after sampling.\n   - `allowed_border=-1`: Border allowed after padding for valid anchors.\n   - `pos_weight=-1`: Weight of positive samples during training.\n   - `debug=False`: Debug mode flag.\n   - `rpn_proposal=dict(...)`: Configuration for generating proposals during training.\n   - `nms_across_levels=False`: Non-maximum suppression (NMS) across levels flag.\n   - `nms_pre=2000`: Number of boxes before NMS.\n   - `nms_post=1000`: Number of boxes kept by NMS.\n   - `max_per_img=1000`: Maximum number of boxes to keep after NMS.\n   - `nms=dict(...)`: Configuration for NMS with type 'nms' and `iou_threshold=0.7`.\n\n4. **Box Size Configuration:**\n   - `min_bbox_size=0`: Minimum allowed box size.\n\n5. **ROI Head Configuration:**\n   - `rcnn=dict(...)`: Configuration for the ROI heads with `assigner=dict(...)`.\n   - `type='MaxIoUAssigner'`: Type of assigner, using `MaxIoUAssigner`.\n   - `pos_iou_thr=0.5`: IoU threshold considered positive.\n   - `neg_iou_thr=0.5`: IoU threshold considered negative.\n\nOverall, the table outlines settings for how samples are selected and processed in a model, detailing thresholds for bounding box selection, NMS, and the sampling strategy for training machine learning models, particularly in object detection frameworks like MMDetection."}
{"page": 54, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_54.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nadd_gt_as_proposals=True\n), # Whether add GT as proposals after sampling.\nmask_size=28, # Size of mask\npos_weight=-1, # The weight of positive samples during training.\ndebug=False)) # Whether to set the debug mode\ntest_cfg = dict( # Config for testing hyperparameters for rpn and rcnn\nrpn=dict( # The config to generate proposals during testing\nnms_across_levels=False, # Whether to do NMS for boxes across levels. Only.\nwork in “GARPNHead’, naive rpn does not support do nms cross levels.\nnms_pre=1000, # The number of boxes before NMS\nnms_post=1000, # The number of boxes to be kept by NMS, Only work in,\n..°GARPNHead’.\nmax_per_img=1000, # The number of boxes to be kept after NMS.\nnms=dict( # Config of NMS\ntype='nms', #Type of NMS\niou_threshold=0.7 # NMS threshold\n5\nmin_bbox_size=0), # The allowed minimal box size\nrcenn=dict( # The config for the roi heads.\nscore_thr=0.05, # Threshold to filter out boxes\nnms=dict( # Config of NMS in the second stage\ntype='nms', # Type of NMS\niou_thr=0.5), # NMS threshold\nmax_per_img=100, # Max number of detections of each image\nmask_thr_binary=0.5)) # Threshold of mask prediction\ndataset_type = 'CocoDataset' # Dataset type, this will be used to define the dataset\ndata_root = 'data/coco/' # Root path of data\nimg_norm_cfg = dict( # Image normalization config to normalize the input images\nmean=[123.675, 116.28, 103.53], # Mean values used to pre-training the pre-trained,\nbackbone models\nstd=[58.395, 57.12, 57.375], # Standard variance used to pre-training the pre-\nstrained backbone models\nto_rgb=True\n) # The channel orders of image used to pre-training the pre-trained backbone models\ntrain_pipeline = [ # Training pipeline\ndict(type='LoadImageFromFile'), # First pipeline to load images from file path\ndict(\ntype='LoadAnnotations', # Second pipeline to load annotations for current image\nwith_bbox=True, # Whether to use bounding box, True for detection\nwith_mask=True, # Whether to use instance mask, True for instance segmentation\npoly2mask=False), # Whether to convert the polygon mask to instance mask, set.\nFalse for acceleration and to save memory\ndict(\ntype='Resize', # Augmentation pipeline that resize the images and their.\nannotations\nimg_scale=(1333, 800), # The largest scale of image\nkeep_ratio=True\n), # whether to keep the ratio between height and width.\ndict(\ntype='RandomFlip', # Augmentation pipeline that flip the images and their.\nannotations\n\nflip_ratio=0.5), # The ratio or probability to flip\n\n(continues on next page)\n\n8.5. An Example of Mask R-CNN 47\n", "vlm_text": "add gt as proposal  $\\mathtt{s}{=}$  True ), # Whether add GT as proposals after sampling. mask_size  $_{:=28}$  , # Size of mask pos_weight  $_{\\cdot}{=}{-}1$  , # The weight of positive samples during training. debug  $=$  False)) # Whether to set the debug mode test_cfg  $=$   dict( # Config for testing hyper parameters for rpn and rcnn rpn  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # The config to generate proposals during testing nm s across levels  $\\overleftarrow{}$  False, # Whether to do NMS for boxes across levels. Only ␣\n\n  $\\hookrightarrow$  work in  \\` GARPNHead \\` , naive rpn does not support do nms cross levels.\n\n → nms_pre  $\\scriptstyle{\\underline{{\\mathbf{\\a}}}}=1000$  , # The number of boxes before NMS nms_post  $\\scriptstyle=1000$  , # The number of boxes to be kept by NMS, Only work in ␣\n\n  $\\hookrightarrow$  \\` GARPNHead \\` .\n\n → max per img  $\\scriptstyle=1000$  , # The number of boxes to be kept after NMS. nms  $=$  dict( # Config of NMS type  $:=$  ' nms ' , #Type of NMS i ou threshold=0.7 # NMS threshold ), min b box size  $\\scriptstyle=\\!0$  ), # The allowed minimal box size rcnn  $\\risingdotseq$  dict( # The config for the roi heads. score_thr  $\\scriptstyle{\\overline{{\\mathbf{\\xi}}}}=\\varnothing.\\,\\varnothing5$  , # Threshold to filter out boxes nms  $=$  dict( # Config of NMS in the second stage  $\\scriptstyle{\\mathrm{type}}={\\mathrm{!nms}}\\,^{\\prime}$  , # Type of NMS iou_thr $\\mathrm{\\Lambda}{=}\\mathbb{0}\\cdot5$ ),# NMS thresholdmax per img  $\\scriptstyle=100$  , # Max number of detections of each image mask thr bin ar  $\\scriptstyle{\\mathfrak{v}}=\\varnothing\\,.\\,5)$  ) # Threshold of mask prediction data set type  $=$   ' Coco Data set ' # Dataset type, this will be used to define the dataset data_root  $=$   ' data/coco/ ' # Root path of data img norm cf g  $=$   dict( # Image normalization config to normalize the input images mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], # Mean values used to pre-training the pre-trained ␣\n\n  $\\hookrightarrow$  backbone models\n\n → std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], # Standard variance used to pre-training the pre-\n\n  $\\hookrightarrow$  trained backbone models\n\n → to_rgb  $=$  True\n\n ) # The channel orders of image used to pre-training the pre-trained backbone models train pipeline  $=$   [ # Training pipeline dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Load Image From File ' ), # First pipeline to load images from file path dict( type  $=$  ' Load Annotations ' , # Second pipeline to load annotations for current image with_bbox  $\\circeq$  True, # Whether to use bounding box, True for detection with_mask  $\\fallingdotseq$  True, # Whether to use instance mask, True for instance segmentation poly2mask  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  False), # Whether to convert the polygon mask to instance mask, set ␣\n\n  $\\hookrightarrow$  False for acceleration and to save memory\n\n → dict( type  $=\"$  Resize ' , # Augmentation pipeline that resize the images and their ␣\n\n  $\\hookrightarrow$  annotations\n\n → img_scale  $=$  (1333, 800), # The largest scale of image keep_ratio  $=$  True ), # whether to keep the ratio between height and width. dict( type  $={}^{1}$  ' RandomFlip ' , # Augmentation pipeline that flip the images and their ␣\n\n  $\\hookrightarrow$  annotations\n\n → flip_ratio  $\\scriptstyle{\\mathfrak{o}}=\\varnothing.5$  ), # The ratio or probability to flip "}
{"page": 55, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_55.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ndict(\ntype='Normalize', # Augmentation pipeline that normalize the input images\nmean=[123.675, 116.28, 103.53], # These keys are the same of img_norm_cfg since.\nthe\nstd=[58.395, 57.12, 57.375], # keys of img_norm_cfg are used here as arguments\nto_rgb=True) ,\ndict(\ntype='Pad', # Padding config\nsize_divisor=32), # The number the padded images should be divisible\ndict(type='DefaultFormatBundle'), # Default format bundle to gather data in the.\npipeline\ndict(\ntype='Collect', # Pipeline that decides which keys in the data should be passed.\nto the detector\nkeys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n]\ntest_pipeline = [\ndict(type='LoadImageFromFile'), # First pipeline to load images from file path\ndict(\ntype='MultiScaleFlipAug', # An encapsulation that encapsulates the testing.\n<augmentations\nimg_scale=(1333, 800), # Decides the largest scale for testing, used for the.\nResize pipeline\nflip=False, # Whether to flip images during testing\ntrans forms=[\ndict(type='Resize', # Use resize augmentation\nkeep_ratio=True), # Whether to keep the ratio between height and width,\n«= the img_scale set here will be suppressed by the img_scale set above.\ndict(type='RandomFlip'), # Thought RandomFlip is added in pipeline, it is.\nnot used because flip=False\n\ndict(\ntype='Normalize', # Normalization config, the values are from img_norm_\nocfg\nmean=[123.675, 116.28, 103.53],\nstd=[58.395, 57.12, 57.375],\nto_rgb=True),\ndict(\ntype='Pad', # Padding config to pad images divisible by 32.\nsize_divisor=32),\ndict(\ntype='ImageToTensor', # convert image to tensor\nkeys=['img']),\ndict(\ntype='Collect', # Collect pipeline that collect necessary keys for,\ntesting.\nkeys=['img'])\n1)\n]\n\ndata = dict(\nsamples_per_gpu=2, # Batch size of a single GPU\nworkers_per_gpu=2, # Worker to pre-fetch data for each single GPU\ntrain=dict( # Train dataset config\n\n(continues on next page)\n\n48 Chapter 8. Tutorial 1: Learn about Configs\n\n", "vlm_text": "dict( type  $=$  ' Normalize ' , # Augmentation pipeline that normalize the input images mean  $\\risingdotseq$  [123.675, 116.28, 103.53], # These keys are the same of img norm cf g since ␣\n\n  $\\hookrightarrow$  the\n\n → std=[58.395, 57.12, 57.375], # keys of img norm cf g are used here as arguments to_rgb  $\\leftrightharpoons$  True), dict( type  $=$  ' Pad ' , # Padding config size divisor  $=\\!32$  ), # The number the padded images should be divisible dict(type  $\\circeq$  ' Default Format Bundle ' ), # Default format bundle to gather data in the ␣\n\n  $\\hookrightarrow$  pipeline\n\n → dict( type  $=$  ' Collect ' , # Pipeline that decides which keys in the data should be passed ␣\n\n  $\\hookrightarrow$  to the detector\n\n → keys  $\\overleftarrow{}$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ,  ' gt_masks ' ])\n\n ] test pipeline  $=$   [ dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Load Image From File ' ), # First pipeline to load images from file path dict( type  $=$  ' Multi Scale Flip Aug ' , # An encapsulation that encapsulates the testing ␣\n\n  $\\hookrightarrow$  augmentations\n\n → img_scale  $=$  (1333, 800), # Decides the largest scale for testing, used for the ␣\n\n  $\\hookrightarrow$  Resize pipeline\n\n → flip  $\\leftrightharpoons$  False, # Whether to flip images during testing transforms  $=$  [ dict(type  $\\circeq$  ' Resize ' , # Use resize augmentation keep_ratio  $\\mathrm{=}$  True), # Whether to keep the ratio between height and width,\n\n  $\\hookrightarrow$  the img_scale set here will be suppressed by the img_scale set above.\n\n → dict(type  $=$  ' RandomFlip ' ), # Thought RandomFlip is added in pipeline, it is ␣\n\n  $\\hookrightarrow$  not used because flip  $\\mathrm{\\textmu}$  False\n\n → dict( type  $\\risingdotseq$  ' Normalize ' , # Normalization config, the values are from img_norm_\n\n  $\\hookrightarrow$  cfg\n\n → mean  $\\risingdotseq$  [123.675, 116.28, 103.53], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], to_rgb  $\\leftrightharpoons$  True), dict( type  $=$  ' Pad ' , # Padding config to pad images divisible by 32. size divisor  $\\scriptstyle{=32}$  ), dict( type  $=$  ' Image To Tensor ' , # convert image to tensor keys  $\\overleftarrow{}$  [ ' img ' ]), dict( type  $=$  ' Collect ' , # Collect pipeline that collect necessary keys for ␣\n\n  $\\hookrightarrow$  testing.\n\n → keys $:=$ ['img'])])\n\n ] data  $=$   dict( \nsamples per gpu  $^{=2}$  , # Batch size of a single GPU workers per gpu  $^{=2}$  , # Worker to pre-fetch data for each single GPU train  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # Train dataset config \n(continues on next page) "}
{"page": 56, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_56.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ntype='CocoDataset', # Type of dataset, refer to https://github.com/open-mmlab/\n—mmdetection/blob/master/mmdet/datasets/coco.py#L19 for details.\nann_file='data/coco/annotations/instances_train2017.json', # Path of annotation.\nfile\nimg_prefix='data/coco/train2017/', # Prefix of image path\npipeline=[ # pipeline, this is passed by the train_pipeline created before.\ndict (type='LoadImageFromFile'),\ndict(\ntype='LoadAnnotations',\nwith_bbox=True,\nwith_mask=True,\npoly2mask=False) ,\ndict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\ndict(type='RandomFlip', flip_ratio=0.5),\ndict(\ntype='Normalize',\nmean=[123.675, 116.28, 103.53],\nstd=[58.395, 57.12, 57.375],\nto_rgb=True),\ndict(type='Pad', size_divisor=32),\ndict (type='DefaultFormatBundle'),\ndict(\ntype='Collect',\nkeys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n)D,\nval=dict( # Validation dataset config\ntype='CocoDataset',\nann_file='data/coco/annotations/instances_val2017.json',\nimg_prefix='data/coco/val2017/',\npipeline=[ # Pipeline is passed by test_pipeline created before\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True) ,\ndict (type='RandomFlip'),\ndict(\ntype='Normalize',\nmean=[123.675, 116.28, 103.53],\nstd=[58.395, 57.12, 57.375],\nto_rgb=True),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img'])\n)\n)D,\ntest=dict( # Test dataset config, modify the ann_file for test-dev/test submission\ntype='CocoDataset',\nann_file='data/coco/annotations/instances_val2017.json',\nimg_prefix='data/coco/val2017/',\n\n(continues on next page)\n\n8.5. An Example of Mask R-CNN 49\n\n", "vlm_text": "type  $=$  ' Coco Data set ' , # Type of dataset, refer to https://github.com/open-mmlab/\n\n  $\\hookrightarrow$  mm detection/blob/master/mmdet/datasets/coco.py#L19 for details.\n\n → ann_file  $=$  ' data/coco/annotations/instances train 2017.json ' , # Path of annotation ␣\n\n  $\\hookrightarrow$  file\n\n → img_prefix  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' data/coco/train2017/ ' , # Prefix of image path pipeline=[ # pipeline, this is passed by the train pipeline created before. dict(type  $=$  ' Load Image From File ' ), dict( type  $:=$  ' Load Annotations ' , with_bbox  $=$  True, with_mask  $\\fallingdotseq$  True, poly2mask  $\\fallingdotseq$  False), dict(type  $=$  ' Resize ' , img_scale  $=$  (1333, 800), keep_ratio  $\\leftrightharpoons$  True), dict(type  $=$  ' RandomFlip ' , flip_ratio  $\\scriptstyle=\\!0$  .5), dict( type  $=$  ' Normalize ' , mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], to_rgb  $\\leftrightharpoons$  True), dict(type  $=$  ' Pad ' , size divisor  $=\\!32$  ), dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Default Format Bundle ' ), dict( type  $=$  ' Collect ' , keys  $=$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ,  ' gt_masks ' ]) ]), val  $=$  dict( # Validation dataset config type  $=$  ' Coco Data set ' , ann_file  $=$  ' data/coco/annotations/instances val 2017.json ' , img_prefix  $=$  ' data/coco/val2017/ ' , pipeline=[ # Pipeline is passed by test pipeline created before dict(type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  ' Load Image From File ' ), dict( type  $={}^{1}$  ' Multi Scale Flip Aug ' , img_scale  $=$  (1333, 800), flip  $\\leftrightharpoons$  False, transforms=[ dict(type  $=$  ' Resize ' , keep_ratio  $=$  True), dict(type  $=$  ' RandomFlip ' ), dict( type  $=$  ' Normalize ' , mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [123.675, 116.28, 103.53], std  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [58.395, 57.12, 57.375], to_rgb  $\\mathrm{\\textmu}$  True), dict(type  $=$  ' Pad ' , size divisor  $\\scriptstyle{=32}$  ), dict(type  $=$  ' Image To Tensor ' , keys  $=$  [ ' img ' ]), dict(type  $=$  ' Collect ' , keys  $=$  [ ' img ' ]) ]) ]), test  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict( # Test dataset config, modify the ann_file for test-dev/test submission type  $=$  ' Coco Data set ' , ann_file  $=$  ' data/coco/annotations/instances val 2017.json ' , img_prefix  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' data/coco/val2017/ ' , "}
{"page": 57, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_57.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\npipeline=[ # Pipeline is passed by test_pipeline created before\ndict (type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True) ,\ndict (type='RandomFlip'),\ndict(\ntype='Normalize',\nmean=[123.675, 116.28, 103.53],\nstd=[58.395, 57.12, 57.375],\nto_rgb=True),\ndict(type='Pad', size_divisor=32),\ndict (type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img'])\n\n1)\n],\nsamples_per_gpu=2 # Batch size of a single GPU used in testing\n))\n\nevaluation = dict( # The config to build the evaluation hook, refer to https://github.\n\n—com/open-mmlab/mmdetection/blob/master/mmdet/core/evaluation/eval_hooks.py#L7 for more.\ndetails.\n\ninterval=1, # Evaluation interval\n\nmetric=['bbox', 'segm']) # Metrics used during evaluation\noptimizer = dict( # Config used to build optimizer, support all the optimizers in.\nPyTorch whose arguments are also the same as those in PyTorch\n\ntype='SGD', # Type of optimizers, refer to https://github.com/open-mmlab/\nmmdetection/blob/master/mmdet/core/optimizer/default_constructor.py#L13 for more.\ndetails\n\nlr=0.02, # Learning rate of optimizers, see detail usages of the parameters in the,\ndocumentation of PyTorch\n\nmomentum=0.9, # Momentum\n\nweight_decay=0.0001) # Weight decay of SGD\noptimizer_config = dict( # Config used to build the optimizer hook, refer to https://\ngithub. com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/optimizer.py#L8 for.\nimplementation details.\n\ngrad_clip=None) # Most of the methods do not use gradient clip\nlr_config = dict( # Learning rate scheduler config used to register LrUpdater hook\n\npolicy='step', # The policy of scheduler, also support CosineAnnealing, Cyclic, etc.\n= Refer to details of supported LrUpdater from https://github.com/open-mmlab/mmcv/blob/\n—master/mmcv/runner/hooks/1r_updater.py#L9.\n\nwarmup='linear', # The warmup policy, also support ‘exp’ and ‘constant.\n\nwarmup_iters=500, # The number of iterations for warmup\n\nwarmup_ratio=\n\n0.001, # The ratio of the starting learning rate used for warmup\n\nstep=[8, 11]) # Steps to decay the learning rate\nrunner = dict(\n\ntype='EpochBasedRunner', # Type of runner to use (i.e. IterBasedRunner or.\n-+EpochBasedRunner)\n\nmax_epochs=12) # Runner that runs the workflow in total max_epochs. For.\n> IterBasedRunner use max_iters~\n\n(continues on next page)\n\n50 Chapter 8. Tutorial 1: Learn about Configs\n", "vlm_text": "The image contains a configuration script for a machine learning model, likely related to the MMDetection library. It includes settings for data processing pipelines, evaluation metrics, optimizer configuration, and learning rate schedules. Specific parameters, such as image scaling, normalization values, batch size, optimizer type, and training schedule, are specified. It also references specific parts of documentation for further details."}
{"page": 58, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_58.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ncheckpoint_config = dict( # Config to set the checkpoint hook, Refer to https://github.\n—com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/checkpoint.py for implementation.\n\ninterval=1) # The save interval is 1\nlog_config = dict( # config to register logger hook\n\ninterval=50, # Interval to print the log\n\nhooks=[\n\n# dict(type='TensorboardLoggerHook') # The Tensorboard logger is also supported\ndict (type='TextLoggerHook')\n\n]) # The logger used to record the training process.\ndist_params = dict(backend='nccl') # Parameters to setup distributed training, the port.\ncan also be set.\nlog_level = 'INFO' # The level of logging.\nload_from = None # load models as a pre-trained model from a given path. This will not,\nresume training.\nresume_from = None # Resume checkpoints from a given path, the training will be resumed.\nfrom the epoch when the checkpoint's is saved.\nworkflow = [('train', 1)] # Workflow for runner. [('train', 1)] means there is only one.\nworkflow and the workflow named 'train' is executed once. The workflow trains the.\nmodel by 12 epochs according to the total_epochs.\nwork_dir = 'work_dir' # Directory to save the model checkpoints and logs for the.\ncurrent experiments.\n\n8.6 FAQ\n\n8.6.1 Ignore some fields in the base configs\n\nSometimes, you may set _delete_=True to ignore some of fields in base configs. You may refer to mmcy for simple\nillustration.\n\nIn MMDetection, for example, to change the backbone of Mask R-CNN with the following config.\n\nmodel = dict(\ntype='MaskRCNN',\npretrained='torchvision://resnet50',\nbackbone=dict (\ntype='ResNet',\ndepth=50,\nnum_stages=4,\nout_indices=(0, 1, 2, 3),\nfrozen_stages=1,\nnorm_cfg=dict(type='BN', requires_grad=True) ,\nnorm_eval=True,\nstyle='pytorch'),\nneck=dict(...),\nrpn_head=dict(...),\nroi_head=dict(...))\n\nResNet and HRNet use different keywords to construct.\n\n8.6. FAQ 51\n\n", "vlm_text": "The image contains a configuration script for a machine learning setup, likely related to setting up training routines. Here's a breakdown of the key components:\n\n1. **checkpoint_config**: Configures checkpoint settings, specifying save intervals and providing a reference URL for implementation.\n\n2. **log_config**: Sets up logging configurations, with intervals for logs and hooks for Tensorboard and text logging.\n\n3. **dist_params**: Contains parameters for distributed training using the 'nccl' backend.\n\n4. **log_level**: Specifies the log level as 'INFO'.\n\n5. **load_from**: Indicates path settings for loading pre-trained models (initially set to `None`).\n\n6. **resume_from**: Specifies how to resume training from checkpoints (initially set to `None`).\n\n7. **workflow**: Defines the training workflow with a tuple `('train', 1)` indicating a single training phase per iteration.\n\n8. **work_dir**: Designates a directory for saving model checkpoints and logs.\n8.6 FAQ \n8.6.1 Ignore some fields in the base configs \nSometimes, you may set  _delete_  $\\bar{\\cdot}$  True  to ignore some of fields in base configs. You may refer to  mmcv  for simple illustration. \nIn MM Detection, for example, to change the backbone of Mask R-CNN with the following config. \nThe image is a code block showing a Python dictionary configuration for setting up a model, specifically a Mask R-CNN (Mask Region-based Convolutional Neural Network). Here's a breakdown of this configuration:\n\n- `model`: This is a dictionary that defines the model configuration.\n  \n- `type`: The type of model is specified as `'MaskRCNN'`.\n  \n- `pretrained`: Indicates a pretrained model is being used, in this case, the ResNet-50 model from torchvision.\n  \n- `backbone`: This is a nested dictionary that specifies the backbone network for the model.\n  - `type`: The backbone type is `'ResNet'`.\n  - `depth`: The depth of the ResNet is 50.\n  - `num_stages`: The number of stages is 4.\n  - `out_indices`: A tuple specifying which stages to output, here, (0, 1, 2, 3).\n  - `frozen_stages`: Specifies that stage 1 is frozen and will not be updated during training.\n  - `norm_cfg`: Another nested dictionary for normalization configuration.\n    - `type`: Batch Normalization ('BN') is used.\n    - `requires_grad`: This is set to `True`, meaning gradients are computed for the normalization layers, allowing them to be updated during training.\n  - `norm_eval`: Evaluates the model with running statistics on current batch if `True`.\n  - `style`: Specifies the style, in this case, it's `'pytorch'`.\n\n- `neck`, `rpn_head`, and `roi_head`: These are placeholders for further configurations specific to the FPN neck, RPN head, and ROI head components of the Mask R-CNN model, but the specific details are not shown in the image.\nResNet  and  HRNet  use different keywords to construct. "}
{"page": 59, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_59.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\nmodel = dict(\npretrained='open-mmlab://msra/hrnetv2_w32',\nbackbone=dict (\n_delete_=True,\ntype='HRNet',\nextra=dict(\nstagel=dict(\nnum_modules=1,\nnum_branches=1,\nblock='BOTTLENECK',\nnum_blocks=(4, ),\nnum_channels=(64, )),\nstage2=dict(\nnum_modules=1,\nnum_branches=2,\nblock='BASIC',\nnum_blocks=(4, 4),\nnum_channels=(32, 64)),\nstage3=dict(\nnum_modules=4,\nnum_branches=3,\nblock='BASIC',\nnum_blocks=(4, 4, 4),\nnum_channels=(32, 64, 128)),\nstage4=dict(\nnum_modules=3,\nnum_branches=4,\nblock='BASIC',\nnum_blocks=(4, 4, 4, 4),\nnum_channels=(32, 64, 128, 256)))),\nneck=dict(...))\n\nThe _delete_=True would replace all old keys in backbone field with new keys.\n\n8.6.2 Use intermediate variables in configs\n\nSome intermediate variables are used in the configs files, like train_pipeline/test_pipeline in datasets. It’s\nworth noting that when modifying intermediate variables in the children configs, user need to pass the intermediate\nvariables into corresponding fields again. For example, we would like to use multi scale strategy to train a Mask R-CNN.\ntrain_pipeline/test_pipeline are intermediate variable we would like modify.\n\n_base_ = './mask_rcnn_r50_fpn_1x_coco.py'\nimg_norm_cfg = dict(\nmean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\ndict (type='LoadImageFromFile'),\ndict(type='LoadAnnotations', with_bbox=True, with_mask=True) ,\ndict\ntype='Resize',\nimg_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n(1333, 768), (1333, 800)],\n\n(continues on next page)\n\n52 Chapter 8. Tutorial 1: Learn about Configs\n\n", "vlm_text": "The image shows a configuration script written in Python for a model, likely related to computer vision or deep learning. It specifies the use of a pretrained model, `open-mmlab://msra/hrnetv2_w32`, and details the `backbone` structure with parameters such as `type`, `num_modules`, `num_branches`, `block`, `num_blocks`, and `num_channels` for different stages (`stage1`, `stage2`, `stage3`, and `stage4`). The `neck` section is also defined but not fully shown. This configuration appears to be for a High-Resolution Network (HRNet) used in Mask RCNN setups for tasks such as image segmentation.\nThe  _delete_  $\\overrightharpoon{\\cdot}$  True  would replace all old keys in  backbone  field with new keys. \n8.6.2 Use intermediate variables in configs \nSome intermediate variables are used in the configs files, like  train pipeline / test pipeline  in datasets. It’s worth noting that when modifying intermediate variables in the children configs, user need to pass the intermediate variables into corresponding fields again. For example, we would like to use multi scale strategy to train a Mask R-CNN. train pipeline / test pipeline  are intermediate variable we would like modify. \n_base_  $=$   ' ./mask r cnn r 50 fp n 1 x coco.py ' img norm cf g  $=$   dict ( mean  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ 123.675 ,  116.28 ,  103.53 ], std  $\\overbar{\\;-\\;}$  [ 58.395 ,  57.12 ,  57.375 ], to_rgb  $\\risingdotseq$  True ) train pipeline  $=$   [ dict ( type  $=\"$  Load Image From File ' ), dict ( type  $=\"$  Load Annotations ' , with_bbox  $\\fallingdotseq$  True , with_mask  $\\risingdotseq$  True ), dict ( type  $\\equiv^{\\dagger}$  Resize ' , img_scale  $=$  [( 1333 ,  640 ), ( 1333 ,  672 ), ( 1333 ,  704 ), ( 1333 ,  736 ), ( 1333 ,  768 ), ( 1333 ,  800 )], "}
{"page": 60, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_60.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n]\n\nmultiscale_mode=\"value\",\nkeep_ratio=True) ,\ndict(type='RandomFlip', flip_ratio=0.5),\ndict(type='Normalize', **img_norm_cfg),\ndict(type='Pad', size_divisor=32),\ndict (type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n\ntest_pipeline = [\n\n]\n\ndict (type='LoadImageFromFile'),\ndict\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntrans forms=[\ndict(type='Resize', keep_ratio=True) ,\ndict(type='RandomFlip')\ndict(type='Normalize', **img_norm_cfg),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n\n)\n\ndata = dict(\n\ntrain=dict (pipeline=train_pipeline),\nval=dict(pipeline=test_pipeline),\ntest=dict(pipeline=test_pipeline))\n\nWe first define the new train_pipeline/test_pipeline and pass them into data.\n\nSimilarly, if we would like to switch from SyncBN to BN or MMSyncBN, we need to substitute every norm_cfg in the\n\nconfig.\n\n_base_ = './mask_rcnn_r50_fpn_1x_coco.py'\nnorm_cfg = dict(type='BN', requires_grad=True)\nmodel = dict(\n\nbackbone=dict (norm_cfg=norm_cfg) ,\nneck=dict (norm_cfg=norm_cfg) ,\nwd)\n\n8.6. FAQ 53\n\n", "vlm_text": "This image shows a segment of code written in Python, likely related to data preprocessing pipelines for a machine learning or deep learning framework such as MMDetection, which is used for object detection tasks. \n\nThe code snippet contains:\n\n1. Definitions for `train_pipeline` and `test_pipeline` configurations using a list of transformation dictionaries. These transformations include data augmentations and preprocessing steps such as:\n   - `RandomFlip`: Randomly flips images with a specified probability.\n   - `Normalize`: Normalizes the image using given parameters.\n   - `Pad`: Pads the image to ensure the dimensions are divisible by a specific number, in this case, 32.\n   - `DefaultFormatBundle` and `Collect`: Formats and collects necessary data from the dictionary.\n   - For testing, additional transformations like `ImageToTensor` are present to prepare the image tensor.\n\n2. The `test_pipeline` involves loading an image from a file and further image transformations including:\n   - `MultiScaleFlipAug`: Multi-scale augmentation to resize the image to a fixed scale and apply additional transformations.\n   - A similar list of transformations for resizing, normalizing, and collecting data as in the training pipeline.\n\n3. A `data` dictionary which organizes the pipelines for training, validation, and testing phases, specifying which pipeline to use for their respective datasets. \n\nThis code is commonly used in the configuration files of machine learning models to standardize the input data processing.\nWe first define the new  train pipeline / test pipeline  and pass them into  data \nSimilarly, if we would like to switch from  SyncBN  to  BN  or  MMSyncBN , we need to substitute every  norm_cfg  in the config. \n_base_  $=$   ' ./mask r cnn r 50 fp n 1 x coco.py ' norm_cfg  $=$   dict ( type  $\\equiv^{\\dagger}$  BN ' , requires grad = True ) model  $=$   dict ( backbone  $=$  dict (norm_cfg  $=$  norm_cfg), neck  $\\fallingdotseq$  dict (norm_cfg  $|=$  norm_cfg), ... ) "}
{"page": 61, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_61.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n54 Chapter 8. Tutorial 1: Learn about Configs\n", "vlm_text": "MMDetection, Release 2.18.0\n\n54 Chapter 8. Tutorial 1: Learn about Configs\n"}
{"page": 62, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_62.jpg", "ocr_text": "CHAPTER\nNINE\n\nTUTORIAL 2: CUSTOMIZE DATASETS\n\n9.1 Support new data format\n\nTo support a new data format, you can either convert them to existing formats (COCO format or PASCAL format) or\ndirectly convert them to the middle format. You could also choose to convert them offline (before training by a script)\nor online (implement a new dataset and do the conversion at training). In MMDetection, we recommend to convert the\ndata into COCO formats and do the conversion offline, thus you only need to modify the config’s data annotation paths\nand classes after the conversion of your data.\n\n9.1.1 Reorganize new data formats to existing format\n\nThe simplest way is to convert your dataset to existing dataset formats (COCO or PASCAL VOC).\n\nThe annotation json files in COCO format has the following necessary keys:\n\n‘images': [\n{\n\"file_name': 'COCO_val2014_000000001268.jpg',\n\"height': 427,\n‘'width': 640,\n‘id': 1268\n3,\n\n],\n\n‘annotations': [\n{\n\"segmentation': [[192.81,\n247.09,\n219.03,\n249.06]], # if you have mask labels\n‘area': 1035.749,\n‘iscrowd': 0,\n‘image_id': 1268,\n\"pbox': [192.81, 224.8, 74.73, 33.43],\n\"category_id': 16,\n‘id': 42986\n3,\n\n(continues on next page)\n\n55\n\n", "vlm_text": "TUTORIAL 2: CUSTOMIZE DATASETS \n9.1 Support new data format \nTo support a new data format, you can either convert them to existing formats (COCO format or PASCAL format) or directly convert them to the middle format. You could also choose to convert them offline (before training by a script) or online (implement a new dataset and do the conversion at training). In MM Detection, we recommend to convert the data into COCO formats and do the conversion offline, thus you only need to modify the config’s data annotation paths and classes after the conversion of your data. \n9.1.1 Reorganize new data formats to existing format \nThe simplest way is to convert your dataset to existing dataset formats (COCO or PASCAL VOC). \nThe annotation json files in COCO format has the following necessary keys: \nThe image contains a portion of a data structure, possibly a JSON or similar format, used to describe image metadata and annotations. It includes the following information:\n\n1. The `images` section provides details about an image file:\n   - `file_name`: 'COCO_val2014_000000001268.jpg'\n   - `height`: 427 pixels\n   - `width`: 640 pixels\n   - `id`: 1268\n\n2. The `annotations` section describes annotations associated with the image:\n   - `segmentation`: A list of coordinates representing the segmentation mask if mask labels are present.\n   - `area`: 1035.749, which could represent the area of the segmented region.\n   - `iscrowd`: 0, indicating whether the annotation represents a crowd of objects.\n   - `image_id`: 1268, linking the annotation to the corresponding image.\n   - `bbox`: A bounding box around the object in the format [x, y, width, height].\n   - `category_id`: 16, which might correspond to a certain category or class in the dataset.\n   - `id`: 42986, an identifier for the annotation."}
{"page": 63, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_63.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n],\n\n‘categories': [\n{'id': 0, 'name': 'car'},\n\n]\n\nThere are three necessary keys in the json file:\n* images: contains a list of images with their information like file_name, height, width, and id.\n* annotations: contains the list of instance annotations.\n* categories: contains the list of categories names and their ID.\n\nAfter the data pre-processing, there are two steps for users to train the customized new dataset with existing format\n(e.g. COCO format):\n\n1. Modify the config file for using the customized dataset.\n2. Check the annotations of the customized dataset.\n\nHere we give an example to show the above two steps, which uses a customized dataset of 5 classes with COCO format\nto train an existing Cascade Mask R-CNN RS50-FPN detector.\n\n1. Modify the config file for using the customized dataset\n\nThere are two aspects involved in the modification of config file:\n\n1. The data field. Specifically, you need to explicitly add the classes fields in data.train, data.val and\ndata.test.\n\n2. The num_classes field in the model part. Explicitly over-write all the num_classes from default value (e.g.\n80 in COCO) to your classes number.\n\nIn configs/my_custom_config.py:\n\n# the new config inherits the base configs to highlight the necessary modification\n-base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py'\n\n#1. dataset settings\ndataset_type = 'CocoDataset'\nclasses = ('a', 'b', ‘'c', ‘d', ‘e')\ndata = dict(\nsamples_per_gpu=2,\nworkers_per_gpu=2,\ntrain=dict(\ntype=dataset_type,\n# explicitly add your class names to the field ‘classes*\nclasses=classes,\nann_file='path/to/your/train/annotation_data',\nimg_prefix='path/to/your/train/image_data'),\nval=dict(\ntype=dataset_type,\n# explicitly add your class names to the field ‘classes*\nclasses=classes,\n\n(continues on next page)\n\n56 Chapter 9. Tutorial 2: Customize Datasets\n\n", "vlm_text": "The image shows a snippet of JSON-like data, which is structured as a Python dictionary or a similar format. This snippet contains a single key called 'categories'. The 'categories' key is associated with a list that contains a single dictionary with two key-value pairs: 'id' with a value of 0, and 'name' with a value of 'car'. This suggests that the data represents or categorizes something as a \"car\" with an associated ID of 0.\nThere are three necessary keys in the json file: \n•  images : contains a list of images with their information like  file_name ,  height ,  width , and  id . •  annotations : contains the list of instance annotations. •  categories : contains the list of categories names and their ID. \nAfter the data pre-processing, there are two steps for users to train the customized new dataset with existing format (e.g. COCO format): \n1. Modify the config file for using the customized dataset. 2. Check the annotations of the customized dataset. \nHere we give an example to show the above two steps, which uses a customized dataset of 5 classes with COCO format to train an existing Cascade Mask R-CNN R50-FPN detector. \n1. Modify the config file for using the customized dataset \nThere are two aspects involved in the modification of config file: \n1. The  data  field. Specifically, you need to explicitly add the  classes  fields in  data.train ,  data.val  and data.test . 2. The  num classes  field in the  model  part. Explicitly over-write all the  num classes  from default value (e.g. 80 in COCO) to your classes number. \nIn  configs/my custom config.py : \nThis image shows a Python configuration script for setting up a dataset. It includes the following components:\n\n1. **Base Configuration**: Inherits from `./cascade_mask_rcnn_r50_fpn_1x_coco.py`.\n\n2. **Dataset Settings**:\n   - `dataset_type`: `'CocoDataset'`\n   - `classes`: A tuple with elements `'a'`, `'b'`, `'c'`, `'d'`, `'e'`\n\n3. **Data Dictionary**:\n   - `samples_per_gpu`: `2`\n   - `workers_per_gpu`: `2`\n\n4. **Train and Validation Configs**:\n   - Both have a `type` set to `dataset_type` and `classes` assigned the value of `classes`.\n   - `train` configuration specifies file paths for annotations and image data:\n     - `ann_file`: `'path/to/your/train/annotation_data'`\n     - `img_prefix`: `'path/to/your/train/image_data'`\n(continues on next page) "}
{"page": 64, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_64.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nann_file='path/to/your/val/annotation_data',\nimg_prefix='path/to/your/val/image_data'),\n\ntest=dict(\ntype=dataset_type,\n# explicitly add your class names to the field ‘classes*\nclasses=classes,\nann_file='path/to/your/test/annotation_data',\nimg_prefix='path/to/your/test/image_data'))\n\n# 2. model settings\n\n# explicitly over-write all the ‘num_classes field from default 80 to 5.\nmodel = dict(\nroi_head=dict(\nbbox_head=\ndict(\ntype='Shared2FCBBoxHead',\n# explicitly over-write all the ‘num_classes* field from default 80 to 5.\nnum_classes=5),\ndict(\ntype='Shared2FCBBoxHead',\n# explicitly over-write all the ‘num_classes* field from default 80 to 5.\nnum_classes=5),\ndict(\ntype='Shared2FCBBoxHead',\n# explicitly over-write all the ‘num_classes* field from default 80 to 5.\nnum_classes=5)],\n# explicitly over-write all the ‘num_classes field from default 80 to 5.\nmask_head=dict (num_classes=5)))\n\n2. Check the annotations of the customized dataset\n\nAssuming your customized dataset is COCO format, make sure you have the correct annotations in the customized\ndataset:\n\n1. The length for categories field in annotations should exactly equal the tuple length of classes fields in your\nconfig, meaning the number of classes (e.g. 5 in this example).\n\n2. The classes fields in your config file should have exactly the same elements and the same order with the name\nin categories of annotations. MMDetection automatically maps the uncontinuous id in categories to the\ncontinuous label indices, so the string order of name in categories field affects the order of label indices.\nMeanwhile, the string order of classes in config affects the label text during visualization of predicted bounding\nboxes.\n\n3. The category_id in annotations field should be valid, i.e., all values in category_id should belong to id\nin categories.\n\nHere is a valid example of annotations:\n\n‘annotations': [\n\n{\n\n\"segmentation': [[192.81,\n\n(continues on next page)\n\n9.1. Support new data format 57\n\n", "vlm_text": "The image shows a snippet of code written in Python. It appears to be a configuration script for setting up a machine learning model, likely for an object detection task. The code specifies paths for validation and test data, including annotation data and image data. It defines model settings, particularly for a region of interest (ROI) head and a mask head within the model. The script explicitly changes the number of classes from the default of 80 to 5, as indicated in multiple comments and settings throughout the code. This is done by setting the `num_classes` parameter to 5 for each component in the model.\n2. Check the annotations of the customized dataset \nAssuming your customized dataset is COCO format, make sure you have the correct annotations in the customized dataset: \n1. The length for  categories  field in annotations should exactly equal the tuple length of  classes  fields in your config, meaning the number of classes (e.g. 5 in this example). 2. The  classes  fields in your config file should have exactly the same elements and the same order with the  name in  categories  of annotations. MM Detection automatically maps the un continuous  id  in  categories  to the continuous label indices, so the string order of  name  in  categories  field affects the order of label indices. Meanwhile, the string order of  classes  in config affects the label text during visualization of predicted bounding boxes. \nin  categories . \nHere is a valid example of annotations: \nThe image is a snippet of code or a data structure in JSON-like format. It shows an entry with the key `'annotations'`, which contains a list. Inside the list, there's an object with a key `'segmentation'` that includes a nested list with a number `192.81`. It appears to be part of data used for image annotation or segmentation tasks.\n(continues on next page) "}
{"page": 65, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_65.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n247.09,\n219.03,\n249.06]], # if you have mask labels\n‘area': 1035.749,\n‘iscrowd': 0,\n‘image_id': 1268,\n\"pbox': [192.81, 224.8, 74.73, 33.43],\n\"category_id': 16,\n‘id': 42986\n3,\n\n],\n\n# MMDetection automatically maps the uncontinuous ‘id* to the continuous label indices.\n‘categories': [\n\n{'id': 1, 'name': 'a'}, {'id': 3, 'name': 'b'}, {'id': 4, 'name': 'c'}, {'id': 16,\n'name': 'd'}, {'id': 17, 'name': ‘'e'},\n\n]\n\nWe use this way to support CityScapes dataset. The script is in cityscapes.py and we also provide the finetuning configs.\nNote\n\n1. For instance segmentation datasets, MMDetection only supports evaluating mask AP of dataset in COCO\nformat for now.\n\n2. It is recommended to convert the data offline before training, thus you can still use CocoDataset and only need\nto modify the path of annotations and the training classes.\n\n9.1.2 Reorganize new data format to middle format\n\nIt is also fine if you do not want to convert the annotation format to COCO or PASCAL format. Actually, we define a\nsimple annotation format and all existing datasets are processed to be compatible with it, either online or offline.\n\nThe annotation of a dataset is a list of dict, each dict corresponds to an image. There are 3 field filename (relative path),\nwidth, height for testing, and an additional field ann for training. ann is also a dict containing at least 2 fields: bboxes\nand labels, both of which are numpy arrays. Some datasets may provide annotations like crowd/difficult/ignored\nbboxes, we use bboxes_ignore and labels_ignore to cover them.\n\nHere is an example.\n\n[\n{\n'filename': 'a.jpg',\n'width': 1280,\n\"height': 720,\n‘ann': {\n\n‘bboxes': <np.ndarray, float32> (mn, 4),\n\n‘labels': <np.ndarray, int64> (n, ),\n\n‘bboxes_ignore': <np.ndarray, float32> (k, 4),\n‘labels_ignore': <np.ndarray, int64> (k, ) (optional field)\n\n(continues on next page)\n\n58 Chapter 9. Tutorial 2: Customize Datasets\n\n", "vlm_text": "This image contains a snippet of code that appears to be part of a dataset or annotation format often used in object detection tasks. Here’s a breakdown:\n\n- **Mask Labels**: There are placeholder values like `247.09`, `219.03`, and `249.06` which might represent points if masks are involved.\n- **Attributes**:\n  - `'area'`: The area of the bounding box is `1035.749`.\n  - `'iscrowd'`: A flag (`0`) indicating whether the annotation refers to a single object or a group.\n  - `'image_id'`: The ID of the image is `1268`.\n  - `'bbox'`: The bounding box coordinates are `[192.81, 224.8, 74.73, 33.43]`.\n  - `'category_id'`: The category ID is `16`.\n  - `'id'`: Unique ID `42986` for the annotation.\n\n- **Categories**: Listed with ID and name pairs:\n  - `{ 'id': 1, 'name': 'a' }`\n  - `{ 'id': 3, 'name': 'b' }`\n  - `{ 'id': 4, 'name': 'c' }`\n  - `{ 'id': 16, 'name': 'd' }`\n  - `{ 'id': 17, 'name': 'e' }`\n\n- **Note**: There is a comment indicating that MMDetection, an object detection framework, automatically maps these IDs to continuous label indices.\nWe use this way to support CityScapes dataset. The script is in  cityscapes.py  and we also provide the finetuning  configs . \nNote \n1. For instance segmentation datasets,  MM Detection only supports evaluating mask AP of dataset in COCO format for now . 2. It is recommended to convert the data offline before training, thus you can still use  Coco Data set  and only need to modify the path of annotations and the training classes. \n9.1.2 Reorganize new data format to middle format \nIt is also fine if you do not want to convert the annotation format to COCO or PASCAL format. Actually, we define a simple annotation format and all existing datasets are processed to be compatible with it, either online or offline. \nThe annotation of a dataset is a list of dict, each dict corresponds to an image. There are 3 field  filename  (relative path), width ,  height  for testing, and an additional field  ann  for training.  ann  is also a dict containing at least 2 fields:  bboxes and  labels , both of which are numpy arrays. Some datasets may provide annotations like crowd/difficult/ignored bboxes, we use  b boxes ignore  and  labels ignore  to cover them. \nHere is an example. \n[ { ' filename ' :  ' a.jpg ' , ' width ' :  1280 , ' height ' :  720 , ' ann ' : { ' bboxes ' :  < np . ndarray, float  $32\\textgreater$   (n,  4 ), ' labels ' :  < np . ndarray, int  $64\\textgreater$   (n, ), ' b boxes ignore ' :  < np . ndarray, float  $32\\textgreater$   (k,  4 ), ' labels ignore ' :  < np . ndarray, int  $64\\textgreater$   (k, ) (optional field) } \n(continues on next page) "}
{"page": 66, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_66.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n3,\n]\n\nThere are two ways to work with custom datasets.\n* online conversion\n\nYou can write a new Dataset class inherited from CustomDataset, and overwrite two methods\nload_annotations(self, ann_file) and get_ann_info(self, idx), like CocoDataset and VOC-\nDataset.\n\n* offline conversion\n\nYou can convert the annotation format to the expected format above and save it to a pickle or json file, like\npascal_voc.py. Then you can simply use CustomDataset.\n\n9.1.3 An example of customized dataset\n\nAssume the annotation is in a new format in text files. The bounding boxes annotations are stored in text file\nannotation. txt as the following\n\n#\n\n000001. jpg\n1280 720\n\n2\n\n10 20 40 60 1\n20 40 50 60 2\n#\n\n000002. jpg\n1280 720\n\n3\n\n50 20 40 60 2\n20 40 30 45 2\n30 40 50 60 3\n\nWe can create a new dataset in mmdet/datasets/my_dataset.py to load the data.\n\nimport mmcv\nimport numpy as np\n\nfrom .builder import DATASETS\nfrom .custom import CustomDataset\n@DATASETS . register_module()\nclass MyDataset(CustomDataset):\nCLASSES = ('person', 'bicycle', 'car', 'motorcycle')\n\ndef load_annotations(self, ann_file):\nann_list = mmcv.list_from_file(ann_file)\n\n(continues on next page)\n\n9.1. Support new data format 59\n\n", "vlm_text": "The provided image does not contain any visible data or recognizable table structure. It only shows a section of what might be code, with characters like \"},\" and \"...]\". Without additional context or a more complete image, it's not possible to determine the contents or purpose of this table.\nThere are two ways to work with custom datasets. \n• online conversion \nYou can write a new Dataset class inherited from  Custom Data set , and overwrite two methods load annotations(self, ann_file)  and  get ann info(self, idx) , like  Coco Data set  and  VOC- Dataset . \n• offline conversion \nYou can convert the annotation format to the expected format above and save it to a pickle or json file, like pascal_voc.py . Then you can simply use  Custom Data set . \n9.1.3 An example of customized dataset \nAssume the annotation is in a new format in text files. The bounding boxes annotations are stored in text file annotation.txt  as the following \nThis table appears to represent annotations or metadata information about a series of image files, possibly for an image dataset. Each section provides details for a different image. The structure of each section is as follows:\n\n1. A line beginning with `#`, which may denote the start of a new image entry.\n2. A filename with a `.jpg` extension, indicating the image file being referenced (e.g., `000001.jpg` and `000002.jpg`).\n3. Two numbers following the filename, which likely indicate the width and height of the image (e.g., `1280 720`).\n4. A single number that follows, indicating the number of annotations or regions of interest in the image (e.g., `2` and `3`).\n5. Subsequent lines contain a series of numbers, which are seemingly grouped in sequences of five. Each sequence could represent an annotation box in the image and may have the structure: `x y width height class`, where `x` and `y` are coordinates, `width` and `height` are the dimensions of the bounding box, and `class` is an identifier for the object class being annotated (in the image).\n\nEach image section details:\n- `000001.jpg` has dimensions `1280 x 720` and contains 2 annotated regions.\n- `000002.jpg` also has dimensions `1280 x 720` and contains 3 annotated regions.\n\nThe numbers on the lines after the count likely describe the position and dimensions of bounding boxes for object detection tasks within each specified image.\nWe can create a new dataset in  mmdet/datasets/my_dataset.py  to load the data. \nThe image contains a snippet of Python code that seems to define a custom dataset for a machine learning project, potentially using the MMDetection framework. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `mmcv` and `numpy` are imported, with `numpy` being aliased as `np`.\n   - `DATASETS` is imported from a module named `.builder`.\n   - `CustomDataset` is imported from a module named `.custom`.\n\n2. **Class Definition**:\n   - A dataset class named `MyDataset` is defined, which inherits from `CustomDataset`.\n   - The dataset class is registered with a decorator `@DATASETS.register_module()`, which integrates this dataset into a framework, most likely MMDetection's dataset registry.\n\n3. **Attributes and Methods**:\n   - A class attribute `CLASSES` is defined as a tuple containing the classes `('person', 'bicycle', 'car', 'motorcycle')`.\n   - A method `load_annotations` is defined, which takes `self` and `ann_file` as parameters. This method reads annotations from a file using `mmcv.list_from_file(ann_file)` and stores it in `ann_list`.\n\nThis code is likely a part of a larger object detection framework setup. The `MyDataset` class appears to be a custom implementation tailored to handle datasets containing annotations for specific classes like person, bicycle, car, and motorcycle."}
{"page": 67, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_67.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ndata_infos = []\nfor i, ann_line in enumerate(ann_list):\nif ann_line != '#':\ncontinue\n\nimg_shape = ann_list[i + 2].splitC' ')\nwidth = int(img_shape[0])\n\nheight = int(img_shape[1])\nbbox_number = int(ann_list[i + 3])\n\nanns = ann_line.split(' ')\n\nbboxes = []\n\nlabels 0\n\nfor anns in ann_list[i + 4:i + 4 + bbox_number]:\nbboxes.append([float(ann) for ann in anns[:4]])\nlabels. append(int(anns[4]))\n\ndata_infos.append(\ndict(\nfilename=ann_list[i + 1],\nwidth=width,\nheight=height,\nann=dict(\nbboxes=np. array (bboxes) .astype(np. float32),\nlabels=np.array(labels) .astype(np.int64))\n))\n\nreturn data_infos\n\ndef get_ann_info(self, idx):\nreturn self.data_infos[idx]['ann']\n\nThen in the config, to use MyDataset you can modify the config as the following\n\ndataset_A_train = dict(\ntype='MyDataset',\nann_file = 'image_list.txt',\npipeline=train_pipeline\n\n9.2 Customize datasets by dataset wrappers\n\nMMDetection also supports many dataset wrappers to mix the dataset or modify the dataset distribution for training.\nCurrently it supports to three dataset wrappers as below:\n\n* RepeatDataset: simply repeat the whole dataset.\n* ClassBalancedDataset: repeat dataset in a class balanced manner.\n\n* ConcatDataset: concat datasets.\n\n60 Chapter 9. Tutorial 2: Customize Datasets\n\n", "vlm_text": "The image contains a snippet of Python code. The code appears to process a list of annotations (`ann_list`) and extracts information about image files, including their dimensions and bounding boxes with associated labels. The processed data is stored in a list called `data_infos`, each entry of which is a dictionary containing:\n\n- `filename`: The name of the image file.\n- `width`: The width of the image.\n- `height`: The height of the image.\n- `ann`: A dictionary with:\n  - `bboxes`: An array of bounding boxes as `np.float32`.\n  - `labels`: An array of labels as `np.int64`.\n\nThe `get_ann_info` method takes an index `idx` and returns the annotation information for the corresponding image from `data_infos`.\n\nKey parts of the code:\n- It loops through `ann_list` and checks if each line starts with `'#'`. If not, it continues to the next line.\n- It extracts image shape and bounding box number from specific indices.\n- Bounding box and label data are extracted and appended to the respective lists.\n- The processed information is appended to the `data_infos` list as a dictionary.\n- The `get_ann_info` function uses the `self.data_infos` list to retrieve information based on the provided index.\nThen in the config, to use  MyDataset  you can modify the config as the following \ndata set A train  $=$   dict ( type = ' MyDataset ' , ann_file  $=$   ' image_list.txt ' , pipeline  $=$  train pipeline ) \n9.2 Customize datasets by dataset wrappers \nMM Detection also supports many dataset wrappers to mix the dataset or modify the dataset distribution for training. Currently it supports to three dataset wrappers as below: \n•  Repeat Data set : simply repeat the whole dataset. •  Class Balanced Data set : repeat dataset in a class balanced manner. •  Con cat Data set : concat datasets. "}
{"page": 68, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_68.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n9.2.1 Repeat dataset\n\nWe use RepeatDataset as wrapper to repeat the dataset. For example, suppose the original dataset is Dataset_A, to\nrepeat it, the config looks like the following\n\ndataset_A_train = dict(\ntype='RepeatDataset',\ntimes=N,\ndataset=dict( # This is the original config of Dataset_A\ntype='Dataset_A',\n\npipeline=train_pipeline\n\n9.2.2 Class balanced dataset\n\nWe use ClassBalancedDataset as wrapper to repeat the dataset based on category frequency. The dataset to repeat\nneeds to instantiate function self .get_cat_ids (idx) to support ClassBalancedDataset. For example, to repeat\nDataset_A with oversample_thr=1e-3, the config looks like the following\n\ndataset_A_train = dict(\ntype='ClassBalancedDataset',\noversample_thr=1e-3,\ndataset=dict( # This is the original config of Dataset_A\ntype='Dataset_A',\n\npipeline=train_pipeline\n\nYou may refer to source code for details.\n\n9.2.3 Concatenate dataset\n\nThere are three ways to concatenate the dataset.\n\n1. If the datasets you want to concatenate are in the same type with different annotation files, you can concatenate\nthe dataset configs like the following.\n\ndataset_A_train = dict(\ntype='Dataset_A',\nann_file = ['anno_file_1', '‘anno_file_2'],\npipeline=train_pipeline\n\nIf the concatenated dataset is used for test or evaluation, this manner supports to evaluate each dataset separately.\nTo test the concatenated datasets as a whole, you can set separate_eval=False as below.\n\ndataset_A_train = dict(\ntype='Dataset_A',\nann_file = ['anno_file_1', '‘anno_file_2'],\n\n(continues on next page)\n\n9.2. Customize datasets by dataset wrappers 61\n\n", "vlm_text": "9.2.1 Repeat dataset \nWe use  Repeat Data set  as wrapper to repeat the dataset. For example, suppose the original dataset is  Dataset_A , to repeat it, the config looks like the following \ndata set A train  $=$   dict ( type  $\\equiv^{\\dagger}$  Repeat Data set ' , times  $\\tt_{\\tau,=N}$  , dataset  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict ( # This is the original config of Dataset_A type  $\\equiv^{\\dagger}$  Dataset_A ' , ... pipeline  $=$  train pipeline ) ) \n9.2.2 Class balanced dataset \nWe use  Class Balanced Data set  as wrapper to repeat the dataset based on category frequency. The dataset to repeat needs to instantiate function  self.get cat ids(idx)  to support  Class Balanced Data set . For example, to repeat Dataset_A  with  over sample thr  $\\scriptstyle:=1e^{-3}$  , the config looks like the following \nThe image shows a code snippet written in Python. It defines a dictionary `dataset_A_train` with the following structure:\n\n- **type**: 'ClassBalancedDataset'\n- **oversample_thr**: 1e-3\n- **dataset**: Another dictionary containing:\n  - **type**: 'Dataset_A'\n  - A comment: \"This is the original config of Dataset_A\"\n  - An ellipsis (`...`) indicating omitted content.\n- **pipeline**: Assigned to `train_pipeline`\n\nThis seems to be a configuration for a dataset used in a machine learning context.\nYou may refer to  source code  for details. \n9.2.3 Concatenate dataset \nThere are three ways to concatenate the dataset. \n1. If the datasets you want to concatenate are in the same type with different annotation files, you can concatenate the dataset configs like the following. \nThe image shows a snippet of Python code. It defines a dictionary called `dataset_A_train`. The dictionary contains three key-value pairs:\n\n1. `type`: The value is a string `'Dataset_A'`.\n2. `ann_file`: The value is a list containing two strings: `'anno_file_1'` and `'anno_file_2'`.\n3. `pipeline`: The value is a variable `train_pipeline`.\nIf the concatenated dataset is used for test or evaluation, this manner supports to evaluate each dataset separately. To test the concatenated datasets as a whole, you can set  separate e val  $\\b=$  False  as below. \nThe image you provided is not of a table in the traditional sense but rather a snippet of Python code. The code is defining a dictionary named `dataset_A_train` with the following key-value pairs:\n\n- `\"type\"`: This key has a value of `'Dataset_A'`. This typically represents the type or name of the dataset being described.\n- `\"ann_file\"`: This key has a value which is a list containing two strings: `'anno_file_1'` and `'anno_file_2'`. These strings likely represent names of annotation files related to the dataset. \n\nThis dictionary is commonly used in programming for setting configuration parameters related to datasets.\n(continues on next page) "}
{"page": 69, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_69.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nseparate_eval=False,\npipeline=train_pipeline\n\n2. Incase the dataset you want to concatenate is different, you can concatenate the dataset configs like the following.\n\ndataset_A_train = dictQ)\ndataset_B_train = dictQ\n\ndata = dict(\nimgs_per_gpu=2,\nworkers_per_gpu=2,\ntrain = [\ndataset_A_train,\ndataset_B_train\n1,\nval = dataset_A_val,\ntest = dataset_A_test\n)\n\nIf the concatenated dataset is used for test or evaluation, this manner also supports to evaluate each dataset\nseparately.\n\n3. We also support to define ConcatDataset explicitly as the following.\n\ndataset_A_val dictO\ndataset_B_val = dict(Q)\n\ndata = dict(\n\nimgs_per_gpu=2,\n\nworkers_per_gpu=2,\n\ntrain=dataset_A_train,\n\nval=dict(\ntype='ConcatDataset',\ndatasets=[dataset_A_val, dataset_B_val],\nseparate_eval=False))\n\nThis manner allows users to evaluate all the datasets as a single one by setting separate_eval=False.\nNote:\n\n1. The option separate_eval=False assumes the datasets use self.data_infos during evaluation. There-\nfore, COCO datasets do not support this behavior since COCO datasets do not fully rely on self. data_infos\nfor evaluation. Combining different types of datasets and evaluating them as a whole is not tested thus is not\nsuggested.\n\n2. Evaluating ClassBalancedDataset and RepeatDataset is not supported thus evaluating concatenated\ndatasets of these types is also not supported.\n\nA more complex example that repeats Dataset_A and Dataset_B by N and M times, respectively, and then concate-\nnates the repeated datasets is as the following.\n\ndataset_A_train = dict(\ntype='RepeatDataset',\ntimes=N,\n\n(continues on next page)\n\n62 Chapter 9. Tutorial 2: Customize Datasets\n\n", "vlm_text": "This image shows a piece of code, not a table. It indicates two parameters being set in a configuration:\n\n- `separate_eval=False`: This suggests that the evaluation is not separate.\n- `pipeline=train_pipeline`: This assigns a training pipeline to a parameter named `pipeline`.\n\nThe code appears to be part of a larger configuration or function call related to machine learning or data processing.\n2. In case the dataset you want to concatenate is different, you can concatenate the dataset configs like the following. \nThe image shows a Python code snippet that appears to be configuring or defining some data parameters for a machine learning or deep learning task. Here's a breakdown of the code:\n\n1. Two empty dictionaries are initialized:\n   - `dataset_A_train = dict()`\n   - `dataset_B_train = dict()`\n\n2. Another dictionary named `data` is defined with various parameters:\n   - `imgs_per_gpu=2`: This suggests that 2 images will be processed per GPU.\n   - `workers_per_gpu=2`: This indicates there will be 2 worker processes per GPU.\n   - `train = [...]`: This is a list containing `dataset_A_train` and `dataset_B_train`, suggesting that these datasets will be used for training.\n   - `val = dataset_A_val`: This indicates that `dataset_A_val` is used for validation purposes.\n   - `test = dataset_A_test`: This suggests that `dataset_A_test` is used for testing.\n\nThe code snippet seems to set up data configurations, possibly for use in a deep learning model training setup, with specific training, validation, and test datasets.\nIf the concatenated dataset is used for test or evaluation, this manner also supports to evaluate each dataset separately. \n3. We also support to define  Con cat Data set  explicitly as the following. \nThis image shows a snippet of Python code related to setting up datasets for training and validation in a machine learning context. Here's a breakdown of the code:\n\n1. `dataset_A_val` and `dataset_B_val` are initialized as empty dictionaries.\n2. `data` is a dictionary that includes:\n   - `imgs_per_gpu`: Set to 2, indicating the number of images processed per GPU.\n   - `workers_per_gpu`: Set to 2, indicating the number of workers per GPU.\n   - `train`: Assigned to `dataset_A_train`, likely referring to the training dataset.\n   - `val`: A dictionary for validation settings with:\n     - `type`: Set to `'ConcatDataset'`, suggesting that the validation datasets will be concatenated.\n     - `datasets`: A list containing `dataset_A_val` and `dataset_B_val`.\n     - `separate_eval`: Set to `False`, indicating whether evaluation of datasets should be separate.\n\nThis setup is typical in machine learning frameworks for managing data input pipelines.\nThis manner allows users to evaluate all the datasets as a single one by setting  separate e val  $\\cdot^{-}$  False . \nNote: \n1. The option  separate e val  $\\b=$  False  assumes the datasets use  self.data_infos  during evaluation. There- fore, COCO datasets do not support this behavior since COCO datasets do not fully rely on  self.data_infos for evaluation. Combining different types of datasets and evaluating them as a whole is not tested thus is not suggested. 2. Evaluating  Class Balanced Data set  and  Repeat Data set  is not supported thus evaluating concatenated \ndatasets of these types is also not supported. \nA more complex example that repeats  Dataset_A  and  Dataset_B  by N and M times, respectively, and then concate- nates the repeated datasets is as the following. \nThe image shows a snippet of code within a table or box, which defines a dictionary named `dataset_A_train`. The dictionary appears to be part of a configuration for a dataset in a machine learning or data processing task. Specifically, the dictionary includes:\n\n- A key `type` with the value `'RepeatDataset'`, which suggests that the dataset is intended to be repeated a certain number of times.\n- A key `times` with a value `N`, which likely specifies how many times the dataset should be repeated.\n\nThis kind of configuration is common in machine learning workflows where datasets need to be processed multiple times or augmented to increase the variety of training data."}
{"page": 70, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_70.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ndataset=dict(\ntype='Dataset_A',\n\npipeline=train_pipeline\n\n)\ndataset_A_val = dict(\n\npipeline=test_pipeline\n)\n\ndataset_A_test = dict(\n\npipeline=test_pipeline\n)\ndataset_B_train = dict(\ntype='RepeatDataset',\ntimes=M,\ndataset=dict(\ntype='Dataset_B',\n\npipeline=train_pipeline\n)\n)\ndata = dict(\nimgs_per_gpu=2,\nworkers_per_gpu=2,\ntrain = [\ndataset_A_train,\ndataset_B_train\n],\nval = dataset_A_val,\ntest = dataset_A_test\n\n9.3 Modify Dataset Classes\n\nWith existing dataset types, we can modify the class names of them to train subset of the annotations. For example, if\nyou want to train only three classes of the current dataset, you can modify the classes of dataset. The dataset will filter\nout the ground truth boxes of other classes automatically.\n\nclasses = ('person', 'bicycle', 'car')\n\ndata = dict(\ntrain=dict(classes=classes),\nval=dict(classes=classes) ,\ntest=dict(classes=classes))\n\nMMDetection V2.0 also supports to read the classes from a file, which is common in real applications. For example,\nassume the classes. txt contains the name of classes as the following.\n\n9.3. Modify Dataset Classes 63\n\n", "vlm_text": "The image shows a configuration script in Python that sets up data pipelines for different datasets. Here's a breakdown:\n\n- `dataset`: Configures `Dataset_A` with a training pipeline (`train_pipeline`).\n- `dataset_A_val`: Defines a validation dataset for `Dataset_A` with a test pipeline (`test_pipeline`).\n- `dataset_A_test`: Defines a test dataset for `Dataset_A` with a test pipeline.\n- `dataset_B_train`: Configures `Dataset_B` as a repeatable training dataset (`RepeatDataset`) repeated `M` times, also using `train_pipeline`.\n- `data`: Overall data configuration specifying:\n  - Images per GPU (`imgs_per_gpu=2`).\n  - Workers per GPU (`workers_per_gpu=2`).\n  - Training list includes `dataset_A_train` and `dataset_B_train`.\n  - Validation set as `dataset_A_val`.\n  - Test set as `dataset_A_test`. \n\nThis setup is commonly used in machine learning and data processing pipelines.\n9.3 Modify Dataset Classes \nWith existing dataset types, we can modify the class names of them to train subset of the annotations. For example, if you want to train only three classes of the current dataset, you can modify the classes of dataset. The dataset will filter out the ground truth boxes of other classes automatically. \nclasses  $=$   ( ' person ' ,  ' bicycle ' ,  ' car ' ) data  $=$   dict ( train  $\\overbar{\\ }$  dict (classes  $,=$  classes), val  $=$  dict (classes  $,=$  classes), test  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  dict (classes  $,=$  classes)) \nMM Detection V2.0 also supports to read the classes from a file, which is common in real applications. For example, assume the  classes.txt  contains the name of classes as the following. "}
{"page": 71, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_71.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nperson\nbicycle\ncar\n\nUsers can set the classes as a file path, the dataset will load it and convert it to a list automatically.\n\nclasses = 'path/to/classes.txt'\n\ndata = dict(\ntrain=dict(classes=classes),\nval=dict(classes=classes) ,\ntest=dict(classes=classes))\n\nNote:\n\n* Before MMDetection v2.5.0, the dataset will filter out the empty GT images automatically if the classes are set\nand there is no way to disable that through config. This is an undesirable behavior and introduces confusion be-\ncause if the classes are not set, the dataset only filter the empty GT images when filter_empty_gt=True and\ntest_mode=False. After MMDetection v2.5.0, we decouple the image filtering process and the classes modifi-\ncation, i.e., the dataset will only filter empty GT images when filter_empty_gt=True and test_mode=False,\nno matter whether the classes are set. Thus, setting the classes only influences the annotations of classes used\nfor training and users could decide whether to filter empty GT images by themselves.\n\n* Since the middle format only has box labels and does not contain the class names, when using CustomDataset,\nusers cannot filter out the empty GT images through configs but only do this offline.\n\n* Please remember to modify the num_classes in the head when specifying classes in dataset. We implemented\nNumClassCheckHook to check whether the numbers are consistent since v2.9.0(after PR#4508).\n\n* The features for setting dataset classes and dataset filtering will be refactored to be more user-friendly in the\nfuture (depends on the progress).\n\n9.4 COCO Panoptic Dataset\n\nNow we support COCO Panoptic Dataset, the format of panoptic annotations is different from COCO format. Both the\nforeground and the background will exist in the annotation file. The annotation json files in COCO Panoptic format\nhas the following necessary keys:\n\n‘images': [\n\n{\n'file_name': '000000001268.jpg',\n\"height': 427,\n‘width': 640,\n‘id': 1268\n3,\n\n]\n\n‘annotations': [\n{\n'filename': '000000001268.jpg',\n‘image_id': 1268,\n\"segments_info': [\n\n{\n\n(continues on next page)\n\n64 Chapter 9. Tutorial 2: Customize Datasets\n\n", "vlm_text": "The table contains a list with three items: \"person,\" \"bicycle,\" and \"car.\" There is no additional context or data provided in the table.\nUsers can set the classes as a file path, the dataset will load it and convert it to a list automatically. \nThe image depicts a code snippet rather than a traditional table with rows and columns. The code is written in Python and is used to create a dictionary. Here is an explanation of the code:\n\n1. `classes = 'path/to/classes.txt'`: This line assigns a file path (as a string) to the variable `classes`. The file path points to a text file presumably containing class labels.\n\n2. `data = dict(...)`: This line creates a dictionary called `data`.\n\n3. Within the `dict(...)` constructor:\n   - `train=dict(classes=classes)`: This creates a dictionary for the training dataset with a key `classes` and assigns it the value of the `classes` variable.\n   - `val=dict(classes=classes)`: This creates a dictionary for the validation dataset with a key `classes` and assigns it the value of the `classes` variable.\n   - `test=dict(classes=classes)`: This creates a dictionary for the test dataset with a key `classes` and assigns it the value of the `classes` variable.\n\nIn summary, this code snippet is defining a dictionary that contains sub-dictionaries for training, validation, and testing datasets, each linking to the specified classes within the `classes.txt` file.\n\nNote : \n• Before MM Detection v2.5.0, the dataset will filter out the empty GT images automatically if the classes are set and there is no way to disable that through config. This is an undesirable behavior and introduces confusion be- cause if the classes are not set, the dataset only filter the empty GT images when  filter empty gt  $=$  True  and test_mode  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  False . After MM Detection v2.5.0, we decouple the image filtering process and the classes modifi- cation, i.e., the dataset will only filter empty GT images when  filter empty gt  $\\cdot^{-}$  True  and  test_mode  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  False , no matter whether the classes are set. Thus, setting the classes only influences the annotations of classes used for training and users could decide whether to filter empty GT images by themselves. \n• Since the middle format only has box labels and does not contain the class names, when using  Custom Data set , users cannot filter out the empty GT images through configs but only do this offline. \n• Please remember to modify the  num classes  in the head when specifying  classes  in dataset. We implemented Num Class Check Hook  to check whether the numbers are consistent since v2.9.0(after PR#4508). \n• The features for setting dataset classes and dataset filtering will be refactored to be more user-friendly in the future (depends on the progress). \n9.4 COCO Panoptic Dataset \nNow we support COCO Panoptic Dataset, the format of panoptic annotations is different from COCO format. Both the foreground and the background will exist in the annotation file. The annotation json files in COCO Panoptic format has the following necessary keys: \nThe image contains a snippet of JSON data. It appears to represent metadata for an image. \n\nKey information includes:\n- The file name is `000000001268.jpg`.\n- The image dimensions are 640x427 pixels.\n- The image has an ID of 1268.\n- There is an `annotations` section that references the same image and includes a field `segments_info`, which might contain additional data (not fully visible here)."}
{"page": 72, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_72.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n\"id':8345037, # One-to-one correspondence with the id in the annotation,\n\nmap.\n\"category_id': 51,\n\"iscrowd': 0,\n\"pbox': (x1, yl, w, h), # The bbox of the background is the outer,\n\narectangle of its mask.\n‘area’: 24315\n\n3,\n3,\n]\n\n\"categories': [ # including both foreground categories and background categories\n{'id': 0, 'name': 'person'},\n\n]\n\nMoreover, the seg_prefix must be set to the path of the panoptic annotation images.\n\ndata = dict(\ntype='CocoPanopticDataset',\ntrain=dict(\n\nseg_prefix = 'path/to/your/train/panoptic/image_annotation_data'\n5\nval=dict(\n\nseg_prefix = 'path/to/your/train/panoptic/image_annotation_data'\n)\n\n9.4. COCO Panoptic Dataset 65\n", "vlm_text": "The image shows a code snippet that appears to be part of a data annotation or object detection process, likely in JSON format. It includes information about:\n\n- An \"id\" with the value `8345037`.\n- A \"category_id\" with the value `51`.\n- An \"iscrowd\" flag set to `0`.\n- A \"bbox\" (bounding box) defined by the coordinates `(x1, y1, w, h)`.\n- An \"area\" with the value `24315`.\n- A \"categories\" list containing an object with `id: 0` and `name: 'person'`.\n\nThe snippet also contains comments explaining the structure.\nMoreover, the  seg_prefix  must be set to the path of the panoptic annotation images. \ndata  =  dict ( type = ' Coco Pan optic Data set ' , train  $\\overbar{\\ }$  dict ( seg_prefix  $=$   ' path/to/your/train/panoptic/image annotation data ' ), val  $\\mathbf{\\beta}\\!=$  dict ( seg_prefix  $=$   ' path/to/your/train/panoptic/image annotation data ' ) ) "}
{"page": 73, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_73.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n66 Chapter 9. Tutorial 2: Customize Datasets\n", "vlm_text": "MMDetection, Release 2.18.0\n\n66 Chapter 9. Tutorial 2: Customize Datasets\n"}
{"page": 74, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_74.jpg", "ocr_text": "CHAPTER\nTEN\n\nTUTORIAL 3: CUSTOMIZE DATA PIPELINES\n\n10.1 Design of Data pipelines\n\nFollowing typical conventions, we use Dataset and DataLoader for data loading with multiple workers. Dataset\nreturns a dict of data items corresponding the arguments of models’ forward method. Since the data in object detection\n\nmay not be the same size (image size, gt bbox size, etc.), we introduce a new DataContainer type in MMCV to help\ncollect and distribute data of different size. See here for more details.\n\nThe data preparation pipeline and the dataset is decomposed. Usually a dataset defines how to process the annotations\nand a data pipeline defines all the steps to prepare a data dict. A pipeline consists of a sequence of operations. Each\noperation takes a dict as input and also output a dict for the next transform.\n\nWe present a classical pipeline in the following figure. The blue blocks are pipeline operations. With the pipeline going\non, each operator can add new keys (marked as green) to the result dict or update the existing keys (marked as orange).\n\nLoadimageFromFile LoadAnnotations Resize RandomFlip Normalize Pad Defoultformat Collect\n{ { { Co { {\n“img’: “img: “img’: img img “img’\n“img_shape’’ *img_shape”: “img_shape’’: “img_shape*: “img_shape”: “img_meta”: {\n“ori_shape\": “ori_shape\": “ori_shape’: “ori_shape\": “ori_shape’:\n“gt_bboxes”: shape’ “pad_shape”: “pad_shape”: “img.\nlabels\": t_ boxes” “gt_bboxes”: “gt_bboxes”:\n“bbox_fields” “gt labels” “gt labels” “gt labels”\n} “bbox_fields” “bbox_fields”: “bbox_fields”:\n“scale” “scale” “scale”\n“scale_idx’: “*scale_idx’ “scale_idx’ “scale_idx\": “scale_idx’:\n“scale_factor’ “scale_factor’: “scale_factor’: “scale_factor\" “scale_factor” “gt bboxes”\n“keep ratio\": “keep_ratio” “keep ratio” “keep_ratio\": “keep_ratio\": “gt labels”:\n} “flip” “flip” “flip” ¥ }\n} *img_norm_ofg\": |} “img_norm_cfg’ 19_norm_cf’\n} “pad_fixed_size”’ ‘pad_fixed_size”:\n“pad_size_divisor’: “pad_size_divisor”:\n) ,\n\npipeline\nfigure\n\nThe operations are categorized into data loading, pre-processing, formatting and test-time augmentation.\n\nHere is a pipeline example for Faster R-CNN.\n\nimg_norm_cfg = dict(\n\nmean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n\ndict (type='LoadImageFromFile'),\n\ndict(type='LoadAnnotations', with_bbox=True) ,\n\ndict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n\ndict(type='RandomFlip', flip_ratio=0.5),\n\ndict(type='Normalize', img_norm_cfg),\n\ndict(type='Pad', size_divisor=32),\n\ndict (type='DefaultFormatBundle'),\n\n(continues on next page)\n\n67\n", "vlm_text": "TUTORIAL 3: CUSTOMIZE DATA PIPELINES \n10.1 Design of Data pipelines \nFollowing typical conventions, we use  Dataset  and  DataLoader  for data loading with multiple workers.  Dataset returns a dict of data items corresponding the arguments of models’ forward method. Since the data in object detection may not be the same size (image size, gt bbox size, etc.), we introduce a new  Data Container  type in MMCV to help collect and distribute data of different size. See  here  for more details. \nThe data preparation pipeline and the dataset is decomposed. Usually a dataset defines how to process the annotations and a data pipeline defines all the steps to prepare a data dict. A pipeline consists of a sequence of operations. Each operation takes a dict as input and also output a dict for the next transform. \nWe present a classical pipeline in the following figure. The blue blocks are pipeline operations. With the pipeline going on, each operator can add new keys (marked as green) to the result dict or update the existing keys (marked as orange). \nThe image is a flowchart depicting a data processing pipeline for image preprocessing tasks typically performed in computer vision, specifically in the field of object detection. The pipeline includes the following steps:\n\n1. **LoadImageFromFile**: Load the image and capture its shape and original shape.\n2. **LoadAnnotations**: Add annotations such as ground truth bounding boxes and labels.\n3. **Resize**: Adjust the image size, updating attributes like padding shape, scale, index, factor, and ratio.\n4. **RandomFlip**: Optionally flip the image, recording the flip status.\n5. **Normalize**: Apply normalization to the image using a specific config.\n6. **Pad**: Add padding if necessary, updating padding attributes.\n7. **DefaultFormatBundle**: Prepare data with a fixed size and divisor for consistent formatting.\n8. **Collect**: Gather all processed image data and metadata for further use.\n\nEach step adds or modifies data attributes, preparing the image for model input.\nfigure The operations are categorized into data loading, pre-processing, formatting and test-time augmentation. \nHere is a pipeline example for Faster R-CNN. \nThe image appears to show a configuration snippet for a deep learning framework, likely related to the preprocessing pipeline for training images for a computer vision model. Here's a breakdown of what each part of the table does:\n\n1. **img_norm_cfg**: This is a dictionary containing normalization parameters.\n   - **mean**: The mean values for each channel (R, G, B) used to normalize the images.\n   - **std**: The standard deviation values for each channel (R, G, B) used for normalization.\n   - **to_rgb**: A Boolean set to `True`, indicating whether to convert images to RGB format.\n\n2. **train_pipeline**: This is a list that outlines the sequence of operations applied to training images.\n   - **LoadImageFromFile**: Loads an image from a file.\n   - **LoadAnnotations**: Loads the annotations, with bounding box information included (indicated by `with_bbox=True`).\n   - **Resize**: Resizes the image to the specified scale (1333, 800) while maintaining the aspect ratio (`keep_ratio=True`).\n   - **RandomFlip**: May randomly flip the image horizontally with a probability defined by `flip_ratio=0.5`.\n   - **Normalize**: Applies normalization to the image using the settings from `img_norm_cfg`.\n   - **Pad**: Pads the image to make its dimensions a multiple of the specified `size_divisor`, which is 32 in this case.\n   - **DefaultFormatBundle**: Transforms and packages the data into a default format that the model can process.\n\nThis pipeline is typically used to ensure that images are preprocessed consistently in a manner that helps improve model training stability and accuracy."}
{"page": 75, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_75.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ndict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\ndict (type='LoadImageFromFile'),\ndict\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntrans forms=[\ndict(type='Resize', keep_ratio=True) ,\ndict(type='RandomFlip'),\ndict(type='Normalize', **img_norm_cfg) ,\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n)\n\n]\n\nFor each operation, we list the related dict fields that are added/updated/removed.\n\n10.1.1 Data loading\n\nLoadImageFromFile\n\n* add: img, img_shape, ori_shape\nLoadAnnotations\n\n* add: gt_bboxes, gt_bboxes_ignore, gt_labels, gt_masks, gt_semantic_seg, bbox_fields, mask_fields\nLoadProposals\n\n* add: proposals\n\n10.1.2 Pre-processing\n\nResize\n* add: scale, scale_idx, pad_shape, scale_factor, keep_ratio\n* update: img, img_shape, *bbox_fields, *mask_fields, *seg_fields\nRandomFlip\n* add: flip\n* update: img, *bbox_fields, *mask_fields, *seg_fields\nPad\n\n* add: pad_fixed_size, pad_size_divisor\n* update: img, pad_shape, *mask_fields, *seg_fields\nRandomCrop\n* update: img, pad_shape, gt_bboxes, gt_labels, gt_masks, *bbox_fields\n\nNormalize\n\n68 Chapter 10. Tutorial 3: Customize Data Pipelines\n", "vlm_text": "The image contains a snippet of Python code that appears to configure a data processing pipeline, presumably for a machine learning application, possibly one involving computer vision. Here's a breakdown of what the code is doing:\n\n1. **LoadImageFromFile**: This seems to load images from a file.\n\n2. **MultiScaleFlipAug**: This might be a transformation to handle multi-scale or flip augmentation:\n   - `img_scale=(1333, 800)`: This likely sets the image scale to a specific size.\n   - `flip=False`: This indicates that the image should not be flipped.\n\n3. **Transforms**: A series of transformations to be applied to the images:\n   - `Resize`: Resizes the image, maintaining the aspect ratio (`keep_ratio=True`).\n   - `RandomFlip`: Randomly flips the image.\n   - `Normalize`: Normalizes the image data using some configuration (`**img_norm_cfg`).\n   - `Pad`: Pads the image to match the size divisor of 32.\n   - `ImageToTensor`: Converts the image to a tensor, which is a common format for input data in machine learning.\n   - `Collect`: Collects the `img` key.\n\nOverall, this code likely defines a series of pre-processing steps for image data before feeding it into a neural network for tasks like image recognition or object detection. This type of setup is common in deep learning frameworks like PyTorch or TensorFlow when working with custom datasets.\nFor each operation, we list the related dict fields that are added/updated/removed. \n10.1.1 Data loading \nLoad Image From File \n\nLoad Annotations • add: gt_bboxes, gt b boxes ignore, gt_labels, gt_masks, gt semantic seg, b box fields, mask fields Load Proposals • add: proposals \n10.1.2 Pre-processing \nResize \n• add: scale, scale_idx, pad_shape, scale factor, keep_ratio • update: img, img_shape, \\*b box fields, \\*mask fields, \\*seg_fields \nRandomFlip \n• add: flip • update: img, \\*b box fields, \\*mask fields, \\*seg_fields \nPad \n• add: pad fixed size, pad size divisor • update: img, pad_shape, \\*mask fields, \\*seg_fields \nRandomCrop \n• update: img, pad_shape, gt_bboxes, gt_labels, gt_masks, \\*b box fields \nNormalize "}
{"page": 76, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_76.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* add: img_norm_cfg\n\n* update: img\nSegRescale\n\n* update: gt_semantic_seg\nPhotoMetricDistortion\n\n* update: img\nExpand\n\n* update: img, gt_bboxes\nMinIoURandomCrop\n\n* update: img, gt_bboxes, gt_labels\nCorrupt\n\n* update: img\n\n10.1.3 Formatting\n\nToTensor\n* update: specified by keys.\nImageToTensor\n* update: specified by keys.\nTranspose\n* update: specified by keys.\n\nToDataContainer\n\n* update: specified by fields.\n\nDefaultFormatBundle\n\n* update: img, proposals, gt_bboxes, gt_bboxes_ignore, gt_labels, gt_masks, gt_semantic_seg\nCollect\n\n* add: img_meta (the keys of img_meta is specified by meta_keys)\n\n* remove: all other keys except for those specified by keys\n\n10.1.4 Test time augmentation\n\nMultiScaleFlipAug\n\n10.1. Design of Data pipelines 69\n", "vlm_text": "• add: img norm cf g • update: img SegRescale • update: gt semantic seg PhotoMetric Distortion • update: img Expand • update: img, gt_bboxes MinI oU Random Crop • update: img, gt_bboxes, gt_labels Corrupt• update: img \n10.1.3 Formatting \nToTensor \n• update: specified by  keys . \n• update: specified by  fields . \nDefault Format Bundle \n• update: img, proposals, gt_bboxes, gt b boxes ignore, gt_labels, gt_masks, gt semantic seg \n• add: img_meta (the keys of img_meta is specified by  meta_keys ) • remove: all other keys except for those specified by  keys \n10.1.4 Test time augmentation \nMulti Scale Flip Aug "}
{"page": 77, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_77.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n10.2 Extend and use custom pipelines\n\n1. Write a new pipeline in a file, e.g., in my_pipeline.py. It takes a dict as input and returns a dict.\n\nimport random\nfrom mmdet.datasets import PIPELINES\n\n@PIPELINES .register_module()\nclass MyTransform:\n\"\"\"Add your transform\n\nArgs:\np (float): Probability of shifts. Default 0.5.\n\nwon\n\ndef __init__(self, p=0.5):\nself.p =p\n\ndef __call__(self, results):\nif random.random() > self.p:\nresults['dummy'] = True\nreturn results\n\n2. Import and use the pipeline in your config file. Make sure the import is relative to where your train script is\nlocated.\n\ncustom_imports = dict(imports=['path.to.my_pipeline'], allow_failed_imports=False)\n\nimg_norm_cfg = dict(\nmean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\ndict (type='LoadImageFromFile'),\ndict(type='LoadAnnotations', with_bbox=True) ,\ndict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\ndict(type='RandomFlip', flip_ratio=0.5),\ndict(type='Normalize', **img_norm_cfg),\ndict(type='Pad', size_divisor=32),\ndict(type='MyTransform', p=0.2),\ndict (type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n\n3. Visualize the output of your augmentation pipeline\n\nTo visualize the output of your agmentation pipeline, tools/misc/browse_dataset .py can help the user to\nbrowse a detection dataset (both images and bounding box annotations) visually, or save the image to a designated\ndirectory. More detials can refer to useful_tools\n\n70 Chapter 10. Tutorial 3: Customize Data Pipelines\n", "vlm_text": "10.2 Extend and use custom pipelines \n1. Write a new pipeline in a file, e.g., in  my pipeline.py . It takes a dict as input and returns a dict.\n\n \nThe image shows a Python code snippet. It defines a custom transform class for a machine learning pipeline using a decorator from `mmdet.datasets`. Here’s a breakdown:\n\n- Imports:\n  - `random`\n  - `Pipelines` from `mmdet.datasets`\n\n- `@PIPELINES.register_module()` is used as a decorator to register the class as a module.\n\n- Class: `MyTransform`\n  - It has a docstring for documentation.\n  - **Args:**\n    - `p (float)`: Probability of shifts, with a default value of 0.5.\n\n- Method: `__init__`\n  - Initializes with a parameter `p`, defaulting to 0.5.\n\n- Method: `__call__`\n  - Takes `results` as a parameter.\n  - Uses `random.random()` to decide if a shift should occur based on the probability `p`.\n  - If the condition is met, it adds `results['dummy'] = True`.\n  - Returns `results`.\n2. Import and use the pipeline in your config file. Make sure the import is relative to where your train script is located. \ncustom imports  $=$   dict (imports  $=$  [ ' path.to.my pipeline ' ], allow failed imports  $\\risingdotseq$  False ) img norm cf g  $=$   dict ( mean  $\\risingdotseq$  [ 123.675 ,  116.28 ,  103.53 ], std  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  [ 58.395 ,  57.12 ,  57.375 ], to_rgb  $\\leftrightharpoons$  True ) train pipeline    $=$   [ dict ( type  $=\"$  Load Image From File ' ), dict ( type  $\\risingdotseq$  Load Annotations ' , with_bbox  $\\risingdotseq$  True ), dict ( type  $=\"$  Resize ' , img_scale  $=$  ( 1333 ,  800 ), keep_ratio  $\\risingdotseq$  True ), dict ( type  $\\risingdotseq$  RandomFlip ' , flip_ratio  $\\scriptstyle=\\0$  .5 ), dict ( type  $\\equiv^{1}$  Normalize ' ,  \\*\\* img norm cf g), dict ( type  $\\scriptstyle{\\varepsilon}$  ' Pad ' , size divisor  $=\\!32$  ), dict ( type  $=\"$  My Transform ' ,   $\\scriptstyle{\\mathfrak{p}}=0\\,.\\,2)$  , dict ( type  $\\circeq$  Default Format Bundle ' ), dict ( type  $\\risingdotseq$  Collect ' , keys  $,=$  [ ' img ' ,  ' gt_bboxes ' ,  ' gt_labels ' ]), ]\n\n \n3. Visualize the output of your augmentation pipeline \nTo visualize the output of your ag ment ation pipeline,  tools/misc/browse data set.py  can help the user to browse a detection dataset (both images and bounding box annotations) visually, or save the image to a designated directory. More detials can refer to  useful tools "}
{"page": 78, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_78.jpg", "ocr_text": "CHAPTER\nELEVEN\n\nTUTORIAL 4: CUSTOMIZE MODELS\n\nWe basically categorize model components into 5 types.\n* backbone: usually an FCN network to extract feature maps, e.g., ResNet, MobileNet.\n* neck: the component between backbones and heads, e.g., FPN, PAFPN.\n* head: the component for specific tasks, e.g., bbox prediction and mask prediction.\n* roi extractor: the part for extracting Rol features from feature maps, e.g., Rol Align.\n\n* loss: the component in head for calculating losses, e.g., FocalLoss, L1Loss, and GHMLoss.\n\n11.1 Develop new components\n\n11.1.1 Add a new backbone\n\nHere we show how to develop new components with an example of MobileNet.\n\n1. Define a new backbone (e.g. MobileNet)\n\nCreate a new file mmdet /models/backbones/mobilenet.py.\n\nimport torch.nn as nn\n\nfrom ..builder import BACKBONES\n@BACKBONES . register_module()\nclass MobileNet(nn.Module):\n\ndef __init__(self, argl, arg2):\npass\n\ndef forward(self, x): # should return a tuple\npass\n\n71\n\n", "vlm_text": "TUTORIAL 4: CUSTOMIZE MODELS \nWe basically categorize model components into 5 types. \n• backbone: usually an FCN network to extract feature maps, e.g., ResNet, MobileNet. • neck: the component between backbones and heads, e.g., FPN, PAFPN. • head: the component for specific tasks, e.g., bbox prediction and mask prediction. • roi extractor: the part for extracting RoI features from feature maps, e.g., RoI Align. • loss: the component in head for calculating losses, e.g., FocalLoss, L1Loss, and GHMLoss. \n11.1 Develop new components \n11.1.1 Add a new backbone \nHere we show how to develop new components with an example of MobileNet. \n1. Define a new backbone (e.g. MobileNet) \nCreate a new file  mmdet/models/backbones/mobilenet.py . import  torch.nn  as  nn from  ..builder  import  BACKBONES @BACKBONES.register module()class  MobileNet (nn . Module): def  __init__ ( self , arg1, arg2): pass def  forward ( self , x): # should return a tuple pass "}
{"page": 79, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_79.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n2. Import the module\n\nYou can either add the following line to mmdet/models/backbones/__init__.py\n\nfrom .mobilenet import MobileNet\n\nor alternatively add\n\ncustom_imports = dict(\nimports=['mmdet.models.backbones.mobilenet'],\nallow_failed_imports=False)\n\nto the config file to avoid modifying the original code.\n\n3. Use the backbone in your config file\n\nmodel = dict(\n\nbackbone=dict (\ntype='MobileNet',\nargl=xxx,\narg2=xxx),\n\n11.1.2 Add new necks\n1. Define a neck (e.g. PAFPN)\n\nCreate a new file mmdet /models/necks/pafpn. py.\n\nfrom ..builder import NECKS\n\n@NECKS .register_module()\nclass PAFPN(nn.Module):\n\ndef __init__(self,\nin_channels,\nout_channels,\nnum_outs,\nstart_level=0,\nend_level=-1,\nadd_extra_convs=False) :\n\npass\n\ndef forward(self, inputs):\n# implementation is ignored\npass\n\n72 Chapter 11. Tutorial 4: Customize Models\n", "vlm_text": "2. Import the module \nYou can either add the following line to  mmdet/models/backbones/__init__.py \nThe image depicts a code snippet, not a traditional table. It consists of code for importing the `MobileNet` module and an alternative method to handle custom imports using the `mmdet` library. Here's a breakdown of the code:\n\n1. The first line imports the `MobileNet` module from a local module named `mobilenet`.\n   ```python\n   from .mobilenet import MobileNet\n   ```\n\n2. The code provides an alternative method to import by using a dictionary `custom_imports` which specifies:\n   - `imports`: This is a list containing the import path `'mmdet.models.backbones.mobilenet'`.\n   - `allow_failed_imports`: This is a boolean set to `False`.\n   ```python\n   custom_imports = dict(\n       imports=['mmdet.models.backbones.mobilenet'],\n       allow_failed_imports=False)\n   ```\n\nOverall, the code relates to importing a `MobileNet` model, either directly or by specifying a custom import path using the `mmdet` library.\nto the config file to avoid modifying the original code.\n\n \n3. Use the backbone in your config file\n\n \nThe image shows a snippet of a configuration in Python dictionary format, typically used for setting up machine learning models or similar tasks. This particular configuration appears to be defining a model with a backbone, using 'MobileNet' as the type. The specific arguments (`arg1` and `arg2`) are placeholders with values set to `xxx`. The ellipses (`...`) suggest that there are additional configurations or parameters in the actual code that are not shown in this snippet. This is not a table in a traditional sense with rows and columns of data; instead, it is a code snippet within a bordered box that may be presented alongside a table or in a document containing tables.\n11.1.2 Add new necks \n1. Define a neck (e.g. PAFPN) \nCreate a new file  mmdet/models/necks/pafpn.py . \nThe image shows a snippet of Python code, not a table. Here's a brief explanation of what's in the code:\n\n- It imports `NECKS` from a module called `..builder`.\n- A decorator `@NECKS.register_module()` is used on the `PAFPN` class, indicating it is part of the `NECKS` module.\n- `PAFPN` is a class that inherits from `nn.Module`.\n- The `__init__` method initializes the class with parameters: `in_channels`, `out_channels`, `num_outs`, `start_level`, `end_level`, and `add_extra_convs`, with default values for the last three.\n- The `forward` method is defined but not implemented (indicated by \"pass\" and the comment \"# implementation is ignored\")."}
{"page": 80, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_80.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n2. Import the module\n\nYou can either add the following line to mmdet/models/necks/__init__.py,\n\nfrom .pafpn import PAFPN\n\nor alternatively add\n\ncustom_imports = dict(\nimports=['mmdet.models.necks.pafpn.py'],\nallow_failed_imports=False)\n\nto the config file and avoid modifying the original code.\n\n3. Modify the config file\n\nneck=dict(\ntype='PAFPN',\nin_channels=[256, 512, 1024, 2048],\nout_channels=256,\nnum_outs=5)\n\n11.1.3 Add new heads\n\nHere we show how to develop a new head with the example of Double Head R-CNN as the following.\n\nFirst, add a new bbox head in mmdet /models/roi_heads/bbox_heads/double_bbox_head.py. Double Head R-\nCNN implements a new bbox head for object detection. To implement a bbox head, basically we need to implement\nthree functions of the new module as the following.\n\nfrom mmdet.models.builder import HEADS\nfrom .bbox_head import BBoxHead\n\nGHEADS . register_module()\nclass DoubleConvFCBBoxHead (BBoxHead) :\nr\"\"\"Bbox head used in Double-Head R-CNN\n\n/-> cls\n/-> shared convs ->\n\\-> reg\nroi features\n/-> cls\n\\-> shared fc ->\n\\-> reg\n\n\"\"\"  # noqa: W605\n\ndef __init__(self,\nnum_convs=0,\nnum_fcs=0,\nconv_out_channels=1024,\nfc_out_channels=1024,\nconv_cfg=None,\n\n(continues on next page)\n\n11.1. Develop new components 73\n\n", "vlm_text": "2. Import the module \nYou can either add the following line to  mmdet/models/necks/__init__.py , \nThe image shows a code snippet, not a traditional table with rows and columns. The code is related to Python imports, specifically for the `PAFPN` module. The snippet offers two methods for importing:\n\n1. Direct Import:\n   ```python\n   from .pafpn import PAFPN\n   ```\n   This line of code imports the `PAFPN` class or function from a module named `pafpn` located in the current package.\n\n2. Alternative Import via Custom Imports:\n   ```python\n   custom_imports = dict(\n       imports=['mmdet.models.necks.pafpn.py'],\n       allow_failed_imports=False\n   )\n   ```\n   This section defines a dictionary `custom_imports` that specifies a list for `imports` with one entry: `'mmdet.models.necks.pafpn.py'`. It also includes a setting, `allow_failed_imports`, which is set to `False`, meaning that failed imports are not allowed and would raise an error. The dictionary seems to configure a custom import mechanism, presumably in a larger context where such mechanisms are used.\nto the config file and avoid modifying the original code. \n3. Modify the config file \nThis is not a table, but rather a code snippet indicating the configuration of a neural network component (likely a feature pyramid network). The code is a dictionary defining parameters:\n\n- `type`: Set to `'PAFPN'`, which suggests it might be a specific type of Feature Pyramid Network.\n- `in_channels`: A list `[256, 512, 1024, 2048]` indicating input channels for each level of the pyramid.\n- `out_channels`: Set to `256`, specifying the number of output channels.\n- `num_outs`: Set to `5`, indicating the number of output feature maps.\n11.1.3 Add new heads \nHere we show how to develop a new head with the example of  Double Head R-CNN  as the following. \nFirst, add a new bbox head in  mmdet/models/roi_heads/bbox_heads/double b box head.py . Double Head R- CNN implements a new bbox head for object detection. To implement a bbox head, basically we need to implement three functions of the new module as the following. \nThe image you provided is not a table; it's a screenshot of a Python code snippet from a deep learning library, likely related to object detection, specifically a bounding box (bbox) head used in a model such as Double-Head R-CNN.\n\nHere is an explanation of the code:\n\n1. **Imports**:\n   - `from mmdet.models.builder import HEADS`: Importing the `HEADS` module from `mmdet.models.builder`.\n   - `from .bbox_head import BBoxHead`: Importing the `BBoxHead` class from a local module `bbox_head`.\n\n2. **Class Definition**:\n   - `@HEADS.register_module()`: A decorator indicating that the `DoubleConvFCBBoxHead` class should be registered within the `HEADS` module registry, a design pattern often used in machine learning libraries to keep track of various components.\n   - `class DoubleConvFCBBoxHead(BBoxHead)`: A new class `DoubleConvFCBBoxHead` which inherits from `BBoxHead`.\n\n3. **Docstring**:\n   - A raw string (indicated by `r\"\"\"...\"\"\"`) is used as a class docstring to describe the purpose and function of the class. It provides a high-level description that this class implements a bbox head used in a Double-Head R-CNN with schematic comments about its architecture.\n\n4. **Constructor (`__init__` method)**:\n   - The `__init__` method initializes the class with given parameters:\n     - `num_convs=0`: The number of convolutional layers.\n     - `num_fcs=0`: The number of fully connected layers.\n     - `conv_out_channels=1024`: The number of output channels for each convolutional layer.\n     - `fc_out_channels=1024`: The number of output channels for each fully connected layer.\n     - `conv_cfg=None`: Configuration for the convolutional layers, if any.\n\nThe code primarily sets up a structure for a bounding box head, which is typically part of an object detection model, allowing for classification (`cls`) and regression (`reg`) of regions of interest (roi features)."}
{"page": 81, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_81.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nnorm_cfg=dict(type='BN'),\n\n*“*kwargs):\nkwargs.setdefault('with_avg_pool', True)\nsuper (DoubleConvFCBBoxHead, self).__init__(**kwargs)\n\ndef forward(self, x_cls, x_reg):\n\nSecond, implement a new Rol Head if it is necessary. We plan to inherit the new DoubleHeadRoIHead from\nStandardRoIHead. We can find that a StandardRoIHead already implements the following functions.\n\nimport torch\n\nfrom mmdet.core import bbox2result, bbox2roi, build_assigner, build_sampler\nfrom ..builder import HEADS, build_head, build_roi_extractor\n\nfrom .base_roi_head import BaseRoIHead\n\nfrom .test_mixins import BBoxTestMixin, MaskTestMixin\n\nGHEADS . register_module()\nclass StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\n\"\"\"Simplest base roi head including one bbox head and one mask head.\n\ndef init_assigner_sampler(self):\ndef init_bbox_head(self, bbox_roi_extractor, bbox_head):\n\ndef init_mask_head(self, mask_roi_extractor, mask_head):\n\ndef forward_dummy(self, x, proposals):\n\ndef forward_train(self,\nx,\nimg_metas,\nproposal_list,\ngt_bboxes,\ngt_labels,\ngt_bboxes_ignore=None,\ngt_masks=None) :\n\ndef _bbox_forward(self, x, rois):\n\ndef _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\nimg_metas):\n\ndef _mask_forward_train(self, x, sampling_results, bbox_feats, gt_masks,\nimg_metas):\n\ndef _mask_forward(self, x, rois=None, pos_inds=None, bbox_feats=None) :\n\n(continues on next page)\n\n74 Chapter 11. Tutorial 4: Customize Models\n\n", "vlm_text": "Second, implement a new RoI Head if it is necessary. We plan to inherit the new  Double Head RoI Head  from \nStandard RoI Head  Standard RoI Head import  torch from  mmdet.core  import  b box 2 result, bbox2roi, build as signer, build sampler from  ..builder  import  HEADS, build_head, build roi extractor from  .base roi head  import  Base RoI Head from  .test mix in s  import  B Box Test Mix in, Mask Test Mix in @HEADS . register module() class  Standard RoI Head (Base RoI Head, B Box Test Mix in, Mask Test Mix in): \"\"\"Simplest base roi head including one bbox head and one mask head. \"\"\" def  in it as signer sampler ( self ): def  in it b box head ( self , b box roi extractor, bbox_head): def  in it mask head ( self , mask roi extractor, mask_head): def  forward dummy ( self , x, proposals): def  forward train ( self , x, img_metas, proposal list, gt_bboxes, gt_labels, gt b boxes ignore  $\\risingdotseq$  None , gt_masks  $=$  None ): def  b box forward ( self , x, rois): def  b box forward train ( self , x, sampling results, gt_bboxes, gt_labels, img_metas): def  mask forward train ( self , x, sampling results, bbox_feats, gt_masks, img_metas): def  mask forward ( self , x, rois  $\\risingdotseq$  None , pos_inds  $\\risingdotseq$  None , bbox_feats = None ): \n(continues on next page) "}
{"page": 82, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_82.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\ndef simple_test(self,\nx,\nproposal_list,\nimg_metas,\nproposals=None,\nrescale=False):\n\"\"\"Test without augmentation.\n\nwon\n\nDouble Head’s modification is mainly in the bbox_forward logic, and it inherits other logics from the\nStandardRolHead. In the mmdet/models/roi_heads/double_roi_head.py, we implement the new Rol Head\nas the following:\n\nfrom ..builder import HEADS\nfrom .standard_roi_head import StandardRoIHead\n\nGHEADS . register_module()\nclass DoubleHeadRoIHead(StandardRolHead) :\n\"\"\"RoI head for Double Head RCNN\n\nhttps://arxiv.org/abs/1904.06493\n\nwon\n\ndef __init__(self, reg_roi_scale_factor, **kwargs):\nsuper (DoubleHeadRoIHead, self).__init__(**kwargs)\nself.reg_roi_scale_factor = reg_roi_scale_factor\n\ndef _bbox_forward(self, x, rois):\nbbox_cls_feats = self.bbox_roi_extractor(\nx[:self.bbox_roi_extractor.num_inputs], rois)\nbbox_reg_feats = self.bbox_roi_extractor(\nx[:self.bbox_roi_extractor.num_inputs],\nrois,\nroi_scale_factor=self.reg_roi_scale_factor)\nif self.with_shared_head:\nbbox_cls_feats = self.shared_head(bbox_cls_feats)\nbbox_reg_feats = self.shared_head(bbox_reg_feats)\ncls_score, bbox_pred = self.bbox_head(bbox_cls_feats, bbox_reg_feats)\n\nbbox_results = dict(\ncls_score=cls_score,\nbbox_pred=bbox_pred,\nbbox_feats=bbox_cls_feats)\nreturn bbox_results\n\nLast, the users need to add the module in mmdet/models/bbox_heads/__init__.py and mmdet/models/\nroi_heads/__init__.py thus the corresponding registry could find and load them.\n\nAlternatively, the users can add\n\n11.1. Develop new components 75\n\n", "vlm_text": "This image shows a snippet of Python code, defining a function called `simple_test`. The function takes several parameters: `self`, `x`, `proposal_list`, `img_metas`, `proposals` (with a default value of `None`), and `rescale` (with a default value of `False`). There's also a docstring that states: \"Test without augmentation.\"\nDouble Head’s modification is mainly in the b box forward logic, and it inherits other logics from the Standard RoI Head . In the  mmdet/models/roi_heads/double roi head.py , we implement the new RoI Head as the following: \nThe image shows a code snippet in Python for a class called `DoubleHeadRoIHead`, which inherits from `StandardRoIHead`. This class is part of an object detection framework and is registered as a module using `@HEADS.register_module`. The code appears to be designed for use with a Double Head RCNN, a specific type of Region-based Convolutional Neural Network (RCNN).\n\nHere's a summary of the key parts:\n\n- **Imports**: It imports `HEADS` from `.builder` and `StandardRoIHead` from `.standard_roi_head`.\n- **Class Definition**: The class `DoubleHeadRoIHead` extends `StandardRoIHead`.\n- **Initialization**: The `__init__` method initializes the class with `reg_roi_scale_factor` and any additional keyword arguments.\n- **Method `bbox_forward`**: This method takes `self`, `x`, and `rois` as arguments and performs operations using the `bbox_roi_extractor` and `shared_head`. It returns a dictionary with classification scores, bounding box predictions, and extracted features.\n\nThere's also a comment linking to a relevant paper on arXiv.\nAlternatively, the users can add "}
{"page": 83, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_83.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ncustom_imports=dict(\nimports=['mmdet.models.roi_heads.double_roi_head', 'mmdet.models.bbox_heads.double_\n«bbox_head'])\n\nto the config file and achieve the same goal.\n\nThe config file of Double Head R-CNN is as the following\n\n_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py\nmodel = dict(\nroi_head=dict(\ntype='DoubleHeadRolIHead' ,\nreg_roi_scale_factor=1.3,\nbbox_head=dict (\n_delete_=True,\ntype='DoubleConvFCBBoxHead' ,\nnum_convs=4,\nnum_fcs=2,\nin_channels=256,\nconv_out_channels=1024,\nfc_out_channels=1024,\nroi_feat_size=7,\nnum_classes=80,\nbbox_coder=dict (\ntype='DeltaXYWHBBoxCoder' ,\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.1, 0.1, 0.2, 0.2]),\nreg_class_agnostic=False,\nloss_cls=dict(\ntype='CrossEntropyLoss', use_sigmoid=False, loss_weight=2.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0, loss_weight=2.0))))\n\nSince MMDetection 2.0, the config system supports to inherit configs such that the users can focus on the modification.\nThe Double Head R-CNN mainly uses a new DoubleHeadRolHead and a new Doubl eConvFCBBoxHead, the arguments\nare set according to the __init__ function of each module.\n\n11.1.4 Add new loss\n\nAssume you want to add a new loss as MyLoss, for bounding box regression. To add a new loss function, the users\nneed implement it in mmdet/models/losses/my_loss.py. The decorator weighted_loss enable the loss to be\nweighted for each element.\n\nimport torch\nimport torch.nn as nn\n\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n@weighted_loss\n\ndef my_loss(pred, target):\nassert pred.size() == target.size() and target.numel() > 0\nloss = torch.abs(pred - target)\nreturn loss\n\n(continues on next page)\n\n76 Chapter 11. Tutorial 4: Customize Models\n\n", "vlm_text": "custom imports  $=$  dict ( imports  $,=$  [ ' mmdet.models.roi_heads.double roi head ' ,  ' mmdet.models.bbox_heads.double_ bbox_head ' ]) ˓ → \nto the config file and achieve the same goal. \nThe config file of Double Head R-CNN is as the following \nThe image contains a configuration script for a machine learning model, likely using the MMDetection framework. Here’s a breakdown of its contents:\n\n- **Base Configuration**: The model builds upon a base configuration file located at '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'.\n\n- **Model Configuration**:\n  - **roi_head**:\n    - **type**: 'DoubleHeadRoHead'\n    - **reg_roi_scale_factor**: 1.3\n    - **bbox_head**:\n      - A dictionary containing various settings and parameters:\n        - **_delete_**: True\n        - **type**: 'DoubleConvFCBBoxHead'\n        - **num_convs**: 4\n        - **num_fcs**: 2\n        - **in_channels**: 256\n        - **conv_out_channels**: 1024\n        - **fc_out_channels**: 1024\n        - **roi_feat_size**: 7\n        - **num_classes**: 80\n        - **bbox_coder**:\n          - **type**: 'DeltaXYWHBBoxCoder'\n          - **target_means**: [0., 0., 0., 0.]\n          - **target_stds**: [0.1, 0.1, 0.2, 0.2]\n    - **reg_class_agnostic**: False\n\n- **Loss Function Configuration**:\n  - **loss_cls**:\n    - **type**: 'CrossEntropyLoss'\n    - **use_sigmoid**: False\n    - **loss_weight**: 2.0\n  - **loss_bbox**:\n    - **type**: 'SmoothL1Loss'\n    - **beta**: 1.0\n    - **loss_weight**: 2.0\n\nThis configuration file is likely meant for customizing a Faster R-CNN model.\nSince MM Detection 2.0, the config system supports to inherit configs such that the users can focus on the modification. The Double Head R-CNN mainly uses a new Double Head RoI Head and a new  Double Con v FCB Box Head , the arguments are set according to the  __init__  function of each module. \n11.1.4 Add new loss \nAssume you want to add a new loss as  MyLoss , for bounding box regression. To add a new loss function, the users need implement it in  mmdet/models/losses/my_loss.py . The decorator  weighted loss  enable the loss to be weighted for each element. \nThe table contains a Python code snippet for defining a custom loss function using PyTorch. Here’s a summary of the code:\n\n1. **Imports:**\n   - `import torch`\n   - `import torch.nn as nn`\n   - `from ..builder import LOSSES`\n   - `from .utils import weighted_loss`\n   \n2. **Function Definition:**\n   - The function `my_loss` is defined, which takes `pred` (prediction) and `target` (ground truth) as inputs.\n   - It uses the `@weighted_loss` decorator.\n   - An assertion checks that `pred` and `target` have the same size and that `target` is not empty.\n   - The loss is calculated as the absolute difference between `pred` and `target` using `torch.abs`.\n   - The calculated loss is returned. \n\nThis code snippet is likely part of a larger framework for handling losses in a machine learning context using PyTorch."}
{"page": 84, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_84.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n@LOSSES ..register_module()\nclass MyLoss(nn. Module):\n\ndef __init__(self, reduction='mean', loss_weight=1.0):\nsuper(MyLoss, self).__init__Q\nself.reduction = reduction\nself.loss_weight = loss_weight\n\ndef forward(self,\npred,\ntarget,\nweight=None,\navg_factor=None,\nreduction_override=None) :\nassert reduction_override in (None, 'none', ‘mean', ‘sum')\nreduction = (\nreduction_override if reduction_override else self.reduction)\nloss_bbox = self.loss_weight * my_loss(\npred, target, weight, reduction=reduction, avg_factor=avg_factor)\nreturn loss_bbox\n\nThen the users need to add it in the mmdet/models/losses/__init__.py.\n\nfrom .my_loss import MyLoss, my_loss\n\nAlternatively, you can add\n\ncustom_imports=dict(\nimports=['mmdet.models.losses.my_loss'])\n\nto the config file and achieve the same goal.\n\nTo use it, modify the loss_xxx field. Since MyLoss is for regression, you need to modify the loss_bbox field in the\nhead.\n\nloss_bbox=dict(type='MyLoss', loss_weight=1.0))\n\n11.1. Develop new components 77\n\n", "vlm_text": "This image depicts a Python class definition for a custom loss function, `MyLoss`, which is a subclass of `nn.Module`. The class includes:\n\n- An `__init__` method that initializes the loss with parameters `reduction` (default is 'mean') and `loss_weight` (default is 1.0). These parameters are stored in the instance variables `self.reduction` and `self.loss_weight`.\n\n- A `forward` method that takes several arguments: `pred`, `target`, `weight`, `avg_factor`, and `reduction_override`. It asserts that `reduction_override` is one of the specified options, determines the reduction method, calculates the loss weighted by `self.loss_weight`, and returns it.\n\nThe class is registered with a decorator `@LOSSES.register_module()`, suggesting it might be part of a larger framework or library for handling different types of losses.\nThen the users need to add it in the  mmdet/models/losses/__init__.py . \nfrom  .my_loss  import  MyLoss, my_loss \nAlternatively, you can add \nto the config file and achieve the same goal. \nTo use it, modify the  loss_xxx  field. Since MyLoss is for regression, you need to modify the  loss_bbox  field in the head. \nloss_bbox  $\\because$  dict ( type = ' MyLoss ' , loss weight = 1.0 )) "}
{"page": 85, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_85.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n78 Chapter 11. Tutorial 4: Customize Models\n", "vlm_text": "MMDetection, Release 2.18.0\n\n78 Chapter 11. Tutorial 4: Customize Models\n"}
{"page": 86, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_86.jpg", "ocr_text": "CHAPTER\nTWELVE\n\nTUTORIAL 5: CUSTOMIZE RUNTIME SETTINGS\n\n12.1 Customize optimization settings\n\n12.1.1 Customize optimizer supported by Pytorch\n\nWe already support to use all the optimizers implemented by PyTorch, and the only modification is to change the\noptimizer field of config files. For example, if you want to use ADAM (note that the performance could drop a lot), the\nmodification could be as the following.\n\noptimizer = dict(type='Adam', lr=0.0003, weight_decay=0.0001)\n\nTo modify the learning rate of the model, the users only need to modify the 1r in the config of optimizer. The users\ncan directly set arguments following the API doc of PyTorch.\n\n12.1.2 Customize self-implemented optimizer\n1. Define a new optimizer\n\nA customized optimizer could be defined as following.\n\nAssume you want to add a optimizer named MyOptimizer, which has arguments a, b, and c. You need to create a new\ndirectory named mmdet/core/optimizer. And then implement the new optimizer in a file, e.g., in mmdet/core/\noptimizer/my_optimizer. py:\n\nfrom .registry import OPTIMIZERS\nfrom torch.optim import Optimizer\n\n@OPTIMIZERS . register_module()\nclass MyOptimizer (Optimizer):\n\ndef __init__(self, a, b, c)\n\n79\n\n", "vlm_text": "TUTORIAL 5: CUSTOMIZE RUNTIME SETTINGS \n12.1 Customize optimization settings\n\n \n12.1.1 Customize optimizer supported by Pytorch \nWe already support to use all the optimizers implemented by PyTorch, and the only modification is to change the optimizer  field of config files. For example, if you want to use  ADAM  (note that the performance could drop a lot), the modification could be as the following. \noptimizer  =  dict ( type = ' Adam ' , lr = 0.0003 , weight decay = 0.0001 ) \nTo modify the learning rate of the model, the users only need to modify the  lr  in the config of optimizer. The users can directly set arguments following the  API doc  of PyTorch.\n\n \n12.1.2 Customize self-implemented optimizer\n\n \n1. Define a new optimizer \nA customized optimizer could be defined as following. \nAssume you want to add a optimizer named  My Optimizer , which has arguments  a ,  b , and  c . You need to create a new directory named  mmdet/core/optimizer . And then implement the new optimizer in a file, e.g., in  mmdet/core/ optimizer/my optimizer.py : \ndef  __init__ ( self , a, b, c) "}
{"page": 87, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_87.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n2. Add the optimizer to registry\nTo find the above module defined above, this module should be imported into the main namespace at first. There are\ntwo options to achieve it.\n\n* Modify mmdet/core/optimizer/__init__.py to import it.\n\nThe newly defined module should be imported in mmdet/core/optimizer/__init__.py so that the registry\nwill find the new module and add it:\n\nfrom .my_optimizer import MyOptimizer\n\n* Use custom_imports in the config to manually import it\n\ncustom_imports = dict(imports=['mmdet.core.optimizer.my_optimizer'], allow_failed_\nimports=False)\n\nThe module mmdet . core. optimizer .my_optimizer will be imported at the beginning of the program and the class\nMyOptimizer is then automatically registered. Note that only the package containing the class MyOptimizer should\nbe imported. mmdet .core. optimizer .my_optimizer .MyOptimizer cannot be imported directly.\n\nActually users can use a totally different file directory structure using this importing method, as long as the module\nroot can be located in PYTHONPATH.\n\n3. Specify the optimizer in the config file\n\nThen you can use MyOptimizer in optimizer field of config files. In the configs, the optimizers are defined by the\nfield optimizer like the following:\n\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n\nTo use your own optimizer, the field can be changed to\n\noptimizer = dict(type='MyOptimizer', a=a_value, b=b_value, c=c_value)\n\n12.1.3 Customize optimizer constructor\n\nSome models may have some parameter-specific settings for optimization, e.g. weight decay for BatchNorm layers.\nThe users can do those fine-grained parameter tuning through customizing optimizer constructor.\n\nfrom mmcv.utils import build_from_cfg\nfrom mmcv.runner.optimizer import OPTIMIZER_BUILDERS, OPTIMIZERS\n\nfrom mmdet.utils import get_root_logger\nfrom .my_optimizer import MyOptimizer\n\n@OPTIMIZER_BUILDERS .. register_module()\nclass MyOptimizerConstructor (object):\ndef __init__(self, optimizer_cfg, paramwise_cfg=None):\n\ndef __call__(self, model):\n\n(continues on next page)\n\n80 Chapter 12. Tutorial 5: Customize Runtime Settings\n\n", "vlm_text": "2. Add the optimizer to registry \nTo find the above module defined above, this module should be imported into the main namespace at first. There are two options to achieve it. \n• Modify  mmdet/core/optimizer/__init__.py  to import it. The newly defined module should be imported in  mmdet/core/optimizer/__init__.py  so that the registry will find the new module and add it: \nfrom  .my optimizer  import  My Optimizer \n• Use  custom imports  in the config to manually import it \ncustom imports  $=$   dict (imports  $,=$  [ ' mmdet.core.optimizer.my optimizer ' ], allow failed  $\\hookrightarrow$  imports  $=$  False ) → \nThe module  mmdet.core.optimizer.my optimizer  will be imported at the beginning of the program and the class My Optimizer  is then automatically registered. Note that only the package containing the class  My Optimizer  should be imported.  mmdet.core.optimizer.my optimizer.My Optimizer  cannot  be imported directly. \nActually users can use a totally different file directory structure using this importing method, as long as the module root can be located in  PYTHONPATH . \n3. Specify the optimizer in the config file \nThen you can use  My Optimizer  in  optimizer  field of config files. In the configs, the optimizers are defined by the field  optimizer  like the following: \nThe table contains two code snippets for configuring an optimizer in a dictionary format:\n\n1. The first snippet sets up a Stochastic Gradient Descent (SGD) optimizer with the following parameters:\n   - `type`: `'SGD'`\n   - `lr`: `0.02` (learning rate)\n   - `momentum`: `0.9`\n   - `weight_decay`: `0.0001`\n\n2. The second snippet provides a template for using a custom optimizer by replacing the type and parameters:\n   - `type`: `'MyOptimizer'`\n   - Custom parameters `a`, `b`, `c` with placeholders `a_value`, `b_value`, `c_value`.\n12.1.3 Customize optimizer constructor \nSome models may have some parameter-specific settings for optimization, e.g. weight decay for BatchNorm layers. The users can do those fine-grained parameter tuning through customizing optimizer constructor. \nThe image shows a snippet of Python code related to optimizer configuration in a machine learning context, possibly using the MMDetection framework. It includes:\n\n1. Imports from various modules such as `mmcv` and `mmdet`, and a custom module `.my_optimizer`.\n2. A decorator `@OPTIMIZER_BUILDERS.register_module()` is used to register a class.\n3. Definition of a class `MyOptimizerConstructor` with an `__init__` method, which takes `optimizer_cfg` and an optional `paramwise_cfg`.\n4. A `__call__` method that takes a `model` as a parameter. \n\nThe code appears to be setting up a custom optimizer."}
{"page": 88, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_88.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nreturn my_optimizer\n\nThe default optimizer constructor is implemented here, which could also serve as a template for new optimizer con-\nstructor.\n\n12.1.4 Additional settings\n\nTricks not implemented by the optimizer should be implemented through optimizer constructor (e.g., set parameter-\nwise learning rates) or hooks. We list some common settings that could stabilize the training or accelerate the training.\nFeel free to create PR, issue for more settings.\n\n* Use gradient clip to stabilize training: Some models need gradient clip to clip the gradients to stabilize the\ntraining process. An example is as below:\n\noptimizer_config = dict(\n_delete_=True, grad_clip=dict (max_norm=35, norm_type=2))\n\nIf your config inherits the base config which already sets the optimizer_config, you might need\n_delete_=True to override the unnecessary settings. See the config documentation for more details.\n\n* Use momentum schedule to accelerate model convergence: We support momentum scheduler to modify\nmodel’s momentum according to learning rate, which could make the model converge in a faster way. Mo-\nmentum scheduler is usually used with LR scheduler, for example, the following config is used in 3D detection\nto accelerate convergence. For more details, please refer to the implementation of CyclicLrUpdater and Cyclic-\nMomentumUpdater.\n\nlr_config = dict(\npolicy='cyclic',\ntarget_ratio=(10, le-4),\ncyclic_times=1,\nstep_ratio_up=0.4,\n\n)\n\nmomentum_config = dict(\npolicy='cyclic',\ntarget_ratio=(0.85 / 0.95, 1),\ncyclic_times=1,\nstep_ratio_up=0.4,\n\n12.2 Customize training schedules\n\nBy default we use step learning rate with 1x schedule, this calls StepLRHook in MMCV. We support many other\nlearning rate schedule here, such as CosineAnnealing and Poly schedule. Here are some examples\n\n* Poly schedule:\n\nlr_config = dict(policy='poly', power=0.9, min_lr=1e-4, by_epoch=False)\n\n* ConsineAnnealing schedule:\n\n12.2. Customize training schedules 81\n\n", "vlm_text": "The table contains a single line of code: `return my_optimizer`. This looks like it could be part of a function in a programming language such as Python, where the function is likely designed to return an object or value named `my_optimizer`. The exact context or purpose of this code is not provided in the image.\nThe default optimizer constructor is implemented  here , which could also serve as a template for new optimizer con- structor. \n12.1.4 Additional settings \nTricks not implemented by the optimizer should be implemented through optimizer constructor (e.g., set parameter- wise learning rates) or hooks. We list some common settings that could stabilize the training or accelerate the training. Feel free to create PR, issue for more settings. \n•  Use gradient clip to stabilize training : Some models need gradient clip to clip the gradients to stabilize the training process. An example is as below: \nThe image shows a snippet of Python code that defines a dictionary named `optimizer_config`. The dictionary contains:\n\n- A key `_delete_` set to `True`.\n- A nested dictionary under the key `grad_clip`, which includes:\n  - `max_norm` set to `35`\n  - `norm_type` set to `2`\nIf your config inherits the base config which already sets the  optimizer config , you might need _delete_  $\\bar{\\cdot}$  True  to override the unnecessary settings. See the  config documentation  for more details. \n•  Use momentum schedule to accelerate model convergence : We support momentum scheduler to modify model’s momentum according to learning rate, which could make the model converge in a faster way. Mo- mentum scheduler is usually used with LR scheduler, for example, the following config is used in 3D detection to accelerate convergence. For more details, please refer to the implementation of  Cyclic Lr Updater  and  Cyclic- Momentum Updater . \nThis is not a table, but a snippet of Python code. It defines two dictionaries: `lr_config` and `momentum_config`. Both dictionaries use a 'cyclic' policy and contain several parameters:\n\n- `lr_config` settings:\n  - `policy='cyclic'`\n  - `target_ratio=(10, 1e-4)`\n  - `cyclic_times=1`\n  - `step_ratio_up=0.4`\n\n- `momentum_config` settings:\n  - `policy='cyclic'`\n  - `target_ratio=(0.85 / 0.95, 1)`\n  - `cyclic_times=1`\n  - `step_ratio_up=0.4`\n\nThese configurations might be used for setting learning rate and momentum schedules, often within machine learning or optimization contexts.\n12.2 Customize training schedules \nBy default we use step learning rate with 1x schedule, this calls  StepLRHook  in MMCV. We support many other learning rate schedule  here , such as  Cosine Annealing  and  Poly  schedule. Here are some examples \n• Poly schedule: \nThe image shows a snippet of Python code, specifically configuring a learning rate schedule. The code is:\n\n```python\nlr_config = dict(policy='poly', power=0.9, min_lr=1e-4, by_epoch=False)\n```\n\nThis dictionary sets the learning rate policy to 'poly', with a power of 0.9, a minimum learning rate of 0.0001, and specifies that adjustments are not made by epoch.\n• Con sine Annealing schedule: "}
{"page": 89, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_89.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nlr_config = dict(\npolicy='CosineAnnealing',\nwarmup='linear',\nwarmup_iters=1000,\nwarmup_ratio=1.0 / 10,\nmin_lr_ratio=le-5)\n\n12.3 Customize workflow\n\nWorkflow is a list of (phase, epochs) to specify the running order and epochs. By default it is set to be\n\nworkflow = [('train', 1)]\n\nwhich means running | epoch for training. Sometimes user may want to check some metrics (e.g. loss, accuracy) about\nthe model on the validate set. In such case, we can set the workflow as\n\n[C'train', 1), C'val', 1]\n\nso that 1 epoch for training and 1 epoch for validation will be run iteratively.\nNote:\n1. The parameters of model will not be updated during val epoch.\n\n2. Keyword total_epochs in the config only controls the number of training epochs and will not affect the vali-\ndation workflow.\n\n3. Workflows [C('train', 1), ('val', 1)] and [('train', 1)] will not change the behavior of EvalHook\nbecause EvalHook is called by after_train_epoch and validation workflow only affect hooks that are called\nthrough after_val_epoch. Therefore, the only difference between [('train', 1), ('val', 1)] and\n[C'train', 1)] is that the runner will calculate losses on validation set after each training epoch.\n\n12.4 Customize hooks\n\n12.4.1 Customize self-implemented hooks\n\n1. Implement a new hook\n\nThere are some occasions when the users might need to implement a new hook. MMDetection supports customized\nhooks in training (#3395) since v2.3.0. Thus the users could implement a hook directly in mmdet or their mmdet-based\ncodebases and use the hook by only modifying the config in training. Before v2.3.0, the users need to modify the code\nto get the hook registered before training starts. Here we give an example of creating a new hook in mmdet and using\nit in training.\n\nfrom mmcv.runner import HOOKS, Hook\n\nGHOOKS . register_module()\nclass MyHook(Hook):\n\ndef __init__(self, a, b):\n\n(continues on next page)\n\n82 Chapter 12. Tutorial 5: Customize Runtime Settings\n\n", "vlm_text": "The table contains a Python dictionary configuration for learning rate scheduling in a machine learning context. Here are the key elements:\n\n- `policy`: 'CosineAnnealing' (type of learning rate schedule)\n- `warmup`: 'linear' (method of warming up the learning rate)\n- `warmup_iters`: 1000 (number of iterations for warming up)\n- `warmup_ratio`: 0.1 (starting ratio of the learning rate during warmup)\n- `min_lr_ratio`: 1e-5 (minimum learning rate ratio)\n12.3 Customize workflow \nWorkflow is a list of (phase, epochs) to specify the running order and epochs. By default it is set to be \nworkflow  =  [( ' train ' ,  1 )] \nwhich means running 1 epoch for training. Sometimes user may want to check some metrics (e.g. loss, accuracy) about the model on the validate set. In such case, we can set the workflow as \n[( ' train ' ,  1 ), ( ' val ' ,  1 )] \nso that 1 epoch for training and 1 epoch for validation will be run iterative ly. \nNote : \n1. The parameters of model will not be updated during val epoch. 2. Keyword  total epochs  in the config only controls the number of training epochs and will not affect the vali- dation workflow. \n3. Workflows  [( ' train ' , 1), ( ' val ' , 1)]  and  [( ' train ' , 1)]  will not change the behavior of  EvalHook because  EvalHook  is called by  after train epoch  and validation workflow only affect hooks that are called through  after val epoch . Therefore, the only difference between  [( ' train ' , 1), ( ' val ' , 1)]  and [( ' train ' , 1)]  is that the runner will calculate losses on validation set after each training epoch. \n12.4 Customize hooks \n12.4.1 Customize self-implemented hooks \n1. Implement a new hook \nThere are some occasions when the users might need to implement a new hook. MM Detection supports customized hooks in training (#3395) since v2.3.0. Thus the users could implement a hook directly in mmdet or their mmdet-based codebases and use the hook by only modifying the config in training. Before v2.3.0, the users need to modify the code to get the hook registered before training starts. Here we give an example of creating a new hook in mmdet and using it in training. \nThe image contains code written in Python, which is not a table. The code defines a custom hook class that can be used in a machine learning model training framework that uses the MMCV library. Here’s a breakdown of the code:\n\n1. **Imports**:\n   ```python\n   from mmcv.runner import HOOKS, Hook\n   ```\n   - This line imports `HOOKS` and `Hook` from the `mmcv.runner` module.\n\n2. **Decorator**:\n   ```python\n   @HOOKS.register_module()\n   ```\n   - This decorator is used to register the custom hook class with the MMCV framework, making it available for use.\n\n3. **Class Definition**:\n   ```python\n   class MyHook(Hook):\n   ```\n   - This line defines a new class `MyHook` that inherits from `Hook`.\n\n4. **Constructor Method**:\n   ```python\n   def __init__(self, a, b):\n   ```\n   - This is the initialization method for the `MyHook` class. It takes two parameters, `a` and `b`.\n\nThis setup is typically used to extend the functionality of a model training process, allowing the execution of custom code at specific points during the training."}
{"page": 90, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_90.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\npass\n\ndef before_run(self, runner):\npass\n\ndef after_run(self, runner):\npass\n\ndef before_epoch(self, runner):\npass\n\ndef after_epoch(self, runner):\npass\n\ndef before_iter(self, runner):\npass\n\ndef after_iter(self, runner):\npass\n\nDepending on the functionality of the hook, the users need to specify what the hook will do at each stage of the training\nin before_run, after_run, before_epoch, after_epoch, before_iter, and after_iter.\n\n2. Register the new hook\nThen we need to make MyHook imported. Assuming the file is in mmdet/core/utils/my_hook. py there are two\nways to do that:\n\n* Modify mmdet/core/utils/__init__.py to import it.\n\nThe newly defined module should be imported in mmdet/core/utils/__init__.py so that the registry will\nfind the new module and add it:\n\nfrom .my_hook import MyHook\n\n* Use custom_imports in the config to manually import it\n\ncustom_imports = dict(imports=['mmdet.core.utils.my_hook'], allow_failed_imports=False)\n\n3. Modify the config\n\ncustom_hooks = [\ndict(type='MyHook', a=a_value, b=b_value)\n]\n\nYou can also set the priority of the hook by adding key priority to 'NORMAL' or 'HIGHEST' as below\n\ncustom_hooks = [\ndict(type='MyHook', a=a_value, b=b_value, priority='NORMAL')\n]\n\nBy default the hook’s priority is set as NORMAL during registration.\n\n12.4. Customize hooks 83\n\n", "vlm_text": "The image displays a snippet of Python code which consists of several method definitions within a class. These methods appear to be placeholders because each method body contains only the `pass` statement, which indicates that they are yet to be implemented. The methods are:\n\n1. `before_run(self, runner)`\n2. `after_run(self, runner)`\n3. `before_epoch(self, runner)`\n4. `after_epoch(self, runner)`\n5. `before_iter(self, runner)`\n6. `after_iter(self, runner)`\n\nEach method takes a `runner` parameter, and the method names suggest that they are intended as hook methods to be executed at specific points in a process, likely related to machine learning or training iterations (before and after a run, epoch, or iteration).\nDepending on the functionality of the hook, the users need to specify what the hook will do at each stage of the training in  before_run ,  after_run ,  before epoch ,  after epoch ,  before it er , and  after_iter . \n2. Register the new hook \nThen we need to make  MyHook  imported. Assuming the file is in  mmdet/core/utils/my_hook.py  there are two ways to do that: \n• Modify  mmdet/core/utils/__init__.py  to import it. The newly defined module should be imported in  mmdet/core/utils/__init__.py  so that the registry will find the new module and add it: \n3. Modify the config \nThe image shows a code snippet in Python. It creates a list called `custom_hooks` containing a single dictionary. The dictionary has three key-value pairs:\n\n- `type` with a value of `'MyHook'`\n- `a` with a value of `a_value`\n- `b` with a value of `b_value`\nYou can also set the priority of the hook by adding key  priority  to  ' NORMAL '  or  ' HIGHEST '  as below \nThe image shows a snippet of Python code that defines a list called `custom_hooks`. The list contains a single dictionary with the following key-value pairs: \n\n- `type` set to `'MyHook'`\n- `a` set to `a_value`\n- `b` set to `b_value`\n- `priority` set to `'NORMAL'`\nBy default the hook’s priority is set as  NORMAL  during registration. "}
{"page": 91, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_91.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n12.4.2 Use hooks implemented in MMCV\n\nIf the hook is already implemented in MMCYV, you can directly modify the config to use the hook as below\n\n4. Example: NumClassCheckHook\n\nWe implement a customized hook named NumClassCheckHook to check whether the num_classes in head matches\nthe length of CLASSSES in dataset.\n\nWe set it in default_runtime.py.\n\ncustom_hooks = [dict(type='NumClassCheckHook')]\n\n12.4.3 Modify default runtime hooks\n\nThere are some common hooks that are not registered through custom_hooks, they are\n* log_config\n* checkpoint_config\n* evaluation\n* Ir_config\n* optimizer_config\n* momentum_config\n\nIn those hooks, only the logger hook has the VERY_LOW priority, others’ priority are NORMAL. The above-mentioned\ntutorials already covers how to modify optimizer_config, momentum_config, and lr_config. Here we reveals\nhow what we can do with log_config, checkpoint_config, and evaluation.\n\nCheckpoint config\n\nThe MMCV runner will use checkpoint_config to initialize CheckpointHook.\n\ncheckpoint_config = dict(interval=1)\n\nThe users could set max_keep_ckpts to only save only small number of checkpoints or decide whether to store state\ndict of optimizer by save_optimizer. More details of the arguments are here\n\nLog config\n\nThe log_config wraps multiple logger hooks and enables to set intervals. Now MMCV supports WandbLoggerHook,\nM1f£lowLoggerHook, and TensorboardLoggerHook. The detail usages can be found in the doc.\n\nlog_config = dict(\ninterval=50,\nhooks=[\ndict (type='TextLoggerHook'),\ndict (type='TensorboardLoggerHook')\nsp)\n\n84 Chapter 12. Tutorial 5: Customize Runtime Settings\n\n", "vlm_text": "12.4.2 Use hooks implemented in MMCV \nIf the hook is already implemented in MMCV, you can directly modify the config to use the hook as below \n4. Example:  Num Class Check Hook \nWe implement a customized hook named  Num Class Check Hook  to check whether the  num classes  in head matches the length of  CLASSSES  in  dataset . \nWe set it in  default runtime.py . \ncustom hooks  $=$   [ dict ( type  $\\circeq$  ' Num Class Check Hook ' )] \n12.4.3 Modify default runtime hooks \nThere are some common hooks that are not registered through  custom hooks , they are \n• log_config • checkpoint config • evaluation • lr_config • optimizer config • momentum config \nIn those hooks, only the logger hook has the  VERY_LOW  priority, others’ priority are  NORMAL . The above-mentioned tutorials already covers how to modify  optimizer config ,  momentum config , and  lr_config . Here we reveals how what we can do with  log_config ,  checkpoint config , and  evaluation . \nCheckpoint config \nThe MMCV runner will use  checkpoint config  to initialize  Checkpoint Hook . \ncheckpoint config  =  dict (interval = 1 ) \nThe users could set  max keep ck pts  to only save only small number of checkpoints or decide whether to store state dict of optimizer by  save optimizer . More details of the arguments are  here \nLog config \nThe  log_config  wraps multiple logger hooks and enables to set intervals. Now MMCV supports  Wand bLogger Hook , Ml flow Logger Hook , and  Tensor board Logger Hook . The detail usages can be found in the  doc . \nlog_config  $=$   dict ( interval  $\\scriptstyle{\\varepsilon=50}$  , hooks  $=$  [ dict ( type  $=\"$  Text Logger Hook ' ), dict ( type  $=\"$  Tensor board Logger Hook ' ) ]) "}
{"page": 92, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_92.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nEvaluation config\n\nThe config of evaluation will be used to initialize the EvalHook. Except the key interval, other arguments such\n\nas metric will be passed to the dataset. evaluate()\n\nevaluation = dict(interval=1, metric='bbox')\n\n12.4. Customize hooks\n\n85\n\n", "vlm_text": "Evaluation config \nThe config of  evaluation  will be used to initialize the  EvalHook . Except the key  interval , other arguments such as  metric  will be passed to the  dataset.evaluate() "}
{"page": 93, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_93.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n86 Chapter 12. Tutorial 5: Customize Runtime Settings\n", "vlm_text": "MMDetection, Release 2.18.0\n\n86 Chapter 12. Tutorial 5: Customize Runtime Settings\n"}
{"page": 94, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_94.jpg", "ocr_text": "CHAPTER\nTHIRTEEN\n\nTUTORIAL 6: CUSTOMIZE LOSSES\n\nMMbDetection provides users with different loss functions. But the default configuration may be not applicable for\ndifferent datasets or models, so users may want to modify a specific loss to adapt the new situation.\n\nThis tutorial first elaborate the computation pipeline of losses, then give some instructions about how to modify each\nstep. The modification can be categorized as tweaking and weighting.\n\n13.1 Computation pipeline of a loss\n\nGiven the input prediction and target, as well as the weights, a loss function maps the input tensor to the final loss scalar.\nThe mapping can be divided into four steps:\n\n1.\n\nSet the sampling method to sample positive and negative samples.\n\n. Get element-wise or sample-wise loss by the loss kernel function.\n. Weighting the loss with a weight tensor element-wisely.\n\n2\n3\n4.\n5\n\nReduce the loss tensor to a scalar.\n\n. Weighting the loss with a scalar.\n\n13.2 Set sampling method (step 1)\n\nFor some loss functions, sampling strategies are needed to avoid imbalance between positive and negative samples.\n\nFor example, when using CrossEntropyLoss in RPN head, we need to set RandomSampler in train_cfg\n\ntrain_cfg=dict(\n\nrpn=dict(\n\nsampler=dict(\ntype='RandomSampler',\nnum=256,\npos_fraction=0.5,\nneg_pos_ub=-1,\nadd_gt_as_proposals=False))\n\nFor some other losses which have positive and negative sample balance mechanism such as Focal Loss, GHMC, and\nQualityFocalLoss, the sampler is no more necessary.\n\n87\n\n", "vlm_text": "TUTORIAL 6: CUSTOMIZE LOSSES \nMM Detection provides users with different loss functions. But the default configuration may be not applicable for different datasets or models, so users may want to modify a specific loss to adapt the new situation. \nThis tutorial first elaborate the computation pipeline of losses, then give some instructions about how to modify each step. The modification can be categorized as tweaking and weighting. \n13.1 Computation pipeline of a loss \nGiven the input prediction and target, as well as the weights, a loss function maps the input tensor to the final loss scalar. The mapping can be divided into four steps: \n1. Set the sampling method to sample positive and negative samples. 2. Get  element-wise  or  sample-wise  loss by the loss kernel function. 3. Weighting the loss with a weight tensor  element-wisely . 4. Reduce the loss tensor to a  scalar . 5. Weighting the loss with a  scalar . \n13.2 Set sampling method (step 1) \nFor some loss functions, sampling strategies are needed to avoid imbalance between positive and negative samples. For example, when using  Cross Entropy Loss  in RPN head, we need to set  Random Sampler  in  train_cfg \ntrain_cfg = dict ( rpn  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  dict ( sampler = dict ( type  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}^{\\dagger}$  ' Random Sampler ' , num = 256 , pos fraction = 0.5 , neg_pos_ub  $\\mathrm{=}\\mathrm{-}\\mathrm{1}$  , add gt as proposals  $,=$  False )) \nFor some other losses which have positive and negative sample balance mechanism such as Focal Loss, GHMC, and Quality Focal Loss, the sampler is no more necessary. "}
{"page": 95, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_95.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n13.3 Tweaking loss\n\nTweaking a loss is more related with step 2, 4, 5, and most modifications can be specified in the config. Here we take\nFocal Loss (FL) as an example. The following code sniper are the construction method and config of FL respectively,\nthey are actually one to one correspondence.\n\n@LOSSES ..register_module()\nclass FocalLoss(nn.Module):\n\ndef __init__(self,\n\nuse_sigmoid=True,\ngamma=2.0,\nalpha=0.25,\nreduction='mean',\nloss_weight=1.0):\n\nloss_cls=dict(\n\ntype='FocalLoss',\nuse_sigmoid=True,\ngamma=2.0,\nalpha=0.25,\nloss_weight=1.0)\n\n13.3.1 Tweaking hyper-parameters (step 2)\n\ngamma and beta are two hyper-parameters in the Focal Loss. Say if we want to change the value of gamma to be 1.5\nand alpha to be 0.5, then we can specify them in the config as follows:\n\nloss_cls=dict(\n\ntype='FocalLoss',\nuse_sigmoid=True,\ngamma=1.5,\nalpha=0.5,\nloss_weight=1.0)\n\n13.3.2 Tweaking the way of reduction (step 3)\n\nThe default way of reduction is mean for FL. Say if we want to change the reduction from mean to sum, we can specify\nit in the config as follows:\n\nloss_cls=dict(\n\ntype='FocalLoss',\nuse_sigmoid=True,\ngamma=2.0,\nalpha=0.25,\nloss_weight=1.0,\nreduction='sum')\n\n88\n\nChapter 13. Tutorial 6: Customize Losses\n\n", "vlm_text": "13.3 Tweaking loss \nTweaking a loss is more related with step 2, 4, 5, and most modifications can be specified in the config. Here we take Focal Loss (FL)  as an example. The following code sniper are the construction method and config of FL respectively, they are actually one to one correspondence. \nThe image contains Python code related to the implementation of a custom loss function called `FocalLoss`, which is a subclass of `nn.Module`. The `__init__` method of `FocalLoss` includes parameters `use_sigmoid`, `gamma`, `alpha`, `reduction`, and `loss_weight`.\n\nBelow this, there is a dictionary named `loss_cls` that holds configuration settings for `FocalLoss`, with keys and values corresponding to the parameters provided in the `__init__` method, such as `type`, `use_sigmoid`, `gamma`, `alpha`, and `loss_weight`.\n13.3.1 Tweaking hyper-parameters (step 2) \ngamma  and  beta  are two hyper-parameters in the Focal Loss. Say if we want to change the value of  gamma  to be 1.5 and  alpha  to be 0.5, then we can specify them in the config as follows: \nThe image contains code that configures a dictionary for a classification loss function. It specifies parameters for \"FocalLoss,\" including:\n\n- `type`: 'FocalLoss'\n- `use_sigmoid`: True\n- `gamma`: 1.5\n- `alpha`: 0.5\n- `loss_weight`: 1.0\n\nThese parameters are commonly used in machine learning models for adjusting the loss function behavior.\n13.3.2 Tweaking the way of reduction (step 3) \nThe default way of reduction is  mean  for FL. Say if we want to change the reduction from  mean  to  sum , we can specify it in the config as follows: \nThe image shows a snippet of Python code that defines a dictionary named `loss_cls`. This dictionary is used to configure the parameters for a loss function, specifically the Focal Loss, which is a type of loss function often used in training deep learning models for classification tasks. The parameters set in the dictionary include:\n\n- `type`: Specifies the type of loss function, which is set to `'FocalLoss'`.\n- `use_sigmoid`: A boolean set to `True`, indicating that a sigmoid function should be used.\n- `gamma`: A float set to `2.0`, which is a hyperparameter of the Focal Loss that adjusts the rate at which easy examples are down-weighted.\n- `alpha`: A float set to `0.25`, another hyperparameter that balances the importance of positive/negative examples.\n- `loss_weight`: A float set to `1.0`, indicating the weight of this loss during optimization.\n- `reduction`: A string set to `'sum'`, indicating that the loss values should be summed."}
{"page": 96, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_96.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n13.3.3 Tweaking loss weight (step 5)\n\nThe loss weight here is a scalar which controls the weight of different losses in multi-task learning, e.g. classification\nloss and regression loss. Say if we want to change to loss weight of classification loss to be 0.5, we can specify it in\nthe config as follows:\n\nloss_cls=dict(\ntype='FocalLoss',\nuse_sigmoid=True,\ngamma=2.0,\nalpha=0.25,\nloss_weight=0.5)\n\n13.4 Weighting loss (step 3)\n\nWeighting loss means we re-weight the loss element-wisely. To be more specific, we multiply the loss tensor with a\nweight tensor which has the same shape. As a result, different entries of the loss can be scaled differently, and so called\nelement-wisely. The loss weight varies across different models and highly context related, but overall there are two\nkinds of loss weights, label_weights for classification loss and bbox_weights for bbox regression loss. You can\nfind them in the get_target method of the corresponding head. Here we take ATSSHead as an example, which inherit\nAnchorHead but overwrite its get_targets method which yields different label_weights and bbox_weights.\n\nclass ATSSHead(AnchorHead) :\n\ndef get_targets(self,\nanchor_list,\nvalid_flag_list,\ngt_bboxes_list,\nimg_metas,\ngt_bboxes_ignore_list=None,\ngt_labels_list=None,\nlabel_channels=1,\nunmap_outputs=True) :\n\n13.4. Weighting loss (step 3) 89\n\n", "vlm_text": "13.3.3 Tweaking loss weight (step 5) \nThe loss weight here is a scalar which controls the weight of different losses in multi-task learning, e.g. classification loss and regression loss. Say if we want to change to loss weight of classification loss to be 0.5, we can specify it in the config as follows: \nloss_cls = dict ( type = ' FocalLoss ' , use s igm oid = True , gamma  $\\scriptstyle-2\\,.\\,\\mathbb{0}$  , alpha  ${\\it\\Delta\\phi}=\\ 0\\ .\\ 25$  , loss weight = 0.5 ) \n13.4 Weighting loss (step 3) \nWeighting loss means we re-weight the loss element-wisely. To be more specific, we multiply the loss tensor with a weight tensor which has the same shape. As a result, different entries of the loss can be scaled differently, and so called element-wisely. The loss weight varies across different models and highly context related, but overall there are two kinds of loss weights,  label weights  for classification loss and  b box weights  for bbox regression loss. You can find them in the  get_target  method of the corresponding head. Here we take  ATSSHead  as an example, which inherit AnchorHead  but overwrite its  get targets  method which yields different  label weights  and  b box weights . \nThis image shows a snippet of Python code defining a class and a method within that class. It is not a table. Here's a breakdown of the code:\n\n- **Class Definition:**\n  - `ATSSHead` is a class that inherits from `AnchorHead`.\n\n- **Method Definition:**\n  - `get_targets(self, ...)`: This is a method within the `ATSSHead` class.\n  - It takes several parameters:\n    - `self`: Refers to the instance of the class.\n    - `anchor_list`\n    - `valid_flag_list`\n    - `gt_bboxes_list`\n    - `img_metas`\n    - `gt_bboxes_ignore_list` (default=None)\n    - `gt_labels_list` (default=None)\n    - `label_channels` (default=1)\n    - `unmap_outputs` (default=True)\n\nThis method likely deals with processing or handling targets for the object detection task."}
{"page": 97, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_97.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n90 Chapter 13. Tutorial 6: Customize Losses\n", "vlm_text": "MMDetection, Release 2.18.0\n\n90 Chapter 13. Tutorial 6: Customize Losses\n"}
{"page": 98, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_98.jpg", "ocr_text": "CHAPTER\nFOURTEEN\n\nTUTORIAL 7: FINETUNING MODELS\n\nDetectors pre-trained on the COCO dataset can serve as a good pre-trained model for other datasets, e.g., CityScapes\nand KITTI Dataset. This tutorial provides instruction for users to use the models provided in the Model Zoo for other\ndatasets to obtain better performance.\n\nThere are two steps to finetune a model on a new dataset.\n« Add support for the new dataset following Tutorial 2: Customize Datasets.\n* Modify the configs as will be discussed in this tutorial.\n\nTake the finetuning process on Cityscapes Dataset as an example, the users need to modify five parts in the config.\n\n14.1 Inherit base configs\n\nTo release the burden and reduce bugs in writing the whole configs, MMDetection V2.0 support inheriting configs\nfrom multiple existing configs. To finetune a Mask RCNN model, the new config needs to inherit _base_/models/\nmask_rcnn_r50_fpn. py to build the basic structure of the model. To use the Cityscapes Dataset, the new config can\nalso simply inherit _base_/datasets/cityscapes_instance. py. For runtime settings such as training schedules,\nthe new config needs to inherit _base_/default_runtime.py. This configs are in the configs directory and the\nusers can also choose to write the whole contents rather than use inheritance.\n\n_base_ = [\n',./_base_/models/mask_rcnn_r50_fpn.py',\n'../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py'\n]\n\n14.2 Modify head\n\nThen the new config needs to modify the head according to the class numbers of the new datasets. By only changing\nnum_classes in the roi_head, the weights of the pre-trained models are mostly reused except the final prediction head.\n\nmodel = dict(\npretrained=None,\nroi_head=dict(\nbbox_head=dict (\n\ntype='Shared2FCBBoxHead',\nin_channels=256,\nfc_out_channels=1024,\nroi_feat_size=7,\n\n(continues on next page)\n\n91\n\n", "vlm_text": "TUTORIAL 7: FINETUNING MODELS \nDetectors pre-trained on the COCO dataset can serve as a good pre-trained model for other datasets, e.g., CityScapes and KITTI Dataset. This tutorial provides instruction for users to use the models provided in the  Model Zoo  for other datasets to obtain better performance. \nThere are two steps to finetune a model on a new dataset. \n• Add support for the new dataset following  Tutorial 2: Customize Datasets . • Modify the configs as will be discussed in this tutorial. \nTake the finetuning process on Cityscapes Dataset as an example, the users need to modify five parts in the config. \n14.1 Inherit base configs \nTo release the burden and reduce bugs in writing the whole configs, MM Detection V2.0 support inheriting configs from multiple existing configs. To finetune a Mask RCNN model, the new config needs to inherit  _base_/models/ mask_rcnn_  $\\mathtt{r50}$  _fpn.py  to build the basic structure of the model. To use the Cityscapes Dataset, the new config can also simply inherit  _base_/datasets/cityscape s instance.py . For runtime settings such as training schedules, the new config needs to inherit  _base_/default runtime.py . This configs are in the  configs  directory and the users can also choose to write the whole contents rather than use inheritance. \n14.2 Modify head \nThen the new config needs to modify the head according to the class numbers of the new datasets. By only changing num classes  in the roi_head, the weights of the pre-trained models are mostly reused except the final prediction head. "}
{"page": 99, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_99.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nnum_classes=8,\nbbox_coder=dict (\ntype='DeltaXYWHBBoxCoder' ,\ntarget_means=[0., 0., 0., 0.],\ntarget_stds=[0.1, 0.1, 0.2, 0.2]),\nreg_class_agnostic=False,\nloss_cls=dict(\ntype='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\nloss_bbox=dict(type='SmoothLiLoss', beta=1.0, loss_weight=1.0)),\nmask_head=dict(\ntype='FCNMaskHead',\nnum_convs=4,\nin_channels=256,\nconv_out_channels=256,\nnum_classes=8,\nloss_mask=dict (\ntype='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n\n14.3 Modify dataset\n\nThe users may also need to prepare the dataset and write the configs about dataset. MMDetection V2.0 already support\nVOC, WIDER FACE, COCO and Cityscapes Dataset.\n\n14.4 Modify training schedule\n\nThe finetuning hyperparameters vary from the default schedule. It usually requires smaller learning rate and less\ntraining epochs\n\n# optimizer\n# Ir is set for a batch size of 8\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\npolicy='step',\nwarmup='linear',\nwarmup_iters=500,\nwarmup_ratio=0.001,\nstep=[7])\n# the max_epochs and step in Ir_config need specifically tuned for the customized dataset\nrunner = dict(max_epochs=8)\nlog_config = dict(interval=100)\n\n92 Chapter 14. Tutorial 7: Finetuning Models\n\n", "vlm_text": "num classes  $^{=8}$  , bbox_coder  $=$  dict ( type  $\\equiv^{\\dagger}$  Delta XY WH B Box Code r ' , target means  $=$  [ 0. ,  0. ,  0. ,  0. ], target stds  $,=$  [ 0.1 ,  0.1 ,  0.2 ,  0.2 ]), reg class agnostic  $=$  False , loss_cls  $=$  dict ( type  $\\equiv^{\\dagger}$  Cross Entropy Loss ' , use s igm oid = False , loss weight  $\\scriptstyle=1.\\,\\mathbb{O}.$  ), loss_bbox  $\\fallingdotseq$  dict ( type  $\\equiv^{1}$  ' Smooth L 1 Loss ' , beta  $\\scriptstyle=1.\\,\\mathbb{0}$  , loss weight  $\\scriptstyle=1\\,.\\,\\mathbb{0})$  ), mask_head = dict ( type  $\\equiv^{\\dagger}$  FC N Mask Head ' , num_convs  ${=}4$  , in channels  $\\iota{=}256$  , con v out channel  $s{=}256$  , num classes  $^{=8}$  , loss_mask  $\\fallingdotseq$  dict ( type $\\L=\\Gamma$ Cross Entropy Loss', use_mask $\\risingdotseq$ True, loss weight=1.0))))\n14.3 Modify dataset \nThe users may also need to prepare the dataset and write the configs about dataset. MM Detection V2.0 already support VOC, WIDER FACE, COCO and Cityscapes Dataset. \n14.4 Modify training schedule \nThe finetuning hyper parameters vary from the default schedule. It usually requires smaller learning rate and less training epochs\n\n \n# optimizer\n\n # lr is set for a batch size of 8 optimizer  $=$   dict ( type  $\\equiv^{1}$  SGD ' ,   $\\scriptstyle1r=0\\,.\\,0\\,1$  , momentum  $\\scriptstyle1=0\\,.\\,9$  , weight decay = 0.0001 ) optimizer config  $=$   dict (grad_clip  $\\risingdotseq$  None ) # learning policy lr_config  $=$   dict ( policy  $\\leftleftarrows$  ' step ' , warmup  $\\acute{=}$  linear ' , warm up it ers  $\\scriptstyle{\\bullet=500}$  , warm up rat i $\\mathfrak{o}{=}\\mathbb{0}\\cdot\\mathbb{0}\\mathbb{0}1$ ,step  $\\leftrightharpoons$  [ 7 ]) # the max_epochs and step in lr_config need specifically tuned for the customized dataset runner  $=$   dict (max_epochs  $\\mathbf{\\varepsilon}\\mathbf{=}\\mathbf{8}.$  ) log_config  $=$   dict (interval  $\\scriptstyle\\cdot=100$  ) "}
{"page": 100, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_100.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n14.5 Use pre-irained model\n\nTo use the pre-trained model, the new config add the link of pre-trained models in the load_from. The users might\nneed to download the model weights before training to avoid the download time during training.\n\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_\n\n«caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.\n+408__segm_mAP-0. 37_20200504_163245-42aa3d00.pth'\n\n# noqga\n\n14.5. Use pre-trained model 93\n", "vlm_text": "14.5 Use pre-trained model \nTo use the pre-trained model, the new config add the link of pre-trained models in the  load_from . The users might need to download the model weights before training to avoid the download time during training. \nload_from    $=$   ' https://download.openmmlab.com/mm detection/v2.0/mask_rcnn/mask r cnn r 50\n\n  $\\hookrightarrow$  caff e fp n m strain-poly 3 x coco/mask r cnn r 50 caff e fp n m strain-poly 3 x coco b box mAP-0.\n\n →\n\n  $\\hookrightarrow408,$  408__segm_mAP-0.37_20200504_163245-42aa3d00.pth ' # noqa\n\n → "}
{"page": 101, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_101.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n94 Chapter 14. Tutorial 7: Finetuning Models\n", "vlm_text": "MMDetection, Release 2.18.0\n\n94 Chapter 14. Tutorial 7: Finetuning Models\n"}
{"page": 102, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_102.jpg", "ocr_text": "CHAPTER\nFIFTEEN\n\nTUTORIAL 8: PYTORCH TO ONNX (EXPERIMENTAL)\n\n© Tutorial 8: Pytorch to ONNX (Experimental)\n\nHow to convert models from Pytorch to ONNX\n* Prerequisite\n* Usage\n\n* Description of all arguments\n\nHow to evaluate the exported models\n* Prerequisite\n* Usage\n+ Description of all arguments\n\n* Results and Models\n\nList of supported models exportable to ONNX\n\nThe Parameters of Non-Maximum Suppression in ONNX Export\n\nReminders\n\nFAQs\n\n15.1 How to convert models from Pytorch to ONNX\n\n15.1.1 Prerequisite\n\n1. Install the prerequisites following get_started.md/Prepare environment.\n\n2. Build custom operators for ONNX Runtime and install MMCV manually following How to build custom oper-\nators for ONNX Runtime\n\n3. Install MMdetection manually following steps 2-3 in get_started.md/Install MMdetection.\n\n95\n", "vlm_text": "TUTORIAL 8: PYTORCH TO ONNX (EXPERIMENTAL) \n•  Tutorial 8: Pytorch to ONNX (Experimental) –  How to convert models from Pytorch to ONNX ∗ Prerequisite ∗ Usage ∗ Description of all arguments –  How to evaluate the exported models ∗ Prerequisite ∗ Usage ∗ Description of all arguments ∗ Results and Models –  List of supported models exportable to ONNX –  The Parameters of Non-Maximum Suppression in ONNX Export –  Reminders –  FAQs \n15.1 How to convert models from Pytorch to ONNX \n15.1.1 Prerequisite \n1. Install the prerequisites following  get started.md/Prepare environment . 2. Build custom operators for ONNX Runtime and install MMCV manually following  How to build custom oper- ators for ONNX Runtime 3. Install MM detection manually following steps 2-3 in  get started.md/Install MM detection . "}
{"page": 103, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_103.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n15.1.2 Usage\n\npython tools/deployment/pytorch2onnx.py \\\n${CONFIG_FILE} \\\n${CHECKPOINT_FILE} \\\n--output-file §${OUTPUT_FILE} \\\n--input-img ${INPUT_IMAGE_PATH} \\\n--shape §{IMAGE_SHAPE} \\\n--test-img §{TEST_IMAGE_PATH} \\\n--opset-version ${OPSET_VERSION} \\\n--cfg-options ${CFG_OPTIONS}\n--dynamic-export \\\n--show \\\n--verify \\\n--simplify \\\n\n15.1.3 Description of all arguments\n\n* config: The path of a model config file.\n* checkpoint : The path of a model checkpoint file.\n* --output-file: The path of output ONNX model. If not specified, it will be set to tmp. onnx.\n\n* --input-img: The path of an input image for tracing and conversion. By default, it will be set to tests/data/\ncolor. jpg.\n\n« --shape: The height and width of input tensor to the model. If not specified, it will be set to 800 1216.\n\n* --test-img: The path of an image to verify the exported ONNX model. By default, it will be set to None,\nmeaning it will use --input-img for verification.\n\n* --opset-version : The opset version of ONNX. If not specified, it will be set to 11.\n\n« --dynamic-export: Determines whether to export ONNX model with dynamic input and output shapes. If\nnot specified, it will be set to False.\n\n* --show: Determines whether to print the architecture of the exported model and whether to show detection\noutputs when --veri fy is set to True. If not specified, it will be set to False.\n\n* --verify: Determines whether to verify the correctness of an exported model. If not specified, it will be set to\nFalse.\n\n« --simplify: Determines whether to simplify the exported ONNX model. If not specified, it will be set to\nFalse.\n\n* --cfg-options: Override some settings in the used config file, the key-value pair in xxx=yyy format will be\nmerged into config file.\n\n* --skip-postprocess: Determines whether export model without post process. If not specified, it will be set\nto False. Notice: This is an experimental option. Only work for some single stage models. Users need to\nimplement the post-process by themselves. We do not guarantee the correctness of the exported model.\n\nExample:\n\npython tools/deployment/pytorch2onnx.py \\\nconfigs/yolo/yolov3_d53_mstrain-608_273e_coco.py \\\ncheckpoints/yolo/yolov3_d53_mstrain-608_273e_coco.pth \\\n\n(continues on next page)\n\n96 Chapter 15. Tutorial 8: Pytorch to ONNX (Experimental)\n\n", "vlm_text": "15.1.2 Usage \npython tools/deployment/py torch 2 on nx.py  \\ \\${ CONFIG FILE }  \\ \\${ CHECKPOINT FILE }  \\ --output-file  $\\mathcal{S}$ {OUTPUT FILE} \\--input-img \\${INPUT IMAGE PATH} \\--shape    $\\mathcal{S}$  { IMAGE SHAPE }  \\ --test-img \\${TEST IMAGE PATH} \\--opset-version  \\${ OP SET VERSION }  \\ --cfg-options  \\${ CF G OPTIONS } --dynamic-export  \\ --show  \\ --verify  \\ --simplify  \\ \n15.1.3 Description of all arguments \n•  config  $:$   The path of a model config file. •  checkpoint  $:$   The path of a model checkpoint file. •  --output-file : The path of output ONNX model. If not specified, it will be set to  tmp.onnx . •  --input-img : The path of an input image for tracing and conversion. By default, it will be set to  tests/data/ color.jpg . •  --shape : The height and width of input tensor to the model. If not specified, it will be set to  800 1216 . •  --test-img  $:$   The path of an image to verify the exported ONNX model. By default, it will be set to  None , meaning it will use  --input-img  for verification. •  --opset-version  $:$   The opset version of ONNX. If not specified, it will be set to  11 . •  --dynamic-export : Determines whether to export ONNX model with dynamic input and output shapes. If not specified, it will be set to  False . •  --show : Determines whether to print the architecture of the exported model and whether to show detection outputs when  --verify  is set to  True . If not specified, it will be set to  False . •  --verify : Determines whether to verify the correctness of an exported model. If not specified, it will be set to False . •  --simplify : Determines whether to simplify the exported ONNX model. If not specified, it will be set to False . •  --cfg-options : Override some settings in the used config file, the key-value pair in  xxx=yyy  format will be merged into config file. •  --skip-post process : Determines whether export model without post process. If not specified, it will be set to  False . Notice: This is an experimental option. Only work for some single stage models. Users need to implement the post-process by themselves. We do not guarantee the correctness of the exported model. \nExample: \nThe image shows a command likely used for converting a PyTorch model to the ONNX format, using a Python script. Here's a breakdown of the command:\n\n```bash\npython tools/deployment/pytorch2onnx.py \\\n    configs/yolo/yolov3_d53_mstrain-608_273e_coco.py \\\n    checkpoints/yolo/yolov3_d53_mstrain-608_273e_coco.pth \\\n```\n\n- `python tools/deployment/pytorch2onnx.py`: This is the script responsible for the conversion.\n- `configs/yolo/yolov3_d53_mstrain-608_273e_coco.py`: This refers to the configuration file for the YOLOv3 model.\n- `checkpoints/yolo/yolov3_d53_mstrain-608_273e_coco.pth`: This is the file containing the trained weights of the model. \n\nThe backslashes (`\\`) indicate that the command continues on the next line."}
{"page": 104, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_104.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n--output-file checkpoints/yolo/yolov3_d53_mstrain-608_273e_coco.onnx \\\n--input-img demo/demo.jpg \\\n--test-img tests/data/color.jpg \\\n--shape 608 608 \\\n--show \\\n--verify \\\n--dynamic-export \\\n--cfg-options \\\nmodel.test_cfg.deploy_nms_pre=-1 \\\n\n15.2 How to evaluate the exported models\n\nWe prepare a tool tools/deplopyment/test.py to evaluate ONNX models with ONNXRuntime and TensorRT.\n\n15.2.1 Prerequisite\n\n¢ Install onnx and onnxruntime (CPU version)\n\npip install onnx onnxruntime==1.5.1\n\n¢ If you want to run the model on GPU, please remove the CPU version before using the GPU version.\n\npip uninstall onnxruntime\npip install onnxruntime-gpu\n\nNote: onnxruntime-gpu is version-dependent on CUDA and CUDNN, please ensure that your environment meets\nthe requirements.\n\n* Build custom operators for ONNX Runtime following How to build custom operators for ONNX Runtime\n\nInstall TensorRT by referring to How to build TensorRT plugins in MMCV (optional)\n\n15.2.2 Usage\n\npython tools/deployment/test.py \\\n${CONFIG_FILE} \\\n${MODEL_FILE} \\\n--out ${OUTPUT_FILE} \\\n--backend §{BACKEND} \\\n--format-only ${FORMAT_ONLY} \\\n--eval ${EVALUATION_METRICS} \\\n--show-dir §{SHOW_DIRECTORY} \\\n----show-score-thr $§{SHOW_SCORE_THRESHOLD} \\,\n----cfg-options ${CFG_OPTIONS} \\\n----eval-options §{EVALUATION_OPTIONS} \\\n\n15.2. How to evaluate the exported models 97\n\n", "vlm_text": "--output-file checkpoints/yolo/yo lov 3 d 53 m strain-608 273 e coco.onnx  \\ --input-img demo/demo.jpg  \\ --test-img tests/data/color.jpg  \\ --shape  608 608  \\ --show  \\ --verify  \\ --dynamic-export  \\ --cfg-options  \\ model.test_cfg.deploy nm s pre  $_{=-1}$   \\ \n15.2 How to evaluate the exported models \nWe prepare a tool  tools/de plop y ment/test.py  to evaluate ONNX models with ON NX Runtime and TensorRT. \n15.2.1 Prerequisite \n• Install onnx and on nx runtime (CPU version) \nThe table contains instructions for installing ONNX and ONNX Runtime Python packages:\n\n1. To install ONNX with a specific version of ONNX Runtime:\n   ```\n   pip install onnx onnxruntime==1.5.1\n   ```\n\n2. Note on running the model on GPU:\n   - You must uninstall the CPU version of ONNX Runtime before installing the GPU version.\n\n3. Commands to switch from CPU to GPU version:\n   ```\n   pip uninstall onnxruntime\n   pip install onnxruntime-gpu\n   ```\nNote: on nx runtime-gpu is version-dependent on CUDA and CUDNN, please ensure that your environment meets the requirements. \n• Build custom operators for ONNX Runtime following  How to build custom operators for ONNX Runtime • Install TensorRT by referring to  How to build TensorRT plugins in MMCV  (optional) \n\n15.2.2 Usage \nThe image shows a command line input for running a Python script, specifically for testing deployments with a set of configurable parameters. The command is structured with placeholders that are meant to be replaced with actual values:\n\n- `python tools/deployment/test.py \\`\n- `${CONFIG_FILE}`: configuration file\n- `${MODEL_FILE}`: model file\n- `--out ${OUTPUT_FILE}`: output file\n- `--backend ${BACKEND}`: backend option\n- `--format-only ${FORMAT_ONLY}`: format-only option\n- `--eval ${EVALUATION_METRICS}`: evaluation metrics\n- `--show-dir ${SHOW_DIRECTORY}`: show directory\n- `----show-score-thr ${SHOW_SCORE_THRESHOLD}`: show score threshold\n- `----cfg-options ${CFG_OPTIONS}`: configuration options\n- `----eval-options ${EVALUATION_OPTIONS}`: evaluation options\n\nEach of these placeholders is intended to be filled with specific information related to the deployment and testing process."}
{"page": 105, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_105.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n15.2.3 Description of all arguments\n\n* config: The path of a model config file.\n\n* model: The path of an input model file.\n\n* --out: The path of output result file in pickle format.\n\n* --backend: Backend for input model to run and should be onnxruntime or tensorrt.\n\n* --format-only : Format the output results without perform evaluation. It is useful when you want to format\nthe result to a specific format and submit it to the test server. If not specified, it will be set to False.\n\n* --eval: Evaluation metrics, which depends on the dataset, e.g., “bbox”, “segm’, “proposal” for COCO, and\n“mAP”, “recall” for PASCAL VOC.\n\n« --show-dir: Directory where painted images will be saved\n* --show-score-thr: Score threshold. Default is set to 0.3.\n\n* --cfg-options: Override some settings in the used config file, the key-value pair in xxx=yyy format will be\nmerged into config file.\n\n* --eval-options: Custom options for evaluation, the key-value pair in xxx=yyy format will be kwargs for\ndataset.evaluate() function\n\nNotes:\n\n* If the deployed backend platform is TensorRT, please add environment variables before running the file:\n\nexport ONNX_BACKEND=MMCVTensorRT\n\n* If you want to use the --dynamic-export parameter in the TensorRT backend to export ONNX, please remove\nthe --simp1lify parameter, and vice versa.\n\n15.2.4 Results and Models\n\nNotes:\n\n« All ONNX models are evaluated with dynamic shape on coco dataset and images are preprocessed according to\nthe original config file. Note that CornerNet is evaluated without test-time flip, since currently only single-scale\nevaluation is supported with ONNX Runtime.\n\n* Mask AP of Mask R-CNN drops by 1% for ONNXRuntime. The main reason is that the predicted masks are\ndirectly interpolated to original image in PyTorch, while they are at first interpolated to the preprocessed input\nimage of the model and then to original image in other backend.\n\n15.3 List of supported models exportable to ONNX\n\nThe table below lists the models that are guaranteed to be exportable to ONNX and runnable in ONNX Runtime.\nNotes:\n¢ Minimum required version of MMCV is 1.3.5\n\n* All models above are tested with Pytorch==1.6.0 and onnxruntime==1.5.1, except for CornerNet. For more\ndetails about the torch version when exporting CornerNet to ONNX, which involves mmcv: : cummax, please\nrefer to the Known Issues in mmcv.\n\n98 Chapter 15. Tutorial 8: Pytorch to ONNX (Experimental)\n", "vlm_text": "15.2.3 Description of all arguments \n•  config : The path of a model config file. •  model : The path of an input model file. •  --out : The path of output result file in pickle format. •  --backend : Backend for input model to run and should be  on nx runtime  or  tensorrt . •  --format-only  $:$   Format the output results without perform evaluation. It is useful when you want to format the result to a specific format and submit it to the test server. If not specified, it will be set to  False . •  --eval : Evaluation metrics, which depends on the dataset, e.g., “bbox”, “segm”, “proposal” for COCO, and “mAP”, “recall” for PASCAL VOC. •  --show-dir : Directory where painted images will be saved •  --show-score-thr : Score threshold. Default is set to  0.3 . •  --cfg-options : Override some settings in the used config file, the key-value pair in  xxx=yyy  format will be merged into config file. •  --eval-options : Custom options for evaluation, the key-value pair in  xxx  $=$  yyy  format will be kwargs for dataset.evaluate()  function \n\n• If the deployed backend platform is TensorRT, please add environment variables before running the file: \nexport  ON NX BACKEND  $^{1=}$  MM CV Tensor RT \n• If you want to use the  --dynamic-export  parameter in the TensorRT backend to export ONNX, please remove the  --simplify  parameter, and vice versa. \n15.2.4 Results and Models \nNotes: \n• All ONNX models are evaluated with dynamic shape on coco dataset and images are pre processed according to the original config file. Note that CornerNet is evaluated without test-time flip, since currently only single-scale evaluation is supported with ONNX Runtime. • Mask AP of Mask R-CNN drops by  $1\\%$   for ON NX Runtime. The main reason is that the predicted masks are directly interpolated to original image in PyTorch, while they are at first interpolated to the pre processed input image of the model and then to original image in other backend. \n15.3 List of supported models exportable to ONNX \nThe table below lists the models that are guaranteed to be exportable to ONNX and runnable in ONNX Runtime. Notes: \n• Minimum required version of MMCV is  1.3.5 •  All models above are tested with Pytorch  $==l.6.0$   and onnxruntim  $\\scriptstyle{\\==I.5.I}$  , except for CornerNet. For more details about the torch version when exporting CornerNet to ONNX, which involves  mmcv::cummax , please refer to the  Known Issues  in mmcv. "}
{"page": 106, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_106.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Though supported, it is not recommended to use batch inference in onnxruntime for DETR, because there is\nhuge performance gap between ONNX and torch model (e.g. 33.5 vs 39.9 mAP on COCO for onnxruntime and\ntorch respectively, with a batch size 2). The main reason for the gap is that these is non-negligible effect on\nthe predicted regressions during batch inference for ONNX, since the predicted coordinates is normalized by\nimg_shape (without padding) and should be converted to absolute format, but img_shape is not dynamically\ntraceable thus the padded img_shape_for_onnx is used.\n\n* Currently only _ single-scale evaluation is supported with ONNX _ Runtime, also\nmmcv: :SoftNonMaxSuppression is only supported for single image by now.\n\n15.4 The Parameters of Non-Maximum Suppression in ONNX Export\n\nIn the process of exporting the ONNX model, we set some parameters for the NMS op to control the number of output\nbounding boxes. The following will introduce the parameter setting of the NMS op in the supported models. You can\nset these parameters through --cfg-options.\n\n* nms_pre: The number of boxes before NMS. The default setting is 1000.\n\n* deploy_nms_pre: The number of boxes before NMS when exporting to ONNX model. The default setting is\n0.\n\n* max_per_img: The number of boxes to be kept after NMS. The default setting is 100.\n\n* max_output_boxes_per_class: Maximum number of output boxes per class of NMS. The default setting is\n200.\n\n15.5 Reminders\n\n* When the input model has custom op such as RoIAlign and if you want to verify the exported ONNX model,\nyou may have to build mmcv with ONNXRuntime from source.\n\n* mmcv.onnx.simplify feature is based on onnx-simplifier. If you want to try it, please refer to onnx in mmcv\nand onnxruntime op in mmcv for more information.\n\n¢ If you meet any problem with the listed models above, please create an issue and it would be taken care of soon.\nFor models not included in the list, please try to dig a little deeper and debug a little bit more and hopefully solve\nthem by yourself.\n\n* Because this feature is experimental and may change fast, please always try with the latest mmcv and mmdetecion.\n\n15.6 FAQs\n\n* None\n\n15.4. The Parameters of Non-Maximum Suppression in ONNX Export 99\n", "vlm_text": "• Though supported, it is  not recommended  to use batch inference in on nx runtime for  DETR , because there is huge performance gap between ONNX and torch model (e.g. 33.5 vs  $39.9\\;\\mathrm{mAP}$   on COCO for on nx runtime and torch respectively, with a batch size 2). The main reason for the gap is that these is non-negligible effect on the predicted regressions during batch inference for ONNX, since the predicted coordinates is normalized by img_shape  (without padding) and should be converted to absolute format, but  img_shape  is not dynamically traceable thus the padded  img shape for on nx  is used. • Currently only single-scale evaluation is supported with ONNX Runtime, also mmcv::Soft Non Max Suppression  is only supported for single image by now. \n15.4 The Parameters of Non-Maximum Suppression in ONNX Export \nIn the process of exporting the ONNX model, we set some parameters for the NMS op to control the number of output bounding boxes. The following will introduce the parameter setting of the NMS op in the supported models. You can set these parameters through  --cfg-options . \n•  nms_pre : The number of boxes before NMS. The default setting is  1000 . •  deploy nm s pre : The number of boxes before NMS when exporting to ONNX model. The default setting is 0 . •  max per img : The number of boxes to be kept after NMS. The default setting is  100 . •  max output boxes per class : Maximum number of output boxes per class of NMS. The default setting is 200 . \n15.5 Reminders \n• When the input model has custom op such as  RoIAlign  and if you want to verify the exported ONNX model, you may have to build  mmcv  with  ON NX Runtime  from source. •  mmcv.onnx.simplify  feature is based on  onnx-simplifier . If you want to try it, please refer to  onnx in  mmcv and  on nx runtime op in  mmcv  for more information. • If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, please try to dig a little deeper and debug a little bit more and hopefully solve them by yourself. • Because this feature is experimental and may change fast, please always try with the latest  mmcv  and  mmdetecion . \n15.6 FAQs \n• None "}
{"page": 107, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_107.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n100 Chapter 15. Tutorial 8: Pytorch to ONNX (Experimental)\n", "vlm_text": "MMDetection, Release 2.18.0\n\n100 Chapter 15. Tutorial 8: Pytorch to ONNX (Experimental)\n"}
{"page": 108, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_108.jpg", "ocr_text": "CHAPTER\nSIXTEEN\n\nTUTORIAL 9: ONNX TO TENSORRT (EXPERIMENTAL)\n\n* Tutorial 9: ONNX to TensorRT (Experimental)\n\nHow to convert models from ONNX to TensorRT\n* Prerequisite\n\n* Usage\n\nHow to evaluate the exported models\n\nList of supported models convertible to TensorRT\n\nReminders\n\n— FAQs\n\n16.1 How to convert models from ONNX to TensorRT\n\n16.1.1 Prerequisite\n\n1. Please refer to get_started.md for installation of MMCV and MMDetection from source.\n\n2. Please refer to ONNXRuntime in mmcv and TensorRT plugin in mmcy to install mmcv-full with ONNXRun-\ntime custom ops and TensorRT plugins.\n\n3. Use our tool pytorch2onnx to convert the model from PyTorch to ONNX.\n\n16.1.2 Usage\n\npython tools/deployment/onnx2tensorrt.py \\\nS{CONFIG} \\\n${MODEL} \\\n--trt-file §{TRT_FILE} \\\n--input-img ${INPUT_IMAGE_PATH} \\\n--shape $§{INPUT_IMAGE_SHAPE} \\\n--min-shape ${MIN_IMAGE_SHAPE} \\\n--max-shape $§{MAX_IMAGE_SHAPE} \\\n--workspace-size {WORKSPACE_SIZE} \\\n--show \\\n--verify \\\n\nDescription of all arguments:\n\n101\n\n", "vlm_text": "TUTORIAL 9: ONNX TO TENSORRT (EXPERIMENTAL) \n•  Tutorial 9: ONNX to TensorRT (Experimental) –  How to convert models from ONNX to TensorRT ∗ Prerequisite ∗ Usage –  How to evaluate the exported models –  List of supported models convertible to TensorRT –  Reminders –  FAQs \n16.1 How to convert models from ONNX to TensorRT \n16.1.1 Prerequisite \n1. Please refer to  get started.md  for installation of MMCV and MM Detection from source. 2. Please refer to  ON NX Runtime in mmcv  and  TensorRT plugin in mmcv  to install  mmcv-full  with ONNXRun- time custom ops and TensorRT plugins. 3. Use our tool  py torch 2 on nx  to convert the model from PyTorch to ONNX. \n16.1.2 Usage \nThe provided image shows a block of code rather than a traditional table. The code is a command-line script used to run a Python script, which likely facilitates the conversion of an ONNX model to a TensorRT model using various parameters or flags. Here's a breakdown of the command:\n\n- `python tools/deployment/onnx2tensorrt.py`: This initial part indicates that a Python script located at `tools/deployment/onnx2tensorrt.py` is being executed.\n- `${CONFIG}`: Represents a placeholder for a configuration file or argument to be replaced by the user.\n- `${MODEL}`: Placeholder for the model file or argument.\n- `--trt-file ${TRT_FILE}`: Specifies the output filename for the TensorRT model.\n- `--input-img ${INPUT_IMAGE_PATH}`: Specifies the path to an input image.\n- `--shape ${INPUT_IMAGE_SHAPE}`: Indicates the shape of the input image.\n- `--min-shape ${MIN_IMAGE_SHAPE}`: Minimum shape of the input image.\n- `--max-shape ${MAX_IMAGE_SHAPE}`: Maximum shape of the input image.\n- `--workspace-size {WORKSPACE_SIZE}`: Specifies the workspace size for TensorRT engine building.\n- `--show`: An option that, when invoked, may display additional information or outcomes.\n- `--verify`: An option probably used to verify the conversion or output.\n\nEach line ends with a backslash (`\\`), which in many programming and scripting languages indicates that the command continues onto the next line. This format helps in making complex command-line inputs more readable."}
{"page": 109, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_109.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* config: The path of a model config file.\n* model : The path of an ONNX model file.\n* --trt-file: The Path of output TensorRT engine file. If not specified, it will be set to tmp. trt.\n\n* --input-img : The path of an input image for tracing and conversion. By default, it will be set to demo/demo.\njpg.\n« --shape: The height and width of model input. If not specified, it will be set to 400 600.\n\n* --min-shape: The minimum height and width of model input. If not specified, it will be set to the same as\n--shape.\n\n* --max-shape: The maximum height and width of model input. If not specified, it will be set to the same as\n--shape.\n\n* --workspace-size : The required GPU workspace size in GiB to build TensorRT engine. If not specified, it\nwill be set to 1 GiB.\n\n* --show: Determines whether to show the outputs of the model. If not specified, it will be set to False.\n\n* --verify: Determines whether to verify the correctness of models between ONNXRuntime and TensorRT. If\nnot specified, it will be set to False.\n\n* --verbose: Determines whether to print logging messages. It’s useful for debugging. If not specified, it will\nbe set to False.\n\nExample:\n\npython tools/deployment/onnx2tensorrt.py \\\nconfigs/retinanet/retinanet_r50_fpn_1x_coco.py \\\ncheckpoints/retinanet_r50_fpn_1x_coco.onnx \\\n--trt-file checkpoints/retinanet_r50_fpn_1x_coco.trt \\\n--input-img demo/demo.jpg \\\n--shape 400 600 \\\n--show \\\n--verify \\\n\n16.2 How to evaluate the exported models\n\nWe prepare a tool tools/deplopyment/test . py to evaluate TensorRT models.\nPlease refer to following links for more information.\n* how-to-evaluate-the-exported-models\n\n* results-and-models\n\n102 Chapter 16. Tutorial 9: ONNX to TensorRT (Experimental)\n", "vlm_text": "•  config  $:$   The path of a model config file. •  model  $:$   The path of an ONNX model file. •  --trt-file : The Path of output TensorRT engine file. If not specified, it will be set to  tmp.trt . •  --input-img  $:$   The path of an input image for tracing and conversion. By default, it will be set to  demo/demo. jpg . •  --shape : The height and width of model input. If not specified, it will be set to  400 600 . •  --min-shape : The minimum height and width of model input. If not specified, it will be set to the same as --shape . •  --max-shape : The maximum height and width of model input. If not specified, it will be set to the same as --shape . •  --workspace-size  : The required GPU workspace size in GiB to build TensorRT engine. If not specified, it will be set to  1  GiB. •  --show : Determines whether to show the outputs of the model. If not specified, it will be set to  False . •  --verify : Determines whether to verify the correctness of models between ON NX Runtime and TensorRT. If not specified, it will be set to  False . •  --verbose : Determines whether to print logging messages. It’s useful for debugging. If not specified, it will be set to  False . \nExample: \npython tools/deployment/on nx 2 tensor rt.py  \\ configs/retinanet/retina net r 50 fp n 1 x coco.py  \\ checkpoints/retina net r 50 fp n 1 x coco.onnx  \\ --trt-file checkpoints/retina net r 50 fp n 1 x coco.trt  \\ --input-img demo/demo.jpg  \\ --shape  400 600  \\ --show  \\ --verify  \\ \n16.2 How to evaluate the exported models \nWe prepare a tool  tools/de plop y ment/test.py  to evaluate TensorRT models. \nPlease refer to following links for more information. \n•  how-to-evaluate-the-exported-models •  results-and-models "}
{"page": 110, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_110.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n16.3 List of supported models convertible to TensorRT\n\nThe table below lists the models that are guaranteed to be convertible to TensorRT.\nNotes:\n\n¢ All models above are tested with Pytorch==1.6.0, onnx==1.7.0 and TensorRT-7.2.1.6.Ubuntu-16.04.x86_64-\ngnu.cuda-10.2.cudnn8.0\n\n16.4 Reminders\n\n¢ If you meet any problem with the listed models above, please create an issue and it would be taken care of soon.\nFor models not included in the list, we may not provide much help here due to the limited resources. Please try\nto dig a little deeper and debug by yourself.\n\n* Because this feature is experimental and may change fast, please always try with the latest mmcv and mmdetecion.\n\n16.5 FAQs\n\n* None\n\n16.3. List of supported models convertible to TensorRT 103\n", "vlm_text": "16.3 List of supported models convertible to TensorRT \nThe table below lists the models that are guaranteed to be convertible to TensorRT. \nNotes: \n•  All models above are tested with Pytorch  $==l.6.0,$  ,   $o n n x{=}{l.7.0}$   and TensorRT-7.2.1.6.Ubuntu-16.04.x86_64- gnu.cuda-10.2.cudnn8.0 \n16.4 Reminders \n• If you meet any problem with the listed models above, please create an issue and it would be taken care of soon. For models not included in the list, we may not provide much help here due to the limited resources. Please try to dig a little deeper and debug by yourself. • Because this feature is experimental and may change fast, please always try with the latest  mmcv  and  mmdetecion . \n16.5 FAQs \n• None "}
{"page": 111, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_111.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n104 Chapter 16. Tutorial 9: ONNX to TensorRT (Experimental)\n", "vlm_text": "MMDetection, Release 2.18.0\n\n104 Chapter 16. Tutorial 9: ONNX to TensorRT (Experimental)\n"}
{"page": 112, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_112.jpg", "ocr_text": "CHAPTER\nSEVENTEEN\n\nTUTORIAL 10: WEIGHT INITIALIZATION\n\nDuring training, a proper initialization strategy is beneficial to speeding up the training or obtaining a higher perfor-\nmance. MMCV provide some commonly used methods for initializing modules like nn. Conv2d. Model initialization\nin MMdetection mainly uses init_cfg. Users can initialize models with following two steps:\n\n1. Define init_cfg for a model or its components in model_cfg, but init_cfg of children components have\nhigher priority and will override init_cfg of parents modules.\n\n2. Build model as usual, but call model .init_weights() method explicitly, and model parameters will be ini-\ntialized as configuration.\n\nThe high-level workflow of initialization in MMdetection is :\n\nmodel_cfg(init_cfg) -> build_from_cfg -> model -> init_weight() -> initialize(self, self.init_cfg) -> children’s\ninit_weight()\n\n17.1 Description\n\nIt is dict or list{dict], and contains the following keys and values:\n* type (str), containing the initializer name in INTIALIZERS, and followed by arguments of the initializer.\n\n* layer (str or list[str]), containing the names of basiclayers in Pytorch or MMCV with learnable parameters that\nwill be initialized, e.g. 'Conv2d','DeformConv2d'.\n\n* override (dict or list[dict]), containing the sub-modules that not inherit from BaseModule and whose initializa-\ntion configuration is different from other layers’ which are in 'layer' key. Initializer defined in type will work\nfor all layers defined in layer, so if sub-modules are not derived Classes of BaseModule but can be initialized\nas same ways of layers in layer, it does not need to use override. override contains:\n\n— type followed by arguments of initializer;\n\n— name to indicate sub-module which will be initialized.\n\n17.2 Initialize parameters\n\nInherit a new model from mmcv. runner .BaseModule or mmdet .models Here we show an example of FooModel.\n\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule\n\nclass FooModel (BaseModule)\ndef __init__(self,\n\n(continues on next page)\n\n105\n", "vlm_text": "TUTORIAL 10: WEIGHT INITIALIZATION\nDuring training, a proper initialization strategy is beneficial to speeding up the training or obtaining a higher perfor- mance.  MMCV  provide some commonly used methods for initializing modules like  nn.Conv2d . Model initialization in MM detection mainly uses  init_cfg . Users can initialize models with following two steps: \n1. Define  init_cfg  for a model or its components in  model_cfg , but  init_cfg  of children components have higher priority and will override  init_cfg  of parents modules. 2. Build model as usual, but call  model.in it weights()  method explicitly, and model parameters will be ini- tialized as configuration. \nThe high-level workflow of initialization in MM detection is : \nmodel_cfg(init_cfg)   $->$   build from cf g   $->$   model   $->$   in it weight()  $->$   initialize(self, self.init_cfg)  $->$   children’s in it weight()\n17.1 Description \nIt is dict or list[dict], and contains the following keys and values: \n•  type  (str), containing the initialize r name in  INTI ALIZ ERS , and followed by arguments of the initialize r. •  layer  (str or list[str]), containing the names of basic layers in Pytorch or MMCV with learnable parameters that will be initialized, e.g.  ' Conv2d ' , ' Deform Con v 2 d ' . •  override  (dict or list[dict]), containing the sub-modules that not inherit from BaseModule and whose initializa- tion configuration is different from other layers’ which are in  ' layer '  key. Initialize r defined in  type  will work for all layers defined in  layer , so if sub-modules are not derived Classes of  BaseModule  but can be initialized as same ways of layers in  layer , it does not need to use  override .  override  contains: –  type  followed by arguments of initialize r; –  name  to indicate sub-module which will be initialized. \n17.2 Initialize parameters \nInherit a new model from  mmcv.runner.BaseModule  or  mmdet.models  Here we show an example of FooModel. "}
{"page": 113, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_113.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nargl,\narg2,\ninit_cfg=None) :\nsuper(FooModel, self).__init__(Cinit_cfg)\n\n* Initialize model by using init_cfg directly in code\n\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule\n# or directly inherit mmdet models\n\nclass FooModel (BaseModule)\ndef __init__(self,\nargl,\narg2,\ninit_cfg=XXX):\nsuper(FooModel, self).__init__(Cinit_cfg)\n\nInitialize model by using init_cfg directly in mmcv.Sequential or mmcv.ModuleList code\n\nfrom mmcv.runner import BaseModule, ModuleList\n\nclass FooModel (BaseModule)\ndef __init__(self,\nargl,\narg2,\ninit_cfg=None) :\nsuper(FooModel, self).__init__(Cinit_cfg)\n\nself.conv1l = ModuleList (init_cfg=XXX)\n\nInitialize model by using init_cfg in config file\n\nmodel = dict(\n\nmodel = dict(\ntype='FooModel',\narg1=XXX,\narg2=XXxX,\ninit_cfg=XXX),\n\n106 Chapter 17. Tutorial 10: Weight initialization\n\n", "vlm_text": "The image you provided does not contain a table. Instead, it shows a snippet of Python code. The code appears to be part of a class definition where the `__init__` method is being defined. The method takes arguments `arg1`, `arg2`, and `init_cfg` (with a default value of `None`). Within the `__init__` method, the `super()` function is used to call the `__init__` method of the superclass, passing `init_cfg` to it. The class name appears to be `FooModel`.\n• Initialize model by using  init_cfg  directly in code \nThe image shows a snippet of Python code. Here's a breakdown:\n\n1. **Imports**:\n   - `torch.nn as nn`: Imports the neural network module from PyTorch.\n   - `from mmcv.runner import BaseModule`: Imports `BaseModule` from the `mmcv.runner` package.\n   \n2. **Comment**:\n   - `# or directly inherit mmdet models`: This comment suggests an alternative to inheriting the `BaseModule`.\n\n3. **Class Definition**:\n   - `class FooModel(BaseModule)`: Defines a class named `FooModel` that inherits from `BaseModule`.\n\n4. **Initialization Method**:\n   - `def __init__(self, arg1, arg2, init_cfg=XXX):`: Constructor method for initializing the `FooModel` class.\n   - `super(FooModel, self).__init__(init_cfg)`: Calls the constructor of the parent `BaseModule` class with the `init_cfg` parameter.\n\nThe ellipsis (`...`) at the end indicates that more code would follow.\n• Initialize model by using  init_cfg  directly in  mmcv.Sequential  or  mmcv.ModuleList  code \nfrom  mmcv.runner  import  BaseModule, ModuleList class FooModel(BaseModule)def  __init__ ( self , arg1, arg2, init_cfg = None ): super (FooModel,  self ) . __init__ (init_cfg) ... self . conv1  $=$   ModuleList(init_cfg  $=$  XXX) \n• Initialize model by using  init_cfg  in config file \nThe image shows a snippet of code, likely in Python, that defines a nested dictionary configuration for a model. Here are the key components:\n\n- The outer dictionary is assigned to the variable `model`.\n- Inside the outer dictionary, there is another dictionary also assigned to `model` with fields:\n  - `type='FooModel'`: Specifies the model type as \"FooModel\".\n  - `arg1=XXX`: A placeholder for the first argument.\n  - `arg2=XXX`: A placeholder for the second argument.\n  - `init_cfg=XXX`: A placeholder for the model's initialization configuration. \n\nThe `XXX` placeholders imply that specific values need to be filled in."}
{"page": 114, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_114.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n17.3 Usage of init_cfg\n\n1. Initialize model by layer key\nIf we only define layer, it just initialize the layer in layer key.\n\nNOTE: Value of layer key is the class name with attributes weights and bias of Pytorch, (so such as\nMultiheadAttention layer is not supported).\n\n¢ Define layer key for initializing module with same configuration.\n\ninit_cfg = dict(type='Constant', layer=['Convid', 'Conv2d', 'Linear'], val=1)\n# initialize whole module with same configuration\n\n¢ Define layer key for initializing layer with different configurations.\n\ninit_cfg = [dict(type='Constant', layer='Convid', val=1),\ndict(type='Constant', layer='Conv2d', val=2),\ndict(type='Constant', layer='Linear', val=3)]\n\n# nn.Convild will be initialized with dict(type='Constant', val=1)\n\n# nn.Conv2d will be initialized with dict(type='Constant', val=2)\n\n# nn.Linear will be initialized with dict(type='Constant', val=3)\n\n1. Initialize model by override key\n\n* When initializing some specific part with its attribute name, we can use override key, and the value in override\nwill ignore the value in init_cfg.\n\n# layers\n\n# self.feat = nn.Convld(3, 1, 3)\n# self.reg = nn.Conv2d(3, 3, 3)\n# self.cls = nn.Linear(1,2)\n\ninit_cfg = dict(type='Constant',\nlayer=['Convid','Conv2d'], val=1, bias=2,\noverride=dict(type='Constant', name='reg', val=3, bias=4))\n# self.feat and self.cls will be initialized with dict(type='Constant', val=1,\n« bias=2)\n# The module called 'reg' will be initialized with dict(type='Constant', val=3, bias=4)\n\nIf layer is None in init_cfg, only sub-module with the name in override will be initialized, and type and other\nargs in override can be omitted.\n\n# layers\n\n# self.feat = nn.Convld(3, 1, 3)\n# self.reg = nn.Conv2d(3, 3, 3)\n# self.cls = nn.Linear(1,2)\n\ninit_cfg = dict(type='Constant', val=1, bias=2, override=dict (name='reg'))\n\n# self.feat and self.cls will be initialized by Pytorch\n# The module called 'reg' will be initialized with dict(type='Constant', val=1, bias=2)\n\n* If we don’t define layer key or override key, it will not initialize anything.\n\n* Invalid usage\n\n17.3. Usage of init_cfg 107\n", "vlm_text": "17.3 Usage of init_cfg \n1. Initialize model by  layer  key \nIf we only define  layer , it just initialize the layer in  layer  key. \nNOTE: Value of  layer  key is the class name with attributes weights and bias of Pytorch, (so such as Multi head Attention layer  is not supported).\n\n \n• Define  layer  key for initializing module with same configuration.\n\n \nThis isn't a table; it's a code snippet. It initializes a configuration dictionary `init_cfg` with a constant type and specifies layers such as 'Conv1d', 'Conv2d', and 'Linear', all initialized to the value 1. Additionally, there's a comment explaining this configuration applies to the whole module.\n• Define  layer  key for initializing layer with different configurations.\n\n \nThe image is not of a traditional table, but rather a code snippet. This code snippet appears to define the initialization configuration for neural network layers using a list of dictionaries. \n\nEach dictionary specifies a layer type and its corresponding initialization constant value:\n1. The `Conv1d` layer will be initialized with a constant value of 1.\n2. The `Conv2d` layer will be initialized with a constant value of 2.\n3. The `Linear` layer will be initialized with a constant value of 3.\n\nThe snippet indicates that each of these layers (Convolutional layers and Linear layers in a neural network) will use the `Constant` initialization method with specified values when being initialized.\n1. Initialize model by  override  key\n\n \n• When initializing some specific part with its attribute name, we can use  override  key, and the value in  override will ignore the value in init_cfg. \n $\\#$   layers # self.feat  $=$   nn.Conv1d(3, 1, 3) # self.reg  $=$   nn.Conv2d(3, 3, 3) # self.cls  $=$   nn.Linear(1,2) init_cfg  $=$   dict ( type  $=\"$  Constant ' , layer  $=$  [ ' Conv1d ' , ' Conv2d ' ], val  $^{=1}$  , bias  ${}=\\!2$  , override  $\\scriptstyle{\\varepsilon}$  dict ( type = ' Constant ' , name  $\\circeq$  ' reg ' , val  $^{=3}$  , bias  ${=}4$  )) # self.feat and self.cls will be initialized with dict(type  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}^{\\dagger}$  Constant ' ,   $V a L\\!=\\!\\!1$  ,  $\\hookrightarrow$  bias=2) → # The module called  ' reg '  will be initialized with dict(type  $\\circeq$  Constant ' ,   $V a l\\!=\\!3$  , bias=4)\n\n \n• If  layer  is None in init_cfg, only sub-module with the name in override will be initialized, and type and other args in override can be omitted. \n# layers # self.feat  $=$  nn.Conv1d(3, 1, 3) # self.reg  $=$   nn.Conv2d(3, 3, 3) # self.cls  $=$   nn.Linear(1,2) init_cfg  $=$   dict ( type  $=\"$  Constant ' , val  $^{=1}$  , bias  ${}=\\!2$  , override  $=$  dict (name  $\\equiv^{\\dagger}$  reg ' )) # self.feat and self.cls will be initialized by Pytorch # The module called  ' reg '  will be initialized with dict(type= ' Constant ' ,   $V a L\\!=\\!\\!1$  , bias=2)\n\n \n• If we don’t define  layer  key or  override  key, it will not initialize anything.\n\n \n• Invalid usage "}
{"page": 115, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_115.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n# It is invalid that override don't have name key\ninit_cfg = dict(type='Constant', layer ['Convid','Conv2d'], val=1, bias=2,\noverride=dict(type='Constant', val=3, bias=4))\n\n# It is also invalid that override has name and other args except type\ninit_cfg = dict(type='Constant', layer ['Convid','Conv2d'], val=1, bias=2,\noverride=dict(name='reg', val=3, bias=4))\n\n1. Initialize model with the pretrained model\n\ninit_cfg = dict(type='Pretrained',\ncheckpoint='torchvision://resnet50')\n\nMore details can refer to the documentation in MMCV and MMCV PR #780\n\nApart from training/testing scripts, We provide lots of useful tools under the tools/ directory.\n\n108 Chapter 17. Tutorial 10: Weight initialization\n", "vlm_text": "The table contains Python code snippets related to model initialization configurations.\n\n1. **Invalid Override Configurations:**\n   - First snippet shows an invalid configuration where `override` is missing a `name` key.\n   ```python\n   init_cfg = dict(type='Constant', layer=['Conv1d', 'Conv2d'], val=1, bias=2,\n                   override=dict(type='Constant', val=3, bias=4))\n   ```\n   - Second snippet shows an invalid configuration where `override` has a `name` key but is missing the `type` key.\n   ```python\n   init_cfg = dict(type='Constant', layer=['Conv1d', 'Conv2d'], val=1, bias=2,\n                   override=dict(name='reg', val=3, bias=4))\n   ```\n\n2. **Pretrained Model Initialization:**\n   - The last snippet demonstrates initializing a model with a pretrained model from `torchvision`.\n   ```python\n   init_cfg = dict(type='Pretrained', checkpoint='torchvision://resnet50')\n   ```\nMore details can refer to the documentation in  MMCV  and MMCV  PR #780 \nApart from training/testing scripts, We provide lots of useful tools under the  tools/  directory. "}
{"page": 116, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_116.jpg", "ocr_text": "CHAPTER\nEIGHTEEN\n\nLOG ANALYSIS\n\ntools/analysis_tools/analyze_logs.py plots loss/mAP curves given a training log file. Run pip install\nseaborn first to install the dependency.\n\npython tools/analysis_tools/analyze_logs.py plot_curve [--keys ${KEYS}] [--title §{TITLE}\n=] [--legend §{LEGEND}] [--backend ${BACKEND}] [--style ${STYLE}] [--out ${0UT_FILE}]\n\n1.0 — loss_cls\n—— loss_bbox\n\n0.8\n\n0.6\n\n0.4\n0.2\nie) 10000 20000 30000 40000\nter loss curve im-\nage\nExamples:\n\n* Plot the classification loss of some run.\n\npython tools/analysis_tools/analyze_logs.py plot_curve log.json --keys loss_cls --\nlegend loss_cls\n\n* Plot the classification and regression loss of some run, and save the figure to a pdf.\n\n109\n", "vlm_text": "LOG ANALYSIS \ntools/analysis tools/analyze logs.py  plots loss/mAP curves given a training log file. Run  pip install seaborn  first to install the dependency. \npython tools/analysis tools/analyze logs.py plot_curve  [ --keys  \\${ KEYS } ] [ --title  \\${ TITLE } ] [ --legend  \\${ LEGEND } ] [ --backend  \\${ BACKEND } ] [ --style  \\${ STYLE } ] [ --out  \\${ OUT_FILE } ] ˓ → \nThe image is a line graph depicting two types of loss values over iterations named \"loss_cls\" and \"loss_bbox.\" \n\n- The x-axis represents the iterations, ranging from 0 to over 40,000.\n- The y-axis represents the loss value, ranging from 0.0 to 1.0.\n- Two lines are plotted: \n  - \"loss_cls\" (classification loss) is shown in blue.\n  - \"loss_bbox\" (bounding box loss) is shown in orange.\n\nBoth lines show a decreasing trend, indicating a reduction in loss over time.\nage \n• Plot the classification loss of some run. \npython tools/analysis tools/analyze logs.py plot_curve log.json --keys loss_cls -- legend loss_cls ˓ → \n• Plot the classification and regression loss of some run, and save the figure to a pdf. "}
{"page": 117, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_117.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\npython tools/analysis_tools/analyze_logs.py plot_curve log.json --keys loss_cls.\n<loss_bbox --out losses.pdf\n\nCompare the bbox mAP of two runs in the same figure.\n\npython tools/analysis_tools/analyze_logs.py plot_curve log1.json log2.json --keys.\nbbox_mAP --legend runl run2\n\nCompute the average training speed.\n\npython tools/analysis_tools/analyze_logs.py cal_train_time log.json [--include-\noutliers]\n\nThe output is expected to be like the following.\n\nslowest epoch 11, average time is 1.2024\nfastest epoch 1, average time is 1.1909\ntime std over epochs is 0.0028\n\naverage iter time: 1.1959 s/iter\n\n110\n\nChapter 18. Log Analysis\n\n", "vlm_text": "The image shows a command being entered into a console or terminal. This command appears to be for analyzing logs and plotting metrics from a JSON log file using a Python script. Here is a breakdown of the command:\n\n- `python tools/analysis_tools/analyze_logs.py plot_curve log.json`:\n  - This part of the command is calling a Python script named `analyze_logs.py`, which is located in the `tools/analysis_tools/` directory.\n  - The script is being executed to perform a function named `plot_curve`.\n  - The input to the script is a JSON file named `log.json`, which likely contains the log data.\n\n- `--keys loss_cls loss_bbox`:\n  - These are additional arguments passed to the script, specifying the keys `loss_cls` and `loss_bbox`, which could refer to class loss and bounding box loss metrics, respectively.\n\n- `--out losses.pdf`:\n  - This part of the command specifies an output file named `losses.pdf`, where the plotted curves are likely to be saved.\n\nThis command is used for analyzing and plotting specified loss metrics from log data during a model training process or similar task.\n• Compare the bbox mAP of two runs in the same figure.\n\n \nThis is not a table; it's a command line input for a Python script used to analyze logs. The command appears to be for generating a plot of some performance metrics. Here's a breakdown:\n\n- `python tools/analysis_tools/analyze_logs.py`: Runs a Python script located at this path.\n- `plot_curve`: Likely an argument to specify the operation to perform (e.g., plotting a curve).\n- `log1.json log2.json`: These are JSON files containing logs to be analyzed.\n- `--keys bbox_mAP`: Specifies which metrics or keys to plot, in this case, \"bbox_mAP,\" which likely refers to bounding box mean Average Precision.\n- `--legend run1 run2`: Adds a legend to the plot with labels \"run1\" and \"run2\".\n• Compute the average training speed. \nThe image shows a command line instruction:\n\n```\npython tools/analysis_tools/analyze_logs.py cal_train_time log.json [--include-outliers]\n```\n\nThis command appears to run a Python script named `analyze_logs.py` located in the `tools/analysis_tools` directory. The script is likely used to calculate training time based on a log file named `log.json`. The optional parameter `--include-outliers` may be used to include outlier data in the analysis.\nThe output is expected to be like the following. \n-----Analyze train time of work_dirs/some_exp/20190611 192040.log.json----- slowest epoch 11, average time is 1.2024 fastest epoch 1, average time is 1.1909 time std over epochs is 0.0028 average iter time: 1.1959 s/iter "}
{"page": 118, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_118.jpg", "ocr_text": "CHAPTER\n\nNINETEEN\n\nRESULT ANALYSIS\n\ntools/analysis_tools/analyze_results.py calculates single image mAP and saves or shows the topk images\n\nwith the highest and lowest scores based on prediction results.\n\nUsage\n\npython tools/analysis_tools/analyze_results.py \\\n${CONFIG} \\\n${PREDICTION_PATH} \\\n${SHOW_DIR} \\\n[--show] \\\n[--wait-time ${WAIT_TIME}] \\\n[--topk ${TOPK}] \\\n[--show-score-thr $§{SHOW _SCORE_THR}] \\\n[--cfg-options $/{CFG_OPTIONS}]\n\nDescription of all arguments:\n* config: The path of a model config file.\n* prediction_path: Output result file in pickle format from tools/test.py\n* show_dir: Directory where painted GT and detection images will be saved\n* --showDetermines whether to show painted images, If not specified, it will be set to False\n\n¢ --wait-time: The interval of show (s), 0 is block\n\n* --topk: The number of saved images that have the highest and lowest topk scores after sorting. If not specified,\n\nit will be set to 20.\n* --show-score-thr: Show score threshold. If not specified, it will be set to 0.\n* --cfg-options: If specified, the key-value pair optional cfg will be merged into config file\nExamples:\nAssume that you have got result file in pickle format from tools/test . py in the path ‘./result.pkl’.\n\n1. Test Faster R-CNN and visualize the results, save images to the directory results/\n\npython tools/analysis_tools/analyze_results.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\nresult.pkl \\\nresults \\\n--show\n\n1. Test Faster R-CNN and specified topk to 50, save images to the directory results/\n\n111\n\n", "vlm_text": "RESULT ANALYSIS \ntools/analysis tools/analyze results.py  calculates single image mAP and saves or shows the topk images with the highest and lowest scores based on prediction results. \nUsage \npython tools/analysis tools/analyze results.py  \\ \\${ CONFIG }  \\ \\${ PREDICTION PATH }  \\ \\${ SHOW_DIR }  \\ [ --show ]  \\ [ --wait-time  \\${ WAIT_TIME } ]  \\ [ --topk  \\${ TOPK } ]  \\ [ --show-score-thr  \\${ SHOW SCORE THR } ]  \\ [ --cfg-options  \\${ CF G OPTIONS } ] \nDescription of all arguments: \n•  config  $:$   The path of a model config file. •  prediction path : Output result file in pickle format from  tools/test.py •  show_dir : Directory where painted GT and detection images will be saved •  --show Determines whether to show painted images, If not specified, it will be set to  False •  --wait-time : The interval of show (s), 0 is block •  --topk : The number of saved images that have the highest and lowest  topk  scores after sorting. If not specified, it will be set to  20 . •  --show-score-thr : Show score threshold. If not specified, it will be set to  0 . •  --cfg-options : If specified, the key-value pair optional cfg will be merged into config file \nExamples : \nAssume that you have got result file in pickle format from  tools/test.py  in the path ‘./result.pkl’. 1. Test Faster R-CNN and visualize the results, save images to the directory  results/ \nThe table contains a command-line script for running a Python analysis tool. The script is broken down with backslashes for continuation:\n\n- `python tools/analysis_tools/analyze_results.py`: Executes a Python script for analyzing results.\n- `configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py`: Specifies the configuration file for the Faster R-CNN model.\n- `result.pkl`: Refers to a pickle file containing the results to analyze.\n- `results`: Specifies the directory to output or save results.\n- `--show`: Indicates that results will be visually displayed.\n\nThis appears to be related to analyzing results from a machine learning model.\n1. Test Faster R-CNN and specified topk to 50, save images to the directory  results/ "}
{"page": 119, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_119.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\npython tools/analysis_tools/analyze_results.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\nresult.pkl \\\nresults \\\n--topk 50\n\n1. If you want to filter the low score prediction results, you can specify the show-score-thr parameter\n\npython tools/analysis_tools/analyze_results.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\nresult.pkl \\\nresults \\\n--show-score-thr 0.3\n\n112 Chapter 19. Result Analysis\n", "vlm_text": "The image displays a command used for analyzing results, likely in a machine learning or computer vision context. \n\nHere's a breakdown of the command:\n\n- `python`: This indicates that a Python script is being run.\n- `tools/analysis_tools/analyze_results.py`: This is the script being executed.\n- `\\`: These backslashes allow the command to be split across multiple lines.\n- `configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py`: This specifies a configuration file, possibly for a Faster R-CNN model.\n- `result.pkl`: This is likely the name of a file containing results to be analyzed.\n- `results`: This might be an output directory or a file where analyzed results will be stored.\n- `--topk 50`: This is an option that specifies the top 50 results to consider for the analysis.\n1. If you want to filter the low score prediction results, you can specify the  show-score-thr  parameter \nThe table contains a command for running a Python script. Here's a breakdown:\n\n- `python tools/analysis_tools/analyze_results.py`: This is the command to execute a Python script for analyzing results.\n- `configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py`: This specifies the configuration file for the analysis, using a Faster R-CNN model.\n- `result.pkl`: This is likely the results file in a pickle format that the script will analyze.\n- `results`: This might specify the output directory or file.\n- `--show-score-thr 0.3`: This is an option to set the score threshold to 0.3, filtering the results.\n\nThis appears to be related to machine learning, possibly for object detection or similar tasks."}
{"page": 120, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_120.jpg", "ocr_text": "CHAPTER\nTWENTY\n\nVISUALIZATION\n\n20.1 Visualize Datasets\n\ntools/misc/browse_dataset.py helps the user to browse a detection dataset (both images and bounding box an-\nnotations) visually, or save the image to a designated directory.\n\npython tools/misc/browse_dataset.py §{CONFIG} [-h] [--skip-type ${SKIP_TYPE[SKIP_TYPE...\n=]}] [--output-dir §${OUTPUT_DIR}] [--not-show] [--show-interval $§{SHOW_INTERVAL}]\n\n20.2 Visualize Models\n\nFirst, convert the model to ONNX as described here. Note that currently only RetinaNet is supported, support for other\nmodels will be coming in later versions. The converted model could be visualized by tools like Netron.\n\n20.3 Visualize Predictions\n\nIf you need a lightweight GUI for visualizing the detection results, you can refer Det VisGUI project.\n\n113\n\n", "vlm_text": "VISUALIZATION \n20.1 Visualize Datasets \ntools/misc/browse data set.py  helps the user to browse a detection dataset (both images and bounding box an- notations) visually, or save the image to a designated directory. \npython tools/misc/browse data set.py  \\${ CONFIG }  [ -h ] [ --skip-type  \\${ SKIP_TYPE [SKIP_TYPE...  $\\scriptscriptstyle\\hookrightarrow]\\mathcal{Y}]$  ] [ --output-dir  \\${ OUTPUT_DIR } ] [ --not-show ] [ --show-interval  \\${ SHOW INTERVAL } ] \n20.2 Visualize Models \nFirst, convert the model to ONNX as described  here . Note that currently only RetinaNet is supported, support for other models will be coming in later versions. The converted model could be visualized by tools like  Netron . \n20.3 Visualize Predictions \nIf you need a lightweight GUI for visualizing the detection results, you can refer  DetVisGUI project . "}
{"page": 121, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_121.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n114 Chapter 20. Visualization\n", "vlm_text": "MMDetection, Release 2.18.0\n\n114 Chapter 20. Visualization\n"}
{"page": 122, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_122.jpg", "ocr_text": "CHAPTER\nTWENTYONE\n\nERROR ANALYSIS\n\ntools/analysis_tools/coco_error_analysis.py analyzes COCO results per category and by different crite-\nrion. It can also make a plot to provide useful information.\n\npython tools/analysis_tools/coco_error_analysis.py §{RESULT} ${OUT_DIR} [-h] [--ann $\n..{ANN}] [--types ${TYPES[TYPES...]}]\n\nExample:\n\nAssume that you have got Mask R-CNN checkpoint file in the path ‘checkpoint’. For other checkpoints, please refer to\nour model zoo. You can use the following command to get the results bbox and segmentation json file.\n\n# out: results.bbox.json and results.segm. json\n\npython tools/test.py \\\nconfigs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\\ncheckpoint/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n--format-only \\\n--options \"jsonfile_prefix=./results\"\n\n1. Get COCO bbox error results per category , save analyze result images to the directory results/\n\npython tools/analysis_tools/coco_error_analysis.py \\\nresults.bbox.json \\\nresults \\\n--ann=data/coco/annotations/instances_val2017.json \\\n\n1. Get COCO segmentation error results per category , save analyze result images to the directory results/\n\npython tools/analysis_tools/coco_error_analysis.py \\\nresults.segm.json \\\nresults \\\n--ann=data/coco/annotations/instances_val2017.json \\\n--types='segm'\n\n115\n\n", "vlm_text": "ERROR ANALYSIS \ntools/analysis tools/coco error analysis.py  analyzes COCO results per category and by different crite- rion. It can also make a plot to provide useful information. \npython tools/analysis tools/coco error analysis.py  \\${ RESULT } \\${ OUT_DIR }  [ -h ] [ --ann  \\$ { ANN } ] [ --types  \\${ TYPES [TYPES...] } ] ˓ → \nExample: \nAssume that you have got  Mask R-CNN checkpoint file  in the path ‘checkpoint’. For other checkpoints, please refer to our  model zoo . You can use the following command to get the results bbox and segmentation json file. \n# out: results.bbox.json and results.segm.json python tools/test.py  \\ configs/mask_rcnn/mask r cnn r 50 fp n 1 x coco.py  \\ checkpoint/mask r cnn r 50 fp n 1 x coco 20200205-d4b0c5d6.pth  \\ --format-only  \\ --options  \"json file prefix  $\\leftleftarrows$  ./results\" \n1. Get COCO bbox error results per category , save analyze result images to the directory  results/ \npython tools/analysis tools/coco error analysis.py  \\ results.bbox.json  \\ results  \\ --ann = data/coco/annotations/instances val 2017.json  \\ \n1. Get COCO segmentation error results per category , save analyze result images to the directory  results/ \npython tools/analysis tools/coco error analysis.py  \\ results.segm.json  \\ results  \\ --ann  $\\risingdotseq$  data/coco/annotations/instances val 2017.json  \\ --types  $\\equiv^{\\dagger}$  segm ' "}
{"page": 123, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_123.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n116 Chapter 21. Error Analysis\n", "vlm_text": "MMDetection, Release 2.18.0\n\n116 Chapter 21. Error Analysis\n"}
{"page": 124, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_124.jpg", "ocr_text": "CHAPTER\nTWENTYTWO\n\nMODEL SERVING\n\nIn order to serve an MMDetection model with TorchServe, you can follow the steps:\n\n22.1 1. Convert model from MMDetection to TorchServe\n\npython tools/deployment/mmdet2torchserve.py ${CONFIG_FILE} ${CHECKPOINT_FILE} \\\n--output-folder ${MODEL_STORE} \\\n--model-name §{MODEL_NAME}\n\nNote: ${ MODEL_STORE} needs to be an absolute path to a folder.\n\n22.2 2. Build mmdet-serve docker image\n\ndocker build -t mmdet-serve:latest docker/serve/\n\n22.3 3. Run mmdet-serve\n\nCheck the official docs for running TorchServe with docker.\nIn order to run in GPU, you need to install nvidia-docker. You can omit the --gpus argument in order to run in CPU.\n\nExample:\n\ndocker run --rm \\\n\n--cpus 8 \\\n\n--gpus device=0 \\\n\n-p8080:8080 -p8081:8081 -p8082:8082 \\\n\n--mount type=bind, source=$MODEL_STORE, target=/home/model-server/model-store \\\nmmndet-serve: latest\n\nRead the docs about the Inference (8080), Management (8081) and Metrics (8082) APis\n\n117\n\n", "vlm_text": "MODEL SERVING \nIn order to serve an  MM Detection  model with  TorchServe , you can follow the steps: \n22.1 1. Convert model from MM Detection to TorchServe \npython tools/deployment/mm det 2 torch serve.py  \\${ CONFIG FILE }   $\\mathcal{S}$  { CHECKPOINT FILE }  \\\n\n --output-folder  \\${ MODEL STORE }  \\\n\n --model-name \\${MODEL_NAME}\nNote : \\${MODEL STORE} needs to be an absolute path to a folder. \n22.2 2. Build  mmdet-serve  docker image \ndocker build -t mmdet-serve:latest docker/serve/ \n22.3 3. Run  mmdet-serve \nCheck the official docs for  running TorchServe with docker . \nIn order to run in GPU, you need to install  nvidia-docker . You can omit the  --gpus  argument in order to run in CPU. \nExample: \ndocker run --rm  \\\n\n --cpus  8  \\\n\n --gpus  device = 0  \\\n\n -p8080:8080 -p8081:8081 -p8082:8082  \\\n\n --mount  type = bind,source  $=$  \\$MODEL STORE ,target  $\\overline{{\\overline{{\\mathbf{\\alpha}}}}}.$  /home/model-server/model-store  \\ mmdet-serve:latest \nRead the docs  about the Inference (8080), Management (8081) and Metrics (8082) APis "}
{"page": 125, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_125.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n22.4 4. Test deployment\n\ncurl -O curl -0 https://raw.githubusercontent.com/pytorch/serve/master/docs/images/3dogs.\n—Ipg\ncurl http://127.0.0.1:8080/predictions/$§{MODEL_NAME} -T 3dogs. jpg\n\nYou should obtain a response similar to:\n\n[\n{\n\"class_name\": \"dog\",\n“pbox\": [\n294 .63409423828125,\n203 .99111938476562,\n417 .048583984375,\n281.62744140625\n],\n\"score\": 0.9987992644309998\n3,\n{\n\"class_name\": \"dog\",\n“pbox\": [\n404 .26019287109375,\n126. 0080795288086,\n574.5091552734375,\n293 .6662292480469\n],\n\"score\": 0.9979367256164551\n3,\n{\n\"class_name\": \"dog\",\n“pbox\": [\n197.2144775390625,\n93. 3067855834961,\n307 .8505554199219,\n276.7560119628906\n],\n\"score\": 0.993338406085968\n}\n]\n\nAnd you can use test_torchserver .py to compare result of torchserver and pytorch, and visualize them.\n\npython tools/deployment/test_torchserver.py ${IMAGE_FILE} ${CONFIG_FILE} ${CHECKPOINT_\n\nFILE} ${MODEL_NAME }\n[--inference-addr ${INFERENCE_ADDR}] [--device §{DEVICE}] [--score-thr ${SCORE_THR}]\n\nExample:\n\npython tools/deployment/test_torchserver.py \\\ndemo/demo.jpg \\\nconfigs/yolo/yolov3_d53_320_273e_coco.py \\\ncheckpoint/yolov3_d53_320_273e_coco-421362b6.pth \\\n\n(continues on next page)\n\n118 Chapter 22. Model Serving\n", "vlm_text": "22.4 4. Test deployment \ncurl -O curl -O https://raw.g it hub user content.com/pytorch/serve/master/docs/images/3dogs.\n\n  $\\hookrightarrow$  jpg\n\n → curl http://127.0.0.1:8080/predictions/ \\${ MODEL_NAME }  -T 3dogs.jpg \nYou should obtain a response similar to:\n\n \n[ { \"class_name\" :  \"dog\" , \"bbox\" : [ 294.63409423828125 , 203.99111938476562 , 417.048583984375 , 281.62744140625 ], \"score\" :  0.9987992644309998 }, { \"class_name\" :  \"dog\" , \"bbox\" : [ 404.26019287109375 , 126.0080795288086 , 574.5091552734375 , 293.6662292480469 ], \"score\" :  0.9979367256164551 }, { \"class_name\" :  \"dog\" , \"bbox\" : [ 197.2144775390625 , 93.3067855834961 , 307.8505554199219 , 276.7560119628906 ], \"score\" :  0.993338406085968 }\n\n ] \nAnd you can use  test torch server.py  to compare result of torch server and pytorch, and visualize them. \npython tools/deployment/test torch server.py  \\${ IMAGE_FILE } \\${ CONFIG FILE } \\${ CHECKPOINT\n\n  $\\hookrightarrow$  FILE } \\${ MODEL_NAME }\n\n →\n\n [ --inference-addr  \\${ INFERENCE A DDR } ] [ --device  \\${ DEVICE } ] [ --score-thr  \\${ SCORE_THR } ] \nExample: \npython tools/deployment/test torch server.py  \\ demo/demo.jpg  \\ configs/yolo/yo lov 3 d 53 320 273 e coco.py  \\ checkpoint/yo lov 3 d 53 320 273 e coco-421362b6.pth  \\ \n(continues on next page) "}
{"page": 126, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_126.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nyolov3\n\n22.4. 4. Test deployment 119\n\n", "vlm_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nyolov3\n\n22.4. 4. Test deployment 119\n\n"}
{"page": 127, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_127.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n120 Chapter 22. Model Serving\n", "vlm_text": "MMDetection, Release 2.18.0\n\n120 Chapter 22. Model Serving\n"}
{"page": 128, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_128.jpg", "ocr_text": "CHAPTER\nTWENTYTHREE\n\nMODEL COMPLEXITY\n\ntools/analysis_tools/get_flops. py is a script adapted from flops-counter.pytorch to compute the FLOPs and\nparams of a given model.\n\npython tools/analysis_tools/get_flops.py ${CONFIG_FILE} [--shape ${INPUT_SHAPE}]\n\nYou will get the results like this.\n\nInput shape: (3, 1280, 800)\nFlops: 239.32 GFLOPs\nParams: 37.74 M\n\nNote: This tool is still experimental and we do not guarantee that the number is absolutely correct. You may well use\nthe result for simple comparisons, but double check it before you adopt it in technical reports or papers.\n\n1. FLOPs are related to the input shape while parameters are not. The default input shape is (1, 3, 1280, 800).\n\n2. Some operators are not counted into FLOPs like GN and custom operators. Refer to mmcv.cnn.\nget_model_complexity_info() for details.\n\n3. The FLOPs of two-stage detectors is dependent on the number of proposals.\n\n121\n\n", "vlm_text": "MODEL COMPLEXITY \ntools/analysis tools/get_flops.py  is a script adapted from  flops-counter.pytorch  to compute the FLOPs and params of a given model. \npython tools/analysis tools/get_flops.py  \\${ CONFIG FILE }  [ --shape  \\${ INPUT SHAPE } ] \nYou will get the results like this. \n============================== Input shape: (3, 1280, 800) Flops: 239.32 GFLOPs Params: 37.74 M ============================== \nNote : This tool is still experimental and we do not guarantee that the number is absolutely correct. You may well use the result for simple comparisons, but double check it before you adopt it in technical reports or papers. \n1. FLOPs are related to the input shape while parameters are not. The default input shape is (1, 3, 1280, 800). \n2. Some operators are not counted into FLOPs like GN and custom operators. Refer to  mmcv.cnn. get model complexity info()  for details. \n3. The FLOPs of two-stage detectors is dependent on the number of proposals. "}
{"page": 129, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_129.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n122 Chapter 23. Model Complexity\n", "vlm_text": "MMDetection, Release 2.18.0\n\n122 Chapter 23. Model Complexity\n"}
{"page": 130, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_130.jpg", "ocr_text": "CHAPTER\nTWENTYFOUR\n\nMODEL CONVERSION\n\n24.1 MMDetection model to ONNX (experimental)\n\nWe provide a script to convert model to ONNX format. We also support comparing the output results between Pytorch\nand ONNX model for verification.\n\npython tools/deployment/pytorch2onnx.py ${CONFIG_FILE} ${CHECKPOINT_FILE} --output_file $\n+ {ONNX_FILE} [--shape ${INPUT_SHAPE} --verify]\n\nNote: This tool is still experimental. Some customized operators are not supported for now. For a detailed description\nof the usage and the list of supported models, please refer to pytorch2onnx.\n\n24.2 MMDetection 1.x model to MMDetection 2.x\n\ntools/model_converters/upgrade_model_version.py upgrades a previous MMDetection checkpoint to the\nnew version. Note that this script is not guaranteed to work as some breaking changes are introduced in the new\nversion. It is recommended to directly use the new checkpoints.\n\npython tools/model_converters/upgrade_model_version.py ${IN_FILE} §{OUT_FILE} [-h] [--\n<num-classes NUM_CLASSES]\n\n24.3 RegNet model to MMDetection\n\ntools/model_converters/regnet2mmdet.py convert keys in pycls pretrained RegNet models to MMDetection\nstyle.\n\npython tools/model_converters/regnet2mmdet.py ${SRC} ${DST} [-h]\n\n123\n\n", "vlm_text": "MODEL CONVERSION \n24.1 MM Detection model to ONNX (experimental) \nWe provide a script to convert model to  ONNX  format. We also support comparing the output results between Pytorch and ONNX model for verification. \npython tools/deployment/py torch 2 on nx.py  \\${ CONFIG FILE } \\${ CHECKPOINT FILE }  --output file    $\\mathcal{S}\n\n$   $\\hookrightarrow$  { ONNX_FILE }  [ --shape  \\${ INPUT SHAPE }  --verify ]\n\n → \nNote : This tool is still experimental. Some customized operators are not supported for now. For a detailed description of the usage and the list of supported models, please refer to  py torch 2 on nx . \n24.2 MM Detection 1.x model to MM Detection 2.x \ntools/model converters/upgrade model version.py  upgrades a previous MM Detection checkpoint to the new version. Note that this script is not guaranteed to work as some breaking changes are introduced in the new version. It is recommended to directly use the new checkpoints. \npython tools/model converters/upgrade model version.py  \\${ IN_FILE } \\${ OUT_FILE }  [ -h ] [ --\n\n  $\\hookrightarrow$  num-classes NUM CLASSES ]\n\n → \n24.3 RegNet model to MM Detection \ntools/model converters/reg net 2 mm det.py  convert keys in pycls pretrained RegNet models to MM Detection style. \npython tools/model converters/reg net 2 mm det.py \\${SRC} \\${DST} [-h]"}
{"page": 131, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_131.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n24.4 Detectron ResNet to Pytorch\n\ntools/model_converters/detectron2pytorch.py converts keys in the original detectron pretrained ResNet\nmodels to PyTorch style.\n\npython tools/model_converters/detectron2pytorch.py ${SRC} ${DST} ${DEPTH} [-h]\n\n24.5 Prepare a model for publishing\n\ntools/model_converters/publish_model1 . py helps users to prepare their model for publishing.\nBefore you upload a model to AWS, you may want to\n\n1. convert model weights to CPU tensors\n\n2. delete the optimizer states and\n\n3. compute the hash of the checkpoint file and append the hash id to the filename.\n\npython tools/model_converters/publish_model.py ${INPUT_FILENAME} ${OUTPUT_FILENAME}\n\nE.g.,\n\npython tools/model_converters/publish_model.py work_dirs/faster_rcnn/latest.pth faster_\n—renn_r50_fpn_1x_20190801.pth\n\nThe final output filename will be faster_rcnn_r50_fpn_1x_20190801-{hash id}.pth.\n\n124 Chapter 24. Model conversion\n\n", "vlm_text": "24.4 Detectron ResNet to Pytorch \ntools/model converters/detect ron 2 py torch.py  converts keys in the original detectron pretrained ResNet models to PyTorch style. \npython tools/model converters/detect ron 2 py torch.py  \\${ SRC } \\${ DST } \\${ DEPTH }  [ -h ] \n24.5 Prepare a model for publishing \ntools/model converters/publish model.py  helps users to prepare their model for publishing. \nBefore you upload a model to AWS, you may want to \n1. convert model weights to CPU tensors 2. delete the optimizer states and 3. compute the hash of the checkpoint file and append the hash id to the filename. \npython tools/model converters/publish model.py  \\${ INPUT FILENAME } \\${ OUTPUT FILENAME } \npython tools/model converters/publish model.py work_dirs/faster r cnn/latest.pth faster_ rcnn_r50_fpn_1x_20190801.pth ˓ → \nThe final output filename will be  faster r cnn r 50 fp n 1 x 20190801-{hash id}.pth . "}
{"page": 132, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_132.jpg", "ocr_text": "CHAPTER\nTWENTYFIVE\n\nDATASET CONVERSION\n\ntools/data_converters/ contains tools to convert the Cityscapes dataset and Pascal VOC dataset to the COCO\nformat.\n\npython tools/dataset_converters/cityscapes.py ${CITYSCAPES_PATH} [-h] [--img-dir ${IMG_\nDIR}] [--gt-dir ${GT_DIR}] [-o ${0UT_DIR}] [--nproc ${NPROC}]\npython tools/dataset_converters/pascal_voc.py ${DEVKIT_PATH} [-h] [-o §${0UT_DIR}]\n\n125\n", "vlm_text": "DATASET CONVERSION \ntools/data converters/  contains tools to convert the Cityscapes dataset and Pascal VOC dataset to the COCO format. \npython tools/data set converters/cityscapes.py  \\${ CITYSCAPE S PATH }  [ -h ] [ --img-dir  \\${ IMG_ DIR } ] [ --gt-dir  \\${ GT_DIR } ] [ -o  \\${ OUT_DIR } ] [ --nproc  \\${ NPROC } ] ˓ → python tools/data set converters/pascal_voc.py  \\${ DEV KIT PATH }  [ -h ] [ -o  \\${ OUT_DIR } ] "}
{"page": 133, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_133.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n126 Chapter 25. Dataset Conversion\n", "vlm_text": "MMDetection, Release 2.18.0\n\n126 Chapter 25. Dataset Conversion\n"}
{"page": 134, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_134.jpg", "ocr_text": "CHAPTER\nTWENTYSIX\n\nBENCHMARK\n\n26.1 Robust Detection Benchmark\n\ntools/analysis_tools/test_robustness.py andtools/analysis_tools/robustness_eval.py helps\nusers to evaluate model robustness. The core idea comes from Benchmarking Robustness in Object Detection:\nAutonomous Driving when Winter is Coming. For more information how to evaluate models on corrupted images and\nresults for a set of standard models please refer to robustness_benchmarking.md.\n\n26.2 FPS Benchmark\n\ntools/analysis_tools/benchmark.py helps users to calculate FPS. The FPS value includes model forward and\npost-processing. In order to get a more accurate value, currently only supports single GPU distributed startup mode.\n\npython -m torch.distributed.launch --nproc_per_node=1 --master_port=${PORT} tools/\nanalysis_tools/benchmark.py \\\n\nS{CONFIG} \\\n\n${CHECKPOINT} \\\n\n[--repeat-num ${REPEAT_NUM}] \\\n\n[--max-iter §{MAX_ITER}] \\\n\n[--log-interval ${LOG_INTERVAL}] \\\n\n--launcher pytorch\n\nExamples: Assuming that you have already downloaded the Faster R-CNN model checkpoint to the directory\ncheckpoints/.\n\npython -m torch.distributed.launch --nproc_per_node=1 --master_port=29500 tools/analysis_\n<stools/benchmark.py \\\nconfigs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\\ncheckpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n--launcher pytorch\n\n127\n\n", "vlm_text": "BENCHMARK \n26.1 Robust Detection Benchmark \ntools/analysis tools/test robustness.py and tools/analysis tools/robustness e val.py helps users to evaluate model robustness. The core idea comes from  Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming . For more information how to evaluate models on corrupted images and results for a set of standard models please refer to  robustness benchmarking.md . \n26.2 FPS Benchmark \ntools/analysis tools/benchmark.py  helps users to calculate FPS. The FPS value includes model forward and post-processing. In order to get a more accurate value, currently only supports single GPU distributed startup mode. \npython -m torch.distributed.launch --n proc per node  $^{=1}$   --master port  $\\cdot^{-}$  \\${ PORT }  tools/\n\n  $\\hookrightarrow$  analysis tools/benchmark.py  \\\n\n → \\${ CONFIG }  \\ \\${ CHECKPOINT }  \\ [ --repeat-num  \\${ REPEAT_NUM } ]  \\ [ --max-iter    $\\mathcal{S}$  { MAX_ITER } ]  \\ [ --log-interval  \\${ LOG INTERVAL } ]  \\ --launcher pytorch \nExamples: Assuming that you have already downloaded the  Faster R-CNN  model checkpoint to the directory checkpoints/ . \npython -m torch.distributed.launch --n proc per node  $^{=1}$   --master port = 29500  tools/analysis_\n\n  $\\hookrightarrow$  tools/benchmark.py  \\\n\n → configs/faster r cnn/faster r cnn r 50 fp n 1 x coco.py  \\ checkpoints/faster r cnn r 50 fp n 1 x coco 20200130-047c8118.pth  \\ --launcher pytorch "}
{"page": 135, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_135.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n128 Chapter 26. Benchmark\n", "vlm_text": "MMDetection, Release 2.18.0\n\n128 Chapter 26. Benchmark\n"}
{"page": 136, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_136.jpg", "ocr_text": "CHAPTER\nTWENTYSEVEN\n\nMISCELLANEOUS\n\n27.1 Evaluating a metric\n\ntools/analysis_tools/eval_metric. py evaluates certain metrics of a pkl result file according to a config file.\n\npython tools/analysis_tools/eval_metric.py ${CONFIG} ${PKL_RESULTS} [-h] [--format-only].\n[--eval $fEVAL[EVAL ...]}]\n\n{--cfg-options ${CFG_OPTIONS [CFG_OPTIONS ...]}]\n\n{--eval-options ${EVAL_OPTIONS [EVAL_OPTIONS ...]}]\n\n27.2 Print the entire config\n\ntools/misc/print_config.py prints the whole config verbatim, expanding all its imports.\n\npython tools/misc/print_config.py §{CONFIG} [-h] [--options ${OPTIONS [OPTIONS...]}]\n\n129\n\n", "vlm_text": "MISCELLANEOUS \n27.1 Evaluating a metric \ntools/analysis tools/e val metric.py  evaluates certain metrics of a pkl result file according to a config file. \npython tools/analysis tools/e val metric.py  \\${ CONFIG } \\${ P KL RESULTS }  [ -h ] [ --format-only ] ␣\n\n  $\\hookrightarrow$  [ --eval  \\${ EVAL [EVAL ...] } ]\n\n → [ --cfg-options  \\${ CF G OPTIONS  [CF G OPTIONS ...] } ] [ --eval-options  \\${ E VAL OPTIONS  [E VAL OPTIONS ...] } ] \n27.2 Print the entire config \ntools/misc/print config.py  prints the whole config verbatim, expanding all its imports. \npython tools/misc/print config.py  \\${ CONFIG }  [ -h ] [ --options  \\${ OPTIONS  [OPTIONS...] } ] "}
{"page": 137, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_137.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n130 Chapter 27. Miscellaneous\n", "vlm_text": "MMDetection, Release 2.18.0\n\n130 Chapter 27. Miscellaneous\n"}
{"page": 138, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_138.jpg", "ocr_text": "CHAPTER\nTWENTYEIGHT\n\nHYPER-PARAMETER OPTIMIZATION\n\n28.1 YOLO Anchor Optimization\n\ntools/analysis_tools/optimize_anchors. py provides two method to optimize YOLO anchors.\n\nOne is k-means anchor cluster which refers from darknet.\n\npython tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm k-means --input-\nshape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir §/{OUTPUT_DIR}\n\nAnother is using differential evolution to optimize anchors.\n\npython tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm differential_\nevolution --input-shape §{INPUT_SHAPE [WIDTH HEIGHT]} --output-dir §{O0UTPUT_DIR}\n\nE.g.,\n\npython tools/analysis_tools/optimize_anchors.py configs/yolo/yolov3_d53_320_273e_coco. py.\n--algorithm differential_evolution --input-shape 608 608 --device cuda --output-dir.\nwork_dirs\n\nYou will get:\n\nloading annotations into memory...\n\nDone (t=9.70s)\n\ncreating index...\n\nindex created!\n\n2021-07-19 19:37:20,951 - mmdet - INFO - Collecting bboxes from annotation...\n[>>>>>>>>>>>>>>>>> >>> >>> >>> >> >>> >> >>> >>> >>>>>>>>>>>] 117266/117266, 15874.5 task/s,.\nelapsed: 7s, ETA: 0s\n\n2021-07-19 19:37:28,753 - mmdet - INFO - Collected 849902 bboxes.\ndifferential_evolution step 1: f(x)= 0.506055\ndifferential_evolution step 2: f(x)= 0.506055\n\ndifferential_evolution step 489: f(x)= 0.386625\n\n2021-07-19 19:46:40,775 - mmdet - INFO Anchor evolution finish. Average IOU: 0.\n-+6133754253387451\n\n2021-07-19 19:46:40,776 - mmdet - INFO Anchor differential evolution result:[[10, 12],.\n=[15, 30], [32, 22], [29, 59], [61, 46], [57, 116], [112, 89], [154, 198], [349, 336]]\n2021-07-19 19:46:40,798 - mmdet - INFO Result saved in work_dirs/anchor_optimize_result.\n\n<sjson (continues on next page)\n\n131\n\n", "vlm_text": "HYPER-PARAMETER OPTIMIZATION \n28.1 YOLO Anchor Optimization \ntools/analysis tools/optimize anchors.py  provides two method to optimize YOLO anchors. \nOne is k-means anchor cluster which refers from  darknet . \npython tools/analysis tools/optimize anchors.py  \\${ CONFIG }  --algorithm k-means --input-  $\\hookrightarrow$  shape  \\${ INPUT SHAPE  [WIDTH HEIGHT] }  --output-dir  \\${ OUTPUT_DIR } → \nAnother is using differential evolution to optimize anchors. \npython tools/analysis tools/optimize anchors.py  \\${ CONFIG }  --algorithm differential  $\\hookrightarrow$  evolution --input-shape  \\${ INPUT SHAPE  [WIDTH HEIGHT] }  --output-dir  \\${ OUTPUT_DIR } → \nE.g., \npython tools/analysis tools/optimize anchors.py configs/yolo/yo lov 3 d 53 320 273 e coco.py ␣  $\\hookrightarrow$  --algorithm differential evolution --input-shape  608 608  --device cuda --output-dir ␣ →  $\\hookrightarrow$  work_dirs → \nThe table contains log information from a machine learning process, specifically about object detection using `mmdet` (likely MMDetection). Here's a summary of the content:\n\n- **Loading Data:** Annotations are loaded into memory.\n- **Index Creation:** An index is successfully created.\n- **Bounding Boxes Collection:** \n  - Process starts with collecting bounding boxes from annotations (117266 total).\n  - 849902 bounding boxes are collected.\n- **Differential Evolution Steps:** \n  - Step 1 f(x) value: 0.506055\n  - Continues through step 489 with f(x) value: 0.386625\n- **Anchor Evolution:** \n  - Finished with an average IOU of 0.\n  - Results of anchor differential evolution: coordinates for anchor boxes are listed.\n- **Results Saved:** Output is saved in a specified directory.\n\nThis log is part of optimization during object detection model training."}
{"page": 139, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_139.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n132 Chapter 28. Hyper-parameter Optimization\n\n", "vlm_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n132 Chapter 28. Hyper-parameter Optimization\n\n"}
{"page": 140, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_140.jpg", "ocr_text": "CHAPTER\nTWENTYNINE\n\nCONVENTIONS\n\nPlease check the following conventions if you would like to modify MMDetection as your own project.\n\n29.1 Loss\n\nIn MMDetection, a dict containing losses and metrics will be returned by model (**data).\n\nFor example, in bbox head,\n\nclass BBoxHead(nn.Module):\n\ndef loss(self, ...):\nlosses = dictQ\n# classification loss\nlosses['loss_cls'] = self.loss_cls(...)\n# classification accuracy\nlosses['acc'] = accuracy(...)\n# bbox regression loss\nlosses['loss_bbox'] = self.loss_bbox(...)\nreturn losses\n\nbbox_head.loss() will be called during model forward. The returned dict contains 'loss_bbox', 'loss_cls',\n\"acc' . Only 'loss_bbox', 'loss_cls' will be used during back propagation, 'acc' will only be used as a metric\nto monitor training process.\n\nBy default, only values whose keys contain 'loss' will be back propagated. This behavior could be changed by\nmodifying BaseDetector.train_step().\n\n29.2 Empty Proposals\n\nIn MMDetection, We have added special handling and unit test for empty proposals of two-stage. We need to deal with\nthe empty proposals of the entire batch and single image at the same time. For example, in CascadeRolHead,\n\n# simple_test method\n\n# There is no proposal in the whole batch\nif rois.shape[0] == 0:\nbbox_results = [[\n\nnp.zeros((0, 5), dtype=np. float32)\n\n(continues on next page)\n\n133\n\n", "vlm_text": "CONVENTIONS \nPlease check the following conventions if you would like to modify MM Detection as your own project. \n29.1 Loss \nIn MM Detection, a  dict  containing losses and metrics will be returned by  model(\\*\\*data) \nFor example, in bbox head, \nclass  BBoxHead (nn . Module): ... def  loss ( self ,  ... ): losses  $=$   dict () # classification loss losses[ ' loss_cls ' ]  $=$   self . loss_cls( ... ) # classification accuracy losses[ ' acc ' ]  $=$   accuracy( ... ) # bbox regression loss losses[ ' loss_bbox ' ]  $=$   self . loss_bbox( ... ) return  losses \nbbox_head.loss()  will be called during model forward. The returned dict contains  ' loss_bbox ' ,  ' loss_cls ' , ' acc '  . Only  ' loss_bbox ' ,  ' loss_cls '  will be used during back propagation,  ' acc '  will only be used as a metric to monitor training process. \nBy default, only values whose keys contain  ' loss '  will be back propagated. This behavior could be changed by modifying  Base Detector.train_step() . \n29.2 Empty Proposals \nIn MM Detection, We have added special handling and unit test for empty proposals of two-stage. We need to deal with the empty proposals of the entire batch and single image at the same time. For example, in Cascade RoI Head,\n\n \n...\n\n # There is no proposal in the whole batch if  rois . shape  $[\\mathbb{O}]\\;==\\;\\mathbb{O}$  : b box results  $=$   [[ np . zeros(( 0 ,  5 ), dtype = np . float32) "}
{"page": 141, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_141.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nfor _ in range(self.bbox_head[-1] .num_classes)\nJ] * num_imgs\nif self.with_mask:\nmask_classes = self.mask_head[-1].num_classes\nsegm_results = [[[] for _ in range(mask_classes)]\nfor _ in range(num_imngs) ]\nresults = list(zip(bbox_results, segm_results))\nelse:\nresults = bbox_results\nreturn results\n\n# There is no proposal in the single image\nfor i in range(self.num_stages):\n\nif i < self.num_stages - 1:\nfor j in range(num_imgs):\n\n# Handle empty proposal\n\nif rois[j].shape[0] > 0:\nbbox_label = cls_score[j][:, :-1].argmax(dim=1)\nrefine_roi = self.bbox_head[i].regress_by_class(\n\nrois[j], bbox_label, bbox_pred[j], img_metas[j])\n\nrefine_roi_list.append(refine_roi)\n\nIf you have customized RoIHead, you can refer to the above method to deal with empty proposals.\n\n29.3 Coco Panoptic Dataset\n\nIn MMDetection, we have supported COCO Panoptic dataset. We clarify a few conventions about the implementation\nof CocoPanopticDataset here.\n\n1. For mmdet<=2.16.0, the range of foreground and background labels in semantic segmentation are different from\nthe default setting of MMDetection. The label © stands for VOID label and the category labels start from 1.\nSince mmdet=2.17.0, the category labels of semantic segmentation start from 9 and label 255 stands for VOID\nfor consistency with labels of bounding boxes. To achieve that, the Pad pipeline supports setting the padding\nvalue for seg.\n\n2. In the evaluation, the panoptic result is a map with the same shape as the original image. Each value in the result\nmap has the format of instance_id * INSTANCE_OFFSET + category_id.\n\n134 Chapter 29. Conventions\n\n", "vlm_text": "The image shows a snippet of Python code dealing with object detection or image processing, likely using deep learning frameworks. It involves bounding box and mask classification, handling proposals in a single image across multiple stages, and refining regions of interest (ROIs). Key components include loops, conditional statements, and function calls related to bounding box and mask operations.\nIf you have customized  RoIHead , you can refer to the above method to deal with empty proposals. \n29.3 Coco Panoptic Dataset \nIn MM Detection, we have supported COCO Panoptic dataset. We clarify a few conventions about the implementation of  Coco Pan optic Data set  here. \n1. For mmdet  $<=2.16.0$  , the range of foreground and background labels in semantic segmentation are different from the default setting of MM Detection. The label  $\\Updownarrow$   stands for  VOID  label and the category labels start from  1 . Since mmdet  $=\\!2.17.0$  , the category labels of semantic segmentation start from    $\\Updownarrow$   and label  255  stands for  VOID for consistency with labels of bounding boxes. To achieve that, the  Pad  pipeline supports setting the padding value for  seg . 2. In the evaluation, the panoptic result is a map with the same shape as the original image. Each value in the result map has the format of  instance id \\* INSTANCE OFFSET   $^+$   category id . "}
{"page": 142, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_142.jpg", "ocr_text": "CHAPTER\nTHIRTY\n\nCOMPATIBILITY OF MMDETECTION 2.X\n\n30.1 MMDetection 2.18.0\n\n30.1.1 DilHead compatibility\n\nIn order to support QueryInst, attn_feats is added into the returned tuple of DI[Head.\n\n30.2 MMDetection 2.14.0\n\n30.2.1 MMCV Version\nIn order to fix the problem that the priority of EvalHook is too low, all hook priorities have been re-adjusted in 1.3.8, so\n\nMMDetection 2.14.0 needs to rely on the latest MMCV 1.3.8 version. For related information, please refer to #1120,\nfor related issues, please refer to #5343.\n\n30.2.2 SSD compatibility\n\nIn v2.14.0, to make SSD more flexible to use, PR5291 refactored its backbone, neck and head. The users can use the\nscript tools/model_converters/upgrade_ssd_version. py to convert their models.\n\npython tools/model_converters/upgrade_ssd_version.py ${OLD_MODEL PATH} ${NEW_MODEL_PATH}\n\n* OLD_MODEL_PATH: the path to load the old version SSD model.\n* NEW_MODEL_PATH: the path to save the converted model weights.\n\n30.3 MMDetection 2.12.0\n\nMMDetection is going through big refactoring for more general and convenient usages during the releases from v2.12.0\nto v2.18.0 (maybe longer). In v2.12.0 MMDetection inevitably brings some BC-breakings, including the MMCV\ndependency, model initialization, model registry, and mask AP evaluation.\n\n135\n\n", "vlm_text": "COMPATIBILITY OF MM DETECTION 2.X \n30.1 MMDetection 2.18.0\n\n \n30.1.1 DIIHead compatibility \nIn order to support QueryInst, attn_feats is added into the returned tuple of DIIHead. \n30.2 MMDetection 2.14.0 \n30.2.1 MMCV Version \nIn order to fix the problem that the priority of EvalHook is too low, all hook priorities have been re-adjusted in 1.3.8, so MM Detection 2.14.0 needs to rely on the latest MMCV 1.3.8 version. For related information, please refer to  #1120 , for related issues, please refer to  #5343 . \n30.2.2 SSD compatibility \nIn v2.14.0, to make SSD more flexible to use,  PR5291  refactored its backbone, neck and head. The users can use the script  tools/model converters/upgrade ssd version.py  to convert their models. \npython tools/model converters/upgrade ssd version.py  \\${ OLD MODEL PATH } \\${ NEW MODEL PATH } \n• OLD MODEL PATH: the path to load the old version SSD model. • NEW MODEL PATH: the path to save the converted model weights. \nMM Detection is going through big refactoring for more general and convenient usages during the releases from v2.12.0 to v2.18.0 (maybe longer). In v2.12.0 MM Detection inevitably brings some BC-breakings, including the MMCV dependency, model initialization, model registry, and mask AP evaluation. "}
{"page": 143, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_143.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n30.3.1 MMCV Version\n\nMMbDetection v2.12.0 relies on the newest features in MMCV 1.3.3, including BaseModule for unified parameter\ninitialization, model registry, and the CUDA operator MultiScaleDeformableAttn for Deformable DETR. Note\nthat MMCV 1.3.2 already contains all the features used by MMDet but has known issues. Therefore, we recommend\nusers to skip MMCV v1.3.2 and use v1.3.2, though v1.3.2 might work for most of the cases.\n\n30.3.2 Unified model initialization\n\nTo unify the parameter initialization in OpenMMLab projects, MMCV supports BaseModule that accepts init_cfg to\nallow the modules’ parameters initialized in a flexible and unified manner. Now the users need to explicitly call model .\ninit_weights() in the training script to initialize the model (as in here, previously this was handled by the detector.\nThe downstream projects must update their model initialization accordingly to use MMDetection v2.12.0. Please\nrefer to PR #4750 for details.\n\n30.3.3 Unified model registry\n\nTo easily use backbones implemented in other OpenMMLab projects, MMDetection v2. 12.0 inherits the model registry\ncreated in MMCV (#760). In this way, as long as the backbone is supported in an OpenMMLab project and that project\nalso uses the registry in MMCV, users can use that backbone in MMDetection by simply modifying the config without\ncopying the code of that backbone into MMDetection. Please refer to PR #5059 for more details.\n\n30.3.4 Mask AP evaluation\n\nBefore PR 4898 and V2.12.0, the mask AP of small, medium, and large instances is calculated based on the bounding\nbox area rather than the real mask area. This leads to higher APs and APm but lower AP1 but will not affect the overall\nmask AP. PR 4898 change it to use mask areas by deleting bbox in mask AP calculation. The new calculation does not\naffect the overall mask AP evaluation and is consistent with Detectron2.\n\n30.4 Compatibility with MMDetection 1.x\n\nMMbDetection 2.0 goes through a big refactoring and addresses many legacy issues. It is not compatible with the 1.x\nversion, i.e., running inference with the same model weights in these two versions will produce different results. Thus,\nMMbDetection 2.0 re-benchmarks all the models and provides their links and logs in the model zoo.\n\nThe major differences are in four folds: coordinate system, codebase conventions, training hyperparameters, and mod-\nular design.\n\n30.4.1 Coordinate System\n\nThe new coordinate system is consistent with Detectron2 and treats the center of the most left-top pixel as (0, 0) rather\nthan the left-top corner of that pixel. Accordingly, the system interprets the coordinates in COCO bounding box and\nsegmentation annotations as coordinates in range [0, width] or [0, height]. This modification affects all the\ncomputation related to the bbox and pixel selection, which is more natural and accurate.\n\n* The height and width of a box with corners (x1, yl) and (x2, y2) in the new coordinate system is computed as\nwidth = x2 - xl and height = y2 - yl. In MMDetection 1.x and previous version, a “+ 1” was added\nboth height and width. This modification are in three folds:\n\n1. Box transformation and encoding/decoding in regression.\n\n136 Chapter 30. Compatibility of MMDetection 2.x\n", "vlm_text": "30.3.1 MMCV Version \nMM Detection v2.12.0 relies on the newest features in MMCV 1.3.3, including  BaseModule  for unified parameter initialization, model registry, and the CUDA operator  Multi Scale De formable At tn  for  Deformable DETR . Note that MMCV 1.3.2 already contains all the features used by MMDet but has known issues. Therefore, we recommend users to skip MMCV v1.3.2 and use v1.3.2, though v1.3.2 might work for most of the cases. \n30.3.2 Unified model initialization \nTo unify the parameter initialization in OpenMMLab projects, MMCV supports  BaseModule  that accepts  init_cfg  to allow the modules’ parameters initialized in a flexible and unified manner. Now the users need to explicitly call  model. in it weights()  in the training script to initialize the model (as in  here , previously this was handled by the detector. The downstream projects must update their model initialization accordingly to use MM Detection v2.12.0 . Please refer to PR #4750 for details. \n30.3.3 Unified model registry \nTo easily use backbones implemented in other OpenMMLab projects, MM Detection v2.12.0 inherits the model registry created in MMCV (#760). In this way, as long as the backbone is supported in an OpenMMLab project and that project also uses the registry in MMCV, users can use that backbone in MM Detection by simply modifying the config without copying the code of that backbone into MM Detection. Please refer to PR #5059 for more details. \n30.3.4 Mask AP evaluation \nBefore  PR 4898  and V2.12.0, the mask AP of small, medium, and large instances is calculated based on the bounding box area rather than the real mask area. This leads to higher  APs  and  APm  but lower  APl  but will not affect the overall mask AP.  PR 4898  change it to use mask areas by deleting  bbox  in mask AP calculation. The new calculation does not affect the overall mask AP evaluation and is consistent with  Detectron2 . \n30.4 Compatibility with MM Detection 1.x \nMM Detection 2.0 goes through a big refactoring and addresses many legacy issues. It is not compatible with the 1.x version, i.e., running inference with the same model weights in these two versions will produce different results. Thus, MM Detection 2.0 re-benchmarks all the models and provides their links and logs in the model zoo. \nThe major differences are in four folds: coordinate system, codebase conventions, training hyper parameters, and mod- ular design. \n30.4.1 Coordinate System \nThe new coordinate system is consistent with  Detectron2  and treats the center of the most left-top pixel as   $(0,0)$   rather than the left-top corner of that pixel. Accordingly, the system interprets the coordinates in COCO bounding box and segmentation annotations as coordinates in range  [0, width]  or  [0, height] . This modification affects all the computation related to the bbox and pixel selection, which is more natural and accurate. \n• The height and width of a box with corners (x1, y1) and   $(\\mathrm{x}2,\\mathrm{y}2)$   in the new coordinate system is computed as width  $\\mathbf{\\xi}=\\mathbf{\\xi}\\mathbf{x}2\\ \\mathbf{\\xi}-\\ \\mathbf{x}1$   and  height  $\\mathbf{\\xi}=\\ \\mathbf{y}2\\ \\mathbf{\\xi}-\\ \\mathbf{y}1$  . In MM Detection 1.x and previous version, a  $^{**}+1^{**}$   was added both height and width. This modification are in three folds: \n1. Box transformation and encoding/decoding in regression. "}
{"page": 144, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_144.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n2. IoU calculation. This affects the matching process between ground truth and bounding box and the NMS\nprocess. The effect to compatibility is very negligible, though.\n\n3. The corners of bounding box is in float type and no longer quantized. This should provide more accurate\nbounding box results. This also makes the bounding box and Rols not required to have minimum size of 1,\nwhose effect is small, though.\n\n* The anchors are center-aligned to feature grid points and in float type. In MMDetection 1.x and previous version,\nthe anchors are in int type and not center-aligned. This affects the anchor generation in RPN and all the anchor-\nbased methods.\n\n* ROIAlign is better aligned with the image coordinate system. The new implementation is adopted from Detec-\ntron2. The Rols are shifted by half a pixel by default when they are used to cropping Rol features, compared to\nMMDetection 1.x. The old behavior is still available by setting aligned=False instead of aligned=True.\n\n* Mask cropping and pasting are more accurate.\n\n1. We use the new RoIAlign to crop mask targets. In MMDetection 1.x, the bounding box is quantized before\nit is used to crop mask target, and the crop process is implemented by numpy. In new implementation, the\nbounding box for crop is not quantized and sent to RoIAlign. This implementation accelerates the training\nspeed by a large margin (~0.1s per iter, ~2 hour when training Mask R50 for 1x schedule) and should be\nmore accurate.\n\n2. In MMDetection 2.0, the “paste_mask()” function is different and should be more accurate than those\nin previous versions. This change follows the modification in Detectron2 and can improve mask AP on\nCOCO by ~0.5% absolute.\n\n30.4.2 Codebase Conventions\n\n* MMDetection 2.0 changes the order of class labels to reduce unused parameters in regression and mask branch\nmore naturally (without +1 and -1). This effect all the classification layers of the model to have a different\nordering of class labels. The final layers of regression branch and mask head no longer keep K+1 channels for\nK categories, and their class orders are consistent with the classification branch.\n\n- In MMDetection 2.0, label “K” means background, and labels [0, K-1] correspond to the K =\nnum_categories object categories.\n\n—- In MMDetection 1.x and previous version, label “0” means background, and labels [1, K] correspond to\nthe K categories.\n\n— Note: The class order of softmax RPN is still the same as that in 1.x in versions<=2.4.0 while sigmoid RPN\nis not affected. The class orders in all heads are unified since MMDetection v2.5.0.\n\n* Low quality matching in R-CNN is not used. In MMDetection 1.x and previous versions, the\nmax_iou_assigner will match low quality boxes for each ground truth box in both RPN and R-CNN training.\nWe observe this sometimes does not assign the most perfect GT box to some bounding boxes, thus MMDetection\n2.0 do not allow low quality matching by default in R-CNN training in the new system. This sometimes may\nslightly improve the box AP (~0.1% absolute).\n\n* Separate scale factors for width and height. In MMDetection 1.x and previous versions, the scale factor is a single\nfloat in mode keep_ratio=True. This is slightly inaccurate because the scale factors for width and height have\nslight difference. MMDetection 2.0 adopts separate scale factors for width and height, the improvement on AP\n~0.1% absolute.\n\n* Configs name conventions are changed. MMDetection V2.0 adopts the new name convention to maintain the\ngradually growing model zoo as the following:\n\n{model]_(model setting)_[backbone]_[neck]_(norm setting)_(misc)_(gpu x batch)_\n.+[schedule]_[dataset].py,\n\n30.4. Compatibility with MMDetection 1.x 137\n", "vlm_text": "2. IoU calculation. This affects the matching process between ground truth and bounding box and the NMS process. The effect to compatibility is very negligible, though. \n3. The corners of bounding box is in float type and no longer quantized. This should provide more accurate bounding box results. This also makes the bounding box and RoIs not required to have minimum size of 1, whose effect is small, though.\n\n \n• The anchors are center-aligned to feature grid points and in float type. In MM Detection 1.x and previous version, the anchors are in  int  type and not center-aligned. This affects the anchor generation in RPN and all the anchor- based methods.\n\n \n• ROIAlign is better aligned with the image coordinate system. The new implementation is adopted from  Detec- tron2 . The RoIs are shifted by half a pixel by default when they are used to cropping RoI features, compared to MM Detection 1.x. The old behavior is still available by setting  aligned=False  instead of  aligned  $\\risingdotseq$  True .\n\n \n• Mask cropping and pasting are more accurate. \n1. We use the new RoIAlign to crop mask targets. In MM Detection 1.x, the bounding box is quantized before it is used to crop mask target, and the crop process is implemented by numpy. In new implementation, the bounding box for crop is not quantized and sent to RoIAlign. This implementation accelerates the training speed by a large margin   $\\mathord{\\sim}0.1\\mathrm{s}$   per iter,   ${\\sim}2$   hour when training Mask R50 for 1x schedule) and should be more accurate. \n2. In MM Detection 2.0, the “ paste_mask() ” function is different and should be more accurate than those in previous versions. This change follows the modification in  Detectron2  and can improve mask AP on COCO by  ${\\sim}0.5\\%$   absolute.\n\n \n30.4.2 Codebase Conventions \n• MM Detection 2.0 changes the order of class labels to reduce unused parameters in regression and mask branch more naturally (without  $+1$   and  $^{-1}$  ). This effect all the classification layers of the model to have a different ordering of class labels. The final layers of regression branch and mask head no longer keep  $\\mathbf{K}\\!+\\!1$   channels for K categories, and their class orders are consistent with the classification branch. \n–  In MM Detection 2.0, label “K” means background, and labels [0, K-1] correspond to the   $\\textsc{K}=$  num categories object categories. –  In MM Detection 1.x and previous version, label “0” means background, and labels [1, K] correspond to the K categories. – Note : The class order of softmax RPN is still the same as that in 1.x in versions  $<=\\!2.4.0$   while sigmoid RPN is not affected. The class orders in all heads are unified since MM Detection   $\\mathrm{v}2.5.0$  .\n\n \n• Low quality matching in R-CNN is not used. In MM Detection 1.x and previous versions, the max i ou as signer  will match low quality boxes for each ground truth box in both RPN and R-CNN training. We observe this sometimes does not assign the most perfect GT box to some bounding boxes, thus MM Detection 2.0 do not allow low quality matching by default in R-CNN training in the new system. This sometimes may slightly improve the box AP (  ${\\sim}0.1\\%$   absolute).\n\n \n• Separate scale factors for width and height. In MM Detection 1.x and previous versions, the scale factor is a single float in mode  keep_ratio  $\\mathrm{=}$  True . This is slightly inaccurate because the scale factors for width and height have slight difference. MM Detection 2.0 adopts separate scale factors for width and height, the improvement on AP  ${\\sim}0.1\\%$   absolute.\n\n \n• Configs name conventions are changed. MM Detection V2.0 adopts the new name convention to maintain the gradually growing model zoo as the following: \nThe table contains a file naming convention typically used in machine learning or deep learning projects. The format is structured as:\n\n- `[model]` - Name of the model used.\n- `(model setting)` - Specific settings for the model.\n- `[backbone]` - The backbone architecture of the model.\n- `[neck]` - Intermediate layers used to connect parts of the model.\n- `(norm setting)` - Normalization settings.\n- `(misc)` - Miscellaneous settings or information.\n- `(gpu x batch)` - Configuration indicating the number of GPUs and batch size.\n- `[schedule]` - The training schedule or plan.\n- `[dataset]` - The dataset used for training/testing.\n\nThe file extension is `.py`, indicating it is a Python script."}
{"page": 145, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_145.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nwhere the (misc) includes DCN and GCBlock, etc. More details are illustrated in the documentation for config\n\n* MMDetection V2.0 uses new ResNet Caffe backbones to reduce warnings when loading pre-trained models.\nMost of the new backbones’ weights are the same as the former ones but do not have conv.bias, except that\nthey use a different img_norm_cfg. Thus, the new backbone will not cause warning of unexpected keys.\n\n30.4.3 Training Hyperparameters\n\nThe change in training hyperparameters does not affect model-level compatibility but slightly improves the perfor-\nmance. The major ones are:\n\nmax_num=1000. This slightly improves both mask AP and bbox AP by ~0.2% absolute.\n\n* The default box regression losses for Mask R-CNN, Faster R-CNN and RetinaNet are changed from smooth L1\noss to L1 loss. This leads to an overall improvement in box AP (~0.6% absolute). However, using L1-loss for\nother methods such as Cascade R-CNN and HTC does not improve the performance, so we keep the original\nsettings for these methods.\n\n* The number of proposals after nms is changed from 2000 to 1000 by setting nms_post=1000 and\nLi\n\n* The sample num of RoIAlign layer is set to be 0 for simplicity. This leads to slightly improvement on mask AP\n(~0.2% absolute).\n\n* The default setting does not use gradient clipping anymore during training for faster training speed. This does\nnot degrade performance of the most of models. For some models such as RepPoints we keep using gradient\nclipping to stabilize the training process and to obtain better performance.\n\n* The default warmup ratio is changed from 1/3 to 0.001 for a more smooth warming up process since the gradient\nclipping is usually not used. The effect is found negligible during our re-benchmarking, though.\n\n30.4.4 Upgrade Models from 1.x to 2.0\n\nTo convert the models trained by MMDetection V1.x to MMDetection V2.0, the users can use the script tools/\nmodel_converters/upgrade_model_version.py to convert their models. The converted models can be run in\nMMbDetection V2.0 with slightly dropped performance (less than 1% AP absolute). Details can be found in configs/\nlegacy.\n\n30.5 pycocotools compatibility\n\nmmpycocotools is the OpenMMlab’s folk of official pycocotools, which works for both MMDetection and De-\ntectron2. Before PR 4939, since pycocotools and mmpycocotool have the same package name, if users already\ninstalled pyccocotools (installed Detectron2 first under the same environment), then the setup of MMDetection will\nskip installing mmpycocotool. Thus MMDetection fails due to the missing mmpycocotools. If MMDetection is in-\nstalled before Detectron2, they could work under the same environment. PR 4939 deprecates mmpycocotools in favor\nof official pycocotools. Users may install MMDetection and Detectron2 under the same environment after PR 4939,\nno matter what the installation order is.\n\n138 Chapter 30. Compatibility of MMDetection 2.x\n", "vlm_text": "where the ( misc ) includes DCN and GCBlock, etc. More details are illustrated in the  documentation for config \n• MM Detection V2.0 uses new ResNet Caffe backbones to reduce warnings when loading pre-trained models. Most of the new backbones’ weights are the same as the former ones but do not have  conv.bias , except that they use a different  img norm cf g . Thus, the new backbone will not cause warning of unexpected keys. \n30.4.3 Training Hyper parameters \nThe change in training hyper parameters does not affect model-level compatibility but slightly improves the perfor- mance. The major ones are: \n• The number of proposals after nms is changed from 2000 to 1000 by setting  nms_post  $\\scriptstyle=1000$   and max_num  $\\scriptstyle1\\equiv1000$  . This slightly improves both mask AP and bbox AP by  ${\\sim}0.2\\%$   absolute. • The default box regression losses for Mask R-CNN, Faster R-CNN and RetinaNet are changed from smooth L1 Loss to L1 loss. This leads to an overall improvement in box AP (  ${\\sim}0.6\\%$   absolute). However, using L1-loss for other methods such as Cascade R-CNN and HTC does not improve the performance, so we keep the original settings for these methods. • The sample num of RoIAlign layer is set to be 0 for simplicity. This leads to slightly improvement on mask AP (  ${\\sim}0.2\\%$   absolute). • The default setting does not use gradient clipping anymore during training for faster training speed. This does not degrade performance of the most of models. For some models such as RepPoints we keep using gradient clipping to stabilize the training process and to obtain better performance. • The default warmup ratio is changed from 1/3 to 0.001 for a more smooth warming up process since the gradient clipping is usually not used. The effect is found negligible during our re-benchmarking, though. \n30.4.4 Upgrade Models from 1.x to 2.0 \nTo convert the models trained by MM Detection V1.x to MM Detection V2.0, the users can use the script  tools/ model converters/upgrade model version.py  to convert their models. The converted models can be run in MM Detection V2.0 with slightly dropped performance (less than   $1\\%$   AP absolute). Details can be found in  configs/ legacy . \n30.5 py coco tools compatibility \nmm py coco tools  is the OpenMMlab’s folk of official  py coco tools , which works for both MM Detection and De- tectron2. Before  PR 4939 , since  py coco tools  and  mm py coco tool  have the same package name, if users already installed  py c coco tools  (installed Detectron2 first under the same environment), then the setup of MM Detection will skip installing  mm py coco tool . Thus MM Detection fails due to the missing  mm py coco tools . If MM Detection is in- stalled before Detectron2, they could work under the same environment.  PR 4939  deprecates mm py coco tools in favor of official py coco tools. Users may install MM Detection and Detectron2 under the same environment after  PR 4939 , no matter what the installation order is. "}
{"page": 146, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_146.jpg", "ocr_text": "CHAPTER\nTHIRTYONE\n\nPROJECTS BASED ON MMDETECTION\n\nThere are many projects built upon MMDetection. We list some of them as examples of how to extend MMDetection\nfor your own projects. As the page might not be completed, please feel free to create a PR to update this page.\n\n31.1 Projects as an extension\n\nSome projects extend the boundary of MMDetection for deployment or other research fields. They reveal the potential\nof what MMDetection can do. We list several of them as below.\n\n* OTEDetection: OpenVINO training extensions for object detection.\n\n* MMDetection3d: OpenMMLab’s next-generation platform for general 3D object detection.\n\n31.2 Projects of papers\n\nThere are also projects released with papers. Some of the papers are published in top-tier conferences (CVPR, ICCV,\nand ECCV), the others are also highly influential. To make this list also a reference for the community to develop\nand compare new object detection algorithms, we list them following the time order of top-tier conferences. Methods\nalready supported and maintained by MMDetection are not listed.\n\n* Involution: Inverting the Inherence of Convolution for Visual Recognition, CVPR21. [paper][ github]\n* Multiple Instance Active Learning for Object Detection, CVPR 2021. [paper][ github]\n\n* Adaptive Class Suppression Loss for Long-Tail Object Detection, CVPR 2021. [paper][github]\n\n* Generalizable Pedestrian Detection: The Elephant In The Room, CVPR2021. [paper][github]\n\n* Group Fisher Pruning for Practical Network Compression, ICML2021. [paper]| github]\n\n* Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax, CVPR2020.\n[paper][ github]\n\n* Coherent Reconstruction of Multiple Humans from a Single Image, CVPR2020. [paper][github]\n\n¢ Look-into-Object: Self-supervised Structure Modeling for Object Recognition, CVPR 2020. [paper][github]\n* Video Panoptic Segmentation, CVPR2020. [paper][ github]\n\n¢ D2Det: Towards High Quality Object Detection and Instance Segmentation, CVPR2020. [paper][ github]\n\n* CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection, CVPR2020. [paper][github]\n\n¢ Learning a Unified Sample Weighting Network for Object Detection, CVPR 2020. [paper][github]\n\n* Scale-equalizing Pyramid Convolution for Object Detection, CVPR2020. [paper] [github]\n\n139\n", "vlm_text": "PROJECTS BASED ON MM DETECTION \nThere are many projects built upon MM Detection. We list some of them as examples of how to extend MM Detection for your own projects. As the page might not be completed, please feel free to create a PR to update this page. \n31.1 Projects as an extension \nSome projects extend the boundary of MM Detection for deployment or other research fields. They reveal the potential of what MM Detection can do. We list several of them as below. \n•  OTE Detection : OpenVINO training extensions for object detection. •  MM Detection 3 d : OpenMMLab’s next-generation platform for general 3D object detection. \n31.2 Projects of papers \nThere are also projects released with papers. Some of the papers are published in top-tier conferences (CVPR, ICCV, and ECCV), the others are also highly influential. To make this list also a reference for the community to develop and compare new object detection algorithms, we list them following the time order of top-tier conferences. Methods already supported and maintained by MM Detection are not listed. \n• Involution: Inverting the Inherence of Convolution for Visual Recognition, CVPR21.  [paper][github] • Multiple Instance Active Learning for Object Detection, CVPR 2021.  [paper][github] • Adaptive Class Suppression Loss for Long-Tail Object Detection, CVPR 2021.  [paper][github] • General iz able Pedestrian Detection: The Elephant In The Room, CVPR2021.  [paper][github] • Group Fisher Pruning for Practical Network Compression, ICML2021.  [paper][github] • Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax, CVPR2020. [paper][github] • Coherent Reconstruction of Multiple Humans from a Single Image, CVPR2020.  [paper][github] • Look-into-Object: Self-supervised Structure Modeling for Object Recognition, CVPR 2020.  [paper][github] • Video Panoptic Segmentation, CVPR2020.  [paper][github] • D2Det: Towards High Quality Object Detection and Instance Segmentation, CVPR2020.  [paper][github] • Centripetal Net: Pursuing High-quality Keypoint Pairs for Object Detection, CVPR2020.  [paper][github] • Learning a Unified Sample Weighting Network for Object Detection, CVPR 2020.  [paper][github] • Scale-equalizing Pyramid Convolution for Object Detection, CVPR2020.  [paper] [github] "}
{"page": 147, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_147.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Revisiting the Sibling Head in Object Detector, CVPR2020. [paper][github]\n\n* PolarMask: Single Shot Instance Segmentation with Polar Representation, CVPR2020. [paper]| github]\n\n¢ Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection, CVPR2020. [paper][github]\n\n* ZeroQ: A Novel Zero Shot Quantization Framework, CVPR2020. [paper][github]\n\n* CBNet: A Novel Composite Backbone Network Architecture for Object Detection, AAAI2020. [paper][github]\n\n* RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation, AAAI2020.\n[paper][ github]\n\n* Training-Time-Friendly Network for Real-Time Object Detection, AAAI2020. [paper]| github]\n\n* Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution, NeurIPS 2019.\n[paper][ github]\n\n* Reasoning R-CNN: Unifying Adaptive Global Reasoning into Large-scale Object Detection, CVPR2019. [pa-\nper][github]\n\n* Learning Rol Transformer for Oriented Object Detection in Aerial Images, CVPR2019. [paper][github]\n* SOLO: Segmenting Objects by Locations. [paper][github]\n\n* SOLOv2: Dynamic, Faster and Stronger. | paper]| github]\n\n* Dense Peppoints: Representing Visual Objects with Dense Point Sets. |[paper][github]\n\n* IterDet: Iterative Scheme for Object Detection in Crowded Environments. [paper][github]\n\n* Cross-Iteration Batch Normalization. |paper]| github]\n\n¢ A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection,\nNeurIPS2020 [paper]| github]\n\n* RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder, NeurlIPS2020\n[paper][ github]\n\n* Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection,\nCVPR2021{paper][ github]\n\n« Instances as Queries, [CCV2021|paper]| github]\n¢ Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV2021[paper][github]\n\n¢ Focal Transformer: Focal Self-attention for Local-Global Interactions in Vision Transformers,\nNeurIPS2021[paper][github]\n\n¢ End-to-End Semi-Supervised Object Detection with Soft Teacher, ICCV2021[paper]| github]\n* CBNetV2: A Novel Composite Backbone Network Architecture for Object Detection |paper]| github]\n« Instances as Queries, ICCV2021 [paper][github]\n\n140 Chapter 31. Projects based on MMDetection\n", "vlm_text": "• Revisiting the Sibling Head in Object Detector, CVPR2020.  [paper][github]\n\n • PolarMask: Single Shot Instance Segmentation with Polar Representation, CVPR2020.  [paper][github]\n\n • Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection, CVPR2020.  [paper][github]\n\n • ZeroQ: A Novel Zero Shot Quantization Framework, CVPR2020.  [paper][github]\n\n • CBNet: A Novel Composite Backbone Network Architecture for Object Detection, AAAI2020.  [paper][github]\n\n • RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation, AAAI2020. [paper][github]\n\n • Training-Time-Friendly Network for Real-Time Object Detection, AAAI2020.  [paper][github]\n\n • Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution, NeurIPS 2019. [paper][github]\n\n • Reasoning R-CNN: Unifying Adaptive Global Reasoning into Large-scale Object Detection, CVPR2019.  [pa- per][github]\n\n • Learning RoI Transformer for Oriented Object Detection in Aerial Images, CVPR2019.  [paper][github]\n\n • SOLO: Segmenting Objects by Locations.  [paper][github]\n\n • SOLOv2: Dynamic, Faster and Stronger.  [paper][github]\n\n • Dense Peppoints: Representing Visual Objects with Dense Point Sets.  [paper][github]\n\n • IterDet: Iterative Scheme for Object Detection in Crowded Environments.  [paper][github]\n\n • Cross-Iteration Batch Normalization.  [paper][github]\n\n • A Ranking-based, Balanced Loss Function Unifying Classification and Local is ation in Object Detection, Neu rIPS 2020  [paper][github]\n\n • Relation Net  $^{++}$  : Bridging Visual Representations for Object Detection via Transformer Decoder, Neu rIPS 2020 [paper][github]\n\n • Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection, CVPR2021 [paper][github]\n\n • Instances as Queries, ICCV2021 [paper][github]\n\n • Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV2021 [paper][github]\n\n • Focal Transformer: Focal Self-attention for Local-Global Interactions in Vision Transformers, Neu rIPS 2021 [paper][github]\n\n • End-to-End Semi-Supervised Object Detection with Soft Teacher, ICCV2021 [paper][github]\n\n • CBNetV2: A Novel Composite Backbone Network Architecture for Object Detection  [paper][github]\n\n • Instances as Queries, ICCV2021  [paper][github] CHANGELOG\n\n "}
{"page": 148, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_148.jpg", "ocr_text": "CHAPTER\nTHIRTYTWO\n\nCHANGELOG\n\n32.1 v2.18.0 (27/10/2021)\n\n32.1.1 Highlights\n\n* Support QueryInst (#6050)\n\n* Refactor dense heads to decouple onnx export logics from get_bboxes and speed up inference (#5317, #6003,\n#6369, #6268, #6315)\n\n32.1.2 New Features\n\n* Support QueryInst (#6050)\n* Support infinite sampler (#5996)\n\n32.1.3 Bug Fixes\n\n¢ Fix init_weight in fen_mask_head (#6378)\n\n¢ Fix type error in imshow_bboxes of RPN (#6386)\n\n¢ Fix broken colab link in MMDetection Tutorial (#6382)\n\n* Make sure the device and dtype of scale_factor are the same as bboxes (#6374)\n* Remove sampling hardcode (#6317)\n\n¢ Fix RandomAffine bbox coordinate recorrection (#6293)\n\n* Fix init bug of final cls/reg layer in convfc head (#6279)\n\n¢ Fix img_shape broken in auto_augment (#6259)\n\n¢ Fix kwargs parameter missing error in two_stage (#6256)\n\n141\n", "vlm_text": "\n32.1 v2.18.0 (27/10/2021) \n32.1.1 Highlights \n• Support  QueryInst  (#6050)\n\n • Refactor dense heads to decouple onnx export logics from  get_bboxes  and speed up inference (#5317, #6003, #6369, #6268, #6315)\n\n \n32.1.2 New Features \n• Support  QueryInst  (#6050)\n\n • Support infinite sampler (#5996)\n\n \n32.1.3 Bug Fixes \n• Fix in it weight in fc n mask head (#6378)\n\n • Fix type error in im show b boxes of RPN (#6386)\n\n • Fix broken colab link in MM Detection Tutorial (#6382)\n\n • Make sure the device and dtype of scale factor are the same as bboxes (#6374)\n\n • Remove sampling hardcode (#6317)\n\n • Fix Random Affine bbox coordinate re correction (#6293)\n\n • Fix init bug of final cls/reg layer in convfc head (#6279)\n\n • Fix img_shape broken in auto augment (#6259)\n\n • Fix kwargs parameter missing error in two_stage (#6256) "}
{"page": 149, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_149.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.1.4 Improvements\n\n* Unify the interface of stuff head and panoptic head (#6308)\n\n¢ Polish readme (#6243)\n\n* Add code-spell pre-commit hook and fix a typo (#6306)\n\n* Fix typo (#6245, #6190)\n\n¢ Fix sampler unit test (#6284)\n\n¢ Fix forward_dummy of YOLACT to enable get_flops (#6079)\n« Fix link error in the config documentation (#6252)\n\n* Adjust the order to beautify the document (#6195)\n\n32.1.5 Refactors\n\n* Refactor one-stage get_bboxes logic (#5317)\n* Refactor ONNX export of One-Stage models (#6003, #6369)\n* Refactor dense_head and speedup (#6268)\n\n* Migrate to use prior_generator in training of dense heads (#6315)\n\n32.1.6 Contributors\n\nA total of 18 developers contributed to this release. Thanks @Boyden, @onnkeat, @st9007a, @vealocia, @yhcao6,\n@DapangpangX, @yellowdolphin, @cclauss, @kennymckormick, @pingguokiller, @collinzrj, @ AndreaPi, @ Aron-\nLin, @BIGWangYuDong, @hhaAndroid, @jshilong, @RangiLyu, @Zww Wayne\n\n32.2 v2.17.0 (28/9/2021)\n\n32.2.1 Highlights\n\n¢ Support PVT and PVTv2\n\n* Support SOLO\n\n¢ Support large scale jittering and New Mask R-CNN baselines\n* Speed up YOLOv3 inference\n\n32.2.2 New Features\n\n* Support PVT and PVTv2 (#5780)\n* Support SOLO (#5832)\n* Support large scale jittering and New Mask R-CNN baselines (#6132)\n\n« Add a general data structrue for the results of models (#5508)\n\nAdded a base class for one-stage instance segmentation (#5904)\n\n142 Chapter 32. Changelog\n", "vlm_text": "32.1.4 Improvements \n• Unify the interface of stuff head and panoptic head (#6308) • Polish readme (#6243) • Add code-spell pre-commit hook and fix a typo (#6306) • Fix typo (#6245, #6190) • Fix sampler unit test (#6284) • Fix  forward dummy  of YOLACT to enable  get_flops  (#6079) • Fix link error in the config documentation (#6252) • Adjust the order to beautify the document (#6195) \n32.1.5 Refactors \n• Refactor one-stage get_bboxes logic (#5317) • Refactor ONNX export of One-Stage models (#6003, #6369) • Refactor dense_head and speedup (#6268) • Migrate to use prior generator in training of dense heads (#6315) \n32.1.6 Contributors \nA total of 18 developers contributed to this release. Thanks   $@$  Boyden,   $@$  onnkeat,   $@\\mathrm{st}9007\\mathrm{a}$  ,   $@$  vealocia,   $@$  yhcao6,  $@$  Da pang pang X,   $@$  yellow dolphin,   $@$  cclauss,   $@$  kenny m ck or mick,   $@$  ping guo killer,   $@$  collinzrj,   $@$  AndreaPi,   $@$  Aron- Lin,   $@$  BIG Wang Yu Dong,   $@$  hhaAndroid,   $@$  jshilong,   $@$  RangiLyu,   $@$  ZwwWayne \n32.2 v2.17.0 (28/9/2021) \n32.2.1 Highlights \n• Support  PVT  and  PVTv2 • Support  SOLO • Support large scale jittering and New Mask R-CNN baselines • Speed up  YOLOv3  inference \n32.2.2 New Features \n• Support  PVT  and  PVTv2  (#5780) • Support  SOLO  (#5832) • Support large scale jittering and New Mask R-CNN baselines (#6132) • Add a general data structrue for the results of models (#5508) • Added a base class for one-stage instance segmentation (#5904) "}
{"page": 150, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_150.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Speed up YOLOv3 inference (#5991)\n\n* Release Swin Transformer pre-trained models (#6100)\n* Support mixed precision training in YOLOX (#5983)\n\n¢ Support val workflow in YOLACT (#5986)\n\n* Add script to test torchserve (#5936)\n\n¢ Support onnxsim with dynamic input shape (#6117)\n\n32.2.3 Bug Fixes\n\n¢ Fix the function naming errors in model_wrappers (#5975)\n\n* Fix regression loss bug when the input is an empty tensor (#5976)\n\n« Fix scores not contiguous error in centernet_head (#6016)\n\n¢ Fix missing parameters bug in imshow_bboxes (#6034)\n\n¢ Fix bug in aug_test of HTC when the length of det_bboxes is 0 (#6088)\n\n* Fix empty proposal errors in the training of some two-stage models (#5941)\n¢ Fix dynamic_axes parameter error in ONNX dynamic shape export (#6104)\n¢ Fix dynamic_shape bug of SyncRandomSizeHook (#6144)\n\n¢ Fix the Swin Transformer config link error in the configuration (#6172)\n\n32.2.4 Improvements\n\nd filter rules in Mosaic transform (#5897)\n\n\\d size divisor in get flops to avoid some potential bugs (#6076)\n\nd Chinese translation of docs_zh-CN/tutorials/customize_dataset .md (#5915)\nid Chinese translation of conventions .md (#5825)\n\nid description of the output of data pipeline (#5886)\n\nd dataset information in the README file for PanopticFPN (#5996)\n\na a an a aa a\n\nid extra_repr for DropBlock layer to get details in the model printing (#6140)\n* Fix CI out of memory and add PyTorch1.9 Python3.9 unit tests (#5862)\n\n¢ Fix download links error of some model (#6069)\n\n¢ Improve the generalization of XML dataset (#5943)\n\n* Polish assertion error messages (#6017)\n\n* Remove opencv-python-headless dependency by albumentations (#5868)\n\n* Check dtype in transform unit tests (#5969)\n\n* Replace the default theme of documentation with PyTorch Sphinx Theme (#6146)\n* Update the paper and code fields in the metafile (#6043)\n\n¢ Support to customize padding value of segmentation map (#6152)\n\n¢ Support to resize multiple segmentation maps (#5747)\n\n32.2. v2.17.0 (28/9/2021) 143\n", "vlm_text": "• Speed up  YOLOv3  inference (#5991)\n\n • Release Swin Transformer pre-trained models (#6100)\n\n • Support mixed precision training in  YOLOX  (#5983)\n\n • Support  val  workflow in  YOLACT  (#5986)\n\n • Add script to test  torchserve  (#5936)\n\n • Support  onnxsim  with dynamic input shape (#6117)\n\n \n32.2.3 Bug Fixes \n• Fix the function naming errors in  model wrappers  (#5975)\n\n • Fix regression loss bug when the input is an empty tensor (#5976)\n\n • Fix scores not contiguous error in  center net head  (#6016)\n\n • Fix missing parameters bug in  im show b boxes  (#6034)\n\n • Fix bug in  aug_test  of  HTC  when the length of  det_bboxes  is 0 (#6088)\n\n • Fix empty proposal errors in the training of some two-stage models (#5941)\n\n • Fix  dynamic axes  parameter error in  ONNX  dynamic shape export (#6104)\n\n • Fix  dynamic shape  bug of  Sync Random Size Hook  (#6144)\n\n • Fix the Swin Transformer config link error in the configuration (#6172)\n\n \n32.2.4 Improvements \n• Add filter rules in  Mosaic  transform (#5897)\n\n • Add size divisor in get flops to avoid some potential bugs (#6076)\n\n • Add Chinese translation of  docs_zh-CN/tutorials/customize data set.md  (#5915)\n\n • Add Chinese translation of  conventions.md  (#5825)\n\n • Add description of the output of data pipeline (#5886)\n\n • Add dataset information in the README file for  Pan optic FP N  (#5996)\n\n • Add  extra_repr  for  DropBlock  layer to get details in the model printing (#6140)\n\n • Fix CI out of memory and add PyTorch1.9 Python3.9 unit tests (#5862)\n\n • Fix download links error of some model (#6069)\n\n • Improve the generalization of XML dataset (#5943)\n\n • Polish assertion error messages (#6017)\n\n • Remove  opencv-python-headless  dependency by  albumen tat ions  (#5868)\n\n • Check dtype in transform unit tests (#5969)\n\n • Replace the default theme of documentation with PyTorch Sphinx Theme (#6146)\n\n • Update the paper and code fields in the metafile (#6043)\n\n • Support to customize padding value of segmentation map (#6152)\n\n • Support to resize multiple segmentation maps (#5747) "}
{"page": 151, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_151.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.2.5 Contributors\n\nA total of 24 developers contributed to this release. Thanks @morkovkal337, @HarborYuan, @guillaumefrd,\n@guigarfr, @www516717402, @ gaotongxiao, @ypwhs, @MartaYang, @shinya7y, @justiceeem, @zhaojinjian0000,\n@VVsssssk, @aravind-anantha, @wangbo-zhao, @czczup, @whai362, @czczup, @marijnl, @AronLin, @BIG-\nWang YuDong, @hhaAndroid, @jshilong, @RangiLyu, @ZwwWayne\n\n32.3 v2.16.0 (30/8/2021)\n\n32.3.1 Highlights\n\n* Support Panoptic FPN and Swin Transformer\n\n32.3.2 New Features\n\n* Support Panoptic FPN and release models (#5577, #5902)\n\n¢ Support Swin Transformer backbone (#5748)\n\n* Release RetinaNet models pre-trained with multi-scale 3x schedule (#5636)\n« Add script to convert unlabeled image list to coco format (#5643)\n\n¢ Add hook to check whether the loss value is valid (#5674)\n\n« Add YOLO anchor optimizing tool (#5644)\n\n* Support export onnx models without post process. (#5851)\n\n* Support classwise evaluation in CocoPanopticDataset (#5896)\n\n« Adapt browse_dataset for concatenated datasets. (#5935)\n\n« Add PatchEmbed and PatchMerging with AdaptivePadding (#5952)\n\n32.3.3 Bug Fixes\n\n« Fix unit tests of YOLOX (#5859)\n\n¢ Fix lose randomness in imshow_det_bboxes (#5845)\n\n* Make output result of ImageToTensor contiguous (#5756)\n\n¢ Fix inference bug when calling regress_by_class in RolHead in some cases (#5884)\n¢ Fix bug in CloU loss where alpha should not have gradient. (#5835)\n\n¢ Fix the bug that multiscale_output is defined but not used in HRNet (#5887)\n\n* Set the priority of EvalHook to LOW. (#5882)\n\n* Fix a YOLOX bug when applying bbox rescaling in test mode (#5899)\n\n¢ Fix mosaic coordinate error (#5947)\n\n* Fix dtype of bbox in RandomAffine. (#5930)\n\n144 Chapter 32. Changelog\n", "vlm_text": "32.2.5 Contributors \nA total of 24 developers contributed to this release. Thanks   $@$  mork o vka 1337,   $@$  HarborYuan,   $@$  guillaume f rd,\n\n  $@$  guigarfr,   $@$  www516717402,   $@$  gao tong xiao,   $@$  ypwhs,   $@$  MartaYang,  $@$  shinya7y,   $@$  justiceeem,   $@$  zhao j in jian 0000,\n\n  $@$  VVsssssk,   $@$  aravind-anantha,   $@$  wangbo-zhao,   $@$  czczup,   $@$  whai362,   $@$  czczup,   $@$  marijnl,   $@$  AronLin,   $@$  BIG- WangYuDong,   $@$  hhaAndroid,   $@.$  jshilong,   $@$  RangiLyu,   $@$  ZwwWayne \n32.3 v2.16.0 (30/8/2021) \n32.3.1 Highlights \n• Support  Panoptic FPN  and  Swin Transformer \n32.3.2 New Features \n• Support  Panoptic FPN  and release models (#5577, #5902) • Support Swin Transformer backbone (#5748) • Release RetinaNet models pre-trained with multi-scale 3x schedule (#5636) • Add script to convert unlabeled image list to coco format (#5643) • Add hook to check whether the loss value is valid (#5674) • Add YOLO anchor optimizing tool (#5644) • Support export onnx models without post process. (#5851) • Support classwise evaluation in Coco Pan optic Data set (#5896) • Adapt browse data set for concatenated datasets. (#5935) • Add  PatchEmbed  and  Patch Merging  with  Adaptive Padding  (#5952) \n32.3.3 Bug Fixes \n• Fix unit tests of YOLOX (#5859) • Fix lose randomness in  im show det b boxes  (#5845) • Make output result of  Image To Tensor  contiguous (#5756) • Fix inference bug when calling  regress by class  in RoIHead in some cases (#5884) • Fix bug in CIoU loss where alpha should not have gradient. (#5835) • Fix the bug that  multi scale output  is defined but not used in HRNet (#5887) • Set the priority of EvalHook to LOW. (#5882) • Fix a YOLOX bug when applying bbox rescaling in test mode (#5899) • Fix mosaic coordinate error (#5947) • Fix dtype of bbox in Random Affine. (#5930) "}
{"page": 152, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_152.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.3.4 Improvements\n\n¢ Add Chinese version of data_pipeline and (#5662)\n\n* Support to remove state dicts of EMA when publishing models. (#5858)\n¢ Refactor the loss function in HTC and SCNet (#5881)\n\n* Use warnings instead of logger.warning (#5540)\n\n* Use legacy coordinate in metric of VOC (#5627)\n\n¢ Add Chinese version of customize_losses (#5826)\n\n¢ Add Chinese version of model_zoo (#5827)\n\n32.3.5 Contributors\nA total of 19 developers contributed to this release. Thanks @ypwhs, @zywvvd, @collinzrj, @OceanPang, @ddo-\n\nnatien, @@haotian-liu, @viibridges, @Muyun99, @guigarfr, @zhaojinjian0000, @jbwang1997,@wangbo-zhao,\n@xyjiarui, @RangiLyu, @jshilong, @AronLin, @BIGWang YuDong, @hhaAndroid, @ZwwWayne\n\n32.4 v2.15.1 (11/8/2021)\n\n32.4.1 Highlights\n\n* Support YOLOX\n\n32.4.2 New Features\n\n* Support YOLOX(#5756, #5758, #5760, #5767, #5770, #5774, #5777, #5808, #5828, #5848)\n\n32.4.3 Bug Fixes\n\n* Update correct SSD models. (#5789)\n¢ Fix casting error in mask structure (#5820)\n\n* Fix MMCV deployment documentation links. (#5790)\n\n32.4.4 Improvements\n\n* Use dynamic MMCV download link in TorchServe dockerfile (#5779)\n\n* Rename the function upsample_like to interpolate_as for more general usage (#5788)\n\n32.4. v2.15.1 (11/8/2021) 145\n", "vlm_text": "32.3.4 Improvements \n• Add Chinese version of  data pipeline  and (#5662) • Support to remove state dicts of EMA when publishing models. (#5858) • Refactor the loss function in HTC and SCNet (#5881) • Use warnings instead of logger.warning (#5540) • Use legacy coordinate in metric of VOC (#5627) • Add Chinese version of customize losses (#5826) • Add Chinese version of model_zoo (#5827) \n32.3.5 Contributors \nA total of 19 developers contributed to this release. Thanks   $@$  ypwhs,   $@$  zywvvd,   $@$  collinzrj,   $@$  OceanPang,   $@$  ddo- natien,   $@\\,\\@$  haotian-liu,   $@$  viibridges,   $@$  Muyun99,   $@$  guigarfr,   $@$  zhao j in jian 0000,   $@$  jbwang1997,  $@$  wangbo-zhao,  $@$  xvjiarui,   $@$  RangiLyu,   $@.$  jshilong,   $@$  AronLin,   $@$  BIG Wang Yu Dong,   $@$  hhaAndroid, @ZwwWayne \n32.4 v2.15.1 (11/8/2021) \n32.4.1 Highlights \n• Support  YOLOX \n32.4.2 New Features \n• Support  YOLOX (#5756, #5758, #5760, #5767, #5770, #5774, #5777, #5808, #5828, #5848) \n32.4.3 Bug Fixes \n• Update correct SSD models. (#5789) • Fix casting error in mask structure (#5820) • Fix MMCV deployment documentation links. (#5790) \n32.4.4 Improvements \n• Use dynamic MMCV download link in TorchServe dockerfile (#5779) • Rename the function  up sample like  to  interpolate as  for more general usage (#5788) "}
{"page": 153, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_153.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.4.5 Contributors\n\nA total of 14 developers contributed to this release. Thanks @HAOCHENYE, @xiaohu2015, @HsLOL, @zhiqwang,\n@Adamdad, @shinya7y, @Johnson-Wang, @RangiLyu, @jshilong, @mmeendez8, @AronLin, @ BIGWang YuDong,\n@hhaAndroid, @Zww Wayne\n\n32.5 v2.15.0 (02/8/2021)\n\n32.5.1 Highlights\n\n* Support adding MIM dependencies during pip installation\n* Support MobileNetV2 for SSD-Lite and YOLOv3\n\n¢ Support Chinese Documentation\n\n32.5.2 New Features\n\n¢ Add function upsample_like (#5732)\n\n* Support to output pdf and epub format documentation (#5738)\n\n* Support and release Cascade Mask R-CNN 3x pre-trained models (#5645)\n« Add ignore_index to CrossEntropyLoss (#5646)\n\n¢ Support adding MIM dependencies during pip installation (#5676)\n\n* Add MobileNetV2 config and models for YOLOv3 (#5510)\n\n* Support COCO Panoptic Dataset (#5231)\n\n* Support ONNX export of cascade models (#5486)\n\n¢ Support DropBlock with RetinaNet (#5544)\n\n* Support MobileNetV2 SSD-Lite (#5526)\n\n32.5.3 Bug Fixes\n\n¢ Fix the device of label in multiclass_nms (#5673)\n\n* Fix error of backbone initialization from pre-trained checkpoint in config file (#5603, #5550)\n* Fix download links of RegNet pretrained weights (#5655)\n\n* Fix two-stage runtime error given empty proposal (#5559)\n\n¢ Fix flops count error in DETR (#5654)\n\n¢ Fix unittest for NumClassCheckHook when it is not used. (#5626)\n\n¢ Fix description bug of using custom dataset (#5546)\n\n* Fix bug of multiclass_nms that returns the global indices (#5592)\n\n¢ Fix valid_mask logic error in RPNHead (#5562)\n\n¢ Fix unit test error of pretrained configs (#5561)\n\n* Fix typo error in anchor_head.py (#5555)\n\n146 Chapter 32. Changelog\n", "vlm_text": "32.4.5 Contributors \nA total of 14 developers contributed to this release. Thanks   $@$  HAOCHENYE,   $@$  xiaohu2015,   $@$  HsLOL,   $@$  zhiqwang,\n\n  $@$  Adamdad,  $@$  shinya7y,   $@$  Johnson-Wang,   $@$  RangiLyu,   $@$  jshilong,   $@$  mmeendez8,   $@$  AronLin,   $@$  BIG Wang Yu Dong,\n\n  $@$  hhaAndroid,   $@$  ZwwWayne \n32.5 v2.15.0 (02/8/2021) \n32.5.1 Highlights \n• Support adding  MIM  dependencies during pip installation • Support Mobile Ne tV 2 for SSD-Lite and YOLOv3 • Support Chinese Documentation \n32.5.2 New Features \n• Add function up sample like (#5732)• Support to output pdf and epub format documentation (#5738) • Support and release Cascade Mask R-CNN  $3\\mathbf{X}$   pre-trained models (#5645) • Add  ignore index  to Cross Entropy Loss (#5646) • Support adding  MIM  dependencies during pip installation (#5676) • Add Mobile Ne tV 2 config and models for YOLOv3 (#5510) • Support COCO Panoptic Dataset (#5231) • Support ONNX export of cascade models (#5486) • Support DropBlock with RetinaNet (#5544) • Support Mobile Ne tV 2 SSD-Lite (#5526) \n32.5.3 Bug Fixes \n• Fix the device of label in multi class nm s (#5673) • Fix error of backbone initialization from pre-trained checkpoint in config file (#5603, #5550) • Fix download links of RegNet pretrained weights (#5655) • Fix two-stage runtime error given empty proposal (#5559) • Fix flops count error in DETR (#5654) • Fix unittest for  Num Class Check Hook  when it is not used. (#5626) • Fix description bug of using custom dataset (#5546) • Fix bug of  multi class nm s  that returns the global indices (#5592) • Fix  valid_mask  logic error in RPNHead (#5562) • Fix unit test error of pretrained configs (#5561) • Fix typo error in anchor head.py (#5555) • Fix bug when using dataset wrappers (#5552) • Fix a typo error in demo/MM Det Tutorial.ipynb (#5511) • Fixing crash in  get root logger  when  cfg.log_level  is not None (#5521) • Fix docker version (#5502) • Fix optimizer parameter error when using  It er Based Runner  (#5490) "}
{"page": 154, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_154.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n« Fix bug when using dataset wrappers (#5552)\n\n¢ Fix a typo error in demo/MMDet_Tutorial.ipynb (#5511)\n\n¢ Fixing crash in get_root_logger when cfg.1log_level is not None (#5521)\n¢ Fix docker version (#5502)\n\n* Fix optimizer parameter error when using IterBasedRunner (#5490)\n\n32.5.4 Improvements\n\n« Add unit tests for MMTracking (#5620)\n\n¢ Add Chinese translation of documentation (#5718, #5618, #5558, #5423, #5593, #5421, #5408. #5369, #5419,\n#5530, #5531)\n\n* Update resource limit (#5697)\n\n* Update docstring for InstaBoost (#5640)\n\n* Support key reduction_override in all loss functions (#5515)\n* Use repeatdataset to accelerate CenterNet training (#5509)\n\n* Remove unnecessary code in autoassign (#5519)\n\n¢ Add documentation about init_cfg (#5273)\n\n32.5.5 Contributors\n\nA total of 18 developers contributed to this release. Thanks @OceanPang, @AronLin, @hellock, @Outsider565,\n@RangiLyu, @ElectronicElephant, @likyoo, @BIGWangYuDong, @hhaAndroid, @noobying, @yyz561, @likyoo,\n@zeakey, @ZwwWayne, @ChenyangLiu, @johnson-magic, @qingswu, @ BuxianChen\n\n32.6 v2.14.0 (29/6/2021)\n\n32.6.1 Highlights\n\n« Add simple_test to dense heads to improve the consistency of single-stage and two-stage detectors\n* Revert the test_mixins to single image test to improve efficiency and readability\n\n« Add Faster R-CNN and Mask R-CNN config using multi-scale training with 3x schedule\n\n32.6.2 New Features\n\n* Support pretrained models from MoCo v2 and SwAV (#5286)\n\n* Add Faster R-CNN and Mask R-CNN config using multi-scale training with 3x schedule (#5179, #5233)\n« Add reduction_override in MSELoss (#5437)\n\n* Stable support of exporting DETR to ONNX with dynamic shapes and batch inference (#5168)\n\n* Stable support of exporting PointRend to ONNX with dynamic shapes and batch inference (#5440)\n\n32.6. v2.14.0 (29/6/2021) 147\n", "vlm_text": "\n32.5.4 Improvements \n• Add unit tests for MMTracking (#5620) • Add Chinese translation of documentation (#5718, #5618, #5558, #5423, #5593, #5421, #5408. #5369, #5419, #5530, #5531) • Update resource limit (#5697) • Update docstring for InstaBoost (#5640) • Support key  reduction override  in all loss functions (#5515) • Use repeat data set to accelerate CenterNet training (#5509) • Remove unnecessary code in autoassign (#5519) • Add documentation about  init_cfg  (#5273) \n32.5.5 Contributors \nA total of 18 developers contributed to this release. Thanks   $@$  OceanPang,   $@$  AronLin,   $@$  hellock,   $@$  Outsider 565,\n\n  $@$  RangiLyu,   $@$  Electronic Elephant,   $@$  likyoo,   $@$  BIG Wang Yu Dong,   $@$  hhaAndroid,   $@$  noobying,   $@$  yyz561,   $@$  likyoo,\n\n  $@$  zeakey,   $@$  ZwwWayne,   $@$  Chen yang Liu,   $@$  johnson-magic,   $@$  qingswu,   $@$  BuxianChen \n32.6 v2.14.0 (29/6/2021) \n32.6.1 Highlights \n• Add  simple test  to dense heads to improve the consistency of single-stage and two-stage detectors • Revert the  test mix in s  to single image test to improve efficiency and readability • Add Faster R-CNN and Mask R-CNN config using multi-scale training with  $3\\mathbf{X}$   schedule \n32.6.2 New Features \n• Support pretrained models from MoCo v2 and SwAV (#5286) • Add Faster R-CNN and Mask R-CNN config using multi-scale training with   $3\\mathbf{X}$   schedule (#5179, #5233) • Add  reduction override  in MSELoss (#5437) • Stable support of exporting DETR to ONNX with dynamic shapes and batch inference (#5168) • Stable support of exporting PointRend to ONNX with dynamic shapes and batch inference (#5440) "}
{"page": 155, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_155.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.6.3 Bug Fixes\n\n* Fix size mismatch bug in multiclass_nms (#4980)\n\n¢ Fix the import path of MultiScaleDeformableAttention (#5338)\n\n« Fix errors in config of GCNet ResNext101 models (#5360)\n\n¢ Fix Grid-RCNN error when there is no bbox result (#5357)\n\n¢ Fix errors in onnx_export of bbox_head when setting reg_class_agnostic (#5468)\nFix type error of AutoAssign in the document (#5478)\n\n* Fix web links ending with .md (#5315)\n\n32.6.4 Improvements\n\n« Add simple_test to dense heads to improve the consistency of single-stage and two-stage detectors (#5264)\n« Add support for mask diagonal flip in TTA (#5403)\n\n* Revert the test_mixins to single image test to improve efficiency and readability (#5249)\n¢ Make YOLOv3 Neck more flexible (#5218)\n\n* Refactor SSD to make it more general (#5291)\n\n* Refactor anchor_generator and point_generator (#5349)\n\n* Allow to configure out the mask_head of the HTC algorithm (#5389)\n\n* Delete deprecated warning in FPN (#5311)\n\n* Move model. pretrained to model. backbone. init_cfg (#5370)\n\n* Make deployment tools more friendly to use (#5280)\n\n* Clarify installation documentation (#5316)\n\n« Add ImageNet Pretrained Models docs (#5268)\n\n« Add FAQ about training loss=nan solution and COCO AP or AR =-1 (#5312, #5313)\n\n* Change all weight links of http to https (#5328)\n\n32.7 v2.13.0 (01/6/2021)\n\n32.7.1 Highlights\n\n¢ Support new methods: CenterNet, Seesaw Loss, MobileNetV2\n\n148 Chapter 32. Changelog\n", "vlm_text": "32.6.3 Bug Fixes \n• Fix size mismatch bug in  multi class nm s  (#4980)\n\n • Fix the import path of  Multi Scale De formable Attention  (#5338)\n\n • Fix errors in config of GCNet ResNext101 models (#5360)\n\n • Fix Grid-RCNN error when there is no bbox result (#5357)\n\n • Fix errors in  on nx export  of bbox_head when setting reg class agnostic (#5468)\n\n • Fix type error of AutoAssign in the document (#5478)\n\n • Fix web links ending with  .md  (#5315)\n\n \n32.6.4 Improvements \n• Add  simple test  to dense heads to improve the consistency of single-stage and two-stage detectors (#5264)\n\n • Add support for mask diagonal flip in TTA (#5403)\n\n • Revert the  test mix in s  to single image test to improve efficiency and readability (#5249)\n\n • Make YOLOv3 Neck more flexible (#5218)\n\n • Refactor SSD to make it more general (#5291)\n\n • Refactor  anchor generator  and  point generator  (#5349)\n\n • Allow to configure out the  mask_head  of the HTC algorithm (#5389)\n\n • Delete deprecated warning in FPN (#5311)\n\n • Move  model.pretrained  to  model.backbone.init_cfg  (#5370)\n\n • Make deployment tools more friendly to use (#5280)\n\n • Clarify installation documentation (#5316)\n\n • Add ImageNet Pretrained Models docs (#5268)\n\n • Add FAQ about training loss  $\\scriptstyle,=$  nan solution and COCO AP or AR  $_{=-1}$   (# 5312, #5313)\n\n • Change all weight links of http to https (#5328)\n\n \n32.7 v2.13.0 (01/6/2021) \n32.7.1 Highlights \n• Support new methods:  CenterNet ,  Seesaw Loss ,  Mobile Ne tV 2 "}
{"page": 156, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_156.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.7.2 New Features\n\n* Support paper Objects as Points (#4602)\n\n* Support paper Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021) (#5128)\n* Support MobileNetV2 backbone and inverted residual block (#5122)\n\n* Support MIM (#5143)\n\n* ONNX exportation with dynamic shapes of CornerNet (#5136)\n\n« Add mask_soft config option to allow non-binary masks (#4615)\n\n¢ Add PWC metafile (#5135)\n\n32.7.3 Bug Fixes\n\n¢ Fix YOLOv3 FP16 training error (#5172)\n* Fix Cacscade R-CNN TTA test error when det_bboxes length is 0 (#5221)\n¢ Fix iou_thr variable naming errors in VOC recall calculation function (#5195)\n\n* Fix Faster R-CNN performance dropped in ONNX Runtime (#5197)\n\nFix DETR dict changed error when using python 3.8 during iteration (#5226)\n\n32.7.4 Improvements\n\n* Refactor ONNX export of two stage detector (#5205)\n\n* Replace MMDetection’s EvalHook with MMCV’s EvalHook for consistency (#4806)\n* Update Rol extractor for ONNX (#5194)\n\n* Use better parameter initialization in YOLOv3 head for higher performance (#5181)\n* Release new DCN models of Mask R-CNN by mixed-precision training (#5201)\n\n* Update YOLOv3 model weights (#5229)\n\n« Add DetectoRS ResNet-101 model weights (#4960)\n\n* Discard bboxes with sizes equals to min_bbox_size (#5011)\n\n* Remove duplicated code in DETR head (#5129)\n\n* Remove unnecessary object in class definition (#5180)\n\n¢ Fix doc link (#5192)\n\n32.7. v2.13.0 (01/6/2021) 149\n", "vlm_text": "32.7.2 New Features \n• Support paper  Objects as Points  (#4602)\n\n • Support paper  Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021)  (#5128)\n\n • Support  Mobile Ne tV 2  backbone and inverted residual block (#5122)\n\n • Support  MIM  (#5143)\n\n • ONNX exportation with dynamic shapes of CornerNet (#5136)\n\n • Add  mask_soft  config option to allow non-binary masks (#4615)\n\n • Add PWC metafile (#5135)\n\n \n32.7.3 Bug Fixes \n\n• Fix Cacscade R-CNN TTA test error when  det_bboxes  length is 0 (#5221)\n\n • Fix  iou_thr  variable naming errors in VOC recall calculation function (#5195)\n\n • Fix Faster R-CNN performance dropped in ONNX Runtime (#5197)\n\n • Fix DETR dict changed error when using python 3.8 during iteration (#5226)\n\n \n32.7.4 Improvements \n• Refactor ONNX export of two stage detector (#5205)\n\n • Replace MM Detection’s EvalHook with MMCV’s EvalHook for consistency (#4806)\n\n • Update RoI extractor for ONNX (#5194)\n\n • Use better parameter initialization in YOLOv3 head for higher performance (#5181)\n\n • Release new DCN models of Mask R-CNN by mixed-precision training (#5201)\n\n • Update YOLOv3 model weights (#5229)\n\n • Add DetectoRS ResNet-101 model weights (#4960)\n\n • Discard bboxes with sizes equals to  min b box size  (#5011)\n\n • Remove duplicated code in DETR head (#5129)\n\n • Remove unnecessary object in class definition (#5180)\n\n • Fix doc link (#5192) "}
{"page": 157, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_157.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.8 v2.12.0 (01/5/2021)\n\n32.8.1 Highlights\n\nSupport new methods: AutoAssign, YOLOF, and Deformable DETR\n\nStable support of exporting models to ONNX with batched images and dynamic shape (#5039)\n\n32.8.2 Backwards Incompatible Changes\n\nMMbDetection is going through big refactoring for more general and convenient usages during the releases from v2.12.0\nto v2.15.0 (maybe longer). In v2.12.0 MMDetection inevitably brings some BC-breakings, including the MMCV\ndependency, model initialization, model registry, and mask AP evaluation.\n\nMMCYV version. MMDetection v2.12.0 relies on the newest features in MMCV 1.3.3, including BaseModule\nfor unified parameter initialization, model registry, and the CUDA operator MultiScaleDeformableAttn for\nDeformable DETR. Note that MMCV 1.3.2 already contains all the features used by MMDet but has known\nissues. Therefore, we recommend users skip MMCV v1.3.2 and use v1.3.3, though v1.3.2 might work for most\ncases.\n\nUnified model initialization (#4750). To unify the parameter initialization in OpenMMLab projects, MMCV\nsupports BaseModule that accepts init_cfg to allow the modules’ parameters initialized in a flexible and uni-\nfied manner. Now the users need to explicitly call model .init_weights() in the training script to initialize\nthe model (as in here, previously this was handled by the detector. The models in MMDetection have been re-\nbenchmarked to ensure accuracy based on PR #4750. The downstream projects should update their code\naccordingly to use MMDetection v2.12.0.\n\nUnified model registry (#5059). To easily use backbones implemented in other OpenMMLab projects, MMDe-\ntection migrates to inherit the model registry created in MMCV (#760). In this way, as long as the backbone\nis supported in an OpenMMLab project and that project also uses the registry in MMCYV, users can use that\nbackbone in MMDetection by simply modifying the config without copying the code of that backbone into\nMMDetection.\n\nMask AP evaluation (#4898). Previous versions calculate the areas of masks through the bounding boxes when\ncalculating the mask AP of small, medium, and large instances. To indeed use the areas of masks, we pop the\nkey bbox during mask AP calculation. This change does not affect the overall mask AP evaluation and aligns\nthe mask AP of similar models in other projects like Detectron2.\n\n32.8.3 New Features\n\nSupport paper AutoAssign: Differentiable Label Assignment for Dense Object Detection (#4295)\nSupport paper You Only Look One-level Feature (#4295)\n\nSupport paper Deformable DETR: Deformable Transformers for End-to-End Object Detection (#4778)\nSupport calculating IoU with FP16 tensor in bbox_overlaps to save memory and keep speed (#4889)\nAdd __repr__ in custom dataset to count the number of instances (#4756)\n\nAdd windows support by updating requirements.txt (#5052)\n\nStable support of exporting models to ONNX with batched images and dynamic shape, including SSD,\nFSAF,FCOS, YOLOv3, RetinaNet, Faster R-CNN, and Mask R-CNN (#5039)\n\n150\n\nChapter 32. Changelog\n", "vlm_text": "32.8 v2.12.0 (01/5/2021) \n32.8.1 Highlights \n• Support new methods:  AutoAssign ,  YOLOF , and  Deformable DETR • Stable support of exporting models to ONNX with batched images and dynamic shape (#5039) \n32.8.2 Backwards Incompatible Changes \nMM Detection is going through big refactoring for more general and convenient usages during the releases from v2.12.0 to v2.15.0 (maybe longer). In v2.12.0 MM Detection inevitably brings some BC-breakings, including the MMCV dependency, model initialization, model registry, and mask AP evaluation. \n• MMCV version. MM Detection v2.12.0 relies on the newest features in MMCV 1.3.3, including  BaseModule for unified parameter initialization, model registry, and the CUDA operator  Multi Scale De formable At tn  for Deformable DETR . Note that MMCV 1.3.2 already contains all the features used by MMDet but has known issues. Therefore, we recommend users skip MMCV v1.3.2 and use v1.3.3, though v1.3.2 might work for most cases. • Unified model initialization (#4750). To unify the parameter initialization in OpenMMLab projects, MMCV supports  BaseModule  that accepts  init_cfg  to allow the modules’ parameters initialized in a flexible and uni- fied manner. Now the users need to explicitly call  model.in it weights()  in the training script to initialize the model (as in  here , previously this was handled by the detector. The models in MM Detection have been re- benchmarked to ensure accuracy based on PR #4750.  The downstream projects should update their code accordingly to use MM Detection v2.12.0 . • Unified model registry (#5059). To easily use backbones implemented in other OpenMMLab projects, MMDe- tection migrates to inherit the model registry created in MMCV (#760). In this way, as long as the backbone is supported in an OpenMMLab project and that project also uses the registry in MMCV, users can use that backbone in MM Detection by simply modifying the config without copying the code of that backbone into MM Detection. • Mask AP evaluation (#4898). Previous versions calculate the areas of masks through the bounding boxes when calculating the mask AP of small, medium, and large instances. To indeed use the areas of masks, we pop the key  bbox  during mask AP calculation. This change does not affect the overall mask AP evaluation and aligns the mask AP of similar models in other projects like Detectron2. \n32.8.3 New Features \n• Support paper  AutoAssign: Differentiable Label Assignment for Dense Object Detection  (#4295) • Support paper  You Only Look One-level Feature  (#4295) • Support paper  Deformable DETR: Deformable Transformers for End-to-End Object Detection  (#4778) • Support calculating IoU with FP16 tensor in  b box overlaps  to save memory and keep speed (#4889) • Add  __repr__  in custom dataset to count the number of instances (#4756) • Add windows support by updating requirements.txt (#5052) • Stable support of exporting models to ONNX with batched images and dynamic shape, including SSD, FSAF,FCOS, YOLOv3, RetinaNet, Faster R-CNN, and Mask R-CNN (#5039) "}
{"page": 158, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_158.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.8.4 Improvements\n\n* Use MMCV MODEL_REGISTRY (#5059)\n\n* Unified parameter initialization for more flexible usage (#4750)\n\n* Rename variable names and fix docstring in anchor head (#4883)\n\n* Support training with empty GT in Cascade RPN (#4928)\n\n* Add more details of usage of test_robustness in documentation (#4917)\n\n* Changing to use pycocotools instead of mmpycocotools to fully support Detectron2 and MMDetection in\none environment (#4939)\n\n* Update torch serve dockerfile to support dockers of more versions (#4954)\n« Add check for training with single class dataset (#4973)\n\n¢ Refactor transformer and DETR Head (#4763)\n\n* Update FPG model zoo (#5079)\n\n* More accurate mask AP of small/medium/large instances (#4898)\n\n32.8.5 Bug Fixes\n\n¢ Fix bug in mean_ap.py when calculating mAP by 11 points (#4875)\n* Fix error when key meta is not in old checkpoints (#4936)\n\n« Fix hanging bug when training with empty GT in VFNet, GFL, and FCOS by changing the place of reduce_mean\n(#4923, #4978, #5058)\n\n* Fix asyncronized inference error and provide related demo (#4941)\n* Fix IoU losses dimensionality unmatch error (#4982)\n\n¢ Fix torch.randperm whtn using PyTorch 1.8 (#5014)\n\n* Fix empty bbox error in mask_head when using CARAFE (#5062)\n¢ Fix supplement_mask bug when there are zero-size Rols (#5065)\n\n« Fix testing with empty rois in Rol Heads (#5081)\n\n32.9 v2.11.0 (01/4/2021)\n\nHighlights\n* Support new method: Localization Distillation for Object Detection\n* Support Pytorch2ONNX with batch inference and dynamic shape\nNew Features\n¢ Support Localization Distillation for Object Detection (#4758)\n\n* Support Pytorch2ONNX with batch inference and dynamic shape for Faster-RCNN and mainstream one-stage\ndetectors (#4796)\n\nImprovements\n\n* Support batch inference in head of RetinaNet (#4699)\n\n32.9. v2.11.0 (01/4/2021) 151\n", "vlm_text": "32.8.4 Improvements \n• Use MMCV  MODEL REGISTRY  (#5059)\n\n • Unified parameter initialization for more flexible usage (#4750)\n\n • Rename variable names and fix docstring in anchor head (#4883)\n\n • Support training with empty GT in Cascade RPN (#4928)\n\n • Add more details of usage of  test robustness  in documentation (#4917)\n\n • Changing to use  py coco tools  instead of  mm py coco tools  to fully support Detectron2 and MM Detection in one environment (#4939)\n\n • Update torch serve dockerfile to support dockers of more versions (#4954)\n\n • Add check for training with single class dataset (#4973)\n\n • Refactor transformer and DETR Head (#4763)\n\n • Update FPG model zoo (#5079)\n\n • More accurate mask AP of small/medium/large instances (#4898)\n\n \n32.8.5 Bug Fixes \n• Fix bug in mean_ap.py when calculating mAP by 11 points (#4875)\n\n • Fix error when key  meta  is not in old checkpoints (#4936)\n\n • Fix hanging bug when training with empty GT in VFNet, GFL, and FCOS by changing the place of  reduce mean (#4923, #4978, #5058)\n\n • Fix a sync roni zed inference error and provide related demo (#4941)\n\n • Fix IoU losses dimensionality unmatch error (#4982)\n\n • Fix torch.randperm whtn using PyTorch 1.8 (#5014)\n\n • Fix empty bbox error in  mask_head  when using CARAFE (#5062)\n\n • Fix  supplement mask  bug when there are zero-size RoIs (#5065)\n\n • Fix testing with empty rois in RoI Heads (#5081)\n\n \n32.9 v2.11.0 (01/4/2021) \nHighlights \n• Support new method:  Localization Distillation for Object Detection\n\n • Support Py torch 2 ON NX with batch inference and dynamic shape\n\n \nNew Features \n• Support  Localization Distillation for Object Detection  (#4758)\n\n • Support Py torch 2 ON NX with batch inference and dynamic shape for Faster-RCNN and mainstream one-stage detectors (#4796)\n\n \nImprovements \n• Support batch inference in head of RetinaNet (#4699) "}
{"page": 159, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_159.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n« Add batch dimension in second stage of Faster-RCNN (#4785)\n* Support batch inference in bbox coder (#4721)\n¢ Add check for ann_ids in COCODataset to ensure it is unique (#4789)\n* support for showing the FPN results (#4716)\n* support dynamic shape for grid_anchor (#4684)\n* Move pycocotools version check to when it is used (#4880)\nBug Fixes\n« Fix a bug of TridentNet when doing the batch inference (#4717)\n¢ Fix a bug of Pytorch2ONNX in FASF (#4735)\n¢ Fix a bug when show the image with float type (#4732)\n\n32.10 v2.10.0 (01/03/2021)\n\n32.10.1 Highlights\n\n* Support new methods: FPG\n* Support ONNX2TensorRT for SSD, FSAF, FCOS, YOLOv3, and Faster R-CNN.\n\n32.10.2 New Features\n\n* Support ONNX2TensorRT for SSD, FSAF, FCOS, YOLOv3, and Faster R-CNN (#4569)\n¢ Support Feature Pyramid Grids (FPG) (#4645)\n\n* Support video demo (#4420)\n\n« Add seed option for sampler (#4665)\n\n* Support to customize type of runner (#4570, #4669)\n\n* Support synchronizing BN buffer in EvalHook (#4582)\n\n« Add script for GIF demo (#4573)\n\n32.10.3 Bug Fixes\n\n¢ Fix ConfigDict AttributeError and add Colab link (#4643)\n¢ Avoid crash in empty gt training of GFL head (#4631)\n¢ Fix iou_thrs bug in RPN evaluation (#4581)\n\n¢ Fix syntax error of config when upgrading model version (#4584)\n\n152 Chapter 32. Changelog\n", "vlm_text": "• Add batch dimension in second stage of Faster-RCNN (#4785)\n\n • Support batch inference in bbox coder (#4721)\n\n • Add check for  ann_ids  in  COCO Data set  to ensure it is unique (#4789)\n\n • support for showing the FPN results (#4716)\n\n • support dynamic shape for grid anchor (#4684)\n\n • Move py coco tools version check to when it is used (#4880)\n\n \nBug Fixes \n• Fix a bug of TridentNet when doing the batch inference (#4717)\n\n • Fix a bug of Py torch 2 ON NX in FASF (#4735)\n\n • Fix a bug when show the image with float type (#4732)\n\n \n32.10 v2.10.0 (01/03/2021) \n32.10.1 Highlights \n• Support new methods:  FPG\n\n • Support ON NX 2 Tensor RT for SSD, FSAF, FCOS, YOLOv3, and Faster R-CNN.\n\n \n32.10.2 New Features \n• Support ON NX 2 Tensor RT for SSD, FSAF, FCOS, YOLOv3, and Faster R-CNN (#4569)\n\n • Support  Feature Pyramid Grids (FPG)  (#4645)\n\n • Support video demo (#4420)\n\n • Add seed option for sampler (#4665)\n\n • Support to customize type of runner (#4570, #4669)\n\n • Support synchronizing BN buffer in  EvalHook  (#4582)\n\n • Add script for GIF demo (#4573)\n\n \n32.10.3 Bug Fixes \n• Fix ConfigDict Attribute Error and add Colab link (#4643)\n\n • Avoid crash in empty gt training of GFL head (#4631)\n\n • Fix  iou_thrs  bug in RPN evaluation (#4581)\n\n • Fix syntax error of config when upgrading model version (#4584) "}
{"page": 160, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_160.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.10.4 Improvements\n\n¢ Refactor unit test file structures (#4600)\n\n* Refactor nms config (#4636)\n\n* Get loading pipeline by checking the class directly rather than through config strings (#4619)\n« Add doctests for mask target generation and mask structures (#4614)\n\n* Use deep copy when copying pipeline arguments (#4621)\n\n* Update documentations (#4642, #4650, #4620, #4630)\n\n* Remove redundant code calling import_modules_from_strings (#4601)\n\n* Clean deprecated FP16 API (#4571)\n\n* Check whether CLASSES is correctly initialized in the initialization of XMLDataset (#4555)\n¢ Support batch inference in the inference API (#4462, #4526)\n\n* Clean deprecated warning and fix ‘meta’ error (#4695)\n\n32.11 v2.9.0 (01/02/2021)\n\n32.11.1 Highlights\n\n* Support new methods: SCNet, Sparse R-CNN\n* Move train_cfg and test_cfg into model in configs\n\n* Support to visualize results based on prediction quality\n\n32.11.2 New Features\n\n* Support SCNet (#4356)\n\n* Support Sparse R-CNN (#4219)\n\n* Support evaluate mAP by multiple IoUs (#4398)\n* Support concatenate dataset for testing (#4452)\n\n* Support to visualize results based on prediction quality (#4441)\n\nAdd ONNX simplify option to Pytorch2ONNX script (#4468)\n\nAdd hook for checking compatibility of class numbers in heads and datasets (#4508)\n\n32.11. v2.9.0 (01/02/2021) 153\n", "vlm_text": "32.10.4 Improvements \n• Refactor unit test file structures (#4600)\n\n • Refactor nms config (#4636)\n\n • Get loading pipeline by checking the class directly rather than through config strings (#4619)\n\n • Add doctests for mask target generation and mask structures (#4614)\n\n • Use deep copy when copying pipeline arguments (#4621)\n\n • Update documentations (#4642, #4650, #4620, #4630)\n\n • Remove redundant code calling  import modules from strings  (#4601)\n\n • Clean deprecated FP16 API (#4571)\n\n • Check whether  CLASSES  is correctly initialized in the initialization of  XMLDataset  (#4555)\n\n • Support batch inference in the inference API (#4462, #4526)\n\n • Clean deprecated warning and fix ‘meta’ error (#4695)\n\n \n32.11 v2.9.0 (01/02/2021) \n32.11.1 Highlights \n• Support new methods:  SCNet ,  Sparse R-CNN\n\n \n• Move  train_cfg  and  test_cfg  into model in configs\n\n • Support to visualize results based on prediction quality\n\n \n32.11.2 New Features \n\n• Support  Sparse R-CNN  (#4219)\n\n • Support evaluate mAP by multiple IoUs (#4398)\n\n • Support concatenate dataset for testing (#4452)\n\n • Support to visualize results based on prediction quality (#4441)\n\n • Add ONNX simplify option to Py torch 2 ON NX script (#4468)\n\n • Add hook for checking compatibility of class numbers in heads and datasets (#4508) "}
{"page": 161, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_161.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.11.3 Bug Fixes\n\n¢ Fix CPU inference bug of Cascade RPN (#4410)\n\n¢ Fix NMS error of CornerNet when there is no prediction box (#4409)\n\n¢ Fix TypeError in CornerNet inference (#4411)\n\n* Fix bug of PAA when training with background images (#4391)\n\n« Fix the error that the window data is not destroyed when out_file is not None and show==False (#4442)\n* Fix order of NMS score_factor that will decrease the performance of YOLOv3 (#4473)\n\n¢ Fix bug in HTC TTA when the number of detection boxes is 0 (#4516)\n\n¢ Fix resize error in mask data structures (#4520)\n\n32.11.4 Improvements\n\n¢ Allow to customize classes in LVIS dataset (#4382)\n\n* Add tutorials for building new models with existing datasets (#4396)\n\n« Add CPU compatibility information in documentation (#4405)\n\n« Add documentation of deprecated ImageToTensor for batch inference (#4408)\n\n« Add more details in documentation for customizing dataset (#4430)\n\n* Switch imshow_det_bboxes visualization backend from OpenCV to Matplotlib (#4389)\n* Deprecate ImageToTensor in image_demo.py (#4400)\n\n* Move train_cfg/test_cfg into model (#4347, #4489)\n\n* Update docstring for reg_decoded_bbox option in bbox heads (#4467)\n\n* Update dataset information in documentation (#4525)\n\n* Release pre-trained R50 and R101 PAA detectors with multi-scale 3x training schedules (#4495)\n« Add guidance for speed benchmark (#4537)\n\n32.12 v2.8.0 (04/01/2021)\n\n32.12.1 Highlights\n\n* Support new methods: Cascade RPN, TridentNet\n\n154 Chapter 32. Changelog\n", "vlm_text": "32.11.3 Bug Fixes \n• Fix CPU inference bug of Cascade RPN (#4410)\n\n • Fix NMS error of CornerNet when there is no prediction box (#4409)\n\n • Fix TypeError in CornerNet inference (#4411)\n\n • Fix bug of PAA when training with background images (#4391)\n\n • Fix the error that the window data is not destroyed when  out_file is not None  and  show  $==$  False  (#4442)\n\n • Fix order of NMS  score factor  that will decrease the performance of YOLOv3 (#4473)\n\n • Fix bug in HTC TTA when the number of detection boxes is 0 (#4516)\n\n • Fix resize error in mask data structures (#4520)\n\n \n32.11.4 Improvements \n• Allow to customize classes in LVIS dataset (#4382)\n\n • Add tutorials for building new models with existing datasets (#4396)\n\n • Add CPU compatibility information in documentation (#4405)\n\n • Add documentation of deprecated  Image To Tensor  for batch inference (#4408)\n\n • Add more details in documentation for customizing dataset (#4430)\n\n • Switch  im show det b boxes  visualization backend from OpenCV to Matplotlib (#4389)\n\n • Deprecate  Image To Tensor  in  image_demo.py  (#4400)\n\n • Move train_cfg/test_cfg into model (#4347, #4489)\n\n • Update docstring for  reg decoded b box  option in bbox heads (#4467)\n\n • Update dataset information in documentation (#4525)\n\n • Release pre-trained R50 and R101 PAA detectors with multi-scale  $3\\mathbf{X}$   training schedules (#4495)\n\n • Add guidance for speed benchmark (#4537)\n\n \n32.12 v2.8.0 (04/01/2021) \n32.12.1 Highlights \n• Support new methods:  Cascade RPN ,  TridentNet "}
{"page": 162, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_162.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.12.2 New Features\n\n* Support Cascade RPN (#1900)\n* Support TridentNet (#3313)\n\n32.12.3 Bug Fixes\n\n¢ Fix bug of show result in async_benchmark (#4367)\n\n¢ Fix scale factor in MaskTestMixin (#4366)\n\n¢ Fix but when returning indices in multiclass_nms (#4362)\n\n¢ Fix bug of empirical attention in resnext backbone error (#4300)\n\n* Fix bug of img_norm_cfg in FCOS-HRNet models with updated performance and models (#4250)\nFix invalid checkpoint and log in Mask R-CNN models on Cityscapes dataset (#4287)\n\n¢ Fix bug in distributed sampler when dataset is too small (#4257)\n\n* Fix bug of ‘PAFPN has no attribute extra_convs_on_inputs’ (#4235)\n\n32.12.4 Improvements\n\n* Update model url from aws to aliyun (#4349)\n\n* Update ATSS for PyTorch 1.6+ (#4359)\n\n* Update script to install ruby in pre-commit installation (#4360)\n\n* Delete deprecated mmdet .ops (#4325)\n\n* Refactor hungarian assigner for more general usage in Sparse R-CNN (#4259)\n* Handle scipy import in DETR to reduce package dependencies (#4339)\n\n* Update documentation of usages for config options after MMCV (1.2.3) supports overriding list in config (#4326)\n* Update pre-train models of faster renn trained on COCO subsets (#4307)\n\n* Avoid zero or too small value for beta in Dynamic R-CNN (#4303)\n\n« Add doccumentation for Pytorch2ONNX (#4271)\n\n« Add deprecated warning FPN arguments (#4264)\n\n¢ Support returning indices of kept bboxes when using nms (#4251)\n\n* Update type and device requirements when creating tensors GFLHead (#4210)\n\n* Update device requirements when creating tensors in CrossEntropyLoss (#4224)\n\n32.12. v2.8.0 (04/01/2021) 155\n", "vlm_text": "32.12.2 New Features \n• Support  Cascade RPN  (#1900)\n\n • Support  TridentNet  (#3313)\n\n \n32.12.3 Bug Fixes \n• Fix bug of show result in a sync benchmark (#4367)\n\n • Fix scale factor in Mask Test Mix in (#4366)\n\n • Fix but when returning indices in  multi class nm s  (#4362)\n\n • Fix bug of empirical attention in resnext backbone error (#4300)\n\n • Fix bug of  img norm cf g  in FCOS-HRNet models with updated performance and models (#4250)\n\n • Fix invalid checkpoint and log in Mask R-CNN models on Cityscapes dataset (#4287)\n\n • Fix bug in distributed sampler when dataset is too small (#4257)\n\n • Fix bug of ‘PAFPN has no attribute extra con vs on inputs’ (#4235)\n\n \n32.12.4 Improvements \n• Update model url from aws to aliyun (#4349)\n\n • Update ATSS for PyTorch   $1.6+$   (#4359)\n\n • Update script to install ruby in pre-commit installation (#4360)\n\n • Delete deprecated  mmdet.ops  (#4325)\n\n • Refactor hungarian assigner for more general usage in Sparse R-CNN (#4259)\n\n • Handle scipy import in DETR to reduce package dependencies (#4339)\n\n • Update documentation of usages for config options after MMCV (1.2.3) supports overriding list in config (#4326)\n\n • Update pre-train models of faster rcnn trained on COCO subsets (#4307)\n\n • Avoid zero or too small value for beta in Dynamic R-CNN (#4303)\n\n • Add doc cu ment ation for Py torch 2 ON NX (#4271)\n\n • Add deprecated warning FPN arguments (#4264)\n\n • Support returning indices of kept bboxes when using nms (#4251)\n\n • Update type and device requirements when creating tensors  GFLHead  (#4210)\n\n • Update device requirements when creating tensors in  Cross Entropy Loss  (#4224) "}
{"page": 163, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_163.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.13 v2.7.0 (30/11/2020)\n\n* Support new method: DETR, ResNest, Faster RC-CNN DCS.\n* Support YOLO, Mask R-CNN, and Cascade R-CNN models exportable to ONNX.\n\n32.13.1 New Features\n\nSupport DETR (#4201, #4206)\nSupport to link the best checkpoint in training (#3773)\n\nSupport to override config through options in inference.py (#4175)\n\nSupport YOLO, Mask R-CNN, and Cascade R-CNN models exportable to ONNX (#4087, #4083)\n\nSupport ResNeSt backbone (#2959)\n\nSupport unclip border bbox regression (#4076)\n\nAdd tpfp func in evaluating AP (#4069)\n\nSupport mixed precision training of SSD detector with other backbones (#4081)\nAdd Faster R-CNN DC5 models (#4043)\n\n32.13.2 Bug Fixes\n\nFix bug of gpu_id in distributed training mode (#4163)\nSupport Albumentations with version higher than 0.5 (#4032)\nFix num_classes bug in faster renn config (#4088)\n\nUpdate code in docs/2_new_data_model.md (#4041)\n\n32.13.3 Improvements\n\nEnsure DCN offset to have similar type as features in VFNet (#4198)\nAdd config links in README files of models (#4190)\n\nAdd tutorials for loss conventions (#3818)\n\nAdd solution to installation issues in 30-series GPUs (#4176)\n\nUpdate docker version in get_started.md (#4145)\n\nAdd model statistics and polish some titles in configs README (#4140)\nClamp neg probability in FreeAnchor (#4082)\n\nSpeed up expanding large images (#4089)\n\nFix Pytorch 1.7 incompatibility issues (#4103)\n\nUpdate trouble shooting page to resolve segmentation fault (#4055)\nUpdate aLRP-Loss in project page (#4078)\n\nClean duplicated reduce_mean function (#4056)\n\nRefactor Q&A (#4045)\n\n156\n\nChapter 32. Changelog\n", "vlm_text": "32.13 v2.7.0 (30/11/2020) \n• Support new method:  DETR ,  ResNest , Faster R-CNN DC5.\n\n • Support YOLO, Mask R-CNN, and Cascade R-CNN models exportable to ONNX.\n\n \n32.13.1 New Features \n• Support  DETR  (#4201, #4206)\n\n • Support to link the best checkpoint in training (#3773)\n\n • Support to override config through options in inference.py (#4175)\n\n • Support YOLO, Mask R-CNN, and Cascade R-CNN models exportable to ONNX (#4087, #4083)\n\n • Support  ResNeSt  backbone (#2959)\n\n • Support unclip border bbox regression (#4076)\n\n • Add tpfp func in evaluating AP (#4069)\n\n • Support mixed precision training of SSD detector with other backbones (#4081)\n\n • Add Faster R-CNN DC5 models (#4043)\n\n \n32.13.2 Bug Fixes \n• Fix bug of  gpu_id  in distributed training mode (#4163)\n\n • Support Albumen tat ions with version higher than 0.5 (#4032)\n\n • Fix num classes bug in faster rcnn config (#4088)\n\n • Update code in docs/2 new data model.md (#4041)\n\n \n32.13.3 Improvements \n• Ensure DCN offset to have similar type as features in VFNet (#4198)\n\n • Add config links in README files of models (#4190)\n\n • Add tutorials for loss conventions (#3818)\n\n • Add solution to installation issues in 30-series GPUs (#4176)\n\n • Update docker version in get started.md (#4145)\n\n • Add model statistics and polish some titles in configs README (#4140)\n\n • Clamp neg probability in FreeAnchor (#4082)\n\n • Speed up expanding large images (#4089)\n\n • Fix Pytorch 1.7 incompatibility issues (#4103)\n\n • Update trouble shooting page to resolve segmentation fault (#4055)\n\n • Update aLRP-Loss in project page (#4078)\n\n • Clean duplicated  reduce mean  function (#4056)\n\n • Refactor Q&A (#4045) "}
{"page": 164, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_164.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.14 v2.6.0 (1/11/2020)\n\nSupport new method: VarifocalNet.\n\nRefactored documentation with more tutorials.\n\n32.14.1 New Features\n\nSupport GIoU calculation in BboxOverlaps2D, and re-implement giou_loss using bbox_overlaps (#3936)\nSupport random sampling in CPU mode (#3948)\nSupport VarifocalNet (#3666, #4024)\n\n32.14.2 Bug Fixes\n\nFix SABL validating bug in Cascade R-CNN (#3913)\n\nAvoid division by zero in PAA head when num_pos=0 (#3938)\n\nFix temporary directory bug of multi-node testing error (#4034, #4017)\nFix --show-dir option in test script (#4025)\n\nFix GA-RetinaNet r50 model url (#3983)\n\nUpdate code in docs and fix broken urls (#3947)\n\n32.14.3 Improvements\n\nRefactor pytorch2onnx API into mmdet.core.export and use generate_inputs_and_wrap_model for py-\ntorch2onnx (#3857, #3912)\n\nUpdate RPN upgrade scripts for v2.5.0 compatibility (#3986)\n\nUse mmcv tensor2imgs (#4010)\n\nUpdate test robustness (#4000)\n\nUpdate trouble shooting page (#3994)\n\nAccelerate PAA training speed (#3985)\n\nSupport batch_size > 1 in validation (#3966)\n\nUse RoIAlign implemented in MMCV for inference in CPU mode (#3930)\n\nDocumentation refactoring (#4031)\n\n32.14. v2.6.0 (1/11/2020) 157\n", "vlm_text": "32.14 v2.6.0 (1/11/2020) \n• Support new method:  Var i focal Net .\n\n • Refactored documentation with more tutorials.\n\n \n32.14.1 New Features \n• Support GIoU calculation in  B box Overlaps 2 D , and re-implement  giou_loss  using  b box overlaps  (#3936)\n\n • Support random sampling in CPU mode (#3948)\n\n • Support VarifocalNet (#3666, #4024)\n\n \n32.14.2 Bug Fixes \n• Fix SABL validating bug in Cascade R-CNN (#3913)\n\n • Avoid division by zero in PAA head when num_pos  $\\mathrm{=}0$   (#3938)\n\n • Fix temporary directory bug of multi-node testing error (#4034, #4017)\n\n • Fix  --show-dir  option in test script (#4025)\n\n • Fix GA-RetinaNet r50 model url (#3983)\n\n • Update code in docs and fix broken urls (#3947)\n\n \n32.14.3 Improvements \n• Refactor py torch 2 on nx API into  mmdet.core.export  and use  generate inputs and wrap model  for py- torch2onnx (#3857, #3912)\n\n \n• Update RPN upgrade scripts for v2.5.0 compatibility (#3986)\n\n \n• Use mmcv  tensor 2 img s  (#4010)\n\n \n• Update test robustness (#4000)\n\n \n• Update trouble shooting page (#3994)\n\n \n• Accelerate PAA training speed (#3985)\n\n \n• Support batch_size  $>1$   in validation (#3966)\n\n \n• Use RoIAlign implemented in MMCV for inference in CPU mode (#3930) "}
{"page": 165, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_165.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.15 v2.5.0 (5/10/2020)\n\n32.15.1 Highlights\n\n¢ Support new methods: YOLACT, CentripetalNet.\n\n« Add more documentations for easier and more clear usage.\n\n32.15.2 Backwards Incompatible Changes\n\nFP16 related methods are imported from mmcv instead of mmdet. (#3766, #3822) Mixed precision training utils\nin mmdet.core.fp16 are moved to mmcv.runner, including force_fp32, auto_fp16, wrap_fp16_model, and\nFp160ptimizerHook. A deprecation warning will be raised if users attempt to import those methods from mmdet .\ncore. fp16, and will be finally removed in V2.10.0.\n\n[0, N-1] represents foreground classes and N indicates background classes for all models. (#3221) Before v2.5.0,\nthe background label for RPN is 0, and N for other heads. Now the behavior is consistent for all models. Thus self.\nbackground_labels in dense_heads is removed and all heads use self.num_classes to indicate the class index\nof background labels. This change has no effect on the pre-trained models in the v2.x model zoo, but will affect the\ntraining of all models with RPN heads. Two-stage detectors whose RPN head uses softmax will be affected because\nthe order of categories is changed.\n\nOnly call get_subset_by_classes when test_mode=True and self. filter_empty_gt=True (#3695) Func-\ntion get_subset_by_classes in dataset is refactored and only filters out images when test_mode=True and self.\nfilter_empty_gt=True. In the original implementation, get_subset_by_classes is not related to the flag self.\nf£ilter_empty_gt and will only be called when the classes is set during initialization no matter test_mode is True or\nFalse. This brings ambiguous behavior and potential bugs in many cases. After v2.5.0, if filter_empty_gt=False,\nno matter whether the classes are specified in a dataset, the dataset will use all the images in the annotations. If\nfilter_empty_gt=True and test_mode=True, no matter whether the classes are specified, the dataset will call\n“get_subset_by_classes* to check the images and filter out images containing no GT boxes. Therefore, the users should\nbe responsible for the data filtering/cleaning process for the test dataset.\n\n32.15.3 New Features\n\n* Test time augmentation for single stage detectors (#3844, #3638)\n\n* Support to show the name of experiments during training (#3764)\n\n« Add Shear, Rotate, Translate Augmentation (#3656, #3619, #3687)\n\n« Add image-only transformations including Constrast, Equalize, Color, and Brightness. (#3643)\n* Support YOLACT (#3456)\n\n* Support CentripetalNet (#3390)\n\n* Support PyTorch 1.6 in docker (#3905)\n\n158 Chapter 32. Changelog\n", "vlm_text": "32.15 v2.5.0 (5/10/2020) \n32.15.1 Highlights \n• Support new methods:  YOLACT ,  Centripetal Net . • Add more documentation s for easier and more clear usage. \n32.15.2 Backwards Incompatible Changes \nFP16 related methods are imported from mmcv instead of mmdet. (#3766, #3822)  Mixed precision training utils in  mmdet.core.fp16  are moved to  mmcv.runner , including  force_fp32 ,  auto_fp16 ,  wrap fp 16 model , and Fp 16 Optimizer Hook . A deprecation warning will be raised if users attempt to import those methods from  mmdet. core.fp16 , and will be finally removed in V2.10.0. \n[0, N-1] represents foreground classes and N indicates background classes for all models. (#3221)  Before v2.5.0, the background label for RPN is 0, and N for other heads. Now the behavior is consistent for all models. Thus  self. background labels  in  dense heads  is removed and all heads use  self.num classes  to indicate the class index of background labels. This change has no effect on the pre-trained models in the  $\\mathrm{v}2.\\mathrm{x}$   model zoo, but will affect the training of all models with RPN heads. Two-stage detectors whose RPN head uses softmax will be affected because the order of categories is changed. \nOnly call  get subset by classes  when  test_mode  $\\risingdotseq$  True  and  self.filter empty gt  $\\risingdotseq$  True  (#3695)  Func- tion  get subset by classes  in dataset is refactored and only filters out images when  test_mode  $=$  True  and  self. filter empty gt  $\\risingdotseq$  True . In the original implementation,  get subset by classes  is not related to the flag  self. filter empty gt  and will only be called when the classes is set during initialization no matter  test_mode  is  True  or False . This brings ambiguous behavior and potential bugs in many cases. After v2.5.0, if  filter empty gt  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  False , no matter whether the classes are specified in a dataset, the dataset will use all the images in the annotations. If filter empty gt  $\\risingdotseq$  True  and  test_mode  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  True , no matter whether the classes are specified, the dataset will call \\`\\`get subset by classes\\` to check the images and filter out images containing no GT boxes. Therefore, the users should be responsible for the data filtering/cleaning process for the test dataset. \n32.15.3 New Features \n• Test time augmentation for single stage detectors (#3844, #3638) • Support to show the name of experiments during training (#3764) • Add Shear, Rotate, Translate Augmentation (#3656, #3619, #3687)• Add image-only transformations including  Constrast ,  Equalize ,  Color , and  Brightness . (#3643) • Support  YOLACT  (#3456) • Support  Centripetal Net  (#3390) • Support PyTorch 1.6 in docker (#3905) "}
{"page": 166, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_166.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.15.4 Bug Fixes\n\n¢ Fix the bug of training ATSS when there is no ground truth boxes (#3702)\n\n¢ Fix the bug of using Focal Loss when there is num_pos is 0 (#3702)\n\n* Fix the label index mapping in dataset browser (#3708)\n\n¢ Fix Mask R-CNN training stuck problem when their is no positive rois (#3713)\n\n¢ Fix the bug of self. rpn_head.test_cfg in RPNTestMixin by using self. rpn_head in rpn head (#3808)\n* Fix deprecated Conv2d from mmcv.ops (#3791)\n\n* Fix device bug in RepPoints (#3836)\n\n* Fix SABL validating bug (#3849)\n\n* Use https: //download.openmmlab.com/mmcv/dist/index.html for installing MMCV (#3840)\n¢ Fix nonzero in NMS for PyTorch 1.6.0 (#3867)\n\n« Fix the API change bug of PAA (#3883)\n\n« Fix typo in bbox_flip (#3886)\n\n* Fix cv2 import error of ligGL.so.1 in Dockerfile (#3891)\n\n32.15.5 Improvements\n* Change to use mmcv.utils.collect_env for collecting environment information to avoid duplicate codes\n(#3779)\n* Update checkpoint file names to v2.0 models in documentation (#3795)\n* Update tutorials for changing runtime settings (#3778), modifying loss (#3777)\n¢ Improve the function of simple_test_bboxes in SABL (#3853)\n* Convert mask to bool before using it as img’s index for robustness and speedup (#3870)\n\n¢ Improve documentation of modules and dataset customization (#3821)\n\n32.16 v2.4.0 (5/9/2020)\n\nHighlights\n« Fix lots of issues/bugs and reorganize the trouble shooting page\n* Support new methods SABL, YOLOv3, and PAA Assign\n* Support Batch Inference\n* Start to publish mmdet package to PyPI since v2.3.0\n* Switch model zoo to download.openmmlab.com\nBackwards Incompatible Changes\n\n* Support Batch Inference (#3564, #3686, #3705): Since v2.4.0, MMDetection could inference model with mul-\ntiple images in a single GPU. This change influences all the test APIs in MMDetection and downstream code-\nbases. To help the users migrate their code, we use replace_ImageToTensor (#3686) to convert legacy test\ndata pipelines during dataset initialization.\n\n32.16. v2.4.0 (5/9/2020) 159\n", "vlm_text": "32.15.4 Bug Fixes \n• Fix the bug of training ATSS when there is no ground truth boxes (#3702)\n\n • Fix the bug of using Focal Loss when there is  num_pos  is 0 (#3702)\n\n • Fix the label index mapping in dataset browser (#3708)\n\n • Fix Mask R-CNN training stuck problem when their is no positive rois (#3713)\n\n • Fix the bug of  self.rpn_head.test_cfg  in  RP N Test Mix in  by using  self.rpn_head  in rpn head (#3808)\n\n • Fix deprecated  Conv2d  from mmcv.ops (#3791)\n\n • Fix device bug in RepPoints (#3836)\n\n • Fix SABL validating bug (#3849)\n\n • Use  https://download.openmmlab.com/mmcv/dist/index.html  for installing MMCV (#3840)\n\n • Fix nonzero in NMS for PyTorch 1.6.0 (#3867)\n\n • Fix the API change bug of PAA (#3883)\n\n • Fix typo in bbox_flip (#3886)\n\n • Fix cv2 import error of ligGL.so.1 in Dockerfile (#3891)\n\n \n32.15.5 Improvements \n• Change to use  mmcv.utils.collect en v  for collecting environment information to avoid duplicate codes (#3779)\n\n • Update checkpoint file names to   $\\mathrm{v}2.0$   models in documentation (#3795)\n\n • Update tutorials for changing runtime settings (#3778), modifying loss (#3777)\n\n • Improve the function of  simple test b boxes  in SABL (#3853)\n\n • Convert mask to bool before using it as img’s index for robustness and speedup (#3870)\n\n • Improve documentation of modules and dataset customization (#3821)\n\n \n32.16 v2.4.0 (5/9/2020) \nHighlights \n• Fix lots of issues/bugs and reorganize the trouble shooting page\n\n • Support new methods  SABL ,  YOLOv3 , and  PAA Assign\n\n • Support Batch Inference\n\n • Start to publish  mmdet  package to PyPI since v2.3.0\n\n • Switch model zoo to download.openmmlab.com\n\n \nBackwards Incompatible Changes \n• Support Batch Inference (#3564, #3686, #3705): Since v2.4.0, MM Detection could inference model with mul- tiple images in a single GPU. This change influences all the test APIs in MM Detection and downstream code- bases. To help the users migrate their code, we use  replace Image To Tensor  (#3686) to convert legacy test data pipelines during dataset initialization. "}
{"page": 167, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_167.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Support RandomFlip with horizontal/vertical/diagonal direction (#3608): Since v2.4.0, MMDetection supports\nhorizontal/vertical/diagonal flip in the data augmentation. This influences bounding box, mask, and image trans-\nformations in data augmentation process and the process that will map those data back to the original format.\n\n* Migrate to use mmlvis and mmpycocotools for COCO and LVIS dataset (#3727). The APIs are fully compatible\nwith the original lvis and pycocotools. Users need to uninstall the existing pycocotools and lvis packages in\ntheir environment first and install mmlvis & mmpycocotools.\n\nBug Fixes\n¢ Fix default mean/std for onnx (#3491)\n* Fix coco evaluation and add metric items (#3497)\n* Fix typo for install.md (#3516)\n¢ Fix atss when sampler per gpu is 1 (#3528)\n« Fix import of fuse_conv_bn (#3529)\n* Fix bug of gaussian_target, update unittest of heatmap (#3543)\n¢ Fixed VOC2012 evaluate (#3553)\nFix scale factor bug of rescale (#3566)\n¢ Fix with_xxx_attributes in base detector (#3567)\n* Fix boxes scaling when number is 0 (#3575)\nFix rfp check when neck config is a list (#3591)\n¢ Fix import of fuse conv bn in benchmark.py (#3606)\n¢ Fix webcam demo (#3634)\n* Fix typo and itemize issues in tutorial (#3658)\n« Fix error in distributed training when some levels of FPN are not assigned with bounding boxes (#3670)\n¢ Fix the width and height orders of stride in valid flag generation (#3685)\n¢ Fix weight initialization bug in Res2Net DCN (#3714)\n¢ Fix bug in OHEMSampler (#3677)\nNew Features\n¢ Support Cutout augmentation (#3521)\n¢ Support evaluation on multiple datasets through ConcatDataset (#3522)\n* Support PAA assign #(3547)\n¢ Support eval metric with pickle results (#3607)\n* Support YOLOv3 (#3083)\n* Support SABL (#3603)\n* Support to publish to Pypi in github-action (#3510)\n* Support custom imports (#3641)\nImprovements\n¢ Refactor common issues in documentation (#3530)\n\n« Add pytorch 1.6 to Cl config (#3532)\n\n160 Chapter 32. Changelog\n", "vlm_text": "• Support RandomFlip with horizontal/vertical/diagonal direction (#3608): Since v2.4.0, MM Detection supports horizontal/vertical/diagonal flip in the data augmentation. This influences bounding box, mask, and image trans- formations in data augmentation process and the process that will map those data back to the original format.\n\n • Migrate to use  mmlvis  and  mm py coco tools  for COCO and LVIS dataset (#3727). The APIs are fully compatible with the original  lvis  and  py coco tools . Users need to uninstall the existing py coco tools and lvis packages in their environment first and install  mmlvis  &  mm py coco tools .\n\n \nBug Fixes \n• Fix default mean/std for onnx (#3491)\n\n • Fix coco evaluation and add metric items (#3497)\n\n • Fix typo for install.md (#3516)\n\n • Fix atss when sampler per gpu is 1 (#3528)\n\n • Fix import of fuse con v bn (#3529)\n\n • Fix bug of gaussian target, update unittest of heatmap (#3543)\n\n • Fixed VOC2012 evaluate (#3553)\n\n • Fix scale factor bug of rescale (#3566)\n\n • Fix with xxx attributes in base detector (#3567)\n\n • Fix boxes scaling when number is 0 (#3575)\n\n • Fix rfp check when neck config is a list (#3591)\n\n • Fix import of fuse conv bn in benchmark.py (#3606)\n\n • Fix webcam demo (#3634)\n\n • Fix typo and itemize issues in tutorial (#3658)\n\n • Fix error in distributed training when some levels of FPN are not assigned with bounding boxes (#3670)\n\n • Fix the width and height orders of stride in valid flag generation (#3685)\n\n • Fix weight initialization bug in Res2Net DCN (#3714)\n\n • Fix bug in OH EM Sampler (#3677)\n\n \nNew Features \n• Support Cutout augmentation (#3521)\n\n • Support evaluation on multiple datasets through Con cat Data set (#3522)\n\n • Support  PAA assign  #(3547)\n\n • Support eval metric with pickle results (#3607)\n\n • Support  YOLOv3  (#3083)\n\n • Support  SABL  (#3603)\n\n • Support to publish to Pypi in github-action (#3510)\n\n • Support custom imports (#3641)\n\n \nImprovements \n• Refactor common issues in documentation (#3530)\n\n • Add pytorch 1.6 to CI config (#3532) "}
{"page": 168, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_168.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n« Add config to runner meta (#3534)\n\n¢ Add eval-option flag for testing (#3537)\n\n¢ Add init_eval to evaluation hook (#3550)\n\n¢ Add include_bkg in ClassBalancedDataset (#3577)\n\n* Using config’s loading in inference_detector (#3611)\n« Add ATSS ResNet-101 models in model zoo (#3639)\n* Update urls to download.openmmlab.com (#3665)\n\n* Support non-mask training for CocoDataset (#3711)\n\n32.17 v2.3.0 (5/8/2020)\n\nHighlights\n\n* The CUDA/C++ operators have been moved to mmcv.ops. For backward compatibility mmdet.ops is kept as\nwarppers of mmcv . ops.\n\n* Support new methods CornerNet, DIOU/CIOU loss, and new dataset: LVIS V1\n* Provide more detailed colab training tutorials and more complete documentation.\n* Support to convert RetinaNet from Pytorch to ONNX.\nBug Fixes\n¢ Fix the model initialization bug of DetectoRS (#3187)\n¢ Fix the bug of module names in NASFCOSHead (#3205)\n« Fix the filename bug in publish_model.py (#3237)\n¢ Fix the dimensionality bug when inside_flags.any() is False in dense heads (#3242)\n« Fix the bug of forgetting to pass flip directions in MultiScaleFlipAug (#3262)\n¢ Fixed the bug caused by default value of stem_channels (#3333)\n¢ Fix the bug of model checkpoint loading for CPU inference (#3318, #3316)\n¢ Fix topk bug when box number is smaller than the expected topk number in ATSSAssigner (#3361)\n« Fix the gt priority bug in center_region_assigner.py (#3208)\n* Fix NaN issue of iou calculation in iou_loss.py (#3394)\n¢ Fix the bug that iou_thrs is not actually used during evaluation in coco.py (#3407)\n« Fix test-time augmentation of RepPoints (#3435)\n¢ Fix runtimeError caused by incontiguous tensor in Res2Net+DCN (#3412)\nNew Features\n* Support CornerNet (#3036)\n* Support DIOU/CIOU loss (#3151)\n* Support LVIS V1 dataset (#)\n* Support customized hooks in training (#3395)\n\n* Support fp16 training of generalized focal loss (#3410)\n\n32.17. v2.3.0 (5/8/2020) 161\n", "vlm_text": "• Add config to runner meta (#3534)\n\n • Add eval-option flag for testing (#3537)\n\n • Add init_eval to evaluation hook (#3550)\n\n • Add include b kg in Class Balanced Data set (#3577)\n\n • Using config’s loading in inference detector (#3611)\n\n • Add ATSS ResNet-101 models in model zoo (#3639)\n\n • Update urls to download.openmmlab.com (#3665)\n\n • Support non-mask training for Coco Data set (#3711)\n\n \n32.17 v2.3.0 (5/8/2020) \nHighlights \n• The   $\\mathrm{ttCUDA/C++}$   operators have been moved to  mmcv.ops . For backward compatibility  mmdet.ops  is kept as warppers of  mmcv.ops .\n\n • Support new methods  CornerNet ,  DIOU / CIOU  loss, and new dataset:  LVIS V1\n\n • Provide more detailed colab training tutorials and more complete documentation.\n\n • Support to convert RetinaNet from Pytorch to ONNX.\n\n \nBug Fixes \n• Fix the model initialization bug of DetectoRS (#3187)\n\n • Fix the bug of module names in NAS FCO S Head (#3205)\n\n • Fix the filename bug in publish model.py (#3237)\n\n • Fix the dimensionality bug when  inside flags.any()  is  False  in dense heads (#3242)\n\n • Fix the bug of forgetting to pass flip directions in  Multi Scale Flip Aug  (#3262)\n\n • Fixed the bug caused by default value of  stem channels  (#3333)\n\n • Fix the bug of model checkpoint loading for CPU inference (#3318, #3316)\n\n • Fix topk bug when box number is smaller than the expected topk number in AT S S As signer (#3361)\n\n • Fix the gt priority bug in center region as signer.py (#3208)\n\n • Fix NaN issue of iou calculation in iou_loss.py (#3394)\n\n • Fix the bug that  iou_thrs  is not actually used during evaluation in coco.py (#3407)\n\n • Fix test-time augmentation of RepPoints (#3435)\n\n • Fix runtime Error caused by in contiguous tensor in Res2Net+DCN (#3412)\n\n \nNew Features \n• Support  CornerNet  (#3036)\n\n • Support  DIOU / CIOU  loss (#3151)\n\n • Support  LVIS V1  dataset (#)\n\n • Support customized hooks in training (#3395)\n\n • Support fp16 training of generalized focal loss (#3410) "}
{"page": 169, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_169.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Support to convert RetinaNet from Pytorch to ONNX (#3075)\nImprovements\n* Support to process ignore boxes in ATSS assigner (#3082)\n« Allow to crop images without ground truth in RandomCrop (#3153)\n¢ Enable the the Accuracy module to set threshold (#3155)\n* Refactoring unit tests (#3206)\n* Unify the training settings of to_float32 and norm_cfg in RegNets configs (#3210)\n¢ Add colab training tutorials for beginners (#3213, #3273)\n\n* Move CUDA/C++ operators into mmcv.ops and keep mmdet.ops as warppers for backward compatibility\n(#3232)(#3457)\n\n* Update installation scripts in documentation (#3290) and dockerfile (#3320)\n* Support to set image resize backend (#3392)\n* Remove git hash in version file (#3466)\n\n* Check mmcvy version to force version compatibility (#3460)\n\n32.18 v2.2.0 (1/7/2020)\n\nHighlights\n\n* Support new methods: DetectoRS, PointRend, Generalized Focal Loss, Dynamic R-CNN\nBug Fixes\n\n« Fix FreeAnchor when no gt in image (#3176)\n\n* Clean up deprecated usage of register_module() (#3092, #3161)\n\nFix pretrain bug in NAS FCOS (#3145)\n\n¢ Fix num_classes in SSD (#3142)\n\n* Fix FCOS warmup (#3119)\n\n¢ Fix rstrip in tools/publish_model. py\n\n¢ Fix flip_ratio default value in RandomFLip pipeline (#3106)\n\n* Fix cityscapes eval with ms_renn (#3112)\n\n¢ Fix RPN softmax (#3056)\n\n« Fix filename of LVIS @v0.5 (#2998)\n\n* Fix nan loss by filtering out-of-frame gt_bboxes in COCO (#2999)\n\n¢ Fix bug in FSAF (#3018)\n\n« Add FocalLoss num_classes check (#2964)\n\n* Fix PISA Loss when there are no gts (#2992)\n\n¢ Avoid nan in iou_calculator (#2975)\n\n* Prevent possible bugs in loading and transforms caused by shallow copy (#2967)\n\nNew Features\n\n162 Chapter 32. Changelog\n", "vlm_text": "• Support to convert RetinaNet from Pytorch to ONNX (#3075)\n\n \nImprovements \n• Support to process ignore boxes in ATSS assigner (#3082)\n\n • Allow to crop images without ground truth in  RandomCrop  (#3153)\n\n • Enable the the  Accuracy  module to set threshold (#3155)\n\n • Refactoring unit tests (#3206)\n\n • Unify the training settings of  to_float32  and  norm_cfg  in RegNets configs (#3210)\n\n • Add colab training tutorials for beginners (#3213, #3273)\n\n • Move   $\\mathrm{ttCUDA/C++}$   operators into  mmcv.ops  and keep  mmdet.ops  as warppers for backward compatibility (#3232)(#3457)\n\n • Update installation scripts in documentation (#3290) and dockerfile (#3320)\n\n • Support to set image resize backend (#3392)\n\n • Remove git hash in version file (#3466)\n\n • Check mmcv version to force version compatibility (#3460)\n\n \n32.18 v2.2.0 (1/7/2020) \nHighlights \n• Support new methods:  DetectoRS ,  PointRend ,  Generalized Focal Loss ,  Dynamic R-CNN\n\n \nBug Fixes \n• Fix FreeAnchor when no gt in image (#3176)\n\n • Clean up deprecated usage of  register module()  (#3092, #3161)\n\n • Fix pretrain bug in NAS FCOS (#3145)\n\n • Fix  num classes  in SSD (#3142)\n\n • Fix FCOS warmup (#3119)\n\n • Fix  rstrip  in  tools/publish model.py\n\n • Fix  flip_ratio  default value in RandomFLip pipeline (#3106)\n\n • Fix cityscapes eval with ms_rcnn (#3112)\n\n • Fix RPN softmax (#3056)\n\n • Fix filename of LVIS@v0.5 (#2998)\n\n • Fix nan loss by filtering out-of-frame gt_bboxes in COCO (#2999)\n\n • Fix bug in FSAF (#3018)\n\n • Add FocalLoss  num classes  check (#2964)\n\n • Fix PISA Loss when there are no gts (#2992)\n\n • Avoid nan in  i ou calculator  (#2975)\n\n • Prevent possible bugs in loading and transforms caused by shallow copy (#2967) \nNew Features "}
{"page": 170, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_170.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nAdd DetectoRS (#3064)\n\nSupport Generalize Focal Loss (#3097)\n\nSupport PointRend (#2752)\n\nSupport Dynamic R-CNN (#3040)\n\nAdd DeepFashion dataset (#2968)\n\nImplement FCOS training tricks (#2935)\n\nUse BaseDenseHead as base class for anchor-base heads (#2963)\nAdd with_cp for BasicBlock (#2891)\n\nAdd stem_channels argument for ResNet (#2954)\n\nImprovements\n\nAdd anchor free base head (#2867)\n\nMigrate to github action (#3137)\n\nAdd docstring for datasets, pipelines, core modules and methods (#3130, #3125, #3120)\nAdd VOC benchmark (#3060)\n\nAdd concat mode in GRol (#3098)\n\nRemove cmd arg autorescale-1r (#3080)\n\nUse len(data['img_metas']) to indicate num_samples (#3073, #3053)\nSwitch to EpochBasedRunner (#2976)\n\n32.19 v2.1.0 (8/6/2020)\n\nHighlights\n\nSupport new backbones: RegNetX, Res2Net\nSupport new methods: NASFCOS, PISA, GRoIE\nSupport new dataset: LVIS\n\nBug Fixes\n\nChange the CLI argument --validate to --no-validate to enable validation after training epochs by default.\n\n(#2651)\n\nAdd missing cython to docker file (#2713)\n\nFix bug in nms cpu implementation (#2754)\n\nFix bug when showing mask results (#2763)\n\nFix gcc requirement (#2806)\n\nFix bug in async test (#2820)\n\nFix mask encoding-decoding bugs in test API (#2824)\nFix bug in test time augmentation (#2858, #2921, #2944)\n\nFix a typo in comment of apis/train (#2877)\n\n32.19. v2.1.0 (8/6/2020)\n\n163\n", "vlm_text": "• Add DetectoRS (#3064)\n\n• Support Generalize Focal Loss (#3097)\n\n • Support PointRend (#2752)\n\n • Support Dynamic R-CNN (#3040)\n\n • Add Deep Fashion dataset (#2968)\n\n • Implement FCOS training tricks (#2935)\n\n • Use Base Dense Head as base class for anchor-base heads (#2963)\n\n • Add  with_cp  for BasicBlock (#2891)\n\n • Add  stem channels  argument for ResNet (#2954)\n\n \nImprovements \n• Add anchor free base head (#2867)\n\n • Migrate to github action (#3137)\n\n • Add docstring for datasets, pipelines, core modules and methods (#3130, #3125, #3120)\n\n • Add VOC benchmark (#3060)\n\n • Add  concat  mode in GRoI (#3098)\n\n • Remove cmd arg auto re scale-lr (#3080)\n\n• Use  len(data[ ' img_metas ' ])  to indicate  num samples  (#3073, #3053)\n\n • Switch to Epoch Based Runner (#2976)\n\n \n32.19 v2.1.0 (8/6/2020) \nHighlights \n• Support new backbones:  RegNetX ,  Res2Net\n\n • Support new methods:  NASFCOS ,  PISA ,  GRoIE\n\n • Support new dataset:  LVIS\n\n \nBug Fixes \n• Change the CLI argument  --validate  to  --no-validate  to enable validation after training epochs by default. (#2651)\n\n • Add missing cython to docker file (#2713)\n\n • Fix bug in nms cpu implementation (#2754)\n\n • Fix bug when showing mask results (#2763)\n\n • Fix gcc requirement (#2806)\n\n • Fix bug in async test (#2820)\n\n • Fix mask encoding-decoding bugs in test API (#2824)\n\n • Fix bug in test time augmentation (#2858, #2921, #2944)\n\n • Fix a typo in comment of apis/train (#2877) "}
{"page": 171, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_171.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ Fix the bug of returning None when no gt bboxes are in the original image in RandomCrop. Fix the bug\nthat misses to handle gt_bboxes_ignore, gt_label_ignore, and gt_masks_ignore in RandomCrop,\n\nMinIoURandomCrop and Expand modules. (#2810)\nFix bug of base_channels of regnet (#2917)\nFix the bug of logger when loading pre-trained weights in base detector (#2936)\n\nNew Features\n\nAdd IoU models (#2666)\n\nAdd colab demo for inference\n\nSupport class agnostic nms (#2553)\n\nAdd benchmark gathering scripts for development only (#2676)\n\nAdd mmdet-based project links (#2736, #2767, #2895)\n\nAdd config dump in training (#2779)\n\nAdd ClassBalancedDataset (#2721)\n\nAdd res2net backbone (#2237)\n\nSupport RegNetX models (#2710)\n\nUse mmcv.FileClient to support different storage backends (#2712)\nAdd ClassBalancedDataset (#2721)\n\nCode Release: Prime Sample Attention in Object Detection (CVPR 2020) (#2626)\nImplement NASFCOS (#2682)\n\nAdd class weight in CrossEntropyLoss (#2797)\n\nSupport LVIS dataset (#2088)\nSupport GRolE (#2584)\n\nImprovements\n\nAllow different x and y strides in anchor heads. (#2629)\n\nMake FSAF loss more robust to no gt (#2680)\n\nCompute pure inference time instead (#2657) and update inference speed (#2730)\nAvoided the possibility that a patch with 0 area is cropped. (#2704)\n\nAdd warnings when deprecated imgs_per_gpu is used. (#2700)\n\nAdd a mask renn example for config (#2645)\n\nUpdate model zoo (#2762, #2866, #2876, #2879, #2831)\n\nAdd ori_filename to img_metas and use it in test show-dir (#2612)\n\nUse img_fields to handle multiple images during image transform (#2800)\n\nAdd upsample_cfg support in FPN (#2787)\n\nAdd ['img'] as default img_fields for back compatibility (#2809)\n\nRename the  pretrained model from open-mmlab://resnet50_caffe and open-mmlab://\nresnet50_caffe_bgr to open-mmlab: //detectron/resnet50_caffe and open-mmlab: //detectron2/\n\nresnet50_caffe. (#2832)\n\n164\n\nChapter 32. Changelog\n", "vlm_text": "• Fix the bug of returning None when no gt bboxes are in the original image in  RandomCrop . Fix the bug that misses to handle  gt b boxes ignore ,  gt label ignore , and  gt masks ignore  in  RandomCrop , MinI oU Random Crop  and  Expand  modules. (#2810)\n\n • Fix bug of  base channels  of regnet (#2917)\n\n • Fix the bug of logger when loading pre-trained weights in base detector (#2936)\n\n \nNew Features \n• Add IoU models (#2666)\n\n • Add colab demo for inference\n\n • Support class agnostic nms (#2553)\n\n • Add benchmark gathering scripts for development only (#2676)\n\n • Add mmdet-based project links (#2736, #2767, #2895)\n\n • Add config dump in training (#2779)\n\n • Add Class Balanced Data set (#2721)\n\n • Add res2net backbone (#2237)\n\n • Support RegNetX models (#2710)\n\n • Use  mmcv.FileClient  to support different storage backends (#2712)\n\n • Add Class Balanced Data set (#2721)\n\n • Code Release: Prime Sample Attention in Object Detection (CVPR 2020) (#2626)\n\n • Implement NASFCOS (#2682)\n\n • Add class weight in Cross Entropy Loss (#2797)\n\n • Support LVIS dataset (#2088)\n\n • Support GRoIE (#2584)\n\n \nImprovements \n• Allow different x and y strides in anchor heads. (#2629)\n\n • Make FSAF loss more robust to no gt (#2680)\n\n • Compute pure inference time instead (#2657) and update inference speed (#2730)\n\n • Avoided the possibility that a patch with 0 area is cropped. (#2704)\n\n • Add warnings when deprecated  img s per gpu  is used. (#2700)\n\n • Add a mask rcnn example for config (#2645)\n\n • Update model zoo (#2762, #2866, #2876, #2879, #2831)\n\n • Add  ori filename  to img_metas and use it in test show-dir (#2612)\n\n • Use  img_fields  to handle multiple images during image transform (#2800)\n\n • Add up sample cf g support in FPN (#2787)\n\n • Add  [ ' img ' ]  as default  img_fields  for back compatibility (#2809)\n\n • Rename the pretrained model from open-mmlab://res net 50 caff e and open-mmlab:// res net 50 caff e b gr  to  open-mmlab://detectron/res net 50 caff e  and  open-mmlab://detectron2/ res net 50 caff e . (#2832) • Added sleep(2) in test.py to reduce hanging problem (#2847) • Support  c10::half  in CARAFE (#2890) • Improve documentation s (#2918, #2714) • Use optimizer constructor in mmcv and clean the original implementation in  mmdet.core.optimizer  (#2947) "}
{"page": 172, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_172.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nAdded sleep(2) in test.py to reduce hanging problem (#2847)\nSupport c10: :half in CARAFE (#2890)\nImprove documentations (#2918, #2714)\n\nUse optimizer constructor in mmcv and clean the original implementation in mmdet . core. optimizer (#2947)\n\n32.20 v2.0.0 (6/5/2020)\n\nIn this release, we made lots of major refactoring and modifications.\n\n1.\n\n7.\n\nFaster speed. We optimize the training and inference speed for common models, achieving up to 30% speedup\nfor training and 25% for inference. Please refer to model zoo for details.\n\nHigher performance. We change some default hyperparameters with no additional cost, which leads to a gain\nof performance for most models. Please refer to compatibility for details.\n\nMore documentation and tutorials. We add a bunch of documentation and tutorials to help users get started\nmore smoothly. Read it here.\n\nSupport PyTorch 1.5. The support for 1.1 and 1.2 is dropped, and we switch to some new APIs.\nBetter configuration system. Inheritance is supported to reduce the redundancy of configs.\n\nBetter modular design. Towards the goal of simplicity and flexibility, we simplify some encapsulation while\nadd more other configurable modules like BBoxCoder, IoUCalculator, OptimizerConstructor, RolHead. Target\ncomputation is also included in heads and the call hierarchy is simpler.\n\nSupport new methods: FSAF and PAFPN (part of PAFPN).\n\nBreaking Changes Models training with MMDetection 1.x are not fully compatible with 2.0, please refer to the com-\npatibility doc for the details and how to migrate to the new version.\n\nImprovements\n\nUnify cuda and cpp API for custom ops. (#2277)\n\nNew config files with inheritance. (#2216)\n\nEncapsulate the second stage into Rol heads. (#1999)\n\nRefactor GCNet/EmpericalAttention into plugins. (#2345)\n\nSet low quality match as an option in loU-based bbox assigners. (#2375)\nChange the codebase’s coordinate system. (#2380)\n\nRefactor the category order in heads. 0 means the first positive class instead of background now. (#2374)\nAdd bbox sampler and assigner registry. (#2419)\n\nSpeed up the inference of RPN. (#2420)\n\nAdd train_cfg and test_cfg as class members in all anchor heads. (#2422)\nMerge target computation methods into heads. (#2429)\n\nAdd bbox coder to support different bbox encoding and losses. (#2480)\n\nUnify the API for regression loss. (#2156)\n\nRefactor Anchor Generator. (#2474)\n\nMake Ir an optional argument for optimizers. (#2509)\n\n32.20. v2.0.0 (6/5/2020) 165\n", "vlm_text": "\n32.20 v2.0.0 (6/5/2020) \nIn this release, we made lots of major refactoring and modifications. \n1.  Faster speed . We optimize the training and inference speed for common models, achieving up to  $30\\%$   speedup for training and  $25\\%$   for inference. Please refer to  model zoo  for details. 2.  Higher performance . We change some default hyper parameters with no additional cost, which leads to a gain of performance for most models. Please refer to  compatibility  for details. 3.  More documentation and tutorials . We add a bunch of documentation and tutorials to help users get started more smoothly. Read it  here . 4.  Support PyTorch 1.5 . The support for 1.1 and 1.2 is dropped, and we switch to some new APIs. 5.  Better configuration system . Inheritance is supported to reduce the redundancy of configs. 6.  Better modular design . Towards the goal of simplicity and flexibility, we simplify some encapsulation while add more other configurable modules like BBoxCoder, I oU Calculator, Optimizer Constructor, RoIHead. Target computation is also included in heads and the call hierarchy is simpler. 7. Support new methods:  FSAF  and PAFPN (part of  PAFPN ). \nBreaking Changes  Models training with MM Detection 1.x are not fully compatible with 2.0, please refer to the  com- patibility doc  for the details and how to migrate to the new version. \nImprovements \n• Unify cuda and cpp API for custom ops. (#2277) • New config files with inheritance. (#2216) • Encapsulate the second stage into RoI heads. (#1999) • Refactor GCNet/Em per ical Attention into plugins. (#2345) • Set low quality match as an option in IoU-based bbox assigners. (#2375) • Change the codebase’s coordinate system. (#2380) • Refactor the category order in heads. 0 means the first positive class instead of background now. (#2374) • Add bbox sampler and assigner registry. (#2419) • Speed up the inference of RPN. (#2420) • Add  train_cfg  and  test_cfg  as class members in all anchor heads. (#2422) • Merge target computation methods into heads. (#2429) • Add bbox coder to support different bbox encoding and losses. (#2480) • Unify the API for regression loss. (#2156) • Refactor Anchor Generator. (#2474) • Make  lr  an optional argument for optimizers. (#2509) "}
{"page": 173, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_173.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Migrate to modules and methods in MMCV. (#2502, #2511, #2569, #2572)\n* Support PyTorch 1.5. (#2524)\n* Drop the support for Python 3.5 and use F-string in the codebase. (#2531)\nBug Fixes\n* Fix the scale factors for resized images without keep the aspect ratio. (#2039)\n* Check if max_num > 0 before slicing in NMS. (#2486)\n* Fix Deformable RoIPool when there is no instance. (#2490)\n¢ Fix the default value of assigned labels. (#2536)\n* Fix the evaluation of Cityscapes. (#2578)\nNew Features\n* Add deep_stem and avg_down option to ResNet, i.e., support ResNetV 1d. (#2252)\n* Add L1 loss. (#2376)\n* Support both polygon and bitmap for instance masks. (#2353, #2540)\n* Support CPU mode for inference. (#2385)\n« Add optimizer constructor for complicated configuration of optimizers. (#2397, #2488)\n* Implement PAFPN. (#2392)\n* Support empty tensor input for some modules. (#2280)\n¢ Support for custom dataset classes without overriding it. (#2408, #2443)\n* Support to train subsets of coco dataset. (#2340)\n« Add iou_calculator to potentially support more IoU calculation methods. (2405)\n* Support class wise mean AP (was removed in the last version). (#2459)\n« Add option to save the testing result images. (#2414)\n¢ Support MomentumUpdaterHook. (#2571)\n\n« Add a demo to inference a single image. (#2605)\n\n32.21 v1.1.0 (24/2/2020)\n\nHighlights\n¢ Dataset evaluation is rewritten with a unified api, which is used by both evaluation hooks and test scripts.\n* Support new methods: CARAFE.\n\nBreaking Changes\n\n* The new MMDDP inherits from the official DDP, thus the __init__ api is changed to be the same as official\nDDP.\n\n* The mask_head field in HTC config files is modified.\n¢ The evaluation and testing script is updated.\n\n¢ In all transforms, instance masks are stored as a numpy array shaped (n, h, w) instead of a list of (h, w) arrays,\nwhere n is the number of instances.\n\n166 Chapter 32. Changelog\n", "vlm_text": "• Migrate to modules and methods in MMCV. (#2502, #2511, #2569, #2572)\n\n • Support PyTorch 1.5. (#2524)\n\n • Drop the support for Python 3.5 and use F-string in the codebase. (#2531)\n\n \nBug Fixes \n• Fix the scale factors for resized images without keep the aspect ratio. (#2039)\n\n • Check if max_num  $>0$   before slicing in NMS. (#2486)\n\n • Fix Deformable RoIPool when there is no instance. (#2490)\n\n • Fix the default value of assigned labels. (#2536)\n\n • Fix the evaluation of Cityscapes. (#2578)\n\n \nNew Features \n• Add deep_stem and avg_down option to ResNet, i.e., support ResNetV1d. (#2252)\n\n • Add L1 loss. (#2376)\n\n • Support both polygon and bitmap for instance masks. (#2353, #2540)\n\n • Support CPU mode for inference. (#2385)\n\n • Add optimizer constructor for complicated configuration of optimizers. (#2397, #2488)\n\n • Implement PAFPN. (#2392)\n\n • Support empty tensor input for some modules. (#2280)\n\n • Support for custom dataset classes without overriding it. (#2408, #2443)\n\n • Support to train subsets of coco dataset. (#2340)\n\n • Add i ou calculator to potentially support more IoU calculation methods. (2405)\n\n • Support class wise mean AP (was removed in the last version). (#2459)\n\n • Add option to save the testing result images. (#2414)\n\n • Support Momentum Updater Hook. (#2571)\n\n • Add a demo to inference a single image. (#2605)\n\n \n32.21 v1.1.0 (24/2/2020) \nHighlights \n• Dataset evaluation is rewritten with a unified api, which is used by both evaluation hooks and test scripts.\n\n • Support new methods:  CARAFE .\n\n \nBreaking Changes \n• The new MMDDP inherits from the official DDP, thus the  __init__  api is changed to be the same as official DDP.\n\n • The  mask_head  field in HTC config files is modified.\n\n • The evaluation and testing script is updated.\n\n • In all transforms, instance masks are stored as a numpy array shaped (n, h, w) instead of a list of (h, w) arrays, where n is the number of instances. "}
{"page": 174, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_174.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nBug Fixes\n\nFix IOU assigners when ignore_iof_thr > 0 and there is no pred boxes. (#2135)\nFix mAP evaluation when there are no ignored boxes. (#2116)\n\nFix the empty Rol input for Deformable Rol Pooling. (#2099)\n\nFix the dataset settings for multiple workflows. (#2103)\n\nFix the warning related to torch. uint8 in PyTorch 1.4. (#2105)\n\nFix the inference demo on devices other than gpu:0. (#2098)\n\nFix Dockerfile. (#2097)\n\nFix the bug that pad_val is unused in Pad transform. (#2093)\n\nFix the albumentation transform when there is no ground truth bbox. (#2032)\n\nImprovements\n\nUse torch instead of numpy for random sampling. (#2094)\n\nMigrate to the new MMDDP implementation in MMCV v0.3. (#2090)\n\nAdd meta information in logs. (#2086)\n\nRewrite Soft NMS with pytorch extension and remove cython as a dependency. (#2056)\nRewrite dataset evaluation. (#2042, #2087, #2114, #2128)\n\nUse numpy array for masks in transforms. (#2030)\n\nNew Features\n\nImplement “CARAFE: Content-Aware ReAssembly of FEatures”. (#1583)\nAdd worker_init_fn() in data_loader when seed is set. (#2066, #2111)\nAdd logging utils. (#2035)\n\n32.22 v1.0.0 (30/1/2020)\n\nThis release mainly improves the code quality and add more docstrings.\n\nHighlights\n\n* DCN is now available with the api build_conv_layer and ConvModule like the normal conv layer.\n\nDocumentation is online now: https://mmdetection.readthedocs.io.\n\nSupport new models: ATSS.\n\nA tool to collect environment information is available for trouble shooting.\n\nBug Fixes\n\nFix the incompatibility of the latest numpy and pycocotools. (#2024)\n\nFix the case when distributed package is unavailable, e.g., on Windows. (#1985)\nFix the dimension issue for refine_bboxes(). (#1962)\n\nFix the typo when seg_prefix is a list. (#1906)\n\nAdd segmentation map cropping to RandomCrop. (#1880)\n\n32.22. v1.0.0 (30/1/2020)\n\n167\n", "vlm_text": "Bug Fixes \n• Fix IOU assigners when ignore i of thr  $>0$   and there is no pred boxes. (#2135) • Fix mAP evaluation when there are no ignored boxes. (#2116) • Fix the empty RoI input for Deformable RoI Pooling. (#2099) • Fix the dataset settings for multiple workflows. (#2103) • Fix the warning related to  torch.uint8  in PyTorch 1.4. (#2105) • Fix the inference demo on devices other than gpu:0. (#2098) • Fix Dockerfile. (#2097) • Fix the bug that  pad_val  is unused in Pad transform. (#2093) • Fix the album ent ation transform when there is no ground truth bbox. (#2032) \nImprovements \n• Use torch instead of numpy for random sampling. (#2094) • Migrate to the new MMDDP implementation in MMCV v0.3. (#2090) • Add meta information in logs. (#2086) • Rewrite Soft NMS with pytorch extension and remove cython as a dependency. (#2056) • Rewrite dataset evaluation. (#2042, #2087, #2114, #2128) • Use numpy array for masks in transforms. (#2030) \nNew Features \n• Implement “CARAFE: Content-Aware ReAssembly of FEatures”. (#1583) • Add  worker in it fn()  in data loader when seed is set. (#2066, #2111) • Add logging utils. (#2035) \n32.22 v1.0.0 (30/1/2020) \nThis release mainly improves the code quality and add more docstrings. \nHighlights \n• Documentation is online now: https://mm detection.read the docs.io. • Support new models:  ATSS . • DCN is now available with the api  build con v layer  and  ConvModule  like the normal conv layer. • A tool to collect environment information is available for trouble shooting. \nBug Fixes \n• Fix the incompatibility of the latest numpy and py coco tools. (#2024) • Fix the case when distributed package is unavailable, e.g., on Windows. (#1985) • Fix the dimension issue for  refine b boxes() . (#1962) • Fix the typo when  seg_prefix  is a list. (#1906) • Add segmentation map cropping to RandomCrop. (#1880) "}
{"page": 175, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_175.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nFix the return value of ga_shape_target_single(). (#1853)\nFix the loaded shape of empty proposals. (#1819)\n\nFix the mask data type when using albumentation. (#1818)\n\nImprovements\n\nEnhance AssignResult and SamplingResult. (#1995)\n\nAdd ability to overwrite existing module in Registry. (#1982)\n\nReorganize requirements and make albumentations and imagecorruptions optional.\nCheck NaN in SSDHead. (#1935)\n\nEncapsulate the DCN in ResNe(X)t into a ConvModule & Conv_layers. (#1894)\nRefactoring for mAP evaluation and support multiprocessing and logging. (#1889)\nInit the root logger before constructing Runner to log more information. (#1865)\nSplit SegResizeFlipPadRescale into different existing transforms. (#1852)\nMove init_dist() to MMCV. (#1851)\n\nDocumentation and docstring improvements. (#1971, #1938, #1869, #1838)\n\nFix the color of the same class for mask visualization. (#1834)\n\nRemove the option keep_all_stages in HTC and Cascade R-CNN. (#1806)\n\nNew Features\n\n(#1969)\n\n« Add two test-time options crop_mask and rle_mask_encode for mask heads. (#2013)\n\nSupport loading grayscale images as single channel. (#1975)\n\nImplement “Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample\n\nSelection”. (#1872)\nAdd sphinx generated docs. (#1859, #1864)\nAdd GN support for flops computation. (#1850)\n\nCollect env info for trouble shooting. (#1812)\n\n32.23 v1.0re1 (13/12/2019)\n\nThe RC1 release mainly focuses on improving the user experience, and fixing bugs.\n\nHighlights\n\nSupport new models: FoveaBox, RepPoints and FreeAnchor.\nAdd a Dockerfile.\n\nAdd a jupyter notebook demo and a webcam demo.\n\nSetup the code style and CI.\n\nAdd lots of docstrings and unit tests.\n\nFix lots of bugs.\n\nBreaking Changes\n\n168\n\nChapter 32. Changelog\n", "vlm_text": "• Fix the return value of  ga shape target single() . (#1853) • Fix the loaded shape of empty proposals. (#1819) • Fix the mask data type when using album ent ation. (#1818) \nImprovements \n• Enhance Assign Result and Sampling Result. (#1995) • Add ability to overwrite existing module in Registry. (#1982) • Reorganize requirements and make albumen tat ions and image corruptions optional. (#1969) • Check NaN in  SSDHead . (#1935) • Encapsulate the DCN in ResNe(X)t into a ConvModule & Con v layers. (#1894) • Refactoring for mAP evaluation and support multiprocessing and logging. (#1889) • Init the root logger before constructing Runner to log more information. (#1865) • Split  Seg Resize Flip PadRe scale  into different existing transforms. (#1852) • Move  init_dist()  to MMCV. (#1851) • Documentation and docstring improvements. (#1971, #1938, #1869, #1838) • Fix the color of the same class for mask visualization. (#1834) • Remove the option  keep all stages  in HTC and Cascade R-CNN. (#1806) \nNew Features \n• Add two test-time options  crop_mask  and  r le mask encode  for mask heads. (#2013) • Support loading grayscale images as single channel. (#1975) • Implement “Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection”. (#1872) • Add sphinx generated docs. (#1859, #1864) • Add GN support for flops computation. (#1850) • Collect env info for trouble shooting. (#1812) \n32.23 v1.0rc1 (13/12/2019) \nThe RC1 release mainly focuses on improving the user experience, and fixing bugs. \nHighlights \n• Support new models:  FoveaBox ,  RepPoints  and  FreeAnchor • Add a Dockerfile. • Add a jupyter notebook demo and a webcam demo. • Setup the code style and CI. • Add lots of docstrings and unit tests. • Fix lots of bugs. \nBreaking Changes "}
{"page": 176, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_176.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ There was a bug for computing COCO-style mAP w.r.t different scales (AP_s, AP_m, AP_]), introduced by #621.\n(#1679)\n\nBug Fixes\n¢ Fix a sampling interval bug in Libra R-CNN. (#1800)\n« Fix the learning rate in SSD300 WIDER FACE. (#1781)\nFix the scaling issue when keep_ratio=False. (#1730)\n* Fix typos. (#1721, #1492, #1242, #1108, #1107)\nFix the shuffle argument in build_dataloader. (#1693)\n* Clip the proposal when computing mask targets. (#1688)\nFix the “index out of range” bug for samplers in some corner cases. (#1610, #1404)\n¢ Fix the NMS issue on devices other than GPU:0. (#1603)\n¢ Fix SSD Head and GHM Loss on CPU. (#1578)\n¢ Fix the OOM error when there are too many gt bboxes. (#1575)\n¢ Fix the wrong keyword argument nms_cfg in HTC. (#1573)\n* Process masks and semantic segmentation in Expand and MinloUCrop transforms. (#1550, #1361)\n* Fix a scale bug in the Non Local op. (#1528)\n¢ Fix a bug in transforms when gt_bboxes_ignore is None. (#1498)\n¢ Fix a bug when img_prefix is None. (#1497)\n* Pass the device argument to grid_anchors and valid_flags. (#1478)\n« Fix the data pipeline for test_robustness. (#1476)\n¢ Fix the argument type of deformable pooling. (#1390)\n« Fix the coco_eval when there are only two classes. (#1376)\n¢ Fix a bug in Modulated DeformableConv when deformable_group>1. (#1359)\n¢ Fix the mask cropping in RandomCrop. (#1333)\n¢ Fix zero outputs in DeformConv when not running on cuda:0. (#1326)\nFix the type issue in Expand. (#1288)\n¢ Fix the inference API. (#1255)\n* Fix the inplace operation in Expand. (#1249)\n¢ Fix the from-scratch training config. (#1196)\n* Fix inplace add in RolExtractor which cause an error in PyTorch 1.2. (#1160)\n* Fix FCOS when input images has no positive sample. (#1136)\n« Fix recursive imports. (#1099)\nImprovements\n* Print the config file and mmdet version in the log. (#1721)\n* Lint the code before compiling in travis CI. (#1715)\n\n« Add a probability argument for the Expand transform. (#1651)\n\n32.23. v1.0re1 (13/12/2019) 169\n", "vlm_text": "• There was a bug for computing COCO-style mAP w.r.t different scales (AP_s, AP_m, AP_l), introduced by  $\\#621$  . (#1679)\n\n \nBug Fixes \n• Fix a sampling interval bug in Libra R-CNN. (#1800)\n\n • Fix the learning rate in SSD300 WIDER FACE. (#1781)\n\n • Fix the scaling issue when  keep_ratio  $=$  False . (#1730)\n\n • Fix typos. (#1721, #1492, #1242, #1108, #1107)\n\n • Fix the shuffle argument in  build data loader . (#1693)\n\n • Clip the proposal when computing mask targets. (#1688)\n\n • Fix the “index out of range” bug for samplers in some corner cases. (#1610, #1404)\n\n • Fix the NMS issue on devices other than GPU:0. (#1603)\n\n • Fix SSD Head and GHM Loss on CPU. (#1578)\n\n • Fix the OOM error when there are too many gt bboxes. (#1575)\n\n • Fix the wrong keyword argument  nms_cfg  in HTC. (#1573)\n\n • Process masks and semantic segmentation in Expand and MinIoUCrop transforms. (#1550, #1361)\n\n • Fix a scale bug in the Non Local op. (#1528)\n\n • Fix a bug in transforms when  gt b boxes ignore  is None. (#1498)\n\n • Fix a bug when  img_prefix  is None. (#1497)\n\n • Pass the device argument to  grid anchors  and  valid flags . (#1478)\n\n • Fix the data pipeline for test robustness. (#1476)\n\n • Fix the argument type of deformable pooling. (#1390)\n\n • Fix the coco_eval when there are only two classes. (#1376)\n\n • Fix a bug in Modulated De formable Con v when de formable group>1. (#1359)\n\n • Fix the mask cropping in RandomCrop. (#1333)\n\n • Fix zero outputs in DeformConv when not running on cuda:0. (#1326)\n\n • Fix the type issue in Expand. (#1288)\n\n • Fix the inference API. (#1255)\n\n • Fix the inplace operation in Expand. (#1249)\n\n • Fix the from-scratch training config. (#1196)\n\n • Fix inplace add in RoI Extractor which cause an error in PyTorch 1.2. (#1160)\n\n • Fix FCOS when input images has no positive sample. (#1136)\n\n • Fix recursive imports. (#1099)\n\n \nImprovements \n• Print the config file and mmdet version in the log. (#1721)\n\n • Lint the code before compiling in travis CI. (#1715)\n\n • Add a probability argument for the  Expand  transform. (#1651) "}
{"page": 177, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_177.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* Update the PyTorch and CUDA version in the docker file. (#1615)\n\n* Raise a warning when specifying --validate in non-distributed training. (#1624, #1651)\n* Beautify the mAP printing. (#1614)\n\n« Add pre-commit hook. (#1536)\n\n« Add the argument in_channels to backbones. (#1475)\n\n¢ Add lots of docstrings and unit tests, thanks to @Erotemic. (#1603, #1517, #1506, #1505, #1491, #1479, #1477,\n#1475, #1474)\n\n« Add support for multi-node distributed test when there is no shared storage. (#1399)\n* Optimize Dockerfile to reduce the image size. (#1306)\n\n* Update new results of HRNet. (#1284, #1182)\n\n« Add an argument no_norm_on_lateral in FPN. (#1240)\n\n* Test the compiling in CL. (#1235)\n\n* Move docs to a separate folder. (#1233)\n\n¢ Add a jupyter notebook demo. (#1158)\n\n* Support different type of dataset for training. (#1133)\n\n* Use int64_t instead of long in cuda kernels. (#1131)\n\n* Support unsquare Rols for bbox and mask heads. (#1128)\n\n* Manually add type promotion to make compatible to PyTorch 1.2. (#1114)\n\n¢ Allowing validation dataset for computing validation loss. (#1093)\n\n¢ Use .scalar_type() instead of .type() to suppress some warnings. (#1070)\nNew Features\n\n¢ Add an option --with_ap to compute the AP for each class. (#1549)\n\n¢ Implement “FreeAnchor: Learning to Match Anchors for Visual Object Detection”. (#1391)\n\n¢ Support Albumentations for augmentations in the data pipeline. (#1354)\n\n* Implement “FoveaBox: Beyond Anchor-based Object Detector”. (#1339)\n\n* Support horizontal and vertical flipping. (#1273, #1115)\n\n¢ Implement “RepPoints: Point Set Representation for Object Detection”. (#1265)\n\n« Add test-time augmentation to HTC and Cascade R-CNN. (#1251)\n\n« Add a COCO result analysis tool. (#1228)\n\n¢ Add Dockerfile. (#1168)\n\n¢ Add a webcam demo. (#1155, #1150)\n\n¢ Add FLOPs counter. (#1127)\n\n« Allow arbitrary layer order for ConvModule. (#1078)\n\n170 Chapter 32. Changelog\n", "vlm_text": "• Update the PyTorch and CUDA version in the docker file. (#1615)\n\n • Raise a warning when specifying  --validate  in non-distributed training. (#1624, #1651)\n\n • Beautify the mAP printing. (#1614)\n\n • Add pre-commit hook. (#1536)\n\n • Add the argument  in channels  to backbones. (#1475)\n\n • Add lots of docstrings and unit tests, thanks to    $@$  Erotemic . (#1603, #1517, #1506, #1505, #1491, #1479, #1477, #1475, #1474)\n\n • Add support for multi-node distributed test when there is no shared storage. (#1399)\n\n • Optimize Dockerfile to reduce the image size. (#1306)\n\n • Update new results of HRNet. (#1284, #1182)\n\n • Add an argument  no norm on lateral  in FPN. (#1240)\n\n • Test the compiling in CI. (#1235)\n\n • Move docs to a separate folder. (#1233)\n\n • Add a jupyter notebook demo. (#1158)\n\n • Support different type of dataset for training. (#1133)\n\n • Use int64_t instead of long in cuda kernels. (#1131)\n\n • Support unsquare RoIs for bbox and mask heads. (#1128)\n\n • Manually add type promotion to make compatible to PyTorch 1.2. (#1114)\n\n • Allowing validation dataset for computing validation loss. (#1093)\n\n • Use  .scalar type()  instead of  .type()  to suppress some warnings. (#1070)\n\n \nNew Features \n• Add an option  --with_ap  to compute the AP for each class. (#1549)\n\n • Implement “FreeAnchor: Learning to Match Anchors for Visual Object Detection”. (#1391)\n\n • Support  Albumen tat ions  for augmentations in the data pipeline. (#1354)\n\n • Implement “FoveaBox: Beyond Anchor-based Object Detector”. (#1339)\n\n • Support horizontal and vertical flipping. (#1273, #1115)\n\n • Implement “RepPoints: Point Set Representation for Object Detection”. (#1265)\n\n • Add test-time augmentation to HTC and Cascade R-CNN. (#1251)\n\n • Add a COCO result analysis tool. (#1228)\n\n • Add Dockerfile. (#1168)\n\n • Add a webcam demo. (#1155, #1150)\n\n • Add FLOPs counter. (#1127)\n\n • Allow arbitrary layer order for ConvModule. (#1078) "}
{"page": 178, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_178.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.24 v1.0rc0 (27/07/2019)\n\n* Implement lots of new methods and components (Mixed Precision Training, HTC, Libra R-CNN, Guided An-\nchoring, Empirical Attention, Mask Scoring R-CNN, Grid R-CNN (Plus), GHM, GCNet, FCOS, HRNet, Weight\nStandardization, etc.). Thank all collaborators!\n\n* Support two additional datasets: WIDER FACE and Cityscapes.\n* Refactoring for loss APIs and make it more flexible to adopt different losses and related hyper-parameters.\n¢ Speed up multi-gpu testing.\n\n* Integrate all compiling and installing in a single script.\n\n32.25 v0.6.0 (14/04/2019)\n\n* Up to 30% speedup compared to the model zoo.\n* Support both PyTorch stable and nightly version.\n* Replace NMS and SigmoidFocalLoss with Pytorch CUDA extensions.\n\n32.26 v0.6rc0(06/02/2019)\n\n* Migrate to PyTorch 1.0.\n\n32.27 v0.5.7 (06/02/2019)\n\n« Add support for Deformable ConvNet v2. (Many thanks to the authors and @chengdazhi)\n* This is the last release based on PyTorch 0.4.1.\n\n32.28 v0.5.6 (17/01/2019)\n\n« Add support for Group Normalization.\n\n* Unify RPNHead and single stage heads (RetinaHead, SSDHead) with AnchorHead.\n\n32.29 v0.5.5 (22/12/2018)\n\nAdd SSD for COCO and PASCAL VOC.\n\nAdd ResNeXt backbones and detection models.\n\n* Refactoring for Samplers/Assigners and add OHEM.\n\nAdd VOC dataset and evaluation scripts.\n\n32.24. v1.0rc0 (27/07/2019) 171\n", "vlm_text": "32.24 v1.0rc0 (27/07/2019) \n• Implement lots of new methods and components (Mixed Precision Training, HTC, Libra R-CNN, Guided An- choring, Empirical Attention, Mask Scoring R-CNN, Grid R-CNN (Plus), GHM, GCNet, FCOS, HRNet, Weight Standardization, etc.). Thank all collaborators!\n\n • Support two additional datasets: WIDER FACE and Cityscapes.\n\n • Refactoring for loss APIs and make it more flexible to adopt different losses and related hyper-parameters.\n\n • Speed up multi-gpu testing.\n\n • Integrate all compiling and installing in a single script.\n\n \n32.25 v0.6.0 (14/04/2019) \n• Up to  $30\\%$   speedup compared to the model zoo.\n\n • Support both PyTorch stable and nightly version.\n\n • Replace NMS and S igm oid Focal Loss with Pytorch CUDA extensions.\n\n \n32.26 v0.6rc0(06/02/2019) \n• Migrate to PyTorch 1.0.\n\n \n32.27 v0.5.7 (06/02/2019) \n• Add support for Deformable ConvNet v2. (Many thanks to the authors and    $@$  chengdazhi )\n\n • This is the last release based on PyTorch 0.4.1.\n\n \n32.28 v0.5.6 (17/01/2019) \n• Add support for Group Normalization.\n\n • Unify RPNHead and single stage heads (RetinaHead, SSDHead) with AnchorHead.\n\n \n32.29 v0.5.5 (22/12/2018) \n• Add SSD for COCO and PASCAL VOC.\n\n • Add ResNeXt backbones and detection models.\n\n • Refactoring for Samplers/Assigners and add OHEM.\n\n • Add VOC dataset and evaluation scripts. "}
{"page": 179, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_179.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n32.30 v0.5.4 (27/11/2018)\n\n« Add SingleStageDetector and RetinaNet.\n\n32.31 v0.5.3 (26/11/2018)\n\n« Add Cascade R-CNN and Cascade Mask R-CNN.\n« Add support for Soft-NMS in config files.\n\n32.32 v0.5.2 (21/10/2018)\n\n« Add support for custom datasets.\n\n« Add a script to convert PASCAL VOC annotations to the expected format.\n\n32.33 v0.5.1 (20/10/2018)\n\n« Add BBoxAssigner and BBoxSampler, the train_cfg field in config files are restructured.\n\n* ConvFCRoIHead/ SharedFCRoIHead are renamed to ConvFCBBoxHead / SharedFCBBoxHead for consistency.\n\n172 Chapter 32. Changelog\n", "vlm_text": "32.30 v0.5.4 (27/11/2018) \n• Add Single Stage Detector and RetinaNet.\n\n \n32.31 v0.5.3 (26/11/2018) \n• Add Cascade R-CNN and Cascade Mask R-CNN.\n\n • Add support for Soft-NMS in config files.\n\n \n32.32 v0.5.2 (21/10/2018) \n• Add support for custom datasets.\n\n • Add a script to convert PASCAL VOC annotations to the expected format.\n\n \n32.33 v0.5.1 (20/10/2018) \n• Add B Box As signer and B Box Sampler, the  train_cfg  field in config files are restructured.\n\n •  Con v FC RoI Head  /  Shared FC RoI Head  are renamed to  Con v FCB Box Head  /  Shared FCB Box Head  for consistency. "}
{"page": 180, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_180.jpg", "ocr_text": "CHAPTER\nTHIRTYTHREE\n\nFREQUENTLY ASKED QUESTIONS\n\nWe list some common troubles faced by many users and their corresponding solutions here. Feel free to enrich the list\nif you find any frequent issues and have ways to help others to solve them. If the contents here do not cover your issue,\nplease create an issue using the provided templates and make sure you fill in all required information in the template.\n\n33.1 MMCV Installation\n\n* Compatibility issue between MMCV and MMDetection; “ConvWS is already registered in conv layer”; “Asser-\ntionError: MMCV==xxx is used but incompatible. Please install mmcv>=xxx, <=xxx.”\n\nPlease install the correct version of MMCV for the version of your MMDetection following the installation\ninstruction.\n\n29\n\n* “No module named ‘mmcv.ops’”; “No module named ‘mmcv._ext’”.\n1. Uninstall existing mmcv in the environment using pip uninstall mmcv.\n\n2. Install mmcv-full following the installation instruction.\n\n33.2 PyTorch/CUDA Environment\n\n* “RTX 30 series card fails when building MMCV or MMDet”\n\n1. Temporary work-around: do MMCV_WITH_OPS=1 MMCV_CUDA_ARGS='-gencode=arch=compute_80,\ncode=sm_80' pip install -e .. The common issue is nvcc fatal : Unsupported gpu\narchitecture 'compute_86'. This means that the compiler should optimize for sm_86, i.e., nvidia\n30 series card, but such optimizations have not been supported by CUDA toolkit 11.0. This work-around\nmodifies the compile flag by adding MMCV_CUDA_ARGS=' -gencode=arch=compute_80, code=sm_80',\nwhich tells nvcc to optimize for sm_80, i.e., Nvidia A100. Although A100 is different from the 30 series\ncard, they use similar ampere architecture. This may hurt the performance but it works.\n\n2. PyTorch developers have updated that the default compiler flags should be fixed by pytorch/pytorch#47585.\nSo using PyTorch-nightly may also be able to solve the problem, though we have not tested it yet.\n\n* “invalid device function” or “no kernel image is available for execution”.\n\n1. Check if your cuda runtime version (under /usr/local/), nvcc --version and conda list\ncudatoolkit version match.\n\n2. Run python mmdet/utils/collect_env.py to check whether PyTorch, torchvision, and MMCV are\nbuilt for the correct GPU architecture. You may need to set TORCH_CUDA_ARCH_LIST to reinstall\nMMCYV. The GPU arch table could be found here, i.e. run TORCH_CUDA_ARCH_LIST=7.0 pip install\n\n173\n", "vlm_text": "FREQUENTLY ASKED QUESTIONS \nWe list some common troubles faced by many users and their corresponding solutions here. Feel free to enrich the list if you find any frequent issues and have ways to help others to solve them. If the contents here do not cover your issue, please create an issue using the  provided templates  and make sure you fill in all required information in the template. \n33.1 MMCV Installation \n• Compatibility issue between MMCV and MM Detection; “ConvWS is already registered in conv layer”; “Asser- tionError: MMCV  $\\scriptstyle{\\left(=-2\\right)}$  xxx is used but incompatible. Please install mmcv  $\\scriptstyle{>=\\mathbf{X}\\mathbf{X}\\mathbf{X}}$  ,  $<=\\tt X X X$  .” Please install the correct version of MMCV for the version of your MM Detection following the  installation instruction . \n• “No module named ‘mmcv.ops’”; “No module named ‘mmcv._ext’”. \n1. Uninstall existing mmcv in the environment using  pip uninstall mmcv . 2. Install mmcv-full following the  installation instruction . \n33.2 PyTorch/CUDA Environment \n• “RTX 30 series card fails when building MMCV or MMDet” \n1. Temporary work-around: do  MMCV_WITH_  ${\\tt O P S}{=}1$   MM CV CUDA ARG S  $\\overleftarrow{}$  ' -gencode  $=$  arch  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  compute_80, code  $\\underline{{\\underline{{\\mathbf{\\Pi}}}}}$  sm_80 '  pip install -e . . The common issue is  nvcc fatal : Unsupported gpu architecture  ' compute_86 ' . This means that the compiler should optimize for sm_86, i.e., nvidia 30 series card, but such optimization s have not been supported by CUDA toolkit 11.0. This work-around modifies the compile flag by adding  MM CV CUDA ARG S  $,=$  ' -gencode  $:=$  arch  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  compute_80,code  $=$  sm_80 ' , which tells  nvcc  to optimize for  $\\mathbf{sm}\\_{80}$  , i.e., Nvidia A100. Although A100 is different from the 30 series card, they use similar ampere architecture. This may hurt the performance but it works. \n2. PyTorch developers have updated that the default compiler flags should be fixed by  pytorch/pytorch#47585 . So using PyTorch-nightly may also be able to solve the problem, though we have not tested it yet. \n• “invalid device function” or “no kernel image is available for execution”. \n1. Check if your cuda runtime version (under  /usr/local/ ),  nvcc --version  and  conda list cuda toolkit  version match. \n2. Run  python mmdet/utils/collect en v.py  to check whether PyTorch, torch vision, and MMCV are built for the correct GPU architecture. You may need to set  TORCH CUDA ARCH LIST  to reinstall MMCV. The GPU arch table could be found  here , i.e. run  TORCH CUDA ARCH LIS  $\\Gamma{=}7\\cdot\\Updownarrow$   pip install "}
{"page": 181, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_181.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmcv-full to build MMCV for Volta GPUs. The compatibility issue could happen when using old GPUS,\ne.g., Tesla K80 (3.7) on colab.\n\n3. Check whether the running environment is the same as that when mmcv/mmdet has compiled. For example,\nyou may compile mmcv using CUDA 10.0 but run it on CUDA 9.0 environments.\n\n* “undefined symbol” or “cannot open xxx.so”.\n\n1. If those symbols are CUDA/C++ symbols (e.g., libcudart.so or GLIBCXX), check whether the CUDA/GCC\nruntimes are the same as those used for compiling mmcy, i.e. run python mmdet/utils/collect_env.\npy to see if \"MMCV Compiler\"/\"MMCV CUDA Compiler\" is the same as \"GCC\"/\"CUDA_HOME\".\n\n2. If those symbols are PyTorch symbols (e.g., symbols containing caffe, aten, and TH), check whether the\nPyTorch version is the same as that used for compiling mmcv.\n\n3. Run python mmdet/utils/collect_env.py to check whether PyTorch, torchvision, and MMCV are\nbuilt by and running on the same environment.\n\n* setuptools.sandbox.UnpickleableException: DistutilsSetupError(“each element of “ext_modules’ option must be\nan Extension instance or 2-tuple’”’)\n\n1. If you are using miniconda rather than anaconda, check whether Cython is installed as indicated in #3379.\nYou need to manually install Cython first and then run command pip install -r requirements.txt.\n\n2. You may also need to check the compatibility between the setuptools, Cython, and PyTorch in your\nenvironment.\n\n* “Segmentation fault”.\n\n1. Check you GCC version and use GCC 5.4. This usually caused by the incompatibility between PyTorch\nand the environment (e.g., GCC < 4.9 for PyTorch). We also recommend the users to avoid using GCC\n5.5 because many feedbacks report that GCC 5.5 will cause “segmentation fault” and simply changing it to\nGCC 5.4 could solve the problem.\n\n2. Check whether PyTorch is correctly installed and could use CUDA op, e.g. type the following command\nin your terminal.\n\npython -c ‘import torch; print(torch.cuda.is_available())'\n\nAnd see whether they could correctly output results.\n\n3. If Pytorch is correctly installed, check whether MMCV is correctly installed.\n\npython -c ‘import mmcv; import mmcv.ops'\n\nIf MMCV is correctly installed, then there will be no issue of the above two commands.\n\n4. If MMCV and Pytorch is correctly installed, you man use ipdb, pdb to set breakpoints or directly add\n‘print’ in mmdetection code and see which part leads the segmentation fault.\n\n33.3 Training\n\n* “Loss goes Nan”\n\n1. Check if the dataset annotations are valid: zero-size bounding boxes will cause the regression loss to be\nNan due to the commonly used transformation for box regression. Some small size (width or height are\nsmaller than 1) boxes will also cause this problem after data augmentation (e.g., instaboost). So check the\ndata and try to filter out those zero-size boxes and skip some risky augmentations on the small-size boxes\nwhen you face the problem.\n\n174 Chapter 33. Frequently Asked Questions\n\n", "vlm_text": "mmcv-full  to build MMCV for Volta GPUs. The compatibility issue could happen when using old GPUS, e.g., Tesla K80 (3.7) on colab. \n3. Check whether the running environment is the same as that when mmcv/mmdet has compiled. For example, you may compile mmcv using CUDA 10.0 but run it on CUDA 9.0 environments.\n\n \n• “undefined symbol” or “cannot open xxx.so”. \n1. If those symbols are  $\\mathrm{ttCUDA/C++}$   symbols (e.g., libcudart.so or GLIBCXX), check whether the CUDA/GCC runtimes are the same as those used for compiling mmcv, i.e. run  python mmdet/utils/collect en v. py  to see if  \"MMCV Compiler\" / \"MMCV CUDA Compiler\"  is the same as  \"GCC\" / \"CUDA_HOME\" . 2. If those symbols are PyTorch symbols (e.g., symbols containing caffe, aten, and TH), check whether the PyTorch version is the same as that used for compiling mmcv. 3. Run  python mmdet/utils/collect en v.py  to check whether PyTorch, torch vision, and MMCV are built by and running on the same environment.\n\n \n• setuptools.sandbox.Unpick le able Exception: Dist utils Setup Error(“each element of ‘ext modules’ option must be an Extension instance or 2-tuple”) \n1. If you are using miniconda rather than anaconda, check whether Cython is installed as indicated in  #3379 . You need to manually install Cython first and then run command  pip install -r requirements.txt . \n2. You may also need to check the compatibility between the  setuptools ,  Cython , and  PyTorch  in your environment.\n\n \n• “Segmentation fault”. \n1. Check you GCC version and use GCC 5.4. This usually caused by the incompatibility between PyTorch and the environment (e.g.,   $\\mathrm{GCC}<4.9$   for PyTorch). We also recommend the users to avoid using GCC 5.5 because many feedbacks report that GCC 5.5 will cause “segmentation fault” and simply changing it to GCC 5.4 could solve the problem. \n2. Check whether PyTorch is correctly installed and could use CUDA op, e.g. type the following command in your terminal. \npython -c  ' import torch; print(torch.cuda.is available()) And see whether they could correctly output results. 3. If Pytorch is correctly installed, check whether MMCV is correctly installed. python -c  ' import mmcv; import mmcv.ops If MMCV is correctly installed, then there will be no issue of the above two commands. \n\n\n\n\n4. If MMCV and Pytorch is correctly installed, you man use  ipdb ,  pdb  to set breakpoints or directly add ‘print’ in mm detection code and see which part leads the segmentation fault.\n\n \n33.3 Training \n• “Loss goes Nan” \n1. Check if the dataset annotations are valid: zero-size bounding boxes will cause the regression loss to be Nan due to the commonly used transformation for box regression. Some small size (width or height are smaller than 1) boxes will also cause this problem after data augmentation (e.g., instaboost). So check the data and try to filter out those zero-size boxes and skip some risky augmentations on the small-size boxes when you face the problem. "}
{"page": 182, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_182.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n2. Reduce the learning rate: the learning rate might be too large due to some reasons, e.g., change of batch\nsize. You can rescale them to the value that could stably train the model.\n\n3. Extend the warmup iterations: some models are sensitive to the learning rate at the start of the training.\nYou can extend the warmup iterations, e.g., change the warmup_iters from 500 to 1000 or 2000.\n\n4. Add gradient clipping: some models requires gradient clipping to stabilize the training process.\nThe default of grad_clip is None, you can add gradient clippint to avoid gradients that are too\nlarge, ie., set optimizer_config=dict(_delete_=True, grad_clip=dict(max_norm=35,\nnorm_type=2)) in your config file. If your config does not inherits from any basic\nconfig that contains optimizer_config=dict(grad_clip=None), you can simply add\noptimizer_config=dict (grad_clip=dict (max_norm=35, norm_type=2)).\n\n* °GPU out of memory”\n\n1. There are some scenarios when there are large amount of ground truth boxes, which may cause OOM\nduring target assignment. You can set gpu_assign_thr=N in the config of assigner thus the assigner will\ncalculate box overlaps through CPU when there are more than N GT boxes.\n\n2. Set with_cp=True in the backbone. This uses the sublinear strategy in PyTorch to reduce GPU memory\ncost in the backbone.\n\n3. Try mixed precision training using following the examples in config/fp16. The loss_scale might need\nfurther tuning for different models.\n\n* “RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one”\n\n1. This error indicates that your module has parameters that were not used in producing loss. This phenomenon\nmay be caused by running different branches in your code in DDP mode.\n\n2. You can set find_unused_parameters = True in the config to solve the above problems or find those\nunused parameters manually.\n\n33.4 Evaluation\n\n¢ COCO Dataset, AP or AR = -1\n\n1. According to the definition of COCO dataset, the small and medium areas in an image are less than 1024\n(32*32), 9216 (96*96), respectively.\n\n2. If the corresponding area has no object, the result of AP and AR will set to -1.\n\n33.4. Evaluation 175\n", "vlm_text": "2. Reduce the learning rate: the learning rate might be too large due to some reasons, e.g., change of batch size. You can rescale them to the value that could stably train the model. 3. Extend the warmup iterations: some models are sensitive to the learning rate at the start of the training. You can extend the warmup iterations, e.g., change the  warm up it ers  from 500 to 1000 or 2000. 4. Add gradient clipping: some models requires gradient clipping to stabilize the training process. The default of  grad_clip  is  None , you can add gradient clippint to avoid gradients that are too large, i.e., set optimizer config=dict(_delete_  $\\bar{\\underline{{\\mathbf{\\alpha}}}}$  True, grad_clip  $\\leftrightharpoons$  dict(max_norm  $\\scriptstyle{|=35}$  , norm_type  $^{=2}$  )) in your config file. If your config does not inherits from any basic config that contains optimizer config  $\\vDash$  dict(grad_clip  $\\risingdotseq$  None) , you can simply add optimizer conf i  $\\mathfrak{g}=$  dict(grad_clip  $\\leftrightharpoons$  dict(max_norm  $\\scriptstyle{|=35}$  , norm_type  $^{=2}$  )) .\n\n \n• ’GPU out of memory” \n1. There are some scenarios when there are large amount of ground truth boxes, which may cause OOM during target assignment. You can set  gpu assign thr  $\\scriptstyle{\\mathrm{\\varepsilon}}=\\!\\mathtt{N}$   in the config of assigner thus the assigner will \ncalculate box overlaps through CPU when there are more than N GT boxes. 2. Set  with_cp  $=$  True  in the backbone. This uses the sublinear strategy in PyTorch to reduce GPU memory cost in the backbone. 3. Try mixed precision training using following the examples in  config/fp16 . The  loss_scale  might need further tuning for different models.\n\n \n• “Runtime Error: Expected to have finished reduction in the prior iteration before starting a new one” \n1. This error indicates that your module has parameters that were not used in producing loss. This phenomenon may be caused by running different branches in your code in DDP mode. 2. You can set  find unused parameters   $=$   True  in the config to solve the above problems or find those unused parameters manually.\n\n \n33.4 Evaluation \n• COCO Dataset, AP or  $\\mathrm{AR}={\\mathrm{-}}1$  \n1. According to the definition of COCO dataset, the small and medium areas in an image are less than 1024  $(32^{*}32)$  , 9216   $(96^{*}96)$  , respectively. 2. If the corresponding area has no object, the result of AP and AR will set to -1. "}
{"page": 183, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_183.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n176 Chapter 33. Frequently Asked Questions\n", "vlm_text": "MMDetection, Release 2.18.0\n\n176 Chapter 33. Frequently Asked Questions\n"}
{"page": 184, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_184.jpg", "ocr_text": "CHAPTER\nTHIRTYFOUR\n\nENGLISH\n\n177\n", "vlm_text": "ENGLISH "}
{"page": 185, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_185.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n178 Chapter 34. English\n", "vlm_text": "MMDetection, Release 2.18.0\n\n178 Chapter 34. English\n"}
{"page": 186, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_186.jpg", "ocr_text": "CHAPTER\nTHIRTYFIVE\n\n179\n", "vlm_text": "CHAPTER\nTHIRTYFIVE\n\n179\n"}
{"page": 187, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_187.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n180 Chapter 35.\n", "vlm_text": "MMDetection, Release 2.18.0\n\n180 Chapter 35.\n"}
{"page": 188, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_188.jpg", "ocr_text": "CHAPTER\nTHIRTYSIX\n\nMMDET.APIS\n\nasync mmdet.apis.async_inference_detector (model, imgs)\nAsync inference image(s) with the detector.\n\nParameters\n\n¢ model (nn. Module) — The loaded detector.\n\n* img(str | ndarray) — Either image files or loaded images.\nReturns Awaitable detection results.\n\nmmndet.apis.get_root_logger (log_file=None, log_level=20)\nGet root logger.\n\nParameters\n\n* log_file (str, optional) — File path of log. Defaults to None.\n\n* log_level (int, optional) — The level of logger. Defaults to logging. INFO.\nReturns The obtained logger\nReturn type logging.Logger\n\nmmdet.apis.inference_detector (model, imgs)\nInference image(s) with the detector.\n\nParameters\n¢ model (nn. Module) — The loaded detector.\n\n¢ imgs(str/ndarray or list[str/ndarray] or tuple[str/ndarray])—Either im-\nage files or loaded images.\n\nReturns If imgs is a list or tuple, the same length list type results will be returned, otherwise return\nthe detection results directly.\n\nmndet.apis.init_detector (config, checkpoint=None, device='cuda:0', cfg_options=None)\nInitialize a detector from config file.\n\nParameters\n* config (str or mmcv . Config) — Config file path or the config object.\n\n* checkpoint (str, optional) - Checkpoint path. If left as None, the model will not load\nany weights.\n\n* cfg_options (dict) — Options to override some settings in the used config.\nReturns The constructed detector.\n\nReturn type nn.Module\n\n181\n", "vlm_text": "MMDET.APIS \nasync  mmdet.apis. a sync inference detector ( model ,  imgs ) Async inference image(s) with the detector. \nParameters \n•  model  ( nn.Module ) – The loaded detector. •  img  ( str | ndarray ) – Either image files or loaded images. \nReturns  Awaitable detection results. \nmmdet.apis. get root logger ( log_file  $=$  None ,  log_level=20 ) Get root logger. \nParameters \n•  log_file  ( str, optional ) – File path of log. Defaults to None. •  log_level  ( int, optional ) – The level of logger. Defaults to logging.INFO. Returns  The obtained logger Return type  logging.Logger \nInference image(s) with the detector. \nParameters \n•  model  ( nn.Module ) – The loaded detector. •  imgs  ( str/ndarray or list[str/ndarray] or tuple[str/ndarray] ) – Either im- age files or loaded images.  If imgs is a list or tuple, the same length list type results will be returned, otherwise return \nthe detection results directly. \nmmdet.apis. in it detector ( config ,  checkpoin  $\\leftrightharpoons$  None ,  device  $\\mathbf{=}$  'cuda:0' ,  cf g options  $\\mathbf{\\hat{\\rho}}$  None ) Initialize a detector from config file. \nParameters \n•  config  (str or  mmcv.Config ) – Config file path or the config object. •  checkpoint  ( str, optional ) – Checkpoint path. If left as None, the model will not load any weights. •  cf g options  ( dict ) – Options to override some settings in the used config. \nReturns  The constructed detector. \nReturn type  nn.Module "}
{"page": 189, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_189.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmndet.apis.multi_gpu_test (model, data_loader, tmpdir=None, gpu_collect=False)\nTest model with multiple gpus.\n\nThis method tests model with multiple gpus and collects the results under two different modes: gpu and cpu\nmodes. By setting ‘gpu_collect=True’ it encodes results to gpu tensors and use gpu communication for results\ncollection. On cpu mode it saves the results on different gpus to ‘tmpdir’ and collects them by the rank 0 worker.\n\nParameters\n* model (nn. Module) — Model to be tested.\n¢ data_loader (nn. Dataloader) — Pytorch data loader.\n\n* tmpdir (str) — Path of directory to save the temporary results from different gpus under\ncpu mode.\n\n* gpu_collect (bool) — Option to use either gpu or cpu to collect results.\nReturns The prediction results.\nReturn type list\n\nmmdet.apis.set_random_seed (seed, deterministic=False)\nSet random seed.\n\nParameters\n* seed (int) — Seed to be used.\n\n¢ deterministic (bool) — Whether to set the deterministic option for CUDNN backend,\ni.e., set torch. backends.cudnn.deterministic to True and torch.backends.cudnn.benchmark to\nFalse. Default: False.\n\nmmndet.apis.show_result_pyplot (model, img, result, score_thr=0.3, title='result', wait_time=0)\nVisualize the detection results on the image.\n\nParameters\n¢ model (nn. Module) — The loaded detector.\n¢ img(str or np.ndarray) — Image filename or loaded image.\n\n* result (tuple[list] or list)-—The detection result, can be either (bbox, segm) or just\nbbox.\n\n* score_thr (float) — The threshold to visualize the bboxes and masks.\n* title (str) — Title of the pyplot figure.\n\n* wait_time (float) — Value of waitKey param. Default: 0.\n\n182 Chapter 36. mmdet.apis\n", "vlm_text": "mmdet.apis. multi gpu test ( model ,  data loader ,  tmpdir  $\\leftrightharpoons$  None ,  gpu collect=False ) Test model with multiple gpus. \nThis method tests model with multiple gpus and collects the results under two different modes: gpu and cpu modes. By setting ‘gpu collect  $\\risingdotseq$  True’ it encodes results to gpu tensors and use gpu communication for results collection. On cpu mode it saves the results on different gpus to ‘tmpdir’ and collects them by the rank 0 worker. \nParameters \n•  model  ( nn.Module ) – Model to be tested. •  data loader  ( nn.Dataloader ) – Pytorch data loader. •  tmpdir  ( str ) – Path of directory to save the temporary results from different gpus under cpu mode. •  gpu collect  ( bool ) – Option to use either gpu or cpu to collect results. \nReturns  The prediction results. \nReturn type  list \nmmdet.apis. set random seed ( seed ,  deterministic  $\\mathbf{\\dot{\\rho}}=\\mathbf{\\rho}$  False ) Set random seed. \nParameters \n•  seed  ( int ) – Seed to be used. •  deterministic  ( bool ) – Whether to set the deterministic option for CUDNN backend, i.e., set  torch.backends.cudnn.deterministic  to True and  torch.backends.cudnn.benchmark  to False. Default: False. \nmmdet.apis. show result py plot ( model ,  img ,  result ,  score_th  $\\scriptstyle r=0.3$  ,  title  $=$  'result' ,  wait_time  $\\mathrm{\\Sigma=}0$  ) Visualize the detection results on the image. \nParameters \n•  model  ( nn.Module ) – The loaded detector. •  img  ( str or np.ndarray ) – Image filename or loaded image. •  result  ( tuple[list] or list ) – The detection result, can be either (bbox, segm) or just bbox. •  score_thr  ( float ) – The threshold to visualize the bboxes and masks. •  title  ( str ) – Title of the pyplot figure. •  wait_time  ( float ) – Value of waitKey param. Default: 0. "}
{"page": 190, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_190.jpg", "ocr_text": "CHAPTER\nTHIRTYSEVEN\n\nMMDET.CORE\n\n37.1 anchor\n\nclass mmdet.core.anchor.AnchorGenerator (strides, ratios, scales=None, base_sizes=None,\n\nscale_major=True, octave_base_scale=None,\n\nscales_per_octave=None, centers=None, center_offset=0.0)\n\nStandard anchor generator for 2D anchor-based detectors.\n\nParameters\n\nstrides (list[int] | list[tuple[int, int]])-— Strides of anchors in multiple fea-\nture levels in order (w, h).\n\nratios (list [float ]) — The list of ratios between the height and width of anchors in a\nsingle level.\n\nscales (list[int] | None) - Anchor scales for anchors in a single level. It cannot be\nset at the same time if octave_base_scale and scales_per_octave are set.\n\nbase_sizes (list[int] | None) -The basic sizes of anchors in multiple levels. If None\nis given, strides will be used as base_sizes. (If strides are non square, the shortest stride is\ntaken.)\n\nscale_major (bool) — Whether to multiply scales first when generating base anchors. If\ntrue, the anchors in the same row will have the same scales. By default it is True in V2.0\n\noctave_base_scale (int) — The base scale of octave.\n\nscales_per_octave (int) — Number of scales for each octave. octave_base_scale and\nscales_per_octave are usually used in retinanet and the scales should be None when they are\nset.\n\ncenters (list[tuple[float, float]] | None) — The centers of the anchor relative\nto the feature grid center in multiple feature levels. By default it is set to be None and not\nused. If a list of tuple of float is given, they will be used to shift the centers of anchors.\n\ncenter_offset (float) — The offset of center in proportion to anchors’ width and height.\nBy default it is 0 in V2.0.\n\n183\n", "vlm_text": "MMDET.CORE \n37.1 anchor \nclass  mmdet.core.anchor. Anchor Generator ( strides ,  ratios ,  scales  $\\leftrightharpoons$  None ,  base_sizes  $\\leftrightharpoons$  None , scale major  $\\mathbf{\\dot{\\rho}}$  True ,  octave base scale  $\\mathbf{\\dot{\\rho}}$  None , scales per octave  $\\mathbf{=}$  None ,  centers  $\\mathbf{\\check{\\Sigma}}$  None ,  center offset  $\\mathord{:=}\\!\\!O.O$  ) Standard anchor generator for 2D anchor-based detectors. \nParameters \n•  strides  ( list[int] | list[tuple[int, int]] ) – Strides of anchors in multiple fea- ture levels in order (w, h). •  ratios  ( list[float] ) – The list of ratios between the height and width of anchors in a single level. •  scales  ( list[int] | None ) – Anchor scales for anchors in a single level. It cannot be set at the same time if  octave base scale  and  scales per octave  are set. •  base_sizes  ( list[int] | None ) – The basic sizes of anchors in multiple levels. If None is given, strides will be used as base_sizes. (If strides are non square, the shortest stride is taken.) •  scale major  ( bool ) – Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 •  octave base scale  ( int ) – The base scale of octave. •  scales per octave  ( int ) – Number of scales for each octave.  octave base scale  and scales per octave  are usually used in retinanet and the  scales  should be None when they are set. •  centers  ( list[tuple[float, float]] | None ) – The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. If a list of tuple of float is given, they will be used to shift the centers of anchors. •  center offset  ( float ) – The offset of center in proportion to anchors’ width and height. By default it is 0 in V2.0. "}
{"page": 191, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_191.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExamples\n\n>>> from mmdet.core import AnchorGenerator\n\n>>> self = AnchorGenerator([16], [1.], [1.], [9])\n\n>>> all_anchors = self.grid_priors([(2, 2)], device='cpu')\n>>> print(all_anchors)\n\n[tensor([[-4.5000, -4.5000, 4.5000, 4.5000\n1.5000, -4.5000, 20.5000, 4.5000],\n-4.5000, 11.5000, 4.5000, 20.5000],\n1.5000, 11.5000, 20.5000, 20.5000]])]\n\n>>> self = AnchorGenerator([16, 32], [1.], [1.], [9, 18])\n\n>>> all_anchors = self.grid_priors([(2, 2), (1, 1)], device='cpu')\n>>> print(all_anchors)\n[tensor([[-4.5000, -4.5000, 4.5000, 4.5000],\n1.5000, -4.5000, 20.5000, 4.5000],\n-4.5000, 11.5000, 4.5000, 20.5000],\n1.5000, 11.5000, 20.5000, 20.5000]]), tensor([[-9., -9., 9., 9.\n\n=]))]\n\ngen_base_anchors()\nGenerate base anchors.\n\nReturns Base anchors of a feature grid in multiple feature levels.\nReturn type list(torch.Tensor)\n\ngen_single_level_base_anchors (base_size, scales, ratios, center=None)\nGenerate base anchors of a single level.\n\nParameters\n¢ base_size(int | float) — Basic size of an anchor.\n¢ scales (torch. Tensor) — Scales of the anchor.\n\n* ratios (torch. Tensor) — The ratio between between the height and width of anchors in\na single level.\n\n* center (tuple[float], optional) — The center of the base anchor related to a single\nfeature grid. Defaults to None.\n\nReturns Anchors in a single-level feature maps.\nReturn type torch.Tensor\n\ngrid_anchors (featmap_sizes, device='cuda')\nGenerate grid anchors in multiple feature levels.\n\nParameters\n¢ featmap_sizes (list [tuple]) — List of feature map sizes in multiple feature levels.\n¢ device (str) — Device where the anchors will be put on.\n\nReturns Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N\n= width * height * num_base_anchors, width and height are the sizes of the corresponding\nfeature level, num_base_anchors is the number of anchors for that level.\n\nReturn type list[torch.Tensor]\n\ngrid_priors (featmap_sizes, dtype=torch.float32, device='cuda')\nGenerate grid anchors in multiple feature levels.\n\n184 Chapter 37. mmdet.core\n", "vlm_text": "Examples \n $>>$   from  mmdet.core  import  Anchor Generator\n\n  $>>$   self  $=$   Anchor Generator([ 16 ], [ 1. ], [ 1. ], [ 9 ])\n\n  $>>$   all anchors  $=$   self . grid priors([( 2 ,  2 )], device  $\\circeq$  ' cpu ' )\n\n  $>>$   print (all anchors)\n\n [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]])]\n\n >>>  self  $=$   Anchor Generator([ 16 ,  32 ], [ 1. ], [ 1. ], [ 9 ,  18 ])\n\n  $>>$   all anchors  $=$   self . grid priors([( 2 ,  2 ), ( 1 ,  1 )], device  $\\equiv^{1}$  ' cpu ' )\n\n  $>>$   print (all anchors)\n\n [tensor([[-4.5000, -4.5000, 4.5000, 4.5000], [11.5000, -4.5000, 20.5000, 4.5000], [-4.5000, 11.5000, 4.5000, 20.5000], [11.5000, 11.5000, 20.5000, 20.5000]]), tensor([[-9., -9., 9., 9.\n\n  $\\c_{\\rightarrow}]])]$  \ngen base anchors () \nGenerate base anchors. \nReturns  Base anchors of a feature grid in multiple feature levels. \nReturn type  list(torch.Tensor) \ngen single level base anchors ( base_size ,  scales ,  ratios ,  center  $\\leftrightharpoons$  None ) Generate base anchors of a single level. \nParameters \n•  base_size  ( int | float ) – Basic size of an anchor. \n•  scales  ( torch.Tensor ) – Scales of the anchor. •  ratios  ( torch.Tensor ) – The ratio between between the height and width of anchors in a single level. •  center  ( tuple[float], optional ) – The center of the base anchor related to a single feature grid. Defaults to None. Returns  Anchors in a single-level feature maps. Return type  torch.Tensor \ngrid anchors ( feat map sizes ,  device  $\\mathbf{=}$  'cuda' ) Generate grid anchors in multiple feature levels. \nParameters \n•  feat map sizes  ( list[tuple] ) – List of feature map sizes in multiple feature levels. •  device  ( str ) – Device where the anchors will be put on. Returns  Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N  $=$   width \\* height \\* num base anchors, width and height are the sizes of the corresponding feature level, num base anchors is the number of anchors for that level. Return type  list[torch.Tensor] \ngrid priors ( feat map sizes ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $=$  'cuda' ) Generate grid anchors in multiple feature levels. "}
{"page": 192, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_192.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n¢ featmap_sizes (list [tuple]) — List of feature map sizes in multiple feature levels.\n¢ dtype (torch. dtype) — Dtype of priors. Default: torch.float32.\n¢ device (str) — The device where the anchors will be put on.\n\nReturns Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N\n= width * height * num_base_anchors, width and height are the sizes of the corresponding\nfeature level, num_base_anchors is the number of anchors for that level.\n\nReturn type list[torch.Tensor]\n\nproperty num_base_anchors\ntotal number of base anchors in a feature grid\n\nType list[int]\n\nproperty num_base_priors\nThe number of priors (anchors) at a point on the feature grid\n\nType list[int]\n\nproperty num_levels\nnumber of feature levels that the generator will be applied\n\nType int\n\nsingle_level_grid_anchors (base_anchors, featmap_size, stride=(16, 16), device='cuda')\nGenerate grid anchors of a single level.\n\nNote: This function is usually called by method self.grid_anchors.\n\nParameters\n* base_anchors (torch. Tensor) — The base anchors of a feature grid.\n¢ featmap_size (tuple [int ]) — Size of the feature maps.\n\n¢ stride (tuple[int], optional) — Stride of the feature map in order (w, h). Defaults\nto (16, 16).\n\n¢ device (str, optional) — Device the tensor will be put on. Defaults to “cuda’.\nReturns Anchors in the overall feature maps.\nReturn type torch.Tensor\n\nsingle_level_grid_priors (featmap_size, level_idx, dtype=torch float32, device='cuda')\nGenerate grid anchors of a single level.\n\nNote: This function is usually called by method self.grid_priors.\n\nParameters\n¢ featmap_size (tuple [int ]) — Size of the feature maps.\n¢ level_idx (int) — The index of corresponding feature map level.\n\n* (obj (dtype) — torch.dtype): Date type of points.Defaults to torch. float32.\n\n37.1. anchor 185\n", "vlm_text": "Parameters \n•  feat map sizes  ( list[tuple] ) – List of feature map sizes in multiple feature levels. \n•  dtype  ( torch.dtype ) – Dtype of priors. Default: torch.float32. •  device  ( str ) – The device where the anchors will be put on. \nReturns  Anchors in multiple feature levels. The sizes of each tensor should be [N, 4], where N  $=$   width \\* height \\* num base anchors, width and height are the sizes of the corresponding feature level, num base anchors is the number of anchors for that level. \nReturn type  list[torch.Tensor] \nproperty num base anchors total number of base anchors in a feature grid \nType  list[int] \nproperty num base priors The number of priors (anchors) at a point on the feature grid \nType  list[int] \nproperty num_levels number of feature levels that the generator will be applied \nType  int \nsingle level grid anchors ( base anchors ,  feat map size ,  stride=(16, 16) ,  device  $=$  'cuda' ) Generate grid anchors of a single level. \nNote:  This function is usually called by method  self.grid anchors . \nParameters \n•  base anchors  ( torch.Tensor ) – The base anchors of a feature grid. \n•  feat map size  ( tuple[int] ) – Size of the feature maps. •  stride  ( tuple[int], optional ) – Stride of the feature map in order (w, h). Defaults to (16, 16). •  device  ( str, optional ) – Device the tensor will be put on. Defaults to ‘cuda’. Returns  Anchors in the overall feature maps. \n\nsingle level grid priors ( feat map size ,  level_idx ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $=$  'cuda' ) Generate grid anchors of a single level. \nNote:  This function is usually called by method  self.grid priors . \nParameters \n•  feat map size  ( tuple[int] ) – Size of the feature maps. •  level_idx  ( int ) – The index of corresponding feature map level. •  (obj  ( dtype ) –  torch.dtype ): Date type of points.Defaults to  torch.float32 . "}
{"page": 193, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_193.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ device (str, optional) — The device the tensor will be put on. Defaults to “cuda’.\nReturns Anchors in the overall feature maps.\nReturn type torch.Tensor\nsingle_level_valid_flags (featmap_size, valid_size, num_base_anchors, device='cuda')\nGenerate the valid flags of anchor in a single feature map.\nParameters\n¢ featmap_size (tuple [int ]) — The size of feature maps, arrange as (h, w).\n¢ valid_size (tuple[int]) — The valid size of the feature maps.\n¢ num_base_anchors (int) — The number of base anchors.\n¢ device (str, optional) — Device where the flags will be put on. Defaults to ‘cuda’.\nReturns The valid flags of each anchor in a single level feature map.\nReturn type torch.Tensor\n\nsparse_priors (prior_idxs, featmap_size, level_idx, dtype=torch.float32, device='cuda')\nGenerate sparse anchors according to the prior_idxs.\n\nParameters\n¢ prior_idxs (Tensor) — The index of corresponding anchors in the feature map.\n¢ featmap_size (tuple [int ]) — feature map size arrange as (h, w).\n¢ level_idx (int) — The level index of corresponding feature map.\n* (obj (device) — torch.dtype): Date type of points.Defaults to torch. float32.\n* (obj — torch.device): The device where the points is located.\nReturns\nAnchor with shape (N, 4), N should be equal to the length of prior_idxs.\nReturn type Tensor\n\nvalid_flags (featmap_sizes, pad_shape, device='cuda')\nGenerate valid flags of anchors in multiple feature levels.\n\nParameters\n¢ featmap_sizes (list (tuple)) — List of feature map sizes in multiple feature levels.\n* pad_shape (tuple) — The padded shape of the image.\n¢ device (str) — Device where the anchors will be put on.\n\nReturns Valid flags of anchors in multiple levels.\n\nReturn type list(torch.Tensor)\n\nclass mmdet.core.anchor.LegacyAnchorGenerator (strides, ratios, scales=None, base_sizes=None,\nscale_major=True, octave_base_scale=None,\nscales_per_octave=None, centers=None,\ncenter_offset=0.0)\nLegacy anchor generator used in MMDetection V 1.x.\n\nNote: Difference to the V2.0 anchor generator:\n\n186 Chapter 37. mmdet.core\n", "vlm_text": "•  device  ( str, optional ) – The device the tensor will be put on. Defaults to ‘cuda’. \nReturns  Anchors in the overall feature maps. \nReturn type  torch.Tensor \nsingle level valid flags ( feat map size ,  valid_size ,  num base anchors ,  device  $=$  'cuda' ) Generate the valid flags of anchor in a single feature map. \nParameters \n•  feat map size  ( tuple[int] ) – The size of feature maps, arrange as (h, w). •  valid_size  ( tuple[int] ) – The valid size of the feature maps. •  num base anchors  ( int ) – The number of base anchors. •  device  ( str, optional ) – Device where the flags will be put on. Defaults to ‘cuda’. \nReturns  The valid flags of each anchor in a single level feature map. \nReturn type  torch.Tensor \nsparse priors ( prior_idxs ,  feat map size ,  level_idx ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $\\mathbf{=}$  'cuda' ) Generate sparse anchors according to the  prior_idxs . \nParameters \n•  prior_idxs  ( Tensor ) – The index of corresponding anchors in the feature map. •  feat map size  ( tuple[int] ) – feature map size arrange as (h, w). •  level_idx  ( int ) – The level index of corresponding feature map. •  (obj  ( device ) –  torch.dtype ): Date type of points.Defaults to  torch.float32 . •  (obj  –  torch.device ): The device where the points is located. \nReturns \nAnchor with shape (N, 4), N should be equal to  the length of  prior_idxs . Return type  Tensor \nvalid flags ( feat map sizes ,  pad_shape ,  device  $=$  'cuda' ) Generate valid flags of anchors in multiple feature levels. \nParameters \n•  feat map sizes  ( list(tuple) ) – List of feature map sizes in multiple feature levels. •  pad_shape  ( tuple ) – The padded shape of the image. •  device  ( str ) – Device where the anchors will be put on. \nReturns  Valid flags of anchors in multiple levels. \nReturn type  list(torch.Tensor) \nclass  mmdet.core.anchor. Legacy Anchor Generator ( strides ,  ratios ,  scale  $\\mathfrak{z}{=}$  None ,  base_size  $\\leftrightharpoons$  None scale major  $\\mathbf{=}$  True ,  octave base scale=None , scales per octave  $\\mathbf{\\dot{\\rho}}$  None ,  centers  $\\mathbf{\\check{\\Sigma}}$  None , center off se  $\\mathord{\\tan}=\\!O.O.$  ) \nLegacy anchor generator used in MM Detection V1.x. \nNote:  Difference to the V2.0 anchor generator: "}
{"page": 194, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_194.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n1. The center offset of V1.x anchors are set to be 0.5 rather than 0.\n\n2. The width/height are minused by 1 when calculating the anchors’ centers and corners to meet the V1.x\ncoordinate system.\n\n3. The anchors’ corners are quantized.\n\nParameters\n\nExamples\n\nstrides (list[int] | list [tuple[int]])-Strides of anchors in multiple feature lev-\nels.\n\nratios (list [float ]) — The list of ratios between the height and width of anchors in a\nsingle level.\n\nscales (list[int] | None) - Anchor scales for anchors in a single level. It cannot be\nset at the same time if octave_base_scale and scales_per_octave are set.\n\nbase_sizes (list [int ])—The basic sizes of anchors in multiple levels. If None is given,\nstrides will be used to generate base_sizes.\n\nscale_major (bool) — Whether to multiply scales first when generating base anchors. If\ntrue, the anchors in the same row will have the same scales. By default it is True in V2.0\n\noctave_base_scale (int) — The base scale of octave.\n\nscales_per_octave (int) — Number of scales for each octave. octave_base_scale and\nscales_per_octave are usually used in retinanet and the scales should be None when they are\nset.\n\ncenters (list[tuple[float, float]] | None) — The centers of the anchor relative\nto the feature grid center in multiple feature levels. By default it is set to be None and not\nused. It a list of float is given, this list will be used to shift the centers of anchors.\n\ncenter_offset (float) — The offset of center in proportion to anchors’ width and height.\nBy default it is 0.5 in V2.0 but it should be 0.5 in v1.x models.\n\n>>> self =\n\n[tensor([[\n\n[16., 0., 24., 8.],\n[ 0., 16., 8., 24.],\n[16., 16., 24., 24.]])]\n\n>>> from mmdet.core import LegacyAnchorGenerator\n\nLegacyAnchorGenerator(\n\n>>> [16], [1.], [1.], [9], center_offset=0.5)\n>>> all_anchors = self.grid_anchors(((2, 2),), device='cpu')\n>>> print(all_anchors)\n\n0., O., 8., 8.],\n\ngen_single_level_base_anchors (base_size, scales, ratios, center=None)\nGenerate base anchors of a single level.\n\nNote: The width/height of anchors are minused by | when calculating the centers and corners to meet the\nV 1.x coordinate system.\n\nParameters\n\n37.1. anchor\n\n187\n\n", "vlm_text": "1. The center offset of V1.x anchors are set to be 0.5 rather than 0. \n2. The width/height are minused by 1 when calculating the anchors’ centers and corners to meet the V1.x coordinate system. \n3. The anchors’ corners are quantized. \nParameters \n•  strides  ( list[int] | list[tuple[int]] ) – Strides of anchors in multiple feature lev- \nels. •  ratios  ( list[float] ) – The list of ratios between the height and width of anchors in a single level. •  scales  ( list[int] | None ) – Anchor scales for anchors in a single level. It cannot be set at the same time if  octave base scale  and  scales per octave  are set. •  base_sizes  ( list[int] ) – The basic sizes of anchors in multiple levels. If None is given, strides will be used to generate base_sizes. •  scale major  ( bool ) – Whether to multiply scales first when generating base anchors. If true, the anchors in the same row will have the same scales. By default it is True in V2.0 •  octave base scale  ( int ) – The base scale of octave. •  scales per octave  ( int ) – Number of scales for each octave.  octave base scale  and scales per octave  are usually used in retinanet and the  scales  should be None when they are set. •  centers  ( list[tuple[float, float]] | None ) – The centers of the anchor relative to the feature grid center in multiple feature levels. By default it is set to be None and not used. It a list of float is given, this list will be used to shift the centers of anchors. •  center offset  ( float ) – The offset of center in proportion to anchors’ width and height. By default it is 0.5 in V2.0 but it should be 0.5 in v1.x models.\n\n \nExamples \n $>>$   from  mmdet.core  import  Legacy Anchor Generator\n\n  $>>$   self  $=$   Legacy Anchor Generator(\n\n  $>>$  [ 16 ], [ 1. ], [ 1. ], [ 9 ], center offset  $\\scriptstyle\\cdot=0\\,.\\;5$  )\n\n  $>>$   all anchors  $=$   self . grid anchors((( 2 ,  2 ),), device  $=\"$  ' cpu ' )\n\n  $>>$   print (all anchors)\n\n [tensor([[ 0., 0., 8., 8.], [16., 0., 24., 8.], [ 0., 16., 8., 24.], [16., 16., 24., 24.]])] \ngen single level base anchors ( base_size ,  scales ,  ratios ,  center  $=$  None ) Generate base anchors of a single level. \nNote:  The width/height of anchors are minused by 1 when calculating the centers and corners to meet the V1.x coordinate system. \nParameters "}
{"page": 195, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_195.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ base_size(int | float) — Basic size of an anchor.\n¢ scales (torch. Tensor) — Scales of the anchor.\n\n* ratios (torch. Tensor) — The ratio between between the height. and width of anchors\nin a single level.\n\n* center (tuple[float], optional) — The center of the base anchor related to a single\nfeature grid. Defaults to None.\n\nReturns Anchors in a single-level feature map.\nReturn type torch.Tensor\nclass mmdet.core.anchor.Mlv1PointGenerator (strides, offset=0.5)\nStandard points generator for multi-level (MIvl) feature maps in 2D points-based detectors.\n\nParameters\n\n* strides (list[int] | list[tuple[int, int]])-— Strides of anchors in multiple fea-\nture levels in order (w, h).\n\n* offset (float) — The offset of points, the value is normalized with corresponding stride.\nDefaults to 0.5.\n\ngrid_priors (featmap_sizes, dtype=torch.float32, device='cuda’, with_stride=False)\nGenerate grid points of multiple feature levels.\n\nParameters\n\n¢ featmap_sizes (list [tuple]) — List of feature map sizes in multiple feature levels,\neach size arrange as as (h, w).\n\n¢ dtype (dtype) — Dtype of priors. Default: torch.float32.\n¢ device (str) — The device where the anchors will be put on.\n¢ with_stride (bool) — Whether to concatenate the stride to the last dimension of points.\n\nReturns Points of multiple feature levels. The sizes of each tensor should be (N, 2) when with\nstride is False, where N = width * height, width and height are the sizes of the corresponding\nfeature level, and the last dimension 2 represent (coord_x, coord_y), otherwise the shape\nshould be (N, 4), and the last dimension 4 represent (coord_x, coord_y, stride_w, stride_h).\n\nReturn type list[torch.Tensor]\n\nproperty num_base_priors\nThe number of priors (points) at a point on the feature grid\nType list[int]\nproperty num_levels\nnumber of feature levels that the generator will be applied\nType int\n\nsingle_level_grid_priors (featmap_size, level_idx, dtype=torch.float32, device='cuda',\nwith_stride=False)\nGenerate grid Points of a single level.\n\nNote: This function is usually called by method self.grid_priors.\n\nParameters\n\n188 Chapter 37. mmdet.core\n", "vlm_text": "•  base_size  ( int | float ) – Basic size of an anchor. \n•  scales  ( torch.Tensor ) – Scales of the anchor. •  ratios  ( torch.Tensor ) – The ratio between between the height. and width of anchors in a single level. •  center  ( tuple[float], optional ) – The center of the base anchor related to a single feature grid. Defaults to None. \nReturns  Anchors in a single-level feature map. \nReturn type  torch.Tensor \nclass  mmdet.core.anchor. Ml vl Point Generator ( strides ,  offset=0.5 ) Standard points generator for multi-level (Mlvl) feature maps in 2D points-based detectors. \nParameters \n•  strides  ( list[int] | list[tuple[int, int]] ) – Strides of anchors in multiple fea- ture levels in order (w, h). •  offset  ( float ) – The offset of points, the value is normalized with corresponding stride. Defaults to 0.5. \ngrid priors ( feat map sizes ,  dtype  $\\mathbf{=}$  torch.float32 ,  device  $=$  'cuda' ,  with stride  $\\mathbf{\\dot{\\rho}}$  False ) Generate grid points of multiple feature levels. \nParameters \n•  feat map sizes  ( list[tuple] ) – List of feature map sizes in multiple feature levels, each size arrange as as (h, w). \n•  dtype  ( dtype ) – Dtype of priors. Default: torch.float32.  device  ( str ) – The device where the anchors will be put on.  with stride  ( bool ) – Whether to concatenate the stride to the last dimension of points. \n\n\nReturns  Points of multiple feature levels. The sizes of each tensor should be (N, 2) when with stride is  False , where  $\\mathbf{N}=$   width \\* height, width and height are the sizes of the corresponding feature level, and the last dimension 2 represent (coord_x, coord_y), otherwise the shape should be (N, 4), and the last dimension 4 represent (coord_x, coord_y, stride_w, stride_h). \nReturn type  list[torch.Tensor] \nproperty num base priors The number of priors (points) at a point on the feature grid \nType  list[int] \nproperty num_levels number of feature levels that the generator will be applied \nType  int \nsingle level grid priors ( feat map size ,  level_idx ,  dtyp  $\\scriptstyle{2=}$  torch.float32 ,  device  $=$  'cuda' , with stride  $\\mathbf{\\dot{\\rho}}=$  False ) \nGenerate grid Points of a single level. \nNote:  This function is usually called by method  self.grid priors . \nParameters "}
{"page": 196, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_196.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ featmap_size (tuple [int ]) — Size of the feature maps, arrange as (h, w).\n\nlevel_idx (int) — The index of corresponding feature map level.\n\n¢ dtype (dtype) — Dtype of priors. Default: torch.float32.\n\n¢ device (str, optional) — The device the tensor will be put on. Defaults to “cuda’.\n¢ with_stride (bool) — Concatenate the stride to the last dimension of points.\n\nReturns Points of single feature levels. The shape of tensor should be (N, 2) when with stride is\nFalse, where N = width * height, width and height are the sizes of the corresponding feature\nlevel, and the last dimension 2 represent (coord_x, coord_y), otherwise the shape should be\n(N, 4), and the last dimension 4 represent (coord_x, coord_y, stride_w, stride_h).\n\nReturn type Tensor\nsingle_level_valid_flags (featmap_size, valid_size, device='cuda')\nGenerate the valid flags of points of a single feature map.\nParameters\n¢ featmap_size (tuple [int ]) — The size of feature maps, arrange as as (h, w).\n\n¢ valid_size (tuple[int]) — The valid size of the feature maps. The size arrange as as\n(h, w).\n\n¢ device (str, optional) -The device where the flags will be put on. Defaults to ‘cuda’.\nReturns The valid flags of each points in a single level feature map.\nReturn type torch.Tensor\n\nsparse_priors (prior_idxs, featmap_size, level_idx, dtype=torch.float32, device='cuda')\nGenerate sparse points according to the prior_idxs.\n\nParameters\n¢ prior_idxs (Tensor) — The index of corresponding anchors in the feature map.\n¢ featmap_size (tuple[int J) — feature map size arrange as (w, h).\n¢ level_idx (int) — The level index of corresponding feature map.\n* (obj (device) — torch.dtype): Date type of points. Defaults to torch. float32.\n* (obj — torch.device): The device where the points is located.\n\nReturns Anchor with shape (N, 2), N should be equal to the length of prior_idxs. And last\ndimension 2 represent (coord_x, coord_y).\n\nReturn type Tensor\n\nvalid_flags (featmap_sizes, pad_shape, device='cuda')\nGenerate valid flags of points of multiple feature levels.\n\nParameters\n\n¢ featmap_sizes (list (tuple)) — List of feature map sizes in multiple feature levels,\neach size arrange as as (h, w).\n\n* pad_shape (tuple (int )) — The padded shape of the image, arrange as (h, w).\n¢ device (str) — The device where the anchors will be put on.\nReturns Valid flags of points of multiple levels.\n\nReturn type list(torch.Tensor)\n\n37.1. anchor 189\n", "vlm_text": "•  feat map size  ( tuple[int] ) – Size of the feature maps, arrange as (h, w). •  level_idx  ( int ) – The index of corresponding feature map level. •  dtype  ( dtype ) – Dtype of priors. Default: torch.float32. •  device  ( str, optional ) – The device the tensor will be put on. Defaults to ‘cuda’. •  with stride  ( bool ) – Concatenate the stride to the last dimension of points. \nReturns  Points of single feature levels. The shape of tensor should be (N, 2) when with stride is False , where  $\\mathbf{N}=$  width \\* height, width and height are the sizes of the corresponding feature level, and the last dimension 2 represent (coord_x, coord_y), otherwise the shape should be (N, 4), and the last dimension 4 represent (coord_x, coord_y, stride_w, stride_h). \nReturn type  Tensor \nsingle level valid flags ( feat map size ,  valid_size ,  device  $=$  'cuda' ) Generate the valid flags of points of a single feature map. \nParameters \n feat map size  ( tuple[int] ) – The size of feature maps, arrange as as (h, w). •  valid_size  ( tuple[int] ) – The valid size of the feature maps. The size arrange as as (h, w). •  device  ( str, optional ) – The device where the flags will be put on. Defaults to ‘cuda’. \nReturns  The valid flags of each points in a single level feature map. Return type  torch.Tensor \nsparse priors ( prior_idxs ,  feat map size ,  level_idx ,  dtype  $=$  torch.float32 ,  device  $\\mathbf{=}$  'cuda' ) Generate sparse points according to the  prior_idxs . \nParameters \n•  prior_idxs  ( Tensor ) – The index of corresponding anchors in the feature map. •  feat map size  ( tuple[int] ) – feature map size arrange as (w, h). •  level_idx  ( int ) – The level index of corresponding feature map. •  (obj  ( device ) –  torch.dtype ): Date type of points. Defaults to  torch.float32 . •  (obj  –  torch.device ): The device where the points is located. Returns  Anchor with shape (N, 2), N should be equal to the length of  prior_idxs . And last dimension 2 represent (coord_x, coord_y). Return type  Tensor \nvalid flags ( feat map sizes ,  pad_shape ,  device  $\\mathbf{=}$  'cuda' ) Generate valid flags of points of multiple feature levels. \nParameters \n•  feat map sizes  ( list(tuple) ) – List of feature map sizes in multiple feature levels, each size arrange as as (h, w). •  pad_shape  ( tuple(int) ) – The padded shape of the image, arrange as (h, w). •  device  ( str ) – The device where the anchors will be put on. \nReturns  Valid flags of points of multiple levels. \nReturn type  list(torch.Tensor) "}
{"page": 197, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_197.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.core. anchor .YOLOAnchorGenerator (strides, base_sizes)\nAnchor generator for YOLO.\n\nParameters\n\n* strides (list[int] | list[tuple[int, int]])-— Strides of anchors in multiple fea-\nture levels.\n\n* base_sizes (list[list[tuple[int, int]]])-The basic sizes of anchors in multiple\nlevels.\n\ngen_base_anchors()\nGenerate base anchors.\n\nReturns Base anchors of a feature grid in multiple feature levels.\nReturn type list(torch.Tensor)\n\ngen_single_level_base_anchors (base_sizes_per_level, center=None)\nGenerate base anchors of a single level.\n\nParameters\n¢ base_sizes_per_level (list[tuple[int, int]])-— Basic sizes of anchors.\n\n* center (tuple[float], optional) — The center of the base anchor related to a single\nfeature grid. Defaults to None.\n\nReturns Anchors in a single-level feature maps.\nReturn type torch.Tensor\n\nproperty num_levels\nnumber of feature levels that the generator will be applied\n\nType int\n\nresponsible_flags (featmap_sizes, gt_bboxes, device='cuda')\nGenerate responsible anchor flags of grid cells in multiple scales.\n\nParameters\n¢ featmap_sizes (list (tuple)) — List of feature map sizes in multiple feature levels.\n* gt_bboxes (Tensor) — Ground truth boxes, shape (n, 4).\n¢ device (str) — Device where the anchors will be put on.\n\nReturns responsible flags of anchors in multiple level\n\nReturn type list(torch.Tensor)\n\nsingle_level_responsible_flags (featmap_size, gt_bboxes, stride, num_base_anchors, device='cuda')\nGenerate the responsible flags of anchor in a single feature map.\n\nParameters\n¢ featmap_size (tuple [int ]) — The size of feature maps.\n* gt_bboxes (Tensor) — Ground truth boxes, shape (n, 4).\n¢ stride (tuple (int )) — stride of current level\n¢ num_base_anchors (int) — The number of base anchors.\n¢ device (str, optional) — Device where the flags will be put on. Defaults to ‘cuda’.\n\nReturns The valid flags of each anchor in a single level feature map.\n\n190 Chapter 37. mmdet.core\n", "vlm_text": "class  mmdet.core.anchor. YO LO Anchor Generator ( strides ,  base_sizes ) Anchor generator for YOLO. \nParameters \n•  strides  ( list[int] | list[tuple[int, int]] ) – Strides of anchors in multiple fea- ture levels. •  base_sizes  ( list[list[tuple[int, int]]] ) – The basic sizes of anchors in multiple levels. \ngen base anchors () \nReturns  Base anchors of a feature grid in multiple feature levels. \nReturn type  list(torch.Tensor) \ngen single level base anchors ( base sizes per level ,  center=None ) Generate base anchors of a single level. \nParameters \n•  base sizes per level  ( list[tuple[int, int]] ) – Basic sizes of anchors. •  center  ( tuple[float], optional ) – The center of the base anchor related to a single feature grid. Defaults to None. Returns  Anchors in a single-level feature maps. Return type  torch.Tensor \nproperty num_levels number of feature levels that the generator will be applied \nType  int \nresponsible flags ( feat map sizes ,  gt_bboxes ,  device  $=$  cuda' ) Generate responsible anchor flags of grid cells in multiple scales. \nParameters \n•  feat map sizes  ( list(tuple) ) – List of feature map sizes in multiple feature levels. •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (n, 4). •  device  ( str ) – Device where the anchors will be put on. Returns  responsible flags of anchors in multiple level Return type  list(torch.Tensor) \nsingle level responsible flags ( feat map size ,  gt_bboxes ,  stride ,  num base anchors ,  device  $=$  'cuda' ) Generate the responsible flags of anchor in a single feature map. \nParameters \n•  feat map size  ( tuple[int] ) – The size of feature maps. •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (n, 4). •  stride  ( tuple(int) ) – stride of current level •  num base anchors  ( int ) – The number of base anchors. •  device  ( str, optional ) – Device where the flags will be put on. Defaults to ‘cuda’. Returns  The valid flags of each anchor in a single level feature map. "}
{"page": 198, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_198.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type torch.Tensor\n\nmmdet.core.anchor.anchor_inside_flags (flat_anchors, valid_flags, img_shape, allowed_border=0)\nCheck whether the anchors are inside the border.\n\nParameters\n\n¢ flat_anchors (torch. Tensor) — Flatten anchors, shape (n, 4).\n\n* valid_flags (torch. Tensor) — An existing valid flags of anchors.\n\n¢ img_shape (tuple (int)) — Shape of current image.\n\n¢ allowed_border (int, optional) — The border to allow the valid anchor. Defaults to 0.\nReturns Flags indicating whether the anchors are inside a valid range.\nReturn type torch.Tensor\n\nmmdet.core.anchor.calc_region(bbox, ratio, featmap_size=None)\nCalculate a proportional bbox region.\n\nThe bbox center are fixed and the new h’ and w’ is h * ratio and w * ratio.\nParameters\n* bbox (Tensor) — Bboxes to calculate regions, shape (n, 4).\n* ratio (float) — Ratio of the output region.\n¢ featmap_size (tuple) — Feature map size used for clipping the boundary.\nReturns x1, yl, x2, y2\nReturn type tuple\n\nmmdet.core.anchor.images_to_levels (target, num_levels)\nConvert targets by image to targets by feature level.\n\n[target_img0, target_img1] -> [target_level0, target_levell, ...]\n\n37.2 bbox\n\nclass mmdet.core.bbox.AssignResult (num_gts, gt_inds, max_overlaps, labels=None)\nStores assignments between predicted and truth boxes.\n\nnum_gts\nthe number of truth boxes considered when computing this assignment\n\nType int\n\ngt_inds\nfor each predicted box indicates the 1-based index of the assigned truth box. 0 means unassigned and -1\nmeans ignore.\n\nType LongTensor\n\nmax_overlaps\nthe iou between the predicted box and its assigned truth box.\n\nType FloatTensor\n\nlabels\nIf specified, for each predicted box indicates the category label of the assigned truth box.\n\n37.2. bbox 191\n", "vlm_text": "Return type  torch.Tensor \nmmdet.core.anchor. anchor inside flags ( flat anchors ,  valid flags ,  img_shape ,  allowed border  $\\mathord{\\leftrightharpoons}\\!O_{.}$  ) Check whether the anchors are inside the border. \nParameters \n•  flat anchors  ( torch.Tensor ) – Flatten anchors, shape (n, 4). •  valid flags  ( torch.Tensor ) – An existing valid flags of anchors. •  img_shape  ( tuple(int) ) – Shape of current image. •  allowed border  ( int, optional ) – The border to allow the valid anchor. Defaults to 0. \nReturns  Flags indicating whether the anchors are inside a valid range. \nReturn type  torch.Tensor \nmmdet.core.anchor. calc region ( bbox ,  ratio ,  feat map size  $=$  None ) Calculate a proportional bbox region. \nThe bbox center are fixed and the new h’ and w’ is h \\* ratio and w \\* ratio. \nParameters \n•  bbox  ( Tensor ) – Bboxes to calculate regions, shape (n, 4). •  ratio  ( float ) – Ratio of the output region. •  feat map size  ( tuple ) – Feature map size used for clipping the boundary. Returns  x1, y1, x2, y2 Return type  tuple \nConvert targets by image to targets by feature level. [target img 0, target img 1]  $->$   [target level 0, target level 1, ...] \n37.2 bbox \nclass  mmdet.core.bbox. Assign Result ( num_gts ,  gt_inds ,  max overlaps ,  labels  $:=$  None ) Stores assignments between predicted and truth boxes. \nnum_gts the number of truth boxes considered when computing this assignment \nType  int gt_inds for each predicted box indicates the 1-based index of the assigned truth box. 0 means unassigned and -1 means ignore. Type  LongTensor max overlaps the iou between the predicted box and its assigned truth box. Type  Float Tensor labels If specified, for each predicted box indicates the category label of the assigned truth box. "}
{"page": 199, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_199.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nType None | LongTensor\n\nExample\n\n>>> # An assign result between 4 predicted boxes and 9 true boxes\n\n>>> # where only two boxes were assigned.\n\n>>> num_gts = 9\n\n>>> max_overlaps = torch.LongTensor([0, .5, .9, 0])\n\n>>> gt_inds = torch.LongTensor([-1, 1, 2, 0])\n\n>>> labels = torch.LongTensor([0, 3, 4, 0])\n\n>>> self = AssignResult(num_gts, gt_inds, max_overlaps, labels)\n\n>>> print(str(self)) # xdoctest: +IGNORE_WANT\n\n<AssignResult(num_gts=9, gt_inds.shape=(4,), max_overlaps.shape=(4,),\nlabels.shape=(4,))>\n\n>>> # Force addition of gt labels (when adding gt as proposals)\n\n>>> new_labels = torch.LongTensor([3, 4, 5])\n\n>>> self.add_gt_(new_labels)\n\n>>> print(str(self)) # xdoctest: +IGNORE_WANT\n\n<AssignResult(num_gts=9, gt_inds.shape=(7,), max_overlaps.shape=(7,),\nlabels.shape=(7,))>\n\nadd_gt_(gt_labels)\nAdd ground truth as assigned results.\n\nParameters gt_labels (torch. Tensor) — Labels of gt boxes\n\nget_extra_property (key)\nGet user-defined property.\n\nproperty info\na dictionary of info about the object\n\nType dict\n\nproperty num_preds\nthe number of predictions in this assignment\n\nType int\n\nclassmethod random(**kwargs)\nCreate random AssignResult for tests or debugging.\n\nParameters\n* num_preds — number of predicted boxes\n\n* num_gts — number of true boxes\n\n* p_ignore (float) — probability of a predicted box assigned to an ignored truth\n\n* p_assigned (float) — probability of a predicted box not being assigned\n\n¢ p_use_label (float | bool) -— with labels or not\n\n¢ rng(None | int | numpy.random.RandomState) — seed or state\nReturns Randomly generated assign results.\n\nReturn type AssignResult\n\n192\n\nChapter 37.\n\nmmdet.core\n\n", "vlm_text": "Type  None | LongTensor\n\n \nExample \n $>>$   # An assign result between 4 predicted boxes and 9 true boxes\n\n  $>>$   # where only two boxes were assigned.\n\n  $>>$   num_gts  $\\c=~9\n\n$   $>>$   max overlaps  $=$   torch . LongTensor([ 0 ,  .5 ,  .9 ,  0 ])\n\n  $>>$   gt_inds  $=$   torch . LongTensor([ - 1 ,  1 ,  2 ,  0 ])\n\n  $>>$   labels  $=$   torch . LongTensor([ 0 ,  3 ,  4 ,  0 ])\n\n  $>>$   self  $=$   Assign Result(num_gts, gt_inds, max overlaps, labels)\n\n >>>  print ( str ( self )) # xdoctest: +IGNORE WANT\n\n <Assign Result(num_gts  $\\scriptstyle{\\varepsilon=9}$  , gt_inds.shape  $=$  (4,), max overlaps.shape  $=$  (4,), labels.shape  $=$  (4,))>\n\n >>>  # Force addition of gt labels (when adding gt as proposals)\n\n  $>>$   new_labels  $=$   torch . LongTensor([ 3 ,  4 ,  5 ])\n\n  $>>$   self . add_gt_(new_labels)\n\n  $>>$   print ( str ( self )) # xdoctest:   $^+$  IGNORE WANT\n\n <Assign Result(num_gts  $_{;=9}$  , gt_inds.shape  $\\mathord{\\mathfrak{z}}(7\\,,)$  , max overlaps.shape  $=$  (7,), labels.shape  $=\\!(7,))\\!>$  \nType  dict \nCreate random Assign Result for tests or debugging. Parameters •  num_preds  – number of predicted boxes •  num_gts  – number of true boxes •  p_ignore  ( float ) – probability of a predicted box assigned to an ignored truth •  p_assigned  ( float ) – probability of a predicted box not being assigned •  p use label  ( float | bool ) – with labels or not •  rng  ( None | int | numpy.random.Random State ) – seed or state Returns  Randomly generated assign results. Return type  Assign Result "}
{"page": 200, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_200.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> from mmdet.core.bbox.assigners.assign_result import * # NOQA\n>>> self = AssignResult.random()\n>>> print(self.info)\n\nset_extra_property (key, value)\nSet user-defined new property.\n\nclass mmdet.core.bbox.BaseAssigner\nBase assigner that assigns boxes to ground truth boxes.\n\nabstract assign(bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None)\nAssign boxes to either a ground truth boxes or a negative boxes.\n\nclass mmdet.core.bbox.BaseBBoxCoder (**kwargs)\nBase bounding box coder.\n\nabstract decode (bboxes, bboxes_pred)\nDecode the predicted bboxes according to prediction and base boxes.\n\nabstract encode (bboxes, gt_bboxes)\nEncode deltas between bboxes and ground truth boxes.\n\nclass mmdet.core.bbox.BaseSampler (num, pos_fraction, neg_pos_ub=- 1, add_gt_as_proposals=True,\n**kwargs)\nBase class of samplers.\n\nsample (assign_result, bboxes, gt_bboxes, gt_labels=None, **kwargs)\nSample positive and negative bboxes.\n\nThis is a simple implementation of bbox sampling given candidates, assigning results and ground truth\nbboxes.\n\nParameters\n\n* assign_result (AssignResult) — Bbox assigning results.\n\n* bboxes (Tensor) — Boxes to be sampled from.\n\n¢ gt_bboxes (Tensor) — Ground truth bboxes.\n\n¢ gt_labels (Tensor, optional) — Class labels of ground truth bboxes.\nReturns Sampling result.\n\nReturn type SamplingResult\n\nExample\n\n>>> from mmdet.core.bbox import RandomSampler\n\n>>> from mmdet.core.bbox import AssignResult\n\n>>> from mmdet.core.bbox.demodata import ensure_rng, random_boxes\n>>> rng = ensure_rng(None)\n\n>>> assign_result = AssignResult.random(rng=rng)\n\n>>> bboxes = random_boxes(assign_result.num_preds, rng=rng)\n\n>>> gt_bboxes = random_boxes(assign_result.num_gts, rng=rng)\n\n>>> gt_labels = None\n\n>>> self = RandomSampler(num=32, pos_fraction=0.5, neg_pos_ub=-1,\n\n(continues on next page)\n\n37.2. bbox 193\n", "vlm_text": "Example \nThe image displays a snippet of Python code set within a table-like format. The code is as follows:\n\n```python\n>>> from mmdet.core.bbox.assigners.assign_result import *  # NOQA\n>>> self = AssignResult.random()\n>>> print(self.info)\n```\n\nThis code is likely part of a demonstration or example related to object detection or bounding box assignment, possibly utilizing the MMDetection library (a PyTorch-based object detection toolbox). Here's a breakdown of what each line does:\n\n1. `from mmdet.core.bbox.assigners.assign_result import *  # NOQA`: Imports all classes and functions from the `assign_result` module within the `bbox` (bounding box) package of the MMDetection library. The `# NOQA` comment is typically used to tell linters to ignore this line, possibly because wildcard imports are generally discouraged for clarity reasons.\n\n2. `self = AssignResult.random()`: Creates an instance of the `AssignResult` class using a method `random()`. This suggests that `AssignResult` has a static or class method named `random` that generates a randomized instance, likely for testing or demonstration purposes.\n\n3. `print(self.info)`: Prints the `info` attribute or method of the `AssignResult` instance. This could display diagnostic information or a summary of the properties associated with the `AssignResult` instance.\nset extra property ( key ,  value ) Set user-defined new property. \nclass  mmdet.core.bbox. Base As signer Base assigner that assigns boxes to ground truth boxes. \nabstract assign ( bboxes ,  gt_bboxes ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_labels=None ) Assign boxes to either a ground truth boxes or a negative boxes. \nclass  mmdet.core.bbox. Base B Box Code r ( \\*\\*kwargs ) Base bounding box coder. \nabstract decode ( bboxes ,  b boxes p red ) Decode the predicted bboxes according to prediction and base boxes. \nabstract encode ( bboxes ,  gt_bboxes ) Encode deltas between bboxes and ground truth boxes. \nclass  mmdet.core.bbox. Base Sampler ( num ,  pos fraction ,  neg_pos_ub=- 1 ,  add gt as proposals  $\\mathbf{\\tilde{=}}$  True , \\*\\*kwargs ) \n(  $\\leftrightharpoons$  ) Sample positive and negative bboxes. This is a simple implementation of bbox sampling given candidates, assigning results and ground truth bboxes. \nParameters •  assign result  ( Assign Result ) – Bbox assigning results. •  bboxes  ( Tensor ) – Boxes to be sampled from. •  gt_bboxes  ( Tensor ) – Ground truth bboxes. •  gt_labels  ( Tensor, optional ) – Class labels of ground truth bboxes. Returns  Sampling result. Return type  Sampling Result \nExample \nThe image shows a Python code snippet related to a machine learning or computer vision framework, likely used for object detection. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `RandomSampler` and `AssignResult` from `mmdet.core.bbox`.\n   - `ensure_rng` and `random_boxes` from `mmdet.core.bbox.demodata`.\n\n2. **Code**:\n   - Initializes a random number generator `rng` using `ensure_rng(None)`.\n   - Creates a random assignment result using `AssignResult.random(rng=rng)`.\n   - Generates random bounding boxes (`bboxes`) and ground truth bounding boxes (`gt_bboxes`) based on the numbers of predictions and ground truths, respectively.\n   - `gt_labels` is set to `None`, indicating no ground truth labels are assigned.\n   - Initializes a `RandomSampler` object with parameters `num=32`, `pos_fraction=0.5`, and `neg_pos_ub=-1`.\n\nThis snippet appears to be part of a setup script for testing or simulating detection models."}
{"page": 201, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_201.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n>>> add_gt_as_proposals=False)\n>>> self = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n\nclass mmdet.core.bbox.BboxOverlaps2D(scale=1.0, dtype=None)\n2D Overlaps (e.g. loUs, GloUs) Calculator.\n\nclass mmdet.core.bbox.CenterRegionAssigner (pos_scale, neg_scale, min_pos_iof=0.01,\nignore_gt_scale=0.5, foreground_dominate=False,\niou_calculator={'type': 'BboxOverlaps2D'})\nAssign pixels at the center region of a bbox as positive.\n\nEach proposals will be assigned with -/, 0, or a positive integer indicating the ground truth index. - -1: negative\nsamples - semi-positive numbers: positive sample, index (0-based) of assigned gt\n\nParameters\n* pos_scale (float) — Threshold within which pixels are labelled as positive.\n* neg_scale (float) — Threshold above which pixels are labelled as positive.\n\n* min_pos_iof (float) — Minimum iof of a pixel with a gt to be labelled as positive. Default:\nle-2\n\n* ignore_gt_scale (float) — Threshold within which the pixels are ignored when the gt is\nlabelled as shadowed. Default: 0.5\n\n¢ foreground_dominate (bool) — If True, the bbox will be assigned as positive when a\ngt’s kernel region overlaps with another’s shadowed (ignored) region, otherwise it is set as\nignored. Default to False.\n\nassign (bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None)\nAssign gt to bboxes.\n\nThis method assigns gts to every bbox (proposal/anchor), each bbox will be assigned with -1, or a semi-\npositive number. -1 means negative sample, semi-positive number is the index (0-based) of assigned gt.\n\nParameters\n* bboxes (Tensor) — Bounding boxes to be assigned, shape(n, 4).\n* gt_bboxes (Tensor) — Groundtruth boxes, shape (k, 4).\n\n¢ gt_bboxes_ignore (tensor, optional) — Ground truth bboxes that are labelled as\nignored, e.g., crowd boxes in COCO.\n\n¢ gt_labels (tensor, optional) — Label of gt_bboxes, shape (num_gts,).\n\nReturns The assigned result. Note that shadowed_labels of shape (N, 2) is also added as an\nassign_result attribute. shadowed_labels is a tensor composed of N pairs of anchor_ind,\nclass_label], where N is the number of anchors that lie in the outer region of a gt, anchor_ind\nis the shadowed anchor index and class_label is the shadowed class label.\n\nReturn type AssignResult\n\n194 Chapter 37. mmdet.core\n", "vlm_text": "The image shows a snippet of Python code. It includes:\n\n```python\nadd_gt_as_proposals=False\nself = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n```\n\n- `add_gt_as_proposals=False` likely indicates a parameter setting where ground truth is not added as proposals.\n- `self = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)` is a method call for sampling with the inputs `assign_result`, `bboxes`, `gt_bboxes`, and `gt_labels`.\nclass  mmdet.core.bbox. B box Overlaps 2 D ( scale  $\\mathrm{=}l.0$  ,  dtype  $=$  None ) 2D Overlaps (e.g. IoUs, GIoUs) Calculator. \nclass  mmdet.core.bbox. Center Region As signer ( pos_scale ,  neg_scale ,  min_pos_io  $\\it\\Delta\\it{f}=0.01$  , ignore gt scale  $\\mathrm{\\Sigma=}0.5$  ,  foreground dominate  $\\mathbf{\\dot{\\rho}}=$  False , i ou calculator  $\\mathbf{=}$  {'type': 'B box Overlaps 2 D'} ) Assign pixels at the center region of a bbox as positive. \nEach proposals will be assigned with    $\\cdot l,\\theta$  , or a positive integer indicating the ground truth index. - -1: negative samples - semi-positive numbers: positive sample, index (0-based) of assigned gt \nParameters \n•  pos_scale  ( float ) – Threshold within which pixels are labelled as positive. •  neg_scale  ( float ) – Threshold above which pixels are labelled as positive. •  min pos i of  ( float ) – Minimum iof of a pixel with a gt to be labelled as positive. Default: 1e-2 •  ignore gt scale  ( float ) – Threshold within which the pixels are ignored when the gt is labelled as shadowed. Default: 0.5 •  foreground dominate  ( bool ) – If True, the bbox will be assigned as positive when a gt’s kernel region overlaps with another’s shadowed (ignored) region, otherwise it is set as ignored. Default to False. \nassign ( bboxes ,  gt_bboxes ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_labels  $\\mathbf{\\check{\\Sigma}}$  None ) Assign gt to bboxes. \nThis method assigns gts to every bbox (proposal/anchor), each bbox will be assigned with -1, or a semi- positive number. -1 means negative sample, semi-positive number is the index (0-based) of assigned gt. \nParameters \n•  bboxes  ( Tensor ) – Bounding boxes to be assigned, shape(n, 4). •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (k, 4). •  gt b boxes ignore  ( tensor, optional ) – Ground truth bboxes that are labelled as ignored , e.g., crowd boxes in COCO. •  gt_labels  ( tensor, optional ) – Label of gt_bboxes, shape (num_gts,). \nReturns  The assigned result. Note that shadowed labels of shape (N, 2) is also added as an assign result  attribute.  shadowed labels  is a tensor composed of N pairs of anchor_ind, class label], where N is the number of anchors that lie in the outer region of a gt, anchor_ind is the shadowed anchor index and class label is the shadowed class label. \nReturn type  Assign Result "}
{"page": 202, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_202.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> self = CenterRegionAssigner(0.2, 0.2)\n\n>>> bboxes = torch.Tensor([[0, 9, 10, 10], [10, 10, 20, 20]])\n>>> gt_bboxes = torch.Tensor([[0, 0, 10, 10]])\n\n>>> assign_result = self.assign(bboxes, gt_bboxes)\n\n>>> expected_gt_inds = torch.LongTensor([1, 0])\n\n>>> assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\nassign_one_hot_gt_indices (is_bbox_in_gt_core, is_bbox_in_gt_shadow, gt_priority=None)\nAssign only one gt index to each prior box.\n\nGts with large gt_priority are more likely to be assigned.\nParameters\n\n¢ is_bbox_in_gt_core (Tensor) — Bool tensor indicating the bbox center is in the core\narea of a gt (e.g. 0-0.2). Shape: (num_prior, num_gt).\n\n¢ is_bbox_in_gt_shadow (Tensor) — Bool tensor indicating the bbox center is in the shad-\nowed area of a gt (e.g. 0.2-0.5). Shape: (num_prior, num_gt).\n\n¢ gt_priority (Tensor) — Priorities of gts. The gt with a higher priority is more likely to\nbe assigned to the bbox when the bbox match with multiple gts. Shape: (num_gt, ).\n\nReturns\nReturns (assigned_gt_inds, shadowed_gt_inds).\n\n* assigned_gt_inds: The assigned gt index of each prior bbox (i.e. index from | to num_gts).\nShape: (num_prior, ).\n\n¢ shadowed_gt_inds: shadowed gt indices. It is a tensor of shape (num_ignore, 2) with first\ncolumn being the shadowed prior bbox indices and the second column the shadowed gt\nindices (1-based).\n\nReturn type tuple\n\nget_gt_priorities(gt_bboxes)\nGet gt priorities according to their areas.\n\nSmaller gt has higher priority.\nParameters gt_bboxes (Tensor) — Ground truth boxes, shape (k, 4).\n\nReturns The priority of gts so that gts with larger priority is more likely to be assigned. Shape\n(k, )\nReturn type Tensor\n\nclass mmdet.core.bbox.CombinedSampler (pos_sampler, neg_sampler, **kwargs)\nA sampler that combines positive sampler and negative sampler.\n\nclass mmdet.core.bbox.DeltaXYWHBBoxCoder (target_means=(0.0, 0.0, 0.0, 0.0), target_stds=(1.0, 1.0, 1.0,\n1.0), clip_border=True, add_ctr_clamp=False, ctr_clamp=32)\nDelta XYWH BBox coder.\n\nFollowing the practice in R-CNN, this coder encodes bbox (x1, y1, x2, y2) into delta (dx, dy, dw, dh) and decodes\ndelta (dx, dy, dw, dh) back to original bbox (x1, y1, x2, y2).\n\nParameters\n\n* target_means (Sequence [float ])—Denormalizing means of target for delta coordinates\n\n37.2. bbox 195\n", "vlm_text": "Example \n>>>  self  $=$   Center Region As signer( 0.2 ,  0.2 ) >>>  bboxes  $=$   torch . Tensor([[ 0 ,  0 ,  10 ,  10 ], [ 10 ,  10 ,  20 ,  20 ]]) >>>  gt_bboxes  $=$   torch . Tensor([[ 0 ,  0 ,  10 ,  10 ]]) >>>  assign result  $=$   self . assign(bboxes, gt_bboxes) >>>  expected gt in ds  $=$   torch . LongTensor([ 1 ,  0 ])  $>>$   assert  torch . all(assign result . gt_inds  $==$   expected gt in ds) \nassign one hot gt indices ( is b box in gt core ,  is b box in gt shadow ,  gt priority  $\\scriptstyle\\sum=.$  None ) Assign only one gt index to each prior box. \nGts with large gt priority are more likely to be assigned. \nParameters \n•  is b box in gt core  ( Tensor ) – Bool tensor indicating the bbox center is in the core area of a gt (e.g. 0-0.2). Shape: (num_prior, num_gt). •  is b box in gt shadow  ( Tensor ) – Bool tensor indicating the bbox center is in the shad- owed area of a gt (e.g. 0.2-0.5). Shape: (num_prior, num_gt). •  gt priority  ( Tensor ) – Priorities of gts. The gt with a higher priority is more likely to be assigned to the bbox when the bbox match with multiple gts. Shape: (num_gt, ). \nReturns \nReturns (assigned gt in ds, shadowed gt in ds). • assigned gt in ds: The assigned gt index of each prior bbox (i.e. index from 1 to num_gts). Shape: (num_prior, ). • shadowed gt in ds: shadowed gt indices. It is a tensor of shape (num_ignore, 2) with first column being the shadowed prior bbox indices and the second column the shadowed gt indices (1-based). \nReturn type  tuple \nget gt priorities ( gt_bboxes ) Get gt priorities according to their areas. \nSmaller gt has higher priority. \nParameters  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (k, 4). \nReturns  The priority of gts so that gts with larger priority is more likely to be assigned. Shape (k, ) \nReturn type  Tensor \nclass  mmdet.core.bbox. Combined Sampler ( pos sampler ,  ne g sampler ,  \\*\\*kwargs ) A sampler that combines positive sampler and negative sampler. \nclass  mmdet.core.bbox. Delta XY WH B Box Code r ( target means  $=$  (0.0, 0.0, 0.0, 0.0) ,  target stds  $\\mathbf{\\tilde{=}}$  (1.0, 1.0, 1.0, 1.0) ,  clip border  $\\bf{\\underline{{\\delta}}}$  True ,  add c tr clamp  $\\leftrightharpoons$  False ,  ctr_clamp  $\\scriptstyle{=32}$  ) \nDelta XYWH BBox coder. \nFollowing the practice in  R-CNN , this coder encodes bbox (x1, y1, x2, y2) into delta (dx, dy, dw, dh) and decodes delta (dx, dy, dw, dh) back to original bbox (x1, y1, x2, y2). \nParameters \n•  target means  ( Sequence[float] ) – De normalizing means of target for delta coordinates "}
{"page": 203, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_203.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* target_stds (Sequence [float ]) — Denormalizing standard deviation of target for delta\ncoordinates\n\n* clip_border (bool, optional) -— Whether clip the objects outside the border of the im-\nage. Defaults to True.\n\n* add_ctr_clamp (bool) - Whether to add center clamp, when added, the predicted box is\nclamped is its center is too far away from the original anchor’s center. Only used by YOLOF.\nDefault False.\n\n* ctr_clamp (int) — the maximum pixel shift to clamp. Only used by YOLOF. Default 32.\n\ndecode (bboxes, pred_bboxes, max_shape=None, wh_ratio_clip=0.016)\nApply transformation pred_bboxes to boxes.\n\nParameters\n¢ bboxes (torch. Tensor) — Basic boxes. Shape (B, N, 4) or (N, 4)\n\n* pred_bboxes (Tensor) — Encoded offsets with respect to each roi. Has shape (B, N,\nnum_classes * 4) or (B, N, 4) or (N, num_classes * 4) or (N, 4). Note N = num_anchors *\nW * H when rois is a grid of anchors.Offset encoding follows'.\n\n¢ (Sequence[int] or torch.Tensor or Sequence[ (max_shape) - Se-\nquence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W).\nIf bboxes shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and\nthe length of max_shape should also be B.\n\n¢ wh_ratio_clip (float, optional) — The allowed ratio between width and height.\nReturns Decoded boxes.\nReturn type torch.Tensor\n\nencode (bboxes, gt_bboxes)\nGet box regression transformation deltas that can be used to transform the bboxes into the gt_bboxes.\n\nParameters\n\n* bboxes (torch. Tensor) — Source boxes, e.g., object proposals.\n\n* gt_bboxes (torch. Tensor) — Target of the transformation, e.g., ground-truth boxes.\nReturns Box transformation deltas\nReturn type torch.Tensor\n\nclass mmdet.core.bbox.DistancePointBBoxCoder (clip_border=True)\nDistance Point BBox coder.\n\nThis coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left, right) and decode it back to the original.\n\nParameters clip_border (bool, optional) -— Whether clip the objects outside the border of the\nimage. Defaults to True.\n\ndecode (points, pred_bboxes, max_shape=None)\nDecode distance prediction to bounding box.\n\nParameters\n* points (Tensor) — Shape (B, N, 2) or (N, 2).\n\n* pred_bboxes (Tensor) — Distance from the given point to 4 boundaries (left, top, right,\nbottom). Shape (B, N, 4) or (N, 4)\n\n' https://gitlab.kitware.com/computer- vision/kwimage/-/blob/928cae35ca8/kwimage/structs/polygon.py#L379 # noga: E501\n\n196 Chapter 37. mmdet.core\n", "vlm_text": "•  target stds  ( Sequence[float] ) – De normalizing standard deviation of target for delta coordinates •  clip border  ( bool, optional ) – Whether clip the objects outside the border of the im- age. Defaults to True. •  add c tr clamp  ( bool ) – Whether to add center clamp, when added, the predicted box is clamped is its center is too far away from the original anchor’s center. Only used by YOLOF. Default False. •  ctr_clamp  ( int ) – the maximum pixel shift to clamp. Only used by YOLOF. Default 32. \ndecode ( bboxes ,  p red b boxes ,  max_shape  $=$  None ,  wh ratio clip=0.016 ) Apply transformation  p red b boxes  to  boxes . \nParameters \n•  bboxes  ( torch.Tensor ) – Basic boxes. Shape (B, N, 4) or (N, 4) \n•  p red b boxes  ( Tensor ) – Encoded offsets with respect to each roi. Has shape (B, N, num classes   $^{*}\\,4$  ) or (B, N, 4) or (N, num classes  $^{*}\\,4$  ) or (N, 4). Note  $\\mathbf{N}=$   num anchors \\*  $\\textrm{W}^{*}\\textrm{H}$   when rois is a grid of anchors.Offset encoding follows 1 . •  (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If bboxes shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and the length of max_shape should also be B. •  wh ratio clip  ( float, optional ) – The allowed ratio between width and height. \nReturns  Decoded boxes. \nReturn type  torch.Tensor \nencode ( bboxes ,  gt_bboxes ) Get box regression transformation deltas that can be used to transform the  bboxes  into the  gt_bboxes . \nParameters \n•  bboxes  ( torch.Tensor ) – Source boxes, e.g., object proposals. •  gt_bboxes  ( torch.Tensor ) – Target of the transformation, e.g., ground-truth boxes. Returns  Box transformation deltas Return type  torch.Tensor \nclass mmdet.core.bbox.Distance Point B Box Code r(clip border $\\bf{=}$ True)Distance Point BBox coder. \nThis coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left, right) and decode it back to the original. \nParameters  clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. \ndecode ( points ,  p red b boxes ,  max_shape=None ) Decode distance prediction to bounding box. \nParameters \n•  points  ( Tensor ) – Shape (B, N, 2) or (N, 2). •  p red b boxes  ( Tensor ) – Distance from the given point to 4 boundaries (left, top, right, bottom). Shape (B, N, 4) or (N, 4) "}
{"page": 204, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_204.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ (Sequence[int] or torch.Tensor or Sequence[ (max_shape) - Se-\nquence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W).\nIf priors shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]], and\nthe length of max_shape should also be B. Default None.\n\nReturns Boxes with shape (N, 4) or (B, N, 4)\nReturn type Tensor\n\nencode (points, gt_bboxes, max_dis=None, eps=0.1)\nEncode bounding box to distances.\n\nParameters\n\n* points (Tensor) — Shape (N, 2), The format is [x, y].\n\n* gt_bboxes (Tensor) — Shape (N, 4), The format is “xyxy”\n\n* max_dis (float) — Upper bound of the distance. Default None.\n\n* eps (float) —a small value to ensure target < max_dis, instead <=. Default 0.1.\nReturns Box transformation deltas. The shape is (N, 4).\nReturn type Tensor\n\nclass mmdet.core.bbox.InstanceBalancedPosSampler (num, pos_fraction, neg_pos_ub=- 1,\nadd_gt_as_proposals=True, **kwargs)\nInstance balanced sampler that samples equal number of positive samples for each instance.\n\nclass mmdet.core.bbox.IoUBalancedNegSampler (num, pos_fraction, floor_thr=- 1, floor_fraction=0,\nnum_bins=3, **kwargs)\nIoU Balanced Sampling.\n\narXiv: https://arxiv.org/pdf/1904.02701.pdf (CVPR 2019)\n\nSampling proposals according to their IoU. floor_fraction of needed Rols are sampled from proposals whose loU\nare lower than floor_thr randomly. The others are sampled from proposals whose IoU are higher than floor_thr.\nThese proposals are sampled from some bins evenly, which are split by num_bins via IoU evenly.\n\nParameters\n* num (int) — number of proposals.\n* pos_fraction (float) — fraction of positive proposals.\n\n¢ floor_thr (float) — threshold (minimum) IoU for IoU balanced sampling, set to -1 if all\nusing IoU balanced sampling.\n\n¢ floor_fraction (float) — sampling fraction of proposals under floor_thr.\n* num_bins (int) — number of bins in IoU balanced sampling.\n\nsample_via_interval (max_overlaps, full_set, num_expected)\nSample according to the iou interval.\n\nParameters\n* max_overlaps (torch. Tensor) — loU between bounding boxes and ground truth boxes.\n¢ full_set (set (int)) — A full set of indices of boxes\n* num_expected (int) — Number of expected samples\n\nReturns Indices of samples\n\nReturn type np.ndarray\n\n37.2. bbox 197\n", "vlm_text": " (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If priors shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]], and the length of max_shape should also be B. Default None. \nReturns  Boxes with shape (N, 4) or (B, N, 4) Return type  Tensor \nencode ( points ,  gt_bboxes ,  max_dis  $\\leftrightharpoons$  None ,  eps=0.1 ) Encode bounding box to distances. \nParameters \n•  points  ( Tensor ) – Shape (N, 2), The format is [x, y]. •  gt_bboxes  ( Tensor ) – Shape (N, 4), The format is “xyxy” •  max_dis  ( float ) – Upper bound of the distance. Default None. •  eps  ( float ) – a small value to ensure target  $<$   max_dis, instead  $<=$  . Default 0.1. \nReturns  Box transformation deltas. The shape is (N, 4). \nReturn type  Tensor \nclass  mmdet.core.bbox. Instance Balanced Pos Sampler ( num ,  pos fraction ,  neg_pos_ub=- 1 , add gt as proposals  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ,  \\*\\*kwargs ) Instance balanced sampler that samples equal number of positive samples for each instance. \nclass  mmdet.core.bbox. I oU Balanced Ne g Sampler ( num ,  pos fraction ,  floor_thr  $\\mathbf{=}\\mathbf{\\cdot}$  - 1 ,  floor fraction  $\\mathbf{\\chi}_{:=0}$  , num_bins  ${}^{\\ast}\\mathrm{=}3$  ,  \\*\\*kwargs ) IoU Balanced Sampling. \narXiv:  https://arxiv.org/pdf/1904.02701.pdf  (CVPR 2019) \nSampling proposals according to their IoU.  floor fraction  of needed RoIs are sampled from proposals whose IoU are lower than  floor_thr  randomly. The others are sampled from proposals whose IoU are higher than  floor_thr . These proposals are sampled from some bins evenly, which are split by  num_bins  via IoU evenly. \nParameters \n•  num  ( int ) – number of proposals. •  pos fraction  ( float ) – fraction of positive proposals. •  floor_thr  ( float ) – threshold (minimum) IoU for IoU balanced sampling, set to -1 if all using IoU balanced sampling. •  floor fraction  ( float ) – sampling fraction of proposals under floor_thr. •  num_bins  ( int ) – number of bins in IoU balanced sampling. \nsample via interval ( max overlaps ,  full_set ,  num expected ) Sample according to the iou interval. \nParameters \n•  max overlaps  ( torch.Tensor ) – IoU between bounding boxes and ground truth boxes. •  full_set  ( set(int) ) – A full set of indices of boxes •  num expected  ( int ) – Number of expected samples Returns  Indices of samples Return type  np.ndarray "}
{"page": 205, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_205.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.core.bbox.MaxIoUAssigner (pos_iou_thr, neg_iou_thr, min_pos_iou=0.0,\ngt_max_assign_all=True, ignore_iof_thr=- 1,\nignore_wrt_candidates=True, match_low_quality=True,\ngpu_assign_thr=- 1, iou_calculator={'type': 'BboxOverlaps2D'})\nAssign a corresponding gt bbox or background to each bbox.\n\nEach proposals will be assigned with -/, or a semi-positive integer indicating the ground truth index.\n* -1: negative sample, no assigned gt\n\n* semi-positive integer: positive sample, index (0-based) of assigned gt\n\nParameters\n* pos_iou_thr (float) — IoU threshold for positive bboxes.\n* neg_iou_thr (float or tuple) —IoU threshold for negative bboxes.\n\n* min_pos_iou (float) — Minimum iou for a bbox to be considered as a positive bbox. Pos-\nitive samples can have smaller IoU than pos_iou_thr due to the 4th step (assign max IoU\nsample to each gt).\n\n* gt_max_assign_all (bool) — Whether to assign all bboxes with the same highest overlap\nwith some gt to that gt.\n\n¢ ignore_iof_thr (float) —IoF threshold for ignoring bboxes (if gt_bboxes_ignore is spec-\nified). Negative values mean not ignoring any bboxes.\n\n* ignore_wrt_candidates (bool) — Whether to compute the iof between bboxes and\ngt_bboxes_ignore, or the contrary.\n\n* match_low_quality (bool) — Whether to allow low quality matches. This is usually al-\nlowed for RPN and single stage detectors, but not allowed in the second stage. Details are\ndemonstrated in Step 4.\n\n* gpu_assign_thr (int) — The upper bound of the number of GT for GPU assign. When the\nnumber of gt is above this threshold, will assign on CPU device. Negative values mean not\nassign on CPU.\n\nassign (bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None)\nAssign gt to bboxes.\n\nThis method assign a gt bbox to every bbox (proposal/anchor), each bbox will be assigned with -1, or a\nsemi-positive number. -1 means negative sample, semi-positive number is the index (0-based) of assigned\ngt. The assignment is done in following steps, the order matters.\n\n1. assign every bbox to the background\n2. assign proposals whose iou with all gts < neg_iou_thr to 0\n3. for each bbox, if the iou with its nearest gt >= pos_iou_thr, assign it to that bbox\n\n4. for each gt bbox, assign its nearest proposals (may be more than one) to itself\n\nParameters\n* bboxes (Tensor) — Bounding boxes to be assigned, shape(n, 4).\n* gt_bboxes (Tensor) — Groundtruth boxes, shape (k, 4).\n\n¢ gt_bboxes_ignore (Tensor, optional) — Ground truth bboxes that are labelled as\nignored, e.g., crowd boxes in COCO.\n\n¢ gt_labels (Tensor, optional) — Label of gt_bboxes, shape (k, ).\n\n198 Chapter 37. mmdet.core\n", "vlm_text": "class  mmdet.core.bbox. MaxI oU As signer ( pos i ou thr ,  ne g i ou thr ,  min pos i ou  $\\mathrm{\\Sigma=}0.0$  , gt max assign all  $\\leftrightharpoons$  True ,  ignore i of  $\\mathit{t h r}{=}{\\mathit{-1}}$  , ignore w rt candidates  $=$  True ,  match low quality  $\\mathbf{=}$  True , gpu assign thr  $=$  - 1 ,  i ou calculator  $\\bf{=}$  {'type': 'B box Overlaps 2 D'} ) \nAssign a corresponding gt bbox or background to each bbox. Each proposals will be assigned with  -1 , or a semi-positive integer indicating the ground truth index. \n\n• -1: negative sample, no assigned gt • semi-positive integer: positive sample, index (0-based) of assigned gt \nParameters \n•  pos i ou thr  ( float ) – IoU threshold for positive bboxes. •  ne g i ou thr  ( float or tuple ) – IoU threshold for negative bboxes. •  min pos i ou  ( float ) – Minimum iou for a bbox to be considered as a positive bbox. Pos- itive samples can have smaller IoU than pos i ou thr due to the 4th step (assign max IoU sample to each gt). •  gt max assign all  ( bool ) – Whether to assign all bboxes with the same highest overlap with some gt to that gt. •  ignore i of thr  ( float ) – IoF threshold for ignoring bboxes (if  gt b boxes ignore  is spec- ified). Negative values mean not ignoring any bboxes. •  ignore w rt candidates  ( bool ) – Whether to compute the iof between  bboxes  and gt b boxes ignore , or the contrary. •  match low quality  ( bool ) – Whether to allow low quality matches. This is usually al- lowed for RPN and single stage detectors, but not allowed in the second stage. Details are demonstrated in Step 4. •  gpu assign thr  ( int ) – The upper bound of the number of GT for GPU assign. When the number of gt is above this threshold, will assign on CPU device. Negative values mean not assign on CPU. \nassign ( bboxes ,  gt_bboxes ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_labels  $=$  None ) Assign gt to bboxes. \nThis method assign a gt bbox to every bbox (proposal/anchor), each bbox will be assigned with  $^{-1}$  , or a semi-positive number. -1 means negative sample, semi-positive number is the index (0-based) of assigned gt. The assignment is done in following steps, the order matters. \n1. assign every bbox to the background 2. assign proposals whose iou with all gts  $<$   ne g i ou thr to 0 3. for each bbox, if the iou with its nearest gt  $>=$   pos i ou thr, assign it to that bbox 4. for each gt bbox, assign its nearest proposals (may be more than one) to itself \nParameters \n•  bboxes  ( Tensor ) – Bounding boxes to be assigned, shape(n, 4). •  gt_bboxes  ( Tensor ) – Ground truth boxes, shape (k, 4). •  gt b boxes ignore  ( Tensor, optional ) – Ground truth bboxes that are labelled as ignored , e.g., crowd boxes in COCO. •  gt_labels  ( Tensor, optional ) – Label of gt_bboxes, shape (k, ). "}
{"page": 206, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_206.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturns The assign result.\n\nReturn type AssignResult\n\nExample\n\n>>> self = MaxIoUAssigner(0.5, 0.5)\n\n>>> bboxes = torch.Tensor([[0, 9, 10, 10], [10, 10, 20, 20]])\n>>> gt_bboxes = torch.Tensor([[0, 9, 10, 9]])\n\n>>> assign_result = self.assign(bboxes, gt_bboxes)\n\n>>> expected_gt_inds = torch.LongTensor([1, 0])\n\n>>> assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\nassign_wrt_overlaps (overlaps, gt_labels=None)\nAssign w.r.t. the overlaps of bboxes with gts.\n\nParameters\n* overlaps (Tensor) — Overlaps between k gt_bboxes and n bboxes, shape(k, n).\n¢ gt_labels (Tensor, optional) — Labels of k gt_bboxes, shape (k, ).\nReturns The assign result.\nReturn type AssignResult\n\nclass mmdet.core.bbox.OHEMSampler (num, pos_fraction, context, neg_pos_ub=- 1,\nadd_gt_as_proposals=True, **kwargs)\nOnline Hard Example Mining Sampler described in Training Region-based Object Detectors with Online Hard\nExample Mining.\n\nclass mmdet.core.bbox.PseudoBBoxCoder (**kwargs)\nPseudo bounding box coder.\n\ndecode (bboxes, pred_bboxes)\ntorch.Tensor: return the given pred_bboxes\n\nencode (bboxes, gt_bboxes)\ntorch.Tensor: return the given bboxes\n\nclass mmdet.core.bbox.PseudoSampler (**kwargs)\nA pseudo sampler that does not do sampling actually.\n\nsample (assign_result, bboxes, gt_bboxes, **kwargs)\nDirectly returns the positive and negative indices of samples.\n\nParameters\n* assign_result (AssignResult) — Assigned results\n* bboxes (torch. Tensor) — Bounding boxes\n¢ gt_bboxes (torch. Tensor) — Ground truth boxes\nReturns sampler results\nReturn type SamplingResult\n\nclass mmdet.core.bbox.RandomSampler (num, pos_fraction, neg_pos_ub=- 1, add_gt_as_proposals=True,\n**kwargs)\nRandom sampler.\n\nParameters\n\n37.2. bbox 199\n\n", "vlm_text": "Returns  The assign result. Return type  Assign Result \nExample \n>>>  self  $=$   MaxI oU As signer( 0.5 ,  0.5 ) >>>  bboxes  $=$   torch . Tensor([[ 0 ,  0 ,  10 ,  10 ], [ 10 ,  10 ,  20 ,  20 ]]) >>>  gt_bboxes  $=$  torch . Tensor([[ 0 ,  0 ,  10 ,  9 ]]) >>>  assign result  $=$   self . assign(bboxes, gt_bboxes) >>>  expected gt in ds    $=$   torch . LongTensor([ 1 ,  0 ]) >>>  assert  torch . all(assign result . gt_inds  $==$  expected gt in ds) \nassign w rt overlaps ( overlaps ,  gt_labels  $\\mathbf{\\check{\\Sigma}}$  None ) Assign w.r.t. the overlaps of bboxes with gts. \nParameters \n•  overlaps  ( Tensor ) – Overlaps between k gt_bboxes and n bboxes, shape  $(\\mathbf{k},\\mathbf{n})$  . •  gt_labels  ( Tensor, optional ) – Labels of k gt_bboxes, shape (k, ). Returns  The assign result. Return type  Assign Result \nclass  mmdet.core.bbox. OH EM Sampler ( num ,  pos fraction ,  context ,  neg_pos_ub=- 1 , add gt as proposals  $\\mathbf{:=}$  True ,  \\*\\*kwargs ) Online Hard Example Mining Sampler described in  Training Region-based Object Detectors with Online Hard Example Mining . \nclass  mmdet.core.bbox. Pseudo B Box Code r ( \\*\\*kwargs ) Pseudo bounding box coder. \ndecode ( bboxes ,  p red b boxes ) torch.Tensor: return the given  p red b boxes encode ( bboxes ,  gt_bboxes ) torch.Tensor: return the given  bboxes \nclass  mmdet.core.bbox. Pseudo Sampler ( \\*\\*kwargs ) A pseudo sampler that does not do sampling actually. \nsample ( assign result ,  bboxes ,  gt_bboxes ,  \\*\\*kwargs ) Directly returns the positive and negative indices of samples. \nParameters •  assign result  ( Assign Result ) – Assigned results •  bboxes  ( torch.Tensor ) – Bounding boxes •  gt_bboxes  ( torch.Tensor ) – Ground truth boxes Returns  sampler results Return type  Sampling Result \nclass  mmdet.core.bbox. Random Sampler ( num ,  pos fraction ,  neg_pos_  $u b\\!\\!=\\!\\!-\\!\\!\\;I$  ,  add gt as proposals  $\\mathbf{\\tilde{=}}$  True , \\*\\*kwargs ) \nRandom sampler. \nParameters "}
{"page": 207, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_207.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* num (int) — Number of samples\n* pos_fraction (float) — Fraction of positive samples\n\n* neg_pos_up (int, optional) — Upper bound number of negative and positive samples.\nDefaults to -1.\n\n* add_gt_as_proposals (bool, optional) — Whether to add ground truth boxes as pro-\nposals. Defaults to True.\n\nrandom_choice (gallery, num)\nRandom select some elements from the gallery.\n\nIf gallery is a Tensor, the returned indices will be a Tensor; If gallery is a ndarray or list, the returned indices\nwill be a ndarray.\n\nParameters\n¢ gallery (Tensor | ndarray | list) — indices pool.\n* num (int) — expected sample num.\n\nReturns sampled indices.\n\nReturn type Tensor or ndarray\n\nclass mmdet.core.bbox.RegionAssigner (center_ratio=0.2, ignore_ratio=0.5)\nAssign a corresponding gt bbox or background to each bbox.\n\nEach proposals will be assigned with -/, 0, or a positive integer indicating the ground truth index.\n* -1: don’t care\n* 0: negative sample, no assigned gt\n\n* positive integer: positive sample, index (1-based) of assigned gt\n\nParameters\n* center_ratio — ratio of the region in the center of the bbox to define positive sample.\n* ignore_ratio — ratio of the region to define ignore samples.\nassign(mlvl_anchors, mlvl_valid_flags, gt_bboxes, img_meta, featmap_sizes, anchor_scale, anchor_strides,\n\ngt_bboxes_ignore=None, gt_labels=None, allowed_border=0)\nAssign gt to anchors.\n\nThis method assign a gt bbox to every bbox (proposal/anchor), each bbox will be assigned with -1, 0, or a\npositive number. -1 means don’t care, 0 means negative sample, positive number is the index (1-based) of\nassigned gt.\n\nThe assignment is done in following steps, and the order matters.\n1. Assign every anchor to 0 (negative)\n\n2. (For each gt_bboxes) Compute ignore flags based on ignore_region then assign -1 to anchors w.r.t.\nignore flags\n\n3. (For each gt_bboxes) Compute pos flags based on center_region then assign gt_bboxes to anchors w.r.t.\npos flags\n\n4. (For each gt_bboxes) Compute ignore flags based on adjacent anchor level then assign -1 to anchors\nw.r.t. ignore flags\n\n5. Assign anchor outside of image to -1\n\n200 Chapter 37. mmdet.core\n", "vlm_text": "•  num  ( int ) – Number of samples •  pos fraction  ( float ) – Fraction of positive samples •  neg_pos_up  ( int, optional ) – Upper bound number of negative and positive samples. Defaults to -1. •  add gt as proposals  ( bool, optional ) – Whether to add ground truth boxes as pro- posals. Defaults to True. \nrandom choice ( gallery ,  num ) Random select some elements from the gallery. If  gallery  is a Tensor, the returned indices will be a Tensor; If  gallery  is a ndarray or list, the returned indices will be a ndarray. \nParameters \n•  gallery  ( Tensor | ndarray | list ) – indices pool. •  num  ( int ) – expected sample num. Returns  sampled indices. Return type  Tensor or ndarray \nclass  mmdet.core.bbox. Region As signer ( center ratio=0.2 ,  ignore ratio  $\\bullet{=}0.5$  ) Assign a corresponding gt bbox or background to each bbox. \nEach proposals will be assigned with  $\\it-1,0.$  , or a positive integer indicating the ground truth index. \n• -1: don’t care • 0: negative sample, no assigned gt • positive integer: positive sample, index (1-based) of assigned gt \nParameters \n•  center ratio  – ratio of the region in the center of the bbox to define positive sample. •  ignore ratio  – ratio of the region to define ignore samples. \nassign ( ml vl anchors ,  ml vl valid flags ,  gt_bboxes ,  img_meta ,  feat map sizes ,  anchor scale ,  anchor strides , gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_labels  $\\mathbf{\\check{\\Sigma}}$  None ,  allowed border  $\\mathord{:=}\\!\\!O_{.}$  ) Assign gt to anchors. \nThis method assign a gt bbox to every bbox (proposal/anchor), each bbox will be assigned with -1, 0, or a positive number. -1 means don’t care, 0 means negative sample, positive number is the index (1-based) of assigned gt. \nThe assignment is done in following steps, and the order matters. \n1. Assign every anchor to 0 (negative) 2. (For each gt_bboxes) Compute ignore flags based on ignore region then assign  $^{-1}$   to anchors w.r.t. ignore flags 3. (For each gt_bboxes) Compute pos flags based on center region then assign gt_bboxes to anchors w.r.t. pos flags 4. (For each gt_bboxes) Compute ignore flags based on adjacent anchor level then assign   $^{-1}$   to anchors w.r.t. ignore flags 5. Assign anchor outside of image to -1 "}
{"page": 208, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_208.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n¢ mlvl_anchors (list [Tensor]) — Multi level anchors.\n¢ mlvl_valid_flags (list [Tensor]) — Multi level valid flags.\n* gt_bboxes (Tensor) — Ground truth bboxes of image\n¢ img_meta (dict) — Meta info of image.\n¢ featmap_sizes (list [Tensor]) — Feature mapsize each level\n¢ anchor_scale (int) — Scale of the anchor.\n¢ anchor_strides (list [int]) — Stride of the anchor.\n* gt_bboxes — Groundtruth boxes, shape (k, 4).\n\n¢ gt_bboxes_ignore (Tensor, optional) — Ground truth bboxes that are labelled as\nignored, e.g., crowd boxes in COCO.\n\n¢ gt_labels (Tensor, optional) — Label of gt_bboxes, shape (k, ).\n\n¢ allowed_border (int, optional) — The border to allow the valid anchor. Defaults to\n0.\n\nReturns The assign result.\n\nReturn type AssignResult\n\nclass mmdet.core.bbox.SamplingResult (pos_inds, neg_inds, bboxes, gt_bboxes, assign_result, gt_flags)\nBbox sampling result.\n\nExample\n\n>>> # xdoctest: +IGNORE_WANT\n>>> from mmdet.core.bbox.samplers.sampling_result import * # NOQA\n>>> self = SamplingResult.random(rng=10)\n>>> print(f'self = fself}')\nself = <SamplingResult({\n\"neg_bboxes': torch.Size([12, 4]),\n\"‘neg_inds': tensor([ 9, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n‘num_gts': 4,\n\"pos_assigned_gt_inds': tensor([], dtype=torch.int64),\n\"pos_bboxes': torch.Size([0, 4]),\n\"pos_inds': tensor([], dtype=torch.int64),\n\"pos_is_gt': tensor([], dtype=torch.uint8)\n\n})>\n\nproperty bboxes\nconcatenated positive and negative boxes\n\nType torch.Tensor\n\nproperty info\nReturns a dictionary of info about the object.\n\nclassmethod random(rng=None, **kwargs)\n\nParameters\n\n37.2. bbox 201\n", "vlm_text": "Parameters \n•  ml vl anchors  ( list[Tensor] ) – Multi level anchors. •  ml vl valid flags  ( list[Tensor] ) – Multi level valid flags. •  gt_bboxes  ( Tensor ) – Ground truth bboxes of image •  img_meta  ( dict ) – Meta info of image. •  feat map sizes  ( list[Tensor] ) – Feature mapsize each level •  anchor scale  ( int ) – Scale of the anchor. •  anchor strides  ( list[int] ) – Stride of the anchor. •  gt_bboxes  – Ground truth boxes, shape (k, 4). •  gt b boxes ignore  ( Tensor, optional ) – Ground truth bboxes that are labelled as ignored , e.g., crowd boxes in COCO. •  gt_labels  ( Tensor, optional ) – Label of gt_bboxes, shape (k, ). •  allowed border  ( int, optional ) – The border to allow the valid anchor. Defaults to 0. Returns  The assign result. Return type  Assign Result \nclass  mmdet.core.bbox. Sampling Result ( pos_inds ,  neg_inds ,  bboxes ,  gt_bboxes ,  assign result ,  gt_flags ) Bbox sampling result. \nExample \n>>>  # xdoctest:   $^+$  IGNORE WANT  $>>$   from  mmdet.core.bbox.samplers.sampling result  import  \\* # NOQA  $>>$   self  $=$   Sampling Result . random(  $\\scriptstyle{\\mathbf{r}}{\\mathfrak{n}}{\\mathfrak{g}}=10.$  )  $>>$   print ( f ' self  $=$   { self } ' ) self  $=$   <Sampling Result({ ' neg_bboxes ' : torch.Size([12, 4]), ' neg_inds ' : tensor([ 0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12]), ' num_gts ' : 4, ' pos assigned gt in ds ' : tensor([], dtype  $=$  torch.int64), ' pos_bboxes ' : torch.Size([0, 4]), ' pos_inds ' : tensor([], dtype  $=$  torch.int64), ' pos_is_gt ' : tensor([], dtype  $=$  torch.uint8) })> \nproperty bboxes \nconcatenated positive and negative boxes Type  torch.Tensor \nproperty info Returns a dictionary of info about the object. \nclass method random (  $.r n g{=}N o n e$  ,  \\*\\*kwargs ) Parameters "}
{"page": 209, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_209.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ rng(None | int | numpy.random.RandomState) — seed or state.\n¢ kwargs (keyword arguments) —\n— num_preds: number of predicted boxes\n— num_gts: number of true boxes\n— p_ignore (float): probability of a predicted box assigned to an ignored truth.\n— p_assigned (float): probability of a predicted box not being assigned.\n\n— p_use_label (float | bool): with labels or not.\n\nReturns Randomly generated sampling result.\n\nReturn type SamplingResult\n\nExample\n\n>>> from mmdet.core.bbox.samplers.sampling_result import * # NOQA\n>>> self = SamplingResult.random()\n>>> print(self.__dict__)\n\nto (device)\n\nChange the device of the data inplace.\n\nExample\n\n>>> self = SamplingResult.random()\n>>> print(f'self = {self.to(None) }')\n>>> # xdoctest: +REQUIRES(--gpu)\n>>> print(f'self = fself.to(0)}')\n\nclass mmdet.core.bbox.ScoreHLRSampler (num, pos_fraction, context, neg_pos_ub=- 1,\n\nadd_gt_as_proposals=True, k=0.5, bias=0, score_thr=0.05,\niou_thr=0.5, **kwargs)\n\nImportance-based Sample Reweighting (ISR_N), described in Prime Sample Attention in Object Detection.\n\nScore hierarchical local rank (HLR) differentiates with RandomSampler in negative part. It firstly computes\nScore-HLR in a two-step way, then linearly maps score hlr to the loss weights.\n\nParameters\n\nnum (int) — Total number of sampled Rols.\npos_fraction (float) — Fraction of positive samples.\ncontext (BaseRolIHead) — Rol head that the sampler belongs to.\n\nneg_pos_ub (int) — Upper bound of the ratio of num negative to num positive, -1 means\nno upper bound.\n\nadd_gt_as_proposals (bool) — Whether to add ground truth as proposals.\nk (float) — Power of the non-linear mapping.\nbias (float) — Shift of the non-linear mapping.\n\nscore_thr (float) — Minimum score that a negative sample is to be considered as valid\nbbox.\n\n202\n\nChapter 37. mmdet.core\n\n", "vlm_text": "•  rng  ( None | int | numpy.random.Random State ) – seed or state. \n•  kwargs  ( keyword arguments ) – –  num_preds: number of predicted boxes –  num_gts: number of true boxes –  p_ignore (float): probability of a predicted box assigned to an ignored truth. –  p_assigned (float): probability of a predicted box not being assigned. –  p use label (float | bool): with labels or not. \nReturns  Randomly generated sampling result. Return type  Sampling Result \nExample \nThe image is not actually a table, but rather a snippet of Python code, likely produced using a library that offers syntax coloring for better readability. Here's a description of the code:\n\n1. The code imports all symbols from the `sampling_result` module in the `mmdet.core.bbox.samplers` package. This is indicated by the `from mmdet.core.bbox.samplers.sampling_result import *` line. It also includes a comment `# NOQA`, which often signifies to linters to ignore warnings or errors on that line.\n\n2. A `SamplingResult` object is created using a method called `random()` from the `SamplingResult` class. It is assigned to the variable `self`.\n\n3. The code then prints the dictionary representation (`__dict__`) of the `self` object, which outputs the internal dictionary used to store an object's (in this case, `SamplingResult`) attributes. \n\nThis example seems to be illustrating a way to use certain features from the MMDetection framework, which is a popular object detection library in Python.\nto ( device ) Change the device of the data inplace. \nExample \nThe image contains a snippet of Python code that seems to be a part of a documentation example or a doctest. Here’s a breakdown of what is happening in each line:\n\n1. `self = SamplingResult.random()`: It initializes a variable `self` with a random sampling result generated by calling a `random()` method on `SamplingResult`.\n\n2. `print(f'self = {self.to(None)}')`: This line prints the result of calling the `to()` method on `self` with `None` as an argument, formatting it into the string 'self = ...'.\n\n3. `# xdoctest: +REQUIRES(--gpu)`: This is a comment line intended for `xdoctest`, a documentation testing tool. It specifies that the following test requires a `--gpu` flag, likely because it needs GPU resources to run.\n\n4. `print(f'self = {self.to(0)}')`: Similar to the second line, this line prints the result of calling the `to()` method on `self`, but this time using `0` as an argument, again formatted into the string 'self = ...'.\n\nOverall, this code appears to be testing or demonstrating the `to()` method of a `SamplingResult` object, showing its output under different conditions.\nclass  mmdet.core.bbox. Score HL R Sampler ( num ,  pos fraction ,  context ,  neg_pos_ub=- 1 , add gt as proposal  $\\backsimeq$  True ,  $k{=}0.5$  ,  bias  $\\mathrel{\\mathop:}=\\!\\!O$  ,  score_  $t h r{=}0.05$  , iou_th  $\\scriptstyle{r=0.5}$  ,  \\*\\*kwargs ) \nImportance-based Sample Re weighting (ISR_N), described in  Prime Sample Attention in Object Detection \nScore hierarchical local rank (HLR) differentiates with Random Sampler in negative part. It firstly computes Score-HLR in a two-step way, then linearly maps score hlr to the loss weights. \nParameters \n•  num  ( int ) – Total number of sampled RoIs. •  pos fraction  ( float ) – Fraction of positive samples. •  context  ( Base RoI Head ) – RoI head that the sampler belongs to. •  neg_pos_ub  ( int ) – Upper bound of the ratio of num negative to num positive, -1 means no upper bound. •  add gt as proposals  ( bool ) – Whether to add ground truth as proposals. •  k  ( float ) – Power of the non-linear mapping. •  bias  ( float ) – Shift of the non-linear mapping. •  score_thr  ( float ) – Minimum score that a negative sample is to be considered as valid bbox. "}
{"page": 210, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_210.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nstatic random_choice (gallery, num)\nRandomly select some elements from the gallery.\n\nIf gallery is a Tensor, the returned indices will be a Tensor; If gallery is a ndarray or list, the returned indices\nwill be a ndarray.\n\nParameters\n¢ gallery (Tensor | ndarray | list) — indices pool.\n* num (int) — expected sample num.\n\nReturns sampled indices.\n\nReturn type Tensor or ndarray\n\nsample (assign_result, bboxes, gt_bboxes, gt_labels=None, img_meta=None, **kwargs)\nSample positive and negative bboxes.\n\nThis is a simple implementation of bbox sampling given candidates, assigning results and ground truth\nbboxes.\n\nParameters\n\n* assign_result (AssignResult) — Bbox assigning results.\n\n* bboxes (Tensor) — Boxes to be sampled from.\n\n¢ gt_bboxes (Tensor) — Ground truth bboxes.\n\n¢ gt_labels (Tensor, optional) — Class labels of ground truth bboxes.\nReturns\n\nSampling result and negative label weights.\nReturn type tuple[SamplingResult, Tensor]\n\nclass mmdet.core.bbox.TBLRBBoxCoder (normalizer=4.0, clip_border=True)\nTBLR BBox coder.\n\nFollowing the practice in FSAF, this coder encodes gt bboxes (x1, yl, x2, y2) into (top, bottom, left, right) and\ndecode it back to the original.\n\nParameters\n\n* normalizer (list | float) — Normalization factor to be divided with when coding the\ncoordinates. If it is a list, it should have length of 4 indicating normalization factor in tblr\ndims. Otherwise it is a unified float factor for all dims. Default: 4.0\n\n* clip_border (bool, optional) -— Whether clip the objects outside the border of the im-\nage. Defaults to True.\n\ndecode (bboxes, pred_bboxes, max_shape=None)\nApply transformation pred_bboxes to boxes.\n\nParameters\n* bboxes (torch. Tensor) — Basic boxes.Shape (B, N, 4) or (N, 4)\n* pred_bboxes (torch. Tensor) — Encoded boxes with shape (B, N, 4) or (N, 4)\n\n¢ (Sequence[int] or torch.Tensor or Sequence[ (max_shape) - Se-\nquence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W).\nIf bboxes shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and\nthe length of max_shape should also be B.\n\n37.2. bbox 203\n", "vlm_text": "static random choice ( gallery ,  num ) Randomly select some elements from the gallery. \nIf  gallery  is a Tensor, the returned indices will be a Tensor; If  gallery  is a ndarray or list, the returned indices will be a ndarray. \nParameters \n•  gallery  ( Tensor | ndarray | list ) – indices pool. •  num  ( int ) – expected sample num. Returns  sampled indices. Return type  Tensor or ndarray \nsample ( assign result ,  bboxes ,  gt_bboxes ,  gt_labels  $\\leftrightharpoons$  None ,  img_meta  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Sample positive and negative bboxes. \nThis is a simple implementation of bbox sampling given candidates, assigning results and ground truth bboxes. \nParameters \n•  assign result  ( Assign Result ) – Bbox assigning results. •  bboxes  ( Tensor ) – Boxes to be sampled from. •  gt_bboxes  ( Tensor ) – Ground truth bboxes. •  gt_labels  ( Tensor, optional ) – Class labels of ground truth bboxes. \nReturns \nSampling result and negative  label weights. Return type  tuple[ Sampling Result , Tensor] \nclass mmdet.core.bbox.T BL RB Box Code r(normalize $\\tt:=\\!4.0$ , clip border $\\mathbf{=}$ True)TBLR BBox coder. \nFollowing the practice in  FSAF , this coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left, right) and decode it back to the original. \nParameters \n•  normalizer  ( list | float ) – Normalization factor to be divided with when coding the coordinates. If it is a list, it should have length of 4 indicating normalization factor in tblr dims. Otherwise it is a unified float factor for all dims. Default: 4.0 •  clip border  ( bool, optional ) – Whether clip the objects outside the border of the im- age. Defaults to True. \ndecode ( bboxes ,  p red b boxes ,  max_shape  $=$  None ) Apply transformation  p red b boxes  to  boxes . \nParameters \n•  bboxes  ( torch.Tensor ) – Basic boxes.Shape (B, N, 4) or (N, 4) •  p red b boxes  ( torch.Tensor ) – Encoded boxes with shape (B, N, 4) or (N, 4) •  (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If bboxes shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and the length of max_shape should also be B. "}
{"page": 211, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_211.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturns Decoded boxes.\nReturn type torch.Tensor\n\nencode (bboxes, gt_bboxes)\n\nGet box regression transformation deltas that can be used to transform the bboxes into the gt_bboxes in\nthe (top, left, bottom, right) order.\n\nParameters\n\n* bboxes (torch. Tensor) — source boxes, e.g., object proposals.\n\n* gt_bboxes (torch. Tensor) — target of the transformation, e.g., ground truth boxes.\nReturns Box transformation deltas\nReturn type torch.Tensor\n\nmmdet.core.bbox.bbox2distance (points, bbox, max_dis=None, eps=0.1)\nDecode bounding box based on distances.\n\nParameters\n\n* points (Tensor) — Shape (n, 2), [x, y].\n\n* bbox (Tensor) — Shape (n, 4), “xyxy” format\n\n* max_dis (float) — Upper bound of the distance.\n\n* eps (float) —a small value to ensure target < max_dis, instead <=\nReturns Decoded distances.\nReturn type Tensor\n\nmmdet.core.bbox.bbox2result (bboxes, labels, num_classes)\nConvert detection results to a list of numpy arrays.\n\nParameters\n\n* bboxes (torch.Tensor | np.ndarray) — shape (n, 5)\n\n¢ labels (torch.Tensor | np.ndarray) — shape (n, )\n\n* num_classes (int) — class number, including background class\nReturns bbox results of each class\nReturn type list(ndarray)\n\nmmdet.core.bbox.bbox2roi (bbox_list)\nConvert a list of bboxes to roi format.\n\nParameters bbox_list (list [Tensor ]) —a list of bboxes corresponding to a batch of images.\nReturns shape (n, 5), [batch_ind, x1, yl, x2, y2]\nReturn type Tensor\n\nmmdet.core.bbox.bbox_cxcywh_to_xyxy (bbox)\nConvert bbox coordinates from (cx, cy, w, h) to (x1, yl, x2, y2).\n\nParameters bbox (Tensor) — Shape (n, 4) for bboxes.\nReturns Converted bboxes.\n\nReturn type Tensor\n\n204 Chapter 37. mmdet.core\n", "vlm_text": "Returns  Decoded boxes. Return type  torch.Tensor \nencode ( bboxes ,  gt_bboxes ) Get box regression transformation deltas that can be used to transform the  bboxes  into the  gt_bboxes  in the (top, left, bottom, right) order. \nParameters •  bboxes  ( torch.Tensor ) – source boxes, e.g., object proposals. •  gt_bboxes  ( torch.Tensor ) – target of the transformation, e.g., ground truth boxes. \nReturns  Box transformation deltas \nReturn type  torch.Tensor \nmmdet.core.bbox. b box 2 distance ( points ,  bbox ,  max_dis  $\\mathbf{\\check{\\Sigma}}$  None ,  eps=0.1 ) Decode bounding box based on distances. \n•  points  ( Tensor ) – Shape (n, 2), [x, y]. •  bbox  ( Tensor ) – Shape (n, 4), “xyxy” format •  max_dis  ( float ) – Upper bound of the distance. •  eps  ( float ) – a small value to ensure target   $<$   max_dis, instead  $<=$  \nmmdet.core.bbox. b box 2 result ( bboxes ,  labels ,  num classes ) Convert detection results to a list of numpy arrays. \nParameters •  bboxes  ( torch.Tensor | np.ndarray ) – shape (n, 5) •  labels  ( torch.Tensor | np.ndarray ) – shape (n, ) •  num classes  ( int ) – class number, including background class \nReturns  bbox results of each class Return type  list(ndarray) \nmmdet.core.bbox. bbox2roi ( bbox_list ) Convert a list of bboxes to roi format. Parameters  bbox_list  ( list[Tensor] ) – a list of bboxes corresponding to a batch of images. Returns  shape (n, 5), [batch_ind, x1, y1, x2, y2] Return type  Tensor \nmmdet.core.bbox. b box cx cy wh to xy xy ( bbox ) Convert bbox coordinates from (cx, cy, w, h) to (x1, y1, x2, y2). \nParameters  bbox  ( Tensor ) – Shape (n, 4) for bboxes. Returns  Converted bboxes. Return type  Tensor "}
{"page": 212, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_212.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmdet .core.bbox.bbox_flip (bboxes, img_shape, direction='horizontal')\nFlip bboxes horizontally or vertically.\n\nParameters\n* bboxes (Tensor) — Shape (..., 4*k)\n\n¢ img_shape (tuple) — Image shape.\n\n¢ direction (str) — Flip direction, options are “horizontal”, “vertical”, “diagonal”. Default:\n\n“horizontal”\nReturns Flipped bboxes.\nReturn type Tensor\n\nmmdet .core.bbox.bbox_mapping (bboxes, img_shape, scale_factor, flip, flip_direction='horizontal')\nMap bboxes from the original image scale to testing scale.\n\nmmdet.core.bbox.bbox_mapping_back (bboxes, img_shape, scale_factor, flip, flip_direction='horizontal')\nMap bboxes from testing scale to original image scale.\n\nmmdet .core.bbox.bbox_overlaps (bboxes1, bboxes2, mode='iou', is_aligned=False, eps=1e-06)\nCalculate overlap between two set of bboxes.\n\nFP16 Contributed by https://github.com/open-mmlab/mmdetection/pull/4889 .. note:\n\nAssume bboxesl is M x 4, bboxes2 is N x 4, when mode is ‘iou',\nthere are some new generated variable when calculating IOU\nusing bbox_overlaps function:\n\n1) is_aligned is False\nareal: Mx 1\narea2: Nx 1\nlt: Mx Nx 2\nrb: Mx N x2\nwh: Mx N x2\noverlap: Mx Nx 1\nunion: Mx Nx1\nious: Mx Nx 1\n\nTotal memory:\n\nS=(9 xNx\n\nWhen using FP16,\nR= (9 xNx\n\nM+N+M) * 4 Byte,\n\nwe can reduce:\nM+N+M) * 4 / 2 Byte\n\nR large than (N + M) * 4 * 2 is always true when N and M >= 1.\nObviously, N+ M <=N * M < 3 * N * M, when N >=2 and M >=2,\nN+ 1< 3 %* N, when N or M is 1.\n\nGiven M = 40 (ground truth), N = 400000 (three anchor boxes\nin per grid, FPN, R-CNNs),\nR = 275 MB (one times)\n\nA special case (dense detection), M = 512 (ground truth),\nR = 3516 MB = 3.43 GB\n\nWhen the batch size is B, reduce:\n\n(continues on next page)\n\n37.2. bbox\n\n205\n\n", "vlm_text": "mmdet.core.bbox. bbox_flip ( bboxes ,  img_shape ,  direction  $=$  'horizontal' ) Flip bboxes horizontally or vertically. \nParameters \n•  img_shape  ( tuple ) – Image shape. •  direction  ( str ) – Flip direction, options are “horizontal”, “vertical”, “diagonal”. Default: “horizontal” \nReturns  Flipped bboxes. \nReturn type  Tensor \nmmdet.core.bbox. b box mapping ( bboxes ,  img_shape ,  scale factor ,  flip ,  flip direction  $=$  'horizontal' ) Map bboxes from the original image scale to testing scale. \nmmdet.core.bbox. b box mapping back ( bboxes ,  img_shape ,  scale factor ,  flip ,  flip direction  $.=$  'horizontal' ) Map bboxes from testing scale to original image scale. \nmmdet.core.bbox. b box overlaps ( bboxes1 ,  bboxes2 ,  mode  $=$  'iou' ,  is_aligned  $\\leftrightharpoons$  False ,  eps=1e-06 ) Calculate overlap between two set of bboxes. \nFP16 Contributed by  https://github.com/open-mmlab/mm detection/pull/4889  .. note: \nThe image contains text related to calculating the Intersection over Union (IoU) between two sets of bounding boxes in computer vision. It covers the following points:\n\n- Assumptions of bounding box dimensions: `bboxes1` is \\(M \\times 4\\), `bboxes2` is \\(N \\times 4\\).\n- Variables generated during the IoU calculation in the `bbox_overlaps` function are listed.\n- Calculation details when `is_aligned` is `False`:\n  - Several variable dimensions: `area1`, `area2`, `lt`, `rb`, `wh`, `overlap`, `union`, and `ious`.\n- Total memory calculation formula: \\(S = (9 \\times N \\times M + N + M) \\times 4\\) Bytes.\n- Memory reduction explanation using FP16: \\(R = (9 \\times N \\times M + N + M) \\times 4 / 2\\) Bytes.\n- Special cases for memory calculations:\n  - Given \\(M = 40\\) and \\(N = 400,000\\), resulting in \\(R = 275\\) MB.\n  - Dense detection with \\(M = 512\\), resulting in \\(R = 3.516\\) MB or 3.43 GB.\n- Consideration of batch size \\(B\\) for further reduction."}
{"page": 213, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_213.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nBxR\nTherefore, CUDA memory runs out frequently.\n\nExperiments on GeForce RTX 2080Ti (11019 MiB):\n\ndtype | M | N |. Use | Real | Ideal |\na-neb) p----) p----2) p---- 2] p----2] 2----:]\n\nFP32 | 512 | 400000 | 8020 MiB | -- | -- |\n\nFP16 | 512 | 400000 | 4504 MiB | 3516 MiB | 3516 MiB |\n\nFP32 | 40 | 400000 | 1540 MiB | -- | -- |\n\nFP16 | 40 | 400000 | 1264 MiB | 276MiB | 275 MiB |\n\n2) is_aligned is True\nareal: Nx 1\narea2: Nx 1\n\n1t: Nx 2\nrb: N x 2\nwh: N x 2\n\noverlap: N x 1\nunion: N x 1\nious: N x 1\n\nTotal memory:\nS = 11x N * 4 Byte\n\nWhen using FP16, we can reduce:\nR= 11xN* 4 / 2 Byte\n\nSo do the 'giou' (large than ‘iou').\nTime-wise, FP16 is generally faster than FP32.\nWhen gpu_assign_thr is not -1, it takes more time on cpu\n\nbut not reduce memory.\nThere, we can reduce half the memory and keep the speed.\n\nIf is_aligned is False, then calculate the overlaps between each bbox of bboxes1 and bboxes2, otherwise the\noverlaps between each aligned pair of bboxes! and bboxes2.\n\nParameters\n* bboxes1 (Tensor) — shape (B, m, 4) in <x1, yl, x2, y2> format or empty.\n\n* bboxes2 (Tensor) — shape (B, n, 4) in <x1, yl, x2, y2> format or empty. B indicates the\nbatch dim, in shape (B1, B2,..., Bn). If is_aligned is True, then m and n must be equal.\n\n* mode (str) — “iou” (intersection over union), “iof” (intersection over foreground) or “giou”\n(generalized intersection over union). Default “iou”’.\n\n* is_aligned (bool, optional) —If True, then m and n must be equal. Default False.\n\n* eps (float, optional) -— A value added to the denominator for numerical stability. De-\nfault le-6.\n\nReturns shape (m, n) if is_aligned is False else shape (m,)\n\nChapter 37. mmdet.core\n\n", "vlm_text": "The image appears to be a screenshot of a text document or slide that explains CUDA memory usage on a GeForce RTX 2080 Ti graphics card with 11,019 MiB of memory. It contains a table that shows memory usage for different data types (FP32 and FP16) and tensor sizes.\n\nKey points from the image include:\n- The table shows memory usage in MiB for tensors when processed using FP32 and FP16 precision. Higher precision (FP32) uses more memory than lower precision (FP16).\n- It suggests that using FP16 precision can reduce memory usage significantly.\n- The \"is_aligned is True\" section lists different variables and their sizes in terms of N (presumably representing some dimension).\n- The section also provides a formula for calculating total memory usage, denoted as \"S,\" and shows that this can be reduced when using FP16, denoted as \"R.\"\n- A note mentions that FP16 is generally faster than FP32.\n- There is also a note that adjusting `gpu_assign_thr` can affect CPU time but not memory reduction.\n- The document concludes with a recommendation to reduce half the memory while keeping speed.\n\nOverall, the image seems to focus on optimizing CUDA memory usage using lower precision (FP16) to make the best use of GPU resources.\nIf  is_aligned  is  False , then calculate the overlaps between each bbox of bboxes1 and bboxes2, otherwise the overlaps between each aligned pair of bboxes1 and bboxes2. \nParameters \n•  bboxes1  ( Tensor ) – shape (B, m, 4) in  $_{<\\mathbf{X}1}$  , y1, x2,   $_{\\mathrm{y}2>}$   format or empty. •  bboxes2  ( Tensor ) – shape (B, n, 4) in <x1, y1, x2,   ${\\mathrm y}2\\!>$   format or empty. B indicates the batch dim, in shape (B1, B2, ..., Bn). If  is_aligned  is  True , then m and n must be equal. •  mode  ( str ) – “iou” (intersection over union), “iof” (intersection over foreground) or “giou” (generalized intersection over union). Default “iou”. •  is_aligned  ( bool, optional ) – If True, then m and n must be equal. Default False. •  eps  ( float, optional ) – A value added to the denominator for numerical stability. De- fault 1e-6. \nReturns  shape   $(\\mathrm{m},\\mathrm{n})$   if  is_aligned  is False else shape (m,) "}
{"page": 214, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_214.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type Tensor\n\nExample\n\n>>> bboxes1 = torch.FloatTensor([\n\n>>> (0, 0, 10, 10],\n\n>>> [10, 10, 20, 20],\n\n>>> (32, 32, 38, 42],\n\n>>> ))\n\n>>> bboxes2 = torch.FloatTensor([\n\n>>> (0, 0, 10, 20],\n\n>>> (0, 10, 10, 19],\n\n>>> [10, 10, 20, 20],\n\n>>> ))\n\n>>> overlaps = bbox_overlaps(bboxes1, bboxes2)\n\n>>> assert overlaps.shape == (3, 3)\n\n>>> overlaps = bbox_overlaps(bboxes1l, bboxes2, is_aligned=True)\n>>> assert overlaps.shape == (3, )\n\nExample\n\n>>> empty = torch.empty(0, 4)\n\n>>> nonempty = torch.FloatTensor([[9, 0, 10, 9]])\n\n>>> assert tuple(bbox_overlaps(empty, nonempty).shape) == (0, 1)\n>>> assert tuple(bbox_overlaps(nonempty, empty).shape) == (1, 0)\n>>> assert tuple(bbox_overlaps(empty, empty).shape) == (0, 9)\n\nmmdet .core .bbox.bbox_rescale(bboxes, scale_factor=1.0)\nRescale bounding box w.r.t. scale_factor.\n\nParameters\n* bboxes (Tensor) — Shape (n, 4) for bboxes or (n, 5) for rois\n¢ scale_factor (float) — rescale factor\n\nReturns Rescaled bboxes.\n\nReturn type Tensor\n\nmmdet .core.bbox.bbox_xyxy_to_cxcywh (bbox)\nConvert bbox coordinates from (x1, yl, x2, y2) to (cx, cy, w, h).\n\nParameters bbox (Tensor) — Shape (n, 4) for bboxes.\nReturns Converted bboxes.\nReturn type Tensor\n\nmmdet.core.bbox.build_assigner (cfg, **default_args)\nBuilder of box assigner.\n\nmmdet.core.bbox.build_bbox_coder (cfg, **default_args)\nBuilder of box coder.\n\nmmdet.core.bbox.build_sampler (cfg, **default_args)\nBuilder of box sampler.\n\n37.2. bbox\n\n207\n\n", "vlm_text": "Return type  Tensor \nExample \n>>>  bboxes1  $=$   torch . Float Tensor([ >>> [ 0 ,  0 ,  10 ,  10 ], >>> [ 10 ,  10 ,  20 ,  20 ], >>> [ 32 ,  32 ,  38 ,  42 ], >>>  ]) >>>  bboxes2  $=$   torch . Float Tensor([ >>> [ 0 ,  0 ,  10 ,  20 ], >>> [ 0 ,  10 ,  10 ,  19 ], >>> [ 10 ,  10 ,  20 ,  20 ], >>>  ])  $>>$   overlaps  $=$   b box overlaps(bboxes1, bboxes2)  $>>$   assert  overlaps . shape  $==$   ( 3 ,  3 )  $>>$   overlaps  $=$   b box overlaps(bboxes1, bboxes2, is_aligned = True )  $>>$   assert  overlaps . shape  ==  ( 3 , ) \nExample \n $>>$   empty  $=$   torch . empty( 0 ,  4 )  $>>$   nonempty  $=$   torch . Float Tensor([[ 0 ,  0 ,  10 ,  9 ]])  $>>$   assert  tuple (b box overlaps(empty, nonempty) . shape)  ==  ( 0 ,  1 )  $>>$   assert  tuple (b box overlaps(nonempty, empty) . shape)  ==  ( 1 ,  0 )  $>>$   assert  tuple (b box overlaps(empty, empty) . shape)  ==  ( 0 ,  0 ) \nmmdet.core.bbox. b box re scale ( bboxes ,  scale factor  ${\\simeq}I.0$  ) \nParameters •  bboxes  ( Tensor ) – Shape (n, 4) for bboxes or (n, 5) for rois •  scale factor  ( float ) – rescale factor Returns  Rescaled bboxes. Return type  Tensor mmdet.core.bbox. b box xy xy to cx cy wh ( bbox ) Convert bbox coordinates from (x1, y1, x2, y2) to (cx, cy, w, h). Parameters  bbox  ( Tensor ) – Shape (n, 4) for bboxes. Returns  Converted bboxes. Return type  Tensor mmdet.core.bbox. build as signer ( cfg ,  \\*\\*default arg s ) Builder of box assigner. mmdet.core.bbox. build b box code r ( cfg ,  \\*\\*default arg s ) Builder of box coder. mmdet.core.bbox. build sampler ( cfg ,  \\*\\*default arg s ) Builder of box sampler. "}
{"page": 215, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_215.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmdet.core.bbox.distance2bbox (points, distance, max_shape=None)\nDecode distance prediction to bounding box.\n\nParameters\n* points (Tensor) — Shape (B, N, 2) or (N, 2).\n\n* distance (Tensor) — Distance from the given point to 4 boundaries (left, top, right, bot-\ntom). Shape (B, N, 4) or (N, 4)\n\n¢ (Sequence[int] or torch.Tensor or Sequence[ (max_shape) - Se-\nquence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If\npriors shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and the\nlength of max_shape should also be B.\n\nReturns Boxes with shape (N, 4) or (B, N, 4)\nReturn type Tensor\n\nmmdet .core.bbox.roi2bbox(rois)\nConvert rois to bounding box format.\n\nParameters rois (torch. Tensor) — Rols with the shape (n, 5) where the first column indicates\nbatch id of each Rol.\n\nReturns Converted boxes of corresponding rois.\n\nReturn type list[torch.Tensor]\n\n37.3 export\n\nmmdet.core. export .add_dummy_nms_for_onnx (boxes, scores, max_output_boxes_per_class=1000,\niou_threshold=0.5, score_threshold=0.05, pre_top_k=- 1,\nafter_top_k=- 1, labels=None)\nCreate a dummy onnx::NonMaxSuppression op while exporting to ONNX.\n\nThis function helps exporting to onnx with batch and multiclass NMS op. It only supports class-agnostic de-\ntection results. That is, the scores is of shape (N, num_bboxes, num_classes) and the boxes is of shape (N,\nnum_boxes, 4).\n\nParameters\n* boxes (Tensor) — The bounding boxes of shape [N, num_boxes, 4]\n* scores (Tensor) — The detection scores of shape [N, num_boxes, num_classes]\n\n* max_output_boxes_per_class (int) — Maximum number of output boxes per class of\nnms. Defaults to 1000.\n\n¢ iou_threshold (float) — IOU threshold of nms. Defaults to 0.5\n\n* score_threshold (float) — score threshold of nms. Defaults to 0.05.\n\n* pre_top_k (bool) — Number of top K boxes to keep before nms. Defaults to -1.\n* after_top_k (int) — Number of top K boxes to keep after nms. Defaults to -1.\n\n* labels (Tensor, optional) — It not None, explicit labels would be used. Otherwise,\nlabels would be automatically generated using num_classed. Defaults to None.\n\nReturns\n\ndets of shape [N, num_det, 5] and class labels of shape [N, num_det].\n\n208 Chapter 37. mmdet.core\n", "vlm_text": "mmdet.core.bbox. distance 2 b box ( points ,  distance ,  max_shape=None ) Decode distance prediction to bounding box. \nParameters \n•  points  ( Tensor ) – Shape (B, N, 2) or (N, 2). •  distance  ( Tensor ) – Distance from the given point to 4 boundaries (left, top, right, bot- tom). Shape (B, N, 4) or (N, 4) •  (Sequence[int] or torch.Tensor or Sequence[ ( max_shape ) – Se- quence[int]],optional): Maximum bounds for boxes, specifies (H, W, C) or (H, W). If priors shape is (B, N, 4), then the max_shape should be a Sequence[Sequence[int]] and the length of max_shape should also be B. \nReturns  Boxes with shape (N, 4) or (B, N, 4) \nReturn type  Tensor \nmmdet.core.bbox. roi2bbox ( rois ) Convert rois to bounding box format. \nParameters  rois  ( torch.Tensor ) – RoIs with the shape (n, 5) where the first column indicates batch id of each RoI. \nReturns  Converted boxes of corresponding rois. \nReturn type  list[torch.Tensor] \n37.3 export \nmmdet.core.export. add dummy nm s for on nx ( boxes ,  scores ,  max output boxes per class  $\\mathbf{=}$  1000 , i ou threshold  $\\mathbf{\\chi}{=}0.5$  ,  score threshold  $\\it{=}0.05$  ,  pre_top_  $k{=}{\\mathfrak{-}}\\,I$  , after_top_  $k{=}{\\mathfrak{-}}\\,I$  ,  labels  $\\scriptstyle\\sum=I$  None ) Create a dummy onnx::Non Max Suppression op while exporting to ONNX. \nThis function helps exporting to onnx with batch and multiclass NMS op. It only supports class-agnostic de- tection results. That is, the scores is of shape (N, num_bboxes, num classes) and the boxes is of shape (N, num_boxes, 4). \nParameters \n•  boxes  ( Tensor ) – The bounding boxes of shape [N, num_boxes, 4] •  scores  ( Tensor ) – The detection scores of shape [N, num_boxes, num classes] •  max output boxes per class  ( int ) – Maximum number of output boxes per class of nms. Defaults to 1000. •  i ou threshold  ( float ) – IOU threshold of nms. Defaults to 0.5 •  score threshold  ( float ) – score threshold of nms. Defaults to 0.05. •  pre_top_k  ( bool ) – Number of top K boxes to keep before nms. Defaults to -1. •  after top k  ( int ) – Number of top K boxes to keep after nms. Defaults to -1. •  labels  ( Tensor, optional ) – It not None, explicit labels would be used. Otherwise, labels would be automatically generated using num classed. Defaults to None. \nReturns \ndets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. "}
{"page": 216, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_216.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type tuple[Tensor, Tensor]\n\nmmdet.core.export . build_model_from_cfg(config_path, checkpoint_path, cfg_options=None)\nBuild a model from config and load the given checkpoint.\n\nParameters\n* config_path (str) — the OpenMMLab config for the model we want to export to ONNX\n* checkpoint_path (str) — Path to the corresponding checkpoint\n\nReturns the built model\n\nReturn type torch.nn.Module\n\nmmdet.core.export .dynamic_clip_for_onnx(x/, y/, x2, y2, max_shape)\nClip boxes dynamically for onnx.\n\nSince torch.clamp cannot have dynamic min and max, we scale the boxes by 1/max_shape and clamp in the\nrange [0, 1].\nParameters\n¢ x1 (Tensor) — The x1 for bounding boxes.\n* yl (Tensor) — The y! for bounding boxes.\n* x2 (Tensor) — The x2 for bounding boxes.\n* y2 (Tensor) — The y2 for bounding boxes.\n* max_shape (Tensor or torch. Size) — The (H,W) of original image.\nReturns The clipped x1, yl, x2, y2.\nReturn type tuple(Tensor)\nmmdet.core.export .generate_inputs_and_wrap_model (config_path, checkpoint_path, input_config,\n\ncfg_options=None)\nPrepare sample input and wrap model for ONNX export.\n\nThe ONNX export API only accept args, and all inputs should be torch.Tensor or corresponding types (such as\ntuple of tensor). So we should call this function before exporting. This function will:\n\n1. generate corresponding inputs which are used to execute the model.\n2. Wrap the model’s forward function.\n\nFor example, the MMDet models’ forward function has a parameter return_loss:bool. As we want to set it\nas False while export API supports neither bool type or kwargs. So we have to replace the forward method like\nmodel. forward = partial(model.forward, return_loss=False).\n\nParameters\n* config_path (str) — the OpenMMLab config for the model we want to export to ONNX\n* checkpoint_path (str) — Path to the corresponding checkpoint\n\n¢ input_config (dict) —the exactly data in this dict depends on the framework. For MMSeg,\nwe can just declare the input shape, and generate the dummy data accordingly. However, for\nMMDet, we may pass the real img path, or the NMS will return None as there is no legal\nbbox.\n\nReturns\n\n37.3. export 209\n", "vlm_text": "Return type  tuple[Tensor, Tensor] \nmmdet.core.export. build model from cf g ( config path ,  checkpoint path ,  cf g options  $=$  None ) Build a model from config and load the given checkpoint. \nParameters \n•  config path  ( str ) – the OpenMMLab config for the model we want to export to ONNX •  checkpoint path  ( str ) – Path to the corresponding checkpoint \nReturns  the built model \nReturn type  torch.nn.Module \nmmdet.core.export. dynamic clip for on nx ( x1 ,  y1 ,  x2 ,  y2 ,  max_shape ) Clip boxes dynamically for onnx. \nSince torch.clamp cannot have dynamic  min  and  max , we scale the  boxes by 1/max_shape and clamp in the range [0, 1]. \nParameters \n•  x1  ( Tensor ) – The x1 for bounding boxes. •  y1  ( Tensor ) – The y1 for bounding boxes. •  x2  ( Tensor ) – The x2 for bounding boxes. •  y2  ( Tensor ) – The y2 for bounding boxes. •  max_shape  ( Tensor or torch.Size ) – The (H,W) of original image. \nReturns  The clipped x1, y1, x2, y2. \nReturn type  tuple(Tensor) \nmmdet.core.export. generate inputs and wrap model ( config path ,  checkpoint path ,  input config , cf g options  $\\mathbf{:=}$  None ) \nPrepare sample input and wrap model for ONNX export. \nThe ONNX export API only accept args, and all inputs should be torch.Tensor or corresponding types (such as tuple of tensor). So we should call this function before exporting. This function will: \n1. generate corresponding inputs which are used to execute the model. 2. Wrap the model’s forward function. \nFor example, the MMDet models’ forward function has a parameter  return loss:bool . As we want to set it as False while export API supports neither bool type or kwargs. So we have to replace the forward method like model.forward  $=$   partial(model.forward, return loss  $=$  False) . \nParameters \n•  config path  ( str ) – the OpenMMLab config for the model we want to export to ONNX •  checkpoint path  ( str ) – Path to the corresponding checkpoint •  input config  ( dict ) – the exactly data in this dict depends on the framework. For MMSeg, we can just declare the input shape, and generate the dummy data accordingly. However, for MMDet, we may pass the real img path, or the NMS will return None as there is no legal bbox. \nReturns "}
{"page": 217, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_217.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(model, tensor_data) wrapped model which can be called by model(*tensor_data) and\na list of inputs which are used to execute the model while exporting.\n\nReturn type tuple\n\nmmdet.core.export.get_k_for_topk(k, size)\nGet k of TopK for onnx exporting.\n\nThe K of TopK in TensorRT should not be a Tensor, while in ONNX Runtime it could be a Tensor.Due to\ndynamic shape feature, we have to decide whether to do TopK and what K it should be while exporting to\nONNX.\n\nIf returned K is less than zero, it means we do not have to do TopK operation.\n\nParameters\n¢« k(int or Tensor) — The set k value for nms from config file.\n* size (Tensor or torch. Size) — The number of elements of TopK’s input tensor\nReturns (int or Tensor): The final K for TopK.\nReturn type tuple\nmmdet.core.export.preprocess_example_input (input_config)\nPrepare an example input image for generate_inputs_and_wrap_model.\nParameters input_config (dict) — customized config describing the example input.\n\nReturns (one_img, one_meta), tensor of the example input image and meta information for the ex-\nample input image.\n\nReturn type tuple\n\nExamples\n\n>>> from mmdet.core.export import preprocess_example_input\n>>> input_config = {\n\n>>> ‘input_shape': (1,3,224,224),\n\n>>> ‘input_path': 'demo/demo.jpg',\n\n>>> \"normalize_cfg': {\n\n>>> \"mean': (123.675, 116.28, 103.53),\n>>> \"std': (58.395, 57.12, 57.375)\n>>> }\n\n>>> }\n\n>>> one_img, one_meta = preprocess_example_input (input_config)\n>>> print (one_img. shape)\n\ntorch.Size([1, 3, 224, 224])\n\n>>> print (one_meta)\n\n{'img_shape': (224, 224, 3),\n\n‘ori_shape': (224, 224, 3),\n\n\"pad_shape': (224, 224, 3),\n\n'filename': '<demo>.png',\n\n\"scale_factor': 1.0,\n\n'flip': False}\n\n210 Chapter 37. mmdet.core\n", "vlm_text": "(model, tensor data) wrapped model which can be called by  model(\\*tensor data)  and a list of inputs which are used to execute the model while exporting. \nReturn type  tuple \nmmdet.core.export. get k for top k ( k ,  size ) Get k of TopK for onnx exporting. \nThe K of TopK in TensorRT should not be a Tensor, while in ONNX Runtime  it could be a Tensor.Due to dynamic shape feature, we have to decide whether to do TopK and what K it should be while exporting to ONNX. \nIf returned K is less than zero, it means we do not have to do  TopK operation. \nParameters \n•  k  ( int or Tensor ) – The set k value for nms from config file. •  size  ( Tensor or torch.Size ) – The number of elements of TopK’s input tensor \nReturns  (int or Tensor): The final K for TopK. \nReturn type  tuple \nmmdet.core.export. pre process example input ( input config ) Prepare an example input image for  generate inputs and wrap model \nParameters  input config  ( dict ) – customized config describing the example input. \nReturns  (one_img, one_meta), tensor of the example input image and meta information for the ex- ample input image. \nReturn type  tuple \nExamples \nThis image shows a Python code snippet used for preprocessing input data, likely for a machine learning application. Here's a breakdown of what it does:\n\n1. **Import**:\n   - A function `preprocess_example_input` is imported from `mmdet.core.export`.\n\n2. **Input Configuration**:\n   - `input_config` is a dictionary defining preprocessing parameters:\n     - `'input_shape'`: `(1, 3, 224, 224)`, which specifies the size and channels of the input image tensor.\n     - `'input_path'`: `'demo/demo.jpg'`, indicating the path to the input image.\n     - `'normalize_cfg'`: A dictionary containing:\n       - `'mean'`: `(123.675, 116.28, 103.53)`, mean values for normalization.\n       - `'std'`: `(58.395, 57.12, 57.375)`, standard deviation values for normalization.\n\n3. **Processing the Input**:\n   - `one_img, one_meta = preprocess_example_input(input_config)`: This line calls the imported function with `input_config` and stores the results in `one_img` and `one_meta`.\n\n4. **Printing Results**:\n   - `print(one_img.shape)`: This prints the shape of the processed image tensor, which is `torch.Size([1, 3, 224, 224])`.\n   - `print(one_meta)`: This prints metadata associated with the image, which includes:\n     - `'img_shape'`: Original image shape `(224, 224, 3)`\n     - `'ori_shape'`: Original shape before preprocessing `(224, 224, 3)`\n     - `'pad_shape'`: Shape after any padding `(224, 224, 3)`\n     - `'filename'`: Image filename `'<demo>.png'`\n     - `'scale_factor'`: Scaling factor `1.0`\n     - `'flip'`: Indicates whether the image was flipped `False`"}
{"page": 218, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_218.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n37.4 mask\n\nclass mmdet.core.mask.BaseInstanceMasks\nBase class for instance masks.\n\nabstract property areas\nareas of each instance.\n\nType ndarray\n\nabstract crop(bbox)\nCrop each mask by the given bbox.\n\nParameters bbox (ndarray) — Bbox in format [x1, yl, x2, y2], shape (4, ).\nReturns The cropped masks.\nReturn type Basel/nstanceMasks\n\nabstract crop_and_resize(bboxes, out_shape, inds, device, interpolation='bilinear', binarize=True)\nCrop and resize masks by the given bboxes.\n\nThis function is mainly used in mask targets computation. It firstly align mask to bboxes by assigned_inds,\nthen crop mask by the assigned bbox and resize to the size of (mask_h, mask_w)\n\nParameters\n* bboxes (Tensor) — Bboxes in format [x1, yl, x2, y2], shape (N, 4)\n* out_shape (tuple [int ]) — Target (h, w) of resized mask\n\n¢ inds (ndarray) — Indexes to assign masks to each bbox, shape (N,) and values should be\nbetween [0, num_masks - 1].\n\ndevice (str) — Device of bboxes\n¢ interpolation (str) — See mmcv.imresize\n\n¢ binarize (boo1)-if True fractional values are rounded to 0 or | after the resize operation.\nif False and unsupported an error will be raised. Defaults to True.\n\nReturns the cropped and resized masks.\nReturn type Basel/nstanceMasks\n\nabstract expand (expanded_h, expanded_w, top, left)\nsee Expand.\n\nabstract flip (flip_direction='horizontal')\nFlip masks alone the given direction.\n\nParameters flip_direction (str) — Either ‘horizontal’ or ‘vertical’.\nReturns The flipped masks.\nReturn type Basel/nstanceMasks\n\nabstract pad(out_shape, pad_val)\nPad masks to the given size of (h, w).\n\nParameters\n* out_shape (tuple[int]) — Target (h, w) of padded mask.\n* pad_val (int) — The padded value.\n\nReturns The padded masks.\n\n37.4. mask 211\n", "vlm_text": "37.4 mask \nclass  mmdet.core.mask. Base Instance Masks Base class for instance masks. \nabstract property areas areas of each instance. Type  ndarray \nabstract crop ( bbox ) Crop each mask by the given bbox. Parameters  bbox  ( ndarray ) – Bbox in format [x1, y1, x2, y2], shape (4, ). Returns  The cropped masks. Return type  Base Instance Masks \nabstract crop and resize ( bboxes ,  out_shape ,  inds ,  device ,  interpolation  $.=$  'bilinear' ,  binarize  $\\mathbf{=}$  True ) Crop and resize masks by the given bboxes. \nThis function is mainly used in mask targets computation. It firstly align mask to bboxes by assigned in ds, then crop mask by the assigned bbox and resize to the size of (mask_h, mask_w) \nParameters •  bboxes  ( Tensor ) – Bboxes in format [x1, y1, x2, y2], shape (N, 4) •  out_shape  ( tuple[int] ) – Target (h, w) of resized mask •  inds  ( ndarray ) – Indexes to assign masks to each bbox, shape (N,) and values should be between [0, num_masks - 1]. •  device  ( str ) – Device of bboxes •  interpolation  ( str ) – See  mmcv.imresize •  binarize  ( bool ) – if True fractional values are rounded to 0 or 1 after the resize operation. if False and unsupported an error will be raised. Defaults to True. Returns  the cropped and resized masks. Return type  Base Instance Masks \nabstract expand(expanded_h, expanded_w, top, left)see  Expand . \nabstract flip ( flip direction  $.=$  'horizontal' ) Flip masks alone the given direction. Parameters  flip direction  ( str ) – Either ‘horizontal’ or ‘vertical’. Returns  The flipped masks. Return type  Base Instance Masks abstract pad(out_shape, pad_val)Pad masks to the given size of (h, w). Parameters •  out_shape  ( tuple[int] ) – Target (h, w) of padded mask. •  pad_val  ( int ) – The padded value. Returns  The padded masks. "}
{"page": 219, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_219.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type Basel/nstanceMasks\n\nabstract rescale(scale, interpolation='nearest')\nRescale masks as large as possible while keeping the aspect ratio. For details can refer to mmcv.imrescale.\n\nParameters\n¢ scale (tuple[int]) — The maximum size (h, w) of rescaled mask.\n¢ interpolation (str) —Same as mmcv.imrescale().\n\nReturns The rescaled masks.\n\nReturn type Basel/nstanceMasks\n\nabstract resize(out_shape, interpolation=\"nearest')\nResize masks to the given out_shape.\n\nParameters\n\n* out_shape — Target (h, w) of resized mask.\n\n¢ interpolation (str) — See mmcv.imresize().\nReturns The resized masks.\nReturn type Basel/nstanceMasks\n\nabstract rotate(out_shape, angle, center=None, scale=1.0, fill_val=0)\nRotate the masks.\n\nParameters\n* out_shape (tuple[int]) — Shape for output mask, format (h, w).\n\n* angle (int | float) — Rotation angle in degrees. Positive values mean counter-\nclockwise rotation.\n\n* center (tuple[float], optional) — Center point (w, h) of the rotation in source im-\nage. If not specified, the center of the image will be used.\n\n¢ scale(int | float) — Isotropic scale factor.\n¢ fill_val (int | float) -— Border value. Default 0 for masks.\nReturns Rotated masks.\n\nshear (out_shape, magnitude, direction='horizontal', border_value=0, interpolation='bilinear')\nShear the masks.\n\nParameters\n* out_shape (tuple[int]) — Shape for output mask, format (h, w).\n* magnitude (int | float) —The magnitude used for shear.\n¢ direction (str) — The shear direction, either “horizontal” or “vertical”.\n\n¢ border_value (int | tuple[int]) — Value used in case of a constant border. Default\n0.\n\n¢ interpolation (str) — Same as in mmcv.imshear().\nReturns Sheared masks.\nReturn type ndarray\n\nabstract to_ndarray()\nConvert masks to the format of ndarray.\n\n212 Chapter 37. mmdet.core\n", "vlm_text": "Return type  Base Instance Masks \nabstract rescale ( scale ,  interpolation  $=$  'nearest' ) Rescale masks as large as possible while keeping the aspect ratio. For details can refer to  mmcv.imrescale . \nParameters •  scale  ( tuple[int] ) – The maximum size (h, w) of rescaled mask. •  interpolation  ( str ) – Same as  mmcv.imrescale() . Returns  The rescaled masks. Return type  Base Instance Masks \nabstract resize ( out_shape ,  interpolation  $=$  'nearest' ) Resize masks to the given out_shape. \nParameters \n•  out_shape  – Target (h, w) of resized mask. •  interpolation  ( str ) – See  mmcv.imresize() . Returns  The resized masks. Return type  Base Instance Masks \nabstract rotate ( out_shape ,  angle ,  center  $\\leftrightharpoons$  None ,  scale  $\\mathbf{=}$  1.0 ,  fill_val=0 ) Rotate the masks. \nParameters \n•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  angle  ( int | float ) – Rotation angle in degrees. Positive values mean counter- clockwise rotation. •  center  ( tuple[float], optional ) – Center point (w, h) of the rotation in source im- age. If not specified, the center of the image will be used. •  scale  ( int | float ) – Isotropic scale factor. •  fill_val  ( int | float ) – Border value. Default 0 for masks. \nReturns  Rotated masks. \nshear ( out_shape ,  magnitude ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  border value  $\\mathrm{=}0$  ,  interpolation  $=$  'bilinear' ) Shear the masks. \nParameters \n•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  magnitude  ( int | float ) – The magnitude used for shear. •  direction  ( str ) – The shear direction, either “horizontal” or “vertical”. •  border value  ( int   $I$   tuple[int] ) – Value used in case of a constant border. Default 0. •  interpolation  ( str ) – Same as in  mmcv.imshear() . Returns  Sheared masks. \nReturn type  ndarray \nabstract to_ndarray () Convert masks to the format of ndarray. "}
{"page": 220, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_220.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturns Converted masks in the format of ndarray.\nReturn type ndarray\n\nabstract to_tensor(dtype, device)\nConvert masks to the format of Tensor.\n\nParameters\n\n¢ dtype (str) — Dtype of converted mask.\n\n¢ device (torch. device) — Device of converted masks.\nReturns Converted masks in the format of Tensor.\nReturn type Tensor\n\nabstract translate (out_shape, offset, direction='horizontal', fill_val=0, interpolation='bilinear')\nTranslate the masks.\n\nParameters\n* out_shape (tuple[int]) — Shape for output mask, format (h, w).\n¢ offset (int | float) —The offset for translate.\n¢ direction (str) — The translate direction, either “horizontal” or “vertical”.\n¢ fill_val (int | float) — Border value. Default 0.\n¢ interpolation (str) — Same as mmcv.imtranslate().\nReturns Translated masks.\n\nclass mmdet.core.mask.BitmapMasks (masks, height, width)\nThis class represents masks in the form of bitmaps.\n\nParameters\n* masks (ndarray) — ndarray of masks in shape (N, H, W), where N is the number of objects.\n¢ height (int) — height of masks\n\n¢ width (int) — width of masks\n\nExample\n\n>>> from mmdet.core.mask.structures import * # NOQA\n\n>>> num_masks, H, W = 3, 32, 32\n\n>>> rng = np.random.RandomState(0)\n\n>>> masks = (rng.rand(num_masks, H, W) > 0.1).astype(np.int)\n>>> self = BitmapMasks(masks, height=H, width=W)\n\n>>> # demo crop_and_resize\n>>> num_boxes = 5\n>>> bboxes = np.array([[0, 9, 30, 10.0]] * num_boxes)\n>>> out_shape = (14, 14)\n>>> inds = torch.randint(9, len(self), size=(num_boxes,))\n>>> device = 'cpu'\n>>> interpolation = 'bilinear'\n>>> new = self.crop_and_resize(\nbboxes, out_shape, inds, device, interpolation)\n\n(continues on next page)\n\n37.4. mask 213\n", "vlm_text": "Returns  Converted masks in the format of ndarray. \nReturn type  ndarray \nabstract to_tensor ( dtype ,  device ) Convert masks to the format of Tensor. \nParameters \n•  dtype  ( str ) – Dtype of converted mask. •  device  ( torch.device ) – Device of converted masks. Returns  Converted masks in the format of Tensor. \nReturn type  Tensor \nabstract translate ( out_shape ,  offset ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  fill_va  $\\imath\\!\\!=\\!\\!O$  ,  interpolation  $.=$  'bilinear' ) Translate the masks. \nParameters \n•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  offset  ( int   $I$   float ) – The offset for translate. •  direction  ( str ) – The translate direction, either “horizontal” or “vertical”. •  fill_val  ( int | float ) – Border value. Default 0. •  interpolation  ( str ) – Same as  mmcv.im translate() . Returns  Translated masks. \nclass  mmdet.core.mask. Bitmap Masks ( masks ,  height ,  width ) This class represents masks in the form of bitmaps. \nParameters \n•  masks  ( ndarray ) – ndarray of masks in shape (N, H, W), where N is the number of objects. •  height  ( int ) – height of masks •  width  ( int ) – width of masks \nExample \nThe image shows a Python code snippet involving image processing tasks, likely related to computer vision. The code imports a module and demonstrates mask creation and resizing operations. Here's a summary of the key parts:\n\n1. Import statement:\n   - Imports functions from `mmdet.core.mask.structures`.\n\n2. Mask creation:\n   - `num_masks`, `H`, `W` are set to 3, 32, and 32, respectively.\n   - A random number generator `rng` is initialized.\n   - A `masks` array is created where values are greater than 0.1.\n   - `BitmapMasks` is instantiated with the `masks`, with specified height `H` and width `W`.\n\n3. Cropping and resizing:\n   - `num_boxes` is set to 5.\n   - `bboxes` is an array indicating bounding box dimensions.\n   - `out_shape` is set to (14, 14).\n   - `inds` generates random indices based on `self`.\n   - Specifies the device as 'cpu' and interpolation method as 'bilinear'.\n   - Calls a method `crop_and_resize` on `self` with the specified parameters.\n\nThis code seems to demonstrate patch-based image processing, useful for tasks like segmentation or detection using bounding boxes."}
{"page": 221, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_221.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n>>> assert len(mew) == num_boxes\n>>> assert new.height, new.width == out_shape\n\nproperty areas\nSee BaseInstanceMasks. areas.\n\ncrop (bbox)\nSee BaseInstanceMasks.cropQ.\n\ncrop_and_resize(bboxes, out_shape, inds, device='cpu', interpolation='bilinear', binarize=True)\nSee BaseInstanceMasks.crop_and_resize().\n\nexpand (expanded_h, expanded_w, top, left)\nSee BaseInstanceMasks.expand().\n\nflip (flip_direction='horizontal')\nSee BaseInstanceMasks. flipQ.\n\npad (out_shape, pad_val=0)\nSee BaseInstanceMasks.pad(Q).\n\nclassmethod random(num_masks=3, height=32, width=32, dtype=<class ‘numpy.uint8'>, rng=None)\nGenerate random bitmap masks for demo / testing purposes.\n\nExample\n\n>>> from mmdet.core.mask.structures import BitmapMasks\n>>> self = BitmapMasks.random()\n\n>>> print('self = {}'.format(self))\n\nself = BitmapMasks(num_masks=3, height=32, width=32)\n\nrescale(scale, interpolation='nearest')\nSee BaseInstanceMasks.rescale().\n\nresize (out_shape, interpolation='nearest')\nSee BaseInstanceMasks.resize(Q).\n\nrotate (out_shape, angle, center=None, scale=1.0, fill_val=0)\nRotate the BitmapMasks.\n\nParameters\n* out_shape (tuple[int]) — Shape for output mask, format (h, w).\n\n* angle (int | float) — Rotation angle in degrees. Positive values mean counter-\nclockwise rotation.\n\n* center (tuple[float], optional) — Center point (w, h) of the rotation in source im-\nage. If not specified, the center of the image will be used.\n\n* scale(int | float) — Isotropic scale factor.\n\n¢ fill_val (int | float) -— Border value. Default 0 for masks.\nReturns Rotated BitmapMasks.\nReturn type BitmapMasks\n\nshear (out_shape, magnitude, direction='horizontal', border_value=0, interpolation='bilinear')\nShear the BitmapMasks.\n\n214 Chapter 37. mmdet.core\n", "vlm_text": "The image shows Python code with two assert statements:\n\n1. `assert len(new) == num_boxes`\n2. `assert new.height, new.width == out_shape`\n\nThese statements are likely checking conditions within a program to ensure that the length of a collection `new` equals `num_boxes`, and that the `height` and `width` attributes of `new` match `out_shape`. If these conditions are not met, the program will raise an AssertionError.\nproperty areas See  Base Instance Masks.areas . \ncrop ( bbox ) See  Base Instance Masks.crop() \ncrop and resize ( bboxes ,  out_shape ,  inds ,  device  $\\mathbf{=}$  'cpu' ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'bilinear' ,  binarize  $\\mathbf{\\tilde{=}}$  True ) See  Base Instance Masks.crop and resize() . \nexpand(expanded_h, expanded_w, top, left)See  Base Instance Masks.expand() . \nflip ( flip direction  $=$  'horizontal' ) See  Base Instance Masks.flip() . \npad ( out_shape ,  pad_va  $\\mathit{l}\\mathrm{=}\\mathit{0.}$  ) See  Base Instance Masks.pad() . \nclass method random ( num_mask  $\\scriptstyle{:=3}$  ,  height=32 ,  width  $\\iota{=}32$  ,  dtyp  $\\scriptstyle{\\mathcal{S}}=$  <class 'numpy.uint8  $^{\\prime}{>}$  ,  $r n g{=}N o n e)$  ) Generate random bitmap masks for demo / testing purposes. \nExample \n $>>$   from  mmdet.core.mask.structures  import  Bitmap Masks >>> self $=$  Bitmap Masks.random() $>>$   print ( ' self  $\\begin{array}{r l}{\\mathbf{\\Sigma}=}&{{}\\{\\mathcal{Y}}}\\end{array}$  ' . format( self )) self  $=$   Bitmap Masks(num_masks  $^{=3}$  , height  $.=32$  , width  $\\scriptstyle{1=32}$  ) \nrescale ( scale ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'nearest' ) See Base Instance Masks.rescale()\nresize ( out_shape ,  interpolation  $=$  'nearest' ) See  Base Instance Masks.resize() . \nrotate ( out_shape ,  angle ,  center  $\\mathbf{\\hat{\\Sigma}}$  None ,  scale  $\\mathbf{\\tilde{=}}$  1.0 ,  fill_va  $l{=}0.$  ) Rotate the Bitmap Masks. \nParameters \n•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  angle  ( int | float ) – Rotation angle in degrees. Positive values mean counter- clockwise rotation. •  center  ( tuple[float], optional ) – Center point (w, h) of the rotation in source im- age. If not specified, the center of the image will be used. •  scale  ( int | float ) – Isotropic scale factor. •  fill_val  ( int | float ) – Border value. Default 0 for masks. Returns  Rotated Bitmap Masks. Return type Bitmap Masks\nshear ( out_shape ,  magnitude ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  border value  $\\scriptstyle{\\cdot=0}$  ,  interpolation  $\\mathbf{\\beta}=$  'bilinear' ) Shear the Bitmap Masks. "}
{"page": 222, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_222.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n* out_shape (tuple[int]) — Shape for output mask, format (h, w).\n* magnitude (int | float) —The magnitude used for shear.\n¢ direction (str) — The shear direction, either “horizontal” or “vertical”.\n¢ border_value (int | tuple[int])-— Value used in case of a constant border.\n¢ interpolation (str) — Same as in mmcv.imshear().\nReturns The sheared masks.\nReturn type BitmapMasks\n\nto_ndarray ()\nSee BaseInstanceMasks.to_ndarray().\n\nto_tensor (dtype, device)\nSee BaseInstanceMasks.to_tensor().\n\ntranslate (out_shape, offset, direction='horizontal', fill_val=0, interpolation='bilinear')\nTranslate the BitmapMasks.\n\nParameters\n* out_shape (tuple[int]) — Shape for output mask, format (h, w).\n¢ offset (int | float) —The offset for translate.\n¢ direction (str) — The translate direction, either “horizontal” or “vertical”.\n¢ fill_val (int | float) -— Border value. Default 0 for masks.\n¢ interpolation (str) — Same as mmcv.imtranslate().\nReturns Translated BitmapMasks.\n\nReturn type BitmapMasks\n\nExample\n\n>>> from mmdet.core.mask.structures import BitmapMasks\n>>> self = BitmapMasks.random(dtype=np.uint8)\n\n>>> out_shape = (32, 32)\n\n>>> offset = 4\n\n>>> direction = 'horizontal'\n>>> fill_val = 0\n>>> interpolation = 'bilinear'\n\n>>> # Note, There seem to be issues when:\n\n>>> # * out_shape is different than self's shape\n\n>>> # * the mask dtype is not supported by cv2.AffineWarp\n\n>>> new = self.translate(out_shape, offset, direction, fill_val,\n>>> interpolation)\n\n>>> assert len(mew) == len(self)\n\n>>> assert new.height, new.width == out_shape\n\nclass mmdet.core.mask.PolygonMasks (masks, height, width)\nThis class represents masks in the form of polygons.\n\n37.4. mask 215\n", "vlm_text": "Parameters \n•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  magnitude  ( int | float ) – The magnitude used for shear. •  direction  ( str ) – The shear direction, either “horizontal” or “vertical”. •  border value  ( int | tuple[int] ) – Value used in case of a constant border. •  interpolation  ( str ) – Same as in  mmcv.imshear() . \nReturns  The sheared masks. Return type Bitmap Masks\nto_ndarray () See  Base Instance Masks.to_ndarray() \nto_tensor ( dtype ,  device ) See  Base Instance Masks.to_tensor() . \ntranslate ( out_shape ,  offset ,  direction  $.=$  'horizontal' ,  fill_val  $\\mathbf{\\chi}_{=0}$  ,  interpolation  $=$  'bilinear' ) Translate the Bitmap Masks. \nParameters \n•  out_shape  ( tuple[int] ) – Shape for output mask, format (h, w). •  offset  ( int | float ) – The offset for translate. •  direction  ( str ) – The translate direction, either “horizontal” or “vertical”. •  fill_val  ( int | float ) – Border value. Default 0 for masks. •  interpolation  ( str ) – Same as  mmcv.im translate() . \nReturns  Translated Bitmap Masks. \nReturn type Bitmap Masks\nExample \n $>>$   from  mmdet.core.mask.structures  import  Bitmap Masks >>>  self  $=$   Bitmap Masks . random(dtype = np . uint8) >>>  out_shape  $=$   ( 32 ,  32 )  $>>$   offset  $\\c=4$   $>>$   direction  $=$   ' horizontal '  $>>$   fill_val  $\\mathbf{\\varepsilon}=\\mathbf{\\varepsilon}\\,\\Updownarrow$   $>>$   interpolation  $=$  ' bilinear '  $>>$   # Note, There seem to be issues when: >>>  #   $^*$   out_shape is different than self ' s shape >>>  # \\* the mask dtype is not supported by cv2.AffineWarp >>>  new    $=$   self . translate(out_shape, offset, direction, fill_val, >>> interpolation)  $>>$   assert  len (new)  $==$   len ( self ) >>>  assert  new . height, new . width  $==$   out_shape \nclass  mmdet.core.mask. Polygon Masks ( masks ,  height ,  width ) This class represents masks in the form of polygons. "}
{"page": 223, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_223.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nPolygons is a list of three levels. The first level of the list corresponds to objects, the second level to the polys\nthat compose the object, the third level to the poly coordinates\n\nParameters\n\n* masks (list [list [ndarray]]) — The first level of the list corresponds to objects, the\nsecond level to the polys that compose the object, the third level to the poly coordinates\n\n¢ height (int) — height of masks\n\n¢ width (int) — width of masks\n\nExample\n\n>>> from mmdet.core.mask.structures import * # NOQA\n\n>>> masks = [\n\n>>> [ np.array([0, 0, 10, 0, 10, 10., 0, 10, 0, 0]) J\n>>> J\n\n>>> height, width = 16, 16\n>>> self = PolygonMasks(masks, height, width)\n\n>>> # demo translate\n\n>>> new = self.translate((16, 16), 4., direction='horizontal')\n>>> assert np.all(new.masks[0][0][1::2] == masks[0][0][1::2])\n\n>>> assert np.all(new.masks[0][0][0::2] == masks[0][0][0::2] + 4)\n\n>>> # demo crop_and_resize\n\n>>> num_boxes = 3\n\n>>> bboxes = np.array([[0, 9, 30, 10.0]] * num_boxes)\n>>> out_shape = (16, 16)\n\n>>> inds = torch.randint(9, len(self), size=(num_boxes,))\n>>> device = 'cpu'\n\n>>> interpolation = 'bilinear'\n\n>>> new = self.crop_and_resize(\n\nwee bboxes, out_shape, inds, device, interpolation)\n>>> assert len(mew) == num_boxes\n\n>>> assert new.height, new.width == out_shape\n\nproperty areas\nCompute areas of masks.\n\nThis func is modified from detectron2. The function only works with Polygons using the shoelace formula.\nReturns areas of each instance\nReturn type ndarray\n\ncrop (bbox)\nsee BaseInstanceMasks.cropQ\n\ncrop_and_resize(bboxes, out_shape, inds, device='cpu', interpolation='bilinear', binarize=True)\nsee BaseInstanceMasks.crop_and_resize()\n\nexpand ( “args, **kwargs)\nTODO: Add expand for polygon\n\nflip (flip_direction='horizontal')\nsee BaseInstanceMasks. flipO\n\n216\n\nChapter 37. mmdet.core\n\n", "vlm_text": "Polygons is a list of three levels. The first level of the list corresponds to objects, the second level to the polys that compose the object, the third level to the poly coordinates \nParameters \n•  masks  ( list[list[ndarray]] ) – The first level of the list corresponds to objects, the second level to the polys that compose the object, the third level to the poly coordinates •  height  ( int ) – height of masks •  width  ( int ) – width of masks \nExample \nThe image shows a Python script using the `mmdet` library to demonstrate operations on polygon masks. It includes:\n\n1. **Import and Initialization:**\n   - Importing `PolygonMasks`.\n   - Creating a mask using a NumPy array.\n   - Setting height and width to 16.\n   - Initializing `PolygonMasks`.\n\n2. **Translate Operation:**\n   - Demonstrating translation by modifying the mask coordinates horizontally.\n   - Asserting conditions to verify the operation's correctness.\n\n3. **Crop and Resize Operation:**\n   - Setting the number of boxes to 3 and defining bounding boxes.\n   - Specifying output shape, indices, and device ('cpu').\n   - Using bilinear interpolation for resizing.\n   - Asserting that the new mask dimensions match the expected output.\nproperty areas \nCompute areas of masks. This func is modified from  detectron2 . The function only works with Polygons using the shoelace formula. \nReturns  areas of each instance \ncrop ( bbox ) see  Base Instance Masks.crop() \ncrop and resize ( bboxes ,  out_shape ,  inds ,  device  $=$  'cpu' ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'bilinear' ,  binarize  $\\mathbf{\\tilde{=}}$  True ) see  Base Instance Masks.crop and resize() \nexpand ( \\*args ,  \\*\\*kwargs ) TODO: Add expand for polygon \nflip ( flip direction  $=$  'horizontal' ) see  Base Instance Masks.flip() "}
{"page": 224, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_224.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\npad (out_shape, pad_val=0)\npadding has no effect on polygons\n\nclassmethod random (num_masks=3, height=32, width=32, n_verts=5, dtype=<class ‘numpy.float32'>,\nrng=None)\nGenerate random polygon masks for demo / testing purposes.\n\nAdapted fromP° !°° !\n\nReferences\n\nExample\n\n>>> from mmdet.core.mask.structures import PolygonMasks\n>>> self = PolygonMasks.random()\n>>> print('self = {}'.format(self))\n\nrescale(scale, interpolation=None)\nsee BaseInstanceMasks.rescale()\n\nresize (out_shape, interpolation=None)\nsee BaseInstanceMasks.resize()\n\nrotate (out_shape, angle, center=None, scale=1.0, fill_val=0)\nSee BaseInstanceMasks.rotate(Q.\n\nshear (out_shape, magnitude, direction='horizontal', border_value=0, interpolation='bilinear')\nSee BaseInstanceMasks.shear().\n\nto_bitmap()\nconvert polygon masks to bitmap masks.\n\nto_ndarray ()\nConvert masks to the format of ndarray.\n\nto_tensor (dtype, device)\nSee BaseInstanceMasks.to_tensor().\n\ntranslate (out_shape, offset, direction='horizontal', fill_val=None, interpolation=None)\nTranslate the PolygonMasks.\n\nExample\n\n>>> self = PolygonMasks.random(dtype=np. int)\n\n>>> out_shape = (self.height, self.width)\n\n>>> new = self.translate(out_shape, 4., direction='horizontal')\n\n>>> assert np.all(new.masks[0][0][1::2] == self.masks[0][0][1::2])\n\n>>> assert np.all(new.masks[0][0][0::2] == self.masks[0][0][0::2] + 4) # noga:.\nE501\n\nmmdet.core.mask.encode_mask_results (mask_results)\nEncode bitmap mask to RLE code.\n\nParameters mask_results (list | tuple[list])-bitmap mask results. In mask scoring renn,\nmask_results is a tuple of (segm_results, segm_cls_score).\n\nReturns RLE encoded mask.\n\n37.4. mask 217\n", "vlm_text": "pad ( out_shape ,  pad_va  $\\mathbf{\\chi}_{}^{}$  ) padding has no effect on polygons\\` \nclass method random ( num_mask  $\\backprime{=}3$  ,  heigh  $t{=}32$  ,  width  $\\iota{=}32$  ,  n_vert  $\\wp{=}5$  ,  dtype  $<<$  class 'numpy.float32'> ,  $\\scriptstyle{r n g=N o n e})$  Generate random polygon masks for demo / testing purposes. Adapted from \nReferences \nExample \n $>>$   from  mmdet.core.mask.structures  import  Polygon Masks >>>  self  $=$   Polygon Masks . random() >>>  print ( ' self =  {} ' . format( self )) \nrescale ( scale ,  interpolation  $\\scriptstyle\\varepsilon$  None ) see  Base Instance Masks.rescale() \nresize ( out_shape ,  interpolation  $\\scriptstyle.\\equiv$  None ) see  Base Instance Masks.resize() \nrotate ( out_shape ,  angle ,  center  $\\leftrightharpoons$  None ,  scale  $\\mathbf{\\tilde{=}}$  1.0 ,  fill_va  $l{=}0.$  ) See  Base Instance Masks.rotate() . \nshear ( out_shape ,  magnitude ,  direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  border value  $\\mathrm{=}0$  ,  interpolation  $\\mathbf{\\beta}=$  'bilinear' ) See  Base Instance Masks.shear() . \nto_bitmap () convert polygon masks to bitmap masks. \nto_ndarray () Convert masks to the format of ndarray. \nto_tensor ( dtype ,  device ) See  Base Instance Masks.to_tensor() . \ntranslate ( out_shape ,  offset ,  direction  $.=$  'horizontal' ,  fill_val  $\\leftrightharpoons$  None ,  interpolation  $\\scriptstyle.\\equiv$  None ) Translate the Polygon Masks. \nExample \n>>>  self  $=$   Polygon Masks . random(dtype = np . int)  $>>$   out_shape  $=$   ( self . height,  self . width)  $>>$   new  $=$   self . translate(out_shape,  4. , direction  $\\c=$  ' horizontal ' )  $>>$   assert  np . all(new . masks[ 0 ][ 0 ][ 1 :: 2 ]  $==$   self . masks[ 0 ][ 0 ][ 1 :: 2 ]) >>>  assert  np . all(new . masks[ 0 ][ 0 ][ 0 :: 2 ]  $==$   self . masks[ 0 ][ 0 ][ 0 :: 2 ]  +  4 ) # noqa: ␣  $\\substack{\\mathrm{\\Large~\\hookrightarrow\\,}E501}$  \nmmdet.core.mask. encode mask results ( mask results ) Encode bitmap mask to RLE code. \nParameters  mask results  ( list | tuple[list] ) – bitmap mask results. In mask scoring rcnn, mask results is a tuple of (seg m results, seg m cls score). \nReturns  RLE encoded mask. "}
{"page": 225, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_225.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmdet .core.mask.mask_target (pos_proposals_list, pos_assigned_gt_inds_list, gt_masks_list, cfg)\n\nReturn type list | tuple\n\nCompute mask target for positive proposals in multiple images.\n\nParameters\n\n* pos_proposals_list (list [Tensor]) — Positive proposals in multiple images.\n\n* pos_assigned_gt_inds_list (list [Tensor]) — Assigned GT indices for each positive\n\nproposals.\n\n* gt_masks_list (list{BaseInstanceMasks]) — Ground truth masks of each image.\n\n* cfg (dict) — Config dict that specifies the mask size.\nReturns Mask target of each image.\n\nReturn type list[Tensor]\n\nExample\n\n>>> import mmcv\n\n>>> import mmdet\n\n>>> from mmdet.core.mask import BitmapMasks\n\n>>> from mmdet.core.mask.mask_target import *\n\n>>> H, W= 17, 18\n\n>>> cfg = mmcv.Config({'mask_size': (13, 14)})\n\n>>> rng = np.random.RandomState(0)\n\n>>> # Positive proposals (tl_x, tl_y, br_x, br_y) for each image\n>>> pos_proposals_list = [\n\n>>> torch. Tensor ([\n\n>> [ 7.2425, 5.5929, 13.9414, 14.9541],\n\n>>> [ 7.3241, 3.6170, 16.3850, 15.3102],\n\n>>> )),\n\n>>> torch. Tensor ([\n\n>>> [ 4.8448, 6.4010, 7.0314, 9.7681],\n\n>>> [ 5.9790, 2.6989, 7.4416, 4.8580],\n\n>>> [ 9.0000, 0.0000, 0.1398, 9.8232],\n\n>>> )),\n\n>>> J\n\n>>> # Corresponding class index for each proposal for each image\n>>> pos_assigned_gt_inds_list = [\n\n>>> torch.LongTensor([7, 9]),\n\n>>> torch.LongTensor([5, 4, 1]),\n\n>>> J\n\n>>> # Ground truth mask for each true object for each image\n>>> gt_masks_list = [\n\n>>> BitmapMasks(rng.rand(8, H, W), height=H, width=W),\n>>> BitmapMasks(rng.rand(6, H, W), height=H, width=W),\n>>> J\n\n>>> mask_targets = mask_target(\n\n>>> pos_proposals_list, pos_assigned_gt_inds_list,\n>>> gt_masks_list, cfg)\n\n>>> assert mask_targets.shape == (5,) + cfg['mask_size']\n\n218\n\nChapter 37.\n\nmmdet.core\n\n", "vlm_text": "Return type  list | tuple \nmmdet.core.mask. mask target ( pos proposals list ,  pos assigned gt in ds list ,  gt masks list ,  cfg ) Compute mask target for positive proposals in multiple images. \nParameters \n•  pos proposals list  ( list[Tensor] ) – Positive proposals in multiple images. •  pos assigned gt in ds list  ( list[Tensor] ) – Assigned GT indices for each positive proposals. •  gt masks list  (list[ Base Instance Masks ]) – Ground truth masks of each image. •  cfg  ( dict ) – Config dict that specifies the mask size. \nReturns  Mask target of each image. \nReturn type  list[Tensor] \nExample \nThe image contains a Python script involving image processing and mask creation for object detection using libraries such as `mmcv` and `mmdet`. Here's a breakdown of the code:\n\n1. **Imports:**\n   - Various modules from `mmcv` and `mmdet` for handling configuration and masks.\n\n2. **Configuration:**\n   - Sets image dimensions `H` and `W` to 17 and 18, respectively.\n   - Defines a configuration `cfg` with `mask_size` set to `(13, 14)`.\n\n3. **Random State:**\n   - Initializes a random state using `np.random.RandomState(0)`.\n\n4. **Positive Proposals:**\n   - Defines lists of tensors representing positive proposals with coordinates `(tL_x, tL_y, br_x, br_y)`.\n\n5. **Class Index Assignment:**\n   - Maps proposals to corresponding class indices in `pos_assigned_gt_inds_list`.\n\n6. **Ground Truth Masks:**\n   - Creates a list of `BitmapMasks` representing ground truth masks for each object in the images.\n   - The masks use random values of shape `(H, W)`.\n\n7. **Mask Targeting:**\n   - Computes `mask_targets` using the `mask_target` function with proposals, indices, ground truth masks, and configuration.\n\n8. **Assertion:**\n   - Asserts that the shape of `mask_targets` is `(5,) + `mask_size`, verifying the expected dimensions.\n\nThis script is likely part of a larger system for training or evaluating visual recognition models."}
{"page": 226, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_226.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmdet.core.mask.split_combined_polys (polys, poly_lens, polys_per_mask)\nSplit the combined 1-D polys into masks.\n\nA mask is represented as a list of polys, and a poly is represented as a 1-D array. In dataset, all masks are\nconcatenated into a single 1-D tensor. Here we need to split the tensor into original representations.\n\nParameters\n* polys (list) —a list (length = image num) of 1-D tensors\n* poly_lens (list) —a list (length = image num) of poly length\n* polys_per_mask (list) —a list (length = image num) of poly number of each mask\n\nReturns a list (length = image num) of list (length = mask num) of list (length = poly num) of numpy\narray.\n\nReturn type list\n\n37.5 evaluation\n\nclass mmdet.core.evaluation.DistEvalHook (dataloader, start=None, interval=1, by_epoch=True,\nsave_best=None, rule=None, test_fn=None,\ngreater_keys=None, less_keys=None,\nbroadcast_bn_buffer=True, tmpdir=None, gpu_collect=False,\nout_dir=None, file_client_args=None, **eval_kwargs)\n\nclass mmdet.core.evaluation.EvalHook (dataloader, start=None, interval=1, by_epoch=True,\nsave_best=None, rule=None, test_fn=None, greater_keys=None,\nless_keys=None, out_dir=None, file_client_args=None,\n**eval_kwargs)\n\nmmdet.core.evaluation.average_precision(recalls, precisions, mode='area')\nCalculate average precision (for single or multiple scales).\n\nParameters\n* recalls (ndarray) — shape (num_scales, num_dets) or (num_dets, )\n* precisions (ndarray) — shape (num_scales, num_dets) or (num_dets, )\n\n* mode (str) — ‘area’ or ‘1 1points’, ‘area’ means calculating the area under precision-recall\ncurve, ‘1 1points’ means calculating the average precision of recalls at [0, 0.1, ..., 1]\n\nReturns calculated average precision\nReturn type float or ndarray\n\nmmdet.core.evaluation.eval_map (det_results, annotations, scale_ranges=None, iou_thr=0.5, dataset=None,\nlogger=None, tpfp_fn=None, nproc=4, use_legacy_coordinate=False)\nEvaluate mAP of a dataset.\n\nParameters\n\n¢ det_results (list [list]) — [[cls1_det, cls2_det, ...], ...]. The outer list indicates im-\nages, and the inner list indicates per-class detected bboxes.\n\n* annotations (list [dict]) — Ground truth annotations where each item of the list indi-\ncates an image. Keys of annotations are:\n\n— bboxes: numpy array of shape (n, 4)\n\n— labels: numpy array of shape (n, )\n\n37.5. evaluation 219\n", "vlm_text": "mmdet.core.mask. split combined poly s ( polys ,  poly_lens ,  poly s per mask ) Split the combined 1-D polys into masks. \nA mask is represented as a list of polys, and a poly is represented as a 1-D array. In dataset, all masks are concatenated into a single 1-D tensor. Here we need to split the tensor into original representations. \nParameters \n•  polys  ( list ) – a list (length  $=$  image num) of 1-D tensors •  poly_lens    $(I i s t)-\\mathbf{a}$   list (length  $=$   image num) of poly length •  poly s per mask  ( list ) – a list (length  $=$   image num) of poly number of each mask \nReturns  a list (length  $=$   image num) of list (length  $=$   mask num) of list (length  $=$   poly num) of numpy array. \nReturn type  list \n37.5 evaluation \nclass  mmdet.core.evaluation. Di stEv al Hook ( dataloader ,  start=None ,  interval=1 ,  by_epoch=True , save_bes  $t{=}$  None ,  rule=None ,  test_fn=None , greater keys  $\\mathbf{\\check{\\Sigma}}$  None ,  less_keys=None , broadcast bn buffer=True ,  tmpdir  $\\leftrightharpoons$  None ,  gpu collect=False , out_dir  $\\leftrightharpoons$  None ,  file client arg s=None ,  \\*\\*e val k war gs ) \nclass  mmdet.core.evaluation. EvalHook ( dataloader ,  start  $=$  None ,  interval=1 ,  by_epoch=True , save_best  $=$  None ,  rule  $=$  None ,  test_fn=None ,  greater keys=None , less_keys  $\\mathbf{\\hat{\\rho}}$  None ,  out_dir=None ,  file client arg s=None , \\*\\*e val k war gs)\nmmdet.core.evaluation. average precision ( recalls ,  precisions ,  mode  $=$  'area' ) Calculate average precision (for single or multiple scales). \nParameters \n•  recalls  ( ndarray ) – shape (num_scales, num_dets) or (num_dets, ) •  precisions  ( ndarray ) – shape (num_scales, num_dets) or (num_dets, ) •  mode  ( str ) – ‘area’ or ‘11points’, ‘area’ means calculating the area under precision-recall curve, ‘11points’ means calculating the average precision of recalls at [0, 0.1, ..., 1] \nReturns  calculated average precision \nReturn type  float or ndarray \nmmdet.core.evaluation. eval_map ( det results ,  annotations ,  scale ranges  $=$  None ,  iou_th  $\\scriptstyle{r=0.5}$  ,  dataset  $\\fallingdotseq$  None , logger  $=$  None ,  tpfp_fn  $\\scriptstyle\\cdot\\equiv$  None ,  nproc  $\\scriptstyle\\bullet=4$  ,  use legacy coordinate  $\\mathbf{\\beta}=$  False ) \nEvaluate mAP of a dataset. \nParameters \n•  det results  ( list[list] ) – [[cls1_det, cls2_det, ...], ...]. The outer list indicates im- ages, and the inner list indicates per-class detected bboxes. •  annotations  ( list[dict] ) – Ground truth annotations where each item of the list indi- cates an image. Keys of annotations are: –  bboxes : numpy array of shape (n, 4) –  labels : numpy array of shape (n, ) "}
{"page": 227, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_227.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n— bboxes_ignore (optional): numpy array of shape (k, 4)\n— labels_ignore (optional): numpy array of shape (k, )\n\n* scale_ranges (list[tuple] | None) — Range of scales to be evaluated, in the format\n[(min1, max1), (min2, max2), ...]. A range of (32, 64) means the area range between (32**2,\n64**2). Default: None.\n\n¢ iou_thr (float) —IoU threshold to be considered as matched. Default: 0.5.\n\n¢ dataset (list[str] | str | None) — Dataset name or dataset classes, there are minor\ndifferences in metrics for different datasets, e.g. “voc07”, “imagenet_det”, etc. Default:\nNone.\n\n* logger (logging.Logger | str | None) — The way to print the mAP summary. See\nmmcv.utils.print_log() for details. Default: None.\n\n* tpfp_fn (callable | None) — The function used to determine true/ false posi-\ntives. If None, tpfp_default() is used as default unless dataset is ‘det’ or ‘vid’\n(tpfp_imagenet() in this case). If it is given as a function, then this function is used\nto evaluate tp & fp. Default None.\n\n* nproc (int) — Processes used for computing TP and FP. Default: 4.\n\n* use_legacy_coordinate (bool) — Whether to use coordinate system in mmdet v1.x.\nwhich means width, height should be calculated as ‘x2 - x1 + 1° and ‘y2 - y1 + 1’ respectively.\nDefault: False.\n\nReturns (mAP, [dict, dict, ...])\nReturn type tuple\n\nmmdet.core.evaluation.eval_recalls (gts, proposals, proposal_nums=None, iou_thrs=0.5, logger=None,\nuse_legacy_coordinate=False)\nCalculate recalls.\n\nParameters\n* gts (list [ndarray]) — a list of arrays of shape (n, 4)\n* proposals (list [ndarray]) — a list of arrays of shape (k, 4) or (k, 5)\n* proposal_nums (int | Sequence[int]) - Top N proposals to be evaluated.\n¢ iou_thrs (float | Sequence[float]) —IoU thresholds. Default: 0.5.\n\n* logger (logging.Logger | str | None) — The way to print the recall summary. See\nmmcv.utils.print_log() for details. Default: None.\n\n* use_legacy_coordinate (bool) — Whether use coordinate system in mmdet v1.x. “1”\nwas added to both height and width which means w, h should be computed as ‘x2 - x1 + 1\nand ‘y2 - yl + 1’. Default: False.\n\nReturns recalls of different ious and proposal nums\nReturn type ndarray\n\nmmdet.core.evaluation.get_classes (dataset)\nGet class names of a dataset.\n\nmmdet.core.evaluation.plot_iou_recal] (recalls, iou_thrs)\nPlot loU-Recalls curve.\n\nParameters\n\n* recalls (ndarray or list) —shape (k,)\n\n220 Chapter 37. mmdet.core\n", "vlm_text": "–  b boxes ignore  (optional): numpy array of shape (k, 4) –  labels ignore  (optional): numpy array of shape (k, ) •  scale ranges  ( list[tuple] | None ) – Range of scales to be evaluated, in the format [(min1, max1), (min2, max2), ...]. A range of (32, 64) means the area range between   $(32^{**}2$  ,  $64^{**}2_{c}$  ). Default: None. •  iou_thr  ( float ) – IoU threshold to be considered as matched. Default: 0.5. •  dataset  ( list[str] | str | None ) – Dataset name or dataset classes, there are minor differences in metrics for different datasets, e.g. “voc07”, “image net det”, etc. Default: None. •  logger  ( logging.Logger | str | None ) – The way to print the mAP summary. See mmcv.utils.print_log()  for details. Default: None. •  tpfp_fn  ( callable | None ) – The function used to determine true/ false posi- tives. If None,  tp fp default()  is used as default unless dataset is ‘det’ or ‘vid’ ( tp fp image net()  in this case). If it is given as a function, then this function is used to evaluate tp & fp. Default None. •  nproc  ( int ) – Processes used for computing TP and FP. Default: 4. •  use legacy coordinate  ( bool ) – Whether to use coordinate system in mmdet v1.x. which means width, height should be calculated as   $\\mathbf{\\dot{x}}2-\\mathbf{x}1+1\\mathbf{\\hat{\\$   and   $\\mathbf{\\dot{y}}2-\\mathbf{y}1+1^{\\prime}$  ’ respectively. Default: False. \nReturns  (mAP, [dict, dict, ...]) \nReturn type  tuple \nmmdet.core.evaluation. e val recalls ( gts ,  proposals ,  proposal num s  $\\leftrightharpoons$  None ,  iou_thrs  $\\mathrm{\\Sigma=}0.5$  ,  logger=None , use legacy coordinate  $\\mathbf{\\beta}=$  False ) \nCalculate recalls. \nParameters \n•  gts  ( list[ndarray] ) – a list of arrays of shape (n, 4) •  proposals  ( list[ndarray] ) – a list of arrays of shape (k, 4) or (k, 5) •  proposal num s  ( int | Sequence[int] ) – Top N proposals to be evaluated. •  iou_thrs  ( float | Sequence[float] ) – IoU thresholds. Default: 0.5. •  logger  ( logging.Logger | str   $I$   None ) – The way to print the recall summary. See mmcv.utils.print_log()  for details. Default: None. •  use legacy coordinate  ( bool ) – Whether use coordinate system in mmdet v1.x. “1” was added to both height and width which means w, h should be computed as   $\\phantom{0}\\phantom{0}\\cdot\\phantom{0}\\times1+1\\phantom{0}\\phantom{0}\\cdot$  and   $\\mathbf{\\dot{y}}2\\mathbf{\\dot{-}y}1+1^{\\mathbf{\\dot{r}}}$  ’. Default: False. \nReturns  recalls of different ious and proposal nums \nReturn type  ndarray \nmmdet.core.evaluation. get classes ( dataset ) Get class names of a dataset. \nmmdet.core.evaluation. plot i ou recall ( recalls ,  iou_thrs ) Plot IoU-Recalls curve. \nParameters \n•  recalls  ( ndarray or list ) – shape (k,) "}
{"page": 228, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_228.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* iou_thrs (ndarray or list) -—same shape as recalls\n\nmmdet.core.evaluation.plot_num_recal] (recalls, proposal_nums)\nPlot Proposal_num-Recalls curve.\n\nParameters\n* recalls (ndarray or list) —shape (k,)\n* proposal_nums (ndarray or list) -—same shape as recalls\n\nmmndet.core.evaluation.print_map_summary (mean_ap, results, dataset=None, scale_ranges=None,\nlogger=None)\nPrint mAP and results of each class.\n\nA table will be printed to show the gts/dets/recall/AP of each class and the mAP.\nParameters\n* mean_ap (float) — Calculated from eval_map().\n* results (list [dict]) — Calculated from eval_map().\n¢ dataset (list[str] | str | None) — Dataset name or dataset classes.\n* scale_ranges (list[tuple] | None) — Range of scales to be evaluated.\n\n* logger (logging.Logger | str | None) — The way to print the mAP summary. See\nmmcv.utils.print_log() for details. Default: None.\n\nmmdet.core.evaluation.print_recall_summary (recalls, proposal_nums, iou_thrs, row_idxs=None,\ncol_idxs=None, logger=None)\nPrint recalls in a table.\n\nParameters\n¢ recalls (ndarray) — calculated from bbox_recalls\n* proposal_nums (ndarray or list) -top N proposals\n¢ iou_thrs (ndarray or list) —iou thresholds\n* row_idxs (ndarray) — which rows(proposal nums) to print\n* col_idxs (ndarray) — which cols(iou thresholds) to print\n\n* logger (logging.Logger | str | None) — The way to print the recall summary. See\nmmcv.utils.print_log() for details. Default: None.\n\n37.6 post_processing\n\nmmdet.core.post_processing. fast_nms (multi_bboxes, multi_scores, multi_coeffs, score_thr, iou_thr, top_k,\nmax_num=- 1)\nFast NMS in YOLACT.\n\nFast NMS allows already-removed detections to suppress other detections so that every instance can be decided to\nbe kept or discarded in parallel, which is not possible in traditional NMS. This relaxation allows us to implement\nFast NMS entirely in standard GPU-accelerated matrix operations.\n\nParameters\n\n* multi_bboxes (Tensor) — shape (n, #class*4) or (n, 4)\n\n37.6. post_processing 221\n", "vlm_text": "•  iou_thrs  ( ndarray or list ) – same shape as  recalls \nmmdet.core.evaluation. plot num recall ( recalls ,  proposal num s ) Plot Proposal num-Recalls curve. \nParameters \n•  recalls  ( ndarray or list ) – shape (k,) •  proposal num s  ( ndarray or list ) – same shape as  recalls \nmmdet.core.evaluation. print map summary ( mean_ap ,  results ,  datase  $\\fallingdotseq$  None ,  scale ranges  $=$  None , logger=None ) \nPrint mAP and results of each class. \nA table will be printed to show the gts/dets/recall/AP of each class and the mAP. \nParameters \n•  mean_ap  ( float ) – Calculated from  eval_map() . •  results  ( list[dict] ) – Calculated from  eval_map() . •  dataset  ( list[str] | str | None ) – Dataset name or dataset classes. •  scale ranges  ( list[tuple] | None ) – Range of scales to be evaluated. •  logger  ( logging.Logger | str | None ) – The way to print the mAP summary. See mmcv.utils.print_log()  for details. Default: None. \nmmdet.core.evaluation. print recall summary ( recalls ,  proposal num s ,  iou_thrs ,  row_idxs  $:=$  None , col_idxs  $=$  None ,  logger=None ) Print recalls in a table. \nParameters \n•  recalls  ( ndarray ) – calculated from  b box recalls •  proposal num s  ( ndarray or list ) – top N proposals •  iou_thrs  ( ndarray or list ) – iou thresholds •  row_idxs  ( ndarray ) – which rows(proposal nums) to print •  col_idxs  ( ndarray ) – which cols(iou thresholds) to print •  logger  ( logging.Logger | str | None ) – The way to print the recall summary. See mmcv.utils.print_log()  for details. Default: None. \n37.6 post processing \nmmdet.core.post processing. fast_nms ( multi b boxes ,  multi scores ,  multi coe ffs ,  score_thr ,  iou_thr ,  top_k , max_num  $\\scriptstyle-I$  ) \nFast NMS in YOLACT.\nFast NMS allows already-removed detections to suppress other detections so that every instance can be decided to be kept or discarded in parallel, which is not possible in traditional NMS. This relaxation allows us to implement Fast NMS entirely in standard GPU-accelerated matrix operations. \nParameters \n•  multi b boxes  ( Tensor ) – shape (n, #class\\*4) or (n, 4) "}
{"page": 229, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_229.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* multi_scores (Tensor) — shape (n, #class+1), where the last column contains scores of\nthe background class, but this will be ignored.\n\n* multi_coeffs (Tensor) — shape (n, #class*coeffs_dim).\n\nscore_thr (float) — bbox threshold, bboxes with scores lower than it will not be consid-\nered.\n\niou_thr (float) — IoU threshold to be considered as conflicted.\n* top_k (int) — if there are more than top_k bboxes before NMS, only top top_k will be kept.\n\n* max_num (int) — if there are more than max_num bboxes after NMS, only top max_num\nwill be kept. If -1, keep all the bboxes. Default: -1.\n\nReturns\n\n(dets, labels, coefficients), tensors of shape (k, 5), (k, 1), and (k, coeffs_dim). Dets are boxes\nwith scores. Labels are 0-based.\n\nReturn type tuple\n\nmmdet .core.post_processing.mask_matrix_nms (masks, labels, scores, filter_thr=- 1, nms_pre=- 1,\nmax_num=- 1, kernel='gaussian', sigma=2.0,\nmask_area=None)\n\nMatrix NMS for multi-class masks.\n\nParameters\n* masks (Tensor) — Has shape (num_instances, h, w)\n¢ labels (Tensor) — Labels of corresponding masks, has shape (num_instances,).\n\n* scores (Tensor) — Mask scores of corresponding masks, has shape (num_instances).\n\nfilter_thr (float) — Score threshold to filter the masks after matrix nms. Default: -1,\nwhich means do not use filter_thr.\n\n* nms_pre (int) — The max number of instances to do the matrix nms. Default: -1, which\nmeans do not use nms_pre.\n\n* max_num(int, optional) -—If there are more than max_num masks after matrix, only top\nmax_num will be kept. Default: -1, which means do not use max_num.\n\n¢ kernel (str) — ‘linear’ or ‘gaussian’.\n\n* sigma (float) — std in gaussian method.\n\n* mask_area (Tensor) — The sum of seg_masks.\nReturns\n\nProcessed mask results.\n\n* scores (Tensor): Updated scores, has shape (n,).\n\n* labels (Tensor): Remained labels, has shape (n,).\n\n* masks (Tensor): Remained masks, has shape (n, w, h).\n\n* keep_inds (Tensor): The indices number of the remaining mask in the input mask, has\nshape (n,).\n\nReturn type tuple(Tensor)\n\nmmdet .core.post_processing.merge_aug_bboxes (aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\nMerge augmented detection bboxes and scores.\n\n222 Chapter 37. mmdet.core\n", "vlm_text": "•  multi scores  ( Tensor ) – shape (n, #class  $_{;+1}$  ), where the last column contains scores of the background class, but this will be ignored. •  multi coe ffs  ( Tensor ) – shape (n, #class\\*coeffs_dim). •  score_thr  ( float ) – bbox threshold, bboxes with scores lower than it will not be consid- ered. •  iou_thr  ( float ) – IoU threshold to be considered as conflicted. •  top_k  ( int ) – if there are more than top_k bboxes before NMS, only top top_k will be kept. •  max_num  ( int ) – if there are more than max_num bboxes after NMS, only top max_num will be kept. If -1, keep all the bboxes. Default: -1. \nReturns \n(dets, labels, coefficients), tensors of shape  $(\\mathbf{k},\\mathsf{5}),(\\mathbf{k},\\mathbf{1})$  ,  and (k, coeffs_dim). Dets are boxes with scores. Labels are 0-based. \nReturn type  tuple \nmmdet.core.post processing. mask matrix nm s ( masks ,  labels ,  scores ,  filter_th  $r{=}{-}\\ I$  ,  nms_pre=- 1 , max_num=- 1 ,  kernel  $'='$  gaussian' ,  sigma  $=\\!2.0$  , mask_area  $=$  None ) \nMatrix NMS for multi-class masks. \nParameters \n•  masks  ( Tensor ) – Has shape (num instances, h, w) •  labels  ( Tensor ) – Labels of corresponding masks, has shape (num instances,). •  scores  ( Tensor ) – Mask scores of corresponding masks, has shape (num instances). •  filter_thr  ( float ) – Score threshold to filter the masks after matrix nms. Default: -1, which means do not use filter_thr. •  nms_pre  ( int ) – The max number of instances to do the matrix nms. Default: -1, which means do not use nms_pre. •  max_num  ( int, optional ) – If there are more than max_num masks after matrix, only top max_num will be kept. Default:  $^{-1}$  , which means do not use max_num. •  kernel  ( str ) – ‘linear’ or ‘gaussian’. •  sigma  ( float ) – std in gaussian method. •  mask_area  ( Tensor ) – The sum of seg_masks. \nReturns \nProcessed mask results. \n• scores (Tensor): Updated scores, has shape (n,). • labels (Tensor): Remained labels, has shape (n,). • masks (Tensor): Remained masks, has shape (n, w, h). •  keep_inds (Tensor): The indices number of  the remaining mask in the input mask, has shape (n,). \nReturn type  tuple(Tensor) \nmmdet.core.post processing. merge aug b boxes ( aug_bboxes ,  aug_scores ,  img_metas ,  r cnn test cf g ) Merge augmented detection bboxes and scores. "}
{"page": 230, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_230.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n* aug_bboxes (list [Tensor ]) — shape (n, 4*#class)\n* aug_scores (list[Tensor] or None) — shape (n, #class)\n¢ img_shapes (list [Tensor ]) — shape (3, ).\n* renn_test_cfg (dict) — renn test config.\nReturns (bboxes, scores)\nReturn type tuple\n\nmmdet .core.post_processing.merge_aug_masks (aug_masks, img_metas, rcnn_test_cfg, weights=None)\nMerge augmented mask prediction.\n\nParameters\n* aug_masks (list [ndarray]) — shape (n, #class, h, w)\n¢ img_shapes (list [ndarray]) — shape (3, ).\n* renn_test_cfg (dict) — renn test config.\n\nReturns (bboxes, scores)\n\nReturn type tuple\n\nmmdet .core.post_processing.merge_aug_proposals (aug_proposals, img_metas, cfg)\nMerge augmented proposals (multiscale, flip, etc.)\n\nParameters\n\n* aug_proposals (list [Tensor ]) — proposals from different testing schemes, shape (n, 5).\nNote that they are not rescaled to the original image size.\n\n¢ img_metas (list[dict]) — list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img _norm_cfg’. For details on the values of these keys see\n\nmmdet/datasets/pipelines/formatting.py: Collect.\n* cfg (dict) — rpn test config.\nReturns shape (n, 4), proposals corresponding to original image scale.\nReturn type Tensor\n\nmmdet .core.post_processing.merge_aug_scores (aug_scores)\nMerge augmented bbox scores.\n\nmmdet.core.post_processing.multiclass_nms (multi_bboxes, multi_scores, score_thr, nms_cfg, max_num=-\n1, score_factors=None, return_inds=False)\nNMS for multi-class bboxes.\n\nParameters\n* multi_bboxes (Tensor) — shape (n, #class*4) or (n, 4)\n\n* multi_scores (Tensor) — shape (n, #class), where the last column contains scores of the\nbackground class, but this will be ignored.\n\nscore_thr (float) — bbox threshold, bboxes with scores lower than it will not be consid-\nered.\n\n¢ nms_thr (float) — NMS IoU threshold\n\n37.6. post_processing 223\n", "vlm_text": "Parameters \n•  aug_bboxes  ( list[Tensor] ) – shape (n, 4\\*#class) •  aug_scores  ( list[Tensor] or None ) – shape (n, #class) •  img_shapes  ( list[Tensor] ) – shape (3, ). •  r cnn test cf g  ( dict ) – rcnn test config. \nReturns  (bboxes, scores) \nReturn type  tuple \nmmdet.core.post processing. merge aug masks ( aug_masks ,  img_metas ,  r cnn test cf g ,  weights  $\\mathbf{\\hat{\\rho}}$  None ) Merge augmented mask prediction. \nParameters \n•  aug_masks  ( list[ndarray] ) – shape (n, #class, h, w) •  img_shapes  ( list[ndarray] ) – shape (3, ). •  r cnn test cf g  ( dict ) – rcnn test config. \nReturns  (bboxes, scores) \nReturn type  tuple \nmmdet.core.post processing. merge aug proposals ( aug proposals ,  img_metas ,  cfg ) Merge augmented proposals (multiscale, flip, etc.) \nParameters \n•  aug proposals  ( list[Tensor] ) – proposals from different testing schemes, shape (n, 5). Note that they are not rescaled to the original image size. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  cfg  ( dict ) – rpn test config. \nReturns  shape (n, 4), proposals corresponding to original image scale. \nReturn type  Tensor \nmmdet.core.post processing. merge aug scores ( aug_scores ) Merge augmented bbox scores. \nmmdet.core.post processing. multi class nm s ( multi b boxes ,  multi scores ,  score_thr ,  nms_cfg ,  max_num=- 1 ,  score factors  $\\mathbf{\\hat{\\Sigma}}$  None ,  return in ds  $\\mathbf{:=}$  False ) NMS for multi-class bboxes. \nParameters \n•  multi b boxes  ( Tensor ) – shape (n, #class\\*4) or (n, 4) •  multi scores  ( Tensor ) – shape (n, #class), where the last column contains scores of the background class, but this will be ignored. •  score_thr  ( float ) – bbox threshold, bboxes with scores lower than it will not be consid- ered. •  nms_thr  ( float ) – NMS IoU threshold "}
{"page": 231, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_231.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* max_num (int, optional) — if there are more than max_num bboxes after NMS, only top\nmax_num will be kept. Default to -1.\n\n* score_factors (Tensor, optional) — The factors multiplied to scores before applying\nNMS. Default to None.\n\n* return_inds (bool, optional) — Whether return the indices of kept bboxes. Default to\nFalse.\n\nReturns\n\n(dets, labels, indices (optional)), tensors of shape (k, 5), (k), and (k). Dets are boxes with\nscores. Labels are 0-based.\n\nReturn type tuple\n\n37.7 utils\n\nclass mmdet.core.utils.DistOptimizerHook(*args, **kwargs)\nDeprecated optimizer hook for distributed training.\n\nmmdet.core.utils.all_reduce_dict(py_dict, op='sum', group=None, to_float=True)\nApply all reduce function for python dict object.\n\nThe code is modified from https://github.com/Megvii- BaseDetection/YOLOX/blob/main/yolox/utils/allreduce_norm.py.\nNOTE: make sure that py_dict in different ranks has the same keys and the values should be in the same shape.\nParameters\n* py_dict (dict) — Dict to be applied all reduce op.\n* op (str) — Operator, could be ‘sum’ or ‘mean’. Default: ‘sum’\n* group (torch.distributed. group, optional) — Distributed group, Default: None.\n* to_float (bool) — Whether to convert all values of dict to float. Default: True.\nReturns reduced python dict object.\nReturn type OrderedDict\n\nmmdet.core.utils.allreduce_grads (params, coalesce=True, bucket_size_mb=- 1)\nAllreduce gradients.\n\nParameters\n* params (list [torch.Parameters]) — List of parameters of a model\n* coalesce (bool, optional)—Whether allreduce parameters as a whole. Defaults to True.\n\n¢ bucket_size_mb (int, optional) — Size of bucket, the unit is MB. Defaults to -1.\n\nmmdet.core.utils.center_of_mass (mask, esp=le-06)\nCalculate the centroid coordinates of the mask.\n\nParameters\n* mask (Tensor) — The mask to be calculated, shape (h, w).\n* esp (float) — Avoid dividing by zero. Default: le-6.\nReturns\n\nthe coordinates of the center point of the mask.\n\n224 Chapter 37. mmdet.core\n", "vlm_text": "•  max_num  ( int, optional ) – if there are more than max_num bboxes after NMS, only top max_num will be kept. Default to -1. •  score factors  ( Tensor, optional ) – The factors multiplied to scores before applying NMS. Default to None. •  return in ds  ( bool, optional ) – Whether return the indices of kept bboxes. Default to False. \nReturns \n(dets, labels, indices (optional)), tensors of shape   $({\\bf k},{\\bf5}),~({\\bf k})$  , and   $(\\mathrm{k})$  . Dets are boxes with scores. Labels are 0-based. \nReturn type  tuple \n37.7 utils \nclass  mmdet.core.utils. Dist Optimizer Hook ( \\*args ,  \\*\\*kwargs ) Deprecated optimizer hook for distributed training. \nmmdet.core.utils. all reduce dic t ( py_dict ,  $\\scriptstyle o p='s u m`$  ,  group  $=$  None ,  to_float=True ) Apply all reduce function for python dict object. The code is modified from  https://github.com/Megvii - Base Detection/YOLOX/blob/main/yolox/utils/all reduce norm.py. NOTE: make sure that py_dict in different ranks has the same keys and the values should be in the same shape. \nParameters \n•  py_dict  ( dict ) – Dict to be applied all reduce op. •  op  ( str ) – Operator, could be ‘sum’ or ‘mean’. Default: ‘sum’ •  group  ( torch.distributed.group , optional) – Distributed group, Default: None. •  to_float  ( bool ) – Whether to convert all values of dict to float. Default: True. \n\nReturn type  Ordered Dic t \nmmdet.core.utils. all reduce grads ( params ,  coalesce  $\\mathbf{=}$  True ,  bucket size mb=- 1 ) Allreduce gradients. \nParameters \n•  params  ( list[torch.Parameters] ) – List of parameters of a model •  coalesce  ( bool, optional ) – Whether allreduce parameters as a whole. Defaults to True. •  bucket size mb  ( int, optional ) – Size of bucket, the unit is MB. Defaults to -1. \nmmdet.core.utils. center of mass ( mask ,  $e s p{=}l e$  -06 ) Calculate the centroid coordinates of the mask. \nParameters \n•  mask  ( Tensor ) – The mask to be calculated, shape (h, w). •  esp  ( float ) – Avoid dividing by zero. Default: 1e-6. \nReturns \nthe coordinates of the center point of the mask. "}
{"page": 232, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_232.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* center_h (Tensor): the center point of the height.\n* center_w (Tensor): the center point of the width.\nReturn type tuple[Tensor]\n\nmmdet.core.utils.filter_scores_and_topk (scores, score_thr, topk, results=None)\nFilter results using score threshold and topk candidates.\n\nParameters\n* scores (Tensor) — The scores, shape (num_bboxes, K).\n* score_thr (float) — The score filter threshold.\n* topk (int) — The number of topk candidates.\n\n* results (dict or list or Tensor, Optional) — The results to which the filtering\ntule is to be applied. The shape of each item is (num_bboxes, N).\n\nReturns\nFiltered results\n* scores (Tensor): The scores after being filtered, shape (num_bboxes_filtered, ).\n* labels (Tensor): The class labels, shape (num_bboxes_filtered, ).\n* anchor_idxs (Tensor): The anchor indexes, shape (num_bboxes_filtered, ).\n\n* filtered_results (dict or list or Tensor, Optional): The filtered results. The shape of each item\nis (num_bboxes_filtered, N).\n\nReturn type tuple\n\nmmdet.core.utils.flip_tensor (src_tensor, flip_direction)\nflip tensor base on flip_direction.\n\nParameters\n* src_tensor (Tensor) — input feature map, shape (B, C, H, W).\n\n¢ flip_direction (str) — The flipping direction. Options are ‘horizontal’, ‘vertical’, ‘diag-\nonal’.\n\nReturns Flipped tensor.\nReturn type out_tensor (Tensor)\n\nmmdet.core.utils.generate_coordinate (featmap_sizes, device='cuda')\nGenerate the coordinate.\n\nParameters\n¢ featmap_sizes (tuple) — The feature to be calculated, of shape (N, C, W, H).\n* device (str) — The device where the feature will be put on.\n\nReturns The coordinate feature, of shape (N, 2, W, H).\n\nReturn type coord_feat (Tensor)\n\nmmdet.core.utils.mask2ndarray (mask)\nConvert Mask to ndarray..\n\n:param mask (BitmapMasks or PolygonMasks or: :param torch.Tensor or np.ndarray): The mask to be con-\nverted.\n\nReturns Ndarray mask of shape (n, h, w) that has been converted\n\n37.7. utils 225\n", "vlm_text": "• center_h (Tensor): the center point of the height. • center_w (Tensor): the center point of the width. \nReturn type  tuple[Tensor] \nmmdet.core.utils. filter scores and top k ( scores ,  score_thr ,  topk ,  results  $\\mathbf{\\hat{\\rho}}$  None ) Filter results using score threshold and topk candidates. \nParameters \n•  scores  ( Tensor ) – The scores, shape (num_bboxes, K). •  score_thr  ( float ) – The score filter threshold. •  topk  ( int ) – The number of topk candidates. •  results  ( dict or list or Tensor, Optional ) – The results to which the filtering rule is to be applied. The shape of each item is (num_bboxes, N). \nReturns \nFiltered results • scores (Tensor): The scores after being filtered, shape (num b boxes filtered, ). • labels (Tensor): The class labels, shape (num b boxes filtered, ). • anchor i dx s (Tensor): The anchor indexes, shape (num b boxes filtered, ). • filtered results (dict or list or Tensor, Optional): The filtered results. The shape of each item is (num b boxes filtered, N). \nReturn type  tuple \nmmdet.core.utils. flip tensor ( src_tensor ,  flip direction ) flip tensor base on flip direction. \nParameters \n•  src_tensor  ( Tensor ) – input feature map, shape (B, C, H, W). •  flip direction  ( str ) – The flipping direction. Options are ‘horizontal’, ‘vertical’, ‘diag- onal’. \nReturns  Flipped tensor. \nReturn type  out_tensor (Tensor) \nmmdet.core.utils. generate coordinate ( feat map sizes ,  device  $=$  'cuda' ) Generate the coordinate. \nParameters \n•  feat map sizes  ( tuple ) – The feature to be calculated, of shape (N, C, W, H). •  device  ( str ) – The device where the feature will be put on. Returns  The coordinate feature, of shape (N, 2, W, H). Return type  coord_feat (Tensor) \nmmdet.core.utils.mask 2 nd array(mask)Convert Mask to ndarray.. \n:param mask ( Bitmap Masks  or  Polygon Masks  or: :param torch.Tensor or np.ndarray): The mask to be con- verted. \nReturns  Ndarray mask of shape (n, h, w) that has been converted "}
{"page": 233, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_233.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type np.ndarray\n\nmmndet.core.utils.multi_apply (func, *args, **kwargs)\nApply function to a list of arguments.\n\nNote: This function applies the func to multiple inputs and map the multiple outputs of the func into different\nlist. Each list contains the same type of outputs corresponding to different inputs.\n\nParameters func (Function) — A function that will be applied to a list of arguments\nReturns A tuple containing multiple list, each list contains a kind of returned results by the function\nReturn type tuple(list)\nmmdet.core.utils.reduce_mean (tensor)\n“Obtain the mean of tensor on different GPUs.\n\nmmdet.core.utils.select_single_mlv] (milvi_tensors, batch_id, detach=True)\nExtract a multi-scale single image tensor from a multi-scale batch tensor based on batch index.\n\nNote: The default value of detach is True, because the proposal gradient needs to be detached during the training\nof the two-stage model. E.g Cascade Mask R-CNN.\n\nParameters\n¢ mlvl_tensors (list [Tensor ]) — Batch tensor for all scale levels, each is a 4D-tensor.\n¢ batch_id (int) — Batch index.\n* detach (bool) — Whether detach gradient. Default True.\n\nReturns Multi-scale single image tensor.\n\nReturn type list[Tensor]\n\nmmndet.core.utils.unmap (data, count, inds, fill=0)\nUnmap a subset of item (data) back to the original set of items (of size count)\n\n226 Chapter 37. mmdet.core\n", "vlm_text": "Return type  np.ndarray \nmmdet.core.utils. multi apply ( func ,  \\*args ,  \\*\\*kwargs ) Apply function to a list of arguments. \nNote:  This function applies the  func  to multiple inputs and map the multiple outputs of the  func  into different list. Each list contains the same type of outputs corresponding to different inputs. \nParameters  func  ( Function ) – A function that will be applied to a list of arguments \nReturns  A tuple containing multiple list, each list contains a kind of returned results by the function \nReturn type  tuple(list) \nmmdet.core.utils. reduce mean ( tensor ) “Obtain the mean of tensor on different GPUs. \nmmdet.core.utils. select single ml vl ( ml vl tensors ,  batch_id ,  detach=True ) Extract a multi-scale single image tensor from a multi-scale batch tensor based on batch index. \nNote: The default value of detach is True, because the proposal gradient needs to be detached during the training of the two-stage model. E.g Cascade Mask R-CNN. \nParameters \n•  ml vl tensors  ( list[Tensor] ) – Batch tensor for all scale levels, each is a 4D-tensor. \n•  batch_id  ( int ) – Batch index. •  detach  ( bool ) – Whether detach gradient. Default True. \nReturns  Multi-scale single image tensor. \nReturn type  list[Tensor] \nmmdet.core.utils. unmap ( data ,  count ,  inds ,  fill=0 ) Unmap a subset of item (data) back to the original set of items (of size count) "}
{"page": 234, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_234.jpg", "ocr_text": "CHAPTER\nTHIRTYEIGHT\n\nMMDET.DATASETS\n\n38.1 datasets\n\nclass mmdet.datasets.CityscapesDataset (ann_file, pipeline, classes=None, data_root=None, img_prefix=\",\nseg_prefix=None, proposal_file=None, test_mode=False,\nfilter_empty_gt=True)\n\nevaluate (results, metric='bbox', logger=None, outfile_prefix=None, classwise=False, proposal_nums=(100,\n300, 1000), iou_thrs=array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]))\nEvaluation in Cityscapes/COCO protocol.\n\nParameters\n* results (list[list | tuple])— Testing results of the dataset.\n\n* metric(str | list[str])- Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro-\nposal’, ‘proposal_fast’.\n\n¢ logger (logging.Logger | str | None) — Logger used for printing related informa-\ntion during evaluation. Default: None.\n\n* outfile_prefix (str | None) — The prefix of output file. It includes the file path and\nthe prefix of filename, e.g., “a/b/prefix”. If results are evaluated with COCO protocol,\nit would be the prefix of output json file. For example, the metric is ‘bbox’ and ‘segm’,\nthen json files would be “a/b/prefix.bbox.json” and “a/b/prefix.segm.json”. If results are\nevaluated with cityscapes protocol, it would be the prefix of output txt/png files. The out-\nput files would be png images under folder “a/b/prefix/xxx/” and the file name of images\nwould be written into a txt file “a/b/prefix/xxx_pred.txt”, where “xxx” is the video name\nof cityscapes. If not specified, a temp file will be created. Default: None.\n\n¢ classwise (bool) — Whether to evaluating the AP for each class.\n\n* proposal_nums (Sequence [int ]) — Proposal number used for evaluating recalls, such\nas recall@ 100, recall @ 1000. Default: (100, 300, 1000).\n\n¢ iou_thrs (Sequence[ float ])—IoU threshold used for evaluating recalls. If set to a list,\nthe average recall of all IoUs will also be computed. Default: 0.5.\n\nReturns COCO style evaluation metric or cityscapes mAP and AP@S0.\nReturn type dict[str, float]\n\nformat_results (results, txtfile_prefix=None)\nFormat the results to txt (standard format for Cityscapes evaluation).\n\nParameters\n\n227\n", "vlm_text": "MMDET.DATASETS \n38.1 datasets \nclass  mmdet.datasets. Cityscape s Data set ( ann_file ,  pipeline ,  classes=None ,  data_root=None ,  img_prefix='' , seg_prefix=None ,  proposal file=None ,  test_mode=False , filter empty gt=True ) \nevaluate ( results ,  metric  $=$  'bbox' ,  logger  $\\leftrightharpoons$  None ,  out file prefix  $\\mathbf{\\beta}=$  None ,  classwise  $\\mathbf{=}$  False ,  proposal num s=(100, 300, 1000) ,  iou_thrs  $=$  array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]) ) Evaluation in Cityscapes/COCO protocol. \nParameters \n•  results  ( list[list | tuple] ) – Testing results of the dataset. \n metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro- posal’, ‘proposal fast’. \n•  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. \n out file prefix  ( str | None ) – The prefix of output file. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If results are evaluated with COCO protocol, it would be the prefix of output json file. For example, the metric is ‘bbox’ and ‘segm’, then json files would be “a/b/prefix.bbox.json” and “a/b/prefix.segm.json”. If results are evaluated with cityscapes protocol, it would be the prefix of output txt/png files. The out- put files would be png images under folder “a/b/prefix/xxx/” and the file name of images would be written into a txt file “a/b/prefix/xxx_pred.txt”, where “xxx” is the video name of cityscapes. If not specified, a temp file will be created. Default: None. \n•  classwise  ( bool ) – Whether to evaluating the AP for each class. \n•  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thrs  ( Sequence[float] ) – IoU threshold used for evaluating recalls. If set to a list, the average recall of all IoUs will also be computed. Default: 0.5. \nReturns  COCO style evaluation metric or cityscapes mAP and  AP  $@50$  . \nReturn type  dict[str, float] \nformat results ( results ,  txt file prefix  $\\mathbf{\\dot{\\rho}}=$  None ) Format the results to txt (standard format for Cityscapes evaluation). \nParameters "}
{"page": 235, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_235.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* results (list) — Testing results of the dataset.\n\n¢ txtfile_prefix(str | None) —The prefix of txt files. It includes the file path and the\nprefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. Default:\nNone.\n\nReturns (result_files, tmp_dir), result_files is a dict containing the json filepaths, tmp_dir is the\ntemporal directory created for saving txt/png files when txtfile_prefix is not specified.\n\nReturn type tuple\n\nresults2txt (results, outfile_prefix)\nDump the detection results to a txt file.\n\nParameters\n* results (list[list | tuple])— Testing results of the dataset.\n\n* outfile_prefix (str) — The filename prefix of the json files. If the prefix is\n“somepath/xxx”, the txt files will be named “somepath/xxx.txt”.\n\nReturns Result txt files which contains corresponding instance segmentation images.\nReturn type list[str]\n\nclass mmdet.datasets.ClassBalancedDataset (dataset, oversample_thr, filter_empty_gt=True)\nA wrapper of repeated dataset with repeat factor.\n\nSuitable for training on class imbalanced datasets like LVIS. Following the sampling strategy in the paper, in\neach epoch, an image may appear multiple times based on its “repeat factor”. The repeat factor for an image is\na function of the frequency the rarest category labeled in that image. The “frequency of category c” in [0, 1] is\ndefined by the fraction of images in the training set (without repeats) in which category c appears. The dataset\nneeds to instantiate self. get_cat_ids() to support ClassBalancedDataset.\n\nThe repeat factor is computed as followed.\n1. For each category c, compute the fraction # of images that contain it: f(c)\n2. For each category c, compute the category-level repeat factor: r(c) = max(1, sqrt(t/f(c)))\n\n3. For each image I, compute the image-level repeat factor: r(I) = maxeinrr(c)\n\nParameters\n* dataset (CustomDataset) — The dataset to be repeated.\n\n* oversample_thr (float) — frequency threshold below which data is repeated. For cat-\negories with f£_c >= oversample_thr, there is no oversampling. For categories with\nf_c < oversample_thr, the degree of oversampling following the square-root inverse fre-\nquency heuristic above.\n\n¢ filter_empty_gt (bool, optional) — If set true, images without bounding boxes will\nnot be oversampled. Otherwise, they will be categorized as the pure background class and\ninvolved into the oversampling. Default: True.\n\nclass mmdet.datasets.CocoDataset (ann_file, pipeline, classes=None, data_root=None, img_prefix=\",\nseg_prefix=None, proposal_file=None, test_mode=False,\nfilter_empty_gt=True)\n\nevaluate (results, metric='bbox', logger=None, jsonfile_prefix=None, classwise=False,\nproposal_nums=(100, 300, 1000), iou_thrs=None, metric_items=None)\nEvaluation in COCO protocol.\n\n228 Chapter 38. mmdet.datasets\n", "vlm_text": "•  txt file prefix  ( str | None ) – The prefix of txt files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. Default: None. \nReturns  (result files, tmp_dir), result files is a dict containing the json filepaths, tmp_dir is the temporal directory created for saving txt/png files when txt file prefix is not specified. \nReturn type  tuple \nresults 2 txt ( results ,  out file prefix ) Dump the detection results to a txt file. \nParameters \n•  results  ( list[list | tuple] ) – Testing results of the dataset. \n out file prefix  ( str ) – The filename prefix of the json files. If the prefix is “somepath/xxx”, the txt files will be named “somepath/xxx.txt”. \nReturns  Result txt files which contains corresponding instance segmentation images. \nReturn type  list[str] \nclass  mmdet.datasets. Class Balanced Data set ( dataset ,  over sample thr ,  filter empty g  $\\leftleftarrows$  True ) A wrapper of repeated dataset with repeat factor. \nSuitable for training on class imbalanced datasets like LVIS. Following the sampling strategy in the  paper , in each epoch, an image may appear multiple times based on its “repeat factor”. The repeat factor for an image is a function of the frequency the rarest category labeled in that image. The “frequency of category   $\\mathbf{c}^{\\ast}$   in [0, 1] is defined by the fraction of images in the training set (without repeats) in which category c appears. The dataset needs to instantiate  self.get cat ids()  to support Class Balanced Data set. \nThe repeat factor is computed as followed. \n1. For each category c, compute the fraction # of images that contain it:    $f(c)$  \n2. For each category c, compute the category-level repeat factor:    $r(c)=m a x(1,s q r t(t/f(c)))$  \n3. For each image I, compute the image-level repeat factor:    $r(I)=m a x_{c i n I}r(c)$  \nParameters \n•  dataset  ( Custom Data set ) – The dataset to be repeated. \n•  over sample thr  ( float ) – frequency threshold below which data is repeated. For cat- egories with    $\\tt f\\_c\\;>=\\;$   over sample thr , there is no oversampling. For categories with  $\\tt f\\_c\\ <\\$   over sample thr , the degree of oversampling following the square-root inverse fre- quency heuristic above. \n•  filter empty gt  ( bool, optional ) – If set true, images without bounding boxes will not be over sampled. Otherwise, they will be categorized as the pure background class and involved into the oversampling. Default: True. \nclass  mmdet.datasets. Coco Data set ( ann_file ,  pipeline ,  classes  $\\leftrightharpoons$  None ,  data_root  $=$  None ,  img_prefix  $\\mathrel{\\mathop:}=^{\\prime\\prime}$  , seg_prefix  $\\mathbf{\\beta}=$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None ,  test_mode  $:=$  False , filter empty g  $t{=}$  True ) \nevaluate ( results ,  metric  $\\mathbf{=}$  'bbox' ,  logge  $\\leftrightharpoons$  None ,  json file prefix  $\\mathbf{\\dot{\\rho}}=$  None ,  classwise  $=$  False , proposal num s  $=$  (100, 300, 1000) ,  iou_thrs  $\\mathbf{\\check{\\Sigma}}$  None ,  metric items=None ) Evaluation in COCO protocol. "}
{"page": 236, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_236.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\n* results (list[list | tuple])— Testing results of the dataset.\n\n* metric(str | list[str])- Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro-\nposal’, ‘proposal_fast’.\n\nlogger (logging.Logger | str | None) — Logger used for printing related informa-\ntion during evaluation. Default: None.\n\njsonfile_prefix (str | None) — The prefix of json files. It includes the file path and\nthe prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De-\nfault: None.\n\nclasswise (bool) — Whether to evaluating the AP for each class.\n\nproposal_nums (Sequence[int]) — Proposal number used for evaluating recalls, such\nas recall@ 100, recall @ 1000. Default: (100, 300, 1000).\n\n¢ iou_thrs (Sequence[float], optional) — IoU threshold used for evaluating re-\ncalls/mAPs. If set to a list, the average of all IoUs will also be computed. If not specified,\n[0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used. Default: None.\n\nmetric_items(list[str] | str, optional) — Metric items that will be returned. If\nnot specified, ['AR@100', 'AR@300', 'AR@1000', 'AR_s@1000', 'AR_m@1000',\n\"AR_1@1000' ] will be used when metric=='proposal', ['mAP', 'mAP_50',\n\"mAP_75', 'mAP_s', 'mAP_m', 'mAP_1\"] will be used when metric=='bbox' or\nmetric=='segm'.\n\nReturns COCO style evaluation metric.\n\nReturn type dict[str, float]\n\nformat_results (results, jsonfile_prefix=None, **kwargs)\nFormat the results to json (standard format for COCO evaluation).\n\nParameters\n\n¢ results (list[tuple | numpy.ndarray]) — Testing results of the dataset.\n\n¢ jsonfile_prefix (str | None) — The prefix of json files. It includes the file path and\nthe prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De-\nfault: None.\n\nReturns (result_files, tmp_dir), result_files is a dict containing the json filepaths, tmp_dir is the\n\ntemporal directory created for saving json files when jsonfile_prefix is not specified.\n\nReturn type tuple\n\nget_ann_info (idx)\nGet COCO annotation by index.\n\nParameters idx (int) — Index of data.\n\nReturns Annotation info of specified index.\n\nReturn type dict\n\nget_cat_ids (idx)\nGet COCO category ids by index.\n\nParameters idx (int) — Index of data.\n\nReturns All categories in the image of specified index.\n\nReturn type list[int]\n\n38.1. datasets\n\n229\n", "vlm_text": "Parameters \n•  results  ( list[list | tuple] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro- posal’, ‘proposal fast’. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. •  json file prefix  ( str | None ) – The prefix of json files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De- fault: None. •  classwise  ( bool ) – Whether to evaluating the AP for each class. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thrs  ( Sequence[float], optional ) – IoU threshold used for evaluating re- calls/mAPs. If set to a list, the average of all IoUs will also be computed. If not specified, [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used. Default: None. •  metric items  ( list[str] | str, optional ) – Metric items that will be returned. If not specified,  [ ' AR@100 ' ,  ' AR@300 ' ,  ' AR@1000 ' ,  ' AR_s@1000 ' ,  ' AR_m@1000 ' , ' AR_l@1000 '  ]  will be used when  metric  $==$  ' proposal ' ,  [ ' mAP ' ,    $\\,\\cdot\\,\\mathtt{m A P}\\_50\\,^{\\bullet}$  , ' mAP_75 ' ,  ' mAP_s ' ,  ' mAP_m ' ,  ' mAP_l ' ]  will be used when  metric  $==$  ' bbox '  or metric  $==$  ' segm ' . \nReturns  COCO style evaluation metric. \nReturn type  dict[str, float] \nformat results ( results ,  json file prefix  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Format the results to json (standard format for COCO evaluation). \nParameters \n•  results  ( list[tuple | numpy.ndarray] ) – Testing results of the dataset. \n•  json file prefix  ( str | None ) – The prefix of json files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De- fault: None. \nReturns  (result files, tmp_dir), result files is a dict containing the json filepaths, tmp_dir is the temporal directory created for saving json files when json file prefix is not specified. \nReturn type  tuple \nget ann info ( idx ) Get COCO annotation by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict \nget cat ids ( idx ) Get COCO category ids by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] "}
{"page": 237, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_237.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nload_annotations (ann_file)\nLoad annotation from COCO style annotation file.\n\nParameters ann_file (str) — Path of annotation file.\nReturns Annotation info from COCO api.\nReturn type list[dict]\n\nresults2json(results, outfile_prefix)\nDump the detection results to a COCO style json file.\n\nThere are 3 types of results: proposals, bbox predictions, mask predictions, and they have different data\ntypes. This method will automatically recognize the type, and dump them to json files.\n\nParameters\n¢ results (list[list | tuple | ndarray]) — Testing results of the dataset.\n\n* outfile_prefix (str) — The filename prefix of the json files. If the pre-\nfix is “somepath/xxx”, the json files will be named “somepath/xxx.bbox.json”,\n\noe,\n\n“somepath/xxx.segm.json”, “somepath/xxx.proposal.json”.\n\nReturns str]: Possible keys are “bbox”, “segm’, “proposal”, and values are corresponding file-\nnames.\nReturn type dict[str\n\nxyxy2xywh (bbox)\nConvert xyxy style bounding boxes to xywh style for COCO evaluation.\n\nParameters bbox (numpy.ndarray) — The bounding boxes, shape (4, ), in xyxy order.\nReturns The converted bounding boxes, in xywh order.\nReturn type list[float]\n\nclass mmdet.datasets.CocoPanopticDataset (ann_file, pipeline, classes=None, data_root=None,\nimg_prefix=\", seg_prefix=None, proposal_file=None,\ntest_mode=False, filter_empty_gt=True)\nCoco dataset for Panoptic segmentation.\n\nThe annotation format is shown as follows. The ann field is optional for testing.\n\n[\n{\n‘filename': f£'{image_id:012}.png',\n‘image_id':9\n\"segments_info': {\n[\n{\n\"id': 8345037, (segment_id in panoptic png,\nconvert from rgb)\n\"category_id': 51,\n‘iscrowd': 0,\n\"bbox': (x1, yl, w, h),\n\"area': 24315,\n\"segmentation': list, (coded mask)\n3,\n}\n\n(continues on next page)\n\n230 Chapter 38. mmdet.datasets\n", "vlm_text": "load annotations ( ann_file ) Load annotation from COCO style annotation file. \nParameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from COCO api. Return type  list[dict] \nresults 2 json ( results ,  out file prefix ) Dump the detection results to a COCO style json file. There are 3 types of results: proposals, bbox predictions, mask predictions, and they have different data types. This method will automatically recognize the type, and dump them to json files. \nParameters \n•  results  ( list[list | tuple | ndarray] ) – Testing results of the dataset. \n•  out file prefix  ( str ) – The filename prefix of the json files. If the pre- fix is “somepath/xxx”, the json files will be named “somepath/xxx.bbox.json”, “somepath/xxx.segm.json”, “somepath/xxx.proposal.json”. Returns  str]: Possible keys are “bbox”, “segm”, “proposal”, and values are corresponding file- names. \nxyxy2xywh ( bbox ) Convert  xyxy  style bounding boxes to  xywh  style for COCO evaluation. Parameters  bbox  ( numpy.ndarray ) – The bounding boxes, shape (4, ), in  xyxy  order. Returns  The converted bounding boxes, in  xywh  order. Return type  list[float] \nclass  mmdet.datasets. Coco Pan optic Data set ( ann_file ,  pipeline ,  classes  $:=$  None ,  data_root  $\\leftrightharpoons$  None , img_pref  $\\scriptstyle{x={'}{'}}$  ,  seg_prefix  $\\mathbf{\\beta}=$  None ,  proposal file  $\\mathbf{\\hat{\\rho}}=$  None , test_mode  $=$  False ,  filter empty gt=True ) \nCoco dataset for Panoptic segmentation. \nThe annotation format is shown as follows. The  ann  field is optional for testing. \nThe image shows a JSON-like data structure typically used in image segmentation or annotation tasks. It contains:\n\n- `filename`: The name of the image file, formatted with an `image_id`.\n- `image_id`: An identifier for the image, here set to `9`.\n- `segments_info`: A dictionary that includes details about image segments:\n  - `id`: A unique identifier for a segment.\n  - `category_id`: An identifier for the category of the segment.\n  - `iscrowd`: A binary flag indicating if the segment is a crowd (0 means no).\n  - `bbox`: The bounding box coordinates and dimensions: `(x1, y1, w, h)`.\n  - `area`: The area covered by the segment.\n  - `segmentation`: A list representing a coded mask for the segment."}
{"page": 238, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_238.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n3,\n]\n\nevaluate (results, metric='PQ', logger=None, jsonfile_prefix=None, classwise=False, **kwargs)\nEvaluation in COCO Panoptic protocol.\n\nParameters\n* results (list [dict]) — Testing results of the dataset.\n\n* metric (str | list[str]) — Metrics to be evaluated. Only support ‘PQ’ at present.\n‘pq’ will be regarded as ‘PQ.\n\n¢ logger (logging.Logger | str | None) — Logger used for printing related informa-\ntion during evaluation. Default: None.\n\n¢ jsonfile_prefix (str | None) — The prefix of json files. It includes the file path and\nthe prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De-\nfault: None.\n\n¢ classwise (bool) — Whether to print classwise evaluation results. Default: False.\nReturns COCO Panoptic style evaluation metric.\nReturn type dict[str, float]\n\nevaluate_pan_json(result_files, outfile_prefix, logger=None, classwise=False)\nEvaluate PQ according to the panoptic results json file.\n\nget_ann_info (idx)\nGet COCO annotation by index.\n\nParameters idx (int) — Index of data.\nReturns Annotation info of specified index.\nReturn type dict\n\nload_annotations (ann_file)\nLoad annotation from COCO Panoptic style annotation file.\n\nParameters ann_file (str) — Path of annotation file.\nReturns Annotation info from COCO api.\nReturn type list[dict]\n\nresults2json(results, outfile_prefix)\nDump the panoptic results to a COCO panoptic style json file.\n\nParameters\n* results (dict) — Testing results of the dataset.\n\n* outfile_prefix (str) — The filename prefix of the json files. If the prefix is\n“somepath/xxx”, the json files will be named “‘somepath/xxx.panoptic.json”\n\nReturns\nstr]: The key is ‘panoptic’ and the value is corresponding filename.\n\nReturn type dict[str\n\n38.1. datasets 231\n", "vlm_text": "The image appears to be a snippet of code or data structure that is continuing from a previous page. It contains what seems to be closing brackets and commas, likely indicating the end of a list, array, or object in a programming or data context. The brackets include closing curly braces (`}`) and a closing square bracket (`]`). There are ellipses (`...`) implying the truncation or continuation of content not fully visible in this image.\nevaluate ( results ,  metric  $\\mathbf{\\Sigma=}^{\\prime}P\\mathcal{Q}^{\\prime}$  ,  logger  $\\leftrightharpoons$  None ,  json file prefix  $\\mathbf{\\beta}=$  None ,  classwise  $\\mathbf{\\dot{\\rho}}=$  False ,  \\*\\*kwargs ) Evaluation in COCO Panoptic protocol. \nParameters \n•  results  ( list[dict] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Only support ‘PQ’ at present. ‘pq’ will be regarded as ‘PQ. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. •  json file prefix  ( str | None ) – The prefix of json files. It includes the file path and the prefix of filename, e.g., “a/b/prefix”. If not specified, a temp file will be created. De- fault: None. •  classwise  ( bool ) – Whether to print classwise evaluation results. Default: False. Returns  COCO Panoptic style evaluation metric. Return type  dict[str, float] \nevaluate pan json ( result files ,  out file prefix ,  logger  $=$  None ,  classwise  $\\mathbf{\\beta}=$  False ) Evaluate PQ according to the panoptic results json file. \nget ann info ( idx ) Get COCO annotation by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict \nload annotations ( ann_file ) Load annotation from COCO Panoptic style annotation file. \nParameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from COCO api. Return type  list[dict] \nresults 2 json ( results ,  out file prefix ) Dump the panoptic results to a COCO panoptic style json file. \nParameters •  results  ( dict ) – Testing results of the dataset. •  out file prefix  ( str ) – The filename prefix of the json files. If the prefix is “somepath/xxx”, the json files will be named “somepath/xxx.panoptic.json” Returns str]: The key is ‘panoptic’ and the value is  corresponding filename. Return type  dict[str "}
{"page": 239, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_239.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.datasets.ConcatDataset (datasets, separate_eval=True)\nA wrapper of concatenated dataset.\n\nSame as torch.utils.data.dataset.ConcatDataset, but concat the group flag for image aspect ratio.\nParameters\n¢ datasets (list[Dataset]) — A list of datasets.\n\n* separate_eval (bool) -— Whether to evaluate the results separately if it is used as validation\ndataset. Defaults to True.\n\nevaluate (results, logger=None, **kwargs)\nEvaluate the results.\n\nParameters\n* results (list[list | tuple])— Testing results of the dataset.\n\n¢ logger (logging.Logger | str | None) — Logger used for printing related informa-\ntion during evaluation. Default: None.\n\nReturns float]: AP results of the total dataset or each separate dataset if self: separate_eval=True.\nReturn type dict[str\n\nget_cat_ids (idx)\nGet category ids of concatenated dataset by index.\n\nParameters idx (int) — Index of data.\nReturns All categories in the image of specified index.\nReturn type list[int]\n\nclass mmdet.datasets.CustomDataset (ann_file, pipeline, classes=None, data_root=None, img_prefix=\",\nseg_prefix=None, proposal_file=None, test_mode=False,\nfilter_empty_gt=True)\nCustom dataset for detection.\n\nThe annotation format is shown as follows. The ann field is optional for testing.\n\n[\n{\n‘filename': 'a.jpg',\n‘width': 1280,\n\"height': 720,\n‘ann': {\n\"pbboxes': <np.ndarray> (n, 4) in (xl, yl, x2, y2) order.\n‘labels': <np.ndarray> (n, ),\n\"pboxes_ignore': <np.ndarray> (k, 4), (optional field)\n‘labels_ignore': <np.ndarray> (k, 4) (optional field)\n}\n3,\n]\nParameters\n\n* ann_file (str) — Annotation file path.\n\n* pipeline (list [dict]) — Processing pipeline.\n\n232 Chapter 38. mmdet.datasets\n", "vlm_text": "class  mmdet.datasets. Con cat Data set ( datasets ,  separate eva  $\\leftrightharpoons$  True ) A wrapper of concatenated dataset. \nSame as  torch.utils.data.dataset.Con cat Data set , but concat the group flag for image aspect ratio. \n•  datasets  (list[ Dataset ]) – A list of datasets. •  separate e val  ( bool ) – Whether to evaluate the results separately if it is used as validation dataset. Defaults to True. \nevaluate ( results ,  logger  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) Evaluate the results. \n•  results  ( list[list | tuple] ) – Testing results of the dataset. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. Returns  float]: AP results of the total dataset or each separate dataset if  self.separate e val  $\\leftrightharpoons$  True . \nget cat ids ( idx ) Get category ids of concatenated dataset by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. \nReturn type  list[int] \nclass  mmdet.datasets. Custom Data set ( ann_file ,  pipeline ,  classes  $\\leftrightharpoons$  None ,  data_root  $\\mathbf{\\dot{=}}$  None ,  img_prefix='' , seg_prefix  $\\mathbf{\\dot{\\rho}}$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None ,  test_mode  $\\mathbf{\\beta}=$  False , filter empty g  $t{=}$  True ) \nCustom dataset for detection. \nThe annotation format is shown as follows. The  ann  field is optional for testing. \nThe image shows a snippet of JSON-like data, probably used for image annotation. Here's a breakdown of the structure:\n\n- `filename`: 'a.jpg'\n- `width`: 1280\n- `height`: 720\n- `ann` (annotations):\n  - `bboxes`: `<np.ndarray>` (n, 4) representing bounding boxes in (x1, y1, x2, y2) order.\n  - `labels`: `<np.ndarray>` (n,) indicating labels.\n  - `bboxes_ignore`: `<np.ndarray>` (k, 4), an optional field for ignored bounding boxes.\n  - `labels_ignore`: `<np.ndarray>` (k, 4), an optional field for ignored labels.\n\nThis format suggests usage for image processing tasks like object detection.\nParameters \n•  ann_file  ( str ) – Annotation file path. •  pipeline  ( list[dict] ) – Processing pipeline. "}
{"page": 240, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_240.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* classes(str | Sequence[str], optional) -Specify classes to load. Ifis None, cls.\nCLASSES will be used. Default: None.\n\n¢ data_root (str, optional) — Data root for ann_file, img_prefix, seg_prefix,\nproposal_file if specified.\n\n* test_mode (bool, optional) — If set True, annotation will not be loaded.\n\n¢ filter_empty_gt (bool, optional) —If set true, images without bounding boxes of the\ndataset’s classes will be filtered out. This option only works when test_mode=False, i.e., we\nnever filter images during tests.\n\nevaluate (results, metric='mAP', logger=None, proposal_nums=(100, 300, 1000), iou_thr=0.5,\nscale_ranges=None)\nEvaluate the dataset.\n\nParameters\n* results (list) — Testing results of the dataset.\n¢metric(str / list[str])— Metrics to be evaluated.\n\n¢ logger (logging.Logger | None | str) — Logger used for printing related informa-\ntion during evaluation. Default: None.\n\n* proposal_nums (Sequence [int ]) — Proposal number used for evaluating recalls, such\nas recall@ 100, recall @ 1000. Default: (100, 300, 1000).\n\n¢ iou_thr (float | list [float])—IoU threshold. Default: 0.5.\n\n¢ scale_ranges (list[tuple] | None) — Scale ranges for evaluating mAP. Default:\nNone.\n\nformat_results (results, **kwargs)\nPlace holder to format result to dataset specific output.\n\nget_ann_info (idx)\nGet annotation by index.\n\nParameters idx (int) — Index of data.\nReturns Annotation info of specified index.\nReturn type dict\n\nget_cat_ids (idx)\nGet category ids by index.\n\nParameters idx (int) — Index of data.\nReturns All categories in the image of specified index.\nReturn type list[int]\n\nclassmethod get_classes (classes=None)\nGet class names of current dataset.\n\nParameters classes (Sequence[str] | str | None) — If classes is None, use default\nCLASSES defined by builtin dataset. If classes is a string, take it as a file name. The file\ncontains the name of classes where each line contains one class name. If classes is a tuple or\nlist, override the CLASSES defined by the dataset.\n\nReturns Names of categories of the dataset.\n\nReturn type tuple[str] or list[str]\n\n38.1. datasets 233\n", "vlm_text": "•  classes  ( str | Sequence[str], optional ) – Specify classes to load. If is None,  cls. CLASSES  will be used. Default: None. •  data_root  ( str, optional ) – Data root for  ann_file ,  img_prefix ,  seg_prefix , proposal file  if specified. •  test_mode  ( bool, optional ) – If set True, annotation will not be loaded. •  filter empty gt  ( bool, optional ) – If set true, images without bounding boxes of the dataset’s classes will be filtered out. This option only works when  test_mode  $\\mathbf{=}$  False , i.e., we never filter images during tests. \nevaluate ( results ,  metri  $\\scriptstyle c=m A P^{\\prime}$  ,  logger  $\\leftrightharpoons$  None ,  proposal_nums=(100, 300, 1000) ,  iou_thr  $\\mathord{\\simeq}\\!O.5$  , scale range  $\\leftrightharpoons$  None ) Evaluate the dataset. \n\n•  results  ( list ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. •  logger  ( logging.Logger | None | str ) – Logger used for printing related informa- tion during evaluation. Default: None. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thr  ( float | list[float] ) – IoU threshold. Default: 0.5. •  scale ranges  ( list[tuple] | None ) – Scale ranges for evaluating mAP. Default: None. \nformat results ( results ,  \\*\\*kwargs ) Place holder to format result to dataset specific output. \nget ann info ( idx ) Get annotation by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict \nget cat ids ( idx ) Get category ids by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] \nclass method get classes (  $=$  ) Get class names of current dataset. Parameters  classes  ( Sequence[str] | str | None ) – If classes is None, use default CLASSES defined by builtin dataset. If classes is a string, take it as a file name. The file contains the name of classes where each line contains one class name. If classes is a tuple or list, override the CLASSES defined by the dataset. Returns  Names of categories of the dataset. Return type  tuple[str] or list[str] "}
{"page": 241, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_241.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nload_annotations (ann_file)\nLoad annotation from annotation file.\n\nload_proposals (proposal_file)\nLoad proposal from proposal file.\n\npre_pipeline (results)\nPrepare results dict for pipeline.\n\nprepare_test_img (idx)\nGet testing data after pipeline.\n\nParameters idx (int) — Index of data.\nReturns Testing data after pipeline with new keys introduced by pipeline.\nReturn type dict\n\nprepare_train_img (idx)\nGet training data and annotations after pipeline.\n\nParameters idx (int) — Index of data.\nReturns Training data and annotation after pipeline with new keys introduced by pipeline.\nReturn type dict\n\nclass mmdet.datasets.DeepFashionDataset (ann_file, pipeline, classes=None, data_root=None,\nimg_prefix=\", seg_prefix=None, proposal_file=None,\ntest_mode=False, filter_empty_gt=True)\n\nclass mmdet.datasets.DistributedGroupSampler (dataset, samples_per_gpu=1, num_replicas=None,\nrank=None, seed=0)\nSampler that restricts data loading to a subset of the dataset.\n\nIt is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such case,\neach process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original\n\ndataset that is exclusive to it.\n\nNote: Dataset is assumed to be of constant size.\n\nParameters\n* dataset — Dataset used for sampling.\n* num_replicas (optional) — Number of processes participating in distributed training.\n* rank (optional) — Rank of the current process within num_replicas.\n* seed (int, optional) -—random seed used to shuffle the sampler if shuffle=True. This\nnumber should be identical across all processes in the distributed group. Default: 0.\nclass mmdet.datasets.DistributedSampler (dataset, num_replicas=None, rank=None, shuffle=True,\nseed=0)\nclass mmdet.datasets.GroupSampler (dataset, samples_per_gpu=1)\n\nmmndet .datasets .LVISDataset\nalias of mmdet.datasets.lvis.LVISVO5Dataset\n\n234 Chapter 38. mmdet.datasets\n", "vlm_text": "load annotations ( ann_file ) Load annotation from annotation file. \nload proposals ( proposal file ) Load proposal from proposal file. \npre pipeline ( results ) Prepare results dict for pipeline. \nprepare test img ( idx ) Get testing data after pipeline. Parameters  idx  ( int ) – Index of data. Returns  Testing data after pipeline with new keys introduced by pipeline. Return type  dict prepare train img ( idx ) Get training data and annotations after pipeline. Parameters  idx  ( int ) – Index of data. Returns  Training data and annotation after pipeline with new keys introduced by pipeline. Return type  dict \nclass  mmdet.datasets. Deep Fashion Data set ( ann_file ,  pipeline ,  classes  $\\leftrightharpoons$  None ,  data_root  $=$  None , img_pref  $\\acute{\\iota}={}^{\\prime\\prime}$  ,  seg_prefix  $\\mathbf{\\hat{\\rho}}$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None , test_mode  $\\mathbf{=}$  False ,  filter empty gt=True ) \nclass  mmdet.datasets. Distributed Group Sampler ( dataset ,  samples per gpu  $\\mathsf{\\Pi}_{=I}$  ,  num replicas  $\\mathbf{=}$  None , rank  $\\leftrightharpoons$  None ,  seed  $\\mathrm{\\Lambda}{}=\\!\\!O_{.}$  ) \nSampler that restricts data loading to a subset of the dataset. \nIt is especially useful in conjunction with  torch.nn.parallel.Distributed Data Parallel . In such case, each process can pass a Distributed Sampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it. \nNote:  Dataset is assumed to be of constant size. \nParameters \n•  dataset  – Dataset used for sampling. •  num replicas  ( optional ) – Number of processes participating in distributed training. •  rank  ( optional ) – Rank of the current process within num replicas. •  seed  ( int, optional ) – random seed used to shuffle the sampler if  shuffle  $=$  True . This number should be identical across all processes in the distributed group. Default: 0. \nclass  mmdet.datasets. Distributed Sampler ( dataset ,  num replicas  $=$  None ,  rank  $\\mathbf{\\beta}=$  None ,  shuffle  $\\mathbf{\\Psi}=$  True , seed  $\\mathrm{\\Lambda}{}=\\!\\!O_{.}$  ) \nclass  mmdet.datasets. Group Sampler ( dataset ,  samples per gpu  $\\scriptstyle{I=I}$  ) \nmmdet.datasets. LV IS Data set alias of  mmdet.datasets.lvis.LV IS V 05 Data set "}
{"page": 242, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_242.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.datasets.LVISVO5Dataset (ann_file, pipeline, classes=None, data_root=None, img_prefix=\",\nseg_prefix=None, proposal_file=None, test_mode=False,\nfilter_empty_gt=True)\n\nevaluate (results, metric='bbox', logger=None, jsonfile_prefix=None, classwise=False,\nproposal_nums=(100, 300, 1000), iou_thrs=array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9,\n0.95]))\nEvaluation in LVIS protocol.\n\nParameters\n* results (list[list | tuple])— Testing results of the dataset.\n\n* metric(str | list[str])- Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro-\nposal’, ‘proposal_fast’.\n\n¢ logger (logging.Logger | str | None) — Logger used for printing related informa-\ntion during evaluation. Default: None.\n\n¢ jsonfile_prefix(str | None) —\n¢ classwise (bool) — Whether to evaluating the AP for each class.\n\n* proposal_nums (Sequence [int ]) — Proposal number used for evaluating recalls, such\nas recall@ 100, recall @ 1000. Default: (100, 300, 1000).\n\n¢ iou_thrs (Sequence[ float ])—IoU threshold used for evaluating recalls. If set to a list,\nthe average recall of all IoUs will also be computed. Default: 0.5.\n\nReturns LVIS style metrics.\nReturn type dict[str, float]\n\nload_annotations (ann_file)\nLoad annotation from lvis style annotation file.\n\nParameters ann_file (str) — Path of annotation file.\nReturns Annotation info from LVIS api.\nReturn type list[dict]\n\nclass mmdet.datasets.LVISV1Dataset (ann_file, pipeline, classes=None, data_root=None, img_prefix=\",\nseg_prefix=None, proposal_file=None, test_mode=False,\nfilter_empty_gt=True)\n\nload_annotations (ann_file)\nLoad annotation from lvis style annotation file.\n\nParameters ann_file (str) — Path of annotation file.\nReturns Annotation info from LVIS api.\nReturn type list[dict]\n\nclass mmdet.datasets.MultiImageMixDataset (dataset, pipeline, dynamic_scale=None,\nskip_type_keys=None)\nA wrapper of multiple images mixed dataset.\n\nSuitable for training on multiple images mixed data augmentation like mosaic and mixup. For the augmentation\npipeline of mixed image data, the get_indexes method needs to be provided to obtain the image indexes, and\nyou can set skip_flags to change the pipeline running process. At the same time, we provide the dynamic_scale\nparameter to dynamically change the output image size.\n\n38.1. datasets 235\n", "vlm_text": "class  mmdet.datasets. LV IS V 05 Data set ( ann_file ,  pipeline ,  classes  $\\mathbf{\\check{\\Sigma}}$  None ,  data_root  $=$  None ,  img_prefix  $\\mathrel{=}^{\\prime}$  ' , seg_prefix  $\\mathbf{\\beta}=$  None ,  proposal file  $\\leftrightharpoons$  None ,  test_mode  $\\mathbf{\\Pi}^{*}=$  False , filter empty gt=True ) \nevaluate ( results ,  metric  $=$  'bbox' ,  logger  $\\mathbf{\\hat{\\Sigma}}$  None ,  json file prefix  $\\mathbf{\\hat{\\rho}}$  None ,  classwise  $\\mathbf{\\dot{\\rho}}=$  False , proposal num s  $=$  (100, 300, 1000) ,  iou_thrs  $\\mathbf{\\tilde{=}}$  array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]) ) \nEvaluation in LVIS protocol. \nParameters •  results  ( list[list | tuple] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘bbox’, ‘segm’, ‘pro- posal’, ‘proposal fast’. •  logger  ( logging.Logger | str | None ) – Logger used for printing related informa- tion during evaluation. Default: None. •  json file prefix  ( str | None ) – •  classwise  ( bool ) – Whether to evaluating the AP for each class. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thrs  ( Sequence[float] ) – IoU threshold used for evaluating recalls. If set to a list, the average recall of all IoUs will also be computed. Default: 0.5. Returns  LVIS style metrics. Return type  dict[str, float] ( ann_file ) Load annotation from lvis style annotation file. \nParameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from LVIS api. Return type  list[dict] \nclass  mmdet.datasets. LV IS V 1 Data set ( ann_file ,  pipeline ,  classes  $\\mathbf{=}$  None ,  data_root  $\\leftrightharpoons$  None ,  img_prefix='' , seg_pref  $\\leftleftarrows$  None ,  proposal file  $\\mathbf{\\dot{\\rho}}$  None ,  test_mode  $\\mathbf{\\beta}=$  False , filter empty g  $t{=}$  True ) \nLoad annotation from lvis style annotation file. Parameters  ann_file  ( str ) – Path of annotation file. Returns  Annotation info from LVIS api. Return type  list[dict] \nclass  mmdet.datasets. Multi Image Mix Data set ( dataset ,  pipeline ,  dynamic scale=None , \nA wrapper of multiple images mixed dataset. \nSuitable for training on multiple images mixed data augmentation like mosaic and mixup. For the augmentation pipeline of mixed image data, the  get indexes  method needs to be provided to obtain the image indexes, and you can set  skip_flags  to change the pipeline running process. At the same time, we provide the  dynamic scale parameter to dynamically change the output image size. "}
{"page": 243, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_243.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n¢ dataset (CustomDataset) — The dataset to be mixed.\n\n* pipeline (Sequence[dict]) — Sequence of transform object or config dict to be com-\nposed.\n\n* dynamic_scale (tuple[int], optional) — The image scale can be changed dynami-\ncally. Default to None.\n\n¢ skip_type_keys (list[str], optional) — Sequence of type string to be skip pipeline.\nDefault to None.\n\nupdate_dynamic_scale(dynamic_scale)\nUpdate dynamic_scale. It is called by an external hook.\n\nParameters dynamic_scale (tuple [int ]) — The image scale can be changed dynamically.\n\nupdate_skip_type_keys (skip_type_keys)\nUpdate skip_type_keys. It is called by an external hook.\n\nParameters skip_type_keys (list[str], optional) — Sequence of type string to be skip\npipeline.\n\nclass mmdet.datasets.RepeatDataset (dataset, times)\nA wrapper of repeated dataset.\n\nThe length of repeated dataset will be times larger than the original dataset. This is useful when the data loading\ntime is long but the dataset is small. Using RepeatDataset can reduce the data loading time between epochs.\n\nParameters\n* dataset (Dataset) — The dataset to be repeated.\n* times (int) — Repeat times.\n\nget_cat_ids (idx)\nGet category ids of repeat dataset by index.\n\nParameters idx (int) — Index of data.\nReturns All categories in the image of specified index.\nReturn type list[int]\n\nclass mmdet.datasets.VOCDataset (**kwargs)\n\nevaluate (results, metric='mAP', logger=None, proposal_nums=(100, 300, 1000), iou_thr=0.5,\nscale_ranges=None)\nEvaluate in VOC protocol.\n\nParameters\n* results (list[list | tuple])— Testing results of the dataset.\n* metric (str | list[str])- Metrics to be evaluated. Options are ‘mAP’, ‘recall’.\n\n¢ logger (logging.Logger | str, optional) -— Logger used for printing related infor-\nmation during evaluation. Default: None.\n\n* proposal_nums (Sequence [int ]) — Proposal number used for evaluating recalls, such\nas recall@ 100, recall @ 1000. Default: (100, 300, 1000).\n\n¢ iou_thr (float | list [float])—IoU threshold. Default: 0.5.\n\n236 Chapter 38. mmdet.datasets\n", "vlm_text": "Parameters \n•  dataset  ( Custom Data set ) – The dataset to be mixed. •  pipeline  ( Sequence[dict] ) – Sequence of transform object or config dict to be com- posed. •  dynamic scale  ( tuple[int], optional ) – The image scale can be changed dynami- cally. Default to None. •  skip type keys  ( list[str], optional ) – Sequence of type string to be skip pipeline. Default to None. \n\nUpdate dynamic scale. It is called by an external hook. Parameters  dynamic scale  ( tuple[int] ) – The image scale can be changed dynamically. \nupdate skip type keys(skip type keys)Update skip type keys. It is called by an external hook. Parameters  skip type keys  ( list[str], optional ) – Sequence of type string to be skip pipeline. \nclass  mmdet.datasets. Repeat Data set ( dataset ,  times ) A wrapper of repeated dataset. \nThe length of repeated dataset will be  times  larger than the original dataset. This is useful when the data loading time is long but the dataset is small. Using Repeat Data set can reduce the data loading time between epochs. \nParameters \n•  dataset  ( Dataset ) – The dataset to be repeated. •  times  ( int ) – Repeat times. \nget cat ids ( idx ) Get category ids of repeat dataset by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] \nclass  mmdet.datasets. VOCDataset ( \\*\\*kwargs ) \nevaluate ( results ,  metric  $\\scriptstyle=m A P^{\\prime}$  ,  logger  $\\leftrightharpoons$  None ,  proposal_nums=(100, 300, 1000) ,  iou_thr=0.5 , scale ranges  $=$  None ) Evaluate in VOC protocol. \nParameters \n•  results  ( list[list | tuple] ) – Testing results of the dataset. •  metric  ( str | list[str] ) – Metrics to be evaluated. Options are ‘mAP’, ‘recall’. •  logger  ( logging.Logger | str, optional ) – Logger used for printing related infor- mation during evaluation. Default: None. •  proposal num s  ( Sequence[int] ) – Proposal number used for evaluating recalls, such as  recall  $@$  100 ,  recall  $@$  1000 . Default: (100, 300, 1000). •  iou_thr  ( float | list[float] ) – IoU threshold. Default: 0.5. "}
{"page": 244, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_244.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ scale_ranges (list[tuple], optional) - Scale ranges for evaluating mAP. If not\nspecified, all bounding boxes would be included in evaluation. Default: None.\n\nReturns AP/recall metrics.\nReturn type dict[str, float]\n\nclass mmdet.datasets.WIDERFaceDataset (**kwargs)\nReader for the WIDER Face dataset in PASCAL VOC format.\n\nConversion scripts can be found in https://github.com/sovrasov/wider-face-pascal-voc-annotations\n\nload_annotations (ann_file)\nLoad annotation from WIDERFace XML style annotation file.\n\nParameters ann_file (str) — Path of XML file.\nReturns Annotation info from XML file.\nReturn type list[dict]\n\nclass mmdet.datasets.XMLDataset (min_size=None, img_subdir='JPEGImages', ann_subdir='Annotations',\n*“*kwargs)\nXML dataset for detection.\n\nParameters\n\n* min_size(int | float, optional) - The minimum size of bounding boxes in the im-\nages. If the size of a bounding box is less than min_size, it would be add to ignored field.\n\n¢ img_subdir (str) — Subdir where images are stored. Default: JPEGImages.\n¢ ann_subdir (str) — Subdir where annotations are. Default: Annotations.\n\nget_ann_info (idx)\nGet annotation from XML file by index.\n\nParameters idx (int) — Index of data.\nReturns Annotation info of specified index.\nReturn type dict\n\nget_cat_ids (idx)\nGet category ids in XML file by index.\n\nParameters idx (int) — Index of data.\nReturns All categories in the image of specified index.\nReturn type list[int]\n\nload_annotations (ann_file)\nLoad annotation from XML style ann_file.\n\nParameters ann_file (str) — Path of XML file.\nReturns Annotation info from XML file.\nReturn type list[dict]\n\nmmdet.datasets.build_dataloader (dataset, samples_per_gpu, workers_per_gpu, num_gpus=1, dist=True,\n\nshuffle=True, seed=None, runner_type='EpochBasedRunner', **kwargs)\nBuild PyTorch DataLoader.\n\nIn distributed training, each GPU/process has a dataloader. In non-distributed training, there is only one dat-\naloader for all GPUs.\n\n38.1. datasets 237\n", "vlm_text": "•  scale ranges  ( list[tuple], optional ) – Scale ranges for evaluating mAP. If not specified, all bounding boxes would be included in evaluation. Default: None. Returns  AP/recall metrics. Return type  dict[str, float] \nclass  mmdet.datasets. WIDER Face Data set ( \\*\\*kwargs ) Reader for the WIDER Face dataset in PASCAL VOC format. \nConversion scripts can be found in  https://github.com/sovrasov/wider-face-pascal-voc-annotations \nload annotations ( ann_file ) Load annotation from WIDERFace XML style annotation file. Parameters  ann_file  ( str ) – Path of XML file. Returns  Annotation info from XML file. Return type  list[dict] \nclass  mmdet.datasets. XMLDataset ( min_size  $=$  None ,  img_subdir  $\\bf{=}$  JPEGImages' ,  ann_subdir  $\\mathbf{\\dot{\\rho}}$  'Annotations' , \\*\\*kwargs ) XML dataset for detection. \n•  min_size  ( int | float, optional ) – The minimum size of bounding boxes in the im- ages. If the size of a bounding box is less than  min_size , it would be add to ignored field. •  img_subdir  ( str ) – Subdir where images are stored. Default: JPEGImages. •  ann_subdir    $(s t r)-S$  ubdir where annotations are. Default: Annotations. \nget ann info ( idx ) Get annotation from XML file by index. Parameters  idx  ( int ) – Index of data. Returns  Annotation info of specified index. Return type  dict get cat ids ( idx ) Get category ids in XML file by index. Parameters  idx  ( int ) – Index of data. Returns  All categories in the image of specified index. Return type  list[int] load annotations ( ann_file ) Load annotation from XML style ann_file. Parameters  ann_file  ( str ) – Path of XML file. Returns  Annotation info from XML file. Return type  list[dict] \nmmdet.datasets. build data loader ( dataset ,  samples per gpu ,  workers per gpu ,  num_gpus  $\\mathord{:=}I$  ,  dist  $\\mathbf{\\dot{\\rho}}$  True , shuffle  $\\mathbf{=}$  True ,  seed  $\\leftrightharpoons$  None ,  runner type  $\\mathbf{\\tilde{=}}$  'Epoch Based Runner' ,  \\*\\*kwargs ) Build PyTorch DataLoader. \nIn distributed training, each GPU/process has a dataloader. In non-distributed training, there is only one dat- aloader for all GPUs. "}
{"page": 245, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_245.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n* dataset (Dataset) — A PyTorch dataset.\n\n* samples_per_gpu (int) — Number of training samples on each GPU, i.e., batch size of\neach GPU.\n\n* workers_per_gpu (int) — How many subprocesses to use for data loading for each GPU.\n* num_gpus (int) — Number of GPUs. Only used in non-distributed training.\n¢ dist (bool) — Distributed training/test or not. Default: True.\n¢ shuffle (bool) — Whether to shuffle the data at every epoch. Default: True.\n* runner_type (str) — Type of runner. Default: EpochBasedRunner\n* kwargs — any keyword argument to be used to initialize DataLoader\nReturns A PyTorch dataloader.\nReturn type DataLoader\n\nmmdet .datasets.get_loading_pipeline (pipeline)\nOnly keep loading image and annotations related configuration.\n\nParameters pipeline (list [dict ]) — Data pipeline configs.\nReturns\nThe new pipeline list with only keep loading image and annotations related configuration.\n\nReturn type list[dict]\n\nExamples\n\n>>> pipelines = [\ndict (type='LoadImageFromFile'),\n\ndict(type='LoadAnnotations', with_bbox=True) ,\ndict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\ndict(type='RandomFlip', flip_ratio=0.5),\ndict(type='Normalize', **img_norm_cfg),\n\ndict(type='Pad', size_divisor=32),\n\ndict (type='DefaultFormatBundle'),\n\ndict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n\n>>> expected_pipelines = [\ndict (type='LoadImageFromFile'),\ndict(type='LoadAnnotations', with_bbox=True)\n\n>>> assert expected_pipelines == sae get_loading_pipeline(pipelines)\n\nmmdet .datasets.replace_ImageToTensor (pipelines)\nReplace the ImageToTensor transform in a data pipeline to DefaultFormatBundle, which is normally useful in\nbatch inference.\n\nParameters pipelines (list [dict]) — Data pipeline configs.\nReturns\n\nThe new pipeline list with all ImageToTensor replaced by DefaultFormatBundle.\n\n238 Chapter 38. mmdet.datasets\n", "vlm_text": "Parameters \n•  dataset  ( Dataset ) – A PyTorch dataset. •  samples per gpu  ( int ) – Number of training samples on each GPU, i.e., batch size of each GPU. •  workers per gpu  ( int ) – How many sub processes to use for data loading for each GPU. •  num_gpus  ( int ) – Number of GPUs. Only used in non-distributed training. •  dist  ( bool ) – Distributed training/test or not. Default: True. •  shuffle  ( bool ) – Whether to shuffle the data at every epoch. Default: True. •  runner type  ( str ) – Type of runner. Default:  Epoch Based Runner •  kwargs  – any keyword argument to be used to initialize DataLoader \nReturns  A PyTorch dataloader. Return type  DataLoader \nmmdet.datasets. get loading pipeline ( pipeline ) Only keep loading image and annotations related configuration. \nParameters  pipeline  ( list[dict] ) – Data pipeline configs. \nReturns \nThe new pipeline list with only keep  loading image and annotations related configuration. \nReturn type  list[dict] \nExamples \nThis image is a screenshot of a Python code snippet. The code is setting up two lists called `pipelines` and `expected_pipelines`. Both lists contain dictionaries that specify different processing steps, with a `type` key indicating the kind of operation, such as 'LoadImageFromFile', 'LoadAnnotations', 'Resize', and more.\n\n- The `pipelines` list includes a series of data processing steps for handling images and their annotations, including loading, resizing, flipping, normalizing, padding, formatting, and collecting data.\n- The `expected_pipelines` list includes only two elements, both related to loading images and annotations with bounding boxes.\n  \nFinally, there is an assertion statement that checks whether the `expected_pipelines` list is equal to the result of the function call `get_loading_pipeline(pipelines)`. \n\nThe code appears to be part of a data preprocessing step, potentially for a machine learning or computer vision project involving image data and annotations.\nmmdet.datasets. replace Image To Tensor ( pipelines ) Replace the Image To Tensor transform in a data pipeline to Default Format Bundle, which is normally useful in batch inference. \nParameters  pipelines  ( list[dict] ) – Data pipeline configs. \nReturns \nThe new pipeline list with all Image To Tensor replaced by  Default Format Bundle. "}
{"page": 246, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_246.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type list\n\nExamples\n\n>>> pipelines = [\ndict (type='LoadImageFromFile'),\ndict\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True) ,\ndict(type='RandomFlip'),\ndict(type='Normalize', mean=[0, 9, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\nsp)\nos ]\n>>> expected_pipelines = [\ndict(type='LoadImageFromFile'),\ndict(\ntype='MultiScaleFlipAug',\nimg_scale=(1333, 800),\nflip=False,\ntransforms=[\ndict(type='Resize', keep_ratio=True) ,\ndict(type='RandomFlip'),\ndict(type='Normalize', mean=[0, 9, 0], std=[1, 1, 1]),\ndict(type='Pad', size_divisor=32),\ndict (type='DefaultFormatBundle'),\ndict(type='Collect', keys=['img']),\nsp)\n]\n\n>>> assert expected_pipelines == replace_ImageToTensor (pipelines)\n\n38.2 pipelines\n\nclass mmdet.datasets.pipelines.Albu(transforms, bbox_params=None, keymap=None,\nupdate_pad_shape=False, skip_img_without_anno=False)\nAlbumentation augmentation.\n\nAdds custom transformations from Albumentations library. Please, visit https://albumentations.readthedocs.io\nto get more information.\n\nAn example of transforms is as followed:\n\n[\ndict(\ntype='ShiftScaleRotate',\nshift_limit=0.0625,\n\n(continues on next page)\n\n38.2. pipelines 239\n\n", "vlm_text": "Return type  list \nExamples \nThe image shows a Python code block, which defines two lists of dictionaries named `pipelines` and `expected_pipelines`. These dictionaries specify image processing operations for a machine learning pipeline. \n\nHere's a breakdown of the operations within each list:\n\n1. **`pipelines`:**\n   - `LoadImageFromFile`: Loads an image from a file.\n   - `MultiScaleFlipAug`: Performs augmentation with multiscale flipping.\n     - `img_scale`: Sets image scale to (1333, 800).\n     - `flip`: Sets flipping to False.\n     - `transforms`: Applies a series of transformations:\n       - `Resize`: Resizes image while keeping ratio.\n       - `RandomFlip`: Applies random flipping.\n       - `Normalize`: Normalizes the image with mean [0, 0, 0] and std [1, 1, 1].\n       - `Pad`: Pads the image with a size divisor of 32.\n       - `ImageToTensor`: Converts the image to a tensor.\n       - `Collect`: Collects the image data with key 'img'.\n\n2. **`expected_pipelines`:**\n   - Similar to `pipelines` but with a slight difference:\n     - Instead of `ImageToTensor`, it uses `DefaultFormatBundle`.\n\nThe code ends with an assertion:\n```python\nassert expected_pipelines == replace_ImageToTensor(pipelines)\n```\nThis checks if the `expected_pipelines` is the same as `pipelines` after replacing `ImageToTensor` with the appropriate transformation.\n38.2 pipelines \nclass  mmdet.datasets.pipelines. Albu ( transforms ,  b box params  $\\mathbf{\\hat{\\rho}}$  None ,  keymap  $\\mathbf{\\varepsilon}=$  None , update pad shape  $\\mathbf{=}$  False ,  skip img without anno=False ) \nAlbum ent ation augmentation. \nAdds custom transformations from Albumen tat ions library. Please, visit  https://albumen tat ions.read the docs.io to get more information. \nAn example of  transforms  is as followed: \nThe table contains code that appears to be part of a configuration or dictionary in Python, likely related to image data augmentation in machine learning:\n\n- `dict`: Indicates a dictionary structure.\n- `type='ShiftScaleRotate'`: Specifies the type of augmentation to be applied, which involves shifting, scaling, and rotating an image.\n- `shift_limit=0.0625`: Specifies the limit for shifting, probably referring to the maximum fraction of total image width or height.\n(continues on next page) "}
{"page": 247, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_247.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nscale_limit=0.0,\nrotate_limit=0,\ninterpolation=1,\np=0.5),\n\ndict(\ntype='RandomBrightnessContrast',\nbrightness_limit=[0.1, 0.3],\ncontrast_limit=[0.1, 0.3],\n\np=0.2),\ndict(type='ChannelShuffle', p=0.1),\ndict(\n\ntype='OneOf',\n\ntransforms=[\ndict(type='Blur', blur_limit=3, p=1.0),\ndict(type='MedianBlur', blur_limit=3, p=1.0)\n\n1,\np=0.1),\n]\nParameters\n* transforms (list [dict ])—A list of albu transformations\n* bbox_params (dict) — Bbox_params for albumentation Compose\n¢ keymap (dict) — Contains { ‘input key’:’albumentation-style key’ }\n¢ skip_img_without_anno (bool) — Whether to skip the image if no ann left after aug\nalbu_builder (cfg)\n\nImport a module from albumentations.\n\nIt inherits some of build_from_cfg() logic.\nParameters cfg (dict) — Config dict. It should at least contain the key “type”.\nReturns The constructed object.\nReturn type obj\n\nstatic mapper (d, keymap)\nDictionary mapper. Renames keys according to keymap provided.\n\nParameters\n\n¢ d (dict) — old dict\n\n¢ keymap (dict) — {‘old_key’:’new_key’}\nReturns new dict.\nReturn type dict\n\nclass mmdet.datasets.pipelines.AutoAugment (policies)\nAuto augmentation.\n\nThis data augmentation is proposed in Learning Data Augmentation Strategies for Object Detection.\n\nTODO: Implement ‘Shear’, ‘Sharpness’ and ‘Rotate’ transforms\n\n240 Chapter 38. mmdet.datasets\n", "vlm_text": "This image shows a snippet of code that appears to define image augmentation operations, possibly using a library like Albumentations. Here's a breakdown of the elements:\n\n1. **Scale, Rotate, Interpolation, Probability (`p`)**:\n   - `scale_limit=0.0`\n   - `rotate_limit=0`\n   - `interpolation=1`\n   - Applied with a probability of `p=0.5`\n\n2. **Random Brightness and Contrast**:\n   - `brightness_limit=[0.1, 0.3]`\n   - `contrast_limit=[0.1, 0.3]`\n   - Probability of applying: `p=0.2`\n\n3. **Channel Shuffle**:\n   - Probability: `p=0.1`\n\n4. **OneOf Transformations**:\n   - Types of blurs applied with different probabilities:\n     - `Blur`: `blur_limit=3`, probability `p=1.0`\n     - `MedianBlur`: `blur_limit=3`, probability `p=1.0`\n   - Overall probability for `OneOf`: `p=0.1`\n\nThese transformations are typically used in data preprocessing to enhance the diversity of training images.\nParameters \n•  transforms  ( list[dict] ) – A list of albu transformations •  b box params  ( dict ) – B box params for album ent ation  Compose •  keymap  ( dict ) – Contains {‘input key’:’album ent ation-style key’} •  skip img without anno  ( bool ) – Whether to skip the image if no ann left after aug \nalb u builder ( cfg ) Import a module from albumen tat ions. It inherits some of  build from cf g()  logic. Parameters  cfg  ( dict ) – Config dict. It should at least contain the key “type”. Returns  The constructed object. Return type  obj static mapper ( d ,  keymap ) Dictionary mapper. Renames keys according to keymap provided. Parameters  $\\begin{array}{r l}&{\\bullet\\,\\,\\,{\\mathsf{d}}\\,(d i c t)-{\\mathrm{old~dict}}}\\\\ &{\\bullet\\,\\,\\,{\\mathsf{k e y m a p}}\\,(d i c t)-\\{\\,^{\\ast}{\\mathrm{old}}\\_{\\mathrm{kcy}}^{\\ast}\\colon{\\mathrm{new}}\\_{\\mathrm{kcy}}^{\\ast}\\}}\\end{array}$  Returns  new dict. Return type  dict \nclass  mmdet.datasets.pipelines. Auto Augment ( policies ) Auto augmentation. \nThis data augmentation is proposed in  Learning Data Augmentation Strategies for Object Detection . TODO: Implement ‘Shear’, ‘Sharpness’ and ‘Rotate’ transforms \n"}
{"page": 248, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_248.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters policies (list [list [dict]])—The policies of auto augmentation. Each policy in\npolicies is a specific augmentation policy, and is composed by several augmentations (dict).\nWhen AutoAugment is called, a random policy in policies will be selected to augment images.\n\nExamples\n\n>>> replace = (104, 116, 124)\n\n>>> policies = [\n\n>>> [\n\n>>> dict(type='Sharpness', prob=0.0, level=8),\n>>> dict(\n\n>>> type='Shear',\n\n>>> prob=0.4,\n\n>>> level=0,\n\n>>> replace=replace,\n\n>>> axis='x')\n\n>>> 1,\n\n>>> [\n\n>>> dict(\n\n>>> type='Rotate',\n\n>>> prob=0.6,\n\n>>> level=10,\n\n>>> replace=replace),\n\n>>> dict(type='Color', prob=1.0, level=6)\n>>> ]\n\n>>> J\n\n>>> augmentation = AutoAugment (policies)\n\n>>> img = np.ones(100, 100, 3)\n\n>>> gt_bboxes = np.ones(10, 4)\n\n>>> results = dict(img=img, gt_bboxes=gt_bboxes)\n>>> results = augmentation(results)\n\nclass mmdet.datasets.pipelines.BrightnessTransform (level, prob=0.5)\nApply Brightness transformation to image. The bboxes, masks and segmentations are not modified.\n\nParameters\n¢ level (int | float) - Should be in range [0, MAX_LEVEL].\n* prob (float) — The probability for performing Brightness transformation.\n\nclass mmdet.datasets.pipelines.Collect (keys, meta_keys=(‘filename'’, ‘ori_filename’, ‘ori_shape’,\n‘img_shape’, '‘pad_shape’, 'scale_factor', ‘flip’, ‘flip_direction’,\n‘img_norm_cfg'))\nCollect data from the loader relevant to the specific task.\n\n99\n\nThis is usually the last stage of the data loader pipeline. Typically keys is set to some subset of “img”,\n\nmers ceed\n\n“gt_bboxes”, “gt_bboxes_ignore’”, “gt_labels”, and/or ““gt_masks”’.\n\nproposals”,\n\nThe “img_meta” item is always populated. The contents of the “img_meta” dictionary depends on “meta_keys”.\nBy default this includes:\n\n* “img_shape’’: shape of the image input to the network as a tuple (h, w, c). Note that images may be zero\npadded on the bottom/right if the batch tensor is larger than this shape.\n\n* “scale_factor”: a float indicating the preprocessing scale\n\n38.2. pipelines 241\n", "vlm_text": "Parameters  policies  ( list[list[dict]] ) – The policies of auto augmentation. Each policy in policies  is a specific augmentation policy, and is composed by several augmentations (dict). When Auto Augment is called, a random policy in  policies  will be selected to augment images. \nExamples \nThis image shows a code snippet that demonstrates how to use an auto-augmentation technique for image data. Here's a breakdown:\n\n1. **Replace Values**: \n   ```python\n   replace = (104, 116, 124)\n   ```\n   - This tuple is used for color replacement during augmentation.\n\n2. **Policies**:\n   ```python\n   policies = [\n       [\n           dict(type='Sharpness', prob=0.0, level=8),\n           dict(type='Shear', prob=0.4, level=0, replace=replace, axis='x')\n       ],\n       [\n           dict(type='Rotate', prob=0.6, level=10, replace=replace),\n           dict(type='Color', prob=1.0, level=6)\n       ]\n   ]\n   ```\n   - A list of policies is defined, with each policy containing different transformation operations such as 'Sharpness', 'Shear', 'Rotate', and 'Color'.\n   - Each operation specifies a type, probability (`prob`), level, and additional parameters specific to the transformation.\n\n3. **Applying AutoAugment**:\n   ```python\n   augmentation = AutoAugment(policies)\n   ```\n   - An `AutoAugment` object is created using the defined policies.\n\n4. **Creating Image Data**:\n   ```python\n   img = np.ones((100, 100, 3))\n   gt_bboxes = np.ones((10, 4))\n   ```\n   - Sample image data (`img`) and ground truth bounding boxes (`gt_bboxes`) are created using `numpy`.\n\n5. **Applying Augmentation**:\n   ```python\n   results = dict(img=img, gt_bboxes=gt_bboxes)\n   results = augmentation(results)\n   ```\n   - The image data and bounding boxes are passed into a dictionary called `results`.\n   - The augmentation is applied to this data. \n\nThis code provides a basic example of setting up auto-augmentation with specific policies to enhance image datasets.\nclass  mmdet.datasets.pipelines. Brightness Transform ( level ,  pro  $\\wp{=}0.5.$  ) Apply Brightness transformation to image. The bboxes, masks and segmentation s are not modified. \nParameters \n•  level  ( int | float ) – Should be in range [0,_MAX_LEVEL]. \n•  prob  ( float ) – The probability for performing Brightness transformation. \nclass  mmdet.datasets.pipelines. Collect ( keys ,  meta_keys  $=$  ('filename', 'ori filename', 'ori_shape', 'img_shape', 'pad_shape', 'scale factor', 'flip', 'flip direction', 'img norm cf g'))\nCollect data from the loader relevant to the specific task. \nThis is usually the last stage of the data loader pipeline. Typically keys is set to some subset of “img”, “proposals”, “gt_bboxes”, “gt b boxes ignore”, “gt_labels”, and/or “gt_masks”. \nThe “img_meta” item is always populated. The contents of the “img_meta” dictionary depends on “meta_keys”. By default this includes: \n• “img_shape”: shape of the image input to the network as a tuple (h, w, c). Note that images may be zero padded on the bottom/right if the batch tensor is larger than this shape. • “scale factor”: a float indicating the preprocessing scale "}
{"page": 249, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_249.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* “flip”: a boolean indicating if image flip transform was used\n* “filename”: path to the image file\n* “ori_shape”: original shape of the image as a tuple (h, w, c)\n* “pad_shape”: image shape after padding\n* “img_norm_cfg”: a dict of normalization information:\n\n— mean - per channel mean subtraction\n\n— std - per channel std divisor\n\n— to_rgb - bool indicating if bgr was converted to rgb\n\nParameters\n\n* keys (Sequence[str]) — Keys of results to be collected in data.\n\n*meta_keys (Sequence[str], optional) - Meta keys to be converted\nto mmcv.DataContainer and collected in data[img_metas]. Default:\n('filename', 'ori_filename', 'ori_shape', 'img_shape', 'pad_shape',\n\"scale_factor', 'flip', 'flip_direction', 'img_norm_cfg')\n\nclass mmdet.datasets.pipelines.ColorTransform (level, prob=0.5)\nApply Color transformation to image. The bboxes, masks, and segmentations are not modified.\n\nParameters\n¢ level (int | float) - Should be in range [0, MAX_LEVEL].\n* prob (float) — The probability for performing Color transformation.\n\nclass mmdet.datasets.pipelines.Compose (transforms)\nCompose multiple transforms sequentially.\n\nParameters transforms (Sequence[dict | callable])—Sequence of transform object or con-\nfig dict to be composed.\n\nclass mmdet.datasets.pipelines.ContrastTrans form (level, prob=0.5)\nApply Contrast transformation to image. The bboxes, masks and segmentations are not modified.\n\nParameters\n¢ level (int | float) - Should be in range [0, MAX_LEVEL].\n* prob (float) — The probability for performing Contrast transformation.\n\nclass mmdet.datasets.pipelines.CutOut (n_holes, cutout_shape=None, cutout_ratio=None, fill_in=(0, 0, 0))\nCutOut operation.\n\nRandomly drop some regions of image used in Cutout.\nParameters\n\n¢« n_holes (int | tuple[int, int]) — Number of regions to be dropped. If it is given\nas a list, number of holes will be randomly selected from the closed interval [n_holes/0],\nn_holes[1]].\n\n* cutout_shape (tuplefint, int] | list[tuple[int, int]]) — The candidate\nshape of dropped regions. It can be tuple/int, int] to use a fixed cutout shape, or list{tuple[int,\nint]] to randomly choose shape from the list.\n\n242 Chapter 38. mmdet.datasets\n", "vlm_text": "• “flip”: a boolean indicating if image flip transform was used • “filename”: path to the image file • “ori_shape”: original shape of the image as a tuple (h, w, c) • “pad_shape”: image shape after padding • “img norm cf g”: a dict of normalization information: –  mean - per channel mean subtraction –  std - per channel std divisor –  to_rgb - bool indicating if bgr was converted to rgb \nParameters \n•  keys  ( Sequence[str] ) – Keys of results to be collected in  data . •  meta_keys ( Sequence[str], optional ) – Meta keys to be converted to mmcv.Data Container and collected in data[img_metas] . Default: ( ' filename ' ,  ' ori filename ' ,  ' ori_shape ' ,  ' img_shape ' ,  ' pad_shape ' , ' scale factor ' ,  ' flip ' ,  ' flip direction ' ,  ' img norm cf g ' ) \nclass  mmdet.datasets.pipelines. Color Transform ( level ,  pro  $\\jmath{=}0.5$  ) Apply Color transformation to image. The bboxes, masks, and segmentation s are not modified. \nParameters \n•  level  ( int | float ) – Should be in range [0,_MAX_LEVEL]. •  prob  ( float ) – The probability for performing Color transformation. \nclass  mmdet.datasets.pipelines. Compose ( transforms ) Compose multiple transforms sequentially. \nParameters  transforms  ( Sequence[dict | callable] ) – Sequence of transform object or con- fig dict to be composed. \nclass  mmdet.datasets.pipelines. Contrast Transform ( level ,  prob=0.5 ) Apply Contrast transformation to image. The bboxes, masks and segmentation s are not modified. \nParameters \n•  level  ( int | float ) – Should be in range [0,_MAX_LEVEL]. •  prob  ( float ) – The probability for performing Contrast transformation. \nclass  mmdet.datasets.pipelines. CutOut ( n_holes ,  cut out shape  $=$  None ,  cut out ratio  $=$  None ,  fill_in=(0, 0, 0) ) CutOut operation. \nRandomly drop some regions of image used in  Cutout . \nParameters \n•  n_holes  ( int | tuple[int, int] ) – Number of regions to be dropped. If it is given as a list, number of holes will be randomly selected from the closed interval [ n_holes[0] , n_holes[1] ]. •  cut out shape  ( tuple[int, int] | list[tuple[int, int]] ) – The candidate shape of dropped regions. It can be  tuple[int, int]  to use a fixed cutout shape, or  list[tuple[int, int]]  to randomly choose shape from the list. "}
{"page": 250, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_250.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* cutout_ratio(tuple[float, float] | list[tuple[float, float]])-—Thecan-\ndidate ratio of dropped regions. It can be tuple/float, float] to use a fixed ratio or\nlist{tuple[float, float]] to randomly choose ratio from the list. Please note that cutout_shape\nand cutout_ratio cannot be both given at the same time.\n\n¢ fill_in(tuple[float, float, float] | tuple[int, int, int])-—The value of\npixel to fill in the dropped regions. Default: (0, 0, 0).\n\nclass mmdet.datasets.pipelines.DefaultFormatBundle\nDefault formatting bundle.\n\nPees 99 ee, 99 ee,\n\nIt simplifies the pipeline of formatting common fields, including “img”, “proposals”, “gt_bboxes”, “gt_labels”,\n“gt_masks” and “gt_semantic_seg”. These fields are formatted as follows.\n\n* img: (1)transpose, (2)to tensor, (3)to DataContainer (stack=True)\n\n* proposals: (1)to tensor, (2)to DataContainer\n\n* gt_bboxes: (1)to tensor, (2)to DataContainer\n\n* gt_bboxes_ignore: (1)to tensor, (2)to DataContainer\n\n* gt_labels: (1)to tensor, (2)to DataContainer\n\n* gt_masks: (1)to tensor, (2)to DataContainer (cpu_only=True)\n\n* gt_semantic_seg: (1)unsqueeze dim-0 (2)to tensor, (3)to DataContainer (stack=True)\n\nclass mmdet.datasets.pipelines.EqualizeTransform(prob=0.5)\nApply Equalize transformation to image. The bboxes, masks and segmentations are not modified.\n\nParameters prob (float) — The probability for performing Equalize transformation.\n\nclass mmdet.datasets.pipelines.Expand(mean=(0, 0, 0), to_rgb=True, ratio_range=(1, 4),\nseg_ignore_label=None, prob=0.5)\nRandom expand the image & bboxes.\n\nRandomly place the original image on a canvas of ‘ratio’ x original image size filled with mean values. The ratio\nis in the range of ratio_range.\n\nParameters\n* mean (tuple) — mean value of dataset.\n* to_rgb (boo1) — if need to convert the order of mean to align with RGB.\n* ratio_range (tuple) — range of expand ratio.\n* prob (float) — probability of applying this transformation\n\nclass mmdet.datasets.pipelines.ImageToTensor (keys)\nConvert image to torch. Tensor by given keys.\n\nThe dimension order of input image is (H, W, C). The pipeline will convert it to (C, H, W). If only 2 dimension\n(H, W) is given, the output would be (1, H, W).\nParameters keys (Sequence [str]) — Key of images to be converted to Tensor.\nclass mmdet.datasets.pipelines.InstaBoost (action_candidate=('normal', ‘horizontal’, ‘skip'),\naction_prob=(1, 0, 0), scale=(0.8, 1.2), dx=15, dy=15,\ntheta=(- 1, 1), color_prob=0.5, hflag=False, aug_ratio=0.5)\n\nData augmentation method in InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy-\nPasting.\n\nRefer to https://github.com/GothicAi/Instaboost for implementation details.\n\n38.2. pipelines 243\n", "vlm_text": "•  cut out ratio  ( tuple[float, float] | list[tuple[float, float]] ) – The can- didate ratio of dropped regions. It can be  tuple[float, float]  to use a fixed ratio or list[tuple[float, float]]  to randomly choose ratio from the list. Please note that  cut out shape and  cut out ratio  cannot be both given at the same time. •  fill_in  ( tuple[float, float, float] | tuple[int, int, int] ) – The value of pixel to fill in the dropped regions. Default:   $(0,\\,0,\\,0)$  . \nclass  mmdet.datasets.pipelines. Default Format Bundle Default formatting bundle. \nIt simplifies the pipeline of formatting common fields, including “img”, “proposals”, “gt_bboxes”, “gt_labels”, “gt_masks” and “gt semantic seg”. These fields are formatted as follows. \n• img: (1)transpose, (2)to tensor, (3)to Data Container (stack  $\\risingdotseq$  True) • proposals: (1)to tensor, (2)to Data Container • gt_bboxes: (1)to tensor, (2)to Data Container • gt b boxes ignore: (1)to tensor, (2)to Data Container • gt_labels: (1)to tensor, (2)to Data Container • gt_masks: (1)to tensor, (2)to Data Container (cpu_only  $\\mathbf{\\acute{=}}$  True) • gt semantic seg: (1)unsqueeze dim-0 (2)to tensor, (3)to Data Container (stack  $\\risingdotseq$  True) \nclass  mmdet.datasets.pipelines. Equalize Transform  $(p r o b{=}0.5)$  ) \nApply Equalize transformation to image. The bboxes, masks and segmentation s are not modified. Parameters  prob  ( float ) – The probability for performing Equalize transformation. \nclass  mmdet.datasets.pipelines. Expand ( mean=(0, 0, 0) ,  to_rgb  $\\mathbf{\\delta}){=}\\mathbf{\\delta}$  True ,  ratio range=(1, 4) , seg ignore la be  $\\leftrightharpoons$  None ,  pro  $\\scriptstyle b=0.5$  ) \nRandom expand the image & bboxes. \nRandomly place the original image on a canvas of ‘ratio’  $\\mathbf{X}$   original image size filled with mean values. The ratio is in the range of ratio range. \nParameters \n•  mean  ( tuple ) – mean value of dataset. •  to_rgb  ( bool ) – if need to convert the order of mean to align with RGB. •  ratio range  ( tuple ) – range of expand ratio. •  prob  ( float ) – probability of applying this transformation \nclass  mmdet.datasets.pipelines. Image To Tensor ( keys ) \nConvert image to  torch.Tensor  by given keys. \nThe dimension order of input image is (H, W, C). The pipeline will convert it to (C, H, W). If only 2 dimension (H, W) is given, the output would be (1, H, W). \nParameters  keys  ( Sequence[str] ) – Key of images to be converted to Tensor. \nclass  mmdet.datasets.pipelines. InstaBoost ( action candidate  $\\mathbf{=}$  ('normal', 'horizontal', 'skip') , action_prob=(1, 0, 0) ,  scale  $\\mathbf{=}$  (0.8, 1.2) ,    $d x{=}I5$  ,    $\\scriptstyle d y=I S$  , \ntheta=(- 1, 1) ,  color_prob  $\\backsimeq\\!O.5$  ,  hflag  $\\mathbf{\\hat{\\mu}}$  False ,  aug_ratio  $\\backsimeq\\!O.5$  ) \nData augmentation method in  InstaBoost: Boosting Instance Segmentation Via Probability Map Guided Copy- Pasting . \nRefer to  https://github.com/GothicAi/Instaboost  for implementation details. "}
{"page": 251, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_251.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\n* action_candidate (tuple) — Action candidates. “normal”, “horizontal”, “vertical”,\n“skip” are supported. Default: (‘normal’, ‘horizontal’, ‘skip’).\n\n* action_prob (tuple) — Corresponding action probabilities. Should be the same length as\naction_candidate. Default: (1, 0, 0).\n\n¢ scale (tuple) — (min scale, max scale). Default: (0.8, 1.2).\n\n¢ dx (int) — The maximum x-axis shift will be (instance width) / dx. Default 15.\n\n* dy (int) — The maximum y-axis shift will be (instance height) / dy. Default 15.\n\n* theta (tuple) — (min rotation degree, max rotation degree). Default: (-1, 1).\n\n* color_prob (float) — Probability of images for color augmentation. Default 0.5.\n* heatmap_flag (bool) — Whether to use heatmap guided. Default False.\n\n* aug_ratio (float) — Probability of applying this transformation. Default 0.5.\n\nclass mmdet.datasets.pipelines.LoadAnnotations (with_bbox=True, with_label=True, with_mask=False,\nwith_seg=False, poly2mask=True,\nfile_client_args={‘backend': ‘disk'})\nLoad multiple types of annotations.\n\nParameters\n* with_bbox (bool) — Whether to parse and load the bbox annotation. Default: True.\n¢ with_label (bool) — Whether to parse and load the label annotation. Default: True.\n¢ with_mask (bool) — Whether to parse and load the mask annotation. Default: False.\n\n* with_seg (bool) — Whether to parse and load the semantic segmentation annotation. De-\nfault: False.\n\n* poly2mask (bool) — Whether to convert the instance masks from polygons to bitmaps. De-\nfault: True.\n\n¢ file_client_args (dict) — Arguments to instantiate a FileClient. See mmcv.fileio.\nFileClient for details. Defaults to dict (backend='disk').\n\nprocess_polygons (polygons)\nConvert polygons to list of ndarray and filter invalid polygons.\n\nParameters polygons (list [list]) — Polygons of one instance.\nReturns Processed polygons.\nReturn type list{numpy.ndarray]\n\nclass mmdet.datasets.pipelines.LoadImageFromFile (to_float32=False, color_type='color',\nfile_client_args={‘backend': ‘disk'})\nLoad an image from file.\nRequired keys are “img_prefix” and “img_info” (a dict that must contain the key “filename’’). Added or updated\nkeys are “filename”, “img”, “img_shape”, “ori_shape” (same as img_shape), “‘pad_shape” (same as img_shape),\n“scale_factor” (1.0) and “img_norm_cfg” (means=0 and stds=1).\n\nParameters\n\n* to_float32 (bool) — Whether to convert the loaded image to a float32 numpy array. If set\nto False, the loaded image is an uint8 array. Defaults to False.\n\n* color_type (str) — The flag argument for mmcv.imfrombytes(). Defaults to ‘color’.\n\n244 Chapter 38. mmdet.datasets\n", "vlm_text": "Parameters \n•  action candidate  ( tuple ) – Action candidates. “normal”, “horizontal”, “vertical”, “skip” are supported. Default: (‘normal’, ‘horizontal’, ‘skip’). •  action pro b  ( tuple ) – Corresponding action probabilities. Should be the same length as action candidate. Default: (1, 0, 0). •  scale  ( tuple ) – (min scale, max scale). Default: (0.8, 1.2). •  dx  ( int ) – The maximum x-axis shift will be (instance width) / dx. Default 15. •  dy  ( int ) – The maximum y-axis shift will be (instance height) / dy. Default 15. •  theta  ( tuple ) – (min rotation degree, max rotation degree). Default: (-1, 1). •  color_prob  ( float ) – Probability of images for color augmentation. Default 0.5. •  heat map flag  ( bool ) – Whether to use heatmap guided. Default False. •  aug_ratio  ( float ) – Probability of applying this transformation. Default 0.5. \nclass  mmdet.datasets.pipelines. Load Annotations ( with_bbox  $\\mathbf{\\beta}=$  True ,  with_labe  $\\acute{=}$  True ,  with_mask  $\\leftrightharpoons$  False , with_seg  $\\mathbf{\\hat{\\mu}}$  False ,  poly2mask  $\\mathbf{\\tilde{\\rho}}$  True , file client arg s  $=$  {'backend': 'disk'} ) \nLoad multiple types of annotations. \nParameters \n•  with_bbox  ( bool ) – Whether to parse and load the bbox annotation. Default: True. •  with_label  ( bool ) – Whether to parse and load the label annotation. Default: True. •  with_mask  ( bool ) – Whether to parse and load the mask annotation. Default: False. •  with_seg  ( bool ) – Whether to parse and load the semantic segmentation annotation. De- fault: False. •  poly2mask  ( bool ) – Whether to convert the instance masks from polygons to bitmaps. De- fault: True. •  file client arg s  ( dict ) – Arguments to instantiate a FileClient. See  mmcv.fileio. FileClient  for details. Defaults to  dict(backend= ' disk ' ) . \nprocess polygons ( polygons ) Convert polygons to list of ndarray and filter invalid polygons. \nParameters  polygons  ( list[list] ) – Polygons of one instance. Returns  Processed polygons. Return type  list[numpy.ndarray] \nclass  mmdet.datasets.pipelines. Load Image From File ( to_float32  $\\leftrightharpoons$  False ,  color_type  $=$  'color' , file client arg  $s{=}i$  {'backend': 'disk'} ) \nLoad an image from file. \nRequired keys are “img_prefix” and “img_info” (a dict that must contain the key “filename”). Added or updated keys are “filename”, “img”, “img_shape”, “ori_shape” (same as  img_shape ), “pad_shape” (same as  img_shape ), “scale factor” (1.0) and “img norm cf g” (means  $\\mathrm{{=}0}$   and stds  $_{\\ast=1}$  ). \nParameters \n•  to_float32  ( bool ) – Whether to convert the loaded image to a float32 numpy array. If set to False, the loaded image is an uint8 array. Defaults to False. •  color_type  ( str ) – The flag argument for  mmcv.im from bytes() . Defaults to ‘color’. "}
{"page": 252, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_252.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ file_client_args (dict) — Arguments to instantiate a FileClient. See mmcv.fileio.\nFileClient for details. Defaults to dict (backend='disk').\n\nclass mmdet.datasets.pipelines.LoadImageFromWebcam (to_float32=False, color_type='color',\nfile_client_args={‘backend': ‘disk'})\nLoad an image from webcam.\n\nSimilar with LoadImageFromFile, but the image read from webcam is in results['img'].\n\nclass mmdet.datasets.pipelines.LoadMultiChannelImageFromFiles (to_float32=False,\ncolor_type='unchanged',\nfile_client_args={'backend':\n‘disk'})\nLoad multi-channel images from a list of separate channel files.\n\nRequired keys are “img_prefix” and “img_info” (a dict that must contain the key “filename”, which is expected\n\nees\n\nto be a list of filenames). Added or updated keys are “filename”, “img”, “img shape”, “ori_shape” (same\nas img_shape), “pad_shape” (same as img_shape), “scale_factor” (1.0) and “img_norm_cfg” (means=0 and\nstds=1).\n\nParameters\n\n* to_float32 (bool) — Whether to convert the loaded image to a float32 numpy array. If set\nto False, the loaded image is an uint8 array. Defaults to False.\n\n* color_type (str) — The flag argument for mmcv.imfrombytes(). Defaults to ‘color’.\n\n¢ file_client_args (dict) — Arguments to instantiate a FileClient. See mmcv.fileio.\nFileClient for details. Defaults to dict (backend='disk').\n\nclass mmdet.datasets.pipelines.LoadProposals (num_max_proposals=None)\nLoad proposal pipeline.\n\nRequired key is “proposals”. Updated keys are “proposals”, “bbox_fields”’.\n\nParameters num_max_proposals (int, optional) — Maximum number of proposals to load. If\nnot specified, all proposals will be loaded.\n\nclass mmdet.datasets.pipelines.MinIoURandomCrop (min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3,\nbbox_clip_border=True)\nRandom crop the image & bboxes, the cropped patches have minimum IoU requirement with original image &\nbboxes, the IoU threshold is randomly selected from min_ious.\n\nParameters\n¢ min_ious (tuple) — minimum IoU threshold for all intersections with\n* boxes (bounding) —\n* min_crop_size (float) — minimum crop’s size (i.e. h,w := a*h, a*w,\n¢* a >= min_crop_size) (where) —\n\n* bbox_clip_border (bool, optional) — Whether clip the objects outside the border of\nthe image. Defaults to True.\n\nNote: The keys for bboxes, labels and masks should be paired. That is, gt_bboxes corresponds to gt_labels and\ngt_masks, and gt_bboxes_ignore to gt_labels_ignore and gt_masks_ignore.\n\n38.2. pipelines 245\n", "vlm_text": "•  file client arg s  ( dict ) – Arguments to instantiate a FileClient. See  mmcv.fileio. FileClient  for details. Defaults to  dict(backend  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' disk ' ) . \nclass  mmdet.datasets.pipelines. Load Image From Webcam ( to_float  $32{=}$  False ,  color_type  $\\mathbf{=}$  'color' , file client arg  $s{=}$  {'backend': 'disk'} ) \nLoad an image from webcam. \nSimilar with  Load Image From File , but the image read from webcam is in  results[ ' img ' ] . \nclass  mmdet.datasets.pipelines. Load MultiChannel Image From Files ( to_float3  $\\boldsymbol{\\mathrm{2}}\\!=\\!\\boldsymbol{\\mathrm{1}}$  False , color_type  $\\mathbf{=}$  'unchanged' , file client arg s={'backend': 'disk'} ) \nLoad multi-channel images from a list of separate channel files. \nRequired keys are “img_prefix” and “img_info” (a dict that must contain the key “filename”, which is expected to be a list of filenames). Added or updated keys are “filename”, “img”, “img_shape”, “ori_shape” (same as  img_shape ), “pad_shape” (same as  img_shape ), “scale factor” (1.0) and “img norm cf g” (means  $\\mathrm{=}0$   and stds  $^{=1}$  ). \nParameters \n•  to_float32  ( bool ) – Whether to convert the loaded image to a float32 numpy array. If set to False, the loaded image is an uint8 array. Defaults to False. •  color_type  ( str ) – The flag argument for  mmcv.im from bytes() . Defaults to ‘color’. •  file client arg s  ( dict ) – Arguments to instantiate a FileClient. See  mmcv.fileio. FileClient  for details. Defaults to  dict(backend= ' disk ' ) . \nclass  mmdet.datasets.pipelines. Load Proposals ( num max proposals  $:=$  None ) Load proposal pipeline. \nRequired key is “proposals”. Updated keys are “proposals”, “b box fields”. \nParameters  num max proposals  ( int, optional ) – Maximum number of proposals to load. If not specified, all proposals will be loaded. \nclass  mmdet.datasets.pipelines. MinI oU Random Crop ( min_ious=(0.1, 0.3, 0.5, 0.7, 0.9) ,  min crop s iz  $\\it{z}{=}0.3$  , b box clip border=True)Random crop the image & bboxes, the cropped patches have minimum IoU requirement with original image & bboxes, the IoU threshold is randomly selected from min_ious. \nParameters \n•  min_ious  ( tuple ) – minimum IoU threshold for all intersections with •  boxes  ( bounding ) – •  min crop size  ( float ) – minimum crop’s size (i.e. h,w :  $=\\mathrm{a}^{*}\\mathrm{h}$  ,  $\\mathrm{a}^{*}\\mathrm{w}$  , •  a  $>=$   min crop size)  ( where ) – •  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. \nNote:  The keys for bboxes, labels and masks should be paired. That is,  gt_bboxes  corresponds to  gt_labels  and gt_masks , and  gt b boxes ignore  to  gt labels ignore  and  gt masks ignore . "}
{"page": 253, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_253.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.datasets.pipelines .MixUp(img_scale=(640, 640), ratio_range=(0.5, 1.5), flip_ratio=0.5,\n\npad_val=114, max_iters=15, min_bbox_size=5,\nmin_area_ratio=0.2, max_aspect_ratio=20)\n\nMixUp data augmentation.\n\nParameters\n\nimg_scale (Sequence[int]) — Image output size after mixup pipeline. Default: (640,\n640).\n\nratio_range (Sequence[float]) — Scale ratio of mixup image. Default: (0.5, 1.5).\nflip_ratio (float) — Horizontal flip ratio of mixup image. Default: 0.5.\npad_val (int) — Pad value. Default: 114.\n\nmax_iters (int)—The maximum number of iterations. If the number of iterations is greater\nthan max_iters, but gt_bbox is still empty, then the iteration is terminated. Default: 15.\n\nmin_bbox_size (float) — Width and height threshold to filter bboxes. If the height or\nwidth of a box is smaller than this value, it will be removed. Default: 5.\n\nmin_area_ratio (float) — Threshold of area ratio between original bboxes and wrapped\nbboxes. If smaller than this value, the box will be removed. Default: 0.2.\n\nmax_aspect_ratio (float) — Aspect ratio of width and height threshold to filter bboxes.\nIf max(h/w, w/h) larger than this value, the box will be removed. Default: 20.\n\nget_indexes (dataset)\nCall function to collect indexes.\n\nParameters dataset (MultiImageMixDataset) — The dataset.\n\nReturns indexes.\n\nReturn type list\n\nclass mmdet.datasets.pipelines.Mosaic(img_scale=(640, 640), center_ratio_range=(0.5, 1.5),\n\nmin_bbox_size=0, pad_val=114)\n\nMosaic augmentation.\n\nGiven 4 images, mosaic transform combines them into one output image. The output image is composed of the\n\nparts from each sub- image.\n\ncenter_y\n\nmosaic transform\n\ncenter_x\n4$------------------------------ +\npad pad\n+----------- +\n|\n| imagel = |-------- +\n| |\n| image2 |\n----4------------- poo---------\n| cropped\npad | image3 image4\n|\n+----|------------- 4+----------- +\n|\n4+------------- +\n\n(continues on next page)\n\n246\n\nChapter 38. mmdet.datasets\n\n", "vlm_text": "class  mmdet.datasets.pipelines. MixUp ( img_scale  $\\scriptstyle\\sum=$  (640, 640) ,  ratio range  $\\scriptstyle\\left=|\\begin{array}{l}{\\begin{array}{r l}\\end{array}}\\end{array}\\right.$  (0.5, 1.5) ,  flip_ratio  $\\bullet{=}0.5$  , pad_va  $\\scriptstyle{l=l}I4$  ,  max_iters  $\\scriptstyle{\\prime=}I S$  ,  min b box s iz  $e{=}5$  , min area ratio  $\\scriptstyle{\\iota=0.2}$  ,  max aspect ratio  $\\bullet{=}2O.$  ) \nMixUp data augmentation. \nParameters \n•  img_scale  ( Sequence[int] ) – Image output size after mixup pipeline. Default: (640, 640). •  ratio range  ( Sequence[float] ) – Scale ratio of mixup image. Default: (0.5, 1.5). •  flip_ratio  ( float ) – Horizontal flip ratio of mixup image. Default: 0.5. •  pad_val  ( int ) – Pad value. Default: 114. •  max_iters  ( int ) – The maximum number of iterations. If the number of iterations is greater than  max_iters , but gt_bbox is still empty, then the iteration is terminated. Default: 15. •  min b box size  ( float ) – Width and height threshold to filter bboxes. If the height or width of a box is smaller than this value, it will be removed. Default: 5. •  min area ratio  ( float ) – Threshold of area ratio between original bboxes and wrapped bboxes. If smaller than this value, the box will be removed. Default: 0.2. •  max aspect ratio  ( float ) – Aspect ratio of width and height threshold to filter bboxes. If max(h/w, w/h) larger than this value, the box will be removed. Default: 20. \nget indexes ( dataset ) Call function to collect indexes. Parameters  dataset  ( Multi Image Mix Data set ) – The dataset. Returns  indexes. Return type  list \nclass  mmdet.datasets.pipelines. Mosaic ( img_scale  $=$  (640, 640) ,  center ratio range  $\\mathbf{\\tilde{=}}$  (0.5, 1.5) , min b box size  $\\mathrm{=}0$  ,  pad_va  $\\scriptstyle{l=I I4.}$  ) \nMosaic augmentation. \nGiven 4 images, mosaic transform combines them into one output image. The output image is composed of the parts from each sub- image. \nThe table illustrates a \"mosaic transform\" layout commonly used in image augmentation for machine learning. It is organized as a 2x2 grid:\n\n- **Upper-left**: Padding (\"pad\").\n- **Upper-right**: Image 1 and Image 2, with padding on the left and top.\n- **Lower-left**: Cropped Image 3 and padding on the left.\n- **Lower-right**: Image 4.\n\nThe transformation involves positioning images in a grid format with potential padding and cropping adjustments. This can help improve the diversity of the training data by combining different images."}
{"page": 254, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_254.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nThe mosaic transform steps are as follows:\n\n1. Choose the mosaic center as the intersections of 4 images\n\n2. Get the left top image according to the index, and randomly\nsample another 3 images from the custom dataset.\n\n3. Sub image will be cropped if image is larger than mosaic patch\n\nParameters\n\n* img_scale (Sequence [int]) — Image size after mosaic pipeline of single image. Default\nto (640, 640).\n\n* center_ratio_range (Sequence [float ]) — Center ratio range of mosaic output. Default\nto (0.5, 1.5).\n\n* min_bbox_size (int / float)—The minimum pixel for filtering invalid bboxes after the\nmosaic pipeline. Default to 0.\n\n* pad_val (int) — Pad value. Default to 114.\nget_indexes (dataset)\nCall function to collect indexes.\nParameters dataset (MultiImageMixDataset) — The dataset.\nReturns indexes.\nReturn type list\n\nclass mmdet.datasets.pipelines.MultiScaleFlipAug (transforms, img_scale=None, scale_factor=None,\nflip=False, flip_direction='horizontal')\nTest-time augmentation with multiple scales and flipping.\n\nAn example configuration is as followed:\n\nimg_scale=[(1333, 400), (1333, 800)],\n\nflip=True,\n\ntrans forms=[\ndict(type='Resize', keep_ratio=True) ,\ndict (type='RandomFlip')\ndict(type='Normalize', img_norm_cfg) ,\ndict(type='Pad', size_divisor=32),\ndict(type='ImageToTensor', keys=['img']),\ndict(type='Collect', keys=['img']),\n\n]\nAfter MultiScaleFLipAug with above configuration, the results are wrapped into lists of the same length as\nfollowed:\ndict(\nimg=[...],\n\nimg_shape=[...],\nscale=[(1333, 400), (1333, 400), (1333, 800), (1333, 800)]\nflip=[False, True, False, True]\n\n38.2. pipelines 247\n", "vlm_text": "The table contains steps for executing a mosaic transform:\n\n1. Choose the mosaic center as the intersections of 4 images.\n2. Get the top left image according to the index, and randomly sample another 3 images from the custom dataset.\n3. Sub image will be cropped if the image is larger than the mosaic patch.\nParameters \n•  img_scale  ( Sequence[int] ) – Image size after mosaic pipeline of single image. Default to (640, 640). •  center ratio range  ( Sequence[float] ) – Center ratio range of mosaic output. Default to (0.5, 1.5). •  min b box size  ( int | float ) – The minimum pixel for filtering invalid bboxes after the mosaic pipeline. Default to 0. •  pad_val  ( int ) – Pad value. Default to 114. \nget indexes ( dataset ) Call function to collect indexes. Parameters  dataset  ( Multi Image Mix Data set ) – The dataset. \nReturns  indexes. \nReturn type  list \nclass  mmdet.datasets.pipelines. Multi Scale Flip Aug ( transforms ,  img_scale  $\\mathbf{\\dot{\\rho}}$  None ,  scale factor=None , flip=False ,  flip direction  $=$  'horizontal' ) \nTest-time augmentation with multiple scales and flipping. \nAn example configuration is as followed: \nThe image you provided is a code snippet that appears to define image processing settings typically used in a deep learning or computer vision pipeline. Here's a breakdown of the contents:\n\n- `img_scale`: A list of tuples defining the target sizes for image scaling. In this case, it includes two tuples: `(1333, 400)` and `(1333, 800)`.\n\n- `flip`: A Boolean setting, `True`, indicating that flipping the images is enabled.\n\n- `transforms`: A list of dictionaries, each specifying a different image transformation type and its corresponding parameters:\n  - `dict(type='Resize', keep_ratio=True)`: Configures the resizing transformation while keeping the aspect ratio of images.\n  - `dict(type='RandomFlip')`: Configures a random image flip transformation.\n  - `dict(type='Normalize', **img_norm_cfg)`: Configures normalization of the image using settings in `img_norm_cfg`.\n  - `dict(type='Pad', size_divisor=32)`: Configures padding of images to a size that is divisible by 32.\n  - `dict(type='ImageToTensor', keys=['img'])`: Converts the image to a tensor format.\n  - `dict(type='Collect', keys=['img'])`: Collects the image tensor for further processing.\n\nThis setup is typically used in configuring data preprocessing for image-based machine learning models.\nAfter Multi Scale FLip Aug with above configuration, the results are wrapped into lists of the same length as followed: \nThe image shows a Python dictionary with the following structure:\n\n- `img`: Holds an unspecified list (denoted by `[...]`).\n- `img_shape`: Holds another unspecified list.\n- `scale`: Contains a list of tuples representing different scales: `(1333, 400)`, `(1333, 400)`, `(1333, 800)`, `(1333, 800)`.\n- `flip`: Contains a list of boolean values: `[False, True, False, True]`.\n\nThe dictionary seems to define some image processing parameters such as image size and whether to flip the image or not."}
{"page": 255, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_255.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\ntransforms (list [dict])— Transforms to apply in each augmentation.\nimg_scale (tuple | list[tuple] | None) — Images scales for resizing.\nscale_factor (float | list[float] | None) — Scale factors for resizing.\nflip (bool) — Whether apply flip augmentation. Default: False.\n\nflip_direction (str | list[str]) — Flip augmentation directions, options are “hori-\nzontal”, “vertical” and “diagonal”. If flip_direction is a list, multiple flip augmentations will\nbe applied. It has no effect when flip == False. Default: “horizontal”.\n\nclass mmdet.datasets.pipelines.Normalize(mean, std, to_rgb=True)\nNormalize the image.\n\nAdded key is “img_norm_cfg”.\n\nParameters\n\nmean (sequence) — Mean values of 3 channels.\nstd (sequence) — Std values of 3 channels.\n\nto_rgb (bool) — Whether to convert the image from BGR to RGB, default is true.\n\nclass mmdet.datasets.pipelines .Pad(size=None, size_divisor=None, pad_to_square=False, pad_val={'‘img':\n\n0, ‘masks’: 0, 'seg': 255})\n\nPad the image & masks & segmentation map.\n\nThere are two padding modes: (1) pad to a fixed size and (2) pad to the minimum size that is divisible by some\n\neres ees\n\nnumber. Added keys are “‘pad_shape”, “‘pad_fixed_size”, “pad_size_divisor”,\n\nParameters\n\nsize (tuple, optional) — Fixed padding size.\nsize_divisor (int, optional) -— The divisor of padded size.\n\npad_to_square (bool) — Whether to pad the image into a square. Currently only used for\nYOLOX. Default: False.\n\npad_val (dict, optional) — A dict for padding value, the default value is dict(img=0,\nmasks=0, seg=255).\n\nclass mmdet.datasets.pipelines.PhotoMetricDistortion (brightness_delta=32, contrast_range=(0.5,\n\n1.5), saturation_range=(0.5, 1.5),\nhue_delta=18)\n\nApply photometric distortion to image sequentially, every transformation is applied with a probability of 0.5.\nThe position of random contrast is in second or second to last.\n\n1.\n\noN Dw PF YN\n\nTan\n\nTan\n\nlom brightness\n\nlom contrast (mode 0)\n\nconvert color from BGR to HSV\n\nTan\n\nTan\n\nlom saturation\n\nlom hue\n\nconvert color from HSV to BGR\n\nTan\n\nTan\n\nlom contrast (mode 1)\n\nlomly swap channels\n\n248\n\nChapter 38. mmdet.datasets\n", "vlm_text": "Parameters \n•  transforms  ( list[dict] ) – Transforms to apply in each augmentation. •  img_scale  ( tuple | list[tuple] | None ) – Images scales for resizing. •  scale factor  ( float | list[float] | None ) – Scale factors for resizing. •  flip  ( bool ) – Whether apply flip augmentation. Default: False. •  flip direction  ( str | list[str] ) – Flip augmentation directions, options are “hori- zontal”, “vertical” and “diagonal”. If flip direction is a list, multiple flip augmentations will be applied. It has no effect when flip  $==$   False. Default: “horizontal”. \nclass  mmdet.datasets.pipelines. Normalize ( mean ,  std ,  to_rg  $b{=}$  True ) Normalize the image. \nAdded key is “img norm cf g”. \nParameters \n•  mean  ( sequence ) – Mean values of 3 channels. •  std  ( sequence ) – Std values of 3 channels. •  to_rgb  ( bool ) – Whether to convert the image from BGR to RGB, default is true. \nclass  mmdet.datasets.pipelines. Pad ( size  $\\leftrightharpoons$  None ,  size divisor  $=$  None ,  pad to square  $\\mathbf{\\dot{\\rho}}=$  False ,  pad_val={'img': 0, 'masks': 0, 'seg': 255} ) \nPad the image & masks & segmentation map. \nThere are two padding modes: (1) pad to a fixed size and (2) pad to the minimum size that is divisible by some number. Added keys are “pad_shape”, “pad fixed size”, “pad size divisor”, \nParameters \n•  size  ( tuple, optional ) – Fixed padding size. •  size divisor  ( int, optional ) – The divisor of padded size. •  pad to square  ( bool ) – Whether to pad the image into a square. Currently only used for YOLOX. Default: False. •  pad_val  ( dict, optional ) – A dict for padding value, the default value is  dict(img  $\\scriptstyle{=}0,$  , mask  $\\wp{=}0$  ,  $s e g{=}255,$  ) . \nclass  mmdet.datasets.pipelines. PhotoMetric Distortion ( brightness delta  $\\scriptstyle{=32}$  ,  contrast range=(0.5, 1.5) ,  saturation range  $=$  (0.5, 1.5) , hue_delta  $\\scriptstyle{\\varepsilon=I8.}$  ) \nApply photometric distortion to image sequentially, every transformation is applied with a probability of 0.5. The position of random contrast is in second or second to last. \n1. random brightness 2. random contrast (mode 0) 3. convert color from BGR to HSV 4. random saturation 5. random hue 6. convert color from HSV to BGR 7. random contrast (mode 1) 8. randomly swap channels "}
{"page": 256, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_256.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\nbrightness_delta (int) — delta of brightness.\ncontrast_range (tuple) — range of contrast.\nsaturation_range (tuple) — range of saturation.\n\nhue_delta (int) — delta of hue.\n\nclass mmdet.datasets.pipelines.RandomAffine (max_rotate_degree=10.0, max_translate_ratio=0.1,\n\nscaling_ratio_range=(0.5, 1.5), max_shear_degree=2.0,\nborder=(0, 0), border_val=(114, 114, 114),\nmin_bbox_size=2, min_area_ratio=0.2,\nmax_aspect_ratio=20)\n\nRandom affine transform data augmentation.\n\nThis operation randomly generates affine transform matrix which including rotation, translation, shear and scaling\n\ntransforms.\n\nParameters\n\nmax_rotate_degree (float) — Maximum degrees of rotation transform. Default: 10.\nmax_translate_ratio (float) — Maximum ratio of translation. Default: 0.1.\n\nscaling_ratio_range (tuple[float]) — Min and max ratio of scaling transform. De-\nfault: (0.5, 1.5).\n\nmax_shear_degree (float) — Maximum degrees of shear transform. Default: 2.\n\nborder (tuple[int]) — Distance from height and width sides of input image to adjust\noutput shape. Only used in mosaic dataset. Default: (0, 0).\n\nborder_val (tuple[int]) — Border padding values of 3 channels. Default: (114, 114,\n114).\n\nmin_bbox_size (float) — Width and height threshold to filter bboxes. If the height or\nwidth of a box is smaller than this value, it will be removed. Default: 2.\n\nmin_area_ratio (float) — Threshold of area ratio between original bboxes and wrapped\nbboxes. If smaller than this value, the box will be removed. Default: 0.2.\n\nmax_aspect_ratio (float) — Aspect ratio of width and height threshold to filter bboxes.\nIf max(h/w, w/h) larger than this value, the box will be removed.\n\nclass mmdet.datasets.pipelines.RandomCenterCropPad (crop_size=None, ratios=(0.9, 1.0, 1.1),\n\nborder=128, mean=None, std=None,\nto_rgb=None, test_mode=False,\ntest_pad_mode=(‘logical_or’, 127),\ntest_pad_add_pix=0, bbox_clip_border=True)\n\nRandom center crop and random around padding for CornerNet.\n\nThis operation generates randomly cropped image from the original image and pads it simultaneously. Different\nfrom RandomCrop, the output shape may not equal to crop_size strictly. We choose a random value from\nratios and the output shape could be larger or smaller than crop_size. The padding operation is also different\nfrom Pad, here we use around padding instead of right-bottom padding.\n\nThe relation between output image (padding image) and original image:\n\noutput image\n\n(continues on next page)\n\n38.2. pipelines\n\n249\n\n", "vlm_text": "Parameters \n•  brightness delta  ( int ) – delta of brightness. •  contrast range  ( tuple ) – range of contrast. •  saturation range  ( tuple ) – range of saturation. •  hue_delta  ( int ) – delta of hue. \nclass mmdet.datasets.pipelines.Random Affine(max rotate de gre $\\it{z}{=}10.0$ , max translate ratio $\\mathord{\\leftrightharpoons}0.I$ ,scaling ratio range $\\mathbf{\\Psi}=$ (0.5, 1.5), max shear de gre $_{z=2.0}$ ,border=(0, 0) ,  border_val  $\\leftrightharpoons$  (114, 114, 114) , min b box size  $_{:=2}$  ,  min area ratio  $\\backprime{=}0.2$  , max aspect ratio  $\\bullet{=}2O$  ) \nRandom affine transform data augmentation. \nThis operation randomly generates affine transform matrix which including rotation, translation, shear and scaling transforms. \nParameters \n•  max rotate degree  ( float ) – Maximum degrees of rotation transform. Default: 10. •  max translate ratio  ( float ) – Maximum ratio of translation. Default: 0.1. •  scaling ratio range  ( tuple[float] ) – Min and max ratio of scaling transform. De- fault: (0.5, 1.5). •  max shear degree  ( float ) – Maximum degrees of shear transform. Default: 2. •  border  ( tuple[int] ) – Distance from height and width sides of input image to adjust output shape. Only used in mosaic dataset. Default:   $(0,0)$  . •  border_val  ( tuple[int] ) – Border padding values of 3 channels. Default: (114, 114, 114). •  min b box size  ( float ) – Width and height threshold to filter bboxes. If the height or width of a box is smaller than this value, it will be removed. Default: 2. •  min area ratio  ( float ) – Threshold of area ratio between original bboxes and wrapped bboxes. If smaller than this value, the box will be removed. Default: 0.2. •  max aspect ratio  ( float ) – Aspect ratio of width and height threshold to filter bboxes. If max(h/w, w/h) larger than this value, the box will be removed. \nclass  mmdet.datasets.pipelines. Random Center Crop Pad ( crop_size  $\\mathbf{\\varepsilon}=$  None ,  ratios=(0.9, 1.0, 1.1) , border=128 ,  mean  $=$  None ,  std=None , to_rgb  $=$  None ,  test_mode  $:=$  False , test pad mode=('logical_or', 127) , test pad add pix $\\mathbf{\\varepsilon}_{:=0}$ , b box clip border=True)\nRandom center crop and random around padding for CornerNet. \nThis operation generates randomly cropped image from the original image and pads it simultaneously. Different from  RandomCrop , the output shape may not equal to  crop_size  strictly. We choose a random value from ratios  and the output shape could be larger or smaller than  crop_size . The padding operation is also different from  Pad , here we use around padding instead of right-bottom padding. \nThe relation between output image (padding image) and original image: \nThe table contains the text \"output image.\" There doesn't appear to be any other content in the table."}
{"page": 257, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_257.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n---------- +\n|\n|\n| original image\n|\n|\n\n---------- +\n\na\no\n5\nct\no\nwR\n\nThere are 5 main areas in the figure:\n\noutput image: output image of this operation, also called padding image in following instruction.\noriginal image: input image of this operation.\n\npadded area: non-intersect area of output image and original image.\n\ncropped area: the overlap of output image and original image.\n\ncenter range: a smaller area where random center chosen from. center range is computed by border and\noriginal image’s shape to avoid our random center is too close to original image’s border.\n\nAlso this operation act differently in train and test mode, the summary pipeline is listed below.\n\nTrain pipeline:\n\n1.\n\n2\n3\n4.\n5\n\n6.\n\nChoose a random_ratio from ratios, the shape of padding image will be random_ratio *\ncrop_size.\n\n. Choose a random_center in center range.\n\n. Generate padding image with center matches the random_center.\n\nInitialize the padding image with pixel value equals to mean.\n\n. Copy the cropped area to padding image.\n\nRefine annotations.\n\nTest pipeline:\n\n1.\n\n2\n3.\n4\n\nCompute output shape according to test_pad_mode.\n\n. Generate padding image with center matches the original image center.\n\nInitialize the padding image with pixel value equals to mean.\n\n. Copy the cropped area to padding image.\n\nParameters\n\n* crop_size (tuple | None) - expected size after crop, final size will computed according\nto ratio. Requires (h, w) in train mode, and None in test mode.\n\nratios (tuple) — random select a ratio from tuple and crop image to (crop_size[0] * ratio)\n* (crop_size[1] * ratio). Only available in train mode.\n\nborder (int) — max distance from center select area to image border. Only available in train\nmode.\n\n* mean (sequence) — Mean values of 3 channels.\n\n250\n\nChapter 38. mmdet.datasets\n\n", "vlm_text": "This image is a diagram illustrating the concept of cropping and padding an image. It shows an \"original image\" section on the right and indicates how the image can be manipulated. The diagram is divided into sections labeled as \"padded area\" at the top and bottom, and a \"cropped area\" with a \"center range\" marked by a dot to suggest where the cropping might be focused within the original image. This visual representation is typically used in image processing to explain how an image might be resized or adjusted by adding padding (extra space) or cropping (cutting out parts of the image).\nThere are 5 main areas in the figure: \n• output image: output image of this operation, also called padding image in following instruction. • original image: input image of this operation. • padded area: non-intersect area of output image and original image. • cropped area: the overlap of output image and original image. • center range: a smaller area where random center chosen from. center range is computed by  border  and original image’s shape to avoid our random center is too close to original image’s border. \nAlso this operation act differently in train and test mode, the summary pipeline is listed below. \nTrain pipeline: \n1. Choose a random ratio from ratios, the shape of padding image will be random ratio \\*crop_size . 2. Choose a  random center  in center range. 3. Generate padding image with center matches the  random center . 4. Initialize the padding image with pixel value equals to  mean . 5. Copy the cropped area to padding image. 6. Refine annotations. \nTest pipeline: \n1. Compute output shape according to  test pad mode . 2. Generate padding image with center matches the original image center. 3. Initialize the padding image with pixel value equals to  mean . 4. Copy the  cropped area  to padding image. \nParameters \n•  crop_size  ( tuple | None ) – expected size after crop, final size will computed according to ratio. Requires (h, w) in train mode, and None in test mode. •  ratios  ( tuple ) – random select a ratio from tuple and crop image to (crop_size[0] \\* ratio) \\* (crop_size[1] \\* ratio). Only available in train mode. •  border  ( int ) – max distance from center select area to image border. Only available in train mode. •  mean  ( sequence ) – Mean values of 3 channels. "}
{"page": 258, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_258.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* std (sequence) — Std values of 3 channels.\n* to_rgb (bool) — Whether to convert the image from BGR to RGB.\n\n* test_mode (bool) — whether involve random variables in transform. In train mode,\ncrop_size is fixed, center coords and ratio is random selected from predefined lists. In test\nmode, crop_size is image’s original shape, center coords and ratio is fixed.\n\n* test_pad_mode (tuple) — padding method and padding shape value, only available in test\nmode. Default is using ‘logical_or’ with 127 as padding shape value.\n\n— ’logical_or’: final_shape = input_shape | padding_shape_value\n\n— ’size_divisor’: final_shape = int( ceil(input_shape / padding_shape_value) *\npadding_shape_value)\n\n* test_pad_add_pix (int) — Extra padding pixel in test mode. Default 0.\n\n* bbox_clip_border (bool, optional) — Whether clip the objects outside the border of\nthe image. Defaults to True.\n\nclass mmdet.datasets.pipelines.RandomCrop (crop_size, crop_type='absolute', allow_negative_crop=False,\nrecompute_bbox=False, bbox_clip_border=True)\nRandom crop the image & bboxes & masks.\n\nThe absolute crop_size is sampled based on crop_type and image_size, then the cropped results are generated.\nParameters\n\n* crop_size (tuple) — The relative ratio or absolute pixels of height and width.\n\n746, 66, cee\n\n* crop_type (str, optional) — one of “relative_range”, “relative”, “absolute”, “‘abso-\nlute_range”. “relative” randomly crops (h * crop_size[0], w * crop_size[1]) part from an\ninput of size (h, w). “relative_range” uniformly samples relative crop size from range\n[crop_size[0], 1] and [crop_size[1], 1] for height and width respectively. ‘‘absolute”\ncrops from an input with absolute size (crop_size[0], crop_size[1]). “‘absolute_range” uni-\nformly samples crop_h in range [crop_size[0], min(h, crop_size[1])] and crop_w in range\n[crop_size[0], min(w, crop_size[1])]. Default “absolute”.\n\n* allow_negative_crop (bool, optional) — Whether to allow a crop that does not con-\ntain any bbox area. Default False.\n\n* recompute_bbox (bool, optional)—Whether to re-compute the boxes based on cropped\ninstance masks. Default False.\n\n* bbox_clip_border (bool, optional) — Whether clip the objects outside the border of\nthe image. Defaults to True.\n\nNote:\n¢ If the image is smaller than the absolute crop size, return the original image.\n\n* The keys for bboxes, labels and masks must be aligned. That is, gt_bboxes corresponds to gt_labels and\ngt_masks, and gt_bboxes_ignore corresponds to gt_labels_ignore and gt_masks_ignore.\n\n* If the crop does not contain any gt-bbox region and allow_negative_crop is set to False, skip this image.\n\nclass mmdet.datasets.pipelines.RandomF lip (flip_ratio=None, direction='horizontal')\nFlip the image & bbox & mask.\n\nIf the input dict contains the key “flip”, then the flag will be used, otherwise it will be randomly decided by a\nratio specified in the init method.\n\n38.2. pipelines 251\n", "vlm_text": "•  std  ( sequence ) – Std values of 3 channels. \n•  to_rgb  ( bool ) – Whether to convert the image from BGR to RGB. •  test_mode  ( bool ) – whether involve random variables in transform. In train mode, crop_size is fixed, center coords and ratio is random selected from predefined lists. In test mode, crop_size is image’s original shape, center coords and ratio is fixed. •  test pad mode  ( tuple ) – padding method and padding shape value, only available in test mode. Default is using ‘logical_or’ with 127 as padding shape value. –  ’logical_or’: final shape  $=$   input shape | padding shape value –  ’size divisor’: final shape  $=$  int( ceil(input shape / padding shape value) \\* padding shape value)•  test pad add pix  ( int ) – Extra padding pixel in test mode. Default 0. •  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. \nclass  mmdet.datasets.pipelines. RandomCrop ( crop_size ,  crop_type  $=$  'absolute' ,  allow negative crop  $\\leftrightharpoons$  False , re compute b box  $\\mathbf{\\beta}=$  False ,  b box clip border=True ) \nRandom crop the image & bboxes & masks. \nThe absolute  crop_size  is sampled based on  crop_type  and  image_size , then the cropped results are generated. \nParameters \n•  crop_size  ( tuple ) – The relative ratio or absolute pixels of height and width. \n•  crop_type  ( str, optional ) – one of “relative range”, “relative”, “absolute”, “abso- lute_range”. “relative” randomly crops (h \\* crop_size[0], w \\* crop_size[1]) part from an input of size (h, w). “relative range” uniformly samples relative crop size from range [crop_size[0], 1] and [crop_size[1], 1] for height and width respectively. “absolute” crops from an input with absolute size (crop_size[0], crop_size[1]). “absolute range” uni- formly samples crop_h in range [crop_size[0], min(h, crop_size[1])] and crop_w in range [crop_size[0], min(w, crop_size[1])]. Default “absolute”. \n•  allow negative crop  ( bool, optional ) – Whether to allow a crop that does not con- tain any bbox area. Default False. \n•  re compute b box  ( bool, optional ) – Whether to re-compute the boxes based on cropped instance masks. Default False. \n•  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. \nNote: \n•  If the image is smaller than the absolute crop size, return the  original image. \n• The keys for bboxes, labels and masks must be aligned. That is,  gt_bboxes  corresponds to  gt_labels  and gt_masks , and  gt b boxes ignore  corresponds to  gt labels ignore  and  gt masks ignore . • If the crop does not contain any gt-bbox region and  allow negative crop  is set to False, skip this image. \nclass  mmdet.datasets.pipelines. RandomFlip ( flip_ratio  $\\leftrightharpoons$  None ,  direction  $.=$  'horizontal' ) Flip the image & bbox & mask. \nIf the input dict contains the key “flip”, then the flag will be used, otherwise it will be randomly decided by a ratio specified in the init method. "}
{"page": 259, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_259.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nWhen random flip is enabled, £l1ip_ratio/direction can either be a float/string or tuple of float/string. There\nare 3 flip modes:\n\n¢ flip_ratio is float, direction is string: the image will be direction’ ‘ly flipped with\nprobability of ~*flip_ratio . E.g., flip_ratio=0.5, direction='horizontal',\nthen image will be horizontally flipped with probability of 0.5.\n\n¢ flip_ratio is float, direction is list of string: the image will be direction[i]* “ly flipped\n\nwith probability of **flip_ratio/len(direction). E.g., flip_ratio=0.5,\ndirection=['horizontal', 'vertical'], then image will be horizontally flipped with\nprobability of 0.25, vertically with probability of 0.25.\n\n¢ flip_ratio is list of float, direction is list of string: given len(flip_ratio) ==\nlen(direction), the image will be direction[i]*°ly flipped with probability\nof **flip_ratio[i]. E.g., flip_ratio=[0.3, 0.5], direction=['horizontal',\n\"vertical'], then image will be horizontally flipped with probability of 0.3, vertically with\nprobability of 0.5.\n\nParameters\n\n¢ flip_ratio (float | list[float], optional) — The flipping probability. Default:\nNone.\n\n¢ direction(str | list[str], optional) — The flipping direction. Options are ‘hori-\nzontal’, ‘vertical’, ‘diagonal’. Default: ‘horizontal’. If input is a list, the length must equal\nflip_ratio. Each element in flip_ratio indicates the flip probability of corresponding\ndirection.\n\nbbox_flip(bboxes, img_shape, direction)\nFlip bboxes horizontally.\n\nParameters\n¢ bboxes (numpy.ndarray) — Bounding boxes, shape (..., 4*k)\n¢ img_shape (tuple[int]) — Image shape (height, width)\n¢ direction (str) — Flip direction. Options are ‘horizontal’, ‘vertical’.\n\nReturns Flipped bounding boxes.\n\nReturn type numpy.ndarray\n\nclass mmdet.datasets.pipelines.RandomShift (shift_ratio=0.5, max_shift_px=32, filter_thr_px=1)\nShift the image and box given shift pixels and probability.\n\nParameters\n¢ shift_ratio (float) — Probability of shifts. Default 0.5.\n* max_shift_px (int) — The max pixels for shifting. Default 32.\n\n¢ filter_thr_px (int) — The width and height threshold for filtering. The bbox and the rest\nof the targets below the width and height threshold will be filtered. Default 1.\n\nclass mmdet.datasets.pipelines.Resize(img_scale=None, multiscale_mode='range’', ratio_range=None,\nkeep_ratio=True, bbox_clip_border=True, backend='cv2',\noverride=False)\nResize images & bbox & mask.\n\nThis transform resizes the input image to some scale. Bboxes and masks are then resized with the same scale\nfactor. If the input dict contains the key “scale”, then the scale in the input dict is used, otherwise the specified\n\n252 Chapter 38. mmdet.datasets\n", "vlm_text": "When random flip is enabled,  flip_ratio / direction  can either be a float/string or tuple of float/string. There are 3 flip modes: \n•  flip_ratio  is float,  direction  is string: the image will be  direction \\`\\` ly flipped with probability of  \\`\\` flip_ratio . E.g., flip_ratio  $\\scriptstyle=\\!0$  .5 , direction  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  ' horizontal ' , then image will be horizontally flipped with probability of 0.5. •  flip_ratio  is float,  direction  is list of string: the image will  be direction[i] \\`\\` ly flipped with probability of  \\`\\` flip_ratio/len(direction) . E.g., flip_ratio=0.5 , direction  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ ' horizontal ' ,  ' vertical ' ] , then image will be horizontally flipped with probability of 0.25, vertically with probability of 0.25. •  flip_ratio  is list of float,  direction  is list of string:  given len(flip_ratio)  $==$  len(direction) , the image will be direction[i] \\`\\` ly flipped with probability of  \\`\\` flip_ratio[i] . E.g., flip_ratio  $=$  [0.3, 0.5] , direction  $\\underline{{\\underline{{\\mathbf{\\delta\\pi}}}}}$  [ ' horizontal ' , ' vertical ' ] , then image will be horizontally flipped with probability of 0.3, vertically with probability of 0.5. \nParameters \n•  flip_ratio  ( float | list[float], optional ) – The flipping probability. Default: None. \n direction  ( str | list[str], optional ) – The flipping direction. Options are ‘hori- zontal’, ‘vertical’, ‘diagonal’. Default: ‘horizontal’. If input is a list, the length must equal flip_ratio . Each element in  flip_ratio  indicates the flip probability of corresponding direction. \nbbox_flip ( bboxes ,  img_shape ,  direction ) Flip bboxes horizontally. \nParameters \n•  bboxes  ( numpy.ndarray ) – Bounding boxes, shape   $(\\ldots,4^{*}\\mathbf{k})$  •  img_shape  ( tuple[int] ) – Image shape (height, width) •  direction  ( str ) – Flip direction. Options are ‘horizontal’, ‘vertical’. Returns  Flipped bounding boxes. Return type  numpy.ndarray \nclass  mmdet.datasets.pipelines. Random Shift ( shift ratio  $\\bullet{=}0.5$  ,  max_shift_  $_{p x=32}$  ,  filter thr px=1 ) Shift the image and box given shift pixels and probability. \nParameters \n•  shift ratio  ( float ) – Probability of shifts. Default 0.5. •  max shift px  ( int ) – The max pixels for shifting. Default 32. •  filter thr px  ( int ) – The width and height threshold for filtering. The bbox and the rest of the targets below the width and height threshold will be filtered. Default 1. \nclass  mmdet.datasets.pipelines. Resize ( img_scale  $=$  None ,  multi scale mode  $=$  range' ,  ratio range  $\\mathbf{\\beta}=$  None , keep_ratio  $\\mathbf{\\chi}=$  True ,  b box clip border  $\\scriptstyle\\mathtt{\\sim}$  True ,  backend  $\\scriptstyle{I=c\\nu2}$  ' , override  $=$  False ) \nResize images & bbox & mask. \nThis transform resizes the input image to some scale. Bboxes and masks are then resized with the same scale factor. If the input dict contains the key “scale”, then the scale in the input dict is used, otherwise the specified scale in the init method is used. If the input dict contains the key “scale factor” (if Multi Scale Flip Aug does not give img_scale but scale factor), the actual scale will be computed by image shape and scale factor. "}
{"page": 260, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_260.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nscale in the init method is used. If the input dict contains the key “scale_factor” (if MultiScaleFlipAug does not\ngive img_scale but scale_factor), the actual scale will be computed by image shape and scale_factor.\n\nimg_scale can either be a tuple (single-scale) or a list of tuple (multi-scale). There are 3 multiscale modes:\n\n* ratio_range is not None: randomly sample a ratio from the ratio range and multiply it with the image\nscale.\n\n* ratio_range is None and multiscale_mode == \"range\": randomly sample a scale from the multi-\nscale range.\n\n* ratio_range is None and multiscale_mode == \"value\": randomly sample a scale from multiple\nscales.\nParameters\n¢ img_scale (tuple or list [tuple]) — Images scales for resizing.\n* multiscale_mode (str) — Either “range” or “value”.\n* ratio_range (tuple[float]) —(min_ratio, max_ratio)\n* keep_ratio (bool) — Whether to keep the aspect ratio when resizing the image.\n\n* bbox_clip_border (bool, optional) — Whether clip the objects outside the border of\nthe image. Defaults to True.\n\n* backend (str) — Image resize backend, choices are ‘cv2’ and ‘pillow’. These two backends\ngenerates slightly different results. Defaults to “cv2’.\n\n* override (bool, optional) — Whether to override scale and scale_factor so as to call\nresize twice. Default False. If True, after the first resizing, the existed scale and scale_factor\nwill be ignored so the second resizing can be allowed. This option is a work-around for\nmultiple times of resize in DETR. Defaults to False.\n\nstatic random_sample(img_scales)\nRandomly sample an img_scale when multiscale_mode=='range'.\n\nParameters img_scales (list [tuple]) — Images scale range for sampling. There must be\ntwo tuples in img_scales, which specify the lower and upper bound of image scales.\n\nReturns Returns a tuple (img_scale, None), where img_scale is sampled scale and None is\njust a placeholder to be consistent with random_select ().\n\nReturn type (tuple, None)\n\nstatic random_sample_ratio(img_scale, ratio_range)\nRandomly sample an img_scale when ratio_range is specified.\n\nA ratio will be randomly sampled from the range specified by ratio_range. Then it would be multiplied\nwith img_scale to generate sampled scale.\n\nParameters\n¢ img_scale (tuple) — Images scale base to multiply with ratio.\n\n¢ ratio_range (tuple[float]) — The minimum and maximum ratio to scale the\nimg_scale.\n\nReturns Returns a tuple (scale, None), where scale is sampled ratio multiplied with\nimg_scale and None is just a placeholder to be consistent with random_select ().\n\nReturn type (tuple, None)\n\n38.2. pipelines 253\n", "vlm_text": "\nimg_scale  can either be a tuple (single-scale) or a list of tuple (multi-scale). There are 3 multiscale modes: \n•  ratio range is not None : randomly sample a ratio from the ratio range and multiply it with the image scale. •  ratio range is None  and  multi scale mode  $==$   \"range\" : randomly sample a scale from the multi- scale range. •  ratio range is None  and  multi scale mode   $==$   \"value\" : randomly sample a scale from multiple scales. \nParameters \n•  img_scale  ( tuple or list[tuple] ) – Images scales for resizing. •  multi scale mode  ( str ) – Either “range” or “value”. •  ratio range  ( tuple[float] ) – (min_ratio, max_ratio) •  keep_ratio  ( bool ) – Whether to keep the aspect ratio when resizing the image. •  b box clip border  ( bool, optional ) – Whether clip the objects outside the border of the image. Defaults to True. •  backend  ( str ) – Image resize backend, choices are ‘cv2’ and ‘pillow’. These two backends generates slightly different results. Defaults to ‘cv2’. •  override  ( bool, optional ) – Whether to override  scale  and  scale factor  so as to call resize twice. Default False. If True, after the first resizing, the existed  scale  and  scale factor will be ignored so the second resizing can be allowed. This option is a work-around for multiple times of resize in DETR. Defaults to False. \nstatic random sample(img_scales)\nRandomly sample an img_scale when  multi scale mode  $==$  ' range ' \nParameters  img_scales  ( list[tuple] ) – Images scale range for sampling. There must be two tuples in img_scales, which specify the lower and upper bound of image scales. Returns  Returns a tuple  (img_scale, None) , where  img_scale  is sampled scale and None is just a placeholder to be consistent with  random select() . Return type  (tuple, None) \nstatic random sample ratio ( img_scale ,  ratio range ) Randomly sample an img_scale when  ratio range  is specified. \nA ratio will be randomly sampled from the range specified by  ratio range . Then it would be multiplied with  img_scale  to generate sampled scale. \nParameters \n•  img_scale  ( tuple ) – Images scale base to multiply with ratio. •  ratio range  ( tuple[float] ) – The minimum and maximum ratio to scale the img_scale . Returns  Returns a tuple  (scale, None) , where  scale  is sampled ratio multiplied with img_scale  and None is just a placeholder to be consistent with  random select() . Return type  (tuple, None) "}
{"page": 261, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_261.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nstatic random_select (img_scales)\nRandomly select an img_scale from given candidates.\n\nParameters img_scales (list [tuple]) — Images scales for selection.\n\nReturns Returns a tuple (img_scale, scale_dix), where img_scale is the selected image\nscale and scale_idx is the selected index in the given candidates.\n\nReturn type (tuple, int)\n\nclass mmdet.datasets.pipelines.Rotate(/evel, scale=1, center=None, img_fill_val=128,\nseg_ignore_label=255, prob=0.5, max_rotate_angle=30,\nrandom_negative_prob=0.5)\nApply Rotate Transformation to image (and its corresponding bbox, mask, segmentation).\n\nParameters\n¢ level (int | float) —The level should be in range (0, MAX_LEVEL].\n* scale (int | float) — Isotropic scale factor. Same in mmcv.imrotate.\n\n* center (int | float | tuple[float]) — Center point (w, h) of the rotation in the\nsource image. If None, the center of the image will be used. Same in mmcv.imrotate.\n\n¢ img_fill_val (int | float | tuple) — The fill value for image border. If float, the\nsame value will be used for all the three channels of image. If tuple, the should be 3 elements\n(e.g. equals the number of channels for image).\n\n* seg_ignore_label (int) — The fill value used for segmentation map. Note this value must\nequals ignore_label in semantic_head of the corresponding config. Default 255.\n\n* prob (float) — The probability for perform transformation and should be in range 0 to 1.\n* max_rotate_angle (int / float) —The maximum angles for rotate transformation.\n* random_negative_prob (float) — The probability that turns the offset negative.\n\nclass mmdet.datasets.pipelines.SegRescale(scale_factor=1, backend='cv2')\nRescale semantic segmentation maps.\n\nParameters\n* scale_factor (float) — The scale factor of the final output.\n\n* backend (str) — Image rescale backend, choices are ‘cv2’ and ‘pillow’. These two backends\ngenerates slightly different results. Defaults to “cv2’.\n\nclass mmdet.datasets.pipelines. Shear (level, img_fill_val=128, seg_ignore_label=255, prob=0.5,\ndirection='horizontal', max_shear_magnitude=0. 3,\nrandom_negative_prob=0.5, interpolation='bilinear')\nApply Shear Transformation to image (and its corresponding bbox, mask, segmentation).\n\nParameters\n¢ level (int | float) —The level should be in range [0, MAX_LEVEL].\n\n¢ img_fill_val (int | float | tuple) -—The filled values for image border. If float, the\nsame fill value will be used for all the three channels of image. If tuple, the should be 3\nelements.\n\n* seg_ignore_label (int) — The fill value used for segmentation map. Note this value must\nequals ignore_label in semantic_head of the corresponding config. Default 255.\n\n* prob (float) — The probability for performing Shear and should be in range [0, 1].\n\n¢ direction (str) — The direction for shear, either “horizontal” or “vertical”.\n\n254 Chapter 38. mmdet.datasets\n", "vlm_text": "static random select(img_scales)Randomly select an img_scale from given candidates. \nParameters  img_scales  ( list[tuple] ) – Images scales for selection. Returns  Returns a tuple  (img_scale, scale_dix) , where  img_scale  is the selected image scale and  scale_idx  is the selected index in the given candidates. Return type  (tuple, int) \nclass  mmdet.datasets.pipelines. Rotate ( level ,  scale  $\\scriptstyle{\\varepsilon=I}$  ,  center  $=$  None ,  img fill va  $l{=}I28$  , seg ignore la be  $l{=}255$  ,  prob  $\\bullet{=}0.5$  ,  max rotate angle  $\\==30$  , random negative pro b $\\backsimeq\\!O.5.$ )\nApply Rotate Transformation to image (and its corresponding bbox, mask, segmentation). \nParameters \n•  level  ( int | float ) – The level should be in range (0,_MAX_LEVEL]. •  scale  ( int | float ) – Isotropic scale factor. Same in  mmcv.imrotate . •  center  ( int | float | tuple[float] ) – Center point (w, h) of the rotation in the source image. If None, the center of the image will be used. Same in  mmcv.imrotate . •  img fill val  ( int | float | tuple ) – The fill value for image border. If float, the same value will be used for all the three channels of image. If tuple, the should be 3 elements (e.g. equals the number of channels for image). •  seg ignore label  ( int ) – The fill value used for segmentation map. Note this value must equals  ignore label  in  semantic head  of the corresponding config. Default 255. •  prob  ( float ) – The probability for perform transformation and should be in range 0 to 1. •  max rotate angle  ( int | float ) – The maximum angles for rotate transformation. •  random negative pro b  ( float ) – The probability that turns the offset negative. \nclass  mmdet.datasets.pipelines. SegRescale ( scale factor  ${\\bf\\Xi}_{=I}$  ,  backend='cv2' ) Rescale semantic segmentation maps. \nParameters \n•  scale factor  ( float ) – The scale factor of the final output. •  backend    $(s t r)$   – Image rescale backend, choices are ‘cv2’ and ‘pillow’. These two backends generates slightly different results. Defaults to ‘cv2’. \nclass  mmdet.datasets.pipelines. Shear ( level ,  img fill val  $\\scriptstyle{\\prime=I28}$  ,  seg ignore la be  $l{=}255$  ,  prob  $\\bullet{=}0.5$  , direction  $\\scriptstyle{\\mathcal{S}}$  'horizontal' ,  max shear magnitude  $\\mathrm{\\Sigma=}0.3$  , random negative pro b  $\\bullet{=}0.5$  ,  interpolation  $\\scriptstyle{\\mathcal{S}}$  'bilinear' ) \nApply Shear Transformation to image (and its corresponding bbox, mask, segmentation). \nParameters \n•  level  ( int | float ) – The level should be in range [0,_MAX_LEVEL]. •  img fill val  ( int | float | tuple ) – The filled values for image border. If float, the same fill value will be used for all the three channels of image. If tuple, the should be 3 elements. •  seg ignore label  ( int ) – The fill value used for segmentation map. Note this value must equals  ignore label  in  semantic head  of the corresponding config. Default 255. •  prob  ( float ) – The probability for performing Shear and should be in range [0, 1]. •  direction  ( str ) – The direction for shear, either “horizontal” or “vertical”. "}
{"page": 262, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_262.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* max_shear_magnitude (float) — The maximum magnitude for Shear transformation.\n\n* random_negative_prob (float) — The probability that turns the offset negative. Should\nbe in range [0,1]\n\n¢ interpolation (str) — Same as in mmcv.imshear().\n\nclass mmdet.datasets.pipelines.ToDataContainer (fields=({'key': ‘img’, ‘stack’: True}, {‘key': 'gt_bboxes'},\n{‘key': 'gt_labels'}))\nConvert results to mmcv.DataContainer by given fields.\n\nParameters fields (Sequence[dict]) — Each field is a dict like dict(key='xxx',\nkwargs). The key in result will be converted to mmcv.DataContainer with\nkwargs. Default: (dict(key='img', stack=True), dict(key='gt_bboxes'),\ndict (key='gt_labels')).\n\nclass mmdet.datasets.pipelines.ToTensor (keys)\nConvert some results to torch. Tensor by given keys.\n\nParameters keys (Sequence [str]) — Keys that need to be converted to Tensor.\n\nclass mmdet.datasets.pipelines.Translate (level, prob=0.5, img_fill_val=128, seg_ignore_label=255,\ndirection='horizontal’, max_translate_offset=250.0,\nrandom_negative_prob=0.5, min_size=0)\nTranslate the images, bboxes, masks and segmentation maps horizontally or vertically.\n\nParameters\n¢ level (int | float)-The level for Translate and should be in range [0, MAX_LEVEL].\n* prob (float) — The probability for performing translation and should be in range [0, 1].\n\n¢ img_fill_val (int | float | tuple) —The filled value for image border. If float, the\nsame fill value will be used for all the three channels of image. If tuple, the should be 3\nelements (e.g. equals the number of channels for image).\n\n* seg_ignore_label (int) — The fill value used for segmentation map. Note this value must\nequals ignore_label in semantic_head of the corresponding config. Default 255.\n\ndirection (str) — The translate direction, either “horizontal” or “vertical”.\n* max_translate_offset (int / float) —The maximum pixel’s offset for Translate.\n* random_negative_prob (float) — The probability that turns the offset negative.\n\n* min_size (int | float) —The minimum pixel for filtering invalid bboxes after the trans-\nlation.\n\nclass mmdet.datasets.pipelines.Transpose (keys, order)\nTranspose some results by given keys.\n\nParameters\n* keys (Sequence[str]) — Keys of results to be transposed.\n* order (Sequence [int ]) — Order of transpose.\n\nmmdet.datasets.pipelines.to_tensor (data)\nConvert objects of various python types to torch. Tensor.\n\nSupported types are: numpy.ndarray, torch. Tensor, Sequence, int and float.\n\nParameters data(torch.Tensor | numpy.ndarray | Sequence | int | float)-—Datato\nbe converted.\n\n38.2. pipelines 255\n", "vlm_text": "•  max shear magnitude  ( float ) – The maximum magnitude for Shear transformation. •  random negative pro b  ( float ) – The probability that turns the offset negative. Should be in range [0,1] •  interpolation  ( str ) – Same as in  mmcv.imshear() . \nclass  mmdet.datasets.pipelines. To Data Container ( fields  $=$  ({'key': 'img', 'stack': True}, {'key': 'gt_bboxes'}, {'key': 'gt_labels'}) ) \nParameters  fields  ( Sequence[dict] ) – Each field is a dict like dict  $\\mathbf{(k e y='x x x'}$  , \\*\\*kwargs) . The  key  in result will be converted to  mmcv.Data Container  with \\*\\*kwargs . Default: (dict(key  $\\acute{=}$  ' img ' , stack  $\\circeq$  True), dict(key= ' gt_bboxes ' ), dict(key= ' gt_labels ' )) . \nclass  mmdet.datasets.pipelines. ToTensor ( keys ) Convert some results to  torch.Tensor  by given keys. \nParameters  keys  ( Sequence[str] ) – Keys that need to be converted to Tensor. \nclass  mmdet.datasets.pipelines. Translate ( level ,  prob  $\\bullet{=}0.5$  ,  img fill va  $l{=}I28$  ,  seg ignore label=255 , direction  $=$  'horizontal' ,  max translate offset=250.0 , random negative pro  $\\mathit{b}\\mathrm{=}0.5$  ,  min_siz  $\\scriptstyle{\\mathcal{S}}=O.$  ) \nTranslate the images, bboxes, masks and segmentation maps horizontally or vertically. \nParameters \n•  level  ( int | float ) – The level for Translate and should be in range [0,_MAX_LEVEL]. •  prob  ( float ) – The probability for performing translation and should be in range [0, 1]. •  img fill val  ( int | float | tuple ) – The filled value for image border. If float, the same fill value will be used for all the three channels of image. If tuple, the should be 3 elements (e.g. equals the number of channels for image). •  seg ignore label  ( int ) – The fill value used for segmentation map. Note this value must equals  ignore label  in  semantic head  of the corresponding config. Default 255. •  direction  ( str ) – The translate direction, either “horizontal” or “vertical”. •  max translate offset  ( int | float ) – The maximum pixel’s offset for Translate. •  random negative pro b  ( float ) – The probability that turns the offset negative. •  min_size  ( int | float ) – The minimum pixel for filtering invalid bboxes after the trans- lation. \nclass  mmdet.datasets.pipelines. Transpose ( keys ,  order ) Transpose some results by given keys. \nParameters \n•  keys  ( Sequence[str] ) – Keys of results to be transposed. •  order  ( Sequence[int] ) – Order of transpose. \nmmdet.datasets.pipelines. to_tensor ( data ) Convert objects of various python types to  torch.Tensor . \nSupported types are:  numpy.ndarray ,  torch.Tensor ,  Sequence ,  int  and  float . \nParameters  data  ( torch.Tensor | numpy.ndarray | Sequence | int | float ) – Data to be converted. "}
{"page": 263, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_263.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n38.3 samplers\n\nclass mmdet.datasets.samplers.DistributedGroupSampler (dataset, samples_per_gpu=1,\nnum_replicas=None, rank=None, seed=0)\n\nSampler that restricts data loading to a subset of the dataset.\n\nIt is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such case,\neach process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original\ndataset that is exclusive to it.\n\nNote: Dataset is assumed to be of constant size.\n\nParameters\n* dataset — Dataset used for sampling.\n* num_replicas (optional) — Number of processes participating in distributed training.\n* rank (optional) — Rank of the current process within num_replicas.\n* seed (int, optional) -—random seed used to shuffle the sampler if shuffle=True. This\nnumber should be identical across all processes in the distributed group. Default: 0.\nclass mmdet.datasets.samplers.DistributedSampler (dataset, num_replicas=None, rank=None,\nshuffle=True, seed=0)\nclass mmdet.datasets.samplers.GroupSampler (dataset, samples_per_gpu=1)\n\nclass mmdet.datasets.samplers.InfiniteBatchSampler (dataset, batch_size=1, world_size=None,\nrank=None, seed=0, shuffle=True)\nSimilar to BatchSampler warping a DistributedSampler. It is designed iteration-based runners like ‘IterBase-\ndRunner and yields a mini-batch indices each time.\n\nThe implementation logic is referred to https://github.com/facebookresearch/detectron2/blob/main/detectron2/\ndata/samplers/grouped_batch_sampler.py\n\nParameters\n¢ dataset (object) — The dataset.\n\n* batch_size (int) — When model is DistributedDataParallel, it is the number of\ntraining samples on each GPU, When model is DataParallel, it is num_gpus * sam-\nples_per_gpu. Default : 1.\n\n¢ world_size(int, optional) —Number of processes participating in distributed training.\nDefault: None.\n\n* rank (int, optional) — Rank of current process. Default: None.\n¢ seed (int) — Random seed. Default: 0.\n¢ shuffle (bool) — Whether shuffle the dataset or not. Default: True.\n\nset_epoch (epoch)\nNot supported in /terationBased runner.\n\nclass mmdet.datasets.samplers.InfiniteGroupBatchSampler (dataset, batch_size=1, world_size=None,\nrank=None, seed=0, shuffle=True)\nSimilar to BatchSampler warping a GroupSampler. It is designed for iteration-based runners like ‘IterBase-\ndRunner and yields a mini-batch indices each time, all indices in a batch should be in the same group.\n\n256 Chapter 38. mmdet.datasets\n", "vlm_text": "38.3 samplers \nclass  mmdet.datasets.samplers. Distributed Group Sampler ( dataset ,  samples per gpu  ${=}I$  , num replicas  $=$  None ,  rank  $\\mathbf{\\beta}=$  None ,  seed  $\\mathbf{\\chi}{=}0$  ) \nSampler that restricts data loading to a subset of the dataset. \nIt is especially useful in conjunction with  torch.nn.parallel.Distributed Data Parallel . In such case, each process can pass a Distributed Sampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it. \nNote:  Dataset is assumed to be of constant size. \nParameters \n•  dataset  – Dataset used for sampling. •  num replicas  ( optional ) – Number of processes participating in distributed training. •  rank  ( optional ) – Rank of the current process within num replicas. \n\n\n•  seed  ( int, optional ) – random seed used to shuffle the sampler if  shuffle  $=$  True . This number should be identical across all processes in the distributed group. Default: 0. \nclass  mmdet.datasets.samplers. Distributed Sampler ( dataset ,  num replicas  $\\mathbf{\\hat{\\Sigma}}$  None ,  rank  $\\mathbf{\\beta}=$  None , shuffle  $\\mathbf{=}$  True ,  seed  $\\scriptstyle\\left=O\\right\\}$  ) \nclass  mmdet.datasets.samplers. Group Sampler ( dataset ,  samples per gpu  $\\scriptstyle{\\varepsilon=I}$  ) \nclass  mmdet.datasets.samplers. Infinite Batch Sampler ( dataset ,  batch_size  $\\mathsf{\\Pi}_{=I}$  ,  world_size  $=$  None , rank  $\\mathbf{\\beta}=$  None ,  seed  $\\mathbf{\\chi}{=}0$  ,  shuf  ${\\mathcal{S}}{=}$  True ) \nSimilar to  Batch Sampler  warping a  Distributed Sampler. It is designed iteration-based runners like \\`IterBase- dRunner  and yields a mini-batch indices each time. \nThe implementation logic is referred to  https://github.com/facebook research/detectron2/blob/main/detectron2/ data/samplers/grouped batch sampler.py \nParameters \n•  dataset  ( object ) – The dataset. \n•  batch_size  ( int ) – When model is  Distributed Data Parallel , it is the number of training samples on each GPU, When model is  Data Parallel , it is  num_gpus \\* sam- ple s per gpu . Default : 1. •  world_size  ( int, optional ) – Number of processes participating in distributed training. Default: None. •  rank  ( int, optional ) – Rank of current process. Default: None. •  seed  ( int ) – Random seed. Default: 0. •  shuffle  ( bool ) – Whether shuffle the dataset or not. Default: True. \nset_epoch ( epoch ) Not supported in  Iteration Based  runner. \nclass  mmdet.datasets.samplers. Infinite Group Batch Sampler ( dataset ,  batch_size  ${=}I$  ,  world_size  $=$  None , rank  $\\mathbf{\\beta}=$  None ,  seed  $\\mathsf{\\chi}\\!=\\!\\!O$  ,  shuffle  $\\mathbf{\\chi}=$  True ) Similar to  Batch Sampler  warping a  Group Sampler. It is designed for iteration-based runners like \\`IterBase- dRunner  and yields a mini-batch indices each time, all indices in a batch should be in the same group. "}
{"page": 264, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_264.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nThe implementation logic is referred to https://github.com/facebookresearch/detectron2/blob/main/detectron2/\ndata/samplers/grouped_batch_sampler.py\n\nParameters\n¢ dataset (object) — The dataset.\n\n* batch_size (int) — When model is DistributedDataParallel, it is the number of\ntraining samples on each GPU. When model is DataParallel, it is num_gpus * sam-\nples_per_gpu. Default : 1.\n\n¢ world_size(int, optional) —Number of processes participating in distributed training.\nDefault: None.\n\n* rank (int, optional) — Rank of current process. Default: None.\n¢ seed (int) — Random seed. Default: 0.\n\n* shuffle (bool) — Whether shuffle the indices of a dummy epoch, it should be noted that\nshuffie can not guarantee that you can generate sequential indices because it need to ensure\nthat all indices in a batch is in a group. Default: True.\n\nset_epoch (epoch)\nNot supported in /terationBased runner.\n\n38.4 api_wrappers\n\nclass mmdet.datasets.api_wrappers .COCO(*args: Any, **kwargs: Any)\nThis class is almost the same as official pycocotools package.\n\nIt implements some snake case function aliases. So that the COCO class has the same interface as LVIS class.\n\n38.4. api_wrappers 257\n", "vlm_text": "The implementation logic is referred to  https://github.com/facebook research/detectron2/blob/main/detectron2/ data/samplers/grouped batch sampler.py \nParameters \n•  dataset  ( object ) – The dataset. •  batch_size  ( int ) – When model is  Distributed Data Parallel , it is the number of training samples on each GPU. When model is  Data Parallel , it is  num_gpus \\* sam- ple s per gpu . Default : 1. •  world_size  ( int, optional ) – Number of processes participating in distributed training. Default: None. •  rank  ( int, optional ) – Rank of current process. Default: None. •  seed  ( int ) – Random seed. Default: 0. •  shuffle  ( bool ) – Whether shuffle the indices of a dummy  epoch , it should be noted that shuffle  can not guarantee that you can generate sequential indices because it need to ensure that all indices in a batch is in a group. Default: True. \nset_epoch ( epoch ) Not supported in  Iteration Based  runner. \n38.4 api wrappers \nclass  mmdet.datasets.api wrappers. COCO ( \\*args: Any ,  \\*\\*kwargs: Any ) This class is almost the same as official py coco tools package. It implements some snake case function aliases. So that the COCO class has the same interface as LVIS class. "}
{"page": 265, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_265.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n258 Chapter 38. mmdet.datasets\n", "vlm_text": "MMDetection, Release 2.18.0\n\n258 Chapter 38. mmdet.datasets\n"}
{"page": 266, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_266.jpg", "ocr_text": "CHAPTER\nTHIRTYNINE\n\nMMDET.MODELS\n\n39.1 detectors\n\nclass mmdet.models.detectors.ATSS (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of ATSS.\n\nclass mmdet.models.detectors.AutoAssign(backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None)\nImplementation of AutoAssign: Differentiable Label Assignment for Dense Object Detection.\n\nclass mmdet.models.detectors.BaseDetector (init_cfg=None)\nBase class for detectors.\n\nabstract aug_test (imgs, img_metas, **kwargs)\nTest function with test time augmentation.\n\nabstract extract_feat (imgs)\nExtract features from images.\n\nextract_feats(imgs)\nExtract features from multiple images.\n\nParameters imgs (list [torch.Tensor])-A list of images. The images are augmented from\nthe same image but in different ways.\n\nReturns Features of different images\nReturn type list[torch.Tensor]\n\nforward (img, img_metas, return_loss=True, **kwargs)\nCalls either forward_train() or forward_test() depending on whether return_loss is True.\n\nNote this setting will change the expected inputs. When return_loss=True, img and img_meta are\nsingle-nested (i.e. Tensor and List[dict]), and when resturn_loss=False, img and img_meta should be\ndouble nested (i.e. List[Tensor], List[List[dict]]), with the outer list indicating test time augmentations.\n\nforward_test (imgs, img_metas, **kwargs)\n\nParameters\n\n¢ imgs (List [Tensor ]) — the outer list indicates test-time augmentations and inner Tensor\nshould have a shape NxCxHxW, which contains all images in the batch.\n\n¢ img_metas (List [List [dict]]) — the outer list indicates test-time augs (multiscale,\nflip, etc.) and the inner list indicates images in a batch.\n\n259\n", "vlm_text": "MMDET.MODELS \n39.1 detectors \nclass  mmdet.models.detectors. ATSS ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) Implementation of  ATSS . class  mmdet.models.detectors. AutoAssign ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained=None ) Implementation of  AutoAssign: Differentiable Label Assignment for Dense Object Detection . class  mmdet.models.detectors. Base Detector ( init_cfg  $\\mathbf{\\beta}=$  None ) Base class for detectors. abstract aug_test ( imgs ,  img_metas ,  \\*\\*kwargs ) Test function with test time augmentation. abstract extract feat ( imgs ) Extract features from images. extract feats ( imgs ) Extract features from multiple images. Parameters  imgs  ( list[torch.Tensor] ) – A list of images. The images are augmented from the same image but in different ways. Returns  Features of different images Return type  list[torch.Tensor] forward ( img ,  img_metas ,  return loss  $=$  True ,  \\*\\*kwargs ) Calls either  forward train()  or  forward test()  depending on whether  return loss  is  True . Note this setting will change the expected inputs. When  return loss  $=$  True , img and img_meta are single-nested (i.e. Tensor and List[dict]), and when  res turn loss  $=$  False , img and img_meta should be double nested (i.e. List[Tensor], List[List[dict]]), with the outer list indicating test time augmentations. forward test ( imgs ,  img_metas ,  \\*\\*kwargs ) Parameters •  imgs  ( List[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains all images in the batch. \nflip, etc.) and the inner list indicates images in a batch. "}
{"page": 267, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_267.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward_train(imgs, img_metas, **kwargs)\n\nParameters\n\n¢ img (list [Tensor]) — List of tensors of shape (1, C, H, W). Typically these should be\nmean centered and std scaled.\n\n¢ img_metas (list [dict]) — List of image info dict where each dict has: ‘img_shape’,\n“scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and\n‘img_norm_cfg’. For details on the values of these keys, see mmdet.datasets.\npipelines. Collect.\n\n¢ kwargs (keyword arguments) — Specific to concrete implementation.\n\nshow_result (img, result, score_thr=0.3, bbox_color=(72, 101, 241), text_color=(72, 101, 241),\nmask_color=None, thickness=2, font_size=13, win_name=\", show=False, wait_time=0,\nout_file=None)\n\nDraw result over img.\n\nParameters\n¢ img(str or Tensor) — The image to be displayed.\n\n¢ result (Tensor or tuple) -—The results to draw over img bbox_result or (bbox_result,\nsegm_result).\n\nscore_thr (float, optional) — Minimum score of bboxes to be shown. Default: 0.3.\n\nbbox_color (str or tuple(int) or Color) — Color of bbox lines. The tuple of color should\nbe in BGR order. Default: ‘green’\n\ntext_color (str or tuple(int) or Color) — Color of texts. The tuple of color should be in\nBGR order. Default: ‘green’\n\n¢ mask_color (None or str or tuple(int) or Color) — Color of masks. The tuple of color\nshould be in BGR order. Default: None\n\n¢ thickness (int) — Thickness of lines. Default: 2\n\n¢ font_size (int) — Font size of texts. Default: 13\n\n¢ win_name (str) — The window name. Default: °’\n\n¢ wait_time (float) — Value of waitKey param. Default: 0.\n\n¢ show (bool) — Whether to show the image. Default: False.\n\n* out_file (str or None) — The filename to write the image. Default: None.\nReturns Only if not show or out_file\nReturn type img (Tensor)\n\ntrain_step (data, optimizer)\nThe iteration step during training.\n\nThis method defines an iteration step during training, except for the back propagation and optimizer up-\ndating, which are done in an optimizer hook. Note that in some complicated cases or models, the whole\nprocess including back propagation and optimizer updating is also defined in this method, such as GAN.\n\nParameters\n\n¢ data (dict) - The output of dataloader.\n\n260 Chapter 39. mmdet.models\n", "vlm_text": "forward train ( imgs ,  img_metas ,  \\*\\*kwargs ) \nParameters \n•  img  ( list[Tensor] ) – List of tensors of shape (1, C, H, W). Typically these should be mean centered and std scaled. •  img_metas  ( list[dict] ) – List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys, see  mmdet.datasets. pipelines.Collect . •  kwargs  ( keyword arguments ) – Specific to concrete implementation. \nshow result ( img ,  result ,  score_th  $\\scriptstyle r=0.3$  ,  bbox_color=(72, 101, 241) ,  text_color=(72, 101, 241) , mask_colo  $\\leftrightharpoons$  None ,  thickness  $_{;=2}$  ,  font_siz  $_{\\mathit{z}=I3}$  ,  win_name  $\\mathbf{\\lambda}={}^{\\prime\\prime}$  ,  show  $\\mathbf{\\hat{\\rho}}$  False ,  wait_time  $\\mathrm{=}0$  , out_file  $\\leftrightharpoons$  None ) Draw  result  over  img . \nParameters \n•  img  ( str or Tensor ) – The image to be displayed. •  result  ( Tensor or tuple ) – The results to draw over  img  b box result or (b box result, seg m result). •  score_thr  ( float, optional ) – Minimum score of bboxes to be shown. Default: 0.3. •  bbox_color  (str or tuple(int) or  Color ) – Color of bbox lines. The tuple of color should be in BGR order. Default: ‘green’ •  text_color  (str or tuple(int) or  Color ) – Color of texts. The tuple of color should be in BGR order. Default: ‘green’ •  mask_color  (None or str or tuple(int) or  Color ) – Color of masks. The tuple of color should be in BGR order. Default: None •  thickness  ( int ) – Thickness of lines. Default: 2 •  font_size  ( int ) – Font size of texts. Default: 13 •  win_name  ( str ) – The window name. Default: ‘’ •  wait_time  ( float ) – Value of waitKey param. Default: 0. •  show  ( bool ) – Whether to show the image. Default: False. •  out_file  ( str or None ) – The filename to write the image. Default: None. \nReturns  Only if not  show  or  out_file Return type  img (Tensor) \ntrain_step ( data ,  optimizer ) The iteration step during training. \nThis method defines an iteration step during training, except for the back propagation and optimizer up- dating, which are done in an optimizer hook. Note that in some complicated cases or models, the whole process including back propagation and optimizer updating is also defined in this method, such as GAN. \nParameters \n•  data  ( dict ) – The output of dataloader. "}
{"page": 268, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_268.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* optimizer (torch.optim.Optimizer | dict) — The optimizer of runner is passed to\ntrain_step(). This argument is unused and reserved.\n\nReturns\nIt should contain at least 3 keys: loss, log_vars, num_samples.\n¢ loss is a tensor for back propagation, which can be a weighted sum of multiple losses.\n¢ log_vars contains all the variables to be sent to the logger.\n\n* num_samples indicates the batch size (when the model is DDP, it means the batch size on\neach GPU), which is used for averaging the logs.\n\nReturn type dict\n\nval_step (data, optimizer=None)\nThe iteration step during validation.\n\nThis method shares the same signature as train_step(), but used during val epochs. Note that the eval-\nuation after training epochs is not implemented with this method, but an evaluation hook.\n\nproperty with_bbox\nwhether the detector has a bbox head\nType boo!\nproperty with_mask\nwhether the detector has a mask head\nType boo!\nproperty with_neck\nwhether the detector has a neck\nType boo!\nproperty with_shared_head\nwhether the detector has a shared head in the Rol Head\n\nType boo!\n\nclass mmdet.models.detectors.CascadeRCNN (backbone, neck=None, rpn_head=None, roi_head=None,\ntrain_cfg=None, test_cfg=None, pretrained=None,\ninit_cfg=None)\nImplementation of Cascade R-CNN: Delving into High Quality Object Detection\n\nshow_result (data, result, **kwargs)\nShow prediction results of the detector.\n\nParameters\n¢ data(str or np.ndarray) — Image filename or loaded image.\n\n¢ result (Tensor or tuple) -—The results to draw over img bbox_result or (bbox_result,\nsegm_result).\n\nReturns The image with bboxes drawn on it.\nReturn type np.ndarray\n\nclass mmdet.models.detectors.CenterNet (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of CenterNet(Objects as Points)\n\n<https://arxiv.org/abs/1904.07850>.\n\n39.1. detectors 261\n", "vlm_text": "•  optimizer  ( torch.optim.Optimizer  | dict) – The optimizer of runner is passed to train_step() . This argument is unused and reserved. \nReturns \nIt should contain at least 3 keys:  loss ,  log_vars ,  num samples . •  loss  is a tensor for back propagation, which can be a weighted sum of multiple losses. •  log_vars  contains all the variables to be sent to the logger. •  num samples  indicates the batch size (when the model is DDP, it means the batch size on each GPU), which is used for averaging the logs. \nReturn type  dict \nval_step ( data ,  optimize  $r{=}$  None ) The iteration step during validation. This method shares the same signature as  train_step() , but used during val epochs. Note that the eval- uation after training epochs is not implemented with this method, but an evaluation hook. \nproperty with_bbox whether the detector has a bbox head \nType  bool \nproperty with_mask whether the detector has a mask head \nproperty with_neck whether the detector has a neck \nType  bool \nproperty with shared head whether the detector has a shared head in the RoI Head \nType  bool \nclass  mmdet.models.detectors. Cascade R CNN ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  rpn_head=None ,  roi_head  $\\leftrightharpoons$  None , train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  pretrained  $\\leftrightharpoons$  None , init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) Implementation of  Cascade R-CNN: Delving into High Quality Object Detection \nshow result ( data ,  result ,  \\*\\*kwargs ) Show prediction results of the detector. \nParameters \n•  data  ( str or np.ndarray ) – Image filename or loaded image. •  result  ( Tensor or tuple ) – The results to draw over  img  b box result or (b box result, seg m result). Returns  The image with bboxes drawn on it. \nReturn type  np.ndarray \nclass  mmdet.models.detectors. CenterNet ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $=$  None ) \nImplementation of CenterNet(Objects as Points) < https://arxiv.org/abs/1904.07850 >. "}
{"page": 269, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_269.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\naug_test (imgs, img_metas, rescale=True)\nAugment testing of CenterNet. Aug test must have flipped image pair, and unlike CornerNet, it will perform\nan averaging operation on the feature map instead of detecting bbox.\n\nParameters\n¢ imgs (list [Tensor ]) — Augmented images.\n\n¢ img_metas (list [list [dict]]) — Meta information of each image, e.g., image size,\nscaling factor, etc.\n\n* rescale (bool) — If True, return boxes in original image space. Default: True.\n\nNote: imgs must including flipped image pairs.\n\nReturns\n\nBBox results of each image and classes. The outer list corresponds to each image. The in-\nner list corresponds to each class.\n\nReturn type list[list{np.ndarray]]\nmerge_aug_results (aug_results, with_nms)\nMerge augmented detection bboxes and score.\n\nParameters\n* aug_results (list [list [Tensor] ]) — Det_bboxes and det_labels of each image.\n¢ with_nms (boo1) — If True, do nms before return boxes.\n\nReturns (out_bboxes, out_labels)\n\nReturn type tuple\n\nclass mmdet.models.detectors.CornerNet (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nCornerNet.\n\nThis detector is the implementation of the paper CornerNet: Detecting Objects as Paired Keypoints .\n\naug_test (igs, img_metas, rescale=False)\nAugment testing of CornerNet.\n\nParameters\n¢ imgs (list [Tensor]) — Augmented images.\n\n¢ img_metas (list [list [dict]]) — Meta information of each image, e.g., image size,\nscaling factor, etc.\n\n¢ rescale (bool) — If True, return boxes in original image space. Default: False.\n\nNote: imgs must including flipped image pairs.\n\nReturns\n\nBBox results of each image and classes. The outer list corresponds to each image. The in-\nner list corresponds to each class.\n\nReturn type list[list{np.ndarray]]\n\n262 Chapter 39. mmdet.models\n", "vlm_text": "aug_test ( imgs ,  img_metas ,  rescale  $=$  True ) Augment testing of CenterNet. Aug test must have flipped image pair, and unlike CornerNet, it will perform an averaging operation on the feature map instead of detecting bbox. \nParameters \n•  imgs  ( list[Tensor] ) – Augmented images. •  img_metas  ( list[list[dict]] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: True. \nNote:  imgs  must including flipped image pairs. \nReturns \nBBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. \nReturn type  list[list[np.ndarray]] \nmerge aug results ( aug results ,  with_nms ) Merge augmented detection bboxes and score. \nParameters \n•  aug results  ( list[list[Tensor]] ) – Det_bboxes and det_labels of each image. \n•  with_nms  ( bool ) – If True, do nms before return boxes. \nReturns  (out_bboxes, out_labels) \nReturn type  tuple \nclass  mmdet.models.detectors. CornerNet ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nCornerNet. \nThis detector is the implementation of the paper  CornerNet: Detecting Objects as Paired Keypoints \naug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\dot{\\rho}}=$  False ) Augment testing of CornerNet. \nParameters \n•  imgs  ( list[Tensor] ) – Augmented images. •  img_metas  ( list[list[dict]] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. \nNote:  imgs  must including flipped image pairs. \nReturns \nBBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. \nReturn type  list[list[np.ndarray]] "}
{"page": 270, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_270.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmerge_aug_results (aug_results, img_metas)\nMerge augmented detection bboxes and score.\n\nParameters\n\n* aug_results (list [list [Tensor] ]) — Det_bboxes and det_labels of each image.\n\n¢ img_metas (list [list [dict]]) — Meta information of each image, e.g., image size,\nscaling factor, etc.\n\nReturns (bboxes, labels)\nReturn type tuple\n\nclass mmdet.models.detectors.DETR (backbone, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of DETR: End-to-End Object Detection with Transformers\n\nforward_dummy (img)\nUsed for computing network flops.\n\nSee mmdetection/tools/analysis_tools/get_flops.py\n\nonnx_export (img, img_metas)\nTest function for exporting to ONNX, without test time augmentation.\n\nParameters\n¢ img (torch. Tensor) — input images.\n¢ img_metas (list [dict ]) — List of image information.\nReturns\ndets of shape [N, num_det, 5] and class labels of shape [N, num_det].\nReturn type tuple[Tensor, Tensor]\nclass mmdet.models.detectors .DeformableDETR(“args, **kwargs)\n\nclass mmdet.models.detectors.FCOS (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of FCOS\n\nclass mmdet.models.detectors.FOVEA (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of FoveaBox\n\nclass mmdet.models.detectors.FSAF (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of FSAF\n\nclass mmdet.models.detectors.FastRCNN (backbone, roi_head, train_cfg, test_cfg, neck=None,\npretrained=None, init_cfg=None)\n\nImplementation of Fast R-CNN\n\nforward_test (imgs, img_metas, proposals, **kwargs)\n\nParameters\n\n¢ imgs (List [Tensor ]) — the outer list indicates test-time augmentations and inner Tensor\nshould have a shape NxCxHxW, which contains all images in the batch.\n\n¢ img_metas (List [List [dict]]) — the outer list indicates test-time augs (multiscale,\nflip, etc.) and the inner list indicates images in a batch.\n\n39.1. detectors 263\n", "vlm_text": "merge aug results ( aug results ,  img_metas ) Merge augmented detection bboxes and score. \nParameters \n•  aug results  ( list[list[Tensor]] ) – Det_bboxes and det_labels of each image. •  img_metas  ( list[list[dict]] ) – Meta information of each image, e.g., image size, scaling factor, etc. \nReturns  (bboxes, labels) \nReturn type  tuple \nclass  mmdet.models.detectors. DETR ( backbone ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg  $=$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) Implementation of  DETR: End-to-End Object Detection with Transformers \nforward dummy ( img ) \nUsed for computing network flops. See  mm detection/tools/analysis tools/get_flops.py \non nx export ( img ,  img_metas ) Test function for exporting to ONNX, without test time augmentation. \nParameters \n•  img  ( torch.Tensor ) – input images. •  img_metas  ( list[dict] ) – List of image information. \nReturns \ndets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. Return type  tuple[Tensor, Tensor] \nclass  mmdet.models.detectors. De formable DE TR ( \\*args ,  \\*\\*kwargs ) \nclass  mmdet.models.detectors. FCOS ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) \nImplementation of  FCOS \nclass  mmdet.models.detectors. FOVEA ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ) \nImplementation of  FoveaBox \nclass  mmdet.models.detectors. FSAF ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\beta}=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nImplementation of  FSAF \nclass  mmdet.models.detectors. FastRCNN ( backbone ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) \nImplementation of  Fast R-CNN \nforward test ( imgs ,  img_metas ,  proposals ,  \\*\\*kwargs ) \nParameters \n•  imgs  ( List[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains all images in the batch. •  img_metas  ( List[List[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. "}
{"page": 271, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_271.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* proposals (List [List [Tensor] ]) — the outer list indicates test-time augs (multiscale,\nflip, etc.) and the inner list indicates images in a batch. The Tensor should have a shape\nPx4, where P is the number of proposals.\n\nclass mmdet.models.detectors.FasterRCNN (backbone, rpn_head, roi_head, train_cfg, test_cfg, neck=None,\npretrained=None, init_cfg=None)\nImplementation of Faster R-CNN\n\nclass mmdet.models.detectors.GFL (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\n\nclass mmdet.models.detectors.GridRCNN (backbone, rpn_head, roi_head, train_cfg, test_cfg, neck=None,\npretrained=None, init_cfg=None)\nGrid R-CNN.\n\nThis detector is the implementation of: - Grid R-CNN (https://arxiv.org/abs/1811.12030) - Grid R-CNN Plus:\nFaster and Better (https://arxiv.org/abs/1906.05688)\n\nclass mmdet.models.detectors.HybridTaskCascade(**kwargs)\nImplementation of HTC\n\nproperty with_semantic\nwhether the detector has a semantic head\n\nType bool\n\nclass mmdet.models.detectors.KnowledgeDistillationSingleStageDetector (backbone, neck,\nbbox_head,\nteacher_config,\nteacher_ckpt=None,\neval_teacher=True,\ntrain_cfg=None,\ntest_cfg=None,\npretrained=None)\n\nImplementation of Distilling the Knowledge in a Neural Network..\n\nParameters\n* teacher_config (str | dict) — Config file path or the config object of teacher model.\n\n* teacher_ckpt (str, optional) — Checkpoint path of teacher model. If left as None, the\nmodel will not load any weights.\n\ncuda(device=None)\nSince teacher_model is registered as a plain object, it is necessary to put the teacher model to cuda when\ncalling cuda function.\n\nforward_train(img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None)\n\nParameters\n\n¢ img (Tensor) — Input images of shape (N, C, H, W). Typically these should be mean cen-\ntered and std scaled.\n\n¢ img_metas (list [dict])— A List of image info dict where each dict has: ‘img_shape’,\n“scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and\n‘img_norm_cfg’. For details on the values of these keys see mmdet.datasets.\npipelines. Collect.\n\n* gt_bboxes (list [Tensor ]) — Each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n264 Chapter 39. mmdet.models\n", "vlm_text": "•  proposals  ( List[List[Tensor]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. The Tensor should have a shape  $\\mathrm{Px4}$  , where P is the number of proposals. \nclass  mmdet.models.detectors. FasterRCNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck  $\\mathbf{\\beta}=$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) \nImplementation of  Faster R-CNN \nclass  mmdet.models.detectors. GFL ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) \nclass  mmdet.models.detectors. GridRCNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nGrid R-CNN. \nThis detector is the implementation of: - Grid R-CNN ( https://arxiv.org/abs/1811.12030 ) - Grid R-CNN Plus: Faster and Better ( https://arxiv.org/abs/1906.05688 ) \nclass  mmdet.models.detectors. Hybrid Task Cascade ( \\*\\*kwargs ) Implementation of  HTC \nproperty with semantic whether the detector has a semantic head \nType  bool \nclass  mmdet.models.detectors. Knowledge Distillation Single Stage Detector ( backbone ,  neck , \nImplementation of  Distilling the Knowledge in a Neural Network. . \nbbox_head , teacher config , teacher ck pt=None , e val teach e  $\\asymp$  True , train_cfg  $\\mathbf{\\beta}=$  None , test_cfg  $=$  None , pretrained  $\\leftrightharpoons$  None ) \nParameters \n•  teacher config  ( str | dict ) – Config file path or the config object of teacher model. \n•  teacher ck pt  ( str, optional ) – Checkpoint path of teacher model. If left as None, the model will not load any weights. \ncuda ( device=None ) Since teacher model is registered as a plain object, it is necessary to put the teacher model to cuda when calling cuda function. \nforward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $:=$  None ) \nParameters \n•  img  ( Tensor ) – Input images of shape (N, C, H, W). Typically these should be mean cen- tered and std scaled. •  img_metas  ( list[dict] ) – A List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. "}
{"page": 272, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_272.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ gt_labels (list [Tensor ]) — Class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — Specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\ntrain (mode=True)\nSet the same train mode for teacher and student model.\n\nclass mmdet.models.detectors.MaskRCNN (backbone, rpn_head, roi_head, train_cfg, test_cfg, neck=None,\npretrained=None, init_cfg=None)\nImplementation of Mask R-CNN\n\nclass mmdet.models.detectors.MaskScoringRCNN (backbone, rpn_head, roi_head, train_cfg, test_cfg,\nneck=None, pretrained=None, init_cfg=None)\nMask Scoring RCNN.\n\nhttps://arxiv.org/abs/1903.00241\n\nclass mmdet.models.detectors .NASFCOS (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nNAS-FCOS: Fast Neural Architecture Search for Object Detection.\n\nhttps://arxiv.org/abs/1906.0442\n\nclass mmdet.models.detectors.PAA(backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of PAA.\n\nclass mmdet.models.detectors.PanopticFPN (backbone, neck=None, rpn_head=None, roi_head=None,\ntrain_cfg=None, test_cfg=None, pretrained=None,\ninit_cfg=None, semantic_head=None,\npanoptic_fusion_head=None)\nImplementation of Panoptic feature pyramid networks\n\nclass mmdet.models.detectors.PointRend (backbone, rpn_head, roi_head, train_cfg, test_cfg, neck=None,\npretrained=None, init_cfg=None)\nPointRend: Image Segmentation as Rendering\n\nThis detector is the implementation of PointRend.\n\nclass mmdet.models.detectors.QueryInst (backbone, rpn_head, roi_head, train_cfg, test_cfg, neck=None,\npretrained=None, init_cfg=None)\nImplementation of Instances as Queries\n\nclass mmdet.models.detectors.RPN (backbone, neck, rpn_head, train_cfg, test_cfg, pretrained=None,\ninit_cfg=None)\nImplementation of Region Proposal Network.\n\naug_test (igs, img_metas, rescale=False)\nTest function with test time augmentation.\n\nParameters\n¢ imgs (list [torch.Tensor]) — List of multiple images\n¢ img_metas (list [dict ]) — List of image information.\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\n\nReturns proposals\n\n39.1. detectors 265\n", "vlm_text": "•  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box \n gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \ntrain ( mode  $=$  True ) Set the same train mode for teacher and student model. \nclass  mmdet.models.detectors. MaskRCNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) \nImplementation of  Mask R-CNN \nclass  mmdet.models.detectors. Mask Scoring R CNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg , neck  $\\mathbf{\\beta}=$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nMask Scoring RCNN. \nhttps://arxiv.org/abs/1903.00241 \nclass  mmdet.models.detectors. NASFCOS ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) NAS-FCOS: Fast Neural Architecture Search for Object Detection. \nhttps://arxiv.org/abs/1906.0442 \nclass  mmdet.models.detectors. PAA ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) \nImplementation of  PAA . \nclass  mmdet.models.detectors. Pan optic FP N ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  rpn_head=None ,  roi_head=None , train_cfg  $=$  None ,  test_cfg  $\\leftrightharpoons$  None ,  pretrained=None , init_cfg  $=$  None ,  semantic head  $\\leftrightharpoons$  None , pan optic fusion head  $\\leftrightharpoons$  None ) \nImplementation of  Panoptic feature pyramid networks \nclass  mmdet.models.detectors. PointRend ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) \nPointRend: Image Segmentation as Rendering This detector is the implementation of  PointRend . \nclass  mmdet.models.detectors. QueryInst ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg ,  neck=None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $=$  None ) \nImplementation of  Instances as Queries \nclass  mmdet.models.detectors. RPN ( backbone ,  neck ,  rpn_head ,  train_cfg ,  test_cfg ,  pretrained=None , init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ) Implementation of Region Proposal Network. \naug_test ( imgs ,  img_metas ,  rescale  $=$  False ) Test function with test time augmentation. \nParameters \nReturns  proposals "}
{"page": 273, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_273.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type list[np.ndarray]\n\nextract_feat (img)\nExtract features.\n\nParameters img (torch. Tensor) — Image tensor with shape (n, c, h ,w).\nReturns\n\nMulti-level features that may have different resolutions.\nReturn type list[torch.Tensor]\n\nforward_dummy (img)\nDummy forward function.\n\nforward_train(img, img_metas, gt_bboxes=None, gt_bboxes_ignore=None)\n\nParameters\n\n¢ img (Tensor) — Input images of shape (N, C, H, W). Typically these should be mean cen-\ntered and std scaled.\n\n¢ img_metas (list [dict])— A List of image info dict where each dict has: ‘img_shape’,\n“scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and\n‘img_norm_cfg’. For details on the values of these keys see mmdet.datasets.\npipelines. Collect.\n\n* gt_bboxes (list [Tensor ]) — Each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n* gt_bboxes_ignore (None | list[Tensor]) — Specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nshow_result (data, result, top_k=20, **kwargs)\nShow RPN proposals on the image.\n\nParameters\n¢ data(str or np.ndarray) — Image filename or loaded image.\n\n¢ result (Tensor or tuple) -—The results to draw over img bbox_result or (bbox_result,\nsegm_result).\n\n* top_k (int) — Plot the first k bboxes only if set positive. Default: 20\nReturns The image with bboxes drawn on it.\nReturn type np.ndarray\n\nsimple_test (img, img_metas, rescale=False)\nTest function without test time augmentation.\n\nParameters\n¢ imgs (list [torch.Tensor]) — List of multiple images\n¢ img_metas (list [dict ]) — List of image information.\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\n\nReturns proposals\n\n266 Chapter 39. mmdet.models\n", "vlm_text": "Return type  list[np.ndarray] \nextract feat ( img ) \nExtract features. Parameters  img  ( torch.Tensor ) – Image tensor with shape (n, c, h ,w). \nReturns Multi-level features that may have  different resolutions. \nReturn type  list[torch.Tensor] \nforward dummy ( img ) Dummy forward function. \nforward train ( img ,  img_metas ,  gt_bboxes  $\\leftrightharpoons$  None ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) \nParameters \n•  img  ( Tensor ) – Input images of shape (N, C, H, W). Typically these should be mean cen- tered and std scaled. •  img_metas  ( list[dict] ) – A List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nshow result ( data ,  result ,  top_  $k{=}20$  ,  \\*\\*kwargs ) Show RPN proposals on the image. \nParameters \n•  data  ( str or np.ndarray ) – Image filename or loaded image. •  result  ( Tensor or tuple ) – The results to draw over  img  b box result or (b box result, seg m result). •  top_k  ( int ) – Plot the first k bboxes only if set positive. Default: 20 Returns  The image with bboxes drawn on it. Return type  np.ndarray \nsimple test ( img ,  img_metas ,  rescale=False ) Test function without test time augmentation. \nParameters \n•  imgs  ( list[torch.Tensor] ) – List of multiple images •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns  proposals "}
{"page": 274, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_274.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type list[np.ndarray]\n\nclass mmdet.models.detectors.RepPointsDetector (backbone, neck, bbox_head, train_cfg=None,\ntest_cfg=None, pretrained=None, init_cfg=None)\nRepPoints: Point Set Representation for Object Detection.\n\nThis detector is the implementation of: - RepPoints detector (https://arxiv.org/pdf/1904. 11490)\n\nclass mmdet.models.detectors.RetinaNet (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of RetinaNet\n\nclass mmdet.models.detectors.SCNet (**kwargs)\nImplementation of SCNet\n\nclass mmdet.models.detectors.SOLO(backbone, neck=None, bbox_head=None, mask_head=None,\ntrain_cfg=None, test_cfg=None, init_cfg=None, pretrained=None)\nSOLO: Segmenting Objects by Locations\n\nclass mmdet.models.detectors.SingleStageDetector (backbone, neck=None, bbox_head=None,\ntrain_cfg=None, test_cfg=None, pretrained=None,\ninit_cfg=None)\n\nBase class for single-stage detectors.\n\nSingle-stage detectors directly and densely predict bounding boxes on the output features of the backbone+neck.\n\naug_test (igs, img_metas, rescale=False)\nTest function with test time augmentation.\n\nParameters\n\n¢ imgs (list [Tensor ]) — the outer list indicates test-time augmentations and inner Tensor\nshould have a shape NxCxHxW, which contains all images in the batch.\n\n¢ img_metas (list[list[dict]]) — the outer list indicates test-time augs (multiscale,\nflip, etc.) and the inner list indicates images in a batch. each dict has image information.\n\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\nReturns\n\nBBox results of each image and classes. The outer list corresponds to each image. The in-\nner list corresponds to each class.\n\nReturn type list[list{np.ndarray]]\n\nextract_feat (img)\nDirectly extract features from the backbone+neck.\n\nforward_dummy (img)\nUsed for computing network flops.\n\nSee mmdetection/tools/analysis_tools/get_flops.py\n\nforward_train(img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None)\n\nParameters\n\n¢ img (Tensor) — Input images of shape (N, C, H, W). Typically these should be mean cen-\ntered and std scaled.\n\n¢ img_metas (list [dict])— A List of image info dict where each dict has: ‘img_shape’,\n“scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and\n\n39.1. detectors 267\n", "vlm_text": "Return type  list[np.ndarray] \nclass  mmdet.models.detectors. Rep Points Detector ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None , test_cfg  $\\leftrightharpoons$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) RepPoints: Point Set Representation for Object Detection. \nThis detector is the implementation of: - RepPoints detector ( https://arxiv.org/pdf/1904.11490 ) \nclass  mmdet.models.detectors. RetinaNet ( backbone ,  neck ,  bbox_head ,  train_cfg=None ,  test_cfg=None , pretrained  $\\fallingdotseq$  None ,  init_cfg  $=$  None ) \nclass  mmdet.models.detectors. SCNet ( \\*\\*kwargs ) Implementation of  SCNet \nclass  mmdet.models.detectors. SOLO ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  bbox_head  $\\leftrightharpoons$  None ,  mask_head=None , train_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  pretrained  $\\leftrightharpoons$  None ) SOLO: Segmenting Objects by Locations \nclass  mmdet.models.detectors. Single Stage Detector ( backbone ,  neck  $\\leftrightharpoons$  None ,  bbox_head=None , train_cfg  $=$  None ,  test_cfg  $=$  None ,  pretrained=None , init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) \nBase class for single-stage detectors. \nSingle-stage detectors directly and densely predict bounding boxes on the output features of the backbone+neck. \naug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test function with test time augmentation. \nParameters \n•  imgs  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns \nBBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. \nReturn type  list[list[np.ndarray]] \nextract feat ( img ) Directly extract features from the backbone+neck. \nforward dummy ( img ) Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py \nforward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $:=$  None ) \nParameters \n•  img  ( Tensor ) – Input images of shape (N, C, H, W). Typically these should be mean cen- tered and std scaled. •  img_metas  ( list[dict] ) – A List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and "}
{"page": 275, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_275.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n‘img_norm_cfg’. For details on the values of these keys see mmdet.datasets.\npipelines. Collect.\n\n* gt_bboxes (list [Tensor ]) — Each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — Class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — Specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nonnx_export (img, img_metas, with_nms=True)\nTest function without test time augmentation.\n\nParameters\n\n¢ img (torch. Tensor) — input images.\n\n¢ img_metas (list [dict ]) — List of image information.\nReturns\n\ndets of shape [N, num_det, 5] and class labels of shape [N, num_det].\nReturn type tuple[Tensor, Tensor]\n\nsimple_test (img, img_metas, rescale=False)\nTest function without test-time augmentation.\n\nParameters\n\n¢ img (torch. Tensor) — Images with shape (N, C, H, W).\n\n¢ img_metas (list [dict ]) — List of image information.\n\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\nReturns\n\nBBox results of each image and classes. The outer list corresponds to each image. The in-\nner list corresponds to each class.\n\nReturn type list[list{np.ndarray]]\n\nclass mmdet.models.detectors.SparseRCNN(*args, **kwargs)\nImplementation of Sparse R-CNN: End-to-End Object Detection with Learnable Proposals\n\nforward_dummy (img)\nUsed for computing network flops.\n\nSee mmdetection/tools/analysis_tools/get_flops.py\n\nforward_train(img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None, gt_masks=None,\nproposals=None, **kwargs)\nForward function of SparseR-CNN and QueryInst in train stage.\n\nParameters\n\n¢ img (Tensor) — of shape (N, C, H, W) encoding input images. Typically these should be\nmean centered and std scaled.\n\n268 Chapter 39. mmdet.models\n", "vlm_text": "‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \non nx export ( img ,  img_metas ,  with_nms  $\\mathbf{:=}$  True ) Test function without test time augmentation. \nParameters \n•  img  ( torch.Tensor ) – input images. •  img_metas  ( list[dict] ) – List of image information. \nReturns \ndets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. Return type  tuple[Tensor, Tensor] \nsimple test ( img ,  img_metas ,  rescale  $=$  False ) Test function without test-time augmentation. \nParameters \n•  img  ( torch.Tensor ) – Images with shape (N, C, H, W). •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns \nBBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. Return type  list[list[np.ndarray]] \nclass  mmdet.models.detectors. SparseRCNN ( \\*args ,  \\*\\*kwargs ) Implementation of  Sparse R-CNN: End-to-End Object Detection with Learnable Proposals \nforward dummy ( img ) \nUsed for computing network flops. See  mm detection/tools/analysis tools/get_flops.py \nforward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_masks  $\\leftrightharpoons$  None , proposal  $\\overleftarrow{}$  None ,  \\*\\*kwargs ) Forward function of SparseR-CNN and QueryInst in train stage. \nParameters \n•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. "}
{"page": 276, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_276.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ img_metas (list [dict]) — list of image info dict where each dict has: ‘img_shape’,\n“scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and\n‘img_norm_cfg’. For details on the values of these keys see mmdet.datasets.\npipelines. Collect.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor) — specify which bounding boxes can be ig-\nnored when computing the loss.\n\n¢ gt_masks (List[Tensor], optional) — Segmentation masks for each box. This is\nrequired to train QueryInst.\n\n* proposals (List [Tensor], optional) — override rpn proposals with custom propos-\nals. Use when with_rpn is False.\n\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\nsimple_test (img, img_metas, rescale=False)\nTest function without test time augmentation.\n\nParameters\n\n¢ imgs (list [torch. Tensor]) — List of multiple images\n\n¢ img_metas (list [dict ]) — List of image information.\n\n¢ rescale (bool) — Whether to rescale the results. Defaults to False.\nReturns\n\nBBox results of each image and classes. The outer list corresponds to each image. The in-\nner list corresponds to each class.\n\nReturn type list[list{np.ndarray]]\n\nclass mmdet.models.detectors.TridentFasterRCNN (backbone, rpn_head, roi_head, train_cfg, test_cfg,\nneck=None, pretrained=None, init_cfg=None)\nImplementation of TridentNet\n\naug_test (igs, img_metas, rescale=False)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nforward_train(img, img_metas, gt_bboxes, gt_labels, **kwargs)\nmake copies of img and gts to fit multi-branch.\n\nsimple_test (img, img_metas, proposals=None, rescale=False)\nTest without augmentation.\n\nclass mmdet.models.detectors.TwoStageDetector (backbone, neck=None, rpn_head=None,\nroi_head=None, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nBase class for two-stage detectors.\n\nTwo-stage detectors typically consisting of a region proposal network and a task-specific regression head.\n\nasync async_simple_test (img, img_meta, proposals=None, rescale=False)\nAsync test without augmentation.\n\n39.1. detectors 269\n", "vlm_text": "•  img_metas  ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see  mmdet.datasets. pipelines.Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor ) – specify which bounding boxes can be ig- nored when computing the loss. •  gt_masks  ( List[Tensor], optional ) – Segmentation masks for each box. This is required to train QueryInst. •  proposals  ( List[Tensor], optional ) – override rpn proposals with custom propos- als. Use when  with_rpn  is False. \nReturns  a dictionary of loss components \nReturn type  dict[str, Tensor] \nsimple test ( img ,  img_metas ,  rescale=False ) Test function without test time augmentation. \nParameters \n•  imgs  ( list[torch.Tensor] ) – List of multiple images •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool ) – Whether to rescale the results. Defaults to False. \nReturns \nBBox results of each image and classes.  The outer list corresponds to each image. The in- ner list corresponds to each class. Return type  list[list[np.ndarray]] \nclass  mmdet.models.detectors. Trident Faster R CNN ( backbone ,  rpn_head ,  roi_head ,  train_cfg ,  test_cfg , neck  $\\mathbf{\\beta}=$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nImplementation of  TridentNet \naug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nforward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  \\*\\*kwargs ) make copies of img and gts to fit multi-branch. \nsimple test ( img ,  img_metas ,  proposals  $\\mathbf{\\varepsilon}=$  None ,  rescale  $\\mathbf{\\beta}=$  False ) Test without augmentation. \nclass  mmdet.models.detectors. Two Stage Detector ( backbone ,  neck  $\\mathbf{\\beta}=$  None ,  rpn_head  $\\leftrightharpoons$  None , roi_head  $\\leftrightharpoons$  None ,  train_cfg  $=$  None ,  test_cfg  $=$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) \nBase class for two-stage detectors. \nTwo-stage detectors typically consisting of a region proposal network and a task-specific regression head. \nasync a sync simple test ( img ,  img_meta ,  proposals  $=$  None ,  rescale  $\\mathbf{=}$  False ) Async test without augmentation. "}
{"page": 277, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_277.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\naug_test (igs, img_metas, rescale=False)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nextract_feat (img)\nDirectly extract features from the backbone+neck.\n\nforward_dummy (img)\nUsed for computing network flops.\n\nSee mmdetection/tools/analysis_tools/get_flops.py\n\nforward_train(img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None, gt_masks=None,\nproposals=None, **kwargs)\n\nParameters\n\n¢ img (Tensor) — of shape (N, C, H, W) encoding input images. Typically these should be\nmean centered and std scaled.\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n* gt_masks (None | Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\n* proposals — override rpn proposals with custom proposals. Use when with_rpn is False.\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\nsimple_test (img, img_metas, proposals=None, rescale=False)\nTest without augmentation.\n\nproperty with_roi_head\nwhether the detector has a Rol head\n\nType bool\n\nproperty with_rpn\nwhether the detector has RPN\n\nType bool\n\nclass mmdet.models.detectors.TwoStagePanopticSegmentor (backbone, neck=None, rpn_head=None,\nroi_head=None, train_cfg=None,\ntest_cfg=None, pretrained=None,\ninit_cfg=None, semantic_head=None,\npanoptic_fusion_head=None)\nBase class of Two-stage Panoptic Segmentor.\n\n270 Chapter 39. mmdet.models\n", "vlm_text": "aug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nextract feat ( img ) Directly extract features from the backbone+neck. \nforward dummy ( img ) Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py \nforward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_masks  $\\mathbf{=}$  None , proposal  $s{=}$  None ,  \\*\\*kwargs ) \nParameters \n•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box \n gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. \n gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. \nReturns  a dictionary of loss components \nReturn type  dict[str, Tensor] \nsimple test ( img ,  img_metas ,  proposals  $\\mathbf{\\varepsilon}=$  None ,  rescale=False ) Test without augmentation. \nproperty with roi head whether the detector has a RoI head \nType  bool \nproperty with_rpn whether the detector has RPN \nType  bool \nclass  mmdet.models.detectors. Two Stage Pan optic Segment or ( backbone ,  neck  $\\leftrightharpoons$  None ,  rpn_head=None , roi_head  $\\leftrightharpoons$  None ,  train_cfg  $\\leftrightharpoons$  None , test_cfg  $=$  None ,  pretrained  $\\mathbf{\\{}=}$  None , init_cfg  $=$  None ,  semantic head  $\\leftrightharpoons$  None , pan optic fusion head=None ) \nBase class of Two-stage Panoptic Segmentor. "}
{"page": 278, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_278.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nAs well as the components in TwoStageDetector, Panoptic Segmentor has extra semantic_head and panop-\ntic_fusion_head.\n\nforward_dummy (img)\nUsed for computing network flops.\n\nSee mmdetection/tools/get_flops.py\n\nforward_train(img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None, gt_masks=None,\ngt_semantic_seg=None, proposals=None, **kwargs)\n\nParameters\n\n¢ img (Tensor) — of shape (N, C, H, W) encoding input images. Typically these should be\nmean centered and std scaled.\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n* gt_masks (None | Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\n* proposals — override rpn proposals with custom proposals. Use when with_rpn is False.\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\nsimple_test (img, img_metas, proposals=None, rescale=False)\nTest without Augmentation.\n\nsimple_test_mask (x, img_metas, det_bboxes, det_labels, rescale=False)\nSimple test for mask head without augmentation.\n\nclass mmdet.models.detectors.VFNet (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of *VarifocalNet (VFNet).<https://arxiv.org/abs/2008.13367>>*_\n\nclass mmdet.models.detectors.YOLACT (backbone, neck, bbox_head, segm_head, mask_head, train_cfg=None,\ntest_cfg=None, pretrained=None, init_cfg=None)\nImplementation of YOLACT\n\naug_test (igs, img_metas, rescale=False)\nTest with augmentations.\n\nforward_dummy (img)\nUsed for computing network flops.\n\nSee mmdetection/tools/analysis_tools/get_flops.py\n\nforward_train(img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None, gt_masks=None)\n\nParameters\n\n39.1. detectors 271\n", "vlm_text": "As well as the components in Two Stage Detector, Panoptic Segmentor has extra semantic head and panop- tic fusion head. \nforward dummy ( ) \nUsed for computing network flops. See  mm detection/tools/get_flops.py \nforward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  gt_masks  $\\mathbf{\\hat{\\rho}}$  None , gt semantic seg  $=$  None ,  proposals  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) \nParameters \n•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. •  proposals  – override rpn proposals with custom proposals. Use when  with_rpn  is False. \nReturns  a dictionary of loss components Return type  dict[str, Tensor] \nsimple test ( img ,  img_metas ,  proposals  $\\mathbf{\\hat{\\rho}}$  None ,  rescale  $\\mathbf{\\beta}=$  False ) Test without Augmentation. \nsimple test mask ( x ,  img_metas ,  det_bboxes ,  det_labels ,  rescale=False ) Simple test for mask head without augmentation. \nclass  mmdet.models.detectors. VFNet ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\boldsymbol{\\mathrm{\\ell}}=$  None ,  init_cfg  $=$  None ) Implementation of  \\`Var i focal Net (VFNet).<https://arxiv.org/abs/2008.13367>\\`_ class  mmdet.models.detectors. YOLACT ( backbone ,  neck ,  bbox_head ,  segm_head ,  mask_head ,  train_cfg  $\\leftrightharpoons$  None , test_cfg  $=$  None ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg=None ) Implementation of  YOLACT aug_test ( imgs ,  img_metas ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Test with augmentations. forward dummy ( img ) Used for computing network flops. See  mm detection/tools/analysis tools/get_flops.py \nforward train ( img ,  img_metas ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ,  gt_masks=None ) \nParameters "}
{"page": 279, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_279.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ img (Tensor) — of shape (N, C, H, W) encoding input images. Typically these should be\nmean centered and std scaled.\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\ngt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n* gt_masks (None | Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\nsimple_test (img, img_metas, rescale=False)\nTest function without test-time augmentation.\n\nclass mmdet.models.detectors.YOLOF (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None)\nImplementation of You Only Look One-level Feature\n\nclass mmdet.models.detectors.YOLOV3 (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\n\nonnx_export (img, img_metas)\nTest function for exporting to ONNX, without test time augmentation.\n\nParameters\n\n¢ img (torch. Tensor) — input images.\n\n¢ img_metas (list [dict ]) — List of image information.\nReturns\n\ndets of shape [N, num_det, 5] and class labels of shape [N, num_det].\nReturn type tuple[Tensor, Tensor]\n\nclass mmdet.models.detectors.YOLOX (backbone, neck, bbox_head, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nImplementation of YOLOX: Exceeding YOLO Series in 2021\n\n272 Chapter 39. mmdet.models\n", "vlm_text": "•  img  ( Tensor ) – of shape (N, C, H, W) encoding input images. Typically these should be mean centered and std scaled. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. \nReturns  a dictionary of loss components \nReturn type  dict[str, Tensor] \nsimple test ( img ,  img_metas ,  rescale=False ) Test function without test-time augmentation. \nclass  mmdet.models.detectors. YOLOF ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ) Implementation of  You Only Look One-level Feature \nclass  mmdet.models.detectors. YOLOV3 ( backbone ,  neck ,  bbox_head ,  train_cfg  $\\mathbf{\\beta}=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \non nx export ( img ,  img_metas ) Test function for exporting to ONNX, without test time augmentation. \nParameters \n•  img  ( torch.Tensor ) – input images. •  img_metas  ( list[dict] ) – List of image information. \nReturns \ndets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. \nReturn type  tuple[Tensor, Tensor] \nclass  mmdet.models.detectors. YOLOX ( backbone ,  neck ,  bbox_head ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) Implementation of  YOLOX: Exceeding YOLO Series in 2021 "}
{"page": 280, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_280.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n39.2 backbones\n\nclass mmdet.models.backbones.CSPDarknet (arch='P5', deepen_factor=1.0, widen_factor=1.0,\n\nout_indices=(2, 3, 4), frozen_stages=- 1, use_depthwise=False,\narch_ovewrite=None, spp_kernal_sizes=(5, 9, 13),\nconv_cfg=None, norm_cfg=(‘eps': 0.001, ‘momentum’: 0.03,\n‘type’: 'BN'}, act_cfg=[‘type': ‘Swish'}, norm_eval=False,\ninit_cfg=('a': 2.23606797749979, ‘distribution’: ‘uniform’,\n‘layer’: 'Conv2d', 'mode': 'fan_in', ‘nonlinearity’: ‘leaky_relu’,\n‘type’: 'Kaiming'})\n\nCSP-Darknet backbone used in YOLOv5 and YOLOX.\n\nParameters\n\nExample\n\narch (str) — Architecture of CSP-Darknet, from {P5, P6}. Default: PS.\n\ndeepen_factor (float) — Depth multiplier, multiply number of channels in each layer by\nthis amount. Default: 1.0.\n\nwiden_factor (float) — Width multiplier, multiply number of blocks in CSP layer by this\namount. Default: 1.0.\n\nout_indices (Sequence [int ]) — Output from which stages. Default: (2, 3, 4).\n\nfrozen_stages (int) — Stages to be frozen (stop grad and set eval mode). -1 means not\nfreezing any parameters. Default: -1.\n\nuse_depthwise (bool) — Whether to use depthwise separable convolution. Default: False.\narch_ovewrite (list) — Overwrite default arch settings. Default: None.\n\nspp_kernal_sizes - (tuple[int]): Sequential of kernel sizes of SPP layers. Default: (5, 9,\n13).\n\nconv_cfg (dict) — Config dict for convolution layer. Default: None.\n\nnorm_cfg (dict) — Dictionary to construct and config norm layer. Default: dict(type=’BN’,\nrequires_grad=True).\n\nact_cfg (dict) — Config dict for activation layer. Default: dict(type=’ LeakyReLU’, nega-\ntive_slope=0.1).\n\nnorm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict. Default:\nNone.\n\n>>> from mmdet.models import CSPDarknet\n\n>>> import torch\n\n>>> self = CSPDarknet (depth=53)\n\n>>> self.evalQ\n\n>>> inputs\n\n>>> level_outputs = self. forward(inputs)\n\n>>> for level_out in level_outputs:\nprint (tuple(level_out.shape))\n\n= torch.rand(1, 3, 416, 416)\n\n(continues on next page)\n\n39.2. backbones\n\n273\n\n", "vlm_text": "39.2 backbones \nclass  mmdet.models.backbones. CSPDarknet ( arch  $\\scriptstyle{\\prime=P5}$  ,  deepen factor  $\\scriptstyle{:=I.O}$  ,  widen facto  ${\\bf\\tau}{=}I.0$  , out_indices=(2, 3, 4) ,  frozen stages=- 1 ,  use depth wise  $\\mathbf{\\beta}=$  False , arch ove write  $\\mathbf{\\beta}=$  None ,  spp kern al sizes=(5, 9, 13) , conv_cfg  $\\mathbf{\\beta}=$  None ,  norm_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} ,  act_cfg  $\\scriptstyle{\\tilde{}}=$  {'type': 'Swish'} ,  norm_eval  $\\leftrightharpoons$  False , init_cfg  $=$  {'a': 2.23606797749979, 'distribution': 'uniform', 'layer': 'Conv2d', 'mode': 'fan_in', 'non linearity': 'leaky_relu', 'type': 'Kaiming'} ) \nCSP-Darknet backbone used in YOLOv5 and YOLOX. \nParameters \n•  arch  ( str ) – Architecture of CSP-Darknet, from {P5, P6}. Default: P5. •  deepen factor  ( float ) – Depth multiplier, multiply number of channels in each layer by this amount. Default: 1.0. •  widen factor  ( float ) – Width multiplier, multiply number of blocks in CSP layer by this amount. Default: 1.0. •  out indices  ( Sequence[int] ) – Output from which stages. Default: (2, 3, 4). •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. Default: -1. •  use depth wise  ( bool ) – Whether to use depthwise separable convolution. Default: False. •  arch ove write  ( list ) – Overwrite default arch settings. Default: None. •  spp kern al sizes  – (tuple[int]): Sequential of kernel sizes of SPP layers. Default: (5, 9, 13). •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $=^{,}$  ’BN’, requires grad  $\\scriptstyle\\varepsilon=$  True). •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\r=$  ’LeakyReLU’, nega- tive_slope  $\\mathrm{{=}}0.1$  ). •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. \nExample \n $>>$   from  mmdet.models  import  CSPDarknet  $>>$   import  torch  $>>$   self  $=$   CSPDarknet(depth  $\\mathord{\\mathrm{\\varepsilon}}\\!=\\!53$  )  $>>$   self . eval()  $>>$   inputs  $=$   torch . rand( 1 ,  3 ,  416 ,  416 )  $>>$   level outputs  $=$   self . forward(inputs)  $>>$   for  level_out  in  level outputs: ... print ( tuple (level_out . shape)) \n(continues on next page) "}
{"page": 281, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_281.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n(1, 256, 52, 52)\n(1, 512, 26, 26)\n(1, 1024, 13, 13)\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\ntrain (mode=True)\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular modules for details of their\nbehaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.\n\nParameters mode (bool) — whether to set training mode (True) or evaluation mode (False).\nDefault: True.\n\nReturns self\nReturn type Module\n\nclass mmdet.models.backbones.Darknet (depth=53, out_indices=(3, 4, 5), frozen_stages=- 1,\nconv_cfg=None, norm_cfg={'requires_grad': True, 'type': 'BN'},\nact_cfg={'negative_slope': 0.1, 'type': 'LeakyReLU'},\nnorm_eval=True, pretrained=None, init_cfg=None)\n\nDarknet backbone.\n\nParameters\n¢ depth (int) — Depth of Darknet. Currently only support 53.\n* out_indices (Sequence[int]) — Output from which stages.\n\n* frozen_stages (int) — Stages to be frozen (stop grad and set eval mode). -1 means not\nfreezing any parameters. Default: -1.\n\n* conv_cfg (dict) — Config dict for convolution layer. Default: None.\n\n* norm_cfg (dict) — Dictionary to construct and config norm layer. Default: dict(type=’BN’,\nrequires_grad=True)\n\n* act_cfg (dict) — Config dict for activation layer. Default: dict(type=’ LeakyReLU’, nega-\ntive_slope=0.1).\n\n* norm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only.\n\n* pretrained (str, optional) -— model pretrained path. Default: None\n\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\n274 Chapter 39. mmdet.models\n", "vlm_text": "forward  $(x)$  Defines the computation performed at every call. \nShould be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \ntrain ( mode  $=$  True ) Sets the module in training mode. \nThis has any effect only on certain modules. See documentation s of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g.  Dropout ,  BatchNorm , etc. \nParameters  mode  ( bool ) – whether to set training mode ( True ) or evaluation mode ( False ). Default:  True . \nReturns  self \nReturn type  Module \nclass  mmdet.models.backbones. Darknet ( depth  $\\mathord{:=}53$  ,  out_indices=(3, 4, 5) ,  frozen stages  $\\scriptstyle{\\prime}=-I$  , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'requires grad': True, 'type': 'BN'} , act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'negative slope': 0.1, 'type': 'LeakyReLU'} , norm_eval  $\\leftrightharpoons$  True ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\leftrightharpoons$  None ) \nDarknet backbone. \nParameters \n•  depth  ( int ) – Depth of Darknet. Currently only support 53. •  out indices  ( Sequence[int] ) – Output from which stages. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. Default: -1. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $\\L_{:=}^{:}$  ’BN’, requires grad  $\\scriptstyle\\varepsilon=$  True) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $=^{!}$  ’LeakyReLU’, nega- tive_slope  $\\mathrm{{=}}0.1$  ). •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None "}
{"page": 282, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_282.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> from mmdet.models import Darknet\n\n>>> import torch\n\n>>> self = Darknet(depth=53)\n\n>>> self.evalQ\n\n>>> inputs = torch.rand(1, 3, 416, 416)\n\n>>> level_outputs = self. forward(inputs)\n\n>>> for level_out in level_outputs:\nprint (tuple(level_out.shape))\n\n(1, 256, 52, 52)\n\n(1, 512, 26, 26)\n\n(1, 1024, 13, 13)\n\nforward(x)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nstatic make_conv_res_block(in_channels, out_channels, res_repeat, conv_cfg=None,\n\nnorm_cfg={'requires_grad': True, ‘type’: 'BN'}, act_cfg={‘negative_slope':\n0.1, 'type': 'LeakyReLU'})\n\nIn Darknet backbone, ConvLayer is usually followed by ResBlock. This function will make that. The Conv\nlayers always have 3x3 filters with stride=2. The number of the filters in Conv layer is the same as the out\nchannels of the ResBlock.\n\nParameters\n\nin_channels (int) - The number of input channels.\nout_channels (int) — The number of output channels.\nres_repeat (int) — The number of ResBlocks.\n\nconv_cfg (dict) — Config dict for convolution layer. Default: None.\n\nnorm_cfg (dict) — Dictionary to construct and config norm layer. Default:\ndict(type=’ BN’, requires_grad=True)\n\nact_cfg (dict) — Config dict for activation layer. Default: dict(type=’ LeakyReLU’, neg-\native_slope=0. 1).\n\ntrain (mode=True)\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular modules for details of their\nbehaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.\n\nParameters mode (bool) — whether to set training mode (True) or evaluation mode (False).\nDefault: True.\n\nReturns self\n\nReturn type Module\n\n39.2. backbones\n\n275\n\n", "vlm_text": "Example \n $>>$   from  mmdet.models  import  Darknet\n\n  $>>$   import  torch\n\n  $>>$   self  $=$   Darknet(depth  $\\mathtt{=}53$  )\n\n  $>>$   self . eval()\n\n  $>>$   inputs  $=$   torch . rand( 1 ,  3 ,  416 ,  416 )\n\n  $>>$   level outputs  $=$   self . forward(inputs)\n\n  $>>$   for  level_out  in  level outputs:\n\n ... print ( tuple (level_out . shape))\n\n ...\n\n (1, 256, 52, 52)\n\n (1, 512, 26, 26)\n\n (1, 1024, 13, 13) \nforward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nstatic make con v res block ( in channels ,  out channels ,  res_repeat ,  conv_cfg  $\\mathbf{\\hat{\\mu}}$  None , norm_cfg  $\\leftleftarrows$  {'requires grad': True, 'type': 'BN'} ,  act_cfg  $\\scriptstyle{\\tilde{}}=$  {'negative slope': 0.1, 'type': 'LeakyReLU'} ) \nIn Darknet backbone, ConvLayer is usually followed by ResBlock. This function will make that. The Conv layers always have 3x3 filters with stride  $_{:=2}$  . The number of the filters in Conv layer is the same as the out channels of the ResBlock. \nParameters \n•  in channels  ( int ) – The number of input channels. •  out channels  ( int ) – The number of output channels. •  res_repeat  ( int ) – The number of ResBlocks. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $\\mathbf{\\varepsilon}\\mathbf{=}\\mathbf{'}\\mathbf{B}\\mathbf{N}^{\\ast}$  ’, requires grad  $=$  True) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\scriptstyle{:=}$  ’LeakyReLU’, neg- at ive slope  $\\mathrm{{=}}0.1$  ). \ntrain ( mode  $=$  True ) Sets the module in training mode. \nThis has any effect only on certain modules. See documentation s of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g.  Dropout ,  BatchNorm , etc. \nParameters  mode  ( bool ) – whether to set training mode ( True ) or evaluation mode ( False ). Default:  True . Returns  self Return type  Module "}
{"page": 283, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_283.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.backbones.DetectoRS_ResNeXt (groups=1, base_width=4, **kwargs)\nResNeXt backbone for DetectoRS.\n\nParameters\n* groups (int) — The number of groups in ResNeXt.\n¢ base_width (int) — The base width of ResNeXt.\n\nmake_res_layer(**kwargs)\nPack all blocks in a stage into a ResLayer for DetectoRS.\n\nclass mmdet.models.backbones .DetectoRS_ResNet (sac=None, stage_with_sac=(False, False, False, False),\nrfp_inplanes=None, output_img=False,\npretrained=None, init_cfg=None, **kwargs)\nResNet backbone for DetectoRS.\n\nParameters\n\n* sac (dict, optional) — Dictionary to construct SAC (Switchable Atrous Convolution).\nDefault: None.\n\n* stage_with_sac (list) — Which stage to use sac. Default: (False, False, False, False).\n\n¢ rfp_inplanes (int, optional) — The number of channels from RFP. Default: None. If\nspecified, an additional conv layer will be added for rfp_feat. Otherwise, the structure is\nthe same as base class.\n\n* output_img (bool) — If True, the input image will be inserted into the starting position of\noutput. Default: False.\n\nforward(x)\nForward function.\n\ninit_weights()\nInitialize the weights.\n\nmake_res_layer(**kwargs)\nPack all blocks in a stage into a ResLayer for DetectoRS.\n\nrfp_forward (x, rfp_feats)\nForward function for RFP.\n\nclass mmdet.models.backbones.HRNet (extra, in_channels=3, conv_cfg=None, norm_cfg={'type': 'BN'},\nnorm_eval=True, with_cp=False, zero_init_residual=False,\nmultiscale_output=True, pretrained=None, init_cfg=None)\nHRNet backbone.\n\nHigh-Resolution Representations for Labeling Pixels and Regions arXiv:.\nParameters\n\n* extra (dict) — Detailed configuration for each stage of HRNet. There must be 4 stages, the\nconfiguration for each stage must have 5 keys:\n\nnum_modules(int): The number of HRModule in this stage.\n\nnum_branches(int): The number of branches in the HRModule.\n\nblock(str): The type of convolution block.\n\nnum_blocks(tuple): The number of blocks in each branch. The length must be equal\nto num_branches.\n\n276 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.backbones. DetectoRS Res NeXt ( group  $\\mathrm{\\Sigma}_{:=I}$  ,  base_width  $\\scriptstyle{=4}$  ,  \\*\\*kwargs ) ResNeXt backbone for DetectoRS. \nParameters \n•  groups  ( int ) – The number of groups in ResNeXt. •  base_width  ( int ) – The base width of ResNeXt. \nmake res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer  for DetectoRS. \nclass  mmdet.models.backbones. DetectoRS Res Net ( sac  $=$  None ,  stage with sac=(False, False, False, False) , rfp in planes  $\\leftrightharpoons$  None ,  output_img  $\\mathbf{\\hat{\\Sigma}}$  False , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) \nResNet backbone for DetectoRS. \nParameters \n•  sac  ( dict, optional ) – Dictionary to construct SAC (Switchable Atrous Convolution). Default: None. •  stage with sac  ( list ) – Which stage to use sac. Default: (False, False, False, False). •  rfp in planes  ( int, optional ) – The number of channels from RFP. Default: None. If specified, an additional conv layer will be added for  rfp_feat . Otherwise, the structure is the same as base class. •  output_img  ( bool ) – If  True , the input image will be inserted into the starting position of output. Default: False. \nforward  $(x)$  Forward function. \nin it weights()Initialize the weights. \nmake res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer  for DetectoRS. \nrfp forward ( x ,  rfp_feats ) Forward function for RFP. \nclass  mmdet.models.backbones. HRNet ( extra ,  in channels  $\\scriptstyle{\\prime=3}$  ,  conv_cfg  $=$  None ,  norm_cfg={'type': 'BN'} , norm_eva  $\\leftrightharpoons$  True ,  with_cp  $\\mathbf{\\varepsilon}=$  False ,  zero in it res i du a  $\\leftrightharpoons$  False , multi scale out pu  $\\mathbf{\\dot{\\rho}}=$  True ,  pretrained=None ,  init_cfg  $=$  None ) \nHRNet backbone. \nHigh-Resolution Representations for Labeling Pixels and Regions arXiv: . \nParameters \n•  extra  ( dict ) – Detailed configuration for each stage of HRNet. There must be 4 stages, the configuration for each stage must have 5 keys: –  num modules(int): The number of HRModule in this stage. –  num branches(int): The number of branches in the HRModule. –  block(str): The type of convolution block. – num_blocks(tuple): The number of blocks in each branch.  The length must be equal to num branches. "}
{"page": 284, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_284.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n— num_channels(tuple): The number of channels in each branch. The length must be\nequal to num_branches.\n\nin_channels (int) — Number of input image channels. Default: 3.\nconv_cfg (dict) — Dictionary to construct and config conv layer.\nnorm_cfg (dict) — Dictionary to construct and config norm layer.\n\nnorm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only. Default: True.\n\nwith_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed. Default: False.\n\nzero_init_residual (bool) — Whether to use zero init for last norm layer in resblocks to\nlet them behave as identity. Default: False.\n\nmultiscale_output (bool) -— Whether to output multi-level features produced by multiple\n\nbranches. If False, only the first level feature will be output. Default: True.\n\n* pretrained (str, optional) — Model pretrained path. Default: None.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict. Default:\n\nNone.\n\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n>>>\n\n>>> from mmdet.models import HRNet\n>>> import torch\n>>> extra = dict(\n\nstagel=dict(\nnum_modules=1,\nnum_branches=1,\nblock='BOTTLENECK',\nnum_blocks=(4, ),\nnum_channels=(64, )),\nstage2=dict(\nnum_modules=1,\nnum_branches=2,\nblock='BASIC',\nnum_blocks=(4, 4),\nnum_channels=(32, 64)),\nstage3=dict(\nnum_modules=4,\nnum_branches=3,\nblock='BASIC',\nnum_blocks=(4, 4, 4),\nnum_channels=(32, 64, 128)),\nstage4=dict(\nnum_modules=3,\nnum_branches=4,\nblock='BASIC',\nnum_blocks=(4, 4, 4, 4),\nnum_channels=(32, 64, 128, 256)))\n\n>>> self = HRNet(extra, in_channels=1)\n>>> self.evalQ\n\n(continues on next page)\n\n39.2. backbones\n\n277\n\n", "vlm_text": "– num channels(tuple): The number of channels in each branch.  The length must be equal to num branches.\n\n •  in channels  ( int ) – Number of input image channels. Default: 3.\n\n •  conv_cfg  ( dict ) – Dictionary to construct and config conv layer.\n\n •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer.\n\n •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. Default: True.\n\n •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.\n\n •  zero in it residual  ( bool ) – Whether to use zero init for last norm layer in resblocks to let them behave as identity. Default: False.\n\n •  multi scale output  ( bool ) – Whether to output multi-level features produced by multiple branches. If False, only the first level feature will be output. Default: True.\n\n •  pretrained  ( str, optional ) – Model pretrained path. Default: None.\n\n •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. \nExample \nThe table contains Python code configuring an HRNet (High-Resolution Network) model using the MMDetection library. Here's a breakdown of the code:\n\n1. **Imports**:\n   - `from mmdet.models import HRNet`: Imports the HRNet model from the MMDetection library.\n   - `import torch`: Imports the PyTorch library.\n\n2. **Model Configuration** (`extra` dictionary):\n   - `stage1`:\n     - `num_modules`: 1\n     - `num_branches`: 1\n     - `block`: 'BOTTLENECK'\n     - `num_blocks`: (4,)\n     - `num_channels`: (64,)\n   - `stage2`:\n     - `num_modules`: 1\n     - `num_branches`: 2\n     - `block`: 'BASIC'\n     - `num_blocks`: (4, 4)\n     - `num_channels`: (32, 64)\n   - `stage3`:\n     - `num_modules`: 4\n     - `num_branches`: 3\n     - `block`: 'BASIC'\n     - `num_blocks`: (4, 4, 4)\n     - `num_channels`: (32, 64, 128)\n   - `stage4`:\n     - `num_modules`: 3\n     - `num_branches`: 4\n     - `block`: 'BASIC'\n     - `num_blocks`: (4, 4, 4, 4)\n     - `num_channels`: (32, 64, 128, 256)\n\n3. **Model Initialization**:\n   - `self = HRNet(extra, in_channels=1)`: Initializes the HRNet model with the configuration `extra` and 1 input channel.\n\n4. **Model Evaluation**:\n   - `self.eval()`: Sets the model in evaluation mode."}
{"page": 285, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_285.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\n>>> inputs = torch.rand(1, 1, 32, 32)\n>>> level_outputs = self. forward(inputs)\n>>> for level_out in level_outputs:\n\na print (tuple(level_out.shape))\n(1, 32, 8, 8)\n\n(1, 64, 4, 4\n\n(1, 128, 2, 2)\n\n(1, 256, 1, 1)\n\nforward(x)\nForward function.\n\nproperty normi\nthe normalization layer named “norm1”\n\nType nn.Module\n\nproperty norm2\nthe normalization layer named “norm2”\n\nType nn.Module\n\ntrain (mode=True)\n\nConvert the model into training mode will keeping the normalization layer freezed.\n\nclass mmdet.models.backbones .HourglassNet (downsample_times=5, num_stacks=2, stage_channels=(256,\n256, 384, 384, 384, 512), stage_blocks=(2, 2, 2, 2, 2, 4),\nfeat_channel=256, norm_cfg={ 'requires_grad': True, ‘type’:\n'BN'}, pretrained=None, init_cfg=None)\n\nHourglassNet backbone.\n\nStacked Hourglass Networks for Human Pose Estimation. More details can be found in the paper .\n\nParameters\n\n¢ downsample_times (int) — Downsample times in a HourglassModule.\n\n* num_stacks (int) — Number of HourglassModule modules stacked, 1 for Hourglass-52, 2\n\nfor Hourglass- 104.\n\n* stage_channels (list [int ]) — Feature channel of each sub-module in a HourglassMod-\n\nule.\n\n* stage_blocks (list [int]) — Number of sub-modules stacked in a HourglassModule.\n\n¢ feat_channel (int) — Feature channel of conv after a HourglassModule.\n\n* norm_cfg (dict) — Dictionary to construct and config norm layer.\n\n* pretrained (str, optional) -— model pretrained path. Default: None\n\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\n278\n\nChapter 39. mmdet.models\n\n", "vlm_text": "The image shows a block of Python code, likely from a deep learning framework like PyTorch. Here's a breakdown of the code:\n\n1. **Creation of `inputs`:**\n   - `inputs = torch.rand(1, 1, 32, 32)`: This line creates a random tensor with the shape `(1, 1, 32, 32)` using PyTorch. The dimensions likely represent batches, channels, height, and width.\n\n2. **Forward pass:**\n   - `level_outputs = self.forward(inputs)`: This line calls a `forward` method on the inputs, typically part of a model's architecture in a neural network, to get `level_outputs`.\n\n3. **Iterating over output levels:**\n   - A loop iterates over `level_outputs`, printing the shape of each `level_out`.\n\n4. **Printed shapes:**\n   - The shapes are: `(1, 32, 8, 8)`, `(1, 64, 4, 4)`, `(1, 128, 2, 2)`, `(1, 256, 1, 1)`. These shapes suggest a series of operations that reduce the spatial dimensions, possibly through convolutional or pooling layers.\nforward  $(x)$  Forward function. \nproperty norm1 the normalization layer named “norm1” \nType  nn.Module \nproperty norm2 \nthe normalization layer named “norm2” Type  nn.Module \ntrain ( mode  $=$  True ) Convert the model into training mode will keeping the normalization layer freezed. \nclass  mmdet.models.backbones. Hourglass Net ( down sample times  $\\scriptstyle{\\prime=5}$  ,  num_stack  $s{=}2$  ,  stage channels=(256, 256, 384, 384, 384, 512) ,  stage_blocks=(2, 2, 2, 2, 2, 4) , feat ch anne  $l{=}256$  ,  norm_cfg  $\\mathsf{\\chi}=\\!\\!\\big/$  'requires grad': True, 'type': 'BN'} ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) \nHourglass Net backbone. \nStacked Hourglass Networks for Human Pose Estimation. More details can be found in the  paper  . \nParameters \n•  down sample times  ( int ) – Downsample times in a Hourglass Module. •  num_stacks  ( int ) – Number of Hourglass Module modules stacked, 1 for Hourglass-52, 2 for Hourglass-104. •  stage channels  ( list[int] ) – Feature channel of each sub-module in a Hourglass Mod- ule. •  stage blocks  ( list[int] ) – Number of sub-modules stacked in a Hourglass Module. •  feat channel  ( int ) – Feature channel of conv after a Hourglass Module. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None "}
{"page": 286, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_286.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> from mmdet.models import HourglassNet\n>>> import torch\n\n>>> self = HourglassNet()\n\n>>> self.evalQ\n\n>>> inputs = torch.rand(1, 3, 511, 511)\n>>> level_outputs = self. forward(inputs)\n>>> for level_output in level_outputs:\na print (tuple(level_output.shape))\n(1, 256, 128, 128)\n\n(1, 256, 128, 128)\n\nforward(x)\nForward function.\n\ninit_weights()\nInit module weights.\n\nclass mmdet.models.backbones .MobileNetV2 (widen_factor=1.0, out_indices=(1, 2, 4, 7), frozen_stages=- 1,\nconv_cfg=None, norm_cfg={ ‘type’: 'BN'}, act_cfg={'type’:\n‘ReLU6'}, norm_eval=False, with_cp=False, pretrained=None,\ninit_cfg=None)\nMobileNetV2 backbone.\n\nParameters\n\n¢ widen_factor (float) — Width multiplier, multiply number of channels in each layer by\nthis amount. Default: 1.0.\n\n* out_indices (Sequence[int], optional) - Output from which stages. Default: (1, 2,\n4,7).\n\n* frozen_stages (int) — Stages to be frozen (all param fixed). Default: -1, which means\nnot freezing any parameters.\n\n* conv_cfg (dict, optional) — Config dict for convolution layer. Default: None, which\nmeans using conv2d.\n\n* norm_cfg (dict) — Config dict for normalization layer. Default: dict(type=’ BN’).\n* act_cfg (dict) — Config dict for activation layer. Default: dict(type=’ReLU6’).\n\n* norm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only. Default: False.\n\n* with_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed. Default: False.\n\n* pretrained (str, optional) -— model pretrained path. Default: None\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nforward(x)\nForward function.\n\nmake_layer (out_channels, num_blocks, stride, expand_ratio)\nStack InvertedResidual blocks to build a layer for MobileNetV2.\n\nParameters\n\n¢ out_channels (int) — out_channels of block.\n\n39.2. backbones 279\n", "vlm_text": "Example \nThe image shows a Python script snippet related to using a neural network model called \"HourglassNet\" with PyTorch. Here's a breakdown:\n\n1. **Import Statements:**\n   - `HourglassNet` is imported from `mmdet.models`.\n   - The `torch` library is imported.\n\n2. **Model Initialization and Evaluation:**\n   - An instance of `HourglassNet` is created with `self = HourglassNet()`.\n   - The model is set to evaluation mode using `self.eval()`.\n\n3. **Input and Forward Pass:**\n   - Random input data of shape (1, 3, 511, 511) is created using `torch.rand`.\n   - The input is forwarded through the model with `self.forward(inputs)` and results are stored in `level_outputs`.\n\n4. **Output Shapes:**\n   - The shapes of the outputs are printed in a loop, resulting in two outputs with shapes `(1, 256, 128, 128)`.\nforward  $(x)$  Forward function. \nin it weights()Init module weights. \nclass  mmdet.models.backbones. Mobile Ne tV 2 ( widen facto  $\\scriptstyle{=}I.O$  ,  out_indices=(1, 2, 4, 7) ,  frozen stage  $\\scriptstyle{\\varepsilon=-\\ I}$  , conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $=$  {'type': 'BN'} ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'ReLU6'} ,  norm_eval  $\\leftrightharpoons$  False ,  with_cp=False ,  pretrained  $\\leftrightharpoons$  None , init_cfg  $=$  None ) \nMobile Ne tV 2 backbone. \nParameters \n•  widen factor  ( float ) – Width multiplier, multiply number of channels in each layer by this amount. Default: 1.0. •  out indices  ( Sequence[int], optional ) – Output from which stages. Default: (1, 2, 4, 7). •  frozen stages  ( int ) – Stages to be frozen (all param fixed). Default: -1, which means not freezing any parameters. •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None, which means using conv2d. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict(type  $=^{:}$  ’BN’). •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\L_{:=}^{:}$  ’ReLU6’). •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. Default: False. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward  $(x)$  Forward function. make_layer ( out channels ,  num_blocks ,  stride ,  expand ratio ) Stack Inverted Residual blocks to build a layer for Mobile Ne tV 2. Parameters •  out channels  ( int ) – out channels of block. "}
{"page": 287, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_287.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ num_blocks (int) — number of blocks.\n¢ stride (int) — stride of the first block. Default: 1\n\n* expand_ratio (int) — Expand the number of channels of the hidden layer in Inverte-\ndResidual by this ratio. Default: 6.\n\ntrain (mode=True)\nConvert the model into training mode while keep normalization layer frozen.\n\nclass mmdet.models.backbones.PyramidVisionTransformer (pretrain_img_size=224, in_channels=3,\n\nembed_dims=64, num_stages=4,\n\nnum_layers=[3, 4, 6, 3], num_heads=[1, 2, 5,\n8], patch_sizes=[4, 2, 2, 2], strides=[4, 2, 2,\n2], paddings=[0, 0, 0, 0], sr_ratios=[8, 4, 2,\n1], out_indices=(0, 1, 2, 3), mlp_ratios=[8, 8,\n\n4, 4], qkv_bias=True, drop_rate=0.0,\nattn_drop_rate=0.0, drop_path_rate=0.1,\nuse_abs_pos_embed=True,\nnorm_after_stage=False,\nuse_conv_{ffn=False, act_cfg=('type':\n'GELU'}, norm_cfg={‘eps': le-06, ‘type’:\n'LN'}, pretrained=None,\nconvert_weights=True, init_cfg=None)\n\nPyramid Vision Transformer (PVT)\n\nImplementation of Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolu-\n\ntions.\n\nParameters\n\npretrain_img_size (int | tuple[int])-The size of input image when pretrain. De-\nfaults: 224.\n\nin_channels (int) — Number of input channels. Default: 3.\nembed_dims (int) —- Embedding dimension. Default: 64.\nnum_stags (int) — The num of stages. Default: 4.\n\nnum_layers (Sequence [int ]) — The layer number of each transformer encode layer. De-\nfault: [3, 4, 6, 3].\n\nnum_heads (Sequence [int ]) — The attention heads of each transformer encode layer. De-\nfault: [1, 2, 5, 8].\n\npatch_sizes (Sequence [int]) — The patch_size of each patch embedding. Default: [4,\n2, 2, 2].\n\nstrides (Sequence[int]) — The stride of each patch embedding. Default: [4, 2, 2, 2].\npaddings (Sequence [int ]) — The padding of each patch embedding. Default: [0, 0, 0, 0].\n\nsr_ratios (Sequence [int ])-—The spatial reduction rate of each transformer encode layer.\nDefault: [8, 4, 2, 1].\n\nout_indices (Sequence[int] | int) -— Output from which stages. Default: (0, 1, 2, 3).\n\nmlp_ratios (Sequence [int ]) — The ratio of the mlp hidden dim to the embedding dim of\neach transformer encode layer. Default: [8, 8, 4, 4].\n\nakv_bias (bool) — Enable bias for qkv if True. Default: True.\n\ndrop_rate (float) — Probability of an element to be zeroed. Default 0.0.\n\n280\n\nChapter 39. mmdet.models\n", "vlm_text": "•  num_blocks  ( int ) – number of blocks. •  stride  ( int ) – stride of the first block. Default: 1 •  expand ratio  ( int ) – Expand the number of channels of the hidden layer in Inverte- dResidual by this ratio. Default: 6. \ntrain ( mode=True ) Convert the model into training mode while keep normalization layer frozen. \n( pre train img size  $\\scriptstyle{\\bullet=224}$  ,  in_channel  $\\mathfrak{s}{=}3$  , embed_dim  $\\mathord{\\breve{=}}64$  ,  num_stage  $s{=}4$  , num_layers=[3, 4, 6, 3] ,  num_heads=[1, 2, 5, 8] ,  patch_sizes=[4, 2, 2, 2] ,  strides=[4, 2, 2, 2] ,  paddings=[0, 0, 0, 0] ,  sr_ratios=[8, 4, 2, 1] ,  out_indices=(0, 1, 2, 3) ,  mlp_ratios=[8, 8, 4, 4] ,  qkv_bias  $\\mathbf{=}$  True ,  drop_rate  $\\bullet{=}0.0$  , at tn drop rate=0.0 ,  drop path rate=0.1 , use abs pos embed=True , norm after stage  $\\mathbf{\\tilde{=}}$  False , use con v ff n  $:=$  False ,  act_cfg={'type': 'GELU'} ,  norm_cfg={'eps': 1e-06, 'type': 'LN'} ,  pretrained  $\\leftrightharpoons$  None , convert weights  $\\mathbf{=}$  True ,  init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ) \nPyramid Vision Transformer (PVT) \nImplementation of  Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolu- tions . \nParameters \n•  pre train img size  ( int | tuple[int] ) – The size of input image when pretrain. De- faults: 224. •  in channels  ( int ) – Number of input channels. Default: 3. •  embed_dims  ( int ) – Embedding dimension. Default: 64. •  num_stags  ( int ) – The num of stages. Default: 4. •  num_layers  ( Sequence[int] ) – The layer number of each transformer encode layer. De- fault: [3, 4, 6, 3]. •  num_heads  ( Sequence[int] ) – The attention heads of each transformer encode layer. De- fault: [1, 2, 5, 8]. •  patch sizes  ( Sequence[int] ) – The patch_size of each patch embedding. Default: [4, 2, 2, 2]. •  strides  ( Sequence[int] ) – The stride of each patch embedding. Default: [4, 2, 2, 2]. •  paddings  ( Sequence[int] ) – The padding of each patch embedding. Default: [0, 0, 0, 0]. •  sr_ratios  ( Sequence[int] ) – The spatial reduction rate of each transformer encode layer. Default: [8, 4, 2, 1]. •  out indices  ( Sequence[int] | int ) – Output from which stages. Default: (0, 1, 2, 3). •  mlp_ratios  ( Sequence[int] ) – The ratio of the mlp hidden dim to the embedding dim of each transformer encode layer. Default: [8, 8, 4, 4]. •  qkv_bias  ( bool ) – Enable bias for qkv if True. Default: True. •  drop_rate  ( float ) – Probability of an element to be zeroed. Default 0.0. "}
{"page": 288, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_288.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward(x)\n\nattn_drop_rate (float) — The drop out rate for attention layer. Default 0.0.\ndrop_path_rate (float) — stochastic depth rate. Default 0.1.\n\nuse_abs_pos_embed (boo1) — If True, add absolute position embedding to the patch em-\nbedding. Defaults: True.\n\nuse_conv_ffn (boo1) — If True, use Convolutional FFN to replace FFN. Default: False.\nact_cfg (dict) — The activation config for FFNs. Default: dict(type=’GELU’).\nnorm_cfg (dict) — Config dict for normalization layer. Default: dict(type=’LN’).\npretrained (str, optional) — model pretrained path. Default: None.\n\nconvert_weights (bool) — The flag indicates whether the pre-trained model is from the\noriginal repo. We may need to convert some keys to make it compatible. Default: True.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict. Default:\nNone.\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\ninit_weights()\nInitialize the weights.\n\nclass mmdet.models.backbones.PyramidVisionTransformerV2 (**kwargs)\nImplementation of PVTv2: Improved Baselines with Pyramid Vision Transformer.\n\nclass mmdet.models.backbones.RegNet (arch, in_channels=3, stem_channels=32, base_channels=32,\n\nstrides=(2, 2, 2, 2), dilations=(1, 1, 1, 1), out_indices=(0, 1, 2, 3),\nstyle='pytorch', deep_stem=False, avg_down=False, frozen_stages=-\n1, conv_cfg=None, norm_cfg={'requires_grad': True, ‘type’: 'BN'},\nnorm_eval=True, dcn=None, stage_with_dcn=(False, False, False,\nFalse), plugins=None, with_cp=False, zero_init_residual=True,\npretrained=None, init_cfg=None)\n\nRegNet backbone.\n\nMore details can be found in paper .\n\nParameters\n\narch (dict) — The parameter of RegNets.\n\nw0 (int): initial width\n\nwa (float): slope of width\n\nwm (float): quantization parameter to quantize the width\n\ndepth (int): depth of the backbone\n— group_w (int): width of group\n— bot_mul (float): bottleneck ratio, i.e. expansion of bottleneck.\n\nstrides (Sequence [int ]) — Strides of the first block of each stage.\n\n39.2. backbones 281\n", "vlm_text": "•  at tn drop rate  ( float ) – The drop out rate for attention layer. Default 0.0. •  drop path rate  ( float ) – stochastic depth rate. Default 0.1. •  use abs pos embed  ( bool ) – If True, add absolute position embedding to the patch em- bedding. Defaults: True. •  use con v ff n  ( bool ) – If True, use Convolutional FFN to replace FFN. Default: False. •  act_cfg  ( dict ) – The activation config for FFNs. Default: dict(type  $=^{!}$  ’GELU’). •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict(type  $=$  ’LN’). •  pretrained  ( str, optional ) – model pretrained path. Default: None. •  convert weights  ( bool ) – The flag indicates whether the pre-trained model is from the original repo. We may need to convert some keys to make it compatible. Default: True. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. \nforward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nin it weights()Initialize the weights. \nclass  mmdet.models.backbones. Pyramid Vision Transformer V 2 ( \\*\\*kwargs ) Implementation of  PVTv2: Improved Baselines with Pyramid Vision Transformer . \n( arch ,  in channels  $\\scriptstyle{:=3}$  ,  stem channels  $\\scriptstyle{\\mathfrak{s}}=32$  ,  base channel  $s{=}32$  , strides=(2, 2, 2, 2) ,  dilations=(1, 1, 1, 1) ,  out_indices=(0, 1, 2, 3) , style  $\\mathbf{=}$  'pytorch' ,  deep_stem  $\\mathbf{\\beta}=$  False ,  avg_down  $=$  False ,  frozen stages=- 1 ,  conv_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}\\mathbf{0}$  None ,  norm_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'requires grad': True, 'type': 'BN'} , norm_eva  $\\leftrightharpoons$  True ,  dcn  $=$  None ,  stage with dc n=(False, False, False, False) ,  plugins  $\\mathbf{\\hat{\\rho}}$  None ,  with_cp=False ,  zero in it res i du a  $\\leftrightharpoons$  True , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=$  None ) \nRegNet backbone. \nMore details can be found in  paper \nParameters \n arch  ( dict ) – The parameter of RegNets. –  w0 (int): initial width –  wa (float): slope of width –  wm (float): quantization parameter to quantize the width –  depth (int): depth of the backbone –  group_w (int): width of group –  bot_mul (float): bottleneck ratio, i.e. expansion of bottleneck.  strides  ( Sequence[int] ) – Strides of the first block of each stage. "}
{"page": 289, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_289.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* base_channels (int) — Base channels after stem layer.\n\n¢ in_channels (int) — Number of input image channels. Default: 3.\n¢ dilations (Sequence [int]) — Dilation of each stage.\n\n* out_indices (Sequence[int]) — Output from which stages.\n\n* style (str) -—pytorch or caffe. If set to “pytorch”, the stride-two layer is the 3x3 conv layer,\notherwise the stride-two layer is the first 1x1 conv layer.\n\n* frozen_stages (int) — Stages to be frozen (all param fixed). -1 means not freezing any\nparameters.\n\n* norm_cfg (dict) — dictionary to construct and config norm layer.\n\n* norm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only.\n\n* with_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed.\n\n* zero_init_residual (bool) - whether to use zero init for last norm layer in resblocks to\nlet them behave as identity.\n\n* pretrained (str, optional) -— model pretrained path. Default: None\n\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nExample\n\n>>> from mmdet.models import RegNet\n>>> import torch\n>>> self = RegNet(\narch=dict(\n\nw0=88,\n\nwa=26.31,\n\nwm=2.25,\n\ngroup_w=48,\n\ndepth=25,\n\nbot_mul=1.0))\n>>> self.evalQ\n>>> inputs = torch.rand(1, 3, 32, 32)\n>>> level_outputs = self. forward(inputs)\n>>> for level_out in level_outputs:\na print (tuple(level_out.shape))\n(1, 96, 8, 8)\n(1, 192, 4, 4)\n(1, 432, 2, 2)\n(1, 1008, 1, 1)\n\nadjust_width_group (widths, bottleneck_ratio, groups)\nAdjusts the compatibility of widths and groups.\n\nParameters\n¢ widths (list [int ]) — Width of each stage.\n\n¢ bottleneck_ratio (float) — Bottleneck ratio.\n\n282 Chapter 39. mmdet.models\n", "vlm_text": "•  base channels  ( int ) – Base channels after stem layer. •  in channels  ( int ) – Number of input image channels. Default: 3. •  dilations  ( Sequence[int] ) – Dilation of each stage. •  out indices  ( Sequence[int] ) – Output from which stages. •  style    $(s t r)$   –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  frozen stages  ( int ) – Stages to be frozen (all param fixed). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – whether to use zero init for last norm layer in resblocks to let them behave as identity. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nExample \nThis image contains a Python code snippet for using the `RegNet` model from `mmdet.models`. Here's a summary of the code:\n\n1. **Imports:**\n   - Import `RegNet` from `mmdet.models`.\n   - Import `torch`.\n\n2. **Initialize RegNet:**\n   - A `RegNet` model named `self` is initialized with specific architecture parameters: \n     - `w0=88`\n     - `wa=26.31`\n     - `wm=2.25`\n     - `group_w=48`\n     - `depth=25`\n     - `bot_mul=1.0`\n\n3. **Evaluate Model:**\n   - Set the model to evaluation mode with `self.eval()`.\n\n4. **Generate Inputs:**\n   - Create random input data with shape `(1, 3, 32, 32)` using `torch.rand`.\n\n5. **Forward Pass:**\n   - Pass the inputs through the model with `self.forward(inputs)` and store the outputs in `level_outputs`.\n\n6. **Print Output Shapes:**\n   - Iterate over each output in `level_outputs` and print its shape:\n     - `(1, 96, 8, 8)`\n     - `(1, 192, 4, 4)`\n     - `(1, 432, 2, 2)`\n     - `(1, 1008, 1, 1)`\nadjust width group ( widths ,  bottleneck ratio ,  groups ) Adjusts the compatibility of widths and groups. \nParameters •  widths  ( list[int] ) – Width of each stage. •  bottleneck ratio  ( float ) – Bottleneck ratio. "}
{"page": 290, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_290.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* groups (int) — number of groups in each stage\nReturns The adjusted widths and groups of each stage.\nReturn type tuple(list)\n\nforward(x)\nForward function.\n\ngenerate_regnet (initial_width, width_slope, width_parameter, depth, divisor=8)\nGenerates per block width from RegNet parameters.\n\nParameters\n\n¢ initial_width ([int]) — Initial width of the backbone\n\n¢ width_slope ([float]) — Slope of the quantized linear function\n\n¢ width_parameter ([int]) — Parameter used to quantize the width.\n\n¢ depth ([int]) — Depth of the backbone.\n\n¢ divisor (int, optional) — The divisor of channels. Defaults to 8.\nReturns return a list of widths of each stage and the number of stages\nReturn type list, int\n\nget_stages_from_blocks (widths)\nGets widths/stage_blocks of network at each stage.\n\nParameters widths (list [int]) — Width in each stage.\nReturns width and depth of each stage\nReturn type tuple(list)\n\nstatic quantize_float (number, divisor)\nConverts a float to closest non-zero int divisible by divisor.\n\nParameters\n\n* number (int) — Original number to be quantized.\n\n¢ divisor (int) — Divisor used to quantize the number.\nReturns quantized number that is divisible by devisor.\nReturn type int\n\nclass mmdet.models.backbones.Res2Net (scales=4, base_width=26, style='pytorch', deep_stem=True,\navg_down=True, pretrained=None, init_cfg=None, **kwargs)\nRes2Net backbone.\n\nParameters\n¢ scales (int) — Scales used in Res2Net. Default: 4\n¢ base_width (int) — Basic width of each scale. Default: 26\n¢ depth (int) — Depth of res2net, from {50, 101, 152}.\n¢ in_channels (int) — Number of input image channels. Default: 3.\n* num_stages (int) — Res2net stages. Default: 4.\n* strides (Sequence [int ]) — Strides of the first block of each stage.\n\n¢ dilations (Sequence [int]) — Dilation of each stage.\n\n39.2. backbones 283\n", "vlm_text": "•  groups  ( int ) – number of groups in each stage Returns  The adjusted widths and groups of each stage. Return type  tuple(list) \nforward  $(x)$  Forward function. \ngenerate reg net ( initial width ,  width slope ,  width parameter ,  depth ,  diviso  $r{=}8.$  ) Generates per block width from RegNet parameters. \n•  initial width  ( [int] ) – Initial width of the backbone •  width slope  ( [float] ) – Slope of the quantized linear function •  width parameter  ( [int] ) – Parameter used to quantize the width. •  depth  ( [int] ) – Depth of the backbone. •  divisor  ( int, optional ) – The divisor of channels. Defaults to 8. Returns  return a list of widths of each stage and the number of stages Return type  list, int \nget stages from blocks ( widths ) Gets widths/stage blocks of network at each stage. Parameters  widths  ( list[int] ) – Width in each stage. Returns  width and depth of each stage Return type  tuple(list) \nstatic quant ize float ( number ,  divisor ) Converts a float to closest non-zero int divisible by divisor. \nParameters •  number  ( int ) – Original number to be quantized. •  divisor  ( int ) – Divisor used to quantize the number. Returns  quantized number that is divisible by devisor. Return type  int \nclass  mmdet.models.backbones. Res2Net ( scales  $\\scriptstyle{:=4}$  ,  base_width  $\\iota{=}26$  ,  style  $=$  'pytorch' ,  deep_stem  $\\scriptstyle{\\mathcal{S}}$  True , avg_down  $.=$  True ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ,  \\*\\*kwargs ) \nRes2Net backbone. \nParameters \n scales  ( int ) – Scales used in Res2Net. Default: 4  base_width  ( int ) – Basic width of each scale. Default: 26  depth  ( int ) – Depth of res2net, from {50, 101, 152}.  in channels  ( int ) – Number of input image channels. Default: 3.  num_stages  ( int ) – Res2net stages. Default: 4.  strides  ( Sequence[int] ) – Strides of the first block of each stage.  dilations  ( Sequence[int] ) – Dilation of each stage. "}
{"page": 291, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_291.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* out_indices (Sequence[int]) — Output from which stages.\n\n* style (str) -—pytorch or caffe. If set to “pytorch”, the stride-two layer is the 3x3 conv layer,\notherwise the stride-two layer is the first 1x1 conv layer.\n\n* deep_stem (bool) — Replace 7x7 conv in input stem with 3 3x3 conv\n\n* avg_down (bool) — Use AvgPool instead of stride conv when downsampling in the bot-\ntle2neck.\n\n* frozen_stages (int) — Stages to be frozen (stop grad and set eval mode). -1 means not\nfreezing any parameters.\n\n* norm_cfg (dict) — Dictionary to construct and config norm layer.\n\n* norm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only.\n\n* plugins (list [dict ]) — List of plugins for stages, each dict contains:\n— cfg (dict, required): Cfg dict to build plugin.\n\n— position (str, required): Position inside block to insert plugin, options are ‘after_conv1’,\n‘after_conv2’, ‘after_conv3’.\n\n— stages (tuple[bool], optional): Stages to apply plugin, length should be same as\n“‘num_stages’.\n\n* with_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed.\n\n* zero_init_residual (bool) — Whether to use zero init for last norm layer in resblocks to\nlet them behave as identity.\n\n* pretrained (str, optional) -— model pretrained path. Default: None\n\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nExample\n\n>>> from mmdet.models import Res2Net\n>>> import torch\n\n>>> self = Res2Net(depth=50, scales=4, base_width=26)\n>>> self.evalQ\n\n>>> inputs = torch.rand(1, 3, 32, 32)\n>>> level_outputs = self. forward(inputs)\n>>> for level_out in level_outputs:\n\na print (tuple(level_out.shape))\n(1, 256, 8, 8)\n\n(1, 512, 4, 4)\n\n(1, 1024, 2, 2)\n\n(1, 2048, 1, 1)\n\nmake_res_layer(**kwargs)\n\nPack all blocks in a stage into a ResLayer.\n\nclass mmdet.models.backbones.ResNeSt (groups=1, base_width=4, radix=2, reduction_factor=4,\n\navg_down_stride=True, **kwargs)\n\nResNeSt backbone.\n\nParameters\n\n284\n\nChapter 39. mmdet.models\n\n", "vlm_text": "•  out indices  ( Sequence[int] ) – Output from which stages. •  style    $(s t r)$   –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  deep_stem  ( bool ) – Replace 7x7 conv in input stem with 3 3x3 conv •  avg_down  ( bool ) – Use AvgPool instead of stride conv when down sampling in the bot- tle2neck. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  plugins  ( list[dict] ) – List of plugins for stages, each dict contains: –  cfg (dict, required): Cfg dict to build plugin. –  position (str, required): Position inside block to insert plugin, options are ‘after con v 1’, ‘after con v 2’, ‘after con v 3’. –  stages (tuple[bool], optional): Stages to apply plugin, length should be same as ‘num_stages’. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – Whether to use zero init for last norm layer in resblocks to let them behave as identity. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nExample \n $>>$   from  mmdet.models  import  Res2Net  $>>$   import  torch  $>>$   self  $=$   Res2Net(depth  $\\scriptstyle{=50}$  , scales  ${=}4$  , base_width  $\\mathbf{\\lambda=}26.$  ) >>>  self . eval()  $>>$   inputs  $=$   torch . rand( 1 ,  3 ,  32 ,  32 )  $>>$   level outputs  $=$   self . forward(inputs)  $>>$   for  level_out  in  level outputs: ... print ( tuple (level_out . shape)) (1, 256, 8, 8) (1, 512, 4, 4) (1, 1024, 2, 2) (1, 2048, 1, 1) \nmake res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer . \nclass  mmdet.models.backbones. ResNeSt ( group  $s{=}I$  ,  base_width  $\\scriptstyle{:=4}$  ,  radix  $_{:=2}$  ,  reduction factor  $\\scriptstyle\\Leftarrow4$  , avg down stride  $:=$  True ,  \\*\\*kwargs ) \nResNeSt backbone. \nParameters "}
{"page": 292, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_292.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ngroups (int) — Number of groups of Bottleneck. Default: 1\nbase_width (int) — Base width of Bottleneck. Default: 4\nradix (int) — Radix of SplitAttentionConv2d. Default: 2\n\nreduction_factor (int) — Reduction factor of inter_channels in SplitAttentionConv2d.\nDefault: 4.\n\navg_down_stride (bool) — Whether to use average pool for stride in Bottleneck. Default:\nTrue.\n\nkwargs (dict) — Keyword arguments for ResNet.\n\nmake_res_layer(**kwargs)\nPack all blocks in a stage into a ResLayer.\n\nclass mmdet.models.backbones.ResNeXt (groups=1, base_width=4, **kwargs)\nResNeXt backbone.\n\nParameters\n\ndepth (int) — Depth of resnet, from {18, 34, 50, 101, 152}.\nin_channels (int) — Number of input image channels. Default: 3.\nnum_stages (int) — Resnet stages. Default: 4.\n\ngroups (int) — Group of resnext.\n\nbase_width (int) — Base width of resnext.\n\nstrides (Sequence [int ]) — Strides of the first block of each stage.\ndilations (Sequence [int ]) — Dilation of each stage.\nout_indices (Sequence [int ]) — Output from which stages.\n\nstyle (str) — pytorch or caffe. If set to “‘pytorch”, the stride-two layer is the 3x3 conv layer,\notherwise the stride-two layer is the first 1x1 conv layer.\n\nfrozen_stages (int) — Stages to be frozen (all param fixed). -1 means not freezing any\nparameters.\n\nnorm_cfg (dict) — dictionary to construct and config norm layer.\n\nnorm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only.\n\nwith_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed.\n\nzero_init_residual (bool) — whether to use zero init for last norm layer in resblocks to\nlet them behave as identity.\n\nmake_res_layer(**kwargs)\nPack all blocks in a stage into a ResLayer\n\n39.2. backbones\n\n285\n", "vlm_text": "•  groups  ( int ) – Number of groups of Bottleneck. Default: 1 •  base_width  ( int ) – Base width of Bottleneck. Default: 4 •  radix  ( int ) – Radix of Split Attention Con v 2 d. Default: 2 •  reduction factor  ( int ) – Reduction factor of inter channels in Split Attention Con v 2 d. Default: 4. •  avg down stride  ( bool ) – Whether to use average pool for stride in Bottleneck. Default: True. •  kwargs  ( dict ) – Keyword arguments for ResNet. \nmake res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer . \nclass  mmdet.models.backbones. ResNeXt ( group  $s{=}I$  ,  base_width  $\\scriptstyle{:=4}$  ,  \\*\\*kwargs ) ResNeXt backbone. \nParameters \n•  depth  ( int ) – Depth of resnet, from {18, 34, 50, 101, 152}. •  in channels  ( int ) – Number of input image channels. Default: 3. •  num_stages  ( int ) – Resnet stages. Default: 4. •  groups  ( int ) – Group of resnext. •  base_width  ( int ) – Base width of resnext. •  strides  ( Sequence[int] ) – Strides of the first block of each stage. •  dilations  ( Sequence[int] ) – Dilation of each stage. •  out indices  ( Sequence[int] ) – Output from which stages. •  style  ( str ) –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  frozen stages  ( int ) – Stages to be frozen (all param fixed). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – whether to use zero init for last norm layer in resblocks to let them behave as identity. \nmake res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer "}
{"page": 293, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_293.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.backbones.ResNet (depth, in_channels=3, stem_channels=None, base_channels=64,\nnum_stages=4, strides=(1, 2, 2, 2), dilations=(1, 1, 1, 1),\nout_indices=(0, 1, 2, 3), style='pytorch', deep_stem=False,\navg_down=False, frozen_stages=- 1, conv_cfg=None,\nnorm_cfg={ 'requires_grad': True, 'type': 'BN'}, norm_eval=True,\ndcn=None, stage_with_dcn=(False, False, False, False),\nplugins=None, with_cp=False, zero_init_residual=True,\npretrained=None, init_cfg=None)\n\nResNet backbone.\n\nParameters\n¢ depth (int) — Depth of resnet, from {18, 34, 50, 101, 152}.\n\n* stem_channels (int | None) — Number of stem channels. If not specified, it will be the\nsame as base_channels. Default: None.\n\n* base_channels (int) — Number of base channels of res layer. Default: 64.\n¢ in_channels (int) — Number of input image channels. Default: 3.\n\n* num_stages (int) — Resnet stages. Default: 4.\n\n* strides (Sequence [int ]) — Strides of the first block of each stage.\n\n¢ dilations (Sequence [int]) — Dilation of each stage.\n\n* out_indices (Sequence[int]) — Output from which stages.\n\n* style (str) -—pytorch or caffe. If set to “pytorch”, the stride-two layer is the 3x3 conv layer,\notherwise the stride-two layer is the first 1x1 conv layer.\n\n* deep_stem (bool) — Replace 7x7 conv in input stem with 3 3x3 conv\n\n* avg_down (bool) — Use AvgPool instead of stride conv when downsampling in the bottle-\nneck.\n\n* frozen_stages (int) — Stages to be frozen (stop grad and set eval mode). -1 means not\nfreezing any parameters.\n\n* norm_cfg (dict) — Dictionary to construct and config norm layer.\n\n* norm_eval (bool) — Whether to set norm layers to eval mode, namely, freeze running stats\n(mean and var). Note: Effect on Batch Norm and its variants only.\n\n* plugins (list [dict ]) — List of plugins for stages, each dict contains:\n— cfg (dict, required): Cfg dict to build plugin.\n\n— position (str, required): Position inside block to insert plugin, options are ‘after_conv1’,\n‘after_conv2’, ‘after_conv3’.\n\n— stages (tuple[bool], optional): Stages to apply plugin, length should be same as\n“‘num_stages’.\n\n* with_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed.\n\n* zero_init_residual (bool) — Whether to use zero init for last norm layer in resblocks to\nlet them behave as identity.\n\n* pretrained (str, optional) -— model pretrained path. Default: None\n\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\n286 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.backbones. ResNet ( depth ,  in_channel  $\\mathfrak{s}{=}3$  ,  stem channels  $\\mathbf{:=}$  None ,  base channel  $\\scriptstyle s=64$  , num_stage  $s{=}4$  ,  strides=(1, 2, 2, 2) ,  dilations=(1, 1, 1, 1) , out_indice  $s{=}$  (0, 1, 2, 3) ,  style  $\\mathbf{\\dot{\\rho}}=\\mathbf{\\dot{\\rho}}$  'pytorch' ,  deep_stem  $\\scriptstyle{\\mathcal{S}}$  False , avg_down  $=$  False ,  frozen stages=- 1 ,  conv_cfg  $=$  None , norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} ,  norm_eval  $\\acute{=}$  True , dcn  $\\scriptstyle.\\equiv$  None ,  stage with dc n  $=$  (False, False, False, False) , plugins  $\\leftrightharpoons$  None ,  with_cp=False ,  zero in it residual  $\\leftrightharpoons$  True , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nResNet backbone. \nParameters \n•  depth  ( int ) – Depth of resnet, from {18, 34, 50, 101, 152}. •  stem channels  ( int | None ) – Number of stem channels. If not specified, it will be the same as  base channels . Default: None. •  base channels  ( int ) – Number of base channels of res layer. Default: 64. •  in channels  ( int ) – Number of input image channels. Default: 3. •  num_stages  ( int ) – Resnet stages. Default: 4. •  strides  ( Sequence[int] ) – Strides of the first block of each stage. •  dilations  ( Sequence[int] ) – Dilation of each stage. •  out indices  ( Sequence[int] ) – Output from which stages. •  style  ( str ) –  pytorch  or  caffe . If set to “pytorch”, the stride-two layer is the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv layer. •  deep_stem  ( bool ) – Replace  $7\\mathrm{x}7$   conv in input stem with 3 3x3 conv •  avg_down  ( bool ) – Use AvgPool instead of stride conv when down sampling in the bottle- neck. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  norm_eval  ( bool ) – Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. •  plugins  ( list[dict] ) – List of plugins for stages, each dict contains: –  cfg (dict, required): Cfg dict to build plugin. –  position (str, required): Position inside block to insert plugin, options are ‘after con v 1’, ‘after con v 2’, ‘after con v 3’. –  stages (tuple[bool], optional): Stages to apply plugin, length should be same as ‘num_stages’. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  zero in it residual  ( bool ) – Whether to use zero init for last norm layer in resblocks to let them behave as identity. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None "}
{"page": 294, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_294.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> from mmdet.models import ResNet\n>>> import torch\n\n>>> self = ResNet(depth=18)\n\n>>> self.evalQ\n\n>>> inputs = torch.rand(1, 3, 32, 32)\n>>> level_outputs = self. forward(inputs)\n>>> for level_out in level_outputs:\na print (tuple(level_out.shape))\n(1, 64, 8, 8)\n\n(1, 128, 4, 4)\n\n(1, 256, 2, 2)\n\n(1, 512, 1, 1)\n\nforward(x)\nForward function.\n\nmake_res_layer (***kwargs)\nPack all blocks in a stage into a ResLayer.\n\nmake_stage_plugins (plugins, stage_idx)\nMake plugins for ResNet stage_idx th stage.\n\nCurrently we support to insert context_block, empirical_attention_block, nonlocal_block into\nthe backbone like ResNet/ResNeXt. They could be inserted after conv 1/conv2/conv3 of Bottleneck.\n\nAn example of plugins format could be:\n\nExamples\n\n>>> plugins=[\n\ndict (cfg=dict(type='xxx', argl='xxx'),\nstages=(False, True, True, True),\nposition='after_conv2'),\n\ndict (cfg=dict(type='yyy'),\nstages=(True, True, True, True),\nposition='after_conv3'),\n\ndict(cfg=dict(type='zzz', postfix='1'),\nstages=(True, True, True, True),\nposition='after_conv3'),\n\ndict(cfg=dict(type='zzz', postfix='2'),\nstages=(True, True, True, True),\nposition='after_conv3')\n\n>>> self = ResNet(depth=18)\n>>> stage_plugins = self.make_stage_plugins(plugins, 0)\n>>> assert len(stage_plugins) == 3\n\nSuppose stage_idx=9, the structure of blocks in the stage would be:\n\nconv1-> conv2->conv3->yyy->zzz1->zzz2\n\nSuppose ‘stage_idx=1’, the structure of blocks in the stage would be:\n\n39.2. backbones 287\n\n", "vlm_text": "Example \nThe image shows a Python code snippet, specifically for testing the ResNet model from MMDetection using PyTorch. It performs the following steps:\n\n1. Imports the `ResNet` class from `mmdet.models` and the `torch` library.\n2. Initializes a ResNet model with a depth of 18 layers and sets it to evaluation mode.\n3. Creates a random input tensor with shape `(1, 3, 32, 32)`.\n4. Passes the inputs through the model to get outputs at different levels.\n5. Loops through the level outputs and prints their shapes. The printed shapes are:\n\n   - `(1, 64, 8, 8)`\n   - `(1, 128, 4, 4)`\n   - `(1, 256, 2, 2)`\n   - `(1, 512, 1, 1)`\nforward  $(x)$  Forward function. \nmake res layer ( \\*\\*kwargs ) Pack all blocks in a stage into a  ResLayer . \nMake plugins for ResNet  stage_idx  th stage. Currently we support to insert  context block ,  empirical attention block ,  non local block  into the backbone like ResNet/ResNeXt. They could be inserted after conv1/conv2/conv3 of Bottleneck. \nAn example of plugins format could be: \nExamples \nThe table displays a Python code snippet. It shows a list of dictionaries named `plugins`, where each dictionary represents a configuration for a plugin with attributes such as `cfg`, `stages`, and `position`. These configurations appear to be for a neural network model, specifically using stages in a `ResNet`.\n\nHere’s a summary:\n\n1. Four plugin configurations are defined, each with specific `type`, `stages`, and `position` values.\n2. Two plugins of type `'zzz'` have different postfixes.\n3. The `ResNet` model is instantiated with a depth of 18.\n4. The function `make_stage_plugins` is called with the `plugins` list and a stage index of 0.\n5. An assertion checks that the length of `stage_plugins` is 3.\n\nThe result for `stage_idx=0` shows the structure of blocks in the stage: \n`conv1 -> conv2 -> conv3 -> yyy -> zzz1 -> zzz2`.\nSuppose ‘stage_idx  $\\scriptstyle=1^{;}$  ’, the structure of blocks in the stage would be: "}
{"page": 295, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_295.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nconvl-> conv2->xxx->conv3->yyy->zzz1->zzz2\n\nIf stages is missing, the plugin would be applied to all stages.\nParameters\n\n¢ plugins (list [dict]) — List of plugins cfg to build. The postfix is required if multiple\nsame type plugins are inserted.\n\n* stage_idx (int) — Index of stage to build\nReturns Plugins for current stage\nReturn type list[dict]\n\nproperty normi\nthe normalization layer named “norm1”\n\nType nn.Module\n\ntrain (mode=True)\nConvert the model into training mode while keep normalization layer freezed.\n\nclass mmdet.models.backbones.ResNetV1d(**kwargs)\nResNetV 1d variant described in Bag of Tricks.\n\nCompared with default ResNet(ResNetV 1b), ResNetV 1d replaces the 7x7 conv in the input stem with three 3x3\nconvs. And in the downsampling block, a 2x2 avg_pool with stride 2 is added before conv, whose stride is\nchanged to 1.\n\nclass mmdet.models.backbones.SSDVGG (depth, with_last_pool=False, ceil_mode=True, out_indices=(3, 4),\nout_feature_indices=(22, 34), pretrained=None, init_cfg=None,\ninput_size=None, 12_norm_scale=None)\n\nVGG Backbone network for single-shot-detection.\n\nParameters\n* depth (int) — Depth of vgg, from {11, 13, 16, 19}.\n* with_last_pool (bool) — Whether to add a pooling layer at the last of the model\n* ceil_mode (bool) — When True, will use ceil instead of floor to compute the output shape.\n* out_indices (Sequence[int]) — Output from which stages.\n* out_feature_indices (Sequence [int J) — Output from which feature map.\n* pretrained (str, optional) -— model pretrained path. Default: None\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\n¢ input_size (int, optional) - Deprecated argumment. Width and height of input, from\n{300, 512}.\n\n¢ 12_norm_scale (float, optional) — Deprecated argumment. L2 normalization layer\ninit scale.\n\n288 Chapter 39. mmdet.models\n", "vlm_text": "conv1-> conv2->xxx->conv3->yyy->zzz1->zzz2 If stages is missing, the plugin would be applied to all stages. \n\nParameters \n plugins  ( list[dict] ) – List of plugins cfg to build. The postfix is required if multiple same type plugins are inserted. \n•  stage_idx  ( int ) – Index of stage to build \nReturns  Plugins for current stage \nReturn type  list[dict] \nproperty norm1 the normalization layer named “norm1” \nType  nn.Module \ntrain ( mode  $=$  True ) Convert the model into training mode while keep normalization layer freezed. \nclass  mmdet.models.backbones. ResNetV1d ( \\*\\*kwargs ) ResNetV1d variant described in  Bag of Tricks . \nCompared with default ResNet(ResNetV1b), ResNetV1d replaces the  $7\\mathrm{x}7$   conv in the input stem with three 3x3 convs. And in the down sampling block, a  $2\\mathrm{x}2$   avg_pool with stride 2 is added before conv, whose stride is changed to 1. \nclass  mmdet.models.backbones. SSDVGG ( depth ,  with last poo  $\\leftrightharpoons$  False ,  ceil_mode  $=$  True ,  out indices=(3, 4) , out feature indices=(22, 34) ,  pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None , input_size  $=$  None ,  l 2 norm scale  $=$  None ) VGG Backbone network for single-shot-detection. \nParameters \n•  with last pool  ( bool ) – Whether to add a pooling layer at the last of the model •  ceil_mode  ( bool ) – When True, will use  ceil  instead of  floor  to compute the output shape. •  out indices  ( Sequence[int] ) – Output from which stages. •  out feature indices  ( Sequence[int] ) – Output from which feature map. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \n\n\n\n\n\n•  input_size  ( int, optional ) – Deprecated argumment. Width and height of input, from {300, 512}. \n•  l 2 norm scale  ( float, optional ) – Deprecated argumment. L2 normalization layer init scale. "}
{"page": 296, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_296.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> self = SSDVGG(input_size=300, depth=11)\n>>> self.evalQ\n\n>>> inputs = torch.rand(1, 3, 300, 300)\n>>> level_outputs = self. forward(inputs)\n>>> for level_out in level_outputs:\n\na print (tuple(level_out.shape))\n(1, 1024, 19, 19)\n\n(1, 512, 10, 10)\n\n(1, 256, 5, 5)\n\n(1, 256, 3, 3)\n\n(1, 256, 1, 1)\n\nforward(x)\nForward function.\n\ninit_weights (pretrained=None)\nInitialize the weights.\n\nclass mmdet.models.backbones.SwinTransformer (pretrain_img_size=224, in_channels=3, embed_dims=96,\n\npatch_size=4, window_size=7, mlp_ratio=4, depths=(2,\n2, 6, 2), num_heads=(3, 6, 12, 24), strides=(4, 2, 2, 2),\nout_indices=(0, 1, 2, 3), qkv_bias=True, qk_scale=None,\npatch_norm=True, drop_rate=0.0, attn_drop_rate=0.0,\n\ndrop_path_rate=0.1, use_abs_pos_embed=False,\nact_cfg=(‘type': 'GELU'}, norm_cfg={'type': 'LN'},\nwith_cp=False, pretrained=None,\nconvert_weights=False, frozen_stages=- 1,\ninit_cfg=None)\n\nSwin Transformer A PyTorch implement of : Swin Transformer: Hierarchical Vision Transformer using Shifted\n\nWindows -\n\nhttps://arxiv.org/abs/2103.14030\n\nInspiration from https://github.com/microsoft/S win- Transformer\n\nParameters\n\npretrain_img_size (int | tuple[int])-The size of input image when pretrain. De-\nfaults: 224.\n\nin_channels (int) — The num of input channels. Defaults: 3.\n\nembed_dims (int) — The feature dimension. Default: 96.\n\npatch_size (int | tuple[int])— Patch size. Default: 4.\n\nwindow_size (int) — Window size. Default: 7.\n\nmlp_ratio (int) — Ratio of mlp hidden dim to embedding dim. Default: 4.\n\ndepths (tuple[int]) — Depths of each Swin Transformer stage. Default: (2, 2, 6, 2).\n\nnum_heads (tuple[int]) — Parallel attention heads of each Swin Transformer stage. De-\nfault: (3, 6, 12, 24).\n\nstrides (tuple [int ])—The patch merging or patch embedding stride of each Swin Trans-\nformer stage. (In swin, we set kernel size equal to stride.) Default: (4, 2, 2, 2).\n\nout_indices (tuple[int]) — Output from which stages. Default: (0, 1, 2, 3).\n\n39.2. backbones\n\n289\n", "vlm_text": "Example \nThis image is a display of Python code using the PyTorch library, specifically dealing with a neural network model, likely a variant of VGG used in SSD (Single Shot Multibox Detector) for object detection. The code is set up as follows:\n\n1. `self = SSDVGG(input_size=300, depth=11)`: This line creates an instance of an SSDVGG network, specifying an input size of 300x300 pixels and a depth of 11.\n   \n2. `self.eval()`: This line puts the model into evaluation mode, which affects certain layers like dropout and batchnorm to behave appropriately during evaluation.\n\n3. `inputs = torch.rand(1, 3, 300, 300)`: This line generates a random tensor simulating an input image batch for the model. The shape `(1, 3, 300, 300)` implies a batch size of 1, 3 color channels (probably RGB), and spatial dimensions of 300x300 pixels.\n\n4. `level_outputs = self.forward(inputs)`: This line performs a forward pass through the model using the `inputs`, resulting in multiple outputs (`level_outputs`). These outputs correspond to different feature maps or layers in the network.\n\n5. The `for` loop iterates over each output in `level_outputs`, printing the shape of each feature map.\n  \n- The shapes printed are:\n  - `(1, 1024, 19, 19)`: A feature map with a batch size of 1, 1024 channels, and spatial dimensions of 19x19.\n  - `(1, 512, 10, 10)`: A feature map with 512 channels and 10x10 spatial dimensions.\n  - `(1, 256, 5, 5)`: Two identical feature maps, each with 256 channels and 5x5 spatial dimensions.\n  - `(1, 256, 3, 3)`: A feature map with 256 channels and 3x3 spatial dimensions.\n  - `(1, 256, 1, 1)`: A feature map with 256 channels and a 1x1 spatial dimension.\n\nThese feature maps are characteristic of SSD networks, which use multiple layers of different resolutions for detecting objects of various sizes.\nforward  $(x)$  Forward function. \nin it weights ( pretrained  $\\leftrightharpoons$  None ) Initialize the weights. \n( pre train img s iz  $\\scriptstyle{z e=224}$  ,  in channels=3 ,  embed_dims=96 , patch_size  $\\scriptstyle=4$  ,  window size=7 ,  mlp_ratio=4 ,  depths=(2, 2, 6, 2) ,  num_heads=(3, 6, 12, 24) ,  strides=(4, 2, 2, 2) , out indices  $=$  (0, 1, 2, 3) ,  qkv_bias  $\\mathbf{:=}$  True ,  qk_scale=None , patch_norm  $\\mathbf{\\beta}=$  True ,  drop_rate=0.0 ,  at tn drop rate=0.0 , drop path rate $\\mathord{=}\\!O.I$ , use abs pos embed=False,act_cfg  $=$  {'type': 'GELU'} ,  norm_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'LN'} , with_cp  $\\leftrightharpoons$  False ,  pretrained  $\\leftrightharpoons$  None , convert weights  $\\mathbf{\\varepsilon}=$  False ,  frozen stages=- 1 , init_cfg  $=$  None ) \nSwin Transformer A PyTorch implement of  $:$   Swin Transformer: Hierarchical Vision Transformer using Shifted Windows  - \nhttps://arxiv.org/abs/2103.14030 \nInspiration from  https://github.com/microsoft/Swin-Transformer \nParameters \n•  pre train img size  ( int | tuple[int] ) – The size of input image when pretrain. De- faults: 224. •  in channels  ( int ) – The num of input channels. Defaults: 3. •  embed_dims  ( int ) – The feature dimension. Default: 96. •  patch_size  ( int | tuple[int] ) – Patch size. Default: 4. •  window size  ( int ) – Window size. Default: 7. •  mlp_ratio  ( int ) – Ratio of mlp hidden dim to embedding dim. Default: 4. •  depths  ( tuple[int] ) – Depths of each Swin Transformer stage. Default: (2, 2, 6, 2). •  num_heads  ( tuple[int] ) – Parallel attention heads of each Swin Transformer stage. De- fault: (3, 6, 12, 24). •  strides  ( tuple[int] ) – The patch merging or patch embedding stride of each Swin Trans- former stage. (In swin, we set kernel size equal to stride.) Default: (4, 2, 2, 2). •  out indices  ( tuple[int] ) – Output from which stages. Default: (0, 1, 2, 3). "}
{"page": 297, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_297.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* qkv_bias (bool, optional) -— If True, add a learnable bias to query, key, value. Default:\nTrue\n\n* qk_scale (float | None, optional) — Override default qk scale of head_dim ** -0.5\nif set. Default: None.\n\n* patch_norm (bool) — If add a norm layer for patch embed and patch merging. Default:\nTrue.\n\n* drop_rate (float) — Dropout rate. Defaults: 0.\n* attn_drop_rate (float) — Attention dropout rate. Default: 0.\n* drop_path_rate (float) — Stochastic depth rate. Defaults: 0.1.\n\n* use_abs_pos_embed (boo1) — If True, add absolute position embedding to the patch em-\nbedding. Defaults: False.\n\n* act_cfg (dict) — Config dict for activation layer. Default: dict(type=’LN’).\n\n* norm_cfg (dict) — Config dict for normalization layer at output of backone. Defaults:\ndict(type=’LN’).\n\n* with_cp (bool, optional) — Use checkpoint or not. Using checkpoint will save some\nmemory while slowing down the training speed. Default: False.\n\n* pretrained (str, optional) — model pretrained path. Default: None.\n\n* convert_weights (bool) — The flag indicates whether the pre-trained model is from the\noriginal repo. We may need to convert some keys to make it compatible. Default: False.\n\n* frozen_stages (int) — Stages to be frozen (stop grad and set eval mode). -1 means not\nfreezing any parameters.\n\n¢ init_cfg (dict, optional) — The Config for initialization. Defaults to None.\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\ninit_weights()\nInitialize the weights.\n\ntrain (mode=True)\nConvert the model into training mode while keep layers freezed.\n\nclass mmdet.models.backbones.TridentResNet (depth, num_branch, test_branch_idx, trident_dilations,\n\n**kwargs)\nThe stem layer, stage 1 and stage 2 in Trident ResNet are identical to ResNet, while in stage 3, Trident BottleBlock\nis utilized to replace the normal BottleBlock to yield trident output. Different branch shares the convolution\nweight but uses different dilations to achieve multi-scale output.\n\n/ stage3(b0) x - stem - stage - stage2 - stage3(b1) - output stage3(b2) /\n\nParameters\n\n¢ depth (int) — Depth of resnet, from {50, 101, 152}.\n\n290\n\nChapter 39. mmdet.models\n", "vlm_text": "•  qkv_bias  ( bool, optional ) – If True, add a learnable bias to query, key, value. Default: True •  qk_scale  ( float | None, optional ) – Override default qk scale of head_dim \\*\\* -0.5 if set. Default: None. •  patch_norm  ( bool ) – If add a norm layer for patch embed and patch merging. Default: True. •  drop_rate  ( float ) – Dropout rate. Defaults: 0. •  at tn drop rate  ( float ) – Attention dropout rate. Default: 0. •  drop path rate  ( float ) – Stochastic depth rate. Defaults: 0.1. •  use abs pos embed  ( bool ) – If True, add absolute position embedding to the patch em- bedding. Defaults: False. •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\scriptstyle{:=}$  ’LN’). •  norm_cfg  ( dict ) – Config dict for normalization layer at output of backone. Defaults: dict $\\scriptstyle(\\mathrm{type}=\\mathrm{LN})$ ).•  with_cp  ( bool, optional ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False. •  pretrained  ( str, optional ) – model pretrained path. Default: None. •  convert weights  ( bool ) – The flag indicates whether the pre-trained model is from the original repo. We may need to convert some keys to make it compatible. Default: False. •  frozen stages  ( int ) – Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. •  init_cfg  ( dict, optional ) – The Config for initialization. Defaults to None. \nforward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nin it weights()\nInitialize the weights. \ntrain ( mode  $=$  True ) Convert the model into training mode while keep layers freezed. \nclass  mmdet.models.backbones. Trident Res Net ( depth ,  num_branch ,  test branch i dx ,  trident dilation s , \\*\\*kwargs ) \nThe stem layer, stage 1 and stage 2 in Trident ResNet are identical to ResNet, while in stage 3, Trident Bottle Block is utilized to replace the normal Bottle Block to yield trident output. Different branch shares the convolution weight but uses different dilations to achieve multi-scale output. \n/ stage3(b0) x - stem - stage1 - stage2 - stage3(b1) - output stage3(b2) / \nParameters \n•  depth  ( int ) – Depth of resnet, from {50, 101, 152}. "}
{"page": 298, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_298.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ num_branch (int) — Number of branches in TridentNet.\n\n* test_branch_idx (int) — In inference, all 3 branches will be used if test_branch_idx==-1,\notherwise only branch with index test_branch_idx will be used.\n\n¢ trident_dilations (tuplefint]) -—  Dilations of different trident branch.\nlen(trident_dilations) should be equal to num_branch.\n\n39.3 necks\n\nclass mmdet.models.necks.BFP (Balanced Feature Pyramids)\nBFP takes multi-level features as inputs and gather them into a single one, then refine the gathered feature and\nscatter the refined results to multi-level features. This module is used in Libra R-CNN (CVPR 2019), see the\npaper Libra R-CNN: Towards Balanced Learning for Object Detection for details.\n\nParameters\n\n¢ in_channels (int) — Number of input channels (feature maps of all levels should have the\nsame channels).\n\n* num_levels (int) — Number of input feature levels.\n* conv_cfg (dict) — The config dict for convolution layers.\n* norm_cfg (dict) — The config dict for normalization layers.\n\n¢ refine_level (int) — Index of integration and refine level of BSF in multi-level features\nfrom bottom to top.\n\n* refine_type (str) — Type of the refine op, currently support [None, ‘conv’, ‘non_local’].\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (inputs)\nForward function.\n\nclass mmdet.models.necks.CTResNetNeck (in_channel, num_deconv_filters, num_deconv_kernels,\nuse_dcn=True, init_cfg=None)\nThe neck used in CenterNet for object classification and box regression.\n\nParameters\n¢ in_channel (int) — Number of input channels.\n* num_deconv_filters (tuple[int]) — Number of filters per stage.\n* num_deconv_kernels (tuple[int]) — Number of kernels per stage.\n* use_dcn (bool) — If True, use DCNv2. Default: True.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (inputs)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\n39.3. necks 291\n", "vlm_text": "•  num_branch  ( int ) – Number of branches in TridentNet. •  test branch i dx  ( int ) – In inference, all 3 branches will be used if  test branch id  $\\scriptstyle{\\mathfrak{c}}==-I$  , otherwise only branch with index  test branch i dx  will be used. •  trident dilation s ( tuple[int] ) – Dilations of different trident branch. len(trident dilation s) should be equal to num_branch. \n39.3 necks \nclass  mmdet.models.necks. BFP ( Balanced Feature Pyramids ) BFP takes multi-level features as inputs and gather them into a single one, then refine the gathered feature and scatter the refined results to multi-level features. This module is used in Libra R-CNN (CVPR 2019), see the paper  Libra R-CNN: Towards Balanced Learning for Object Detection  for details. \nParameters \n•  in channels  ( int ) – Number of input channels (feature maps of all levels should have the same channels). •  num_levels  ( int ) – Number of input feature levels. •  conv_cfg  ( dict ) – The config dict for convolution layers. •  norm_cfg  ( dict ) – The config dict for normalization layers. •  refine level  ( int ) – Index of integration and refine level of BSF in multi-level features from bottom to top. •  refine type  ( str ) – Type of the refine op, currently support [None, ‘conv’, ‘non_local’]. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( inputs ) Forward function. \nclass  mmdet.models.necks. C TRes Net Neck ( in_channel ,  num dec on v filters ,  num dec on v kernels , use_dcn  $.=$  True ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) The neck used in  CenterNet  for object classification and box regression. \nParameters \n•  in_channel  ( int ) – Number of input channels. •  num dec on v filters  ( tuple[int] ) – Number of filters per stage. •  num dec on v kernels  ( tuple[int] ) – Number of kernels per stage. •  use_dcn  ( bool ) – If True, use DCNv2. Default: True. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( inputs ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. "}
{"page": 299, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_299.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ninit_weights()\nInitialize the weights.\n\nclass mmdet.models.necks.ChannelMapper (in_channels, out_channels, kernel_size=3, conv_cfg=None,\nnorm_cfg=None, act_cfg={'type': 'ReLU'}, num_outs=None,\ninit_cfg={‘distribution': ‘uniform’, ‘layer’: 'Conv2d', 'type':\n'Xavier'})\nChannel Mapper to reduce/increase channels of backbone features.\n\nThis is used to reduce/increase channels of backbone features.\nParameters\n¢ in_channels (List [int ]) — Number of input channels per scale.\n* out_channels (int) — Number of output channels (used at each scale).\n\n¢ kernel_size (int, optional) — kernel_size for reducing channels (used at each scale).\nDefault: 3.\n\n* conv_cfg (dict, optional) — Config dict for convolution layer. Default: None.\n* norm_cfg (dict, optional) — Config dict for normalization layer. Default: None.\n\n* act_cfg (dict, optional) — Config dict for activation layer in ConvModule. Default:\ndict(type=’ReLU’).\n\n* num_outs(int, optional)—Number of output feature maps. There would be extra_convs\nwhen num_outs larger than the length of in_channels.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nExample\n\n>>> import torch\n\n>>> in_channels = [2, 3, 5, 7]\n\n>>> scales = [340, 170, 84, 43]\n\n>>> inputs = [torch.rand(1, c, s, s)\n\nsae for c, s in zip(in_channels, scales)]\n>>> self = ChannelMapper(in_channels, 11, 3).evalQ\n>>> outputs = self. forward(inputs)\n\n>>> for i in range(len(outputs)):\n\nos print(f'outputs[{i}].shape = foutputs[i].shape}')\noutputs[0].shape = torch.Size([1, 11, 340, 340])\noutputs[1].shape = torch.Size([1, 11, 170, 170])\noutputs[2].shape = torch.Size([1, 11, 84, 84])\noutputs[3].shape = torch.Size([1, 11, 43, 43])\n\nforward (inputs)\nForward function.\n\nclass mmdet.models.necks.DilatedEncoder (in_channels, out_channels, block_mid_channels,\nnum_residual_blocks)\nDilated Encoder for YOLOF <https://arxiv.org/abs/2103.09460>>.\n\nThis module contains two types of components:\n\n* the original FPN lateral convolution layer and fpn convolution layer, which are 1x1 conv + 3x3\nconv\n\n292 Chapter 39. mmdet.models\n", "vlm_text": "in it weights()Initialize the weights. \nclass  mmdet.models.necks. Channel Mapper ( in channels ,  out channels ,  kernel size  $\\scriptstyle{:=3}$  ,  conv_cfg=None , norm_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  act_cfg  $=$  {'type': 'ReLU'} ,  num_outs=None , init_cfg  $=$  {'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) \nChannel Mapper to reduce/increase channels of backbone features. This is used to reduce/increase channels of backbone features. \n\nParameters \n•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale). •  kernel size  ( int, optional ) – kernel size for reducing channels (used at each scale). Default: 3. •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict, optional ) – Config dict for normalization layer. Default: None. •  act_cfg  ( dict, optional ) – Config dict for activation layer in ConvModule. Default: dict(type  $=^{!}$  ’ReLU’). •  num_outs  ( int, optional ) – Number of output feature maps. There would be extra con vs when num_outs larger than the length of in channels. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nExample \n $>>$   import  torch  $>>$   in channels  $=$   [ 2 ,  3 ,  5 ,  7 ]  $>>$   scales    $=$   [ 340 ,  170 ,  84 ,  43 ]  $>>$   inputs    $=$   [torch . rand( 1 , c, s, s) ... for  c, s  in  zip (in channels, scales)]  $>>$   self  $=$   Channel Mapper(in channels,  11 ,  3 ) . eval()  $>>$   outputs  $=$   self . forward(inputs)  $>>$   for  i  in  range ( len (outputs)): ... print ( f ' outputs[ { i } ].shape  $=$   { outputs[i] . shape } ' ) outputs[0].shape  $=$   torch.Size([1, 11, 340, 340]) outputs[1].shape  $=$   torch.Size([1, 11, 170, 170]) outputs[2].shape  $=$   torch.Size([1, 11, 84, 84]) outputs[3].shape  $=$   torch.Size([1, 11, 43, 43]) \nforward ( inputs ) Forward function. \nclass  mmdet.models.necks. Dilated Encoder ( in channels ,  out channels ,  block mid channels , num residual blocks ) \nDilated Encoder for YOLOF < https://arxiv.org/abs/2103.09460 >\\`. \nThis module contains two types of components: •  the original FPN lateral convolution layer and fpn convolution layer,  which are 1x1 conv   $^+$   3x3 conv "}
{"page": 300, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_300.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* the dilated residual block\n\nParameters\n¢ in_channels (int) — The number of input channels.\n* out_channels (int) — The number of output channels.\n* block_mid_channels (int) — The number of middle block output channels\n¢ num_residual_blocks (int) — The number of residual blocks.\nforward (feature)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.necks.FPG(in_channels, out_channels, num_outs, stack_times, paths,\ninter_channels=None, same_down_trans=None,\nsame_up_trans={‘kernel_size': 3, ‘padding’: 1, ‘stride’: 2, ‘type’: 'conv'},\nacross_lateral_trans={'kernel_size': 1, 'type': 'conv'},\nacross_down_trans={‘kernel_size': 3, ‘type’: 'conv'}, across_up_trans=None,\nacross_skip_trans=({'type': ‘identity'}, output_trans={‘kernel_size': 3, 'type':\n‘last_conv'}, start_level=0, end_level=- 1, add_extra_convs=False,\nnorm_cfg=None, skip_inds=None, init_cfg=[{'type': 'Caffe2Xavier', ‘layer’:\n‘Conv2d'}, {'type': ‘Constant’, ‘layer’: ['_BatchNorm’, '_InstanceNorm',\n‘GroupNorm’, 'LayerNorm'], 'val': 1.0}])\n\nFPG.\n\nImplementation of Feature Pyramid Grids (FPG). This implementation only gives the basic structure stated in\nthe paper. But users can implement different type of transitions to fully explore the the potential power of the\nstructure of FPG.\n\nParameters\n\n¢ in_channels (int) — Number of input channels (feature maps of all levels should have the\nsame channels).\n\n* out_channels (int) — Number of output channels (used at each scale)\n* num_outs (int) — Number of output scales.\n* stack_times (int) — The number of times the pyramid architecture will be stacked.\n\n* paths (list [str]) — Specify the path order of each stack level. Each element in the list\nshould be either ‘bu’ (bottom-up) or ‘td’ (top-down).\n\n¢ inter_channels (int) — Number of inter channels.\n\n* same_up_trans (dict) — Transition that goes down at the same stage.\n* same_down_trans (dict) — Transition that goes up at the same stage.\n\n* across_lateral_trans (dict) — Across-pathway same-stage\n\n* across_down_trans (dict) — Across-pathway bottom-up connection.\n\n* across_up_trans (dict) — Across-pathway top-down connection.\n\n39.3. necks 293\n", "vlm_text": "• the dilated residual block \nParameters \n•  in channels  ( int ) – The number of input channels. •  out channels  ( int ) – The number of output channels. •  block mid channels  ( int ) – The number of middle block output channels •  num residual blocks  ( int ) – The number of residual blocks. \nforward ( feature ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.necks. FPG ( in channels ,  out channels ,  num_outs ,  stack times ,  paths , inter channel  $\\leftrightharpoons$  None ,  same down tr an  $:=$  None , same up trans={'kernel size': 3, 'padding': 1, 'stride': 2, 'type': 'conv'} , across lateral trans  $\\mathbf{\\tilde{=}}$  {'kernel size': 1, 'type': 'conv'} , across down trans={'kernel size': 3, 'type': 'conv'} ,  across up trans  $\\mathbf{\\check{\\Sigma}}$  None , across skip trans={'type': 'identity'} ,  output trans={'kernel size': 3, 'type': 'last_conv'} ,  start level  $\\mathbf{\\chi}{=}0$  ,  end_level=- 1 ,  add extra con vs  $=$  False , norm_cfg $\\mathbf{\\dot{\\Sigma}}$ None, skip_inds $\\mathbf{\\varepsilon}=$ None, init_cfg $=$ [{'type': 'Caff e 2 Xavier', 'layer':'Conv2d'}, {'type': 'Constant', 'layer': ['_BatchNorm', 'Instance Norm', 'GroupNorm', 'LayerNorm'], 'val': 1.0}])\nFPG. \nImplementation of  Feature Pyramid Grids (FPG) . This implementation only gives the basic structure stated in the paper. But users can implement different type of transitions to fully explore the the potential power of the structure of FPG. \nParameters \n•  in channels  ( int ) – Number of input channels (feature maps of all levels should have the same channels). •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  stack times  ( int ) – The number of times the pyramid architecture will be stacked. •  paths  ( list[str] ) – Specify the path order of each stack level. Each element in the list should be either ‘bu’ (bottom-up) or ‘td’ (top-down). •  inter channels  ( int ) – Number of inter channels. •  same up trans  ( dict ) – Transition that goes down at the same stage. •  same down trans  ( dict ) – Transition that goes up at the same stage. •  across lateral trans  ( dict ) – Across-pathway same-stage •  across down trans  ( dict ) – Across-pathway bottom-up connection. •  across up trans  ( dict ) – Across-pathway top-down connection. "}
{"page": 301, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_301.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nacross_skip_trans (dict) — Across-pathway skip connection.\noutput_trans (dict) — Transition that trans the output of the last stage.\n\nstart_level (int) — Index of the start input backbone level used to build the feature pyra-\nmid. Default: 0.\n\nend_level (int) — Index of the end input backbone level (exclusive) to build the feature\npyramid. Default: -1, which means the last level.\n\nadd_extra_convs (boo1) — It decides whether to add conv layers on top of the original fea-\nture maps. Default to False. If True, its actual mode is specified by extra_convs_on_inputs.\n\nnorm_cfg (dict) — Config dict for normalization layer. Default: None.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (inputs)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.necks.FPN(in_channels, out_channels, num_outs, start_level=0, end_level=- 1,\n\nadd_extra_convs=False, relu_before_extra_convs=False,\nno_norm_on_lateral=False, conv_cfg=None, norm_cfg=None, act_cfg=None,\nupsample_cfg={'mode': ‘nearest'}, init_cfg={‘distribution': ‘uniform’, ‘layer’:\n‘Conv2d', 'type': 'Xavier'})\n\nFeature Pyramid Network.\n\nThis is an implementation of paper Feature Pyramid Networks for Object Detection.\n\nParameters\n\nin_channels (List [int ]) — Number of input channels per scale.\nout_channels (int) — Number of output channels (used at each scale)\nnum_outs (int) — Number of output scales.\n\nstart_level (int) — Index of the start input backbone level used to build the feature pyra-\nmid. Default: 0.\n\nend_level (int) — Index of the end input backbone level (exclusive) to build the feature\npyramid. Default: -1, which means the last level.\n\nadd_extra_convs (bool | str) — If bool, it decides whether to add conv layers\non top of the original feature maps. Default to False. If True, it is equivalent to\nadd_extra_convs=’on_input’. If str, it specifies the source feature map of the extra convs.\nOnly the following options are allowed\n\n— ’on_input’: Last feat map of neck inputs (i.e. backbone feature).\n— ’on_lateral’: Last feature map after lateral convs.\n— ’on_output’: The last output feature map after fpn convs.\n\nrelu_before_extra_convs (bool) — Whether to apply relu before the extra conv. Default:\nFalse.\n\n294\n\nChapter 39. mmdet.models\n", "vlm_text": "•  across skip trans  ( dict ) – Across-pathway skip connection. •  output trans  ( dict ) – Transition that trans the output of the last stage. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool ) – It decides whether to add conv layers on top of the original fea- ture maps. Default to False. If True, its actual mode is specified by  extra con vs on inputs . •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( inputs ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.necks. FPN ( in channels ,  out channels ,  num_outs ,  start_leve  ${\\mathit{l}}{=}{\\mathit{O}}$  ,  end_level=- 1 , add extra con vs  $\\mathbf{=}$  False ,  re lu before extra con vs  $\\mathbf{\\check{\\mathbf{\\Psi}}}$  False , no norm on later a  $\\leftrightharpoons$  False ,  conv_cfg  $\\leftrightharpoons$  None ,  norm_cfg  $\\scriptstyle{\\tilde{\\mathbf{\\alpha}}}=I$  None ,  act_cfg  $\\mathbf{\\beta}=$  None , up sample cf g  $\\mathbf{\\dot{\\Sigma}}$  {'mode': 'nearest'} ,  init_cfg  $=$  {'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) \nFeature Pyramid Network. \nThis is an implementation of paper  Feature Pyramid Networks for Object Detection . \nParameters \n•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool | str ) – If bool, it decides whether to add conv layers on top of the original feature maps. Default to False. If True, it is equivalent to add extra con vs  $=$  ’on_input’ . If str, it specifies the source feature map of the extra convs. Only the following options are allowed –  ’on_input  $\\ddots$   Last feat map of neck inputs (i.e. backbone feature). –  ’on_lateral’: Last feature map after lateral convs. –  ’on_output’: The last output feature map after fpn convs. •  re lu before extra con vs  ( bool ) – Whether to apply relu before the extra conv. Default: False. "}
{"page": 302, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_302.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* no_norm_on_lateral (bool) — Whether to apply norm on lateral. Default: False.\n\n* conv_cfg (dict) — Config dict for convolution layer. Default: None.\n\n* norm_cfg (dict) — Config dict for normalization layer. Default: None.\n\n* act_cfg (str) — Config dict for activation layer in ConvModule. Default: None.\n\n* upsample_cfg (dict) — Config dict for interpolate layer. Default: dict(mode=’nearest’)\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nExample\n\n>>> import torch\n\n>>> in_channels = [2, 3, 5, 7]\n\n>>> scales = [340, 170, 84, 43]\n\n>>> inputs = [torch.rand(1, c, s, s)\n\nsae for c, s in zip(in_channels, scales)]\n\n>>> self = FPN(in_channels, 11, len(in_channels)).eval()\n>>> outputs = self. forward(inputs)\n\n>>> for i in range(len(outputs)):\n\nprint(f'outputs[{i}].shape = foutputs[i].shape}')\n\noutputs[0].shape = torch.Size([1, 11, 340, 340])\noutputs[1].shape = torch.Size([1, 11, 170, 170])\noutputs[2].shape = torch.Size([1, 11, 84, 84])\noutputs[3].shape = torch.Size([1, 11, 43, 43])\n\nforward (inputs)\n\nForward function.\n\nclass mmdet.models.necks.FPN_CARAFE (in_channels, out_channels, num_outs, start_level=0, end_level=- 1,\n\nnorm_cfg=None, act_cfg=None, order=('conv’, ‘norm’, ‘act'),\nupsample_cfg={'encoder_dilation': 1, 'encoder_kernel': 3, 'type':\n‘carafe’, 'up_group': 1, 'up_kernel': 5}, init_cfg=None)\n\nFPN_CARAFE is a more flexible implementation of FPN. It allows more choice for upsample methods during\nthe top-down pathway.\n\nIt can reproduce the performance of ICCV 2019 paper CARAFE: Content-Aware ReAssembly of FEatures Please\nrefer to https://arxiv.org/abs/1905.02188 for more details.\n\nParameters\n¢ in_channels (list [int ]) — Number of channels for each input feature map.\n* out_channels (int) — Output channels of feature pyramids.\n* num_outs (int) — Number of output stages.\n* start_level (int) — Start level of feature pyramids. (Default: 0)\n* end_level (int) — End level of feature pyramids. (Default: -1 indicates the last level).\n\n* norm_cfg (dict) — Dictionary to construct and config norm layer.\n\n* activate (str) — Type of activation function in ConvModule (Default: None indicates w/o\nactivation).\n\n* order (dict) — Order of components in ConvModule.\n\n* upsample (str) — Type of upsample layer.\n\n39.3. necks 295\n\n", "vlm_text": "•  no norm on lateral  ( bool ) – Whether to apply norm on lateral. Default: False. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  act_cfg  ( str ) – Config dict for activation layer in ConvModule. Default: None. •  up sample cf g  ( dict ) – Config dict for interpolate layer. Default:  dict(mode  $\\mathbf{=}$  ’nearest’) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nExample \n $>>$   import  torch  $>>$   in channels  $=$   [ 2 ,  3 ,  5 ,  7 ]  $>>$   scales  $=$   [ 340 ,  170 ,  84 ,  43 ]  $>>$   inputs  $=$   [torch . rand( 1 , c, s, s) ... for  c, s  in  zip (in channels, scales)]  $>>$   self  $=$   FPN(in channels,  11 ,  len (in channels)) . eval()  $>>$   outputs  $=$   self . forward(inputs)  $>>$   for  i  in  range ( len (outputs)): ... print ( f ' outputs[ { i } ].shape  $=$   { outputs[i] . shape } ' ) outputs[0].shape  $=$   torch.Size([1, 11, 340, 340]) outputs[1].shape  $=$   torch.Size([1, 11, 170, 170]) outputs[2].shape  $=$   torch.Size([1, 11, 84, 84]) outputs[3].shape  $=$   torch.Size([1, 11, 43, 43]) \nforward ( inputs ) Forward function. \nclass  mmdet.models.necks. FPN_CARAFE ( in channels ,  out channels ,  num_outs ,  start_leve  $\\mathbf{\\chi}{=}0$  ,  end_level=- 1 , norm_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  act_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  order=('conv', 'norm', 'act') , up sample cf g $=$ {'encoder dilation': 1, 'encoder kernel': 3, 'type':'carafe', 'up_group': 1, 'up_kernel': 5} ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ) \nFPN_CARAFE is a more flexible implementation of FPN. It allows more choice for upsample methods during the top-down pathway. \nIt can reproduce the performance of ICCV 2019 paper CARAFE: Content-Aware ReAssembly of FEatures Please refer to  https://arxiv.org/abs/1905.02188  for more details. \nParameters \n•  in channels  ( list[int] ) – Number of channels for each input feature map. •  out channels  ( int ) – Output channels of feature pyramids. •  num_outs  ( int ) – Number of output stages. •  start level  ( int ) – Start level of feature pyramids. (Default: 0) •  end_level  ( int ) – End level of feature pyramids. (Default: -1 indicates the last level). •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  activate  ( str ) – Type of activation function in ConvModule (Default: None indicates w/o activation). •  order  ( dict ) – Order of components in ConvModule. •  upsample  ( str ) – Type of upsample layer. "}
{"page": 303, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_303.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* upsample_cfg (dict) — Dictionary to construct and config upsample layer.\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nforward (inputs)\nForward function.\n\ninit_weights()\nInitialize the weights of module.\n\nslice_as(src, dst)\nSlice src as dst\n\nNote: src should have the same or larger size than dst.\n\nParameters\n¢ src (torch. Tensor) — Tensors to be sliced.\n¢ dst (torch. Tensor) — src will be sliced to have the same size as dst.\nReturns Sliced tensor.\nReturn type torch.Tensor\ntensor_add(a, b)\nAdd tensors a and b that might have different sizes.\n\nclass mmdet.models.necks.HRFPN (High Resolution Feature Pyramids)\npaper: High-Resolution Representations for Labeling Pixels and Regions.\n\nParameters\n¢ in_channels (list) — number of channels for each branch.\n* out_channels (int) — output channels of feature pyramids.\n* num_outs (int) — number of output stages.\n* pooling_type (str) — pooling for generating feature pyramids from {MAX, AVG}.\n* conv_cfg (dict) — dictionary to construct and config conv layer.\n* norm_cfg (dict) — dictionary to construct and config norm layer.\n\n* with_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed.\n\n* stride (int) — stride of 3x3 convolutional layers\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (inputs)\nForward function.\n\nclass mmdet.models.necks .NASFCOS_FPN (in_channels, out_channels, num_outs, start_level=1, end_level=- 1,\nadd_extra_convs=False, conv_cfg=None, norm_cfg=None,\ninit_cfg=None)\n\nFPN structure in NASFPN.\n\nImplementation of paper NAS-FCOS: Fast Neural Architecture Search for Object Detection\n\nParameters\n\n296 Chapter 39. mmdet.models\n", "vlm_text": "•  up sample cf g  ( dict ) – Dictionary to construct and config upsample layer. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward ( inputs ) Forward function. \nin it weights()Initialize the weights of module. \nslice_as(src, dst)Slice  src  as  dst \nNote:  src  should have the same or larger size than  dst \nParameters \n•  src  ( torch.Tensor ) – Tensors to be sliced. •  dst  ( torch.Tensor ) –  src  will be sliced to have the same size as  dst . Returns  Sliced tensor. Return type  torch.Tensor \ntensor_add  $(a,b)$  Add tensors  a  and  b  that might have different sizes. \nclass  mmdet.models.necks. HRFPN ( High Resolution Feature Pyramids ) paper:  High-Resolution Representations for Labeling Pixels and Regions . \nParameters \n•  in channels  ( list ) – number of channels for each branch. •  out channels  ( int ) – output channels of feature pyramids. •  num_outs  ( int ) – number of output stages. •  pooling type  ( str ) – pooling for generating feature pyramids from {MAX, AVG}. •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. •  stride  ( int ) – stride of 3x3 convolutional layers •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( inputs ) Forward function. \nclass  mmdet.models.necks. NAS FCO S FP N ( in channels ,  out channels ,  num_outs ,  start level  $\\scriptstyle{\\dot{=}}I$  ,  end_level=- 1 , add extra con v  $\\mathbf{:=}$  False ,  conv_cfg  $\\mathbf{\\beta}=$  None ,  norm_cfg  $=$  None , init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ) \nFPN structure in NASFPN. \nImplementation of paper  NAS-FCOS: Fast Neural Architecture Search for Object Detection \nParameters "}
{"page": 304, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_304.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nin_channels (List [int ]) — Number of input channels per scale.\nout_channels (int) — Number of output channels (used at each scale)\nnum_outs (int) — Number of output scales.\n\nstart_level (int) — Index of the start input backbone level used to build the feature pyra-\nmid. Default: 0.\n\nend_level (int) — Index of the end input backbone level (exclusive) to build the feature\npyramid. Default: -1, which means the last level.\n\nadd_extra_convs (boo1) — It decides whether to add conv layers on top of the original fea-\nture maps. Default to False. If True, its actual mode is specified by extra_convs_on_inputs.\n\nconv_cfg (dict) — dictionary to construct and config conv layer.\nnorm_cfg (dict) — dictionary to construct and config norm layer.\n\ninit_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\nforward (inputs)\nForward function.\n\ninit_weights()\nInitialize the weights of module.\n\nclass mmdet.models.necks.NASFPN (in_channels, out_channels, num_outs, stack_times, start_level=0,\n\nNAS-FPN.\n\nend_level=- 1, add_extra_convs=False, norm_cfg=None, init_cfg=({'‘layer':\n‘Conv2d', 'type': 'Caffe2Xavier'})\n\nImplementation of NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection\n\nParameters\n\nin_channels (List [int ]) — Number of input channels per scale.\n\nout_channels (int) — Number of output channels (used at each scale)\n\nnum_outs (int) — Number of output scales.\n\nstack_times (int) — The number of times the pyramid architecture will be stacked.\n\nstart_level (int) — Index of the start input backbone level used to build the feature pyra-\nmid. Default: 0.\n\nend_level (int) — Index of the end input backbone level (exclusive) to build the feature\npyramid. Default: -1, which means the last level.\n\nadd_extra_convs (boo1) — It decides whether to add conv layers on top of the original fea-\nture maps. Default to False. If True, its actual mode is specified by extra_convs_on_inputs.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (inputs)\nForward function.\n\nclass mmdet.models.necks.PAFPN(in_channels, out_channels, num_outs, start_level=0, end_level=- 1,\n\nadd_extra_convs=False, relu_before_extra_convs=False,\nno_norm_on_lateral=False, conv_cfg=None, norm_cfg=None,\nact_cfg=None, init_cfg=({‘distribution': ‘uniform’, ‘layer’: 'Conv2d’, 'type':\n'Xavier'})\n\nPath Aggregation Network for Instance Segmentation.\n\nThis is an implementation of the PAFPN in Path Aggregation Network.\n\n39.3. necks\n\n297\n", "vlm_text": "•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool ) – It decides whether to add conv layers on top of the original fea- ture maps. Default to False. If True, its actual mode is specified by  extra con vs on inputs . •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward ( inputs ) Forward function. in it weights()Initialize the weights of module. \nclass  mmdet.models.necks. NASFPN ( in channels ,  out channels ,  num_outs ,  stack times ,  start_leve  $\\mathord{\\left/{\\vphantom{\\left(\\frac{\\partial U}{\\partial t}\\right)}}\\right.\\kern-\\nulldelimiterspace}l\\mathrm{=}O$  , end_level  $\\mathbf{\\chi}=\\!\\cdot$  - 1 ,  add extra con vs  $\\mathbf{=}$  False ,  norm_cfg  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'type': 'Caff e 2 Xavier'} ) \nNAS-FPN. \nImplementation of  NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection \nParameters \n•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  stack times  ( int ) – The number of times the pyramid architecture will be stacked. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool ) – It decides whether to add conv layers on top of the original fea- ture maps. Default to False. If True, its actual mode is specified by  extra con vs on inputs . •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( inputs ) Forward function. \nclass  mmdet.models.necks. PAFPN ( in channels ,  out channels ,  num_outs ,  start_leve  ${\\mathit{l}}{=}{\\mathit{O}}$  ,  end_level=- 1 , add extra con vs  $\\mathbf{:=}$  False ,  re lu before extra con vs  $\\mathbf{\\hat{\\rho}}$  False , no norm on lateral  $'=$  False ,  conv_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}\\mathbf{0}$  None ,  norm_cfg  $\\leftrightharpoons$  None , act_cfg  $\\mathbf{\\dot{\\Sigma}}$  None ,  init_cfg  $=$  {'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) \nPath Aggregation Network for Instance Segmentation. This is an implementation of the  PAFPN in Path Aggregation Network . "}
{"page": 305, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_305.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\nin_channels (List [int ]) — Number of input channels per scale.\nout_channels (int) — Number of output channels (used at each scale)\nnum_outs (int) — Number of output scales.\n\nstart_level (int) — Index of the start input backbone level used to build the feature pyra-\nmid. Default: 0.\n\nend_level (int) — Index of the end input backbone level (exclusive) to build the feature\npyramid. Default: -1, which means the last level.\n\nadd_extra_convs (bool | str) — If bool, it decides whether to add conv layers\non top of the original feature maps. Default to False. If True, it is equivalent to\nadd_extra_convs=’on_input’. If str, it specifies the source feature map of the extra convs.\nOnly the following options are allowed\n\n— ’on_input’: Last feat map of neck inputs (i.e. backbone feature).\n— ’on_lateral’: Last feature map after lateral convs.\n— ’on_output’: The last output feature map after fpn convs.\n\nrelu_before_extra_convs (bool) — Whether to apply relu before the extra conv. Default:\nFalse.\n\nno_norm_on_lateral (bool) — Whether to apply norm on lateral. Default: False.\nconv_cfg (dict) — Config dict for convolution layer. Default: None.\n\nnorm_cfg (dict) — Config dict for normalization layer. Default: None.\n\nact_cfg (str) — Config dict for activation layer in ConvModule. Default: None.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (inputs)\nForward function.\n\nclass mmdet.models.necks.RFP (Recursive Feature Pyramid)\nThis is an implementation of RFP in DetectoRS. Different from standard FPN, the input of RFP should be multi\nlevel features along with origin input image of backbone.\n\nParameters\n\nrfp_steps (int) — Number of unrolled steps of RFP.\n\nrfp_backbone (dict) — Configuration of the backbone for RFP.\naspp_out_channels (int) — Number of output channels of ASPP module.\naspp_dilations (tuple[int]) — Dilation rates of four branches. Default: (1, 3, 6, 1)\n\ninit_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\nforward (inputs)\nForward function.\n\ninit_weights()\nInitialize the weights.\n\n298\n\nChapter 39. mmdet.models\n", "vlm_text": "Parameters \n•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num_outs  ( int ) – Number of output scales. •  start level  ( int ) – Index of the start input backbone level used to build the feature pyra- mid. Default: 0. •  end_level  ( int ) – Index of the end input backbone level (exclusive) to build the feature pyramid. Default: -1, which means the last level. •  add extra con vs  ( bool | str ) – If bool, it decides whether to add conv layers on top of the original feature maps. Default to False. If True, it is equivalent to add extra con vs  $=$  ’on_input’ . If str, it specifies the source feature map of the extra convs. Only the following options are allowed –  ’on_input’: Last feat map of neck inputs (i.e. backbone feature). –  ’on_lateral’: Last feature map after lateral convs. –  ’on_output’: The last output feature map after fpn convs. •  re lu before extra con vs  ( bool ) – Whether to apply relu before the extra conv. Default: False. •  no norm on lateral  ( bool ) – Whether to apply norm on lateral. Default: False. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  act_cfg  ( str ) – Config dict for activation layer in ConvModule. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( inputs ) Forward function. \nclass  mmdet.models.necks. RFP ( Recursive Feature Pyramid ) This is an implementation of RFP in  DetectoRS . Different from standard FPN, the input of RFP should be multi level features along with origin input image of backbone. \nParameters \n•  rfp_steps  ( int ) – Number of unrolled steps of RFP. •  rfp backbone  ( dict ) – Configuration of the backbone for RFP. •  a spp out channels  ( int ) – Number of output channels of ASPP module. •  a spp dilation s  ( tuple[int] ) – Dilation rates of four branches. Default: (1, 3, 6, 1) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward ( inputs ) Forward function. \nin it weights()Initialize the weights. "}
{"page": 306, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_306.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.necks.SSDNeck(in_channels, out_channels, level_strides, level_paddings,\n\n12_norm_scale=20.0, last_kernel_size=3, use_depthwise=False,\nconv_cfg=None, norm_cfg=None, act_cfg={'type': 'ReLU'},\ninit_cfg=[{'type': ‘Xavier’, ‘distribution’: ‘uniform’, ‘layer’: 'Conv2d'},\n{'type’: ‘Constant’, ‘val’: 1, ‘layer’: 'BatchNorm2d'}])\n\nExtra layers of SSD backbone to generate multi-scale feature maps.\n\nParameters\n\nin_channels (Sequence [int ]) — Number of input channels per scale.\nout_channels (Sequence [int ]) — Number of output channels per scale.\nlevel_strides (Sequence [int ]) — Stride of 3x3 conv per level.\nlevel_paddings (Sequence [int ]) — Padding size of 3x3 conv per level.\n\n12_norm_scale (float /None) — L2 normalization layer init scale. If None, not use L2\nnormalization on the first input feature.\n\nlast_kernel_size (int) — Kernel size of the last conv layer. Default: 3.\nuse_depthwise (bool) — Whether to use DepthwiseSeparableConv. Default: False.\nconv_cfg (dict) — Config dict for convolution layer. Default: None.\n\nnorm_cfg (dict) — Dictionary to construct and config norm layer. Default: None.\nact_cfg (dict) — Config dict for activation layer. Default: dict(type=’ReLU’).\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (inputs)\nForward function.\n\nclass mmdet.models.necks .YOLOV3Neck(num_scales, in_channels, out_channels, conv_cfg=None,\n\nnorm_cfg={ 'requires_grad': True, ‘type’: 'BN'},\nact_cfg={‘negative_slope': 0.1, 'type': 'LeakyReLU'}, init_cfg=None)\n\nThe neck of YOLOV3.\n\nIt can be treated as a simplified version of FPN. It will take the result from Darknet backbone and do some\nupsampling and concatenation. It will finally output the detection result.\n\nNote:\n\nThe input feats should be from top to bottom. i.e., from high-lvl to low-lvl\n\nBut YOLOV3Neck will process them in reversed order. i.e., from bottom (high-lvl) to top (low-lvl)\n\nParameters\n\nnum_scales (int) — The number of scales / stages.\n\nin_channels (List [int]) — The number of input channels per scale.\nout_channels (List [int ]) — The number of output channels per scale.\nconv_cfg (dict, optional) — Config dict for convolution layer. Default: None.\n\nnorm_cfg (dict, optional) — Dictionary to construct and config norm layer. Default:\ndict(type=’ BN’, requires_grad=True)\n\nact_cfg (dict, optional) — Config dict for activation layer. Default:\ndict(type=’ LeakyReLU’, negative_slope=0. 1).\n\n39.3. necks\n\n299\n", "vlm_text": "class  mmdet.models.necks. SSDNeck ( in channels ,  out channels ,  level strides ,  level padding s , l 2 norm scale  $\\it{=}20.0$  ,  last kernel size  $\\scriptstyle{:=3}$  ,  use depth wise  $:=$  False , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $\\leftrightharpoons$  None ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'ReLU'} , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  [{'type': 'Xavier', 'distribution': 'uniform', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': 'Batch Norm 2 d'}] ) \nExtra layers of SSD backbone to generate multi-scale feature maps. \nParameters \n•  in channels  ( Sequence[int] ) – Number of input channels per scale. •  out channels  ( Sequence[int] ) – Number of output channels per scale. •  level strides  ( Sequence[int] ) – Stride of 3x3 conv per level. •  level padding s  ( Sequence[int] ) – Padding size of 3x3 conv per level. •  l 2 norm scale  ( float|None ) – L2 normalization layer init scale. If None, not use L2 normalization on the first input feature. •  last kernel size  ( int ) – Kernel size of the last conv layer. Default: 3. •  use depth wise  ( bool ) – Whether to use Depth wise Separable Con v. Default: False. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: None. •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\fallingdotseq$  ReLU’). •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( inputs ) Forward function. \nclass  mmdet.models.necks. YOLOV3Neck ( num_scales ,  in channels ,  out channels ,  conv_cfg=None , norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} , act_cfg  $=$  {'negative slope': 0.1, 'type': 'LeakyReLU'} ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ) \nThe neck of YOLOV3. \nIt can be treated as a simplified version of FPN. It will take the result from Darknet backbone and do some upsampling and concatenation. It will finally output the detection result. \nNote: The input feats should be from top to bottom.  i.e., from high-lvl to low-lvl But YOLOV3Neck will process them in reversed order.  i.e., from bottom (high-lvl) to top (low-lvl) \nParameters \n•  num_scales  ( int ) – The number of scales / stages. •  in channels  ( List[int] ) – The number of input channels per scale. •  out channels  ( List[int] ) – The number of output channels per scale. •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict, optional ) – Dictionary to construct and config norm layer. Default: dict(type  $:=$  ’BN’, requires grad  $=$  True) •  act_cfg ( dict, optional ) – Config dict for activation layer. Default: dict(type  $:=$  ’LeakyReLU’, negative slope  $\\mathrm{\\Lambda}{=}0.1$  ). "}
{"page": 307, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_307.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nforward (feats)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.necks .YOLOXPAFPN (in_channels, out_channels, num_csp_blocks=3,\nuse_depthwise=False, upsample_cfg={'mode': ‘nearest’,\n‘scale_factor': 2}, conv_cfg=None, norm_cfg={'eps': 0.001,\n‘momentum’: 0.03, 'type': 'BN'}, act_cfg={'type': ‘Swish'},\ninit_cfg=('a': 2.23606797749979, ‘distribution': ‘uniform’, ‘layer’:\n‘Conv2d', ‘mode’: 'fan_in’, ‘nonlinearity’: ‘leaky_relu', 'type':\n‘Kaiming'})\nPath Aggregation Network used in YOLOX.\nParameters\n\n¢ in_channels (List [int ])— Number of input channels per scale.\n\n* out_channels (int) — Number of output channels (used at each scale)\n\n* num_csp_blocks (int) — Number of bottlenecks in CSPLayer. Default: 3\n\n* use_depthwise (bool) — Whether to depthwise separable convolution in blocks. Default:\nFalse\n\n* upsample_cfg (dict) — Config dict for interpolate layer. Default: dict(scale_factor=2,\nmode=’nearest’)\n\n* conv_cfg (dict, optional) — Config dict for convolution layer. Default: None, which\nmeans using conv2d.\n\n* norm_cfg (dict) — Config dict for normalization layer. Default: dict(type=’ BN’)\n* act_cfg (dict) — Config dict for activation layer. Default: dict(type=’Swish’)\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict. Default:\nNone.\n\nforward (inputs)\n\nParameters inputs (tuple[Tensor]) — input features.\nReturns YOLOXPAFPN features.\nReturn type tuple[Tensor]\n\n300 Chapter 39. mmdet.models\n", "vlm_text": "•  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.necks. YOLOXPAFPN ( in channels ,  out channels ,  num csp block  $\\wp=3.$  , use depth wise  $\\mathbf{=}$  False ,  up sample cf g  $=$  {'mode': 'nearest', 'scale factor': 2} ,  conv_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ,  norm_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'Swish'} , init_cfg  $=$  {'a': 2.23606797749979, 'distribution': 'uniform', 'layer': 'Conv2d', 'mode': 'fan_in', 'non linearity': 'leaky_relu', 'type': 'Kaiming'} ) \nPath Aggregation Network used in YOLOX. \nParameters \n•  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( int ) – Number of output channels (used at each scale) •  num csp blocks  ( int ) – Number of bottlenecks in CSPLayer. Default: 3 •  use depth wise  ( bool ) – Whether to depthwise separable convolution in blocks. Default: False •  up sample cf g  ( dict ) – Config dict for interpolate layer. Default:  dict(scale factor  ${\\it\\Delta\\phi}=\\!2{\\it\\Delta\\Psi}$  , mode  $\\mathbf{=}$  ’nearest’) •  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None, which means using conv2d. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict(type  $=^{:}$  ’BN’) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $='$  Swish’) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None. \nforward ( inputs ) \nParameters  inputs  ( tuple[Tensor] ) – input features. Returns  YOLOXPAFPN features. Return type  tuple[Tensor] "}
{"page": 308, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_308.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n39.4 dense_heads\n\nclass mmdet.models.dense_heads .ATSSHead(num_classes, in_channels, stacked_convs=4, conv_cfg=None,\n\nnorm_cfg={‘num_groups': 32, 'requires_grad': True, 'type':\n'GN'}, reg_decoded_bbox=True,\nloss_centerness={'loss_weight': 1.0, 'type': 'CrossEntropyLoss',\n‘use_sigmoid': True}, init_cfg={‘layer': 'Conv2d', ‘override’:\n{‘bias_prob': 0.01, ‘name’: ‘atss_cls’, ‘std’: 0.01, 'type':\n‘Normal'}, 'std': 0.01, ‘type’: 'Normal'}, **kwargs)\n\nBridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection.\n\nATSS head structure is similar with FCOS, however ATSS use anchor boxes and assign label by Adaptive Training\nSample Selection instead max-iou.\n\nhttps://arxiv.org/abs/1912.02424\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\nUsually a tuple of classification scores and bbox prediction\n\ncls_scores (list{Tensor]): Classification scores for all scale levels, each is a 4D-tensor,\nthe channels number is num_anchors * num_classes.\n\nbbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-\ntensor, the channels number is num_anchors * 4.\n\nReturn type tuple\n\nforward_single(x, scale)\nForward feature of a single scale level.\n\nParameters\n\n¢ x (Tensor) — Features of a single scale level.\n\n* € (scale) — obj: mmcv.cnn.Scale): Learnable scale module to resize the bbox prediction.\nReturns\n\ncls_score (Tensor): Cls scores for a single scale level the channels number is\nnum_anchors * num_classes.\n\nbbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is\nnum_anchors * 4.\n\ncenterness (Tensor): Centerness for a single scale level, the channel number is (N,\nnum_anchors * 1, H, W).\n\nReturn type tuple\n\nget_targets(anchor_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list=None,\ngt_labels_list=None, label_channels=1, unmap_outputs=True)\nGet targets for ATSS head.\n\nThis method is almost the same as AnchorHead. get_targets(). Besides returning the targets as the parent\nmethod does, it also returns the anchors as the first element of the returned tuple.\n\n39.4. dense_heads 301\n", "vlm_text": "39.4 dense heads \nclass  mmdet.models.dense heads. ATSSHead ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{:=4}$  ,  conv_cfg  $\\mathbf{\\dot{\\Sigma}}$  None , norm_cfg  $=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  reg decoded b box  $\\mathbf{\\beta}=$  True , loss center ness={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'atss_cls', 'std': 0.01, 'type':'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) \nBridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection. \nATSS head structure is similar with FCOS, however ATSS use anchor boxes and assign label by Adaptive Training Sample Selection instead max-iou. \nhttps://arxiv.org/abs/1912.02424 \nforward ( feats ) Forward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nUsually a tuple of classification scores and bbox prediction \ncls_scores (list[Tensor]): Classification scores for all scale  levels, each is a 4D-tensor, the channels number is num anchors \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for all scale  levels, each is a 4D- tensor, the channels number is num anchors   $^{*}\\,4$  . \nReturn type  tuple \nforward single ( x ,  scale ) Forward feature of a single scale level. \nParameters \n•  x  ( Tensor ) – Features of a single scale level. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. \nReturns \ncls_score (Tensor): Cls scores for a single scale level  the channels number is num anchors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale  level, the channels number is num anchors \\* 4. centerness (Tensor): Centerness for a single scale level, the  channel number is (N, num anchors \\* 1, H, W). \nReturn type  tuple \nget targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $\\leftleftarrows$  None , gt labels list=None ,  label channels  $\\mathrm{\\Sigma}_{:=I}$  ,  un map outputs  $\\mathbf{\\tilde{=}}$  True ) Get targets for ATSS head. \nThis method is almost the same as  AnchorHead.get targets() . Besides returning the targets as the parent method does, it also returns the anchors as the first element of the returned tuple. "}
{"page": 309, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_309.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nloss (cls_scores, bbox_preds, centernesses, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* centernesses (list[Tensor]) — Centerness for each scale level with shape (N,\nnum_anchors * 1, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\ngt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (list[Tensor] | None) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single (anchors, cls_score, bbox_pred, centerness, labels, label_weights, bbox_targets,\nnum_total_samples)\nCompute loss of a single scale level.\n\nParameters\n\n* cls_score (Tensor) — Box scores for each scale level Has shape (N, num_anchors *\nnum_classes, H, W).\n\n¢ bbox_pred (Tensor) - Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W).\n\n* anchors (Tensor) — Box reference for each scale level with shape (N, num_total_anchors,\n4).\n\nlabels (Tensor) — Labels of each anchors with shape (N, num_total_anchors).\n\n¢ label_weights (Tensor) — Label weights of each anchor with shape (N,\nnum_total_anchors)\n\n¢ bbox_targets (Tensor) — BBox regression targets of each anchor weight shape (N,\nnum_total_anchors, 4).\n\n* num_total_samples (int) — Number os positive samples that is reduced over all GPUs.\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\n302 Chapter 39. mmdet.models\n", "vlm_text": "loss ( cls_scores ,  bbox_preds ,  center ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  center ness es  ( list[Tensor] ) – Centerness for each scale level with shape (N, num anchors \\* 1, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components. \nloss single ( anchors ,  cls_score ,  bbox_pred ,  centerness ,  labels ,  label weights ,  b box targets , num total samples ) Compute loss of a single scale level. \nParameters \n•  cls_score  ( Tensor ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W). •  bbox_pred  ( Tensor ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W). •  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  num total samples  ( int ) – Number os positive samples that is reduced over all GPUs. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] "}
{"page": 310, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_310.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads.AnchorFreeHead (num_classes, in_channels, feat_channels=256,\n\nstacked_convs=4, strides=(4, 8, 16, 32, 64),\ndcn_on_last_conv=False, conv_bias='auto',\nloss_cls={‘alpha': 0.25, 'gamma': 2.0, ‘loss_weight':\n1.0, ‘type’: 'FocalLoss', 'use_sigmoid': True},\nloss_bbox={'loss_weight': 1.0, ‘type’: 'IoULoss'},\nbbox_coder=({'type': 'DistancePointBBoxCoder'},\nconv_cfg=None, norm_cfg=None, train_cfg=None,\ntest_cfg=None, init_cfg={'layer': 'Conv2d', ‘override’:\n{‘bias_prob': 0.01, ‘name’: ‘conv_cls', ‘std’: 0.01, 'type':\n‘Normal'}, ‘std’: 0.01, ‘type’: 'Normal'})\n\nAnchor-free head (FCOS, Fovea, RepPoints, etc.).\n\nParameters\n\nnum_classes (int) — Number of categories excluding the background category.\n\nin_channels (int) — Number of channels in the input feature map.\n\nfeat_channels (int) — Num\n\nstacked_convs (int) — Numl\n\nber of hidden channels. Used in child classes.\n\nber of stacking convs of the head.\n\nstrides (tuple) —- Downsample factor of each feature map.\n\ndcn_on_last_conv (bool) —\n\nconv_bias (bool | str)-I\n\nIf true, use den in the last layer of towers. Default: False.\n\nspecified as auto, it will be decided by the norm_cfg. Bias\n\nof conv will be set as True if norm_cfg is None, otherwise False. Default: “auto”.\n\nloss_cls (dict) — Config of classification loss.\n\nloss_bbox (dict) — Config o:\n\nlocalization loss.\n\nbbox_coder (dict) — Config of bbox coder. Defaults ‘DistancePointBBoxCoder’.\n\nconv_cfg (dict) — Config dict for convolution layer. Default: None.\n\nnorm_cfg (dict) — Config dict for normalization layer. Default: None.\n\ntrain_cfg (dict) — Training\n\nconfig of anchor head.\n\ntest_cfg (dict) — Testing config of anchor head.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\naug_test (feats, img_metas, rescale=False)\nTest function with test time augmentation.\n\nParameters\n\n¢ feats (list [Tensor ]) — the outer list indicates test-time augmentations and inner Tensor\nshould have a shape NxCxHxW, which contains features for all images in the batch.\n\n¢ img_metas (list[list[d\n\nict JJ) — the outer list indicates test-time augs (multiscale,\n\nflip, etc.) and the inner list indicates images in a batch. each dict has image information.\n\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\n\nReturns bbox results of each class\n\nReturn type list[ndarray]\n\nforward (feats)\nForward features from the upstream network.\n\n39.4. dense_heads\n\n303\n", "vlm_text": "class  mmdet.models.dense heads. Anchor Free Head ( num classes ,  in channels ,  feat channels=256 , stacked con vs  $\\scriptstyle{\\prime}=4$  ,  strides=(4, 8, 16, 32, 64) , dc n on last con v  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}\\mathbf{0}$  False ,  conv_bias  $=$  'auto' , loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss weight':1.0, 'type': 'FocalLoss', 'use s igm oid': True} , loss_bbox={'loss weight': 1.0, 'type': 'IoULoss'} , bbox_coder={'type': 'Distance Point B Box Code r'} , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $\\leftrightharpoons$  None ,  train_cfg=None , test_cfg  $\\mathbf{\\beta}=$  None ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ) \nAnchor-free head (FCOS, Fovea, RepPoints, etc.). \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. Used in child classes. •  stacked con vs  ( int ) – Number of stacking convs of the head. •  strides  ( tuple ) – Downsample factor of each feature map. •  dc n on last con v  ( bool ) – If true, use dcn in the last layer of towers. Default: False. •  conv_bias    $(b o o I\\ \\ I\\ \\ s t r)-]$  If specified as  auto , it will be decided by the norm_cfg. Bias of conv will be set as True if  norm_cfg  is None, otherwise False. Default: “auto”. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  bbox_coder  ( dict ) – Config of bbox coder. Defaults ‘Distance Point B Box Code r’. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \naug_test ( feats ,  img_metas ,  rescale  $:=$  False ) Test function with test time augmentation. \n•  feats  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains features for all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns  bbox results of each class Return type  list[ndarray] \nforward ( feats ) Forward features from the upstream network. "}
{"page": 311, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_311.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\nUsually contain classification scores and bbox predictions.\n\ncls_scores (list{Tensor]): Box scores for each scale level, each is a 4D-tensor, the chan-\nnel number is num_points * num_classes.\n\nbbox_preds (list[Tensor]): Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\nReturn type tuple\n\nforward_single(x)\nForward features of a single scale level.\n\nParameters x (Tensor) — FPN feature maps of the specified stride.\nReturns\n\nScores for each class, bbox predictions, features after classification and regression conv\nlayers, some models needs these features like FCOS.\n\nReturn type tuple\n\nget_points (featmap_sizes, dtype, device, flatten=False)\nGet points according to feature map sizes.\n\nParameters\n¢ featmap_sizes (list [tuple]) — Multi-level feature map sizes.\n¢ dtype (torch. dtype) — Type of points.\n¢ device (torch. device) — Device of points.\n\nReturns points of each image.\n\nReturn type tuple\n\nabstract get_targets (points, gt_bboxes_list, gt_labels_list)\nCompute regression, classification and centerness targets for points in multiple images.\n\nParameters\n* points (list [Tensor ]) — Points of each fpn level, each has shape (num_points, 2).\n\n* gt_bboxes_list (list [Tensor ]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\n\n¢ gt_labels_list (list [Tensor]) — Ground truth labels of each box, each has shape\n(num_gt,).\n\nabstract loss(cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box scores for each scale level, each is a 4D-tensor, the\nchannel number is num_points * num_classes.\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\n304 Chapter 39. mmdet.models\n", "vlm_text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nUsually contain classification scores and bbox predictions. cls_scores (list[Tensor]): Box scores for each scale level,  each is a 4D-tensor, the chan- nel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for each scale  level, each is a 4D- tensor, the channel number is num_points   $^{*}\\,4$  . \nReturn type  tuple \nforward single  $(x)$  Forward features of a single scale level. \nParameters  x  ( Tensor ) – FPN feature maps of the specified stride. \nReturns \nScores for each class, bbox predictions, features  after classification and regression conv layers, some models needs these features like FCOS. \nReturn type  tuple \nget_points ( feat map sizes ,  dtype ,  device ,  flatten  $=$  False ) Get points according to feature map sizes. \nParameters \n•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  dtype  ( torch.dtype ) – Type of points. •  device  ( torch.device ) – Device of points. \nReturns  points of each image. \nReturn type  tuple \nabstract get targets ( points ,  gt b boxes list ,  gt labels list ) Compute regression, classification and centerness targets for points in multiple images. \nParameters \n•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). \nabstract loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute loss of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . "}
{"page": 312, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_312.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nclass mmdet.models.dense_heads.AnchorHead (num_classes, in_channels, feat_channels=256,\n\nanchor_generator={'ratios': [0.5, 1.0, 2.0], ‘scales’: [8, 16,\n32], ‘strides’: [4, 8, 16, 32, 64], 'type': 'AnchorGenerator'},\nbbox_coder={'clip_border': True, 'target_means': (0.0, 0.0,\n0.0, 0.0), ‘target_stds': (1.0, 1.0, 1.0, 1.0), ‘type’:\n\n‘DeltaX YWHBBoxCoder'}, reg_decoded_bbox=False,\nloss_cls={‘loss_weight': 1.0, 'type': 'CrossEntropyLoss'’,\n‘use_sigmoid': True}, loss_bbox=({'beta':\nO.1111111111111111, ‘loss_weight': 1.0, ‘type’:\n‘SmoothL1Loss'}, train_cfg=None, test_cfg=None,\ninit_cfg=(‘layer': 'Conv2d', 'std': 0.01, 'type': 'Normal'})\n\nAnchor-based head (RPN, RetinaNet, SSD, etc.).\n\nParameters\n\nnum_classes (int) — Number of categories excluding the background category.\nin_channels (int) — Number of channels in the input feature map.\nfeat_channels (int) — Number of hidden channels. Used in child classes.\nanchor_generator (dict) — Config dict for anchor generator\n\nbbox_coder (dict) — Config of bounding box coder.\n\nreg_decoded_bbox (boo1) - If true, the regression loss would be applied directly on de-\ncoded bounding boxes, converting both the predicted boxes and regression targets to abso-\nlute coordinates format. Default False. It should be True when using JoULoss, GloULoss,\nor DIoULoss in the bbox head.\n\nloss_cls (dict) — Config of classification loss.\nloss_bbox (dict) — Config of localization loss.\ntrain_cfg (dict) — Training config of anchor head.\ntest_cfg (dict) — Testing config of anchor head.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\naug_test (feats, img_metas, rescale=False)\nTest function with test time augmentation.\n\nParameters\n\n¢ feats (list [Tensor ]) — the outer list indicates test-time augmentations and inner Tensor\nshould have a shape NxCxHxW, which contains features for all images in the batch.\n\n¢ img_metas (list[list[dict]]) — the outer list indicates test-time augs (multiscale,\nflip, etc.) and the inner list indicates images in a batch. each dict has image information.\n\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\n\nReturns\n\n39.4. dense_heads 305\n", "vlm_text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. \nclass  mmdet.models.dense heads. AnchorHead ( num classes ,  in channels ,  feat channels  $\\imath{=}256$  , anchor_generator={'ratios': [0.5, 1.0, 2.0], 'scales': [8, 16, 32], 'strides': [4, 8, 16, 32, 64], 'type': 'AnchorGenerator'} , bbox_coder={'clip border': True, 'target means': (0.0, 0.0, 0.0, 0.0), 'target_stds': (1.0, 1.0, 1.0, 1.0), 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box  $\\mathbf{\\beta}=$  False , loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True}, loss_bbox={'beta':0.1111111111111111, 'loss_weight': 1.0, 'type': 'Smooth L 1 Loss'} ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg  $\\leftrightharpoons$  None , init_cfg  $\\scriptstyle{\\prime}=$  {'layer': 'Conv2d', 'std': 0.01, 'type': 'Normal'} ) \nAnchor-based head (RPN, RetinaNet, SSD, etc.). \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. Used in child classes. •  anchor generator  ( dict ) – Config dict for anchor generator •  bbox_coder  ( dict ) – Config of bounding box coder. •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \naug_test ( feats ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Test function with test time augmentation. \nParameters \n•  feats  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains features for all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns "}
{"page": 313, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_313.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nEach item in result_list is 2-tuple. The first item is bboxes with shape (n, 5), where 5 rep-\nresent (tl_x, tl_y, br_x, br_y, score). The shape of the second tensor in the tuple is labels\nwith shape (n,), The length of list should always be 1.\n\nReturn type list[tuple[Tensor, Tensor]]\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\nA tuple of classification scores and bbox prediction.\n\n¢ cls_scores (list{Tensor]): Classification scores for all scale levels, each is a 4D-tensor, the\nchannels number is num_base_priors * num_classes.\n\n* bbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-tensor, the\nchannels number is num_base_priors * 4.\n\nReturn type tuple\n\nforward_single(x)\nForward feature of a single scale level.\n\nParameters x (Tensor) — Features of a single scale level.\n\nReturns cls_score (Tensor): Cls scores for a single scale level the channels number is\nnum_base_priors * num_classes. bbox_pred (Tensor): Box energies / deltas for a single scale\nlevel, the channels number is num_base_priors * 4.\n\nReturn type tuple\n\nget_anchors (featmap_sizes, img_metas, device='cuda')\nGet anchors according to feature map sizes.\n\nParameters\n¢ featmap_sizes (list [tuple]) — Multi-level feature map sizes.\n¢ img_metas (list [dict ]) — Image meta info.\n¢ device (torch.device | str) — Device for returned tensors\n\nReturns anchor_list (list{Tensor]): Anchors of each image. valid_flag_list (list{Tensor]): Valid\nflags of each image.\n\nReturn type tuple\n\nget_targets(anchor_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list=None,\ngt_labels_list=None, label_channels=1, unmap_outputs=True,\nreturn_sampling_results=False)\nCompute regression and classification targets for anchors in multiple images.\n\nParameters\n\n¢ anchor_list (list [list [Tensor]]) — Multi level anchors of each image. The outer\nlist indicates images, and the inner list corresponds to feature levels of the image. Each\nelement of the inner list is a tensor of shape (num_anchors, 4).\n\n¢ valid_flag_list (list [list [Tensor] ]) — Multi level valid flags of each image. The\nouter list indicates images, and the inner list corresponds to feature levels of the image.\nEach element of the inner list is a tensor of shape (num_anchors, )\n\n306 Chapter 39. mmdet.models\n", "vlm_text": "Each item in result list is 2-tuple.  The first item is  bboxes  with shape (n, 5), where 5 rep- resent (tl_x, tl_y, br_x, br_y, score). The shape of the second tensor in the tuple is  labels with shape (n,), The length of list should always be 1. \nReturn type  list[tuple[Tensor, Tensor]] \nforward ( feats ) Forward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nA tuple of classification scores and bbox prediction. • cls_scores (list[Tensor]): Classification scores for all scale levels, each is a 4D-tensor, the channels number is num base priors \\* num classes. • bbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-tensor, the channels number is num base priors   $^{*}\\,4$  . \nReturn type  tuple \nforward single  $(x)$  Forward feature of a single scale level. \nParameters  x  ( Tensor ) – Features of a single scale level. \nReturns  cls_score (Tensor): Cls scores for a single scale level the channels number is num base priors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num base priors  $^{*}\\,4$  . \nReturn type  tuple \nget anchors ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get anchors according to feature map sizes. \nParameters \n•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. \n•  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – Device for returned tensors \nReturns  anchor list (list[Tensor]): Anchors of each image. valid flag list (list[Tensor]): Valid flags of each image. \nReturn type  tuple \nget targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $t{=}$  None , gt labels list  $=$  None ,  label channels  $\\scriptstyle{\\prime=}I$  ,  un map outputs  $\\mathbf{\\tilde{=}}$  True , return sampling result  $\\mathbf{\\hat{=}}$  False ) Compute regression and classification targets for anchors in multiple images. \nParameters \n•  anchor list  ( list[list[Tensor]] ) – Multi level anchors of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, 4). •  valid flag list  ( list[list[Tensor]] ) – Multi level valid flags of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, ) "}
{"page": 314, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_314.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes of each image.\n¢ img_metas (list [dict ]) — Meta info of each image.\n* gt_bboxes_ignore_list (list [Tensor]) — Ground truth bboxes to be ignored.\n¢ gt_labels_list (list [Tensor ]) — Ground truth labels of each box.\n¢ label_channels (int) — Channel of label.\n* unmap_outputs (bool) — Whether to map outputs back to the original set of anchors.\nReturns\nUsually returns a tuple containing learning targets.\n¢ labels_list (list{Tensor]): Labels of each level.\n¢ label_weights_list (list{Tensor]): Label weights of each level.\n* bbox_targets_list (list[Tensor]): BBox targets of each level.\n* bbox_weights_list (list{Tensor]): BBox weights of each level.\n* num_total_pos (int): Number of positive samples in all images.\n* num_total_neg (int): Number of negative samples in all images.\nadditional_returns: This function enables user-defined returns from\nself._get_targets_single. These returns are currently refined to properties at each\nfeature map (i.e. having HxW dimension). The results will be concatenated after the end\nReturn type tuple\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss. Default: None\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single(cls_score, bbox_pred, anchors, labels, label_weights, bbox_targets, bbox_weights,\nnum_total_samples)\nCompute loss of a single scale level.\n\nParameters\n\n39.4. dense_heads 307\n", "vlm_text": "•  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. \nReturns \nUsually returns a tuple containing learning targets. • labels list (list[Tensor]): Labels of each level. • label weights list (list[Tensor]): Label weights of each level. • b box targets list (list[Tensor]): BBox targets of each level. • b box weights list (list[Tensor]): BBox weights of each level. • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. \nadditional returns: This function enables user-defined returns from self.get targets single . These returns are currently refined to properties at each feature map (i.e. having HxW dimension). The results will be concatenated after the end \nReturn type  tuple \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nloss single ( cls_score ,  bbox_pred ,  anchors ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) Compute loss of a single scale level. \nParameters "}
{"page": 315, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_315.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* cls_score (Tensor) — Box scores for each scale level Has shape (N, num_anchors *\nnum_classes, H, W).\n\n¢ bbox_pred (Tensor) - Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W).\n\n* anchors (Tensor) — Box reference for each scale level with shape (N, num_total_anchors,\n4).\n\n¢ labels (Tensor) — Labels of each anchors with shape (N, num_total_anchors).\n\n¢ label_weights (Tensor) — Label weights of each anchor with shape (N,\nnum_total_anchors)\n\n¢ bbox_targets (Tensor) — BBox regression targets of each anchor weight shape (N,\nnum_total_anchors, 4).\n\nbbox_weights (Tensor) — BBox regression loss weights of each anchor with shape (N,\nnum_total_anchors, 4).\n\n* num_total_samples (int) — If sampling, num total samples equal to the number of total\nanchors; Otherwise, it is the number of positive anchors.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nclass mmdet.models.dense_heads.AutoAssignHead (*args, force_topk=False, topk=9,\npos_loss_weight=0.25, neg_loss_weight=0.75,\ncenter_loss_weight=0.75, **kwargs)\nAutoAssignHead head used in AutoAssign.\n\nMore details can be found in the paper .\nParameters\n\n* force_topk (bool) — Used in center prior initialization to handle extremely small gt. De-\nfault is False.\n\n* topk (int) — The number of points used to calculate the center prior when no point falls in\ngt_bbox. Only work when force_topk if True. Defaults to 9.\n\n* pos_loss_weight (float) — The loss weight of positive loss and with default value 0.25.\n* neg_loss_weight (float) — The loss weight of negative loss and with default value 0.75.\n\n* center_loss_weight (float) — The loss weight of center prior loss and with default value\n0.75.\n\nforward_single(x, scale, stride)\nForward features of a single scale level.\n\nParameters\n¢ x (Tensor) — FPN feature maps of the specified stride.\n* € (scale) — obj: mmcv.cnn.Scale): Learnable scale module to resize the bbox prediction.\n\n¢ stride (int) — The corresponding stride for feature maps, only used to normalize the\nbbox prediction when self.norm_on_bbox is True.\n\nReturns scores for each class, bbox predictions and centerness predictions of input feature maps.\n\nReturn type tuple\n\n308 Chapter 39. mmdet.models\n", "vlm_text": "•  cls_score  ( Tensor ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W). •  bbox_pred  ( Tensor ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W). •  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  b box weights  ( Tensor ) – BBox regression loss weights of each anchor with shape (N, num total anchors, 4). •  num total samples  ( int ) – If sampling, num total samples equal to the number of total anchors; Otherwise, it is the number of positive anchors. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nclass  mmdet.models.dense heads. Auto Assign Head ( \\*args ,  force_topk  $=$  False ,  topk  $z{=}9$  , pos loss weigh  $t{=}0.25$  ,  ne g loss weight=0.75 , center loss weigh  $t{=}0.75$  ,  \\*\\*kwargs ) \nAuto Assign Head head used in AutoAssign. \nMore details can be found in the  paper  . \nParameters \n•  force_topk  ( bool ) – Used in center prior initialization to handle extremely small gt. De- fault is False. •  topk  ( int ) – The number of points used to calculate the center prior when no point falls in gt_bbox. Only work when force_topk if True. Defaults to 9. •  pos loss weight  ( float ) – The loss weight of positive loss and with default value 0.25. •  ne g loss weight  ( float ) – The loss weight of negative loss and with default value 0.75. •  center loss weight  ( float ) – The loss weight of center prior loss and with default value 0.75. \nforward single ( x ,  scale ,  stride ) Forward features of a single scale level. \nParameters \n•  x  ( Tensor ) – FPN feature maps of the specified stride. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. •  stride  ( int ) – The corresponding stride for feature maps, only used to normalize the bbox prediction when self.norm on b box is True. \nReturns  scores for each class, bbox predictions and centerness predictions of input feature maps. \nReturn type  tuple "}
{"page": 316, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_316.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nget_neg_loss_single(cls_score, objectness, gt_labels, ious, inside_gt_bbox_mask)\nCalculate the negative loss of all points in feature map.\n\nParameters\n\n* cls_score (Tensor) — All category scores for each point on the feature map. The shape\nis (num_points, num_class).\n\n* objectness (Tensor) — Foreground probability of all points and is shape of (num_points,\n1).\n\n¢ gt_labels (Tensor) — The zeros based label of all gt with shape of (num_gt).\n\n* ious (Tensor) — Float tensor with shape of (num_points, num_gt). Each value represent\nthe iou of pred_bbox and gt_bboxes.\n\n¢ inside_gt_bbox_mask (Tensor) — Tensor of bool type, with shape of (num_points,\nnum_gt), each value is used to mark whether this point falls within a certain gt.\n\nReturns\n¢ neg_loss (Tensor): The negative loss of all points in the feature map.\nReturn type tuple[Tensor]\n\nget_pos_loss_single(cls_score, objectness, reg_loss, gt_labels, center_prior_weights)\nCalculate the positive loss of all points in gt_bboxes.\n\nParameters\n\n* cls_score (Tensor) — All category scores for each point on the feature map. The shape\nis (num_points, num_class).\n\n* objectness (Tensor) — Foreground probability of all points, has shape (num_points, 1).\n\n* reg_loss (Tensor) — The regression loss of each gt_bbox and each prediction box, has\nshape of (num_points, num_gt).\n\n¢ gt_labels (Tensor) — The zeros based gt_labels of all gt with shape of (num_gt,).\n\n* center_prior_weights (Tensor) — Float tensor with shape of (num_points, num_gt).\nEach value represents the center weighting coefficient.\n\nReturns\n* pos_loss (Tensor): The positive loss of all points in the gt_bboxes.\nReturn type tuple[Tensor]\n\nget_targets (points, gt_bboxes_list)\nCompute regression targets and each point inside or outside gt_bbox in multiple images.\n\nParameters\n* points (list [Tensor ]) — Points of all fpn level, each has shape (num_points, 2).\n\n* gt_bboxes_list (list [Tensor ]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\nReturns\n* inside_gt_bbox_mask_list (list[Tensor]): Each Tensor is with bool type and shape of\n\n(num_points, num_gt), each value is used to mark whether this point falls within a cer-\ntain gt.\n\n* concat_lvl_bbox_targets (list[Tensor]): BBox targets of each level. Each tensor has shape\n(num_points, num_gt, 4).\n\n39.4. dense_heads 309\n", "vlm_text": "get ne g loss single ( cls_score ,  objectness ,  gt_labels ,  ious ,  inside gt b box mask ) Calculate the negative loss of all points in feature map. \nParameters \n•  cls_score  ( Tensor ) – All category scores for each point on the feature map. The shape is (num_points, num_class). •  objectness  ( Tensor ) – Foreground probability of all points and is shape of (num_points, 1). •  gt_labels  ( Tensor ) – The zeros based label of all gt with shape of (num_gt). •  ious  ( Tensor ) – Float tensor with shape of (num_points, num_gt). Each value represent the iou of pred_bbox and gt_bboxes. •  inside gt b box mask  ( Tensor ) – Tensor of bool type, with shape of (num_points, num_gt), each value is used to mark whether this point falls within a certain gt. \nReturns \n• neg_loss (Tensor): The negative loss of all points in the feature map. \nReturn type  tuple[Tensor] \nget pos loss single ( cls_score ,  objectness ,  reg_loss ,  gt_labels ,  center prior weights ) Calculate the positive loss of all points in gt_bboxes. \nParameters \n•  cls_score  ( Tensor ) – All category scores for each point on the feature map. The shape is (num_points, num_class). •  objectness  ( Tensor ) – Foreground probability of all points, has shape (num_points, 1). •  reg_loss  ( Tensor ) – The regression loss of each gt_bbox and each prediction box, has shape of (num_points, num_gt). •  gt_labels  ( Tensor ) – The zeros based gt_labels of all gt with shape of (num_gt,). •  center prior weights  ( Tensor ) – Float tensor with shape of (num_points, num_gt). Each value represents the center weighting coefficient. \nReturns \n• pos_loss (Tensor): The positive loss of all points in the gt_bboxes. \nReturn type  tuple[Tensor] \nget targets ( points ,  gt b boxes list ) Compute regression targets and each point inside or outside gt_bbox in multiple images. \nParameters \n•  points  ( list[Tensor] ) – Points of all fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). \nReturns \n• inside gt b box mask list (list[Tensor]): Each Tensor is with bool type and shape of (num_points, num_gt), each value is used to mark whether this point falls within a cer- tain gt. • con cat lv l b box targets (list[Tensor]): BBox targets of each level. Each tensor has shape (num_points, num_gt, 4). "}
{"page": 317, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_317.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type tuple(list[Tensor])\n\ninit_weights()\nInitialize weights of the head.\n\nIn particular, we have special initialization for classified conv’s and regression conv’s bias\n\nloss (cls_scores, bbox_preds, objectnesses, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box scores for each scale level, each is a 4D-tensor, the\nchannel number is num_points * num_classes.\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\n* objectnesses (list [Tensor]) — objectness for each scale level, each is a 4D-tensor,\nthe channel number is num_points * 1.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\nclass mmdet .models.dense_heads.CascadeRPNHead (num_stages, stages, train_cfg, test_cfg, init_cfg=None)\n\nThe CascadeRPNHead will predict more accurate region proposals, which is required for two-stage detectors\n(such as Fast/Faster R-CNN). CascadeRPN consists of a sequence of RPNStage to progressively improve the\naccuracy of the detected proposals.\n\nMore details can be found in https: //arxiv.org/abs/1909.06720.\nParameters\n* num_stages (int) — number of CascadeRPN stages.\n* stages (list [dict]) — list of configs to build the stages.\n* train_cfg (list [dict]) — list of configs at training time each stage.\n* test_cfg (dict) — config at testing time.\n\naug_test_rpn(x, img_metas)\nAugmented forward test function.\n\nforward_train(x, img_metas, gt_bboxes, gt_labels=None, gt_bboxes_ignore=None, proposal_cfg=None)\nForward train function.\n\nget_bboxes ()\nget_bboxes() is implemented in StageCascadeRPNHead.\n\nloss()\nloss() is implemented in StageCascadeRPNHead.\n\n310\n\nChapter 39. mmdet.models\n", "vlm_text": "Return type tuple(list[Tensor])\nin it weights()\nInitialize weights of the head. In particular, we have special initialization for classified conv’s and regression conv’s bias \nloss ( cls_scores ,  bbox_preds ,  object ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute loss of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  object ness es  ( list[Tensor] ) – objectness for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\,1$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components. Return type  dict[str, Tensor] \nclass  mmdet.models.dense heads. Cascade RP N Head ( num_stages ,  stages ,  train_cfg ,  test_cfg ,  init_cfg  $=$  None ) The Cascade RP N Head will predict more accurate region proposals, which is required for two-stage detectors (such as Fast/Faster R-CNN). CascadeRPN consists of a sequence of RPNStage to progressively improve the accuracy of the detected proposals. \nMore details can be found in  https://arxiv.org/abs/1909.06720 . \nParameters \n•  num_stages  ( int ) – number of CascadeRPN stages. •  stages  ( list[dict] ) – list of configs to build the stages. •  train_cfg  ( list[dict] ) – list of configs at training time each stage. •  test_cfg  ( dict ) – config at testing time. \naug test rp n ( x ,  img_metas ) Augmented forward test function. \nforward train ( x ,  img_metas ,  gt_bboxes ,  gt_labels  $\\mathbf{\\hat{\\rho}}$  None ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  proposal cf g=None ) Forward train function. \nget_bboxes () get_bboxes() is implemented in Stage Cascade RP N Head. \nloss()loss() is implemented in Stage Cascade RP N Head. "}
{"page": 318, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_318.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nsimple_test_rpn(x, img_metas)\nSimple forward test function.\n\nclass mmdet.models.dense_heads.CenterNetHead (in_channel, feat_channel, num_classes,\n\nloss_center_heatmap={'‘loss_weight': 1.0, 'type':\n‘GaussianFocalLoss'}, loss_wh=({'loss_weight': 0.1,\n‘type’: 'L1Loss'}, loss_offset={‘loss_weight': 1.0, 'type':\n'L1Loss'}, train_cfg=None, test_cfg=None,\ninit_cfg=None)\n\nObjects as Points Head. CenterHead use center_point to indicate object’s position. Paper link <https://arxiv.org/\n\nabs/1904.07850>\n\nParameters\n¢ in_channel (int) — Number of channel in the input feature map.\n¢ feat_channel (int) — Number of channel in the intermediate feature map.\n* num_classes (int) — Number of categories excluding the background category.\n\n* loss_center_heatmap (dict | None) — Config of center heatmap loss. Default: Gaus-\nsianFocalLoss.\n\n* loss_wh (dict | None) — Config of wh loss. Default: L1Loss.\n* loss_offset (dict | None) — Config of offset loss. Default: L1Loss.\n\n* train_cfg (dict | None) — Training config. Useless in CenterNet, but we keep this vari-\nable for SingleStageDetector. Default: None.\n\n* test_cfg (dict | None) — Testing config of CenterNet. Default: None.\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\ndecode_heatmap (center_heatmap_pred, wh_pred, offset_pred, img_shape, k=100, kernel=3)\nTransform outputs into detections raw bbox prediction.\n\nParameters\n\n* center_heatmap_pred (Tensor) — center predict heatmap, shape (B, num_classes, H,\nW).\n\n¢ wh_pred (Tensor) — wh predict, shape (B, 2, H, W).\n\n* offset_pred (Tensor) — offset predict, shape (B, 2, H, W).\n\n¢ img_shape (list [int ]) — image shape in [h, w] format.\n\n¢ k (int) - Get top k center keypoints from heatmap. Default 100.\n\n¢ kernel (int) — Max pooling kernel for extract local maximum pixels. Default 3.\nReturns\n\nDecoded output of CenterNetHead, containing\n\nthe following Tensors:\n\n* batch_bboxes (Tensor): Coords of each box with shape (B, k, 5)\n* batch_topk_labels (Tensor): Categories of each box with shape (B, k)\n\nReturn type tuple[torch.Tensor]\n\nforward (feats)\nForward features. Notice CenterNet head does not use FPN.\n\n39.4. dense_heads 311\n", "vlm_text": "simple test rp n ( x ,  img_metas ) Simple forward test function. \nclass  mmdet.models.dense heads. Center Net Head ( in_channel ,  feat channel ,  num classes , loss center heat map={'loss weight': 1.0, 'type': 'Gaussian Focal Loss'} ,  loss_wh={'loss weight': 0.1, 'type': 'L1Loss'} ,  loss_offse  $\\mathbf{\\dot{\\rho}}$  {'loss weight': 1.0, 'type': 'L1Loss'} ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg  $\\mathbf{\\beta}=$  None , init_cfg  $=$  None ) \nObjects as Points Head. CenterHead use center point to indicate object’s position. Paper link < https://arxiv.org/ abs/1904.07850 > \nParameters \n•  in_channel  ( int ) – Number of channel in the input feature map. •  feat channel  ( int ) – Number of channel in the intermediate feature map. •  num classes  ( int ) – Number of categories excluding the background category. •  loss center heat map  ( dict | None ) – Config of center heatmap loss. Default: Gaus- s ian Focal Loss. •  loss_wh  ( dict | None ) – Config of wh loss. Default: L1Loss. •  loss offset  ( dict | None ) – Config of offset loss. Default: L1Loss. •  train_cfg  ( dict | None ) – Training config. Useless in CenterNet, but we keep this vari- able for Single Stage Detector. Default: None. •  test_cfg  ( dict | None ) – Testing config of CenterNet. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \ndecode heat map ( center heat map p red ,  wh_pred ,  offset p red ,  img_shape ,  $k{=}I O O$  ,  kerne  $l{=}3$  ) Transform outputs into detections raw bbox prediction. \n\n•  center heat map p red  ( Tensor ) – center predict heatmap, shape (B, num classes, H, W). •  wh_pred  ( Tensor ) – wh predict, shape (B, 2, H, W). •  offset p red  ( Tensor ) – offset predict, shape (B, 2, H, W). •  img_shape  ( list[int] ) – image shape in [h, w] format. •  k  ( int ) – Get top k center keypoints from heatmap. Default 100. •  kernel  ( int ) – Max pooling kernel for extract local maximum pixels. Default 3. \nReturns Decoded output of Center Net Head, containing the following Tensors: • batch b boxes (Tensor): Coords of each box with shape (B, k, 5) • batch top k labels (Tensor): Categories of each box with shape   $(\\mathbf{B},\\mathbf{k})$  Return type  tuple[torch.Tensor] \nforward ( feats ) Forward features. Notice CenterNet head does not use FPN. "}
{"page": 319, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_319.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\ncenter predict heatmaps for all levels, the channels number is num_classes.\nwh_preds (List[Tensor]): wh predicts for all levels, the channels number is 2.\noffset_preds (List[Tensor]): offset predicts for all levels, the channels number is 2.\nReturn type center_heatmap_preds (List[Tensor])\n\nforward_single (feat)\nForward feature of a single level.\n\nParameters feat (Tensor) — Feature of a single level.\nReturns\ncenter predict heatmaps, the channels number is num_classes.\n\nwh_pred (Tensor): wh predicts, the channels number is 2. offset_pred (Tensor): offset pre-\ndicts, the channels number is 2.\n\nReturn type center_heatmap_pred (Tensor)\n\nget_bboxes (center_heatmap_preds, wh_preds, offset_preds, img_metas, rescale=True, with_nms=False)\nTransform network output for a batch into bbox predictions.\n\nParameters\n\n* center_heatmap_preds (list [Tensor]) — Center predict heatmaps for all levels with\nshape (B, num_classes, H, W).\n\n¢ wh_preds (list [Tensor ]) — WH predicts for all levels with shape (B, 2, H, W).\n¢ offset_preds (list [Tensor]) — Offset predicts for all levels with shape (B, 2, H, W).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* rescale (bool) — If True, return boxes in original image space. Default: True.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default: False.\nReturns\n\nEach item in result_list is 2-tuple. The first item is an (n, 5) tensor, where 5 represent (tl_x,\ntl_y, br_x, br_y, score) and the score between 0 and 1. The shape of the second tensor in\nthe tuple is (n,), and each element represents the class label of the corresponding box.\n\nReturn type list[tuple[Tensor, Tensor]]\n\nget_targets(gt_bboxes, gt_labels, feat_shape, img_shape)\nCompute regression and classification targets in multiple images.\n\nParameters\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box.\n* feat_shape (list [int]) — feature map shape with value [B, _, H, W]\n\n¢ img_shape (list [int ]) — image shape in [h, w] format.\n\n312 Chapter 39. mmdet.models\n", "vlm_text": "Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \ncenter predict heatmaps for  all levels, the channels number is num classes. wh_preds (List[Tensor]): wh predicts for all levels, the channels  number is 2. offset p reds (List[Tensor]): offset predicts for all levels, the  channels number is 2. \n\nforward single ( feat ) Forward feature of a single level. \nParameters  feat  ( Tensor ) – Feature of a single level. \nReturns \ncenter predict heatmaps, the  channels number is num classes. wh_pred (Tensor): wh predicts, the channels number is 2. offset p red (Tensor): offset pre- dicts, the channels number is 2. \n\nget_bboxes ( center heat map p reds ,  wh_preds ,  offset p reds ,  img_metas ,  rescale  $=$  True ,  with_nms=False ) Transform network output for a batch into bbox predictions. \nParameters \n•  center heat map p reds  ( list[Tensor] ) – Center predict heatmaps for all levels with shape (B, num classes, H, W). •  wh_preds  ( list[Tensor] ) – WH predicts for all levels with shape (B, 2, H, W). •  offset p reds  ( list[Tensor] ) – Offset predicts for all levels with shape (B, 2, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: True. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: False. \nReturns \nEach item in result list is 2-tuple.  The first item is an (n, 5) tensor, where 5 represent (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1. The shape of the second tensor in the tuple is (n,), and each element represents the class label of the corresponding box. \nReturn type  list[tuple[Tensor, Tensor]] \nget targets ( gt_bboxes ,  gt_labels ,  feat_shape ,  img_shape ) Compute regression and classification targets in multiple images. \nParameters \n•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box. •  feat_shape  ( list[int] ) – feature map shape with value [B, _, H, W] •  img_shape  ( list[int] ) – image shape in [h, w] format. "}
{"page": 320, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_320.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturns\n\nThe float value is mean avg_factor, the dict has components _ below: - cen-\nter_heatmap_target (Tensor): targets of center heatmap, shape (B, num_classes, H,\nW). - wh_target (Tensor): targets of wh predict, shape (B, 2, H, W). - offset_target\n(Tensor): targets of offset predict, shape (B, 2, H, W). - wh_offset_target_weight (Tensor):\nweights of wh and offset predict, shape (B, 2, H, W).\n\nReturn type tuple[dict,float]\n\ninit_weights()\nInitialize weights of the head.\n\nloss (center_heatmap_preds, wh_preds, offset_preds, gt_bboxes, gt_labels, img_metas,\ngt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n* center_heatmap_preds (list [Tensor]) — center predict heatmaps for all levels with\nshape (B, num_classes, H, W).\n\n¢ wh_preds (list [Tensor ]) — wh predicts for all levels with shape (B, 2, H, W).\n¢ offset_preds (list [Tensor ]) — offset predicts for all levels with shape (B, 2, H, W).\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss. Default: None\n\nReturns\nwhich has components below:\n* loss_center_heatmap (Tensor): loss of center heatmap.\n* loss_wh (Tensor): loss of hw heatmap\n* loss_offset (Tensor): loss of offset heatmap.\nReturn type dict[str, Tensor]\n\nclass mmdet.models.dense_heads.CentripetalHead( “args, centripetal_shift_channels=2,\nguiding_shift_channels=2,\nfeat_adaption_conv_kernel=3,\nloss_guiding_shift={‘beta': 1.0, ‘loss_weight': 0.05,\n‘type’: ‘SmoothL1Loss'}, loss_centripetal_shift={‘beta':\n1.0, ‘loss_weight': 1, 'type': 'SmoothL1Loss'},\ninit_cfg=None, **kwargs)\n\nHead of CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection.\n\nCentripetalHead inherits from CornerHead. It removes the embedding branch and adds guiding shift and cen-\ntripetal shift branches. More details can be found in the paper .\n\nParameters\n\n* num_classes (int) — Number of categories excluding the background category.\n\n39.4. dense_heads 313\n", "vlm_text": "Returns \nThe float value is mean avg_factor, the dict has  components below: - cen- ter heat map target (Tensor): targets of center heatmap, shape (B, num classes, H, W). - wh_target (Tensor): targets of wh predict, shape (B, 2, H, W). - offset target (Tensor): targets of offset predict, shape (B, 2, H, W). - wh offset target weight (Tensor): weights of wh and offset predict, shape (B, 2, H, W). \nReturn type  tuple[dict,float] \nin it weights()Initialize weights of the head. \nloss ( center heat map p reds ,  wh_preds ,  offset p reds ,  gt_bboxes ,  gt_labels ,  img_metas , gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None ) Compute losses of the head. \nParameters \n•  center heat map p reds  ( list[Tensor] ) – center predict heatmaps for all levels with shape (B, num classes, H, W). •  wh_preds  ( list[Tensor] ) – wh predicts for all levels with shape (B, 2, H, W). •  offset p reds  ( list[Tensor] ) – offset predicts for all levels with shape (B, 2, H, W). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None \nReturns \nwhich has components below: \n• loss center heat map (Tensor): loss of center heatmap. • loss_wh (Tensor): loss of hw heatmap • loss offset (Tensor): loss of offset heatmap. \nReturn type  dict[str, Tensor] \nclass  mmdet.models.dense heads. Centripetal Head ( \\*args ,  centripetal shift channels=2 , \nguiding shift channels  $_{:=2}$  , feat adaption con v kern e  $l{=}3$  , loss guiding shift={'beta': 1.0, 'loss weight': 0.05,'type': 'Smooth L 1 Loss'} ,  loss centripetal shift={'beta': 1.0, 'loss weight': 1, 'type': 'Smooth L 1 Loss'} , init_cfg  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) \nHead of Centripetal Net: Pursuing High-quality Keypoint Pairs for Object Detection. \nCentripetal Head inherits from  CornerHead . It removes the embedding branch and adds guiding shift and cen- tripetal shift branches. More details can be found in the  paper  . \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. "}
{"page": 321, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_321.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ in_channels (int) — Number of channels in the input feature map.\n\n* num_feat_levels (int) — Levels of feature from the previous module. 2 for HourglassNet-\n104 and 1 for HourglassNet-52. HourglassNet- 104 outputs the final feature and intermediate\nsupervision feature and HourglassNet-52 only outputs the final feature. Default: 2.\n\n* corner_emb_channels (int) — Channel of embedding vector. Default: 1.\n\n* train_cfg (dict | None) — Training config. Useless in CornerHead, but we keep this\nvariable for SingleStageDetector. Default: None.\n\n* test_cfg (dict | None) — Testing config of CornerHead. Default: None.\n\n* loss_heatmap (dict | None) - Config of corner heatmap loss. Default: GaussianFocal-\nLoss.\n\n* loss_embedding (dict | None) — Config of corner embedding loss. Default: Associa-\ntiveEmbeddingLoss.\n\n* loss_offset (dict | None) — Config of corner offset loss. Default: SmoothL1Loss.\n* loss_guiding_shift (dict) — Config of guiding shift loss. Default: SmoothL1Loss.\n\n* loss_centripetal_shift (dict) — Config of centripetal shift loss. Default:\nSmoothL 1Loss.\n\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nforward_single (x, /vl_ind)\nForward feature of a single level.\n\nParameters\n\n¢ x (Tensor) — Feature of a single level.\n\n¢ 1lvl_ind (int) — Level index of current feature.\nReturns\n\nA tuple of CentripetalHead’s output for current feature level. Containing the following Ten-\nsors:\n\n¢ tl_heat (Tensor): Predicted top-left corner heatmap.\n* br_heat (Tensor): Predicted bottom-right corner heatmap.\n¢ tl_off (Tensor): Predicted top-left offset heatmap.\n\n* br_off (Tensor): Predicted bottom-right offset heatmap.\n\na\n\n| guiding shift (Tensor): Predicted top-left guiding shift heatmap.\n* br_guiding shift (Tensor): Predicted bottom-right guiding shift heatmap.\n\n¢ tl_centripetal_shift (Tensor): Predicted top-left centripetal shift heatmap.\n* br_centripetal_shift (Tensor): Predicted bottom-right centripetal shift heatmap.\nReturn type tuple[Tensor]\n\nget_bboxes (¢l_heats, br_heats, tl_offs, br_offs, tl_guiding_shifts, br_guiding_shifts, tl_centripetal_shifts,\nbr_centripetal_shifts, img_metas, rescale=False, with_nms=True)\nTransform network output for a batch into bbox predictions.\n\nParameters\n\n¢ tl_heats (list [Tensor]) — Top-left corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n314 Chapter 39. mmdet.models\n", "vlm_text": "•  in channels  ( int ) – Number of channels in the input feature map. •  num feat levels  ( int ) – Levels of feature from the previous module. 2 for Hourglass Net- 104 and 1 for Hourglass Net-52. Hourglass Net-104 outputs the final feature and intermediate supervision feature and Hourglass Net-52 only outputs the final feature. Default: 2. •  corner em b channels  ( int ) – Channel of embedding vector. Default: 1. •  train_cfg  ( dict | None ) – Training config. Useless in CornerHead, but we keep this variable for Single Stage Detector. Default: None. •  test_cfg  ( dict | None ) – Testing config of CornerHead. Default: None. •  loss heat map  ( dict | None ) – Config of corner heatmap loss. Default: Gaussian Focal- Loss. •  loss embedding  ( dict | None ) – Config of corner embedding loss. Default: Associa- ti ve Embedding Loss. •  loss offset  ( dict | None ) – Config of corner offset loss. Default: Smooth L 1 Loss. •  loss guiding shift  ( dict ) – Config of guiding shift loss. Default: Smooth L 1 Loss. •  loss centripetal shift  ( dict ) – Config of centripetal shift loss. Default: Smooth L 1 Loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward single  $(x,l\\nu l\\_i n d)$  Forward feature of a single level. \nParameters \n•  x  ( Tensor ) – Feature of a single level. •  lvl_ind  ( int ) – Level index of current feature. \nReturns \nsors: • tl_heat (Tensor): Predicted top-left corner heatmap. • br_heat (Tensor): Predicted bottom-right corner heatmap. • tl_off (Tensor): Predicted top-left offset heatmap. • br_off (Tensor): Predicted bottom-right offset heatmap. • tl guiding shift (Tensor): Predicted top-left guiding shift heatmap. • br guiding shift (Tensor): Predicted bottom-right guiding shift heatmap. • tl centripetal shift (Tensor): Predicted top-left centripetal shift heatmap. • br centripetal shift (Tensor): Predicted bottom-right centripetal shift heatmap. \nReturn type  tuple[Tensor] \nget_bboxes ( tl_heats ,  br_heats ,  tl_offs ,  br_offs ,  tl guiding shifts ,  br guiding shifts ,  tl centripetal shifts , br centripetal shifts ,  img_metas ,  rescale  $\\mathbf{\\varepsilon}=$  False ,  with_nms  $\\mathbf{\\hat{=}}$  True ) Transform network output for a batch into bbox predictions. \nParameters \n•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). "}
{"page": 322, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_322.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ br_heats (list [Tensor ]) — Bottom-right corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ tl_offs (list [Tensor]) — Top-left corner offsets for each level with shape (N, cor-\nner_offset_channels, H, W).\n\n¢ br_offs (list [Tensor]) — Bottom-right corner offsets for each level with shape (N,\ncorner_offset_channels, H, W).\n\n¢ tl_guiding_shifts (list [Tensor])—Top-left guiding shifts for each level with shape\n(N, guiding_shift_channels, H, W). Useless in this function, we keep this arg because it’s\nthe raw output from CentripetalHead.\n\n¢ br_guiding_shifts (list [Tensor]) — Bottom-right guiding shifts for each level with\nshape (N, guiding_shift_channels, H, W). Useless in this function, we keep this arg because\nit’s the raw output from CentripetalHead.\n\n¢ tl_centripetal_shifts (list [Tensor]) — Top-left centripetal shifts for each level\nwith shape (N, centripetal_shift_channels, H, W).\n\n¢ br_centripetal_shifts (list [Tensor]) — Bottom-right centripetal shifts for each\nlevel with shape (N, centripetal_shift_channels, H, W).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n¢ rescale (bool) — If True, return boxes in original image space. Default: False.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default: True.\n\ninit_weights()\nInitialize the weights.\n\nloss (tl_heats, br_heats, tl_offs, br_offs, tl_guiding_shifts, br_guiding_shifts, tl_centripetal_shifts,\nbr_centripetal_shifts, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ tl_heats (list [Tensor]) — Top-left corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ br_heats (list [Tensor ]) — Bottom-right corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ tl_offs (list [Tensor]) — Top-left corner offsets for each level with shape (N, cor-\nner_offset_channels, H, W).\n\n¢ br_offs (list [Tensor]) — Bottom-right corner offsets for each level with shape (N,\ncorner_offset_channels, H, W).\n\n¢ tl_guiding_shifts (list [Tensor])—Top-left guiding shifts for each level with shape\n(N, guiding_shift_channels, H, W).\n\n¢ br_guiding_shifts (list [Tensor]) — Bottom-right guiding shifts for each level with\nshape (N, guiding_shift_channels, H, W).\n\n¢ tl_centripetal_shifts (list [Tensor]) — Top-left centripetal shifts for each level\nwith shape (N, centripetal_shift_channels, H, W).\n\n¢ br_centripetal_shifts (list [Tensor]) — Bottom-right centripetal shifts for each\nlevel with shape (N, centripetal_shift_channels, H, W).\n\n39.4. dense_heads 315\n", "vlm_text": "•  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  tl guiding shifts  ( list[Tensor] ) – Top-left guiding shifts for each level with shape (N, guiding shift channels, H, W). Useless in this function, we keep this arg because it’s the raw output from Centripetal Head. •  br guiding shifts  ( list[Tensor] ) – Bottom-right guiding shifts for each level with shape (N, guiding shift channels, H, W). Useless in this function, we keep this arg because it’s the raw output from Centripetal Head. •  tl centripetal shifts  ( list[Tensor] ) – Top-left centripetal shifts for each level with shape (N, centripetal shift channels, H, W). •  br centripetal shifts  ( list[Tensor] ) – Bottom-right centripetal shifts for each level with shape (N, centripetal shift channels, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. \nin it weights()Initialize the weights. \nloss ( tl_heats ,  br_heats ,  tl_offs ,  br_offs ,  tl guiding shifts ,  br guiding shifts ,  tl centripetal shifts , br centripetal shifts ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{=}$  None ) Compute losses of the head. \nParameters \n•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  tl guiding shifts  ( list[Tensor] ) – Top-left guiding shifts for each level with shape (N, guiding shift channels, H, W). •  br guiding shifts  ( list[Tensor] ) – Bottom-right guiding shifts for each level with shape (N, guiding shift channels, H, W). •  tl centripetal shifts  ( list[Tensor] ) – Top-left centripetal shifts for each level with shape (N, centripetal shift channels, H, W). •  br centripetal shifts  ( list[Tensor] ) – Bottom-right centripetal shifts for each level with shape (N, centripetal shift channels, H, W). "}
{"page": 323, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_323.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [left, top, right, bottom] format.\n\n¢ gt_labels (list [Tensor ]) — Class indices corresponding to each box.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (list[Tensor] | None) — Specify which bounding boxes can be\nignored when computing the loss.\n\nReturns\n\nA dictionary of loss components. Containing the following losses:\n\n¢ det_loss (list{Tensor]): Corner keypoint losses of all feature levels.\n\n¢ off_loss (list[Tensor]): Corner offset losses of all feature levels.\n\n* guiding_loss (list[Tensor]): Guiding shift losses of all feature levels.\n\n* centripetal_loss (list[Tensor]): Centripetal shift losses of all feature levels.\nReturn type dict[str, Tensor]\n\nloss_single(tl_hmp, br_hmp, tl_off, br_off, tl_guiding_shift, br_guiding_shift, tl_centripetal_shift,\nbr_centripetal_shift, targets)\nCompute losses for single level.\n\nParameters\n\n¢ tl_hmp (Tensor) — Top-left corner heatmap for current level with shape (N, num_classes,\nH, W).\n\n¢ br_hmp (Tensor) - Bottom-right corner heatmap for current level with shape (N,\nnum_classes, H, W).\n\n¢ tl_off (Tensor) — Top-left corner offset for current level with shape (N, cor-\nner_offset_channels, H, W).\n\n¢ br_off (Tensor) — Bottom-right corner offset for current level with shape (N, cor-\nner_offset_channels, H, W).\n\n¢ tl_guiding_shift (Tensor) — Top-left guiding shift for current level with shape (N,\nguiding _shift_channels, H, W).\n\n¢ br_guiding_shift (Tensor) — Bottom-right guiding shift for current level with shape\n(N, guiding_shift_channels, H, W).\n\n¢ tl_centripetal_shift (Tensor) — Top-left centripetal shift for current level with shape\n(N, centripetal_shift_channels, H, W).\n\n¢ br_centripetal_shift (Tensor) — Bottom-right centripetal shift for current level with\nshape (N, centripetal_shift_channels, H, W).\n\n* targets (dict) — Corner target generated by get_targets.\nReturns\nLosses of the head’s different branches containing the following losses:\n¢ det_loss (Tensor): Corner keypoint loss.\n¢ off_loss (Tensor): Corner offset loss.\n\n¢ guiding_loss (Tensor): Guiding shift loss.\n\n316 Chapter 39. mmdet.models\n", "vlm_text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [left, top, right, bottom] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – Specify which bounding boxes can be ignored when computing the loss. \nReturns \nA dictionary of loss components. Containing the following losses: • det_loss (list[Tensor]): Corner keypoint losses of all feature levels. • off_loss (list[Tensor]): Corner offset losses of all feature levels. • guiding loss (list[Tensor]): Guiding shift losses of all feature levels. • centripetal loss (list[Tensor]): Centripetal shift losses of all feature levels. \nReturn type  dict[str, Tensor] \nloss single ( tl_hmp ,  br_hmp ,  tl_off ,  br_off ,  tl guiding shift ,  br guiding shift ,  tl centripetal shift , br centripetal shift ,  targets ) \nCompute losses for single level. \nParameters \n•  tl_hmp  ( Tensor ) – Top-left corner heatmap for current level with shape (N, num classes, H, W). •  br_hmp  ( Tensor ) – Bottom-right corner heatmap for current level with shape (N, num classes, H, W). •  tl_off  ( Tensor ) – Top-left corner offset for current level with shape (N, cor- ner offset channels, H, W). •  br_off  ( Tensor ) – Bottom-right corner offset for current level with shape (N, cor- ner offset channels, H, W). •  tl guiding shift  ( Tensor ) – Top-left guiding shift for current level with shape (N, guiding shift channels, H, W). •  br guiding shift  ( Tensor ) – Bottom-right guiding shift for current level with shape (N, guiding shift channels, H, W). •  tl centripetal shift  ( Tensor ) – Top-left centripetal shift for current level with shape (N, centripetal shift channels, H, W). •  br centripetal shift  ( Tensor ) – Bottom-right centripetal shift for current level with shape (N, centripetal shift channels, H, W). •  targets  ( dict ) – Corner target generated by  get targets . \nReturns \nLosses of the head’s different branches containing the following losses: • det_loss (Tensor): Corner keypoint loss. • off_loss (Tensor): Corner offset loss. • guiding loss (Tensor): Guiding shift loss. "}
{"page": 324, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_324.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* centripetal_loss (Tensor): Centripetal shift loss.\nReturn type tuple[torch.Tensor]\n\nclass mmdet.models.dense_heads.CornerHead (num_classes, in_channels, num_feat_levels=2,\ncorner_emb_channels=1, train_cfg=None, test_cfg=None,\nloss_heatmap={‘alpha': 2.0, 'gamma': 4.0, ‘loss_weight': 1,\n‘type’: 'GaussianFocalLoss'}, loss_embedding={ ‘pull_weight':\n0.25, 'push_weight': 0.25, 'type':\n‘AssociativeEmbeddingLoss'}, loss_offset={‘beta': 1.0,\n‘loss_weight': 1, 'type': 'SmoothL1Loss'}, init_cfg=None)\n\nHead of CornerNet: Detecting Objects as Paired Keypoints.\n\nCode is modified from the official github repo .\nMore details can be found in the paper .\nParameters\n* num_classes (int) — Number of categories excluding the background category.\n¢ in_channels (int) — Number of channels in the input feature map.\n\n* num_feat_levels (int) — Levels of feature from the previous module. 2 for HourglassNet-\n104 and | for HourglassNet-52. Because HourglassNet-104 outputs the final feature and\nintermediate supervision feature and HourglassNet-52 only outputs the final feature. Default:\n2.\n\n* corner_emb_channels (int) — Channel of embedding vector. Default: 1.\n\n* train_cfg (dict | None) — Training config. Useless in CornerHead, but we keep this\nvariable for SingleStageDetector. Default: None.\n\n* test_cfg (dict | None) — Testing config of CornerHead. Default: None.\n\n* loss_heatmap (dict | None) - Config of corner heatmap loss. Default: GaussianFocal-\nLoss.\n\n* loss_embedding (dict | None) — Config of corner embedding loss. Default: Associa-\ntiveEmbeddingLoss.\n\n* loss_offset (dict | None) — Config of corner offset loss. Default: SmoothL1Loss.\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\ndecode_heatmap (1l_heat, br_heat, tl_off, br_off, t_emb=None, br_emb=None, tl_centripetal_shift=None,\nbr_centripetal_shift=None, img_meta=None, k=100, kernel=3, distance_threshold=0.5,\nnum_dets=1000)\nTransform outputs for a single batch item into raw bbox predictions.\n\nParameters\n\n¢ tl_heat (Tensor) — Top-left corner heatmap for current level with shape (N, num_classes,\nH, W).\n\n¢ br_heat (Tensor) — Bottom-right corner heatmap for current level with shape (N,\nnum_classes, H, W).\n\n¢ tl_off (Tensor) — Top-left corner offset for current level with shape (N, cor-\nner_offset_channels, H, W).\n\n¢ br_off (Tensor) — Bottom-right corner offset for current level with shape (N, cor-\nner_offset_channels, H, W).\n\n39.4. dense_heads 317\n", "vlm_text": "• centripetal loss (Tensor): Centripetal shift loss. \nReturn type  tuple[torch.Tensor] \nclass  mmdet.models.dense heads. CornerHead ( num classes ,  in channels ,  num feat levels=2 , corner em b channel  $\\mathord{\\breve{=}}I$  ,  train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None , loss heat map $\\leftrightharpoons$ {'alpha': 2.0, 'gamma': 4.0, 'loss_weight': 1,'type': 'Gaussian Focal Loss'} ,  loss embedding={'pull weight': 0.25, 'push_weight': 0.25, 'type': 'Associative Embedding Loss'} ,  loss offset={'beta': 1.0, 'loss weight': 1, 'type': 'Smooth L 1 Loss'} ,  init_cfg=None ) \nHead of CornerNet: Detecting Objects as Paired Keypoints. \nCode is modified from the  official github repo  . \nMore details can be found in the  paper  . \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  num feat levels  ( int ) – Levels of feature from the previous module. 2 for Hourglass Net- 104 and 1 for Hourglass Net-52. Because Hourglass Net-104 outputs the final feature and intermediate supervision feature and Hourglass Net-52 only outputs the final feature. Default: 2. •  corner em b channels  ( int ) – Channel of embedding vector. Default: 1. •  train_cfg  ( dict | None ) – Training config. Useless in CornerHead, but we keep this variable for Single Stage Detector. Default: None. •  test_cfg  ( dict | None ) – Testing config of CornerHead. Default: None. •  loss heat map  ( dict | None ) – Config of corner heatmap loss. Default: Gaussian Focal- Loss. •  loss embedding  ( dict | None ) – Config of corner embedding loss. Default: Associa- ti ve Embedding Loss. •  loss offset  ( dict | None ) – Config of corner offset loss. Default: Smooth L 1 Loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \ndecode heat map ( tl_heat ,  br_heat ,  tl_off ,  br_off ,  tl_emb  $=$  None ,  br_emb  $\\leftrightharpoons$  None ,  tl centripetal shift  $\\mathbf{\\dot{\\rho}}=$  None , br centripetal sh if  $\\mathbf{\\dot{=}}$  None ,  img_meta  $\\mathbf{\\beta}=$  None ,  $k{=}I O O$  ,  kernel  $\\scriptstyle{I=3}$  ,  distance threshold  $\\mathit{I}\\!\\!=\\!\\!0.5$  , num_dets=1000 ) \nTransform outputs for a single batch item into raw bbox predictions. \nParameters \n•  tl_heat  ( Tensor ) – Top-left corner heatmap for current level with shape (N, num classes, H, W). •  br_heat  ( Tensor ) – Bottom-right corner heatmap for current level with shape (N, num classes, H, W). •  tl_off  ( Tensor ) – Top-left corner offset for current level with shape (N, cor- ner offset channels, H, W). •  br_off  ( Tensor ) – Bottom-right corner offset for current level with shape (N, cor- ner offset channels, H, W). "}
{"page": 325, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_325.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ tl_emb (Tensor | None) - Top-left corner embedding for current level with shape (N,\ncorner_emb_channels, H, W).\n\n¢ br_emb (Tensor | None) — Bottom-right corner embedding for current level with shape\n(N, corner_emb_channels, H, W).\n\n¢ tl_centripetal_shift (Tensor | None) — Top-left centripetal shift for current level\nwith shape (N, 2, H, W).\n\n¢ br_centripetal_shift (Tensor | None) — Bottom-right centripetal shift for current\nlevel with shape (N, 2, H, W).\n\nimg_meta (dict) — Meta information of current image, e.g., image size, scaling factor,\netc.\n\n¢ k (int) - Get top k corner keypoints from heatmap.\n¢ kernel (int) — Max pooling kernel for extract local maximum pixels.\n\n¢ distance_threshold (float) — Distance threshold. Top-left and bottom-right corner\nkeypoints with feature distance less than the threshold will be regarded as keypoints from\nsame object.\n\n* num_dets (int) — Num of raw boxes before doing nms.\nReturns\nDecoded output of CornerHead, containing the following Tensors:\n* bboxes (Tensor): Coords of each box.\n¢ scores (Tensor): Scores of each box.\n* clses (Tensor): Categories of each box.\nReturn type tuple[torch.Tensor]\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\nUsually a tuple of corner heatmaps, offset heatmaps and embedding heatmaps.\n\n¢ tl_heats (list[Tensor]): Top-left corner heatmaps for all levels, each is a 4D-tensor, the\nchannels number is num_classes.\n\n* br_heats (list{Tensor]): Bottom-right corner heatmaps for all levels, each is a 4D-tensor,\nthe channels number is num_classes.\n\n¢ tl_embs (list[Tensor] | list{None]): Top-left embedding heatmaps for all levels, each is a\n4D-tensor or None. If not None, the channels number is corner_emb_channels.\n\n* br_embs (list[Tensor] | list{ None]): Bottom-right embedding heatmaps for all levels, each\nis a 4D-tensor or None. If not None, the channels number is corner_emb_channels.\n\n¢ tl_offs (list{Tensor]): Top-left offset heatmaps for all levels, each is a 4D-tensor. The chan-\nnels number is corner_offset_channels.\n\n* br_offs (list[Tensor]): Bottom-right offset heatmaps for all levels, each is a 4D-tensor. The\nchannels number is corner_offset_channels.\n\nReturn type tuple\n\n318 Chapter 39. mmdet.models\n", "vlm_text": "•  tl_emb  ( Tensor | None ) – Top-left corner embedding for current level with shape (N, corner em b channels, H, W). •  br_emb  ( Tensor | None ) – Bottom-right corner embedding for current level with shape (N, corner em b channels, H, W). •  tl centripetal shift  ( Tensor | None ) – Top-left centripetal shift for current level with shape (N, 2, H, W). •  br centripetal shift  ( Tensor | None ) – Bottom-right centripetal shift for current level with shape (N, 2, H, W). •  img_meta  ( dict ) – Meta information of current image, e.g., image size, scaling factor, etc. •  k  ( int ) – Get top k corner keypoints from heatmap. •  kernel  ( int ) – Max pooling kernel for extract local maximum pixels. •  distance threshold  ( float ) – Distance threshold. Top-left and bottom-right corner keypoints with feature distance less than the threshold will be regarded as keypoints from same object. •  num_dets  ( int ) – Num of raw boxes before doing nms. \nReturns \nDecoded output of CornerHead, containing the following Tensors: \n• bboxes (Tensor): Coords of each box. • scores (Tensor): Scores of each box. • clses (Tensor): Categories of each box. \nReturn type  tuple[torch.Tensor] forward ( feats ) \nForward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nUsually a tuple of corner heatmaps, offset heatmaps and embedding heatmaps. \n• tl_heats (list[Tensor]): Top-left corner heatmaps for all levels, each is a 4D-tensor, the channels number is num classes. • br_heats (list[Tensor]): Bottom-right corner heatmaps for all levels, each is a 4D-tensor, the channels number is num classes. • tl_embs (list[Tensor] | list[None]): Top-left embedding heatmaps for all levels, each is a 4D-tensor or None. If not None, the channels number is corner em b channels. • br_embs (list[Tensor] | list[None]): Bottom-right embedding heatmaps for all levels, each is a 4D-tensor or None. If not None, the channels number is corner em b channels. • tl_offs (list[Tensor]): Top-left offset heatmaps for all levels, each is a 4D-tensor. The chan- nels number is corner offset channels. • br_offs (list[Tensor]): Bottom-right offset heatmaps for all levels, each is a 4D-tensor. The channels number is corner offset channels. \nReturn type  tuple "}
{"page": 326, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_326.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward_single (x, /vl_ind, return_pool=False)\nForward feature of a single level.\n\nParameters\n¢ x (Tensor) — Feature of a single level.\n¢ 1lvl_ind (int) — Level index of current feature.\n* return_pool (bool) — Return corner pool feature or not.\nReturns\nA tuple of CornerHead’s output for current feature level. Containing the following Tensors:\n¢ tl_heat (Tensor): Predicted top-left corner heatmap.\n* br_heat (Tensor): Predicted bottom-right corner heatmap.\n\n¢ tLemb (Tensor | None): Predicted top-left embedding heatmap. None for\nself. with_corner_emb == False.\n\n* br_emb (Tensor | None): Predicted bottom-right embedding heatmap. None for\nself. with_corner_emb == False.\n\n¢ tl_off (Tensor): Predicted top-left offset heatmap.\n\n* br_off (Tensor): Predicted bottom-right offset heatmap.\n\n* tl_pool (Tensor): Top-left corner pool feature. Not must have.\n\n* br_pool (Tensor): Bottom-right corner pool feature. Not must have.\nReturn type tuple[Tensor]\n\nget_bboxes (¢l_heats, br_heats, tl_embs, br_embs, tl_offs, br_offs, img_metas, rescale=False,\nwith_nms=True)\nTransform network output for a batch into bbox predictions.\n\nParameters\n\n¢ tl_heats (list [Tensor]) — Top-left corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ br_heats (list [Tensor ]) — Bottom-right corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\ntl_embs (list [Tensor]) — Top-left corner embeddings for each level with shape (N,\ncorner_emb_channels, H, W).\n\n¢ br_embs (list [Tensor]) — Bottom-right corner embeddings for each level with shape\n(N, corner_emb_channels, H, W).\n\ntl_offs (list [Tensor]) — Top-left corner offsets for each level with shape (N, cor-\nner_offset_channels, H, W).\n\nbr_offs (list [Tensor]) — Bottom-right corner offsets for each level with shape (N,\ncorner_offset_channels, H, W).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\nrescale (bool) — If True, return boxes in original image space. Default: False.\n\n¢ with_nms (boo1) — If True, do nms before return boxes. Default: True.\n\n39.4. dense_heads 319\n", "vlm_text": "forward single ( x ,  lvl_ind ,  return pool  $\\mathbf{\\{}=}$  False ) Forward feature of a single level. \nParameters \n•  x  ( Tensor ) – Feature of a single level. •  lvl_ind  ( int ) – Level index of current feature. •  return pool  ( bool ) – Return corner pool feature or not. \nReturns \nA tuple of CornerHead’s output for current feature level. Containing the following Tensors: • tl_heat (Tensor): Predicted top-left corner heatmap. • br_heat (Tensor): Predicted bottom-right corner heatmap. • tl_emb (Tensor | None): Predicted top-left embedding heatmap. None for self.with corner em b  $\\cdot==F a l s e$  . • br_emb (Tensor | None): Predicted bottom-right embedding heatmap. None for self.with corner em b  $\\cdot==F a l s e$  . • tl_off (Tensor): Predicted top-left offset heatmap. • br_off (Tensor): Predicted bottom-right offset heatmap. • tl_pool (Tensor): Top-left corner pool feature. Not must have. • br_pool (Tensor): Bottom-right corner pool feature. Not must have. \nReturn type  tuple[Tensor] \nget_bboxes ( tl_heats ,  br_heats ,  tl_embs ,  br_embs ,  tl_offs ,  br_offs ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False , with_nms  $\\mathbf{=}$  True ) Transform network output for a batch into bbox predictions. \nParameters \n•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_embs  ( list[Tensor] ) – Top-left corner embeddings for each level with shape (N, corner em b channels, H, W). •  br_embs  ( list[Tensor] ) – Bottom-right corner embeddings for each level with shape (N, corner em b channels, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. "}
{"page": 327, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_327.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nget_targets(gt_bboxes, gt_labels, feat_shape, img_shape, with_corner_emb=False,\nwith_guiding_shift=False, with_centripetal_shift=False)\nGenerate corner targets.\n\nIncluding corner heatmap, corner offset.\nOptional: corner embedding, corner guiding shift, centripetal shift.\nFor CornerNet, we generate corner heatmap, corner offset and corner embedding from this function.\n\nFor CentripetalNet, we generate corner heatmap, corner offset, guiding shift and centripetal shift from this\n\nfunction.\nParameters\n* gt_bboxes (list[Tensor]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\n¢ gt_labels (list [Tensor]) — Ground truth labels of each box, each has shape (num_gt,).\n¢ feat_shape (list [int]) — Shape of output feature, [batch, channel, height, width].\n¢ img_shape (list [int ]) — Shape of input image, [height, width, channel].\n¢ with_corner_emb (bool) — Generate corner embedding target or not. Default: False.\n¢ with_guiding_shift (bool) — Generate guiding shift target or not. Default: False.\n¢ with_centripetal_shift (bool) — Generate centripetal shift target or not. Default:\nFalse.\nReturns\n\nGround truth of corner heatmap, corner offset, corner embedding, guiding shift and cen-\ntripetal shift. Containing the following keys:\n\n* topleft_heatmap (Tensor): Ground truth top-left corner heatmap.\n\n* bottomright_heatmap (Tensor): Ground truth bottom-right corner heatmap.\n\n* topleft_offset (Tensor): Ground truth top-left corner offset.\n\n* bottomright_offset (Tensor): Ground truth bottom-right corner offset.\n\n* corner_embedding (list[list[list[int]]]): Ground truth corner embedding. Not must have.\n\n* topleft_guiding shift (Tensor): Ground truth top-left corner guiding shift. Not must have.\n\n* bottomright_guiding_shift (Tensor): Ground truth bottom-right corner guiding shift. Not\nmust have.\n\n* topleft_centripetal_shift (Tensor): Ground truth top-left corner centripetal shift. Not must\nhave.\n\n* bottomright_centripetal_shift (Tensor): Ground truth bottom-right corner centripetal shift.\nNot must have.\n\nReturn type dict\n\ninit_weights()\nInitialize the weights.\n\nloss (tl_heats, br_heats, tl_embs, br_embs, tl_offs, br_offs, gt_bboxes, gt_labels, img_metas,\ngt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n320 Chapter 39. mmdet.models\n", "vlm_text": "get targets ( gt_bboxes ,  gt_labels ,  feat_shape ,  img_shape ,  with corner em b  $=$  False , with guiding sh if  $\\leftrightharpoons$  False ,  with centripetal shift  $\\leftrightharpoons$  False ) Generate corner targets. \nIncluding corner heatmap, corner offset. Optional: corner embedding, corner guiding shift, centripetal shift. For CornerNet, we generate corner heatmap, corner offset and corner embedding from this function. \nFor Centripetal Net, we generate corner heatmap, corner offset, guiding shift and centripetal shift from this function. \nParameters \n•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). •  feat_shape  ( list[int] ) – Shape of output feature, [batch, channel, height, width]. •  img_shape  ( list[int] ) – Shape of input image, [height, width, channel]. •  with corner em b  ( bool ) – Generate corner embedding target or not. Default: False. •  with guiding shift  ( bool ) – Generate guiding shift target or not. Default: False. •  with centripetal shift  ( bool ) – Generate centripetal shift target or not. Default: False. \nReturns \nGround truth of corner heatmap, corner offset, corner embedding, guiding shift and cen- tripetal shift. Containing the following keys: \n• top left heat map (Tensor): Ground truth top-left corner heatmap. • bottom right heat map (Tensor): Ground truth bottom-right corner heatmap. • top left offset (Tensor): Ground truth top-left corner offset. • bottom right offset (Tensor): Ground truth bottom-right corner offset. • corner embedding (list[list[list[int]]]): Ground truth corner embedding. Not must have. • top left guiding shift (Tensor): Ground truth top-left corner guiding shift. Not must have. • bottom right guiding shift (Tensor): Ground truth bottom-right corner guiding shift. Not must have. • top left centripetal shift (Tensor): Ground truth top-left corner centripetal shift. Not must have. • bottom right centripetal shift (Tensor): Ground truth bottom-right corner centripetal shift. Not must have. \nReturn type  dict \nin it weights()\nInitialize the weights. \nloss ( tl_heats ,  br_heats ,  tl_embs ,  br_embs ,  tl_offs ,  br_offs ,  gt_bboxes ,  gt_labels ,  img_metas , gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None ) Compute losses of the head. \nParameters "}
{"page": 328, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_328.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ tl_heats (list [Tensor]) — Top-left corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ br_heats (list [Tensor ]) — Bottom-right corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ tl_embs (list [Tensor]) — Top-left corner embeddings for each level with shape (N,\ncorner_emb_channels, H, W).\n\n¢ br_embs (list [Tensor]) — Bottom-right corner embeddings for each level with shape\n(N, corner_emb_channels, H, W).\n\n¢ tl_offs (list [Tensor]) — Top-left corner offsets for each level with shape (N, cor-\nner_offset_channels, H, W).\n\nbr_offs (list [Tensor]) — Bottom-right corner offsets for each level with shape (N,\ncorner_offset_channels, H, W).\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [left, top, right, bottom] format.\n\ngt_labels (list [Tensor ]) — Class indices corresponding to each box.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (list[Tensor] | None) — Specify which bounding boxes can be\nignored when computing the loss.\n\nReturns\nA dictionary of loss components. Containing the following losses:\n¢ det_loss (list{Tensor]): Corner keypoint losses of all feature levels.\n* pull_loss (list[Tensor]): Part one of AssociativeEmbedding losses of all feature levels.\n¢ push_loss (list{Tensor]): Part two of AssociativeEmbedding losses of all feature levels.\n¢ off_loss (list[Tensor]): Corner offset losses of all feature levels.\n\nReturn type dict[str, Tensor]\n\nloss_single(tl_hmp, br_hmp, tl_emb, br_emb, tl_off, br_off, targets)\nCompute losses for single level.\n\nParameters\n\n¢ tl_hmp (Tensor) — Top-left corner heatmap for current level with shape (N, num_classes,\nH, W).\n\n¢ br_hmp (Tensor) - Bottom-right corner heatmap for current level with shape (N,\nnum_classes, H, W).\n\n¢ tl_emb (Tensor) — Top-left corner embedding for current level with shape (N, cor-\nner_emb_channels, H, W).\n\n¢ br_emb (Tensor) — Bottom-right corner embedding for current level with shape (N, cor-\nner_emb_channels, H, W).\n\n¢ tl_off (Tensor) — Top-left corner offset for current level with shape (N, cor-\nner_offset_channels, H, W).\n\n¢ br_off (Tensor) — Bottom-right corner offset for current level with shape (N, cor-\nner_offset_channels, H, W).\n\n39.4. dense_heads 321\n", "vlm_text": "•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_embs  ( list[Tensor] ) – Top-left corner embeddings for each level with shape (N, corner em b channels, H, W). •  br_embs  ( list[Tensor] ) – Bottom-right corner embeddings for each level with shape (N, corner em b channels, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [left, top, right, bottom] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – Specify which bounding boxes can be ignored when computing the loss. \nReturns \nA dictionary of loss components. Containing the following losses: \n• det_loss (list[Tensor]): Corner keypoint losses of all feature levels. • pull_loss (list[Tensor]): Part one of Associative Embedding losses of all feature levels. • push_loss (list[Tensor]): Part two of Associative Embedding losses of all feature levels. • off_loss (list[Tensor]): Corner offset losses of all feature levels. \nReturn type  dict[str, Tensor] \nloss single ( tl_hmp ,  br_hmp ,  tl_emb ,  br_emb ,  tl_off ,  br_off ,  targets ) Compute losses for single level. \nParameters \n•  tl_hmp  ( Tensor ) – Top-left corner heatmap for current level with shape (N, num classes, H, W). •  br_hmp  ( Tensor ) – Bottom-right corner heatmap for current level with shape (N, num classes, H, W). •  tl_emb  ( Tensor ) – Top-left corner embedding for current level with shape (N, cor- ner em b channels, H, W). •  br_emb  ( Tensor ) – Bottom-right corner embedding for current level with shape (N, cor- ner em b channels, H, W). •  tl_off  ( Tensor ) – Top-left corner offset for current level with shape (N, cor- ner offset channels, H, W). •  br_off  ( Tensor ) – Bottom-right corner offset for current level with shape (N, cor- ner offset channels, H, W). "}
{"page": 329, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_329.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* targets (dict) — Corner target generated by get_targets.\nReturns\nLosses of the head’s different branches containing the following losses:\n¢ det_loss (Tensor): Corner keypoint loss.\n¢ pull_loss (Tensor): Part one of AssociativeEmbedding loss.\n¢ push_loss (Tensor): Part two of AssociativeEmbedding loss.\n¢ off_loss (Tensor): Corner offset loss.\nReturn type tuple[torch.Tensor]\n\nonnx_export (7/_heats, br_heats, tl_embs, br_embs, tl_offs, br_offs, img_metas, rescale=False,\nwith_nms=True)\nTransform network output for a batch into bbox predictions.\n\nParameters\n\n¢ tl_heats (list [Tensor]) — Top-left corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ br_heats (list [Tensor ]) — Bottom-right corner heatmaps for each level with shape (N,\nnum_classes, H, W).\n\n¢ tl_embs (list [Tensor]) — Top-left corner embeddings for each level with shape (N,\ncorner_emb_channels, H, W).\n\n¢ br_embs (list [Tensor]) — Bottom-right corner embeddings for each level with shape\n(N, corner_emb_channels, H, W).\n\n¢ tl_offs (list [Tensor]) — Top-left corner offsets for each level with shape (N, cor-\nner_offset_channels, H, W).\n\n¢ br_offs (list [Tensor]) — Bottom-right corner offsets for each level with shape (N,\ncorner_offset_channels, H, W).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n¢ rescale (bool) — If True, return boxes in original image space. Default: False.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default: True.\n\nReturns First tensor bboxes with shape [N, num_det, 5], 5 arrange as (x1, yl, x2, y2, score) and\nsecond element is class labels of shape [N, num_det].\n\nReturn type tuple[Tensor, Tensor]\n\n322 Chapter 39. mmdet.models\n", "vlm_text": "•  targets  ( dict ) – Corner target generated by  get targets . \nReturns \nLosses of the head’s different branches containing the following losses: \n• det_loss (Tensor): Corner keypoint loss. • pull_loss (Tensor): Part one of Associative Embedding loss. • push_loss (Tensor): Part two of Associative Embedding loss. • off_loss (Tensor): Corner offset loss. \nReturn type  tuple[torch.Tensor] \non nx export ( tl_heats ,  br_heats ,  tl_embs ,  br_embs ,  tl_offs ,  br_offs ,  img_metas ,  rescale=False , with_nms  $\\mathbf{\\tilde{=}}$  True ) Transform network output for a batch into bbox predictions. \nParameters \n•  tl_heats  ( list[Tensor] ) – Top-left corner heatmaps for each level with shape (N, num classes, H, W). •  br_heats  ( list[Tensor] ) – Bottom-right corner heatmaps for each level with shape (N, num classes, H, W). •  tl_embs  ( list[Tensor] ) – Top-left corner embeddings for each level with shape (N, corner em b channels, H, W). •  br_embs  ( list[Tensor] ) – Bottom-right corner embeddings for each level with shape (N, corner em b channels, H, W). •  tl_offs  ( list[Tensor] ) – Top-left corner offsets for each level with shape (N, cor- ner offset channels, H, W). •  br_offs  ( list[Tensor] ) – Bottom-right corner offsets for each level with shape (N, corner offset channels, H, W). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. \nReturns  First tensor bboxes with shape [N, num_det, 5], 5 arrange as (x1, y1, x2, y2, score) and second element is class labels of shape [N, num_det]. \nReturn type  tuple[Tensor, Tensor] "}
{"page": 330, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_330.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads .DETRHead (num_classes, in_channels, num_query=100, num_reg_fcs=2,\ntransformer=None, sync_cls_avg_factor=False,\npositional_encoding={‘normalize': True, '‘num_feats': 128,\n‘type’: ‘SinePositionalEncoding'}, loss_cls={'bg_cls_weight':\n0.1, 'class_weight': 1.0, ‘loss_weight': 1.0, 'type':\n'CrossEntropyLoss’, 'use_sigmoid': False},\nloss_bbox=({'loss_weight': 5.0, ‘type': 'L1Loss'},\nloss_iou={'loss_weight': 2.0, 'type': 'GloULoss'},\ntrain_cfg={‘assigner': {'cls_cost': {'type': 'ClassificationCost',\n‘weight’: 1.0}, '‘iou_cost': {‘iou_mode': 'giou', 'type': 'IoUCost’,\n‘weight’: 2.0}, 'reg_cost': {'type': 'BBoxL1Cost', 'weight': 5.0},\n‘type’: 'HungarianAssigner'}}, test_cfg={'max_per_img': 100},\ninit_cfg=None, **kwargs)\n\nImplements the DETR transformer head.\n\nSee paper: End-to-End Object Detection with Transformers for details.\nParameters\n* num_classes (int) — Number of categories excluding the background.\n¢ in_channels (int) — Number of channels in the input feature map.\n* num_query (int) — Number of query in Transformer.\n\n* num_reg_fcs (int, optional) — Number of fully-connected layers used in FFN, which\nis then used for the regression head. Default 2.\n\n* (obj (test_cfg) —*mmcv.ConfigDict*|dict): Config for transformer. Default: None.\n\n* sync_cls_avg_factor (bool) — Whether to sync the avg_factor of all ranks. Default to\nFalse.\n\n* (obj — *mmcv.ConfigDict*|dict): Config for position encoding.\n\n* (obj — mmcv.ConfigDict \\dict): Config of the classification loss. Default ‘CrossEntropy-\nLoss.\n\n* (obj — mmcv.ConfigDict \\dict): Config of the regression loss. Default ‘L1Loss.\n\n* (obj — mmcv.ConfigDict \\dict): Config of the regression iou loss. Default ‘“GloULoss.\n\n* (obj —*mmcv.ConfigDict*|dict): Training config of transformer head.\n\n* (obj — “mmcv.ConfigDict*|dict): Testing config of transformer head.\n\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\nforward (feats, img_metas)\nForward function.\n\nParameters\n¢ feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-tensor.\n¢ img_metas (list [dict ]) — List of image information.\n\nReturns\nOutputs for all scale levels.\n\n¢ all_cls_scores_list (list{Tensor]): Classification scores for each scale level. Each is a 4D-\ntensor with shape [nb_dec, bs, num_query, cls_out_channels]. Note cls_out_channels\nshould includes background.\n\n39.4. dense_heads 323\n", "vlm_text": "class  mmdet.models.dense heads. DETRHead ( num classes ,  in channels ,  num_query=100 ,  num reg fcs=2 , transforme  $r{=}$  None ,  sync cls avg factor=False , positional encoding={'normalize': True, 'num_feats': 128, 'type': 'Sine Positional Encoding'} ,  loss_cls={'bg cls weight': 0.1, 'class_weight': 1.0, 'loss_weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': False} , loss_bbox={'loss weight': 5.0, 'type': 'L1Loss'} , loss_iou={'loss weight': 2.0, 'type': 'GIoULoss'} , train_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'assigner': {'cls_cost': {'type': 'Classification Cost', 'weight': 1.0}, 'iou_cost': {'iou_mode': 'giou', 'type': 'IoUCost', 'weight': 2.0}, 'reg_cost': {'type': 'BBoxL1Cost', 'weight': 5.0}, 'type': 'Hungarian As signer'}} ,  test_cfg  $=$  {'max_per_img': 100} , init_cfg  $\\leftrightharpoons$  None ,  \\*\\*kwargs ) \nImplements the DETR transformer head. \nSee  paper: End-to-End Object Detection with Transformers  for details. \nParameters \n•  num classes  ( int ) – Number of categories excluding the background. •  in channels  ( int ) – Number of channels in the input feature map. •  num_query  ( int ) – Number of query in Transformer. •  num reg fcs  ( int, optional ) – Number of fully-connected layers used in  FFN , which is then used for the regression head. Default 2. •  (obj  ( test_cfg ) –  \\` mmcv.ConfigDict\\`|dict): Config for transformer. Default: None. •  sync cls avg factor  ( bool ) – Whether to sync the avg_factor of all ranks. Default to False. •  (obj  –  \\` mmcv.ConfigDict\\`|dict): Config for position encoding. •  (obj  –  mmcv.ConfigDict\\`|dict): Config of the classification loss. Default \\`Cross Entropy- Loss . •  (obj  –  mmcv.ConfigDict\\`|dict): Config of the regression loss. Default \\`L1Loss . •  (obj  –  mmcv.ConfigDict\\`|dict): Config of the regression iou loss. Default \\`GIoULoss . •  (obj  –  \\` mmcv.ConfigDict\\`|dict): Training config of transformer head. •  (obj  –  \\` mmcv.ConfigDict\\`|dict): Testing config of transformer head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward ( feats ,  img_metas ) Forward function. \nParameters \n•  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – List of image information. \nReturns \nOutputs for all scale levels. • all cls scores list (list[Tensor]): Classification scores for each scale level. Each is a 4D- tensor with shape [nb_dec, bs, num_query, cls out channels]. Note  cls out channels should includes background. "}
{"page": 331, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_331.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ all_bbox_preds_list (list[Tensor]): Sigmoid regression outputs for each scale level. Each\nis a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs,\nnum_query, 4].\n\nReturn type tuple[list{Tensor], list{Tensor]]\n\nforward_onnx (feats, img_metas)\nForward function for exporting to ONNX.\n\nOver-write forward because: masks is directly created with zero (valid position tag) and has the same spatial\nsize as x. Thus the construction of masks is different from that in forward.\n\nParameters\n¢ feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-tensor.\n¢ img_metas (list [dict ]) — List of image information.\n\nReturns\nOutputs for all scale levels.\n\n¢ all_cls_scores_list (list{Tensor]): Classification scores for each scale level. Each is a 4D-\ntensor with shape [nb_dec, bs, num_query, cls_out_channels]. Note cls_out_channels\nshould includes background.\n\n¢ all_bbox_preds_list (list[Tensor]): Sigmoid regression outputs for each scale level. Each\nis a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs,\nnum_query, 4].\n\nReturn type tuple[list{Tensor], list{Tensor]]\n\nforward_single (x, img_metas)\n“Forward function for a single feature level.\n\nParameters\n¢ x (Tensor) — Input feature from backbone’s single stage, shape [bs, c, h, w].\n¢ img_metas (list [dict ]) — List of image information.\n\nReturns\n\nOutputs from the classification head, shape [nb_dec, bs, num_query, cls_out_channels].\nNote cls_out_channels should includes background.\n\nall_bbox_preds (Tensor): Sigmoid outputs from the regression head with normalized\ncoordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4].\n\nReturn type all_cls_scores (Tensor)\n\nforward_single_onnx(x, img_metas)\n“Forward function for a single feature level with ONNX exportation.\n\nParameters\n¢ x (Tensor) — Input feature from backbone’s single stage, shape [bs, c, h, w].\n¢ img_metas (list [dict ]) — List of image information.\n\nReturns\n\nOutputs from the classification head, shape [nb_dec, bs, num_query, cls_out_channels].\nNote cls_out_channels should includes background.\n\nall_bbox_preds (Tensor): Sigmoid outputs from the regression head with normalized\ncoordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4].\n\n324 Chapter 39. mmdet.models\n", "vlm_text": "• all b box p reds list (list[Tensor]): Sigmoid regression outputs for each scale level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. \nReturn type  tuple[list[Tensor], list[Tensor]] \nforward on nx ( feats ,  img_metas ) Forward function for exporting to ONNX. Over-write  forward  because:  masks  is directly created with zero (valid position tag) and has the same spatial size as  $x$  . Thus the construction of  masks  is different from that in  forward . \nParameters \n•  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – List of image information. \n\nReturns \n\n• all cls scores list (list[Tensor]): Classification scores for each scale level. Each is a 4D- tensor with shape [nb_dec, bs, num_query, cls out channels]. Note  cls out channels should includes background. • all b box p reds list (list[Tensor]): Sigmoid regression outputs for each scale level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. \nReturn type  tuple[list[Tensor], list[Tensor]] \nforward single ( x ,  img_metas ) “Forward function for a single feature level. \nParameters \n•  x  ( Tensor ) – Input feature from backbone’s single stage, shape [bs, c, h, w]. •  img_metas  ( list[dict] ) – List of image information. \nReturns \nOutputs from the classification head,  shape [nb_dec, bs, num_query, cls out channels]. Note cls out channels should includes background. all b box p reds (Tensor): Sigmoid outputs from the regression  head with normalized coordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4]. \nReturn type  all cls scores (Tensor) \nforward single on nx ( x ,  img_metas ) “Forward function for a single feature level with ONNX exportation. \nParameters \n•  x  ( Tensor ) – Input feature from backbone’s single stage, shape [bs, c, h, w]. •  img_metas  ( list[dict] ) – List of image information. \nReturns \nOutputs from the classification head,  shape [nb_dec, bs, num_query, cls out channels]. Note cls out channels should includes background. all b box p reds (Tensor): Sigmoid outputs from the regression  head with normalized coordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4]. "}
{"page": 332, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_332.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type all_cls_scores (Tensor)\n\nforward_train(x, img_metas, gt_bboxes, gt_labels=None, gt_bboxes_ignore=None, proposal_cfg=None,\n**kwargs)\nForward function for training mode.\n\nParameters\n¢ x (list [Tensor]) — Features from backbone.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes (Tensor) — Ground truth bboxes of the image, shape (num_gts, 4).\n¢ gt_labels (Tensor) — Ground truth labels of each box, shape (num_gts,).\n\n* gt_bboxes_ignore (Tensor) — Ground truth bboxes to be ignored, shape\n(num_ignored_gts, 4).\n\n* proposal_cfg (mmcv.Config) — Test / postprocessing configuration, if None, test_cfg\nwould be used.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nget_bboxes (all_cls_scores_list, all_bbox_preds_list, img_metas, rescale=False)\nTransform network outputs for a batch into bbox predictions.\n\nParameters\n\n¢ all_cls_scores_list (list [Tensor ]) — Classification outputs for each feature level.\nEach is a 4D-tensor with shape [nb_dec, bs, num_query, cls_out_channels].\n\n¢ all_bbox_preds_list (list [Tensor]) — Sigmoid regression outputs for each feature\nlevel. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape\n[nb_dec, bs, num_query, 4].\n\n¢ img_metas (list [dict ]) — Meta information of each image.\n\n¢ rescale (bool, optional) — If True, return boxes in original image space. Default\nFalse.\n\nReturns Each item in result_list is 2-tuple. The first item is an (n, 5) tensor, where the first 4\ncolumns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\nbetween 0 and 1. The second item is a (n,) tensor where each item is the predicted class label\nof the corresponding box.\n\nReturn type list[list{Tensor, Tensor]]\n\nget_targets(cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, img_metas,\ngt_bboxes_ignore_list=None)\n“Compute regression and classification targets for a batch image.\n\nOutputs from a single decoder layer of a single feature level are used.\nParameters\n\n¢ cls_scores_list (list [Tensor]) — Box score logits from a single decoder layer for\neach image with shape [num_query, cls_out_channels].\n\n¢ bbox_preds_list (list [Tensor]) — Sigmoid outputs from a single decoder layer for\neach image, with normalized coordinate (cx, cy, w, h) and shape [num_query, 4].\n\n39.4. dense_heads 325\n", "vlm_text": "Return type  all cls scores (Tensor) \nforward train ( x ,  img_metas ,  gt_bboxes ,  gt_labels  $\\mathbf{\\hat{\\rho}}$  None ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ,  proposal cf g  $=$  None , \\*\\*kwargs ) Forward function for training mode. \nParameters \n•  x  ( list[Tensor] ) – Features from backbone. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt_bboxes  ( Tensor ) – Ground truth bboxes of the image, shape (num_gts, 4). •  gt_labels  ( Tensor ) – Ground truth labels of each box, shape (num_gts,). •  gt b boxes ignore ( Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). •  proposal cf g  ( mmcv.Config ) – Test / post processing configuration, if None, test_cfg would be used. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nget_bboxes ( all cls scores list ,  all b box p reds list ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Transform network outputs for a batch into bbox predictions. \nParameters \n•  all cls scores list  ( list[Tensor] ) – Classification outputs for each feature level. Each is a 4D-tensor with shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds list  ( list[Tensor] ) – Sigmoid regression outputs for each feature level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  img_metas  ( list[dict] ) – Meta information of each image. •  rescale  ( bool, optional ) – If True, return boxes in original image space. Default False. \nReturns  Each item in result list is 2-tuple. The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. \nReturn type  list[list[Tensor, Tensor]] \nget targets ( cls scores list ,  b box p reds list ,  gt b boxes list ,  gt labels list ,  img_metas , gt b boxes ignore lis $\\leftleftarrows$ None)“Compute regression and classification targets for a batch image. \nOutputs from a single decoder layer of a single feature level are used. \nParameters \n cls scores list  ( list[Tensor] ) – Box score logits from a single decoder layer for each image with shape [num_query, cls out channels]. •  b box p reds list  ( list[Tensor] ) – Sigmoid outputs from a single decoder layer for each image, with normalized coordinate (cx, cy, w, h) and shape [num_query, 4]. "}
{"page": 333, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_333.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes for each image with shape\n(num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels_list (list [Tensor]) - Ground truth class indices for each image with\nshape (num_gts, ).\n\n¢ img_metas (list [dict ]) — List of image meta information.\n\n* gt_bboxes_ignore_list (list[Tensor], optional) — Bounding boxes which can\nbe ignored for each image. Default None.\n\nReturns\na tuple containing the following targets.\n¢ labels_list (list[Tensor]): Labels for all images.\n* label_weights_list (list{Tensor]): Label weights for all images.\n* bbox_targets_list (list[Tensor]): BBox targets for all images.\n* bbox_weights_list (list{Tensor]): BBox weights for all images.\n* num_total_pos (int): Number of positive samples in all images.\n* num_total_neg (int): Number of negative samples in all images.\nReturn type tuple\n\ninit_weights()\nInitialize weights of the transformer head.\n\nloss (all_cls_scores_list, all_bbox_preds_list, gt_bboxes_list, gt_labels_list, img_metas,\ngt_bboxes_ignore=None)\n“Loss function.\n\nOnly outputs from the last feature level are used for computing losses by default.\nParameters\n\n¢ all_cls_scores_list (list [Tensor ]) — Classification outputs for each feature level.\nEach is a 4D-tensor with shape [nb_dec, bs, num_query, cls_out_channels].\n\n¢ all_bbox_preds_list (list [Tensor]) — Sigmoid regression outputs for each feature\nlevel. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape\n[nb_dec, bs, num_query, 4].\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes for each image with shape\n(num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels_list (list [Tensor]) - Ground truth class indices for each image with\nshape (num_gts, ).\n\n¢ img_metas (list [dict ]) — List of image meta information.\n\n* gt_bboxes_ignore (list[Tensor], optional) — Bounding boxes which can be ig-\nnored for each image. Default None.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single(cls_scores, bbox_preds, gt_bboxes_list, gt_labels_list, img_metas,\ngt_bboxes_ignore_list=None)\n“Loss function for outputs from a single decoder layer of a single feature level.\n\nParameters\n\n326 Chapter 39. mmdet.models\n", "vlm_text": "•  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore list  ( list[Tensor], optional ) – Bounding boxes which can be ignored for each image. Default None. \nReturns \na tuple containing the following targets. • labels list (list[Tensor]): Labels for all images. • label weights list (list[Tensor]): Label weights for all images. • b box targets list (list[Tensor]): BBox targets for all images. • b box weights list (list[Tensor]): BBox weights for all images. • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. \nReturn type  tuple \nin it weights()Initialize weights of the transformer head. \nloss ( all cls scores list ,  all b box p reds list ,  gt b boxes list ,  gt labels list ,  img_metas , gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) “Loss function. \nOnly outputs from the last feature level are used for computing losses by default. \nParameters \n•  all cls scores list  ( list[Tensor] ) – Classification outputs for each feature level. Each is a 4D-tensor with shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds list  ( list[Tensor] ) – Sigmoid regression outputs for each feature level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore  ( list[Tensor], optional ) – Bounding boxes which can be ig- nored for each image. Default None. \nReturn type  dict[str, Tensor] \nloss single ( cls_scores ,  bbox_preds ,  gt b boxes list ,  gt labels list ,  img_metas , gt b boxes ignore lis $\\leftleftarrows$ None)“Loss function for outputs from a single decoder layer of a single feature level. \nParameters "}
{"page": 334, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_334.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* cls_scores (Tensor) — Box score logits from a single decoder layer for all images. Shape\n[bs, num_query, cls_out_channels].\n\n¢ bbox_preds (Tensor) — Sigmoid outputs from a single decoder layer for all images, with\nnormalized coordinate (cx, cy, w, h) and shape [bs, num_query, 4].\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes for each image with shape\n(num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels_list (list [Tensor]) - Ground truth class indices for each image with\nshape (num_gts, ).\n\n¢ img_metas (list [dict ]) — List of image meta information.\n\n* gt_bboxes_ignore_list (list[Tensor], optional) — Bounding boxes which can\nbe ignored for each image. Default None.\n\nReturns\nA dictionary of loss components for outputs from a single decoder layer.\nReturn type dict[str, Tensor]\n\nonnx_export (all_cls_scores_list, all_bbox_preds_list, img_metas)\nTransform network outputs into bbox predictions, with ONNX exportation.\n\nParameters\n\n¢ all_cls_scores_list (list [Tensor ]) — Classification outputs for each feature level.\nEach is a 4D-tensor with shape [nb_dec, bs, num_query, cls_out_channels].\n\n¢ all_bbox_preds_list (list [Tensor]) — Sigmoid regression outputs for each feature\nlevel. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape\n[nb_dec, bs, num_query, 4].\n\n¢ img_metas (list [dict ]) — Meta information of each image.\nReturns\n\ndets of shape [N, num_det, 5] and class labels of shape [N, num_det].\nReturn type tuple[Tensor, Tensor]\n\nsimple_test_bboxes (feats, img_metas, rescale=False)\nTest det bboxes without test-time augmentation.\n\nParameters\n\n¢ feats (tuple[torch. Tensor]) — Multi-level features from the upstream network, each\nis a 4D-tensor.\n\n¢ img_metas (list [dict ]) — List of image information.\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\nReturns\n\nEach item in result_list is 2-tuple. The first item is bboxes with shape (n, 5), where 5 rep-\nresent (tl_x, tl_y, br_x, br_y, score). The shape of the second tensor in the tuple is labels\nwith shape (n,)\n\nReturn type list[tuple[Tensor, Tensor]]\n\n39.4. dense_heads 327\n", "vlm_text": "•  cls_scores  ( Tensor ) – Box score logits from a single decoder layer for all images. Shape [bs, num_query, cls out channels]. •  bbox_preds  ( Tensor ) – Sigmoid outputs from a single decoder layer for all images, with normalized coordinate (cx, cy, w, h) and shape [bs, num_query, 4]. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore list  ( list[Tensor], optional ) – Bounding boxes which can be ignored for each image. Default None. \nReturns \nA dictionary of loss components for outputs from  a single decoder layer. \nReturn type  dict[str, Tensor] \non nx export ( all cls scores list ,  all b box p reds list ,  img_metas ) Transform network outputs into bbox predictions, with ONNX exportation. \nParameters \n•  all cls scores list  ( list[Tensor] ) – Classification outputs for each feature level. Each is a 4D-tensor with shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds list  ( list[Tensor] ) – Sigmoid regression outputs for each feature level. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  img_metas  ( list[dict] ) – Meta information of each image. \nReturns \ndets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. Return type  tuple[Tensor, Tensor] \nsimple test b boxes ( feats ,  img_metas ,  rescale=False ) Test det bboxes without test-time augmentation. \nParameters \n•  feats  ( tuple[torch.Tensor] ) – Multi-level features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – List of image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns \nEach item in result list is 2-tuple.  The first item is  bboxes  with shape (n, 5), where 5 rep- resent (tl_x, tl_y, br_x, br_y, score). The shape of the second tensor in the tuple is  labels with shape (n,) \nReturn type  list[tuple[Tensor, Tensor]] "}
{"page": 335, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_335.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads .DecoupledSOLOHead (*args, init_cfg=[{'type': ‘Normal’, ‘layer’: 'Conv2d',\n\n‘std': 0.01}, {'type’: ‘Normal’, 'std': 0.01, 'bias_prob':\n0.01, ‘override’: {‘name': 'conv_mask_list_x'}},\n{'type’: ‘Normal’, 'std': 0.01, 'bias_prob': 0.01,\n‘override’: {‘name': 'conv_mask_list_y'}}, {'type':\n‘Normal’, 'std': 0.01, ‘bias_prob': 0.01, ‘override’:\n{‘name': 'conv_cls'}}], **kwargs)\n\nDecoupled SOLO mask head used in “SOLO: Segmenting Objects by Locations.\n\n<https://arxiv.org/abs/1912.04488>>_\n\nParameters init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nget_results (mlvl_mask_preds_x, mlvl_mask_preds_y, mlvl_cls_scores, img_metas, rescale=None,\n**kwargs)\nGet multi-image mask results.\n\nParameters\n\n¢ mlvl_mask_preds_x (list [Tensor]) — Multi-level mask prediction from x branch.\nEach element in the list has shape (batch_size, num_grids ,h ,w).\n\nmlvl_mask_preds_y (list [Tensor]) — Multi-level mask prediction from y branch.\nEach element in the list has shape (batch_size, num_grids ,h ,w).\n\n¢ mlvl_cls_scores (list [Tensor]) — Multi-level scores. Each element in the list has\nshape (batch_size, num_classes znum_grids ,num_grids).\n\n¢ img_metas (list [dict]) — Meta information of all images.\nReturns\nProcessed results of multiple images.Each InstanceData usually contains following keys.\n* scores (Tensor): Classification scores, has shape (num_instance,).\n¢ labels (Tensor): Has shape (num_instances,).\n¢ masks (Tensor): Processed mask results, has shape (num_instances, h, w).\nReturn type list{InstanceData]\n\nloss (mlvl_mask_preds_x, mlvl_mask_preds_y, mlvl_cls_preds, gt_labels, gt_masks, img_metas,\ngt_bboxes=None, **kwargs)\nCalculate the loss of total batch.\n\nParameters\n\n¢ mlvl_mask_preds_x (list [Tensor]) — Multi-level mask prediction from x branch.\nEach element in the list has shape (batch_size, num_grids ,h ,w).\n\n¢ mlvl_mask_preds_x — Multi-level mask prediction from y branch. Each element in the\nlist has shape (batch_size, num_grids ,h ,w).\n\n328 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.dense heads. Decoupled SOLO Head ( \\*args ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  [{'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}, {'type': 'Normal', 'std': 0.01, 'bias_prob':0.01, 'override': {'name': 'con v mask list x'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01,'override': {'name': 'con v mask list y'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override':{'name': 'conv_cls'}}] ,  \\*\\*kwargs ) \nDecoupled SOLO mask head used in  \\` SOLO: Segmenting Objects by Locations. \n< https://arxiv.org/abs/1912.04488 \nParameters  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nget results ( ml vl mask p reds x ,  ml vl mask p reds y ,  ml vl cls scores ,  img_metas ,  rescale=None , \\*\\*kwargs ) Get multi-image mask results. \nParameters \n•  ml vl mask p reds x  ( list[Tensor] ) – Multi-level mask prediction from x branch. Each element in the list has shape (batch_size, num_grids ,h ,w). •  ml vl mask p reds y  ( list[Tensor] ) – Multi-level mask prediction from y branch. Each element in the list has shape (batch_size, num_grids ,h ,w). •  ml vl cls scores  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes ,num_grids ,num_grids). •  img_metas  ( list[dict] ) – Meta information of all images. \nReturns \nProcessed results of multiple images.Each  Instance Data  usually contains following keys. \n• scores (Tensor): Classification scores, has shape (num instance,). • labels (Tensor): Has shape (num instances,). • masks (Tensor): Processed mask results, has shape (num instances, h, w). \nReturn type  list[ Instance Data ] \nloss ( ml vl mask p reds x ,  ml vl mask p reds y ,  ml vl cls p reds ,  gt_labels ,  gt_masks ,  img_metas , gt_bboxes  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Calculate the loss of total batch. \nParameters \n•  ml vl mask p reds x  ( list[Tensor] ) – Multi-level mask prediction from x branch. Each element in the list has shape (batch_size, num_grids ,h ,w). •  ml vl mask p reds x  – Multi-level mask prediction from y branch. Each element in the list has shape (batch_size, num_grids ,h ,w). "}
{"page": 336, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_336.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ mlvl_cls_preds (list [Tensor]) — Multi-level scores. Each element in the list has\nshape (batch_size, num_classes, num_grids ,num_grids).\n\n¢ gt_labels (list [Tensor]) — Labels of multiple images.\n\n¢ gt_masks (list [Tensor]) — Ground truth masks of multiple images. Each has shape\n(num_instances, h, w).\n\n¢ img_metas (list [dict ]) — Meta information of multiple images.\n\n* gt_bboxes (list [Tensor]) — Ground truth bboxes of multiple images. Default: None.\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nclass mmdet.models.dense_heads .DecoupledSOLOLightHead (*args, den_cfg=None, init_cfg=[{'type':\n‘Normal’, ‘layer’: 'Conv2d', ‘std’: 0.01},\n{'type': ‘Normal’, 'std': 0.01, ‘bias_prob':\n0.01, ‘override’: {'‘name':\n‘conv_mask_list_x'}}, {'‘type’: ‘Normal’, 'std':\n0.01, ‘bias_prob': 0.01, ‘override’: {‘name':\n‘conv_mask_list_y'}}, {‘type’: ‘Normal’, 'std':\n0.01, ‘bias_prob': 0.01, ‘override’: {‘name':\n‘conv_cls'}}], **kwargs)\n\nDecoupled Light SOLO mask head used in SOLO: Segmenting Objects by Locations\n\nParameters\n¢ with_dcn (bool) — Whether use den in mask_convs and cls_convs, default: False.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.dense_heads .DeformableDETRHead ( “args, with_box_refine=False,\nas_two_stage=False, transformer=None,\n**kwargs)\nHead of DeformDETR: Deformable DETR: Deformable Transformers for End-to- End Object Detection.\n\nCode is modified from the official github repo.\nMore details can be found in the paper .\nParameters\n\n¢ with_box_refine (bool) — Whether to refine the reference points in the decoder. Defaults\nto False.\n\n* as_two_stage (bool) — Whether to generate the proposal from the outputs of encoder.\n\n* (obj (transformer) — ConfigDict): ConfigDict is used for building the Encoder and De-\ncoder.\n\n39.4. dense_heads 329\n", "vlm_text": "•  ml vl cls p reds  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes, num_grids ,num_grids). •  gt_labels  ( list[Tensor] ) – Labels of multiple images. •  gt_masks  ( list[Tensor] ) – Ground truth masks of multiple images. Each has shape (num instances, h, w). •  img_metas  ( list[dict] ) – Meta information of multiple images. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of multiple images. Default: None. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nclass  mmdet.models.dense heads. Decoupled SOLO Light Head ( \\*args ,  dcn_cfg  $=$  None ,  init_cfg=[{'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}, {'type': 'Normal', 'std': 0.01, 'bias_prob':0.01, 'override': {'name': 'con v mask list x'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override': {'name': 'con v mask list y'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override': {'name': 'conv_cls'}}] ,  \\*\\*kwargs ) \nDecoupled Light SOLO mask head used in  SOLO: Segmenting Objects by Locations \nParameters \n•  with_dcn  ( bool ) – Whether use dcn in mask_convs and cls_convs, default: False. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.dense heads. De formable DET RHead ( \\*args ,  with box refine  $\\mathbf{\\beta}=$  False , as two stage  $\\mathbf{=}$  False ,  transformer=None , \\*\\*kwargs ) \nHead of DeformDETR: Deformable DETR: Deformable Transformers for End-to- End Object Detection. \nCode is modified from the  official github repo . More details can be found in the  paper  . \nParameters \n•  with box refine  ( bool ) – Whether to refine the reference points in the decoder. Defaults to False. •  as two stage  ( bool ) – Whether to generate the proposal from the outputs of encoder. •  (obj  ( transformer ) –  ConfigDict ): ConfigDict is used for building the Encoder and De- coder. "}
{"page": 337, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_337.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward (mlvi_feats, img_metas)\nForward function.\n\nParameters\n\n¢ mlvl_feats (tuple [Tensor ]) — Features from the upstream network, each is a 4D-tensor\nwith shape (N, C, H, W).\n\n¢ img_metas (list [dict ]) — List of image information.\n\nReturns Outputs from the classification head, shape [nb_dec, bs, num_query, cls_out_channels].\nNote cls_out_channels should includes background. all_bbox_preds (Tensor): Sigmoid out-\nputs from the regression head with normalized coordinate format (cx, cy, w, h). Shape\n[nb_dec, bs, num_query, 4]. enc_outputs_class (Tensor): The score of each point on en-\ncode feature map, has shape (N, h*w, num_class). Only when as_two_stage is True it would\nbe returned, otherwise None would be returned. enc_outputs_coord (Tensor): The proposal\ngenerate from the encode feature map, has shape (N, h*w, 4). Only when as_two_stage is\nTrue it would be returned, otherwise None would be returned.\n\nReturn type all_cls_scores (Tensor)\n\nget_bboxes (all_cls_scores, all_bbox_preds, enc_cls_scores, enc_bbox_preds, img_metas, rescale=False)\nTransform network outputs for a batch into bbox predictions.\n\nParameters\n\n¢ all_cls_scores (Tensor) — Classification score of all decoder layers, has shape [nb_dec,\nbs, num_query, cls_out_channels].\n\n¢ all_bbox_preds (Tensor) — Sigmoid regression outputs of all decode layers. Each\nis a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs,\nnum_query, 4].\n\n* enc_cls_scores (Tensor) — Classification scores of points on encode feature map , has\nshape (N, h*w, num_classes). Only be passed when as_two_stage is True, otherwise is\nNone.\n\n* enc_bbox_preds (Tensor) — Regression results of each points on the encode feature map,\nhas shape (N, h*w, 4). Only be passed when as_two_stage is True, otherwise is None.\n\n¢ img_metas (list [dict ]) — Meta information of each image.\n\n¢ rescale (bool, optional) — If True, return boxes in original image space. Default\nFalse.\n\nReturns Each item in result_list is 2-tuple. The first item is an (n, 5) tensor, where the first 4\ncolumns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\nbetween 0 and 1. The second item is a (n,) tensor where each item is the predicted class label\nof the corresponding box.\n\nReturn type list[list{Tensor, Tensor]]\n\ninit_weights()\nInitialize weights of the DeformDETR head.\n\nloss (all_cls_scores, all_bbox_preds, enc_cls_scores, enc_bbox_preds, gt_bboxes_list, gt_labels_list,\nimg_metas, gt_bboxes_ignore=None)\n“Loss function.\n\nParameters\n\n¢ all_cls_scores (Tensor) — Classification score of all decoder layers, has shape [nb_dec,\nbs, num_query, cls_out_channels].\n\n330 Chapter 39. mmdet.models\n", "vlm_text": "forward ( mlvl_feats ,  img_metas ) Forward function. \nParameters \n•  mlvl_feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor with shape (N, C, H, W). \nlist[dict] \nReturns  Outputs from the classification head, shape [nb_dec, bs, num_query, cls out channels]. Note cls out channels should includes background. all b box p reds (Tensor): Sigmoid out- puts from the regression head with normalized coordinate format (cx, cy, w, h). Shape [nb_dec, bs, num_query, 4]. enc outputs class (Tensor): The score of each point on en- code feature map, has shape (N,  $\\mathbf{h}^{\\ast}\\mathbf{w}$  , num_class). Only when as two stage is True it would be returned, otherwise  None  would be returned. enc outputs coord (Tensor): The proposal generate from the encode feature map, has shape   $(\\mathsf{N},\\,\\mathsf{h}^{\\ast}\\mathbf{w},\\,4)$  . Only when as two stage is True it would be returned, otherwise  None  would be returned. \nReturn type  all cls scores (Tensor) \nget_bboxes ( all cls scores ,  all b box p reds ,  enc cls scores ,  enc b box p reds ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Transform network outputs for a batch into bbox predictions. \nParameters \n•  all cls scores  ( Tensor ) – Classification score of all decoder layers, has shape [nb_dec, bs, num_query, cls out channels]. •  all b box p reds  ( Tensor ) – Sigmoid regression outputs of all decode layers. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  enc cls scores  ( Tensor ) – Classification scores of points on encode feature map , has shape (N,   $\\mathbf{h}^{\\ast}\\mathbf{w}.$  , num classes). Only be passed when as two stage is True, otherwise is None. •  enc b box p reds  ( Tensor ) – Regression results of each points on the encode feature map, has shape (N, h\\*w, 4). Only be passed when as two stage is True, otherwise is None. •  img_metas  ( list[dict] ) – Meta information of each image. •  rescale  ( bool, optional ) – If True, return boxes in original image space. Default False. \nReturns  Each item in result list is 2-tuple. The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. \nReturn type  list[list[Tensor, Tensor]] \nin it weights()Initialize weights of the DeformDETR head. \nloss ( all cls scores ,  all b box p reds ,  enc cls scores ,  enc b box p reds ,  gt b boxes list ,  gt labels list , img_metas, gt b boxes ignore $\\mathbf{\\dot{\\rho}}$ None)“Loss function. \nParameters \n•  all cls scores  ( Tensor ) – Classification score of all decoder layers, has shape [nb_dec, bs, num_query, cls out channels]. "}
{"page": 338, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_338.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ all_bbox_preds (Tensor) — Sigmoid regression outputs of all decode layers. Each\nis a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs,\nnum_query, 4].\n\n* enc_cls_scores (Tensor) — Classification scores of points on encode feature map , has\nshape (N, h*w, num_classes). Only be passed when as_two_stage is True, otherwise is\nNone.\n\n* enc_bbox_preds (Tensor) — Regression results of each points on the encode feature map,\nhas shape (N, h*w, 4). Only be passed when as_two_stage is True, otherwise is None.\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes for each image with shape\n(num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels_list (list [Tensor]) - Ground truth class indices for each image with\nshape (num_gts, ).\n\n¢ img_metas (list [dict ]) — List of image meta information.\n\n* gt_bboxes_ignore (list[Tensor], optional) — Bounding boxes which can be ig-\nnored for each image. Default None.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nclass mmdet.models.dense_heads .EmbeddingRPNHead (num_proposals=100, proposal_feature_channel=256,\ninit_cfg=None, **kwargs)\nRPNHead in the Sparse R-CNN .\n\nUnlike traditional RPNHead, this module does not need FPN input, but just decode init_proposal_bboxes and\nexpand the first dimension of init_proposal_bboxes and init_proposal_features to the batch_size.\n\nParameters\n* num_proposals (int) — Number of init_proposals. Default 100.\n\n* proposal_feature_channel (int) — Channel number of init_proposal_feature. Defaults\nto 256.\n\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nforward_dummy (img, img_metas)\nDummy forward function.\n\nUsed in flops calculation.\n\nforward_train(img, img_metas)\nForward function in training stage.\n\ninit_weights()\nInitialize the init_proposal_bboxes as normalized.\n\n[c_x, c_y, w, h], and we initialize it to the size of the entire image.\n\nsimple_test (img, img_metas)\nForward function in testing stage.\n\nsimple_test_rpn(img, img_metas)\nForward function in testing stage.\n\n39.4. dense_heads 331\n", "vlm_text": "•  all b box p reds  ( Tensor ) – Sigmoid regression outputs of all decode layers. Each is a 4D-tensor with normalized coordinate format (cx, cy, w, h) and shape [nb_dec, bs, num_query, 4]. •  enc cls scores  ( Tensor ) – Classification scores of points on encode feature map , has shape (N, h\\*w, num classes). Only be passed when as two stage is True, otherwise is None. •  enc b box p reds  ( Tensor ) – Regression results of each points on the encode feature map, has shape (N, h\\*w, 4). Only be passed when as two stage is True, otherwise is None. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt labels list  ( list[Tensor] ) – Ground truth class indices for each image with shape (num_gts, ). •  img_metas  ( list[dict] ) – List of image meta information. •  gt b boxes ignore  ( list[Tensor], optional ) – Bounding boxes which can be ig- nored for each image. Default None.  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nclass  mmdet.models.dense heads. Embedding RP N Head ( num proposals  $\\mathbf{\\tilde{=}}$  100 ,  proposal feature ch anne  $!{=}256$  , init_cfg  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) \nRPNHead in the  Sparse R-CNN \nUnlike traditional RPNHead, this module does not need FPN input, but just decode  in it proposal b boxes  and expand the first dimension of  in it proposal b boxes  and  in it proposal features  to the batch_size. \nParameters \n•  num proposals  ( int ) – Number of in it proposals. Default 100. •  proposal feature channel  ( int ) – Channel number of in it proposal feature. Defaults to 256. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward dummy ( img ,  img_metas ) Dummy forward function. Used in flops calculation. \nforward train ( img ,  img_metas ) Forward function in training stage. \nin it weights()Initialize the in it proposal b boxes as normalized. [c_x, c_y, w, h], and we initialize it to the size of the entire image. \nsimple test ( img ,  img_metas ) Forward function in testing stage. \nsimple test rp n ( img ,  img_metas ) Forward function in testing stage. "}
{"page": 339, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_339.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads .FCOSHead (num_classes, in_channels, regress_ranges=((- 1, 64), (64, 128),\n\n(128, 256), (256, 512), (512, 100000000.0)),\ncenter_sampling=False, center_sample_radius=1.5,\nnorm_on_bbox=False, centerness_on_reg=False,\n\nloss_cls={‘alpha': 0.25, 'gamma': 2.0, ‘loss_weight': 1.0, ‘type’:\n\n‘FocalLoss', 'use_sigmoid': True}, loss_bbox={'loss_weight':\n1.0, ‘type’: 'IoULoss'}, loss_centerness={'loss_weight': 1.0,\n‘type’: 'CrossEntropyLoss’, 'use_sigmoid': True},\n\nnorm_cfg={‘num_groups': 32, 'requires_grad': True, 'type':\n\n'GN'}, init_cfg=[{‘layer': 'Conv2d’, ‘override’: {‘bias_prob': 0.01,\n\n‘name’: 'conv_cls', 'std': 0.01, 'type': 'Normal'}, ‘std’: 0.01,\n‘type’: 'Normal'}, **kwargs)\n\nAnchor-free head used in FCOS.\n\nThe FCOS head does not use anchor boxes. Instead bounding boxes are predicted at each pixel and a centerness\nmeasure is used to suppress low-quality predictions. Here norm_on_bbox, centerness_on_reg, den_on_last_conv\nare training tricks used in official repo, which will bring remarkable mAP gains of up to 4.9. Please see https:\n\n//github.com/tianzhi0549/FCOS for more detail.\n\nParameters\n\nnum_classes (int) — Number of categories excluding the background category.\nin_channels (int) — Number of channels in the input feature map.\n\nstrides (list[int] | list[tuple[int, int]]) — Strides of points in multiple fea-\nture levels. Default: (4, 8, 16, 32, 64).\n\nregress_ranges (tuple[tuple[int, int]])—Regress range of multiple level points.\ncenter_sampling (boo1) — If true, use center sampling. Default: False.\ncenter_sample_radius (float) — Radius of center sampling. Default: 1.5.\n\nnorm_on_bbox (boo1) — If true, normalize the regression targets with FPN strides. Default:\nFalse.\n\ncenterness_on_reg (boo1) — If true, position centerness on the regress branch. Please re-\nfer to https://github.com/tianzhi0549/FCOS/issues/89#issuecomment-5 16877042. Default:\nFalse.\n\nconv_bias (bool | str) -— If specified as auto, it will be decided by the norm_cfg. Bias\nof conv will be set as True if norm_cfg is None, otherwise False. Default: “auto”.\n\nloss_cls (dict) — Config of classification loss.\nloss_bbox (dict) — Config of localization loss.\nloss_centerness (dict) — Config of centerness loss.\n\nnorm_cfg (dict) - dictionary to construct and config norm layer. Default:\nnorm_cfg=dict(type=’GN’, num_groups=32, requires_grad=True).\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\n332\n\nChapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.dense heads. FCOSHead ( num classes ,  in channels ,  regress_ranges=((- 1, 64), (64, 128), (128, 256), (256, 512), (512, 100000000.0)) , center sampling  $\\scriptstyle{\\tilde{}}=$  False ,  center sample radius=1.5 , norm on b box  $\\mathbf{\\beta}=$  False ,  center ness on reg  $\\mathbf{\\hat{\\Sigma}}$  False , loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 1.0, 'type':'FocalLoss', 'use s igm oid': True} ,  loss_bbox={'loss weight': 1.0, 'type': 'IoULoss'} ,  loss center ness={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , norm_cfg  $=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) \nAnchor-free head used in  FCOS . \nThe FCOS head does not use anchor boxes. Instead bounding boxes are predicted at each pixel and a centerness measure is used to suppress low-quality predictions. Here norm on b box, center ness on reg, dc n on last con v are training tricks used in official repo, which will bring remarkable mAP gains of up to 4.9. Please see  https: //github.com/tian zhi 0549/FCOS  for more detail. \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  strides  ( list[int] | list[tuple[int, int]] ) – Strides of points in multiple fea- ture levels. Default: (4, 8, 16, 32, 64). •  regress ranges  ( tuple[tuple[int, int]] ) – Regress range of multiple level points. •  center sampling  ( bool ) – If true, use center sampling. Default: False. •  center sample radius  ( float ) – Radius of center sampling. Default: 1.5. •  norm on b box  ( bool ) – If true, normalize the regression targets with FPN strides. Default: False. •  center ness on reg  ( bool ) – If true, position centerness on the regress branch. Please re- fer to  https://github.com/tian zhi 0549/FCOS/issues/89#issue comment-516877042 . Default: False. •  conv_bias  ( bool | str ) – If specified as  auto , it will be decided by the norm_cfg. Bias of conv will be set as True if  norm_cfg  is None, otherwise False. Default: “auto”. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  loss center ness  ( dict ) – Config of centerness loss. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $=$  dict(type  $\\risingdotseq$  ’GN’, num_group  $\\wp{=}32$  , requires grad  $\\scriptstyle\\varepsilon=$  True). •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. "}
{"page": 340, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_340.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> self = FCOSHead(11, 7)\n\n>>> feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]\n>>> cls_score, bbox_pred, centerness = self. forward(feats)\n\n>>> assert len(cls_score) == len(self.scales)\n\ncenterness_target (pos_bbox_targets)\nCompute centerness targets.\n\nParameters pos_bbox_targets (Tensor) — BBox targets of positive bboxes in shape\n(num_pos, 4)\n\nReturns Centerness target.\nReturn type Tensor\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns cls_scores (list{Tensor]): Box scores for each scale level, each is a 4D-tensor, the channel\nnumber is num_points * num_classes. bbox_preds (list[Tensor]): Box energies / deltas for\neach scale level, each is a 4D-tensor, the channel number is num_points * 4. centernesses\n(list[Tensor]): centerness for each scale level, each is a 4D-tensor, the channel number is\nnum_points * 1.\n\nReturn type tuple\n\nforward_single(x, scale, stride)\nForward features of a single scale level.\n\nParameters\n¢ x (Tensor) — FPN feature maps of the specified stride.\n* € (scale) — obj: mmcv.cnn.Scale): Learnable scale module to resize the bbox prediction.\n\n¢ stride (int) — The corresponding stride for feature maps, only used to normalize the\nbbox prediction when self.norm_on_bbox is True.\n\nReturns scores for each class, bbox predictions and centerness predictions of input feature maps.\nReturn type tuple\n\nget_targets (points, gt_bboxes_list, gt_labels_list)\nCompute regression, classification and centerness targets for points in multiple images.\n\nParameters\n* points (list [Tensor ]) — Points of each fpn level, each has shape (num_points, 2).\n\n* gt_bboxes_list (list [Tensor ]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\n\n¢ gt_labels_list (list [Tensor]) — Ground truth labels of each box, each has shape\n(num_gt,).\n\nReturns concat_lvl_labels (list[Tensor]): Labels of each level. concat_lvl_bbox_targets\n(list{Tensor]): BBox targets of each level.\n\nReturn type tuple\n\n39.4. dense_heads 333\n", "vlm_text": "Example \nThe table contains a snippet of Python code related to a neural network model. Here's an explanation of the code:\n\n1. `self = FCOSHead(11, 7)`:\n   - This initializes an object `self` using the `FCOSHead` class with parameters `11` and `7`.\n\n2. `feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]`:\n   - This creates a list of tensors named `feats`. The tensors are randomly generated with dimensions `(1, 7, s, s)`, where `s` is a scale from the list `[4, 8, 16, 32, 64]`.\n\n3. `cls_score, bbox_pred, centerness = self.forward(feats)`:\n   - This line calls the `forward` method on the `self` object using `feats` as input and unpacks the output into `cls_score`, `bbox_pred`, and `centerness`.\n\n4. `assert len(cls_score) == len(self.scales)`:\n   - This assertion checks if the length of `cls_score` is equal to the length of `self.scales`. If not, an error will be raised.\n\nThis code is likely used to process multi-scale features through a network head, such as for object detection in a neural network model.\ncenter ness target ( pos b box targets ) Compute centerness targets. \nParameters  pos b box targets  ( Tensor ) – BBox targets of positive bboxes in shape (num_pos, 4) \nReturns  Centerness target. \nReturn type  Tensor \nforward ( feats ) Forward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns  cls_scores (list[Tensor]): Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box energies  $/$   deltas for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\mathrm{~}4$  . center ness es (list[Tensor]): centerness for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\,1$  . \nforward single ( x ,  scale ,  stride ) Forward features of a single scale level. \nParameters \n•  x  ( Tensor ) – FPN feature maps of the specified stride. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. \n•  stride  ( int ) – The corresponding stride for feature maps, only used to normalize the bbox prediction when self.norm on b box is True. \nReturns  scores for each class, bbox predictions and centerness predictions of input feature maps. \nReturn type  tuple \nget targets ( points ,  gt b boxes list ,  gt labels list ) Compute regression, classification and centerness targets for points in multiple images. \nParameters \n•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). Returns  con cat lv l labels (list[Tensor]): Labels of each level. con cat lv l b box targets (list[Tensor]): BBox targets of each level. \nReturn type  tuple "}
{"page": 341, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_341.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nloss (cls_scores, bbox_preds, centernesses, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box scores for each scale level, each is a 4D-tensor, the\nchannel number is num_points * num_classes.\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\n* centernesses (list [Tensor]) — centerness for each scale level, each is a 4D-tensor,\nthe channel number is num_points * 1.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\nclass mmdet.models.dense_heads .FSAFHead(*args, score_threshold=None, init_cfg=None, **kwargs)\nAnchor-free head used in FSAF.\n\nThe head contains two subnetworks. The first classifies anchor boxes and the second regresses deltas for the\n\nanchors (num_anchors is 1 for anchor- free methods)\n\nParameters\n\nExample\n\n¢ *args — Same as its base class in RetinaHead\n\nscore_threshold (float, optional) -The score_threshold to calculate positive recall.\nIf given, prediction scores lower than this value is counted as incorrect prediction. Default\nto None.\n\ninit_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\n**kwargs — Same as its base class in RetinaHead\n\n>>> import torch\n>>> self = FSAFHead(11, 7)\n\n>>> K =\n\ntorch.rand(1, 7, 32, 32)\n\n>>> cls_score, bbox_pred = self. forward_single(x)\n\n>>> # Each anchor predicts a score for each class except background\n>>> cls_per_anchor = cls_score.shape[1] / self.num_anchors\n\n>>> box_per_anchor = bbox_pred.shape[1] / self.num_anchors\n\n>>> assert cls_per_anchor == self.num_classes\n\n>>> assert box_per_anchor == 4\n\ncalculate_pos_recal1 (cls_scores, labels_list, pos_inds)\nCalculate positive recall with score threshold.\n\n334\n\nChapter 39. mmdet.models\n\n", "vlm_text": "loss ( cls_scores ,  bbox_preds ,  center ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute loss of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  center ness es  ( list[Tensor] ) – centerness for each scale level, each is a 4D-tensor, the channel number is num_points   $^{\\ast}\\,1$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. \n\nReturn type  dict[str, Tensor] \nclass  mmdet.models.dense heads. FSAFHead ( \\*args ,  score threshold  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ,  \\*\\*kwargs ) Anchor-free head used in  FSAF . \nThe head contains two sub networks. The first classifies anchor boxes and the second regresses deltas for the anchors (num anchors is 1 for anchor- free methods) \nParameters \n•  \\*args  – Same as its base class in  RetinaHead •  score threshold  ( float, optional ) – The score threshold to calculate positive recall. If given, prediction scores lower than this value is counted as incorrect prediction. Default to None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None •  \\*\\*kwargs  – Same as its base class in  RetinaHead \nExample \n $>>$   import  torch  $>>$   self  $=$   FSAFHead( 11 ,  7 )  $>>$   x  $=$   torch . rand( 1 ,  7 ,  32 ,  32 )  $>>$   cls_score, bbox_pred  $=$   self . forward single(x)  $>>$   # Each anchor predicts a score for each class except background  $>>$   cls per anchor  $=$   cls_score . shape[ 1 ]  /  self . num anchors  $>>$   box per anchor  $=$   bbox_pred . shape[ 1 ]  /  self . num anchors >>>  assert  cls per anchor  $==$   self . num classes >>>  assert  box per anchor  $==~4$  \ncalculate pos recall ( cls_scores ,  labels list ,  pos_inds ) Calculate positive recall with score threshold. "}
{"page": 342, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_342.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Classification scores at all fpn levels. Each tensor is in\nshape (N, num_classes * num_anchors, H, W)\n\n¢ labels_list (list [Tensor ]) — The label that each anchor is assigned to. Shape (N *\nH * W * num_anchors, )\n\n* pos_inds (list [Tensor]) — List of bool tensors indicating whether the anchor is as-\nsigned to a positive label. Shape (N * H * W * num_anchors, )\n\nReturns A single float number indicating the positive recall.\nReturn type Tensor\n\ncollect_loss_level_single(cls_loss, reg_loss, assigned_gt_inds, labels_seq)\nGet the average loss in each FPN level w.r.t. each gt label.\n\nParameters\n\n¢ cls_loss (Tensor) — Classification loss of each feature map pixel, shape (num_anchor,\nnum_class)\n\n* reg_loss (Tensor) — Regression loss of each feature map pixel, shape (num_anchor, 4)\n\n* assigned_gt_inds (Tensor) — It indicates which gt the prior is assigned to (0-based, -1:\nno assignment). shape (num_anchor),\n\n¢ labels_seq — The rank of labels. shape (num_gt)\nReturns (num_gt), average loss of each gt in this level\nReturn type shape\n\nforward_single(x)\nForward feature map of a single scale level.\n\nParameters x (Tensor) — Feature map of a single scale level.\nReturns\n\ncls_score (Tensor): Box scores for each scale level Has shape (N, num_points *\nnum_classes, H, W).\n\nbbox_pred (Tensor): Box energies / deltas for each scale level with shape (N,\nnum_points * 4, H, W).\n\nReturn type tuple (Tensor)\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor ])-— Box scores for each scale level Has shape (N, num_points\n* num_classes, H, W).\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_points * 4, H, W).\n\n* gt_bboxes (list [Tensor ]) — each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n39.4. dense_heads 335\n", "vlm_text": "Parameters \n•  cls_scores  ( list[Tensor] ) – Classification scores at all fpn levels. Each tensor is in shape (N, num classes \\* num anchors, H, W) •  labels list  ( list[Tensor] ) – The label that each anchor is assigned to. Shape (N \\* H \\* W \\* num anchors, ) •  pos_inds  ( list[Tensor] ) – List of bool tensors indicating whether the anchor is as- signed to a positive label. Shape   $(\\mathrm{N}*\\mathrm{H}*\\mathrm{W}*$   num anchors, ) \nReturns  A single float number indicating the positive recall. \nReturn type  Tensor \ncollect loss level single ( cls_loss ,  reg_loss ,  assigned gt in ds ,  labels_seq ) Get the average loss in each FPN level w.r.t. each gt label. \nParameters \n•  cls_loss  ( Tensor ) – Classification loss of each feature map pixel, shape (num_anchor, num_class) •  reg_loss  ( Tensor ) – Regression loss of each feature map pixel, shape (num_anchor, 4) •  assigned gt in ds  ( Tensor ) – It indicates which gt the prior is assigned to (0-based, -1: no assignment). shape (num_anchor), •  labels_seq  – The rank of labels. shape (num_gt) \nReturns  (num_gt), average loss of each gt in this level \nReturn type  shape \nforward single  $(x)$  Forward feature map of a single scale level. \nParameters  x  ( Tensor ) – Feature map of a single scale level. \nReturns \ncls_score (Tensor): Box scores for each scale level  Has shape (N, num_points \\* num classes, H, W). bbox_pred (Tensor): Box energies / deltas for each scale  level with shape (N, num_points \\* 4, H, W). Return type  tuple (Tensor) \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute loss of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num_points \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num_points \\* 4, H, W). •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. "}
{"page": 343, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_343.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ngt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\nreweight_loss_single(cls_loss, reg_loss, assigned_gt_inds, labels, level, min_levels)\nReweight loss values at each level.\n\nReassign loss values at each level by masking those where the pre-calculated loss is too large. Then return\nthe reduced losses.\n\nParameters\n\ncls_loss (Tensor) — Element-wise classification loss. Shape: (num_anchors,\nnum_classes)\n\nreg_loss (Tensor) — Element-wise regression loss. Shape: (num_anchors, 4)\n\nassigned_gt_inds (Tensor) — The gt indices that each anchor bbox is assigned to. -1\ndenotes a negative anchor, otherwise it is the gt index (0-based). Shape: (num_anchors, ),\n\nlabels (Tensor) — Label assigned to anchors. Shape: (num_anchors, ).\nlevel (int) — The current level index in the pyramid (0-4 for RetinaNet)\n\nmin_levels (Tensor) — The best-matching level for each gt. Shape: (num_gts, ),\n\nReturns\n\ncls_loss: Reduced corrected classification loss. Scalar.\nreg_loss: Reduced corrected regression loss. Scalar.\n\npos_flags (Tensor): Corrected bool tensor indicating the final positive anchors. Shape:\n(num_anchors, ).\n\nReturn type tuple\n\nclass mmdet.models.dense_heads.FeatureAdaption(in_channels, out_channels, kernel_size=3,\n\ndeform_groups=4, init_cfg={'layer': 'Conv2d',\n‘override’: {‘name': 'conv_adaption', ‘std’: 0.01, 'type':\n‘Normal'}, 'std': 0.1, ‘type’: 'Normal'})\n\nFeature Adaption Module.\n\nFeature Adaption Module is implemented based on DCN v1. It uses anchor shape prediction rather than feature\nmap to predict offsets of deform conv layer.\n\nParameters\n\n¢ in_channels (int) — Number of channels in the input feature map.\n\n* out_channels (int) — Number of channels in the output feature map.\n\n¢ kernel_size (int) — Deformable conv kernel size.\n\n¢ deform_groups (int) — Deformable conv group size.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (x, shape)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n336\n\nChapter 39. mmdet.models\n", "vlm_text": "•  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nre weight loss single ( cls_loss ,  reg_loss ,  assigned gt in ds ,  labels ,  level ,  min_levels ) Reweight loss values at each level. \nReassign loss values at each level by masking those where the pre-calculated loss is too large. Then return the reduced losses. \nParameters \n•  cls_loss  ( Tensor ) – Element-wise classification loss. Shape: (num anchors, num classes) •  reg_loss  ( Tensor ) – Element-wise regression loss. Shape: (num anchors, 4) •  assigned gt in ds  ( Tensor ) – The gt indices that each anchor bbox is assigned to. -1 denotes a negative anchor, otherwise it is the gt index (0-based). Shape: (num anchors, ), •  labels  ( Tensor ) – Label assigned to anchors. Shape: (num anchors, ). •  level  ( int ) – The current level index in the pyramid (0-4 for RetinaNet) •  min_levels  ( Tensor ) – The best-matching level for each gt. Shape: (num_gts, ), \nReturns \n• cls_loss: Reduced corrected classification loss. Scalar. • reg_loss: Reduced corrected regression loss. Scalar. • pos_flags (Tensor): Corrected bool tensor indicating the final positive anchors. Shape: (num anchors, ). \nReturn type  tuple \nclass  mmdet.models.dense heads. Feature Adaption ( in channels ,  out channels ,  kernel size  ${\\scriptstyle\\cdot=}3$  , deform groups  $\\scriptstyle{\\prime}=4$  ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'name': 'con v adaption', 'std': 0.01, 'type': 'Normal'}, 'std': 0.1, 'type': 'Normal'} ) \nFeature Adaption Module. \nFeature Adaption Module is implemented based on DCN v1. It uses anchor shape prediction rather than feature map to predict offsets of deform conv layer. \nParameters \n•  in channels  ( int ) – Number of channels in the input feature map. •  out channels  ( int ) – Number of channels in the output feature map. •  kernel size  ( int ) – Deformable conv kernel size. •  deform groups  ( int ) – Deformable conv group size. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( x ,  shape ) Defines the computation performed at every call. Should be overridden by all subclasses. "}
{"page": 344, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_344.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.dense_heads .FoveaHead (num_classes, in_channels, base_edge_list=(16, 32, 64, 128,\n256), scale_ranges=((8, 32), (16, 64), (32, 128), (64, 256),\n(128, 512)), sigma=0.4, with_deform=False,\ndeform_groups=4, init_cfg={'layer': 'Conv2d'’, ‘override’:\n{‘bias_prob': 0.01, ‘name’: 'conv_cls', 'std': 0.01, 'type':\n‘Normal'}, ‘std’: 0.01, 'type': 'Normal'}, **kwargs)\nFoveaBox: Beyond Anchor-based Object Detector https://arxiv.org/abs/1904.03797\n\nforward_single(x)\nForward features of a single scale level.\n\nParameters x (Tensor) — FPN feature maps of the specified stride.\nReturns\n\nScores for each class, bbox predictions, features after classification and regression conv\nlayers, some models needs these features like FCOS.\n\nReturn type tuple\n\nget_targets(gt_bbox_list, gt_label_list, featmap_sizes, points)\nCompute regression, classification and centerness targets for points in multiple images.\n\nParameters\n* points (list [Tensor ]) — Points of each fpn level, each has shape (num_points, 2).\n\n* gt_bboxes_list (list [Tensor ]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\n\n¢ gt_labels_list (list [Tensor]) — Ground truth labels of each box, each has shape\n(num_gt,).\n\nloss (cls_scores, bbox_preds, gt_bbox_list, gt_label_list, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box scores for each scale level, each is a 4D-tensor, the\nchannel number is num_points * num_classes.\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n39.4. dense_heads 337\n", "vlm_text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.dense heads. FoveaHead ( num classes ,  in channels ,  base edge list  $=$  (16, 32, 64, 128, 256) ,  scale_ranges=((8, 32), (16, 64), (32, 128), (64, 256), (128, 512)) ,  sigma=0.4 ,  with deform  $\\leftrightharpoons$  False , deform group  $\\scriptstyle{\\mathfrak{s}}=4$  ,  init_cfg  $\\scriptstyle{\\tilde{}}=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) \nFoveaBox: Beyond Anchor-based Object Detector  https://arxiv.org/abs/1904.03797 \nforward single  $(x)$  Forward features of a single scale level. \nParameters  x  ( Tensor ) – FPN feature maps of the specified stride. \nReturns \nScores for each class, bbox predictions, features  after classification and regression conv layers, some models needs these features like FCOS. \nReturn type  tuple \nget targets ( gt b box list ,  gt label list ,  feat map sizes ,  points ) Compute regression, classification and centerness targets for points in multiple images. \nParameters \n•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). \nloss ( cls_scores ,  bbox_preds ,  gt b box list ,  gt label list ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute loss of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. "}
{"page": 345, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_345.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads.FreeAnchorRetinaHead (num_classes, in_channels, stacked_convs=4,\n\nconv_cfg=None, norm_cfg=None,\npre_anchor_topk=50, bbox_thr=0.6,\ngamma=2.0, alpha=0.5, **kwargs)\n\nFreeAnchor RetinaHead used in https://arxiv.org/abs/1909.02466.\n\nParameters\n\nnum_classes (int) — Number of categories excluding the background category.\nin_channels (int) — Number of channels in the input feature map.\n\nstacked_convs (int) — Number of conv layers in cls and reg tower. Default: 4.\nconv_cfg (dict) — dictionary to construct and config conv layer. Default: None.\n\nnorm_cfg (dict) - dictionary to construct and config norm layer. Default:\nnorm_cfg=dict(type=’GN’, num_groups=32, requires_grad=True).\n\npre_anchor_topk (int) — Number of boxes that be token in each bag.\n\nbbox_thr (float) — The threshold of the saturated linear function. It is usually the same\nwith the IoU threshold used in NMS.\n\ngamma (float) —- Gamma parameter in focal loss.\n\nalpha (float) — Alpha parameter in focal loss.\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\nnegative_bag_loss(cls_prob, box_prob)\nCompute negative bag loss.\n\nFL((1-\n\nPajeAy!\n\nPajea,) * (1 — PY).\n\nBox_probability of matched samples.\n\nPv : Classification probability of negative samples.\n\nParameters\n\n¢ cls_prob (Tensor) — Classification probability, in shape (num_img, num_anchors,\nnum_classes).\n\n338\n\nChapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.dense heads. Free Anchor Retina Head ( num classes ,  in channels ,  stacked con v  $\\mathfrak{z}{=}4$  , \nconv_cfg  $=$  None ,  norm_cfg  $\\mathbf{\\hat{\\Sigma}}$  None , pre anchor top  $k{=}50$  ,  bbox_th  $\\it{r=0.6}$  , gamma  $\\it{:=2.0}$  ,  alpha  $\\mathord{=}\\!O.5$  ,  \\*\\*kwargs ) \nFreeAnchor RetinaHead used in  https://arxiv.org/abs/1909.02466 . \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of conv layers in cls and reg tower. Default: 4. •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. Default: None. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $=$  dict(type  $\\risingdotseq$  ’GN’, num_group  $\\wp{=}32$  , requires grad  $\\risingdotseq$  True). •  pre anchor top k  ( int ) – Number of boxes that be token in each bag. •  bbox_thr  ( float ) – The threshold of the saturated linear function. It is usually the same with the IoU threshold used in NMS. •  gamma  ( float ) – Gamma parameter in focal loss. •  alpha  ( float ) – Alpha parameter in focal loss. \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components.  dict[str, Tensor] \nnegative bag loss ( cls_prob ,  box_prob ) Compute negative bag loss.  $F L((1-P_{a_{j}\\in A_{+}})*(1-P_{j}^{b g})).$   $P_{a_{j}\\in A_{+}}$  : Box probability of matched samples.  $P_{j}^{b g}$  : Classification probability of negative samples. \nParameters \n•  cls_prob  ( Tensor ) – Classification probability, in shape (num_img, num anchors, num classes). "}
{"page": 346, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_346.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* box_prob (Tensor) — Box probability, in shape (num_img, num_anchors, num_classes).\nReturns Negative bag loss in shape (num_img, num_anchors, num_classes).\nReturn type Tensor\n\npositive_bag_loss(matched_cls_prob, matched_box_prob)\nCompute positive bag loss.\n\n—log(Mean — max(Péi* * Pi?°)).\n\nPs: matched_cls_prob, classification probability of matched samples.\n\nPige: matched_box_prob, box probability of matched samples.\nParameters\n\n* matched_cls_prob (Tensor) — Classification probability of matched samples in shape\n(num_gt, pre_anchor_topk).\n\n* matched_box_prob (Tensor) — BBox probability of matched samples, in shape (num_gt,\npre_anchor_topk).\n\nReturns Positive bag loss in shape (num_gt,).\nReturn type Tensor\n\nclass mmdet.models.dense_heads.GARPNHead (in_channels, init_cfg={‘layer': 'Conv2d', ‘override’:\n{‘bias_prob': 0.01, 'name': 'conv_loc’, 'std': 0.01, ‘type’:\n‘Normal'}, ‘std’: 0.01, 'type': 'Normal'}, **kwargs)\nGuided-Anchor-based RPN head.\nforward_single(x)\nForward feature of a single scale level.\n\nloss (cls_scores, bbox_preds, shape_preds, loc_preds, gt_bboxes, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss. Default: None\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nclass mmdet.models.dense_heads.GARetinaHead (num_classes, in_channels, stacked_convs=4,\nconv_cfg=None, norm_cfg=None, init_cfg=None,\n**kwargs)\nGuided-Anchor-based RetinaNet head.\n\n39.4. dense_heads 339\n", "vlm_text": "•  box_prob  ( Tensor ) – Box probability, in shape (num_img, num anchors, num classes). \nReturns  Negative bag loss in shape (num_img, num anchors, num classes). \nReturn type  Tensor \npositive bag loss(matched cls pro b, matched box pro b)Compute positive bag loss. \n\n$$\n-l o g(M e a n-m a x(P_{i j}^{c l s}*P_{i j}^{l o c})).\n$$\n \n $P_{i j}^{c l s}$  : matched cls pro b, classification probability of matched samples.  $P_{i j}^{l o c}$  : matched box pro b, box probability of matched samples. \n\nParameters \n•  matched cls pro b  ( Tensor ) – Classification probability of matched samples in shape (num_gt, pre anchor top k). •  matched box pro b  ( Tensor ) – BBox probability of matched samples, in shape (num_gt, pre anchor top k). \nReturns  Positive bag loss in shape (num_gt,). \nReturn type  Tensor \nclass  mmdet.models.dense heads. GARPNHead ( in channels ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_loc', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) \nGuided-Anchor-based RPN head. \nforward single  $(x)$  Forward feature of a single scale level. \nloss ( cls_scores ,  bbox_preds ,  shape p reds ,  loc_preds ,  gt_bboxes ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None \n\nReturn type  dict[str, Tensor] \nclass  mmdet.models.dense heads. GA Retina Head ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{:=4}$  , conv_cfg  $\\leftrightharpoons$  None ,  norm_cfg  $\\mathbf{\\beta}=$  None ,  init_cfg  $=$  None , \\*\\*kwargs ) \nGuided-Anchor-based RetinaNet head. "}
{"page": 347, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_347.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward_single(x)\nForward feature map of a single scale level.\n\nclass mmdet.models.dense_heads .GFLHead (num_classes, in_channels, stacked_convs=4, conv_cfg=None,\n\nnorm_cfg={‘num_groups': 32, 'requires_grad': True, ‘type’:\n'GN'}, loss_dfl={'loss_weight': 0.25, 'type':\n‘DistributionFocalLoss'}, bbox_coder={ 'type':\n‘DistancePointBBoxCoder'}, reg_max=16, init_cfg={'layer':\n‘Conv2d', ‘override’: {'bias_prob': 0.01, ‘name’: 'gfi_cls’, ‘std’:\n0.01, ‘type’: 'Normal'}, ‘std’: 0.01, ‘type’: 'Normal'}, **kwargs)\nGeneralized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.\n\nGFL head structure is similar with ATSS, however GFL uses 1) joint representation for classification and local-\nization quality, and 2) flexible General distribution for bounding box locations, which are supervised by Quality\nFocal Loss (QFL) and Distribution Focal Loss (DFL), respectively\n\nhttps://arxiv.org/abs/2006.04388\nParameters\n* num_classes (int) — Number of categories excluding the background category.\n¢ in_channels (int) — Number of channels in the input feature map.\n* stacked_convs (int) — Number of conv layers in cls and reg tower. Default: 4.\n* conv_cfg (dict) — dictionary to construct and config conv layer. Default: None.\n\n* norm_cfg (dict) — dictionary to construct and config norm layer. Default: dict(type=’GN’,\nnum_groups=32, requires_grad=True).\n\n* loss_qfl (dict) — Config of Quality Focal Loss (QFL).\n* bbox_coder (dict) — Config of bbox coder. Defaults ‘DistancePointBBoxCoder’.\n\n* reg_max (int) — Max value of integral set :math: /0, ..., reg_max} in QFL setting. Default:\n16.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nExample\n\n>>> self = GFLHead(11, 7)\n\n>>> feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]\n>>> cls_quality_score, bbox_pred = self. forward(feats)\n\n>>> assert len(cls_quality_score) == len(self.scales)\n\nanchor_center (anchors)\nGet anchor centers from anchors.\n\nParameters anchors (Tensor) — Anchor list with shape (N, 4), “xyxy” format.\nReturns Anchor centers with shape (N, 2), “xy” format.\nReturn type Tensor\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\n340\n\nChapter 39. mmdet.models\n\n", "vlm_text": "forward single  $(x)$  Forward feature map of a single scale level. \nclass  mmdet.models.dense heads. GFLHead ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  ,  conv_cfg=None , norm_cfg  $=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  loss_dfl={'loss weight': 0.25, 'type': 'Distribution Focal Loss'} ,  bbox_coder={'type': 'Distance Point B Box Code r'} ,  reg_max=16 ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'gfl_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) \nGeneralized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection. \nGFL head structure is similar with ATSS, however GFL uses 1) joint representation for classification and local- ization quality, and 2) flexible General distribution for bounding box locations, which are supervised by Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), respectively \nhttps://arxiv.org/abs/2006.04388 \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of conv layers in cls and reg tower. Default: 4. •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. Default: None. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: dict(type=’GN’, num_groups  $_{:=32}$  , requires grad  $\\scriptstyle\\varepsilon=$  True). •  loss_qfl  ( dict ) – Config of Quality Focal Loss (QFL). •  bbox_coder  ( dict ) – Config of bbox coder. Defaults ‘Distance Point B Box Code r’. •  reg_max  ( int ) – Max value of integral set :math:  {0, ..., reg_max}  in QFL setting. Default: 16. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nExample \nThis image shows a snippet of Python code using the PyTorch library. Here’s a breakdown:\n\n1. `self = GFLHead(11, 7)`: An instance of `GFLHead` is being created with parameters 11 and 7.\n2. `feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]`: A list comprehension creating random tensors of shape `(1, 7, s, s)` for each `s` in the list `[4, 8, 16, 32, 64]`.\n3. `cls_quality_score, bbox_pred = self.forward(feats)`: The method `forward` is called on `self` with `feats` as input, and it returns two values: `cls_quality_score` and `bbox_pred`.\n4. `assert len(cls_quality_score) == len(self.scales)`: An assertion checks if the length of `cls_quality_score` is equal to the length of `self.scales`. \n\nThis code is likely a part of a testing script or a model simulation involving object detection or feature map processing in a neural network context.\nGet anchor centers from anchors. Parameters  anchors  ( Tensor ) – Anchor list with shape (N, 4), “xyxy” format. Returns  Anchor centers with shape (N, 2), “xy” format. Return type  Tensor \nforward ( feats ) \nForward features from the upstream network. Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. "}
{"page": 348, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_348.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturns\nUsually a tuple of classification scores and bbox prediction\n\ncls_scores (list{Tensor]): Classification and quality (IoU) joint scores for all scale lev-\nels, each is a 4D-tensor, the channel number is num_classes.\n\nbbox_preds (list[Tensor]): Box distribution logits for all scale levels, each is a 4D-\ntensor, the channel number is 4*(n+1), n is max value of integral set.\n\nReturn type tuple\n\nforward_single(x, scale)\nForward feature of a single scale level.\n\nParameters\n\n¢ x (Tensor) — Features of a single scale level.\n\n* € (scale) — obj: mmcv.cnn.Scale): Learnable scale module to resize the bbox prediction.\nReturns\n\ncls_score (Tensor): Cls and quality joint scores for a single scale level the channel num-\nber is num_classes.\n\nbbox_pred (Tensor): Box distribution logits for a single scale level, the channel number\nis 4*(n+1), n is max value of integral set.\n\nReturn type tuple\n\nget_targets(anchor_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list=None,\ngt_labels_list=None, label_channels=1, unmap_outputs=True)\nGet targets for GFL head.\n\nThis method is almost the same as AnchorHead. get_targets(). Besides returning the targets as the parent\nmethod does, it also returns the anchors as the first element of the returned tuple.\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor]) — Cls and quality scores for each scale level has shape (N,\nnum_classes, H, W).\n\n¢ bbox_preds (list [Tensor ])-— Box distribution logits for each scale level with shape (N,\n4*(n+1), H, W), nis max value of integral set.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\ngt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (list[Tensor] | None) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single (anchors, cls_score, bbox_pred, labels, label_weights, bbox_targets, stride, num_total_samples)\nCompute loss of a single scale level.\n\n39.4. dense_heads 341\n", "vlm_text": "Returns \nUsually a tuple of classification scores and bbox prediction \ncls_scores (list[Tensor]): Classification and quality (IoU)  joint scores for all scale lev- els, each is a 4D-tensor, the channel number is num classes. bbox_preds (list[Tensor]): Box distribution logits for all  scale levels, each is a 4D- tensor, the channel number is  $4^{*}(\\mathsf{n}\\!+\\!1)$  , n is max value of integral set. \nReturn type  tuple \nforward single ( x ,  scale ) Forward feature of a single scale level. \nParameters \n•  x  ( Tensor ) – Features of a single scale level. •  (  ( scale ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. \nReturns \ncls_score (Tensor): Cls and quality joint scores for a single  scale level the channel num- ber is num classes. bbox_pred (Tensor): Box distribution logits for a single scale  level, the channel number is  $4^{*}(\\mathsf{n}\\!+\\!1)$  , n is max value of integral set. \nReturn type  tuple \nget targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $t{=}$  None , gt labels list  $=$  None ,  label channels  $\\scriptstyle{\\prime=}I$  ,  un map outputs  $=$  True ) Get targets for GFL head. \nThis method is almost the same as  AnchorHead.get targets() . Besides returning the targets as the parent method does, it also returns the anchors as the first element of the returned tuple. \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Cls and quality scores for each scale level has shape (N, num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box distribution logits for each scale level with shape (N,  $4^{*}(\\mathsf{n}\\!+\\!1)$  , H, W), n is max value of integral set. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – specify which bounding boxes can be ignored when computing the loss. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nloss single ( anchors ,  cls_score ,  bbox_pred ,  labels ,  label weights ,  b box targets ,  stride ,  num total samples ) Compute loss of a single scale level. "}
{"page": 349, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_349.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\n* anchors (Tensor) — Box reference for each scale level with shape (N, num_total_anchors,\n\n4).\n\n* cls_score (Tensor) — Cls and quality joint scores for each scale level has shape (N,\n\nnum_classes, H, W).\n\n* bbox_pred (Tensor) - Box distribution logits for each scale level with shape (N, 4*(n+1),\n\nH, W), n is max value of integral set.\n\n¢ labels (Tensor) — Labels of each anchors with shape (N, num_total_anchors).\n\n¢ label_weights (Tensor) — Label weights of each anchor with shape (N,\n\nnum_total_anchors)\n\n¢ bbox_targets (Tensor) — BBox regression targets of each anchor weight shape (N,\n\nnum_total_anchors, 4).\n\n¢ stride (tuple) — Stride in this scale level.\n\n* num_total_samples (int) — Number of positive samples that is reduced over all GPUs.\n\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\nclass mmdet.models.dense_heads .GuidedAnchorHead (num_classes, in_channels, feat_channels=256,\n\napprox_anchor_generator={'octave_base_scale': 8,\n‘ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3,\n‘strides’: [4, 8, 16, 32, 64], ‘type’: 'AnchorGenerator'},\nsquare_anchor_generator={ 'ratios': [1.0], ‘scales’:\n[8], ‘strides’: [4, 8, 16, 32, 64], ‘type’:\n‘AnchorGenerator'}, anchor_coder={ ‘target_means':\n[0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0],\n‘type’: 'DeltaXYWHBBoxCoder'},\n\nbbox_coder={ 'target_means': [0.0, 0.0, 0.0, 0.0],\n‘target_stds': [1.0, 1.0, 1.0, 1.0], ‘type’:\n‘DeltaXYWHBBoxCoder'}, reg_decoded_bbox=False,\ndeform_groups=4, loc_filter_thr=0.01,\ntrain_cfg=None, test_cfg=None, loss_loc={‘alpha':\n0.25, 'gamma': 2.0, 'loss_weight': 1.0, 'type':\n‘FocalLoss', 'use_sigmoid': True}, loss_shape={'beta':\n0.2, ‘loss_weight': 1.0, 'type': 'BoundedIoULoss'},\nloss_cls={‘loss_weight': 1.0, 'type':\n‘CrossEntropyLoss’, 'use_sigmoid': True},\nloss_bbox=({'beta': 1.0, 'loss_weight': 1.0, 'type':\n‘SmoothL1Loss'}, init_cfg={‘layer': 'Conv2d',\n‘override’: {'bias_prob': 0.01, ‘name’: 'conv_loc',\n‘std’: 0.01, ‘type’: 'Normal'}, ‘std’: 0.01, ‘type’:\n‘Normal'})\n\nGuided-Anchor-based head (GA-RPN, GA-RetinaNet, etc.).\n\nThis GuidedAnchorHead will predict high-quality feature guided anchors and locations where anchors will be\nkept in inference. There are mainly 3 categories of bounding-boxes.\n\n* Sampled 9 pairs for target assignment. (approxes)\n\n* The square boxes where the predicted anchors are based on. (squares)\n\n* Guided anchors.\n\n342\n\nChapter 39. mmdet.models\n", "vlm_text": "Parameters \n•  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  cls_score  ( Tensor ) – Cls and quality joint scores for each scale level has shape (N, num classes, H, W). •  bbox_pred  ( Tensor ) – Box distribution logits for each scale level with shape (N  ${\\bf\\Phi},4{\\bf\\Phi}^{*}({\\bf n}{+}1)$  , H, W), n is max value of integral set. •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  stride  ( tuple ) – Stride in this scale level. •  num total samples  ( int ) – Number of positive samples that is reduced over all GPUs. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \n\napprox anchor generator={'octave base scale': 8, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [4, 8, 16, 32, 64], 'type': 'AnchorGenerator'} , square anchor generator={'ratios': [1.0], 'scales': [8], 'strides': [4, 8, 16, 32, 64], 'type': 'Anchor Generator'} ,  anchor code r={'target means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} , bbox_coder={'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box=False , deform groups=4 ,  loc filter thr=0.01 , train_cfg=None ,  test_cfg=None ,  loss_loc={'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True}, loss_shape={'beta':0.2, 'loss weight': 1.0, 'type': 'Bounded I oU Loss'} , loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_bbox={'beta': 1.0, 'loss weight': 1.0, 'type':'Smooth L 1 Loss'} ,  init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'conv_loc', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ) \nGuided-Anchor-based head (GA-RPN, GA-RetinaNet, etc.). \nThis Guided Anchor Head will predict high-quality feature guided anchors and locations where anchors will be kept in inference. There are mainly 3 categories of bounding-boxes. \n• Sampled 9 pairs for target assignment. (approxes) • The square boxes where the predicted anchors are based on. (squares) • Guided anchors. "}
{"page": 350, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_350.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nPlease refer to https://arxiv.org/abs/1901.03278 for more details.\nParameters\n\n* num_classes (int) — Number of classes.\n¢ in_channels (int) — Number of channels in the input feature map.\n¢ feat_channels (int) — Number of hidden channels.\n* approx_anchor_generator (dict) — Config dict for approx generator\n* square_anchor_generator (dict) — Config dict for square generator\n* anchor_coder (dict) — Config dict for anchor coder\n* bbox_coder (dict) — Config dict for bbox coder\n\n* reg_decoded_bbox (boo1) - If true, the regression loss would be applied directly on de-\ncoded bounding boxes, converting both the predicted boxes and regression targets to abso-\nlute coordinates format. Default False. It should be True when using JoULoss, GloULoss,\nor DIoULoss in the bbox head.\n\n¢ deform_groups — (int): Group number of DCN in FeatureAdaption module.\n* loc_filter_thr (float) — Threshold to filter out unconcerned regions.\n\n* loss_loc (dict) — Config of location loss.\n\n* loss_shape (dict) — Config of anchor shape loss.\n\n* loss_cls (dict) — Config of classification loss.\n\n* loss_bbox (dict) — Config of bbox regression loss.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\nA tuple of classification scores and bbox prediction.\n\n¢ cls_scores (list{Tensor]): Classification scores for all scale levels, each is a 4D-tensor, the\nchannels number is num_base_priors * num_classes.\n\n* bbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-tensor, the\nchannels number is num_base_priors * 4.\n\nReturn type tuple\n\nforward_single(x)\nForward feature of a single scale level.\n\nParameters x (Tensor) — Features of a single scale level.\n\nReturns cls_score (Tensor): Cls scores for a single scale level the channels number is\nnum_base_priors * num_classes. bbox_pred (Tensor): Box energies / deltas for a single scale\nlevel, the channels number is num_base_priors * 4.\n\nReturn type tuple\n\n39.4. dense_heads 343\n", "vlm_text": "Please refer to  https://arxiv.org/abs/1901.03278  for more details. \nParameters \n•  num classes  ( int ) – Number of classes. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. •  approx anchor generator  ( dict ) – Config dict for approx generator •  square anchor generator  ( dict ) – Config dict for square generator •  anchor code r  ( dict ) – Config dict for anchor coder •  bbox_coder  ( dict ) – Config dict for bbox coder •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  deform groups  – (int): Group number of DCN in Feature Adaption module. •  loc filter thr  ( float ) – Threshold to filter out unconcerned regions. •  loss_loc  ( dict ) – Config of location loss. •  loss_shape  ( dict ) – Config of anchor shape loss. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of bbox regression loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) \nForward features from the upstream network. Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nA tuple of classification scores and bbox prediction. • cls_scores (list[Tensor]): Classification scores for all scale levels, each is a 4D-tensor, the channels number is num base priors \\* num classes. • bbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-tensor, the channels number is num base priors  $^{*}\\,4$  . Return type  tuple \nforward single  $(x)$  \nForward feature of a single scale level. \nParameters  x  ( Tensor ) – Features of a single scale level. Returns  cls_score (Tensor): Cls scores for a single scale level the channels number is num base priors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num base priors  $^{*}\\,4$  . Return type  tuple "}
{"page": 351, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_351.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nga_loc_targets(gt_bboxes_list, featmap_sizes)\nCompute location targets for guided anchoring.\n\nEach feature map is divided into positive, negative and ignore regions. - positive regions: target 1, weight\n1 - ignore regions: target 0, weight 0 - negative regions: target 0, weight 0.1\n\nParameters\n\n* gt_bboxes_list (list [Tensor]) — Gt bboxes of each image.\n\n¢ featmap_sizes (list [tuple]) — Multi level sizes of each feature maps.\nReturns tuple\n\nga_shape_targets (approx_list, inside_flag_list, square_list, gt_bboxes_list, img_metas,\ngt_bboxes_ignore_list=None, unmap_outputs=True)\nCompute guided anchoring targets.\n\nParameters\n* approx_list (list [list]) — Multi level approxs of each image.\n¢ inside_flag_list (list [list ]) — Multi level inside flags of each image.\n* square_list (list [list ]) — Multi level squares of each image.\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes of each image.\n¢ img_metas (list [dict ]) — Meta info of each image.\n* gt_bboxes_ignore_list (list [Tensor]) — ignore list of gt bboxes.\n* unmap_outputs (bool) — unmap outputs or not.\nReturns tuple\n\nget_anchors (featmap_sizes, shape_preds, loc_preds, img_metas, use_loc_filter=False, device='cuda')\nGet squares according to feature map sizes and guided anchors.\n\nParameters\n\n¢ featmap_sizes (list [tuple]) — Multi-level feature map sizes.\n\n¢ shape_preds (list [tensor]) — Multi-level shape predictions.\n\n* loc_preds (list [tensor]) — Multi-level location predictions.\n\n¢ img_metas (list [dict ]) — Image meta info.\n\n* use_loc_filter (bool) — Use loc filter or not.\n\n¢ device (torch.device | str) — device for returned tensors\nReturns\n\nsquare approxs of each image, guided anchors of each image, loc masks of each image\nReturn type tuple\n\nget_bboxes (cls_scores, bbox_preds, shape_preds, loc_preds, img_metas, cfg=None, rescale=False)\nTransform network outputs of a batch into bbox results.\n\nNote: When score_factors is not None, the cls_scores are usually multiplied by it then obtain the real score\nused in NMS, such as CenterNess in FCOS, IoU branch in ATSS.\n\nParameters\n\n¢ cls_scores (list [Tensor]) — Classification scores for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * num_classes, H, W).\n\n344 Chapter 39. mmdet.models\n", "vlm_text": "ga loc targets ( gt b boxes list ,  feat map sizes ) Compute location targets for guided anchoring. \nEach feature map is divided into positive, negative and ignore regions. - positive regions: target 1, weight 1 - ignore regions: target 0, weight 0 - negative regions: target 0, weight 0.1 \nParameters \n•  gt b boxes list  ( list[Tensor] ) – Gt bboxes of each image. •  feat map sizes  ( list[tuple] ) – Multi level sizes of each feature maps. \nReturns  tuple \nga shape targets ( approx list ,  inside flag list ,  square list ,  gt b boxes list ,  img_metas , gt b boxes ignore list $=$ None, un map output $\\mathfrak{s}=$ True)Compute guided anchoring targets. \nParameters \n•  approx list  ( list[list] ) – Multi level approxs of each image. •  inside flag list  ( list[list] ) – Multi level inside flags of each image. •  square list  ( list[list] ) – Multi level squares of each image. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – ignore list of gt bboxes. •  un map outputs  ( bool ) – unmap outputs or not. \nReturns  tuple \nget anchors ( feat map sizes ,  shape p reds ,  loc_preds ,  img_metas ,  use loc filter=False ,  device  $=$  'cuda' ) Get squares according to feature map sizes and guided anchors. \nParameters \n•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  shape p reds  ( list[tensor] ) – Multi-level shape predictions. •  loc_preds  ( list[tensor] ) – Multi-level location predictions. •  img_metas  ( list[dict] ) – Image meta info. •  use loc filter  ( bool ) – Use loc filter or not. •  device  ( torch.device | str ) – device for returned tensors \nReturns \nsquare approxs of each image, guided anchors of each image,  loc masks of each image Return type  tuple \nget_bboxes ( cls_scores ,  bbox_preds ,  shape p reds ,  loc_preds ,  img_metas ,  cfg  $\\mathbf{\\beta}=$  None ,  rescale  $=$  False ) Transform network outputs of a batch into bbox results. \nNote: When score factors is not None, the cls_scores are usually multiplied by it then obtain the real score used in NMS, such as CenterNess in FCOS, IoU branch in ATSS. \nParameters \n•  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). "}
{"page": 352, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_352.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * 4, H, W).\n\n* score_factors (list[Tensor], Optional) — Score factor for all scale level, each is\na 4D-tensor, has shape (batch_size, num_priors * 1, H, W). Default None.\n\n¢ img_metas (list[dict], Optional) — Image meta info. Default None.\n\n* cfg (mmcv.Config, Optional) — Test / postprocessing configuration, if None, test_cfg\nwould be used. Default None.\n\n* rescale (bool) — If True, return boxes in original image space. Default False.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default True.\nReturns\n\nEach item in result_list is 2-tuple. The first item is an (n, 5) tensor, where the first 4\ncolumns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\nbetween 0 and 1. The second item is a (n,) tensor where each item is the predicted class\nlabel of the corresponding box.\n\nReturn type list[list{Tensor, Tensor]]\n\nget_sampled_approxs (featmap_sizes, img_metas, device='cuda')\nGet sampled approxs and inside flags according to feature map sizes.\n\nParameters\n¢ featmap_sizes (list [tuple]) — Multi-level feature map sizes.\n¢ img_metas (list [dict ]) — Image meta info.\n¢ device (torch.device | str) — device for returned tensors\nReturns approxes of each image, inside flags of each image\nReturn type tuple\n\nloss (cls_scores, bbox_preds, shape_preds, loc_preds, gt_bboxes, gt_labels, img_metas,\ngt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss. Default: None\n\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\n39.4. dense_heads 345\n", "vlm_text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  score factors  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, num_priors \\* 1, H, W). Default None. •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. \nReturns \nEach item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. \nReturn type  list[list[Tensor, Tensor]] \nget sampled approx s ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get sampled approxs and inside flags according to feature map sizes. \nParameters \n•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – device for returned tensors \nReturns  approxes of each image, inside flags of each image \nReturn type  tuple \nloss ( cls_scores ,  bbox_preds ,  shape p reds ,  loc_preds ,  gt_bboxes ,  gt_labels ,  img_metas , gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] "}
{"page": 353, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_353.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads .LDHead(num_classes, in_channels, loss_ld={'T': 10, ‘loss_weight': 0.25,\n‘type’: '‘LocalizationDistillationLoss'}, **kwargs)\nLocalization distillation Head. (Short description)\n\nIt utilizes the learned bbox distributions to transfer the localization dark knowledge from teacher to student.\nOriginal paper: Localization Distillation for Object Detection.\n\nParameters\n* num_classes (int) — Number of categories excluding the background category.\n¢ in_channels (int) — Number of channels in the input feature map.\n\n* loss_ld (dict) — Config of Localization Distillation Loss (LD), T is the temperature for\ndistillation.\n\nforward_train(x, out_teacher, img_metas, gt_bboxes, gt_labels=None, gt_bboxes_ignore=None,\nproposal_cfg=None, **kwargs)\n\nParameters\n¢ x (list [Tensor ]) — Features from FPN.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes (Tensor) — Ground truth bboxes of the image, shape (num_gts, 4).\n¢ gt_labels (Tensor) — Ground truth labels of each box, shape (num_gts,).\n\n* gt_bboxes_ignore (Tensor) -— Ground truth bboxes to be ignored, shape\n(num_ignored_gts, 4).\n\n* proposal_cfg (mmcv.Config) — Test / postprocessing configuration, if None, test_cfg\nwould be used\n\nReturns\nThe loss components and proposals of each image.\n¢ losses (dict[str, Tensor]): A dictionary of loss components.\n* proposal_list (list{Tensor]): Proposals of each image.\nReturn type tuple[dict, list]\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, soft_target, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor]) — Cls and quality scores for each scale level has shape (N,\nnum_classes, H, W).\n\n¢ bbox_preds (list [Tensor ])— Box distribution logits for each scale level with shape (N,\n4*(n+1), H, W), nis max value of integral set.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n346 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.dense heads. LDHead ( num classes ,  in channels ,  loss_ld={'T': 10, 'loss_weight': 0.25, 'type': 'Localization Distillation Loss'} ,  \\*\\*kwargs ) \nLocalization distillation Head. (Short description) \nIt utilizes the learned bbox distributions to transfer the localization dark knowledge from teacher to student. Original paper:  Localization Distillation for Object Detection. \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  loss_ld  ( dict ) – Config of Localization Distillation Loss (LD), T is the temperature for distillation. \nforward train ( x ,  out teacher ,  img_metas ,  gt_bboxes ,  gt_labels  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  None ,  gt b boxes ignore=None , proposal cf g  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ,  \\*\\*kwargs ) \nParameters \n•  x  ( list[Tensor] ) – Features from FPN. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt_bboxes  ( Tensor ) – Ground truth bboxes of the image, shape (num_gts, 4). •  gt_labels  ( Tensor ) – Ground truth labels of each box, shape (num_gts,). •  gt b boxes ignore ( Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). •  proposal cf g  ( mmcv.Config ) – Test / post processing configuration, if None, test_cfg would be used \nReturns \nThe loss components and proposals of each image. • losses (dict[str, Tensor]): A dictionary of loss components. • proposal list (list[Tensor]): Proposals of each image. Return type  tuple[dict, list] \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  soft target ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Cls and quality scores for each scale level has shape (N, num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box distribution logits for each scale level with shape (N,  $4^{*}(\\mathsf{n}\\!+\\!1)$  , H, W), n is max value of integral set. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. "}
{"page": 354, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_354.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_bboxes_ignore (list[Tensor] | None) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single (anchors, cls_score, bbox_pred, labels, label_weights, bbox_targets, stride, soft_targets,\nnum_total_samples)\nCompute loss of a single scale level.\n\nParameters\n\n* anchors (Tensor) — Box reference for each scale level with shape (N, num_total_anchors,\n4).\n\n* cls_score (Tensor) — Cls and quality joint scores for each scale level has shape (N,\nnum_classes, H, W).\n\n* bbox_pred (Tensor) - Box distribution logits for each scale level with shape (N, 4*(n+1),\nH, W), n is max value of integral set.\n\n¢ labels (Tensor) — Labels of each anchors with shape (N, num_total_anchors).\n\n¢ label_weights (Tensor) — Label weights of each anchor with shape (N,\nnum_total_anchors)\n\n¢ bbox_targets (Tensor) — BBox regression targets of each anchor weight shape (N,\nnum_total_anchors, 4).\n\n¢ stride (tuple) — Stride in this scale level.\n\n* num_total_samples (int) — Number of positive samples that is reduced over all GPUs.\nReturns Loss components and weight targets.\nReturn type dict[tuple, Tensor]\n\nclass mmdet.models.dense_heads .NASFCOSHead (*args, init_cfg=None, **kwargs)\nAnchor-free head used in NASFCOS.\n\nIt is quite similar with FCOS head, except for the searched structure of classification branch and bbox regression\nbranch, where a structure of “dconv3x3, conv3x3, dconv3x3, conv1x1” is utilized instead.\n\nclass mmdet.models.dense_heads.PAAHead (*args, topk=9, score_voting=True, covariance_type=‘diag',\n**kwargs)\nHead of PAAAssignment: Probabilistic Anchor Assignment with IoU Prediction for Object Detection.\n\nCode is modified from the official github repo.\nMore details can be found in the paper .\nParameters\n* topk (int) — Select topk samples with smallest loss in each level.\n* score_voting (bool) — Whether to use score voting in post-process.\n\n* covariance_type — String describing the type of covariance parameters to be used in\nsklearn.mixture.GaussianMixture. It must be one of:\n\n— ’full’: each component has its own general covariance matrix\n— ’tied’: all components share the same general covariance matrix\n\n— ‘diag’: each component has its own diagonal covariance matrix\n\n39.4. dense_heads 347\n", "vlm_text": "•  gt b boxes ignore  ( list[Tensor] | None ) – specify which bounding boxes can be ignored when computing the loss. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nloss single ( anchors ,  cls_score ,  bbox_pred ,  labels ,  label weights ,  b box targets ,  stride ,  soft targets , num total samples ) Compute loss of a single scale level. \nParameters \n•  anchors  ( Tensor ) – Box reference for each scale level with shape (N, num total anchors, 4). •  cls_score  ( Tensor ) – Cls and quality joint scores for each scale level has shape (N, num classes, H, W). •  bbox_pred  ( Tensor ) – Box distribution logits for each scale level with shape (N  $\\mathsf{I},4^{*}\\mathsf{(n+1)}$  , H, W), n is max value of integral set. •  labels  ( Tensor ) – Labels of each anchors with shape (N, num total anchors). •  label weights ( Tensor ) – Label weights of each anchor with shape (N, num total anchors) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (N, num total anchors, 4). •  stride  ( tuple ) – Stride in this scale level. •  num total samples  ( int ) – Number of positive samples that is reduced over all GPUs. Returns  Loss components and weight targets. Return type  dict[tuple, Tensor] \nclass  mmdet.models.dense heads. NAS FCO S Head ( \\*args ,  init_cfg  $=$  None ,  \\*\\*kwargs ) Anchor-free head used in  NASFCOS . \nIt is quite similar with FCOS head, except for the searched structure of classification branch and bbox regression branch, where a structure of “dconv3x3, conv3x3, dconv3x3, conv1x1” is utilized instead. \nclass  mmdet.models.dense heads. PAAHead ( \\*args ,  topk  $_{:=9}$  ,  score voting  $=$  True ,  co variance type  $\\mathbf{=}$  diag' , \\*\\*kwargs ) \n\nMore details can be found in the  paper  . \nParameters \n•  topk  ( int ) – Select topk samples with smallest loss in each level. •  score voting  ( bool ) – Whether to use score voting in post-process. •  co variance type  – String describing the type of covariance parameters to be used in sklearn.mixture.Gaussian Mixture . It must be one of: –  ’full’: each component has its own general covariance matrix –  ’tied’: all components share the same general covariance matrix –  ’diag’: each component has its own diagonal covariance matrix "}
{"page": 355, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_355.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n— ’spherical’: each component has its own single variance\n\nDefault: ‘diag’. From ‘full’ to ‘spherical’, the gmm fitting process is faster yet the perfor-\nmance could be influenced. For most cases, ‘diag’ should be a good choice.\n\nget_bboxes (cls_scores, bbox_preds, score_factors=None, img_metas=None, cfg=None, rescale=False,\nwith_nms=True, **kwargs)\nTransform network outputs of a batch into bbox results.\n\nNote: When score_factors is not None, the cls_scores are usually multiplied by it then obtain the real score\nused in NMS, such as CenterNess in FCOS, IoU branch in ATSS.\n\nParameters\n\n¢ cls_scores (list [Tensor]) — Classification scores for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * num_classes, H, W).\n\nbbox_preds (list [Tensor]) — Box energies / deltas for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * 4, H, W).\n\n* score_factors (list[Tensor], Optional) — Score factor for all scale level, each is\na 4D-tensor, has shape (batch_size, num_priors * 1, H, W). Default None.\n\n¢ img_metas (list[dict], Optional) — Image meta info. Default None.\n\n* cfg (mmcv.Config, Optional) — Test / postprocessing configuration, if None, test_cfg\nwould be used. Default None.\n\n* rescale (bool) — If True, return boxes in original image space. Default False.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default True.\nReturns\n\nEach item in result_list is 2-tuple. The first item is an (n, 5) tensor, where the first 4\ncolumns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\nbetween 0 and 1. The second item is a (n,) tensor where each item is the predicted class\nlabel of the corresponding box.\n\nReturn type list[list{Tensor, Tensor]]\n\nget_pos_loss (anchors, cls_score, bbox_pred, label, label_weight, bbox_target, bbox_weight, pos_inds)\nCalculate loss of all potential positive samples obtained from first match process.\n\nParameters\n¢ anchors (list [Tensor ]) — Anchors of each scale.\n\n* cls_score (Tensor) — Box scores of single image with shape (num_anchors,\nnum_classes)\n\n¢ bbox_pred (Tensor) — Box energies / deltas of single image with shape (num_anchors,\n4)\n\n¢ label (Tensor) — classification target of each anchor with shape (num_anchors,)\n\n¢ label_weight (Tensor) - Classification loss weight of each anchor with shape\n(num_anchors).\n\n* bbox_target (dict) — Regression target of each anchor with shape (num_anchors, 4).\n* bbox_weight (Tensor) — Bbox weight of each anchor with shape (num_anchors, 4).\n* pos_inds (Tensor) — Index of all positive samples got from first assign process.\n\nReturns Losses of all positive samples in single image.\n\n348 Chapter 39. mmdet.models\n", "vlm_text": "–  ’spherical’: each component has its own single variance Default: ‘diag’. From ‘full’ to ‘spherical’, the gmm fitting process is faster yet the perfor- mance could be influenced. For most cases, ‘diag’ should be a good choice. \nget_bboxes ( cls_scores ,  bbox_preds ,  score factors  $\\mathbf{\\hat{\\Sigma}}$  None ,  img_metas  $\\leftrightharpoons$  None ,  cfg  $=$  None ,  rescale  $\\mathbf{\\Pi}=$  False , with_nms  $\\mathbf{=}$  True ,  \\*\\*kwargs ) Transform network outputs of a batch into bbox results. \nNote: When score factors is not None, the cls_scores are usually multiplied by it then obtain the real score used in NMS, such as CenterNess in FCOS, IoU branch in ATSS. \nParameters \n•  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  score factors  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, num_priors \\* 1, H, W). Default None. •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. \nReturns \nEach item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. \nReturn type  list[list[Tensor, Tensor]] \nget pos loss ( anchors ,  cls_score ,  bbox_pred ,  label ,  label weight ,  b box target ,  b box weight ,  pos_inds ) Calculate loss of all potential positive samples obtained from first match process. \nParameters \n•  anchors  ( list[Tensor] ) – Anchors of each scale. •  cls_score  ( Tensor ) – Box scores of single image with shape (num anchors, num classes) •  bbox_pred  ( Tensor ) – Box energies / deltas of single image with shape (num anchors, 4) •  label  ( Tensor ) – classification target of each anchor with shape (num anchors,) •  label weight  ( Tensor ) – Classification loss weight of each anchor with shape (num anchors). •  b box target  ( dict ) – Regression target of each anchor with shape (num anchors, 4). •  b box weight  ( Tensor ) – Bbox weight of each anchor with shape (num anchors, 4). •  pos_inds  ( Tensor ) – Index of all positive samples got from first assign process. \nReturns  Losses of all positive samples in single image. "}
{"page": 356, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_356.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type Tensor\n\nget_targets(anchor_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list=None,\ngt_labels_list=None, label_channels=1, unmap_outputs=True)\nGet targets for PAA head.\n\nThis method is almost the same as AnchorHead.get_targets(). We direct return the results from\n_get_targets_single instead map it to levels by images_to_levels function.\n\nParameters\n\n¢ anchor_list (list [list [Tensor]]) — Multi level anchors of each image. The outer\nlist indicates images, and the inner list corresponds to feature levels of the image. Each\nelement of the inner list is a tensor of shape (num_anchors, 4).\n\nvalid_flag_list (list [list [Tensor] ]) — Multi level valid flags of each image. The\nouter list indicates images, and the inner list corresponds to feature levels of the image.\nEach element of the inner list is a tensor of shape (num_anchors, )\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes of each image.\n\n¢ img_metas (list [dict ]) — Meta info of each image.\n\n* gt_bboxes_ignore_list (list [Tensor]) — Ground truth bboxes to be ignored.\n\n¢ gt_labels_list (list [Tensor ]) — Ground truth labels of each box.\n\n¢ label_channels (int) — Channel of label.\n\n* unmap_outputs (bool) — Whether to map outputs back to the original set of anchors.\nReturns\n\nUsually returns a tuple containing learning targets.\n\n¢ labels (list{Tensor]): Labels of all anchors, each with shape (num_anchors,).\n\n¢ label_weights (list{Tensor]): Label weights of all anchor. each with shape\n(num_anchors,).\n\n¢ bbox_targets (list[Tensor]): BBox targets of all anchors. each with shape\n(num_anchors, 4).\n\n¢ bbox_weights (list{Tensor]): BBox weights of all anchors. each with shape\n(num_anchors, 4).\n\n* pos_inds (list[Tensor]): Contains all index of positive sample in all anchor.\n¢ gt_inds (list[Tensor]): Contains all gt_index of positive sample in all anchor.\nReturn type tuple\n\ngmm_separation_scheme(gmm_assignment, scores, pos_inds_gmm)\nA general separation scheme for gmm model.\n\nIt separates a GMM distribution of candidate samples into three parts, 0 1 and uncertain areas, and you can\nimplement other separation schemes by rewriting this function.\n\nParameters\n\n* gmm_assignment (Tensor) — The prediction of GMM which is of shape (num_samples,).\nThe 0/1 value indicates the distribution that each sample comes from.\n\n* scores (Tensor) — The probability of sample coming from the fit GMM distribution. The\ntensor is of shape (num_samples,).\n\n39.4,\n\ndense_heads 349\n", "vlm_text": "Return type  Tensor \nget targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore list  $t{=}$  None , gt labels list=None ,  label channel  $\\varsigma{=}I$  ,  un map outputs  $:=$  True ) Get targets for PAA head. \nThis method is almost the same as  AnchorHead.get targets() . We direct return the results from get targets single instead map it to levels by images to levels function. \nParameters \n•  anchor list  ( list[list[Tensor]] ) – Multi level anchors of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, 4). •  valid flag list  ( list[list[Tensor]] ) – Multi level valid flags of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num anchors, ) •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. \nReturns \nUsually returns a tuple containing learning targets. \n•  labels (list[Tensor]): Labels of all anchors, each with  shape (num anchors,). •  label weights (list[Tensor]): Label weights of all anchor.  each with shape (num anchors,). •  b box targets (list[Tensor]): BBox targets of all anchors.  each with shape (num anchors, 4). •  b box weights (list[Tensor]): BBox weights of all anchors.  each with shape (num anchors, 4). •  pos_inds (list[Tensor]): Contains all index of positive  sample in all anchor. •  gt_inds (list[Tensor]): Contains all gt_index of positive  sample in all anchor. \nReturn type  tuple \ng mm separation scheme ( g mm assignment ,  scores ,  pos in ds g mm ) A general separation scheme for gmm model. \nIt separates a GMM distribution of candidate samples into three parts, 0 1 and uncertain areas, and you can implement other separation schemes by rewriting this function. \nParameters \n•  g mm assignment  ( Tensor ) – The prediction of GMM which is of shape (num samples,). The 0/1 value indicates the distribution that each sample comes from. •  scores  ( Tensor ) – The probability of sample coming from the fit GMM distribution. The tensor is of shape (num samples,). "}
{"page": 357, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_357.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* pos_inds_gmm (Tensor) — All the indexes of samples which are used to fit GMM model.\nThe tensor is of shape (num_samples,)\n\nReturns\nThe indices of positive and ignored samples.\n* pos_inds_temp (Tensor): Indices of positive samples.\n* ignore_inds_temp (Tensor): Indices of ignore samples.\nReturn type tuple[Tensor]\n\nloss (cls_scores, bbox_preds, iou_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n¢ iou_preds (list [Tensor])—iou_preds for each scale level with shape (N, num_anchors\n* 1,H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (list[Tensor] | None) — Specify which bounding boxes can be\nignored when are computing the loss.\n\nReturns A dictionary of loss gmm_assignment.\nReturn type dict[str, Tensor]\n\npaa_reassign(pos_losses, label, label_weight, bbox_weight, pos_inds, pos_gt_inds, anchors)\nFit loss to GMM distribution and separate positive, ignore, negative samples again with GMM model.\n\nParameters\n* pos_losses (Tensor) — Losses of all positive samples in single image.\n¢ label (Tensor) — classification target of each anchor with shape (num_anchors,)\n\n¢ label_weight (Tensor) - Classification loss weight of each anchor with shape\n(num_anchors).\n\n* bbox_weight (Tensor) — Bbox weight of each anchor with shape (num_anchors, 4).\n* pos_inds (Tensor) — Index of all positive samples got from first assign process.\n* pos_gt_inds (Tensor) — Gt_index of all positive samples got from first assign process.\n¢ anchors (list [Tensor ]) — Anchors of each scale.\nReturns\nUsually returns a tuple containing learning targets.\n\n¢ label (Tensor): classification target of each anchor after paa assign, with shape\n(num_anchors,)\n\n350 Chapter 39. mmdet.models\n", "vlm_text": "•  pos in ds g mm  ( Tensor ) – All the indexes of samples which are used to fit GMM model. The tensor is of shape (num samples,) \nReturns \nThe indices of positive and ignored samples. • pos in ds temp (Tensor): Indices of positive samples. • ignore in ds temp (Tensor): Indices of ignore samples. \nReturn type  tuple[Tensor] \nloss ( cls_scores ,  bbox_preds ,  iou_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  iou_preds  ( list[Tensor] ) – iou_preds for each scale level with shape (N, num anchors \\* 1, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] | None ) – Specify which bounding boxes can be ignored when are computing the loss. \nReturns  A dictionary of loss g mm assignment. \nReturn type  dict[str, Tensor] \npaa reassign ( pos_losses ,  label ,  label weight ,  b box weight ,  pos_inds ,  pos gt in ds ,  anchors ) Fit loss to GMM distribution and separate positive, ignore, negative samples again with GMM model. \nParameters \n•  pos_losses  ( Tensor ) – Losses of all positive samples in single image. •  label  ( Tensor ) – classification target of each anchor with shape (num anchors,) •  label weight  ( Tensor ) – Classification loss weight of each anchor with shape (num anchors). •  b box weight  ( Tensor ) – Bbox weight of each anchor with shape (num anchors, 4). •  pos_inds  ( Tensor ) – Index of all positive samples got from first assign process. •  pos gt in ds  ( Tensor ) – Gt_index of all positive samples got from first assign process. •  anchors  ( list[Tensor] ) – Anchors of each scale. \nReturns \nUsually returns a tuple containing learning targets. • label (Tensor): classification target of each anchor after paa assign, with shape (num anchors,) "}
{"page": 358, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_358.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ label_weight (Tensor): Classification loss weight of each anchor after paa assign, with\nshape (num_anchors).\n\n* bbox_weight (Tensor): Bbox weight of each anchor with shape (num_anchors, 4).\n* num_pos (int): The number of positive samples after paa assign.\nReturn type tuple\n\nscore_voting (det_bboxes, det_labels, mlvl_bboxes, mlvl_nms_scores, score_thr)\nImplementation of score voting method works on each remaining boxes after NMS procedure.\n\nParameters\n\n¢ det_bboxes (Tensor) — Remaining boxes after NMS procedure, with shape (k, 5), each\ndimension means (x1, yl, x2, y2, score).\n\ndet_labels (Tensor) — The label of remaining boxes, with shape (k, 1),Labels are 0-\nbased.\n\n¢ mlvl_bboxes (Tensor) — All boxes before the NMS procedure, with shape\n(num_anchors,4).\n\n¢ mlvl_nms_scores (Tensor) — The scores of all boxes which is used in the NMS proce-\ndure, with shape (num_anchors, num_class)\n\n¢ score_thr (float) — The score threshold of bboxes.\nReturns\nUsually returns a tuple containing voting results.\n\n¢ det_bboxes_voted (Tensor): Remaining boxes after score voting procedure, with shape\n(k, 5), each dimension means (x1, y1, x2, y2, score).\n\n¢ det_labels_voted (Tensor): Label of remaining bboxes after voting, with shape\n(num_anchors,).\n\nReturn type tuple\n\nclass mmdet.models.dense_heads.PISARetinaHead (num_classes, in_channels, stacked_convs=4,\nconv_cfg=None, norm_cfg=None,\nanchor_generator={‘octave_base_scale': 4, 'ratios':\n[0.5, 1.0, 2.0], 'scales_per_octave': 3, ‘strides’: [8, 16,\n32, 64, 128], 'type': 'AnchorGenerator'},\ninit_cfg={'layer': 'Conv2d'’, ‘override’: {'bias_prob':\n0.01, ‘name’: 'retina_cls’, 'std': 0.01, ‘type’: 'Normal'},\n‘std’: 0.01, ‘type’: 'Normal'}, **kwargs)\n\nPISA Retinanet Head.\n\nThe head owns the same structure with Retinanet Head, but differs in two aspects: 1. Importance-based\nSample Reweighting Positive (ISR-P) is applied to\n\nchange the positive loss weights.\n2. Classification-aware regression loss is adopted as a third loss.\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n39.4. dense_heads 351\n", "vlm_text": "• label weight (Tensor): Classification loss weight of each anchor after paa assign, with shape (num anchors). • b box weight (Tensor): Bbox weight of each anchor with shape (num anchors, 4). • num_pos (int): The number of positive samples after paa assign. \nReturn type  tuple \nscore voting(det_bboxes, det_labels, ml vl b boxes, ml vl nm s scores, score_thr)Implementation of score voting method works on each remaining boxes after NMS procedure. \nParameters \n•  det_bboxes  ( Tensor ) – Remaining boxes after NMS procedure, with shape (k, 5), each dimension means (x1, y1, x2, y2, score). •  det_labels  ( Tensor ) – The label of remaining boxes, with shape (k, 1),Labels are 0- based. •  ml vl b boxes  ( Tensor ) – All boxes before the NMS procedure, with shape (num anchors,4). •  ml vl nm s scores  ( Tensor ) – The scores of all boxes which is used in the NMS proce- dure, with shape (num anchors, num_class) •  score_thr  ( float ) – The score threshold of bboxes. \nReturns \nUsually returns a tuple containing voting results. •  det b boxes voted (Tensor): Remaining boxes after  score voting procedure, with shape (k, 5), each dimension means (x1, y1, x2, y2, score). •  det labels voted (Tensor): Label of remaining bboxes  after voting, with shape (num anchors,). \nReturn type  tuple \nclass  mmdet.models.dense heads. PISA Retina Head ( num classes ,  in channels ,  stacked con vs=4 , \nconv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $=$  None , anchor generator={'octave base scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} , init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'retina_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ,  \\*\\*kwargs ) \nPISA Retinanet Head. \nThe head owns the same structure with Retinanet Head, but differs in two  aspects: 1. Importance-based Sample Re weighting Positive (ISR-P) is applied to \nchange the positive loss weights. 2. Classification-aware regression loss is adopted as a third loss. \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) "}
{"page": 359, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_359.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes of each image with shape (num_obj,\n4).\n\n¢ gt_labels (list [Tensor]) — Ground truth labels of each image with shape (num_obj,\n4).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (list [Tensor ]) — Ignored gt bboxes of each image. Default: None.\n\nReturns\nLoss dict, comprise classification loss, regression loss and _ car! loss.\n\nReturn type dict\n\nclass mmdet.models.dense_heads .PISASSDHead (num_classes=80, in_channels=(512, 1024, 512, 256, 256,\n256), stacked_convs=0, feat_channels=256,\nuse_depthwise=False, conv_cfg=None, norm_cfg=None,\nact_cfg=None, anchor_generator={ 'basesize_ratio_range':\n(0.1, 0.9), ‘input_size': 300, 'ratios': ([2], [2, 3], [2, 3], [2,\n3], [2], [2]), 'scale_major': False, 'strides': [8, 16, 32, 64,\n100, 300], ‘type’: '‘SSDAnchorGenerator'},\nbbox_coder={'clip_border': True, 'target_means': [0.0, 0.0,\n0.0, 0.0], ‘target_stds': [1.0, 1.0, 1.0, 1.0], ‘type’:\n\n‘DeltaX YWHBBoxCoder'}, reg_decoded_bbox=False,\ntrain_cfg=None, test_cfg=None, init_cfg={‘bias': 0,\n‘distribution’: ‘uniform’, ‘layer’: 'Conv2d’, 'type': 'Xavier'})\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes of each image with shape (num_obj,\n4).\n\n¢ gt_labels (list [Tensor]) — Ground truth labels of each image with shape (num_obj,\n4).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (list [Tensor ]) — Ignored gt bboxes of each image. Default: None.\nReturns\nLoss dict, comprise classification loss regression loss and carl loss.\n\nReturn type dict\n\n352 Chapter 39. mmdet.models\n", "vlm_text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image with shape (num_obj, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each image with shape (num_obj, 4). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] ) – Ignored gt bboxes of each image. Default: None. \nLoss dict, comprise classification loss, regression loss and  carl loss. Return type  dict \n( num classes=80 ,  in_channels=(512, 1024, 512, 256, 256, 256) ,  stacked con v  $\\mathord{:=}O$  ,  feat channels=256 , use depth wise  $\\mathbf{\\dot{\\rho}}=$  False ,  conv_cfg=None ,  norm_cfg=None , act_cfg=None ,  anchor generator={'base size ratio range': (0.1, 0.9), 'input_size': 300, 'ratios': ([2], [2, 3], [2, 3], [2, 3], [2], [2]), 'scale_major': False, 'strides': [8, 16, 32, 64, 100, 300], 'type': 'SSD Anchor Generator'} , bbox_coder={'clip border': True, 'target means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box=False , train_cfg  $=$  None ,  test_cfg=None ,  init_cfg={'bias': 0, 'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image with shape (num_obj, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each image with shape (num_obj, 4). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( list[Tensor] ) – Ignored gt bboxes of each image. Default: None. \nReturns \nLoss dict, comprise classification loss regression loss and  carl loss. Return type  dict "}
{"page": 360, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_360.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads.RPNHead (in_channels, init_cfg={'layer': 'Conv2d', 'std': 0.01, ‘type’:\n‘Normal'}, num_convs=1, **kwargs)\nRPN head.\n\nParameters\n¢ in_channels (int) — Number of channels in the input feature map.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n* num_convs (int) — Number of convolution layers in the head. Default 1.\n\nforward_single(x)\nForward feature map of a single scale level.\n\nloss (cls_scores, bbox_preds, gt_bboxes, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nonnx_export (x, img_metas)\nTest without augmentation.\n\nParameters\n¢ x (tuple[Tensor]) — Features from the upstream network, each is a 4D-tensor.\n¢ img_metas (list [dict ]) — Meta info of each image.\n\nReturns dets of shape [N, num_det, 5].\n\nReturn type Tensor\n\n39.4. dense_heads 353\n", "vlm_text": "class  mmdet.models.dense heads. RPNHead ( in channels ,  init_cfg  $=$  {'layer': 'Conv2d', 'std': 0.01, 'type': 'Normal'} ,  num_convs  $\\mathsf{\\chi}_{=I}$  ,  \\*\\*kwargs ) \nRPN head. \nParameters \n•  in channels  ( int ) – Number of channels in the input feature map. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. •  num_convs  ( int ) – Number of convolution layers in the head. Default 1. \nforward single  $(x)$  Forward feature map of a single scale level. loss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components.  dict[str, Tensor] \non nx export ( x ,  img_metas ) Test without augmentation. \nParameters \n•  x  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D-tensor. •  img_metas  ( list[dict] ) – Meta info of each image. Returns  dets of shape [N, num_det, 5]. Return type  Tensor "}
{"page": 361, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_361.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads.RepPointsHead (num_classes, in_channels, point_feat_channels=256,\nnum_points=9, gradient_mul=0.1, point_strides=[8, 16,\n32, 64, 128], point_base_scale=4, loss_cls={‘alpha':\n0.25, 'gamma': 2.0, ‘loss_weight': 1.0, ‘type’: 'FocalLoss',\n‘use_sigmoid': True}, loss_bbox_init={‘beta':\nO.1111111111111111, ‘loss_weight': 0.5, ‘type’:\n‘SmoothL1Loss'}, loss_bbox_refine={‘beta':\nO.1111111111111111, ‘loss_weight': 1.0, ‘type’:\n‘SmoothL1Loss'}, use_grid_points=False,\ncenter_init=True, transform_method='moment',\nmoment_mul=0.01, init_cfg={‘layer': 'Conv2d',\noverride’: {'bias_prob': 0.01, ‘name’: 'reppoints_cls_out’,\n‘std': 0.01, 'type': 'Normal'}, ‘std’: 0.01, 'type': 'Normal'},\n*“*kwargs)\n\nRepPoint head.\n\nParameters\n* point_feat_channels (int) — Number of channels of points features.\n\n* gradient_mul (float) — The multiplier to gradients from points refinement and recogni-\ntion.\n\n* point_strides (Iterable) — points strides.\n\n* point_base_scale (int) — bbox scale for assigning labels.\n\n* loss_cls (dict) — Config of classification loss.\n\n* loss_bbox_init (dict) — Config of initial points loss.\n\n* loss_bbox_refine (dict) — Config of points loss in refinement.\n\n* use_grid_points (boo1) — If we use bounding box representation, the\n\n¢ is represented as grid points on the bounding box. (reppoints) —\n* center_init (bool) — Whether to use center point assignment.\n\n* transform_method (str) — The methods to transform RepPoints to bbox.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\ncenters_to_bboxes (point_list)\nGet bboxes according to center points.\n\nOnly used in MaxIoUAssigner.\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\nUsually contain classification scores and bbox predictions.\n\ncls_scores (list{Tensor]): Box scores for each scale level, each is a 4D-tensor, the chan-\nnel number is num_points * num_classes.\n\nbbox_preds (list[Tensor]): Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\nReturn type tuple\n\n354 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.dense heads. Rep Points Head ( num classes ,  in channels ,  point feat channel  $'s{=}256$  , num_points  $\\scriptstyle{:=9}$  ,  gradient mu  $l{=}0.1$  ,  point strides=[8, 16, 32, 64, 128] ,  point base scale=4 ,  loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True}, loss b box in it={'beta':0.1111111111111111, 'loss_weight': 0.5, 'type': 'Smooth L 1 Loss'} ,  loss b box refine={'beta': 0.1111111111111111, 'loss_weight': 1.0, 'type': 'Smooth L 1 Loss'} ,  use grid points  $=$  False , center in it  $\\mathbf{=}$  True ,  transform method  $\\leftrightharpoons$  moment' , moment_mul=0.01 ,  init_cfg={'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'rep points cls out', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} , \\*\\*kwargs ) \nRepPoint head. \nParameters \n•  point feat channels  ( int ) – Number of channels of points features. •  gradient mul  ( float ) – The multiplier to gradients from points refinement and recogni- tion. •  point strides  ( Iterable ) – points strides. •  point base scale  ( int ) – bbox scale for assigning labels. •  loss_cls  ( dict ) – Config of classification loss. •  loss b box in it  ( dict ) – Config of initial points loss. •  loss b box refine  ( dict ) – Config of points loss in refinement. •  use grid points  ( bool ) – If we use bounding box representation, the •  is represented as grid points on the bounding box.  ( reppoints ) – •  center in it  ( bool ) – Whether to use center point assignment. •  transform method  ( str ) – The methods to transform RepPoints to bbox. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \ncenters to b boxes ( point_list ) Get bboxes according to center points. Only used in  MaxI oU As signer . \nforward ( feats ) Forward features from the upstream network. Parameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns Usually contain classification scores and bbox predictions. cls_scores (list[Tensor]): Box scores for each scale level,  each is a 4D-tensor, the chan- nel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for each scale  level, each is a 4D- tensor, the channel number is num_points   $^{*}\\,4$  . Return type  tuple "}
{"page": 362, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_362.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward_single(x)\nForward feature map of a single FPN level.\n\ngen_grid_from_reg (reg, previous_boxes)\nBase on the previous bboxes and regression values, we compute the regressed bboxes and generate the grids\non the bboxes.\n\nParameters\n* reg — the regression value to previous bboxes.\n* previous_boxes — previous bboxes.\nReturns generate grids on the regressed bboxes.\n\nget_points (featmap_sizes, img_metas, device)\nGet points according to feature map sizes.\n\nParameters\n¢ featmap_sizes (list [tuple]) — Multi-level feature map sizes.\n¢ img_metas (list [dict ]) — Image meta info.\n\nReturns points of each image, valid flags of each image\n\nReturn type tuple\n\nget_targets(proposals_list, valid_flag_list, gt_bboxes_list, img_metas, gt_bboxes_ignore_list=None,\ngt_labels_list=None, stage='init', label_channels=1, unmap_outputs=True)\nCompute corresponding GT box and classification targets for proposals.\n\nParameters\n\n* proposals_list (list [list]) — Multi level points/bboxes of each image.\n\n¢ valid_flag_list (list [list]) — Multi level valid flags of each image.\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes of each image.\n\n¢ img_metas (list [dict ]) — Meta info of each image.\n\n* gt_bboxes_ignore_list (list [Tensor]) — Ground truth bboxes to be ignored.\n\n¢ gt_bboxes_list — Ground truth labels of each box.\n\n* stage (str) — init or refine. Generate target for init stage or refine stage\n\n¢ label_channels (int) — Channel of label.\n\n* unmap_outputs (bool) — Whether to map outputs back to the original set of anchors.\nReturns\n\n¢ labels_list (list{Tensor]): Labels of each level.\n\n¢ label_weights_list (list{Tensor]): Label weights of each level. # noqa: E501\n\n* bbox_gt_list (list{Tensor]): Ground truth bbox of each level.\n\n* proposal_list (list{Tensor]): Proposals(points/bboxes) of each level. # noqa: E501\n\n* proposal_weights_list (list{Tensor]): Proposal weights of each level. # noqa: E501\n\n* num_total_pos (int): Number of positive samples in all images. # noqa: E501\n\n* num_total_neg (int): Number of negative samples in all images. # noqa: E501\n\nReturn type tuple\n\n39.4. dense_heads 355\n", "vlm_text": "forward single  $(x)$  Forward feature map of a single FPN level. \ngen grid from reg ( reg ,  previous boxes ) Base on the previous bboxes and regression values, we compute the regressed bboxes and generate the grids on the bboxes. \nParameters \n•  reg  – the regression value to previous bboxes. •  previous boxes  – previous bboxes. Returns  generate grids on the regressed bboxes. \nget_points ( feat map sizes ,  img_metas ,  device ) Get points according to feature map sizes. \nParameters \n•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. Returns  points of each image, valid flags of each image \nReturn type  tuple \nget targets ( proposals list ,  valid flag list ,  gt b boxes list ,  img_metas ,  gt b boxes ignore lis  $\\leftleftarrows$  None , gt labels list  $=$  None ,  stage  $:=$  'init' ,  label channel  $\\scriptstyle{\\mathfrak{s}}=I$  ,  un map outputs  $\\backsimeq$  True ) Compute corresponding GT box and classification targets for proposals. \nParameters \n•  proposals list  ( list[list] ) – Multi level points/bboxes of each image. •  valid flag list  ( list[list] ) – Multi level valid flags of each image. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt b boxes list  – Ground truth labels of each box. •  stage  ( str ) –  init  or  refine . Generate target for init stage or refine stage •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. \nReturns \n• labels list (list[Tensor]): Labels of each level. • label weights list (list[Tensor]): Label weights of each level. # noqa: E501 • b box gt list (list[Tensor]): Ground truth bbox of each level. • proposal list (list[Tensor]): Proposals(points/bboxes) of each level. # noqa: E501 • proposal weights list (list[Tensor]): Proposal weights of each level. # noqa: E501 • num total pos (int): Number of positive samples in all images. # noqa: E501 • num total ne g (int): Number of negative samples in all images. # noqa: E501 \nReturn type  tuple "}
{"page": 363, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_363.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nloss (cls_scores, pts_preds_init, pts_preds_refine, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box scores for each scale level, each is a 4D-tensor, the\nchannel number is num_points * num_classes.\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\noffset_to_pts(center_list, pred_list)\nChange from point offset to point coordinate.\n\npoints2bbox (pts, y_first=True)\nConverting the points set into bounding box.\n\nParameters\n* pts — the input points sets (fields), each points set (fields) is represented as 2n scalar.\n\n¢ y_first — if y_first=True, the point set is represented as [yl, xl, y2, x2... yn, xn],\notherwise the point set is represented as [x1, yl, x2, y2... xn, yn].\n\nReturns each points set is converting to a bbox [x1, yl, x2, y2].\n\nclass mmdet.models.dense_heads.RetinaHead (num_classes, in_channels, stacked_convs=4, conv_cfg=None,\n\nnorm_cfg=None, anchor_generator={'octave_base_scale': 4,\n‘ratios’: [0.5, 1.0, 2.0], 'scales_per_octave': 3, ‘strides’: [8,\n16, 32, 64, 128], ‘type’: 'AnchorGenerator'}, init_cfg={‘layer':\n‘Conv2d', ‘override’: {'bias_prob': 0.01, ‘name’: 'retina_cls',\n‘std': 0.01, ‘type’: 'Normal'}, ‘std’: 0.01, ‘type’: 'Normal'},\n**kwargs)\n\nAn anchor-based head used in RetinaNet.\n\nThe head contains two subnetworks. The first classifies anchor boxes and the second regresses deltas for the\nanchors.\n\nExample\n\n>>> import torch\n\n>>> self = RetinaHead(11, 7)\n\n>>> x = torch.rand(1, 7, 32, 32)\n\n>>> cls_score, bbox_pred = self. forward_single(x)\n\n>>> # Each anchor predicts a score for each class except background\n>>> cls_per_anchor = cls_score.shape[1] / self.num_anchors\n\n>>> box_per_anchor = bbox_pred.shape[1] / self.num_anchors\n\n>>> assert cls_per_anchor == (self.num_classes)\n\n>>> assert box_per_anchor == 4\n\n356\n\nChapter 39. mmdet.models\n\n", "vlm_text": "loss ( cls_scores ,  pts p reds in it ,  pts p reds refine ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute loss of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level, each is a 4D-tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. \noffset to pts ( center list ,  pred_list ) Change from point offset to point coordinate. \npoints 2 b box ( pts ,  y_first=True ) Converting the points set into bounding box. \nParameters \n•  pts  – the input points sets (fields), each points set (fields) is represented as 2n scalar. •  y_first  – if y_firs  $\\mathrel{\\mathop:}=$  True, the point set is represented as [y1, x1, y2, x2 ... yn, xn], otherwise the point set is represented as [x1, y1, x2, y2 ... xn, yn]. Returns  each points set is converting to a bbox [x1, y1, x2, y2]. \n( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  ,  conv_cfg  $\\mathbf{\\beta}=$  None , norm_cfg  $=$  None ,  anchor generator={'octave base scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'retina_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} , \\*\\*kwargs ) \nAn anchor-based head used in  RetinaNet . \nThe head contains two sub networks. The first classifies anchor boxes and the second regresses deltas for the anchors.\n\n \nExample \n $>>$   import  torch\n\n  $>>$   self  $=$   RetinaHead( 11 ,  7 )\n\n  $>>$   $\\tt{x\\ =}$  torch.rand(1, 7, 32, 32)\n\n $>>$   cls_score, bbox_pred  $=$   self . forward single(x)\n\n >>>  # Each anchor predicts a score for each class except background\n\n  $>>$   cls per anchor  $=$   cls_score . shape[ 1 ]  /  self . num anchors\n\n  $>>$   box per anchor  $=$   bbox_pred . shape[ 1 ]  /  self . num anchors\n\n  $>>$   assert  cls per anchor  $==$   ( self . num classes)\n\n  $>>$   assert  box per anchor  $==~4$  "}
{"page": 364, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_364.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward_single(x)\nForward feature of a single scale level.\n\nParameters x (Tensor) — Features of a single scale level.\nReturns\n\ncls_score (Tensor): Cls scores for a single scale level the channels number is\nnum_anchors * num_classes.\n\nbbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is\nnum_anchors * 4.\n\nReturn type tuple\n\nclass mmdet.models.dense_heads.RetinaSepBNHead (num_classes, num_ins, in_channels, stacked_convs=4,\nconv_cfg=None, norm_cfg=None, init_cfg=None,\n*“*kwargs)\n“RetinaHead with separate BN.\n\nIn RetinaHead, conv/norm layers are shared across different FPN levels, while in RetinaSepBNHead, conv layers\nare shared across different FPN levels, but BN layers are separated.\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\nUsually a tuple of classification scores and bbox prediction\n\ncls_scores (list{Tensor]): Classification scores for all scale levels, each is a 4D-tensor,\nthe channels number is num_anchors * num_classes.\n\nbbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-\ntensor, the channels number is num_anchors * 4.\n\nReturn type tuple\n\ninit_weights()\nInitialize weights of the head.\n\n39.4. dense_heads 357\n", "vlm_text": "forward single  $(x)$  \nForward feature of a single scale level. \nParameters  x  ( Tensor ) – Features of a single scale level. \nReturns \ncls_score (Tensor): Cls scores for a single scale level  the channels number is num anchors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale  level, the channels number is num anchors  $^{*}\\,4$  . \nReturn type  tuple \nclass  mmdet.models.dense heads. RetinaS epB N Head ( num classes ,  num_ins ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  , conv_cfg  $\\mathbf{\\beta}=$  None ,  norm_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  init_cfg  $=$  None , \\*\\*kwargs ) \n“RetinaHead with separate BN. \nIn RetinaHead, conv/norm layers are shared across different FPN levels, while in RetinaS epB N Head, conv layers are shared across different FPN levels, but BN layers are separated. \nforward ( feats ) \nForward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nUsually a tuple of classification scores and bbox prediction cls_scores (list[Tensor]): Classification scores for all scale  levels, each is a 4D-tensor, the channels number is num anchors \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for all scale  levels, each is a 4D- tensor, the channels number is num anchors   $^{*}\\,4$  . \nReturn type  tuple \nin it weights()Initialize weights of the head. "}
{"page": 365, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_365.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads.SABLRetinaHead (num_classes, in_channels, stacked_convs=4,\nfeat_channels=256,\napprox_anchor_generator={'octave_base_scale': 4,\n‘ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, ‘strides’:\n[8, 16, 32, 64, 128], 'type': 'AnchorGenerator'},\nsquare_anchor_generator={'ratios': [1.0], ‘scales’: [4],\n‘strides’: [8, 16, 32, 64, 128], 'type':\n‘AnchorGenerator'}, conv_cfg=None, norm_cfg=None,\nbbox_coder={'num_buckets': 14, 'scale_factor': 3.0,\n‘type’: 'BucketingBBoxCoder'},\nreg_decoded_bbox=False, train_cfg=None,\ntest_cfg=None, loss_cls={‘alpha': 0.25, 'gamma': 2.0,\n‘loss_weight': 1.0, 'type': 'FocalLoss’, ‘use_sigmoid':\nTrue}, loss_bbox_cls={'loss_weight': 1.5, ‘type’:\n‘CrossEntropyLoss’, 'use_sigmoid': True},\nloss_bbox_reg={‘beta’: 0.1111111111111111,\n‘loss_weight': 1.5, ‘type’: ‘SmoothLI1Loss'},\ninit_cfg={'layer': 'Conv2d'’, ‘override’: {'bias_prob':\n0.01, ‘name’: 'retina_cls’, 'std': 0.01, ‘type’: 'Normal'},\n‘std': 0.01, ‘type’: 'Normal'})\n\nSide-Aware Boundary Localization (SABL) for RetinaNet.\n\nThe anchor generation, assigning and sampling in SABLRetinaHead are the same as GuidedAnchorHead for\nguided anchoring.\n\nPlease refer to https://arxiv.org/abs/1912.04260 for more details.\nParameters\n* num_classes (int) — Number of classes.\n¢ in_channels (int) — Number of channels in the input feature map.\n\n* stacked_convs (int) — Number of Convs for classification and regression branches. De-\nfaults to 4.\n\n¢ feat_channels (int) — Number of hidden channels. Defaults to 256.\n\n* approx_anchor_generator (dict) — Config dict for approx generator.\n* square_anchor_generator (dict) — Config dict for square generator.\n* conv_cfg (dict) — Config dict for ConvModule. Defaults to None.\n\n* norm_cfg (dict) — Config dict for Norm Layer. Defaults to None.\n\n* bbox_coder (dict) — Config dict for bbox coder.\n\n* reg_decoded_bbox (boo1) - If true, the regression loss would be applied directly on de-\ncoded bounding boxes, converting both the predicted boxes and regression targets to abso-\nlute coordinates format. Default False. It should be True when using JoULoss, GloULoss,\nor DIoULoss in the bbox head.\n\n* train_cfg (dict) — Training config of SABLRetinaHead.\n\n* test_cfg (dict) — Testing config of SABLRetinaHead.\n\n* loss_cls (dict) — Config of classification loss.\n\n* loss_bbox_cls (dict) — Config of classification loss for bbox branch.\n\n* loss_bbox_reg (dict) — Config of regression loss for bbox branch.\n\n358 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.dense heads. S ABL Retina Head ( num classes ,  in channels ,  stacked con vs  $\\scriptstyle{\\prime}=4$  , \napprox anchor generator={'octave base scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} , square anchor generator={'ratios': [1.0], 'scales': [4], 'strides': [8, 16, 32, 64, 128], 'type': 'Anchor Generator'} ,  conv_cfg=None ,  norm_cfg=None , bbox_coder={'num buckets': 14, 'scale factor': 3.0, 'type': 'Bucketing B Box Code r'} , reg decoded b box=False ,  train_cfg=None , test_cfg=None ,  loss_cls={'alpha': 0.25, 'gamma': 2.0, 'loss weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True} ,  loss b box cls={'loss weight': 1.5, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_bbox_reg={'beta': 0.1111111111111111, 'loss weight': 1.5, 'type': 'Smooth L 1 Loss'} , init_cfg  $=$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'retina_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} ) \nSide-Aware Boundary Localization (SABL) for RetinaNet. \nThe anchor generation, assigning and sampling in S ABL Retina Head are the same as Guided Anchor Head for guided anchoring. \nPlease refer to  https://arxiv.org/abs/1912.04260  for more details. \nParameters \n•  num classes  ( int ) – Number of classes. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of Convs for classification and regression branches. De- faults to 4. •  feat channels  ( int ) – Number of hidden channels. Defaults to 256. •  approx anchor generator  ( dict ) – Config dict for approx generator. •  square anchor generator  ( dict ) – Config dict for square generator. •  conv_cfg  ( dict ) – Config dict for ConvModule. Defaults to None. •  norm_cfg  ( dict ) – Config dict for Norm Layer. Defaults to None. •  bbox_coder  ( dict ) – Config dict for bbox coder. •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  train_cfg  ( dict ) – Training config of S ABL Retina Head. •  test_cfg  ( dict ) – Testing config of S ABL Retina Head. •  loss_cls  ( dict ) – Config of classification loss. •  loss b box cls  ( dict ) – Config of classification loss for bbox branch. •  loss b box reg  ( dict ) – Config of regression loss for bbox branch. "}
{"page": 366, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_366.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nget_anchors (featmap_sizes, img_metas, device='cuda')\nGet squares according to feature map sizes and guided anchors.\n\nParameters\n¢ featmap_sizes (list [tuple]) — Multi-level feature map sizes.\n¢ img_metas (list [dict ]) — Image meta info.\n¢ device (torch.device | str) — device for returned tensors\nReturns square approxs of each image\nReturn type tuple\n\nget_bboxes (cls_scores, bbox_preds, img_metas, cfg=None, rescale=False)\nTransform network outputs of a batch into bbox results.\n\nNote: When score_factors is not None, the cls_scores are usually multiplied by it then obtain the real score\nused in NMS, such as CenterNess in FCOS, IoU branch in ATSS.\n\nParameters\n\n¢ cls_scores (list [Tensor]) — Classification scores for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * num_classes, H, W).\n\nbbox_preds (list [Tensor]) — Box energies / deltas for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * 4, H, W).\n\n* score_factors (list[Tensor], Optional) — Score factor for all scale level, each is\na 4D-tensor, has shape (batch_size, num_priors * 1, H, W). Default None.\n\n¢ img_metas (list[dict], Optional) — Image meta info. Default None.\n\n* cfg (mmcv.Config, Optional) — Test / postprocessing configuration, if None, test_cfg\nwould be used. Default None.\n\n* rescale (bool) — If True, return boxes in original image space. Default False.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default True.\nReturns\n\nEach item in result_list is 2-tuple. The first item is an (n, 5) tensor, where the first 4\ncolumns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\nbetween 0 and 1. The second item is a (n,) tensor where each item is the predicted class\nlabel of the corresponding box.\n\nReturn type list[list{Tensor, Tensor]]\n\n39.4. dense_heads 359\n", "vlm_text": "•  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nget anchors ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get squares according to feature map sizes and guided anchors. \nParameters \n•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – device for returned tensors \nReturns  square approxs of each image \nReturn type  tuple \nget_bboxes ( cls_scores ,  bbox_preds ,  img_metas ,  cfg  $\\mathbf{\\beta}=$  None ,  rescale  $=$  False ) Transform network outputs of a batch into bbox results. \nNote: When score factors is not None, the cls_scores are usually multiplied by it then obtain the real score used in NMS, such as CenterNess in FCOS, IoU branch in ATSS. \nParameters \n•  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  score factors  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, num_priors \\* 1, H, W). Default None. •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. \nReturns \nEach item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. \nReturn type  list[list[Tensor, Tensor]] "}
{"page": 367, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_367.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nget_target (approx_list, inside_flag_list, square_list, gt_bboxes_list, img_metas,\ngt_bboxes_ignore_list=None, gt_labels_list=None, label_channels=None, sampling=True,\nunmap_outputs=True)\nCompute bucketing targets. :param approx_list: Multi level approxs of each image. :type approx_list:\nlist[list] :param inside_flag_list: Multi level inside flags of each\n\nimage.\n\nParameters\n* square_list (list [list]) — Multi level squares of each image.\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes of each image.\n¢ img_metas (list [dict ]) — Meta info of each image.\n* gt_bboxes_ignore_list (list [Tensor]) — ignore list of gt bboxes.\n* gt_bboxes_list — Gt bboxes of each image.\n¢ label_channels (int) — Channel of label.\n¢ sampling (bool) — Sample Anchors or not.\n* unmap_outputs (bool) — unmap outputs or not.\nReturns\nReturns a tuple containing learning targets.\n¢ labels_list (list{Tensor]): Labels of each level.\n¢ label_weights_list (list{Tensor]): Label weights of each level.\n* bbox_cls_targets_list (list{Tensor]): BBox cls targets of each level.\n* bbox_cls_weights_list (list{Tensor]): BBox cls weights of each level.\n* bbox_reg_targets_list (list[Tensor]): BBox reg targets of each level.\n* bbox_reg_weights_list (list{Tensor]): BBox reg weights of each level.\n* num_total_pos (int): Number of positive samples in all images.\n* num_total_neg (int): Number of negative samples in all images.\nReturn type tuple\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nclass mmdet.models.dense_heads .SOLOHead (num_classes, in_channels, feat_channels=256,\nstacked_convs=4, strides=(4, 8, 16, 32, 64), scale_ranges=((8,\n32), (16, 64), (32, 128), (64, 256), (128, 512)), pos_scale=0.2,\nnum_grids=[40, 36, 24, 16, 12], cls_down_index=0,\nloss_mask=None, loss_cls=None, norm_cfg={‘num_groups':\n32, 'requires_grad': True, ‘type’: 'GN'}, train_cfg=None,\ntest_cfg=None, init_cfg=[{'type': ‘Normal’, ‘layer’: 'Conv2d',\n‘std': 0.01}, {‘type': ‘Normal’, ‘std’: 0.01, 'bias_prob': 0.01,\n‘override’: {‘name': 'conv_mask_list'}}, {‘type': ‘Normal’, 'std':\n0.01, ‘bias_prob': 0.01, ‘override’: {‘name': 'conv_cls'}}])\n\nSOLO mask head used in “SOLO: Segmenting Objects by Locations.\n\n<https://arxiv.org/abs/1912.04488>>_\n\n360 Chapter 39. mmdet.models\n", "vlm_text": "get_target ( approx list ,  inside flag list ,  square list ,  gt b boxes list ,  img_metas , gt b boxes ignore list  $\\leftleftarrows$  None ,  gt labels list  $\\leftrightharpoons$  None ,  label channels  $\\overleftarrow{}$  None ,  sampling  $\\mathbf{\\dot{\\Sigma}}$  True , un map outputs $\\backsimeq$ True)\nlist[list] :param inside flag list: Multi level inside flags of each \nimage. \nParameters \n•  square list  ( list[list] ) – Multi level squares of each image. •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – ignore list of gt bboxes. •  gt b boxes list  – Gt bboxes of each image. •  label channels  ( int ) – Channel of label. •  sampling  ( bool ) – Sample Anchors or not. •  un map outputs  ( bool ) – unmap outputs or not. \nReturns \nReturns a tuple containing learning targets. • labels list (list[Tensor]): Labels of each level. • label weights list (list[Tensor]): Label weights of each level. • b box cls targets list (list[Tensor]): BBox cls targets of each level. • b box cls weights list (list[Tensor]): BBox cls weights of each level. • b box reg targets list (list[Tensor]): BBox reg targets of each level. • b box reg weights list (list[Tensor]): BBox reg weights of each level. • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. \nReturn type  tuple \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. \n\nstacked con vs  $\\scriptstyle{\\;=4}$  ,  strides=(4, 8, 16, 32, 64) ,  scale ranges=((8, 32), (16, 64), (32, 128), (64, 256), (128, 512)) ,  pos_scale=0.2 , num_grids=[40, 36, 24, 16, 12] ,  cls down index=0 , loss_mask  $\\leftrightharpoons$  None ,  loss_cls  $\\leftrightharpoons$  None ,  norm_cfg={'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  train_cfg=None , test_cfg  $=$  None ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  [{'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01,'override': {'name': 'con v mask list'}}, {'type': 'Normal', 'std': 0.01, 'bias_prob': 0.01, 'override': {'name': 'conv_cls'}}] ) \nSOLO mask head used in  \\` SOLO: Segmenting Objects by Locations. < https://arxiv.org/abs/1912.04488 >\\`_ "}
{"page": 368, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_368.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n* num_classes (int) — Number of categories excluding the background category.\n¢ in_channels (int) — Number of channels in the input feature map.\n¢ feat_channels (int) — Number of hidden channels. Used in child classes. Default: 256.\n* stacked_convs (int) — Number of stacking convs of the head. Default: 4.\n* strides (tuple) — Downsample factor of each feature map.\n\n* scale_ranges (tuple[tuple[int, int]])-— Area range of multiple level masks, in the\nformat [(min1, max1), (min2, max2), ...]. A range of (16, 64) means the area range between\n(16, 64).\n\n* pos_scale (float) — Constant scale factor to control the center region.\n\n* num_grids (list [int]) — Divided image into a uniform grids, each feature map has a\ndifferent grid value. The number of output channels is grid ** 2. Default: [40, 36, 24, 16,\n12].\n\n* cls_down_index (int) — The index of downsample operation in classification branch. De-\nfault: 0.\n\n¢ loss_mask (dict) — Config of mask loss.\n* loss_cls (dict) — Config of classification loss.\n\n* norm_cfg (dict) — dictionary to construct and config norm layer. Default:\nnorm_cfg=dict(type=’GN’, num_groups=32,\n\nrequires_grad=True).\n* train_cfg (dict) — Training config of head.\n* test_cfg (dict) — Testing config of head.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nget_results(mlvl_mask_preds, mlvl_cls_scores, img_metas, **kwargs)\nGet multi-image mask results.\n\nParameters\n\n¢ mlvl_mask_preds (list [Tensor]) — Multi-level mask prediction. Each element in the\nlist has shape (batch_size, num_grids**2 ,h ,w).\n\n¢ mlvl_cls_scores (list [Tensor]) — Multi-level scores. Each element in the list has\nshape (batch_size, num_classes, num_grids ,num_grids).\n\n¢ img_metas (list [dict]) — Meta information of all images.\nReturns\n\nProcessed results of multiple images.Each InstanceData usually contains following keys.\n\n39.4. dense_heads 361\n", "vlm_text": "Parameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels. Used in child classes. Default: 256. •  stacked con vs  ( int ) – Number of stacking convs of the head. Default: 4. •  strides  ( tuple ) – Downsample factor of each feature map. •  scale ranges  ( tuple[tuple[int, int]] ) – Area range of multiple level masks, in the format [(min1, max1), (min2, max2), ...]. A range of (16, 64) means the area range between (16, 64). •  pos_scale  ( float ) – Constant scale factor to control the center region. •  num_grids  ( list[int] ) – Divided image into a uniform grids, each feature map has a different grid value. The number of output channels is grid   $^{**}2$  . Default: [40, 36, 24, 16, 12]. •  cls down index  ( int ) – The index of downsample operation in classification branch. De- fault: 0. •  loss_mask  ( dict ) – Config of mask loss. •  loss_cls  ( dict ) – Config of classification loss. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $\\mathbf{\\beta}=$  dict(type  $\\scriptstyle{:=}$  ’GN’, num_groups  $\\scriptstyle{:=32}$  , requires grad=True). •  train_cfg  ( dict ) – Training config of head. •  test_cfg  ( dict ) – Testing config of head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nget results ( ml vl mask p reds ,  ml vl cls scores ,  img_metas ,  \\*\\*kwargs ) Get multi-image mask results. \nParameters \n•  ml vl mask p reds  ( list[Tensor] ) – Multi-level mask prediction. Each element in the list has shape (batch_size, num_grids\\*\\*2 ,h ,w). •  ml vl cls scores  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes, num_grids ,num_grids). •  img_metas  ( list[dict] ) – Meta information of all images. \nReturns \nProcessed results of multiple images.Each  Instance Data  usually contains following keys. "}
{"page": 369, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_369.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* scores (Tensor): Classification scores, has shape (num_instance,).\n\n¢ labels (Tensor): Has shape (num_instances,).\n\n¢ masks (Tensor): Processed mask results, has shape (num_instances, h, w).\nReturn type list{InstanceData]\n\nloss (mlvl_mask_preds, mlvl_cls_preds, gt_labels, gt_masks, img_metas, gt_bboxes=None, **kwargs)\nCalculate the loss of total batch.\n\nParameters\n\n¢ mlvl_mask_preds (list [Tensor]) — Multi-level mask prediction. Each element in the\nlist has shape (batch_size, num_grids**2 ,h ,w).\n\n¢ mlvl_cls_preds (list [Tensor]) — Multi-level scores. Each element in the list has\nshape (batch_size, num_classes, num_grids ,num_grids).\n\n¢ gt_labels (list [Tensor]) — Labels of multiple images.\n\n¢ gt_masks (list [Tensor]) — Ground truth masks of multiple images. Each has shape\n(num_instances, h, w).\n\n¢ img_metas (list [dict ]) — Meta information of multiple images.\n\n* gt_bboxes (list [Tensor]) — Ground truth bboxes of multiple images. Default: None.\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nresize_feats (feats)\nDownsample the first feat and upsample last feat in feats.\n\nclass mmdet.models.dense_heads .SSDHead (num_classes=80, in_channels=(512, 1024, 512, 256, 256, 256),\nstacked_convs=0, feat_channels=256, use_depthwise=False,\nconv_cfg=None, norm_cfg=None, act_cfg=None,\nanchor_generator={ ‘basesize_ratio_range’: (0.1, 0.9),\n‘input_size': 300, ‘ratios': ({2], [2, 3], [2, 3], [2, 3], [2], [2]),\n‘scale_major': False, ‘strides': [8, 16, 32, 64, 100, 300], ‘type’:\n‘SSDAnchorGenerator'}, bbox_coder=({'clip_border': True,\n‘target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0,\n1.0], 'type': 'DeltaXYWHBBoxCoder'}, reg_decoded_bbox=False,\ntrain_cfg=None, test_cfg=None, init_cfg={‘bias': 0,\n‘distribution’: ‘uniform’, ‘layer’: 'Conv2d', 'type': 'Xavier'})\n\nSSD head used in https://arxiv.org/abs/1512.02325.\nParameters\n* num_classes (int) — Number of categories excluding the background category.\n¢ in_channels (int) — Number of channels in the input feature map.\n* stacked_convs (int) — Number of conv layers in cls and reg tower. Default: 0.\n\n¢ feat_channels (int) — Number of hidden channels when stacked_convs > 0. Default:\n256.\n\n* use_depthwise (bool) — Whether to use DepthwiseSeparableConv. Default: False.\n* conv_cfg (dict) — Dictionary to construct and config conv layer. Default: None.\n* norm_cfg (dict) — Dictionary to construct and config norm layer. Default: None.\n\n* act_cfg (dict) — Dictionary to construct and config activation layer. Default: None.\n\n362 Chapter 39. mmdet.models\n", "vlm_text": "• scores (Tensor): Classification scores, has shape (num instance,). • labels (Tensor): Has shape (num instances,). • masks (Tensor): Processed mask results, has shape (num instances, h, w). \nReturn type  list[ Instance Data ] \nloss ( ml vl mask p reds ,  ml vl cls p reds ,  gt_labels ,  gt_masks ,  img_metas ,  gt_bboxes  $\\mathbf{\\check{\\Sigma}}$  None ,  \\*\\*kwargs ) Calculate the loss of total batch. \nParameters \n•  ml vl mask p reds  ( list[Tensor] ) – Multi-level mask prediction. Each element in the list has shape (batch_size, num_grids\\*  $^{*}2$   ,h ,w). •  ml vl cls p reds  ( list[Tensor] ) – Multi-level scores. Each element in the list has shape (batch_size, num classes, num_grids ,num_grids). •  gt_labels  ( list[Tensor] ) – Labels of multiple images. •  gt_masks  ( list[Tensor] ) – Ground truth masks of multiple images. Each has shape (num instances, h, w). •  img_metas  ( list[dict] ) – Meta information of multiple images. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of multiple images. Default: None. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nresize feats ( feats ) Downsample the first feat and upsample last feat in feats. \nclass  mmdet.models.dense heads. SSDHead ( num_classe  $s{=}8O$  ,  in channels  $\\mathbf{:=}$  (512, 1024, 512, 256, 256, 256) , \nstacked con vs  $\\mathord{\\breve{=}}0$  ,  feat channels=256 ,  use depth wise=False , conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg  $=$  None ,  act_cfg=None , anchor generator={'base size ratio range': (0.1, 0.9), 'input_size': 300, 'ratios': ([2], [2, 3], [2, 3], [2, 3], [2], [2]), 'scale_major': False, 'strides': [8, 16, 32, 64, 100, 300], 'type': 'SSD Anchor Generator'} ,  bbox_coder={'clip border': True, 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'type': 'Delta XY WH B Box Code r'} ,  reg decoded b box=False , train_cfg  $=$  None ,  test_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  init_cfg  $=_{l}$  {'bias': 0, 'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ) \nSSD head used in  https://arxiv.org/abs/1512.02325 . \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  stacked con vs  ( int ) – Number of conv layers in cls and reg tower. Default: 0. •  feat channels  ( int ) – Number of hidden channels when stacked con vs   $>0$  . Default: 256. •  use depth wise  ( bool ) – Whether to use Depth wise Separable Con v. Default: False. •  conv_cfg  ( dict ) – Dictionary to construct and config conv layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: None. •  act_cfg  ( dict ) – Dictionary to construct and config activation layer. Default: None. "}
{"page": 370, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_370.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* anchor_generator (dict) — Config dict for anchor generator\n* bbox_coder (dict) — Config of bounding box coder.\n\n* reg_decoded_bbox (boo1) - If true, the regression loss would be applied directly on de-\ncoded bounding boxes, converting both the predicted boxes and regression targets to abso-\nlute coordinates format. Default False. It should be True when using JoULoss, GloULoss,\nor DIoULoss in the bbox head.\n\n* train_cfg (dict) — Training config of anchor head.\n* test_cfg (dict) — Testing config of anchor head.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\n\ncls_scores (list{Tensor]): Classification scores for all scale levels, each is a 4D-tensor, the\nchannels number is num_anchors * num_classes.\n\nbbox_preds (list[Tensor]): Box energies / deltas for all scale levels, each is a 4D-tensor,\nthe channels number is num_anchors * 4.\n\nReturn type tuple\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single(cls_score, bbox_pred, anchor, labels, label_weights, bbox_targets, bbox_weights,\nnum_total_samples)\nCompute loss of a single image.\n\nParameters\n\n¢ cls_score (Tensor) — Box scores for eachimage Has shape (num_total_anchors,\nnum_classes).\n\n39.4. dense_heads 363\n", "vlm_text": "•  anchor generator  ( dict ) – Config dict for anchor generator •  bbox_coder  ( dict ) – Config of bounding box coder. •  reg decoded b box  ( bool ) – If true, the regression loss would be applied directly on de- coded bounding boxes, converting both the predicted boxes and regression targets to abso- lute coordinates format. Default False. It should be  True  when using  IoULoss ,  GIoULoss , or  DIoULoss  in the bbox head. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) Forward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \ncls_scores (list[Tensor]): Classification scores for all scale  levels, each is a 4D-tensor, the channels number is num anchors \\* num classes. bbox_preds (list[Tensor]): Box energies / deltas for all scale  levels, each is a 4D-tensor, the channels number is num anchors  $^{*}\\,4$  . \nReturn type  tuple \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Returns  A dictionary of loss components. \n\nloss single ( cls_score ,  bbox_pred ,  anchor ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) Compute loss of a single image. \nParameters \n•  cls_score  ( Tensor ) – Box scores for eachimage Has shape (num total anchors, num classes). "}
{"page": 371, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_371.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ bbox_pred (Tensor) — Box energies / deltas for each image level with shape\n(num_total_anchors, 4).\n\n* anchors (Tensor) — Box reference for each scale level with shape (num_total_anchors,\n4).\n\n* labels (Tensor) — Labels of each anchors with shape (num_total_anchors,).\n\n¢ label_weights (Tensor) - Label weights of each anchor with shape\n(num_total_anchors,)\n\n¢ bbox_targets (Tensor) — BBox regression targets of each anchor weight shape\n(num_total_anchors, 4).\n\n* bbox_weights (Tensor) — BBox regression loss weights of each anchor with shape\n(num_total_anchors, 4).\n\n* num_total_samples (int) — If sampling, num total samples equal to the number of total\nanchors; Otherwise, it is the number of positive anchors.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nproperty num_anchors\nReturns: list[int]: Number of base_anchors on each point of each level.\n\nclass mmdet.models.dense_heads .StageCascadeRPNHead (in_channels, anchor_generator={'ratios': [1.0],\n‘scales': [8], 'strides': [4, 8, 16, 32, 64], ‘type’:\n‘AnchorGenerator'}, adapt_cfg={'dilation': 3,\n‘type’: ‘dilation'}, bridged_feature=False,\nwith_cls=True, sampling=True, init_cfg=None,\n**kwargs)\nStage of CascadeRPNHead.\n\nParameters\n¢ in_channels (int) — Number of channels in the input feature map.\n* anchor_generator (dict) — anchor generator config.\n* adapt_cfg (dict) — adaptation config.\n* bridged_feature (bool, optional) — whether update rpn feature. Default: False.\n¢ with_cls (bool, optional) — whether use classification branch. Default: True.\n* sampling (bool, optional) - whether use sampling. Default: True.\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\nanchor_offset (anchor_list, anchor_strides, featmap_sizes)\nGet offset for deformable conv based on anchor shape NOTE: currently support deformable kernel_size=3\nand dilation=1\n\nParameters\n¢ anchor_list (list [list [tensor]))—[NI, NLVL, NA, 4] list of multi-level anchors\n¢ anchor_strides (list [int]) — anchor stride of each level\n\nReturns\n[NLVL, NA, 2, 18]: offset of DeformConv kernel.\n\nReturn type offset_list (list{tensor])\n\n364 Chapter 39. mmdet.models\n", "vlm_text": "•  bbox_pred  ( Tensor ) – Box energies / deltas for each image level with shape (num total anchors, 4). •  anchors  ( Tensor ) – Box reference for each scale level with shape (num total anchors, 4). •  labels  ( Tensor ) – Labels of each anchors with shape (num total anchors,). •  label weights ( Tensor ) – Label weights of each anchor with shape (num total anchors,) •  b box targets  ( Tensor ) – BBox regression targets of each anchor weight shape (num total anchors, 4). •  b box weights  ( Tensor ) – BBox regression loss weights of each anchor with shape (num total anchors, 4). •  num total samples  ( int ) – If sampling, num total samples equal to the number of total anchors; Otherwise, it is the number of positive anchors. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nproperty num anchors Returns: list[int]: Number of base anchors on each point of each level. \nclass  mmdet.models.dense heads. Stage Cascade RP N Head ( in channels ,  anchor generator={'ratios': [1.0], 'scales': [8], 'strides': [4, 8, 16, 32, 64], 'type': 'Anchor Generator'} ,  adapt_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'dilation': 3, 'type': 'dilation'} ,  bridged feature  $\\mathbf{=}$  False , with_cls  $\\mathbf{=}$  True ,  sampling  $\\scriptstyle{\\tilde{}}=$  True ,  init_cfg=None , \\*\\*kwargs ) \nStage of Cascade RP N Head. \nParameters \n•  in channels  ( int ) – Number of channels in the input feature map. •  anchor generator  ( dict ) – anchor generator config. •  adapt_cfg  ( dict ) – adaptation config. •  bridged feature  ( bool, optional ) – whether update rpn feature. Default: False. •  with_cls  ( bool, optional ) – whether use classification branch. Default: True. •  sampling  ( bool, optional ) – whether use sampling. Default: True. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nanchor offset ( anchor list ,  anchor strides ,  feat map sizes ) Get offset for deformable conv based on anchor shape NOTE: currently support deformable kernel size  $_{:=3}$  and dilation  $^{=1}$  \nParameters \n•  anchor list  ( list[list[tensor]) ) – [NI, NLVL, NA, 4] list of multi-level anchors •  anchor strides  ( list[int] ) – anchor stride of each level \nReturns \n[NLVL, NA, 2, 18]: offset of DeformConv  kernel. \nReturn type  offset list (list[tensor]) "}
{"page": 372, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_372.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward (feats, offset_list=None)\nForward function.\n\nforward_single (x, offset)\nForward function of single scale.\n\nget_bboxes (anchor_list, cls_scores, bbox_preds, img_metas, cfg, rescale=False)\nGet proposal predict.\n\nParameters\n¢ anchor_list (list [list]) — Multi level anchors of each image.\n\n¢ cls_scores (list [Tensor]) — Classification scores for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * num_classes, H, W).\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * 4, H, W).\n\n¢ img_metas (list[dict], Optional) — Image meta info. Default None.\n\n* cfg (mmcv.Config, Optional) — Test / postprocessing configuration, if None, test_cfg\nwould be used.\n\n¢ rescale (bool) — If True, return boxes in original image space. Default: False.\nReturns\n\nLabeled boxes in shape (n, 5), where the first 4 columns are bounding box positions\n(tlx, tly, br_x, br_y) and the 5-th column is a score between 0 and 1.\n\nReturn type Tensor\n\nget_targets(anchor_list, valid_flag_list, gt_bboxes, img_metas, featmap_sizes, gt_bboxes_ignore=None,\nlabel_channels=1)\nCompute regression and classification targets for anchors.\n\nParameters\n¢ anchor_list (list [list]) — Multi level anchors of each image.\n¢ valid_flag_list (list [list]) — Multi level valid flags of each image.\n* gt_bboxes (list [Tensor]) — Ground truth bboxes of each image.\n¢ img_metas (list [dict ]) — Meta info of each image.\n¢ featmap_sizes (list [Tensor]) — Feature mapsize each level\n* gt_bboxes_ignore (list [Tensor]) — Ignore bboxes of each images\n¢ label_channels (int) — Channel of label.\nReturns cls_reg_targets (tuple)\n\nloss (anchor_list, valid_flag_list, cls_scores, bbox_preds, gt_bboxes, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n¢ anchor_list (list [list]) — Multi level anchors of each image.\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n39.4. dense_heads 365\n", "vlm_text": "forward ( feats ,  offset_lis  $\\mathbf{\\dot{=}}$  None ) Forward function. \nforward single ( x ,  offset ) Forward function of single scale. \nget_bboxes ( anchor list ,  cls_scores ,  bbox_preds ,  img_metas ,  cfg ,  rescale  $\\mathbf{\\Pi}^{*}=$  False ) Get proposal predict. \nParameters \n•  anchor list  ( list[list] ) – Multi level anchors of each image. •  cls_scores  ( list[Tensor] ) – Classification scores for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* num classes, H, W). •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. \nReturns \nLabeled boxes in shape (n, 5), where the first 4 columns  are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. \nReturn type  Tensor \nget targets ( anchor list ,  valid flag list ,  gt_bboxes ,  img_metas ,  feat map sizes ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , label channels  $\\imath\\!=\\!I$  ) Compute regression and classification targets for anchors. \nParameters \n•  anchor list  ( list[list] ) – Multi level anchors of each image. •  valid flag list  ( list[list] ) – Multi level valid flags of each image. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  feat map sizes  ( list[Tensor] ) – Feature mapsize each level •  gt b boxes ignore  ( list[Tensor] ) – Ignore bboxes of each images •  label channels  ( int ) – Channel of label. \n\nloss ( anchor list ,  valid flag list ,  cls_scores ,  bbox_preds ,  gt_bboxes ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. \nParameters \n•  anchor list  ( list[list] ) – Multi level anchors of each image. •  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) "}
{"page": 373, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_373.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss. Default: None\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single(cls_score, bbox_pred, anchors, labels, label_weights, bbox_targets, bbox_weights,\nnum_total_samples)\nLoss function on single scale.\n\nrefine_bboxes (anchor_list, bbox_preds, img_metas)\nRefine bboxes through stages.\n\nregion_targets (anchor_list, valid_flag_list, gt_bboxes_list, img_metas, featmap_sizes,\ngt_bboxes_ignore_list=None, gt_labels_list=None, label_channels=1,\nunmap_outputs=True)\nSee StageCascadeRPNHead.get_targets().\n\nclass mmdet.models.dense_heads .VFNetHead (num_classes, in_channels, regress_ranges=((- 1, 64), (64,\n128), (128, 256), (256, 512), (512, 100000000.0)),\ncenter_sampling=False, center_sample_radius=1.5,\nsync_num_pos=True, gradient_mul=0.1,\nbbox_norm_type='reg_denom', loss_cls_fl={‘alpha': 0.25,\n‘gamma': 2.0, ‘loss_weight': 1.0, 'type': 'FocalLoss'’,\n‘use_sigmoid': True}, use_vfl=True, loss_cls={‘alpha': 0.75,\n‘gamma': 2.0, ‘iou_weighted': True, ‘loss_weight': 1.0, ‘type’:\n‘VarifocalLoss’, ‘use_sigmoid': True},\nloss_bbox={'loss_weight': 1.5, 'type': 'GIoULoss'},\nloss_bbox_refine={'loss_weight': 2.0, 'type': 'GloULoss'},\nnorm_cfg=({‘num_groups': 32, 'requires_grad': True, 'type':\n‘'GN'}, use_atss=True, reg_decoded_bbox=True,\nanchor_generator={ 'center_offset': 0.0, 'octave_base_scale':\n8, 'ratios': [1.0], 'scales_per_octave': 1, ‘strides’: [8, 16, 32,\n64, 128], 'type': '‘AnchorGenerator'}, init_cfg={'layer':\n‘Conv2d', ‘override’: {'bias_prob': 0.01, ‘name’: 'vfnet_cls',\n‘std’: 0.01, ‘type’: 'Normal'}, ‘std’: 0.01, ‘type’: 'Normal'},\n**kwargs)\n\nHead of “VarifocalNet (VFNet): An IoU-aware Dense Object Detec-\ntor.<https://arxiv.org/abs/2008.13367>*_.\n\nThe VENet predicts loU-aware classification scores which mix the object presence confidence and object local-\nization accuracy as the detection score. It is built on the FCOS architecture and uses ATSS for defining posi-\ntive/negative training examples. The VFNet is trained with Varifocal Loss and empolys star-shaped deformable\nconvolution to extract features for a bbox.\n\nParameters\n* num_classes (int) — Number of categories excluding the background category.\n¢ in_channels (int) — Number of channels in the input feature map.\n* regress_ranges (tuple[tuple[int, int]])—Regress range of multiple level points.\n\n* center_sampling (boo1) — If true, use center sampling. Default: False.\n\n366 Chapter 39. mmdet.models\n", "vlm_text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None \nReturns  A dictionary of loss components. Return type  dict[str, Tensor] \nloss single ( cls_score ,  bbox_pred ,  anchors ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) Loss function on single scale. \nrefine b boxes ( anchor list ,  bbox_preds ,  img_metas ) Refine bboxes through stages. \nregion targets ( anchor list ,  valid flag list ,  gt b boxes list ,  img_metas ,  feat map sizes , gt b boxes ignore list=None ,  gt labels list  $\\leftleftarrows$  None ,  label channels=1 , un map outputs $\\mathbf{=}$ True)See  Stage Cascade RP N Head.get targets() . \nclass  mmdet.models.dense heads. VFNetHead ( num classes ,  in channels ,  regress_ranges=((- 1, 64), (64, \n128), (128, 256), (256, 512), (512, 100000000.0)) , center sampling  $=$  False ,  center sample radius=1.5 , sync num pos  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ,  gradient mu  $l{=}0.1$  , b box norm type  $=$  'reg_denom' ,  loss cls fl={'alpha': 0.25, 'gamma': 2.0, 'loss weight': 1.0, 'type': 'FocalLoss', 'use s igm oid': True} ,  use_vfl=True ,  loss_cls={'alpha': 0.75, 'gamma': 2.0, 'i ou weighted': True, 'loss weight': 1.0, 'type': 'Var i focal Loss', 'use s igm oid': True} , loss_bbox={'loss weight': 1.5, 'type': 'GIoULoss'} , loss b box refine={'loss weight': 2.0, 'type': 'GIoULoss'} , norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'num_groups': 32, 'requires grad': True, 'type': 'GN'} ,  use_atss  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ,  reg decoded b box=True , anchor generator={'center offset': 0.0, 'octave base scale': 8, 'ratios': [1.0], 'scales_per_octave': 1, 'strides': [8, 16, 32, 64, 128], 'type': 'Anchor Generator'} ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Conv2d', 'override': {'bias_prob': 0.01, 'name': 'vfnet_cls', 'std': 0.01, 'type': 'Normal'}, 'std': 0.01, 'type': 'Normal'} , \\*\\*kwargs ) \nHead of \\`Var i focal Net (VFNet): An IoU-aware Dense Object Detec- tor.<https://arxiv.org/abs/2008.13367>\\`_ . \nThe VFNet predicts IoU-aware classification scores which mix the object presence confidence and object local- ization accuracy as the detection score. It is built on the FCOS architecture and uses ATSS for defining posi- tive/negative training examples. The VFNet is trained with Varifocal Loss and empolys star-shaped deformable convolution to extract features for a bbox. \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  regress ranges  ( tuple[tuple[int, int]] ) – Regress range of multiple level points. •  center sampling  ( bool ) – If true, use center sampling. Default: False. "}
{"page": 374, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_374.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* center_sample_radius (float) — Radius of center sampling. Default: 1.5.\n\n* sync_num_pos (boo1) — If true, synchronize the number of positive examples across GPUs.\nDefault: True\n\n* gradient_mul (float) -The multiplier to gradients from bbox refinement and recognition.\nDefault: 0.1.\n\n* bbox_norm_type (str) — The bbox normalization type, ‘reg_denom’ or ‘stride’. Default:\nreg_denom\n\n* loss_cls_fl (dict) — Config of focal loss.\n\n* use_vfl (boo1) — If true, use varifocal loss for training. Default: True.\n* loss_cls (dict) — Config of varifocal loss.\n\n* loss_bbox (dict) — Config of localization loss, GloU Loss.\n\n* loss_bbox — Config of localization refinement loss, GloU Loss.\n\n* norm_cfg (dict) — dictionary to construct and config norm layer. Default:\nnorm_cfg=dict(type=’GN’, num_groups=32, requires_grad=True).\n\n* use_atss (bool) — If true, use ATSS to define positive/negative examples. Default: True.\n* anchor_generator (dict) — Config of anchor generator for ATSS.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nExample\n\n>>> self = VFNetHead(11, 7)\n\n>>> feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]\n>>> cls_score, bbox_pred, bbox_pred_refine= self. forward(feats)\n>>> assert len(cls_score) == len(self.scales)\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\n\ncls_scores (list{Tensor]): Box iou-aware scores for each scale level, each is a 4D-tensor,\nthe channel number is num_points * num_classes.\n\nbbox_preds (list[Tensor]): Box offsets for each scale level, each is a 4D-tensor, the chan-\nnel number is num_points * 4.\n\nbbox_preds_refine (list{Tensor]): Refined Box offsets for each scale level, each is a 4D-\ntensor, the channel number is num_points * 4.\n\nReturn type tuple\n\nforward_single(x, scale, scale_refine, stride, reg_denom)\nForward features of a single scale level.\n\nParameters\n\n¢ x (Tensor) — FPN feature maps of the specified stride.\n\n39.4. dense_heads 367\n", "vlm_text": "•  center sample radius  ( float ) – Radius of center sampling. Default: 1.5. •  sync num pos  ( bool ) – If true, synchronize the number of positive examples across GPUs. Default: True •  gradient mul  ( float ) – The multiplier to gradients from bbox refinement and recognition. Default: 0.1. •  b box norm type  ( str ) – The bbox normalization type, ‘reg_denom’ or ‘stride’. Default: reg_denom •  loss cls fl  ( dict ) – Config of focal loss. •  use_vfl  ( bool ) – If true, use varifocal loss for training. Default: True. •  loss_cls  ( dict ) – Config of varifocal loss. •  loss_bbox  ( dict ) – Config of localization loss, GIoU Loss. •  loss_bbox  – Config of localization refinement loss, GIoU Loss. •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: norm_cfg  $=$  dict(type  $\\risingdotseq$  ’GN’, num_groups  $\\scriptstyle=32$  , requires grad  $\\risingdotseq$  True). •  use_atss  ( bool ) – If true, use ATSS to define positive/negative examples. Default: True. •  anchor generator  ( dict ) – Config of anchor generator for ATSS. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict.\n\n \nExample \n>>>  self  $=$   VFNetHead( 11 ,  7 )\n\n  $>>$   feats  $=$   [torch . rand( 1 ,  7 , s, s)  for  s  in  [ 4 ,  8 ,  16 ,  32 ,  64 ]]\n\n  $>>$   cls_score, bbox_pred, b box p red refine  $=$   self . forward(feats)\n\n  $>>$   assert  len (cls_score)  $==$   len ( self . scales) \nforward ( feats ) Forward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \ncls_scores (list[Tensor]): Box iou-aware scores for each scale  level, each is a 4D-tensor, the channel number is num_points \\* num classes. bbox_preds (list[Tensor]): Box offsets for each  scale level, each is a 4D-tensor, the chan- nel number is num_points  $^{\\ast}\\,4$  . b box p reds refine (list[Tensor]): Refined Box offsets for  each scale level, each is a 4D- tensor, the channel number is num_points  $^{\\ast}\\,4$  . Return type  tuple \nforward single (  $\\acute{x}$  ,  scale ,  scale refine ,  stride ,  reg_denom ) Forward features of a single scale level. \nParameters •  x  ( Tensor ) – FPN feature maps of the specified stride. "}
{"page": 375, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_375.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* ( (scale_refine) — obj: mmcv.cnn.Scale): Learnable scale module to resize the bbox\nprediction.\n\n* (obj: mmcv.cnn.Scale): Learnable scale module to resize the refined bbox prediction.\n\nstride (int) — The corresponding stride for feature maps, used to normalize the bbox\nprediction when bbox_norm_type = ‘stride’.\n\n* reg_denom (int) — The corresponding regression range for feature maps, only used to\nnormalize the bbox prediction when bbox_norm_type = ‘reg_denom’.\n\nReturns\n\niou-aware cls scores for each box, bbox predictions and refined bbox predictions of input\nfeature maps.\n\nReturn type tuple\n\nget_anchors (featmap_sizes, img_metas, device='cuda')\nGet anchors according to feature map sizes.\n\nParameters\n¢ featmap_sizes (list [tuple]) — Multi-level feature map sizes.\n¢ img_metas (list [dict ]) — Image meta info.\n¢ device (torch.device | str) — Device for returned tensors\n\nReturns anchor_list (list{Tensor]): Anchors of each image. valid_flag_list (list{Tensor]): Valid\nflags of each image.\n\nReturn type tuple\n\nget_atss_targets (cls_scores, mlvl_points, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nA wrapper for computing ATSS targets for points in multiple images.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box iou-aware scores for each scale level with shape (N,\nnum_points * num_classes, H, W).\n\n¢ mlvl_points (list [Tensor ]) — Points of each fpn level, each has shape (num_points,\n2).\n\n* gt_bboxes (list[Tensor]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\n\ngt_labels (list [Tensor ])-— Ground truth labels of each box, each has shape (num_gt,).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None / Tensor) — Ground truth bboxes to be ignored, shape\n(num_ignored_gts, 4). Default: None.\n\nReturns\n\nlabels_list (list[Tensor]): Labels of each level. label_weights (Tensor): Label weights of all\nlevels. bbox_targets_list (list|Tensor]): Regression targets of each\n\nlevel, (1, t, r, b).\nbbox_weights (Tensor): Bbox weights of all levels.\n\nReturn type tuple\n\n368 Chapter 39. mmdet.models\n", "vlm_text": "•  (  ( scale refine ) – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the bbox prediction. •  (  – obj:  mmcv.cnn.Scale ): Learnable scale module to resize the refined bbox prediction. •  stride  ( int ) – The corresponding stride for feature maps, used to normalize the bbox prediction when b box norm type  $=$   ‘stride’. •  reg_denom  ( int ) – The corresponding regression range for feature maps, only used to normalize the bbox prediction when b box norm type  $=$  ‘reg_denom’. \nReturns \niou-aware cls scores for each box, bbox predictions and  refined bbox predictions of input feature maps. \nReturn type  tuple \nget anchors ( feat map sizes ,  img_metas ,  device  $\\mathbf{=}$  'cuda' ) Get anchors according to feature map sizes. \nParameters \n•  feat map sizes  ( list[tuple] ) – Multi-level feature map sizes. •  img_metas  ( list[dict] ) – Image meta info. •  device  ( torch.device | str ) – Device for returned tensors \nReturns  anchor list (list[Tensor]): Anchors of each image. valid flag list (list[Tensor]): Valid flags of each image. \nReturn type  tuple \nget at s s targets ( cls_scores ,  ml vl points ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{=}$  None ) A wrapper for computing ATSS targets for points in multiple images. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box iou-aware scores for each scale level with shape (N, num_points \\* num classes, H, W). •  ml vl points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). Default: None. \nReturns \nlabels list (list[Tensor]): Labels of each level. label weights (Tensor): Label weights of all levels. b box targets list (list[Tensor]): Regression targets of each level, (l, t, r, b). b box weights (Tensor): Bbox weights of all levels. \nReturn type  tuple "}
{"page": 376, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_376.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nget_fcos_targets (points, gt_bboxes_list, gt_labels_list)\nCompute FCOS regression and classification targets for points in multiple images.\n\nParameters\n* points (list [Tensor ]) — Points of each fpn level, each has shape (num_points, 2).\n\n* gt_bboxes_list (list [Tensor ]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\n\n¢ gt_labels_list (list [Tensor]) — Ground truth labels of each box, each has shape\n(num_gt,).\n\nReturns labels (list[Tensor]): Labels of each level. label_weights: None, to be compatible with\nATSS targets. bbox_targets (list[Tensor]): BBox targets of each level. bbox_weights: None,\nto be compatible with ATSS targets.\n\nReturn type tuple\n\nget_targets(cls_scores, mlvl_points, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore)\nA wrapper for computing ATSS and FCOS targets for points in multiple images.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box iou-aware scores for each scale level with shape (N,\nnum_points * num_classes, H, W).\n\n¢ mlvl_points (list [Tensor ]) — Points of each fpn level, each has shape (num_points,\n2).\n\n* gt_bboxes (list[Tensor]) — Ground truth bboxes of each image, each has shape\n(num_gt, 4).\n\ngt_labels (list [Tensor ])-— Ground truth labels of each box, each has shape (num_gt,).\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None / Tensor) — Ground truth bboxes to be ignored, shape\n(num_ignored_gts, 4).\n\nReturns\n\nlabels_list (list[Tensor]): Labels of each level. label_weights (Tensor/None): Label weights\nof all levels. bbox_targets_list (list{Tensor]): Regression targets of each\n\nlevel, (1, t, r, b).\nbbox_weights (Tensor/None): Bbox weights of all levels.\nReturn type tuple\n\nloss (cls_scores, bbox_preds, bbox_preds_refine, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n¢ cls_scores (list [Tensor ]) — Box iou-aware scores for each scale level, each is a 4D-\ntensor, the channel number is num_points * num_classes.\n\n¢ bbox_preds (list [Tensor ]) — Box offsets for each scale level, each is a 4D-tensor, the\nchannel number is num_points * 4.\n\n* bbox_preds_refine (list [Tensor ]) — Refined Box offsets for each scale level, each is\na 4D-tensor, the channel number is num_points * 4.\n\n39.4. dense_heads 369\n", "vlm_text": "get fco s targets ( points ,  gt b boxes list ,  gt labels list ) Compute FCOS regression and classification targets for points in multiple images. \nParameters \n•  points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). Returns  labels (list[Tensor]): Labels of each level. label weights: None, to be compatible with ATSS targets. b box targets (list[Tensor]): BBox targets of each level. b box weights: None, to be compatible with ATSS targets. \nReturn type  tuple \nget targets ( cls_scores ,  ml vl points ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore ) A wrapper for computing ATSS and FCOS targets for points in multiple images. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box iou-aware scores for each scale level with shape (N, num_points \\* num classes, H, W). •  ml vl points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes of each image, each has shape (num_gt, 4). •  gt_labels  ( list[Tensor] ) – Ground truth labels of each box, each has shape (num_gt,). •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | Tensor ) – Ground truth bboxes to be ignored, shape (num ignored gts, 4). \nReturns \nlabels list (list[Tensor]): Labels of each level. label weights (Tensor/None): Label weights of all levels. b box targets list (list[Tensor]): Regression targets of each level, (l, t, r, b). b box weights (Tensor/None): Bbox weights of all levels. \nReturn type  tuple \nloss ( cls_scores ,  bbox_preds ,  b box p reds refine ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute loss of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box iou-aware scores for each scale level, each is a 4D- tensor, the channel number is num_points \\* num classes. •  bbox_preds  ( list[Tensor] ) – Box offsets for each scale level, each is a 4D-tensor, the channel number is num_points  $^{\\ast}\\,4$  . •  b box p reds refine  ( list[Tensor] ) – Refined Box offsets for each scale level, each is a 4D-tensor, the channel number is num_points  $^{*}\\,4$  . "}
{"page": 377, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_377.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\nimg_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss. Default: None.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nproperty num_anchors\nReturns: int: Number of anchors on each point of feature map.\n\nstar_dcn_offset (bbox_pred, gradient_mul, stride)\nCompute the star deformable conv offsets.\n\nParameters\n\n¢ bbox_pred (Tensor) — Predicted bbox distance offsets (1, r, t, b).\n¢ gradient_mul (float) — Gradient multiplier.\n\n¢ stride (int) — The corresponding stride for feature maps, used to project the bbox onto\nthe feature map.\n\nReturns The offsets for deformable convolution.\n\nReturn type dcn_offsets (Tensor)\ntransform_bbox_targets (decoded_bboxes, mlvl_points, num_imgs)\nTransform bbox_targets (x1, yl, x2, y2) into (1, t, r, b) format.\n\nParameters\n\n* decoded_bboxes (list [Tensor ]) — Regression targets of each level, in the form of (x1,\nyl, x2, y2).\n\n¢ mlvl_points (list [Tensor ]) — Points of each fpn level, each has shape (num_points,\n2).\n\n* num_imgs (int) — the number of images in a batch.\n\nReturns\n\nRegression targets of each level in the form of (I, t, r, b).\nReturn type bbox_targets (list[Tensor])\n\nclass mmdet.models.dense_heads .YOLACTHead (num_classes, in_channels,\n\nanchor_generator={‘octave_base_scale': 3, 'ratios': [0.5, 1.0,\n2.0], 'scales_per_octave': I, 'strides': [8, 16, 32, 64, 128],\n‘type’: ‘AnchorGenerator'}, loss_cls={'loss_weight': 1.0,\n‘reduction’: ‘none’, 'type': 'CrossEntropyLoss', 'use_sigmoid':\nFalse}, loss_bbox={'beta': 1.0, ‘loss_weight': 1.5, ‘type’:\n‘SmoothL1Loss'}, num_head_convs=1, num_protos=32,\nuse_ohem=True, conv_cfg=None, norm_cfg=None,\ninit_cfg=(‘bias': 0, ‘distribution': ‘uniform’, ‘layer’: 'Conv2d',\n‘type’: 'Xavier'}, **kwargs)\n\nYOLACT box head used in https://arxiv.org/abs/1904.02689.\n\n370 Chapter 39. mmdet.models\n", "vlm_text": "•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nproperty num anchors Returns: int: Number of anchors on each point of feature map. \nstar dc n offset ( bbox_pred ,  gradient mul ,  stride ) Compute the star deformable conv offsets. \nParameters \n•  bbox_pred  ( Tensor ) – Predicted bbox distance offsets (l, r, t, b). •  gradient mul  ( float ) – Gradient multiplier. •  stride  ( int ) – The corresponding stride for feature maps, used to project the bbox onto the feature map. Returns  The offsets for deformable convolution. \nReturn type  dc n offsets (Tensor) \ntransform b box targets ( decoded b boxes ,  ml vl points ,  num_imgs ) Transform b box targets (x1, y1, x2, y2) into (l, t, r, b) format. \nParameters \n•  decoded b boxes  ( list[Tensor] ) – Regression targets of each level, in the form of (x1, y1, x2, y2). •  ml vl points  ( list[Tensor] ) – Points of each fpn level, each has shape (num_points, 2). •  num_imgs  ( int ) – the number of images in a batch. \nReturns \nRegression targets of each level in  the form of (l, t, r, b). Return type  b box targets (list[Tensor]) \nclass  mmdet.models.dense heads. YOLACTHead ( num classes ,  in channels , \nanchor generator={'octave base scale': 3, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 1, 'strides': [8, 16, 32, 64, 128], 'type': 'Anchor Generator'} ,  loss_cls={'loss weight': 1.0, 'reduction': 'none', 'type': 'Cross Entropy Loss', 'use s igm oid': False}, loss_bbox $\\mathbf{\\beta}=$ {'beta': 1.0, 'loss_weight': 1.5, 'type':'Smooth L 1 Loss'} ,  num head con vs=1 ,  num_protos=32 , use_ohem  $\\mathbf{\\beta}=$  True ,  conv_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  norm_cfg=None , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'bias': 0, 'distribution': 'uniform', 'layer': 'Conv2d', 'type': 'Xavier'} ,  \\*\\*kwargs ) \nYOLACT box head used in  https://arxiv.org/abs/1904.02689 . "}
{"page": 378, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_378.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nNote that YOLACT head is a light version of RetinaNet head. Four differences are described as follows:\n1. YOLACT box head has three-times fewer anchors.\n2. YOLACT box head shares the convs for box and cls branches.\n3. YOLACT box head uses OHEM instead of Focal loss.\n4. YOLACT box head predicts a set of mask coefficients for each box.\n\nParameters\n* num_classes (int) — Number of categories excluding the background category.\n¢ in_channels (int) — Number of channels in the input feature map.\n* anchor_generator (dict) — Config dict for anchor generator\n* loss_cls (dict) — Config of classification loss.\n* loss_bbox (dict) — Config of localization loss.\n* num_head_convs (int) — Number of the conv layers shared by box and cls branches.\n* num_protos (int) — Number of the mask coefficients.\n\n* use_ohem (bool) — If true, loss_single_OHEM will be used for cls loss calculation. If\nfalse, loss_single will be used.\n\n* conv_cfg (dict) — Dictionary to construct and config conv layer.\n* norm_cfg (dict) — Dictionary to construct and config norm layer.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\nforward_single(x)\nForward feature of a single scale level.\nParameters x (Tensor) — Features of a single scale level.\n\nReturns cls_score (Tensor): Cls scores for a single scale level the channels number is\nnum_anchors * num_classes. bbox_pred (Tensor): Box energies / deltas for a single scale\nlevel, the channels number is num_anchors * 4. coeff_pred (Tensor): Mask coefficients for a\nsingle scale level, the channels number is num_anchors * num_protos.\n\nReturn type tuple\n\nget_bboxes (cls_scores, bbox_preds, coeff_preds, img_metas, cfg=None, rescale=False)\n“Similar to func:AnchorHead. get_bboxes, but additionally processes coeff_preds.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level with shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* coeff_preds (list [Tensor]) — Mask coefficients for each scale level with shape (N,\nnum_anchors * num_protos, H, W)\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* cfg (mmcv.Config | None) — Test / postprocessing configuration, if None, test_cfg\nwould be used\n\n39.4. dense_heads 371\n", "vlm_text": "Note that YOLACT head is a light version of RetinaNet head. Four differences are described as follows: \n1. YOLACT box head has three-times fewer anchors. 2. YOLACT box head shares the convs for box and cls branches. 3. YOLACT box head uses OHEM instead of Focal loss. 4. YOLACT box head predicts a set of mask coefficients for each box. \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  anchor generator  ( dict ) – Config dict for anchor generator •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. •  num head con vs  ( int ) – Number of the conv layers shared by box and cls branches. •  num_protos  ( int ) – Number of the mask coefficients. •  use_ohem  ( bool ) – If true,  loss single OH EM  will be used for cls loss calculation. If false,  loss single  will be used. •  conv_cfg  ( dict ) – Dictionary to construct and config conv layer. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward single  $(x)$  \nForward feature of a single scale level. \nParameters  x  ( Tensor ) – Features of a single scale level. Returns  cls_score (Tensor): Cls scores for a single scale level the channels number is num anchors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num anchors   $^{*}\\,4$  . coeff_pred (Tensor): Mask coefficients for a single scale level, the channels number is num anchors \\* num_protos. Return type  tuple \nget_bboxes ( cls_scores ,  bbox_preds ,  co eff p reds ,  img_metas ,  cfg  $=$  None ,  rescale  $:=$  False ) “Similar to func: AnchorHead.get_bboxes , but additionally processes co eff p reds. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level with shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  co eff p reds  ( list[Tensor] ) – Mask coefficients for each scale level with shape (N, num anchors \\* num_protos, H, W)•  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  cfg  ( mmcv.Config | None ) – Test / post processing configuration, if None, test_cfg would be used "}
{"page": 379, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_379.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ rescale (bool) — If True, return boxes in original image space. Default: False.\nReturns\n\nEach item in result_list is a 3-tuple. The first item is an (n, 5) tensor, where the first 4\ncolumns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\nbetween 0 and 1. The second item is an (n,) tensor where each item is the predicted class\nlabel of the corresponding box. The third item is an (n, num_protos) tensor where each\nitem is the predicted mask coefficients of instance inside the corresponding box.\n\nReturn type list[tuple[Tensor, Tensor, Tensor]]\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nA combination of the func:AnchorHead. loss and func:SSDHead. loss.\n\nWhen self.use_ohem == True, it functions like SSDHead.1loss, otherwise, it follows AnchorHead.\nloss. Besides, it additionally returns sampling_results.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level Has shape (N,\nnum_anchors * num_classes, H, W)\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_anchors * 4, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\ngt_labels (list [Tensor ]) — Class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — Specify which bounding boxes can be\nignored when computing the loss. Default: None\n\nReturns dict[str, Tensor]: A dictionary of loss components. List[:obj:SamplingResult]: Sam-\npler results for each image.\n\nReturn type tuple\n\nloss_single_OHEM(cls_score, bbox_pred, anchors, labels, label_weights, bbox_targets, bbox_weights,\nnum_total_samples)\n“See func:SSDHead. loss.\n\nclass mmdet.models.dense_heads.YOLACTProtonet (num_classes, in_channels=256, proto_channels=(256,\n256, 256, None, 256, 32), proto_kernel_sizes=(3, 3, 3, -\n2, 3, 1), include_last_relu=True, num_protos=32,\nloss_mask_weight=1.0, max_masks_to_train=100,\ninit_cfg=({ ‘distribution’: ‘uniform’, ‘override’: {‘name':\n‘protonet'}, 'type': 'Xavier'})\nYOLACT mask head used in https://arxiv.org/abs/1904.02689.\n\nThis head outputs the mask prototypes for YOLACT.\nParameters\n¢ in_channels (int) — Number of channels in the input feature map.\n* proto_channels (tuple[int])— Output channels of protonet convs.\n* proto_kernel_sizes (tuple[int]) — Kernel sizes of protonet convs.\n\n* include_last_relu (Bool) — If keep the last relu of protonet.\n\n372 Chapter 39. mmdet.models\n", "vlm_text": "•  rescale  ( bool ) – If True, return boxes in original image space. Default: False. \nReturns \nEach item in result list is  a 3-tuple. The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is an (n,) tensor where each item is the predicted class label of the corresponding box. The third item is an (n, num_protos) tensor where each item is the predicted mask coefficients of instance inside the corresponding box. \nReturn type  list[tuple[Tensor, Tensor, Tensor]] \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\beta}=$  None ) A combination of the func: AnchorHead.loss  and func: SSDHead.loss . \nWhen  self.use_ohem  $==$   True , it functions like  SSDHead.loss , otherwise, it follows  AnchorHead. loss . Besides, it additionally returns  sampling results . \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (N, num anchors \\* num classes, H, W) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num anchors \\* 4, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – Specify which bounding boxes can be ignored when computing the loss. Default: None Returns  dict[str, Tensor]: A dictionary of loss components. List[:obj: Sampling Result ]: Sam- pler results for each image. \nReturn type  tuple \nloss single OH EM ( cls_score ,  bbox_pred ,  anchors ,  labels ,  label weights ,  b box targets ,  b box weights , num total samples ) “See func: SSDHead.loss . \nclass  mmdet.models.dense heads. YO L ACT Proton et ( num classes ,  in channels  $:=$  256 ,  proto channels=(256, 256, 256, None, 256, 32) ,  proto kernel sizes=(3, 3, 3, - 2, 3, 1) ,  include last re lu  $=$  True ,  num_protos  $\\scriptstyle{\\varepsilon=32}$  , loss mask weight=1.0 ,  max masks to train  $\\scriptstyle{\\mathcal{S}}$  100 , init_cfg  $=$  {'distribution': 'uniform', 'override': {'name': 'protonet'}, 'type': 'Xavier'} ) \nYOLACT mask head used in  https://arxiv.org/abs/1904.02689 This head outputs the mask prototypes for YOLACT. \n\nParameters \n•  in channels  ( int ) – Number of channels in the input feature map. •  proto channels  ( tuple[int] ) – Output channels of protonet convs. •  proto kernel sizes  ( tuple[int] ) – Kernel sizes of protonet convs. •  include last re lu  ( Bool ) – If keep the last relu of protonet. "}
{"page": 380, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_380.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* num_protos (int) — Number of prototypes.\n\n* num_classes (int) — Number of categories excluding the background category.\n\n* loss_mask_weight (float) — Reweight the mask loss by this factor.\n\n* max_masks_to_train (int) —- Maximum number of masks to train for each image.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\ncrop (masks, boxes, padding=1)\nCrop predicted masks by zeroing out everything not in the predicted bbox.\n\nParameters\n\n¢ masks (Tensor) — shape [H, W, N].\n\n* boxes (Tensor) — bbox coords in relative point form with shape [N, 4].\nReturns The cropped masks.\nReturn type Tensor\n\nforward (x, coeff_pred, bboxes, img_meta, sampling_results=None)\nForward feature from the upstream network to get prototypes and linearly combine the prototypes, using\nmasks coefficients, into instance masks. Finally, crop the instance masks with given bboxes.\n\nParameters\n¢ x (Tensor) — Feature from the upstream network, which is a 4D-tensor.\n\n* coeff_pred (list [Tensor]) — Mask coefficients for each scale level with shape (N,\nnum_anchors * num_protos, H, W).\n\n* bboxes (list [Tensor]) — Box used for cropping with shape (N, num_anchors * 4, H,\nW). During training, they are ground truth boxes. During testing, they are predicted boxes.\n\n¢ img_meta (list [dict]) — Meta information of each image, e.g., image size, scaling fac-\ntor, etc.\n\n¢ sampling_results (List[:obj:SamplingResult]) — Sampler results for each image.\nReturns Predicted instance segmentation masks.\nReturn type list[Tensor]\n\nget_seg_masks (mask_pred, label_pred, img_meta, rescale)\nResize, binarize, and format the instance mask predictions.\n\nParameters\n¢ mask_pred (Tensor) — shape (N, H, W).\n¢ label_pred (Tensor) — shape (N, ).\n¢ img_meta (dict) — Meta information of each image, e.g., image size, scaling factor, etc.\n* rescale (bool) — If rescale is False, then returned masks will fit the scale of imgs[0].\nReturns Mask predictions grouped by their predicted classes.\nReturn type list[ndarray]\n\nget_targets(mask_pred, gt_masks, pos_assigned_gt_inds)\nCompute instance segmentation targets for each image.\n\nParameters\n\n¢ mask_pred (Tensor) — Predicted prototypes with shape (num_classes, H, W).\n\n39.4. dense_heads 373\n", "vlm_text": "•  num_protos  ( int ) – Number of prototypes. •  num classes  ( int ) – Number of categories excluding the background category. •  loss mask weight  ( float ) – Reweight the mask loss by this factor. •  max masks to train  ( int ) – Maximum number of masks to train for each image. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \ncrop ( masks ,  boxes ,  padding  $\\scriptstyle{:=I}$  ) Crop predicted masks by zeroing out everything not in the predicted bbox. \nParameters •  masks  ( Tensor ) – shape [H, W, N]. •  boxes  ( Tensor ) – bbox coords in relative point form with shape [N, 4]. Returns  The cropped masks. Return type  Tensor \nforward ( x ,  coeff_pred ,  bboxes ,  img_meta ,  sampling results  $\\mathbf{\\beta}=$  None ) Forward feature from the upstream network to get prototypes and linearly combine the prototypes, using masks coefficients, into instance masks. Finally, crop the instance masks with given bboxes. \nParameters \n•  x  ( Tensor ) – Feature from the upstream network, which is a 4D-tensor. •  coeff_pred  ( list[Tensor] ) – Mask coefficients for each scale level with shape (N, num anchors \\* num_protos, H, W).•  bboxes  ( list[Tensor] ) – Box used for cropping with shape (N, num anchors \\* 4, H, W). During training, they are ground truth boxes. During testing, they are predicted boxes. •  img_meta  ( list[dict] ) – Meta information of each image, e.g., image size, scaling fac- tor, etc. •  sampling results  (List[:obj: Sampling Result ]) – Sampler results for each image. Returns  Predicted instance segmentation masks. \nReturn type  list[Tensor] \nget seg masks ( mask_pred ,  label_pred ,  img_meta ,  rescale ) Resize, binarize, and format the instance mask predictions. \nParameters \n•  mask_pred  ( Tensor ) – shape (N, H, W). •  label_pred  ( Tensor ) – shape (N, ). •  img_meta  ( dict ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool ) – If rescale is False, then returned masks will fit the scale of imgs[0]. Returns  Mask predictions grouped by their predicted classes. Return type  list[ndarray] \nget targets ( mask_pred ,  gt_masks ,  pos assigned gt in ds ) Compute instance segmentation targets for each image. \nParameters \n•  mask_pred  ( Tensor ) – Predicted prototypes with shape (num classes, H, W). "}
{"page": 381, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_381.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* gt_masks (Tensor) — Ground truth masks for each image with the same shape of the input\nimage.\n\n* pos_assigned_gt_inds (Tensor) — GT indices of the corresponding positive samples.\nReturns\n\nInstance segmentation targets with shape (num_instances, H, W).\nReturn type Tensor\n\nloss (mask_pred, gt_masks, gt_bboxes, img_meta, sampling_results)\nCompute loss of the head.\n\nParameters\n¢ mask_pred (list [Tensor ]) — Predicted prototypes with shape (num_classes, H, W).\n\n¢ gt_masks (list [Tensor])— Ground truth masks for each image with the same shape of\nthe input image.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ img_meta (list [dict]) — Meta information of each image, e.g., image size, scaling fac-\ntor, etc.\n\n¢ sampling_results (List[:obj:SamplingResult]) — Sampler results for each image.\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nsanitize_coordinates (x/, x2, img_size, padding=0, cast=True)\nSanitizes the input coordinates so that x1 < x2, xl != x2, xl >= 0, and x2 <= image_size. Also converts\nfrom relative to absolute coordinates and casts the results to long tensors.\n\nWarning: this does things in-place behind the scenes so copy if necessary.\n\nParameters\n\n* _x1 (Tensor) — shape (N, ).\n\n* _x2 (Tensor) — shape (N, ).\n\n¢ img_size (int) — Size of the input image.\n\n¢ padding (int) — x1 >= padding, x2 <= image_size-padding.\n\n* cast (bool) - If cast is false, the result won’t be cast to longs.\nReturns x1 (Tensor): Sanitized _x1. x2 (Tensor): Sanitized _x2.\nReturn type tuple\n\nsimple_test (feats, det_bboxes, det_labels, det_coeffs, img_metas, rescale=False)\nTest function without test-time augmentation.\n\nParameters\n\n¢ feats (tuple[torch. Tensor]) — Multi-level features from the upstream network, each\nis a 4D-tensor.\n\n¢ det_bboxes (list [Tensor])—BBox results of each image. each element is (n, 5) tensor,\nwhere 5 represent (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1.\n\n¢ det_labels (list [Tensor ]) — BBox results of each image. each element is (n, ) tensor,\neach element represents the class label of the corresponding box.\n\n374 Chapter 39. mmdet.models\n", "vlm_text": "•  gt_masks  ( Tensor ) – Ground truth masks for each image with the same shape of the input image. •  pos assigned gt in ds  ( Tensor ) – GT indices of the corresponding positive samples. \nReturns \nInstance segmentation targets with shape  (num instances, H, W). Return type  Tensor \nloss ( mask_pred ,  gt_masks ,  gt_bboxes ,  img_meta ,  sampling results ) Compute loss of the head. \nParameters \n•  mask_pred  ( list[Tensor] ) – Predicted prototypes with shape (num classes, H, W). •  gt_masks  ( list[Tensor] ) – Ground truth masks for each image with the same shape of the input image. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  img_meta  ( list[dict] ) – Meta information of each image, e.g., image size, scaling fac- tor, etc. •  sampling results  (List[:obj: Sampling Result ]) – Sampler results for each image. \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] \nsanitize coordinates  $(x l,x2$  ,  img_size ,  padding  $\\scriptstyle{\\prime=0}$  ,  cast=True ) Sanitizes the input coordinates so that   $\\tt X1<x2$  ,   $\\mathrm{x}1\\stackrel{!}{=}\\mathrm{x}2$  ,   $\\mathrm{x}1>=0$  , and   $\\mathbf{x}2<=$  image_size. Also converts from relative to absolute coordinates and casts the results to long tensors. \nWarning: this does things in-place behind the scenes so copy if necessary. \nParameters \n•  _x1  ( Tensor ) – shape (N, ). •  _x2  ( Tensor ) – shape (N, ). •  img_size  ( int ) – Size of the input image. •  padding    $(i n t)-\\mathrm{x}1>=$   padding,  $\\mathbf{X}2<=$   image_size-padding. •  cast  ( bool ) – If cast is false, the result won’t be cast to longs. \nReturns  x1 (Tensor): Sanitized _x1. x2 (Tensor): Sanitized  $\\_{\\mathrm{X}2}$  . \nReturn type  tuple \nsimple test ( feats ,  det_bboxes ,  det_labels ,  det_coeffs ,  img_metas ,  rescale  $\\mathbf{\\dot{\\rho}}=$  False ) Test function without test-time augmentation. \nParameters \n•  feats  ( tuple[torch.Tensor] ) – Multi-level features from the upstream network, each is a 4D-tensor. •  det_bboxes  ( list[Tensor] ) – BBox results of each image. each element is (n, 5) tensor, where 5 represent (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1. •  det_labels  ( list[Tensor] ) – BBox results of each image. each element is (n, ) tensor, each element represents the class label of the corresponding box. "}
{"page": 382, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_382.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* det_coeffs (list [Tensor]) — BBox coefficient of each image. each element is (n, m)\ntensor, m is vector length.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\nReturns\n\nencoded masks. The c-th item in the outer list corresponds to the c-th class. Given the c-\nth outer list, the i-th item in that inner list is the mask for the i-th box with class label\nc.\n\nReturn type list[list]\n\nclass mmdet.models.dense_heads .YOLACTSegmHead (num_classes, in_channels=256,\nloss_segm={'‘loss_weight': 1.0, ‘type’:\n‘CrossEntropyLoss’, 'use_sigmoid': True},\ninit_cfg=({ ‘distribution’: ‘uniform’, ‘override’: {‘name':\n‘segm_conv'}, 'type': 'Xavier'})\nYOLACT segmentation head used in https://arxiv.org/abs/1904.02689.\n\nApply a semantic segmentation loss on feature space using layers that are only evaluated during training to\nincrease performance with no speed penalty.\n\nParameters\n¢ in_channels (int) — Number of channels in the input feature map.\n* num_classes (int) — Number of categories excluding the background category.\n* loss_segm (dict) — Config of semantic segmentation loss.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward(x)\nForward feature from the upstream network.\n\nParameters x (Tensor) — Feature from the upstream network, which is a 4D-tensor.\nReturns\n\nPredicted semantic segmentation map with shape (N, num_classes, H, W).\nReturn type Tensor\n\nget_targets(segm_pred, gt_masks, gt_labels)\nCompute semantic segmentation targets for each image.\n\nParameters\n\n* segm_pred (Tensor) — Predicted semantic segmentation map with shape (num_classes,\nH, W).\n\n* gt_masks (Tensor) — Ground truth masks for each image with the same shape of the input\nimage.\n\n¢ gt_labels (Tensor) — Class indices corresponding to each box.\nReturns\nSemantic segmentation targets with shape (num_classes, H, W).\n\nReturn type Tensor\n\n39.4. dense_heads 375\n", "vlm_text": "•  det_coeffs  ( list[Tensor] ) – BBox coefficient of each image. each element is (n, m) tensor, m is vector length. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns \nencoded masks. The c-th item in the outer list  corresponds to the c-th class. Given the c- th outer list, the i-th item in that inner list is the mask for the i-th box with class label c. \nReturn type  list[list] \nclass  mmdet.models.dense heads. YO LAC TSe gm Head ( num classes ,  in channels=256 , loss_segm={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'distribution': 'uniform', 'override': {'name': 'segm_conv'}, 'type': 'Xavier'} ) \nYOLACT segmentation head used in  https://arxiv.org/abs/1904.02689 \nApply a semantic segmentation loss on feature space using layers that are only evaluated during training to increase performance with no speed penalty. \nParameters \n•  in channels  ( int ) – Number of channels in the input feature map. •  num classes  ( int ) – Number of categories excluding the background category. •  loss_segm  ( dict ) – Config of semantic segmentation loss. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward  $(x)$  Forward feature from the upstream network. \nParameters  x  ( Tensor ) – Feature from the upstream network, which is a 4D-tensor. \nReturns \nPredicted semantic segmentation map with shape  (N, num classes, H, W). Return type  Tensor \nget targets ( segm_pred ,  gt_masks ,  gt_labels ) Compute semantic segmentation targets for each image. \nParameters \n•  segm_pred  ( Tensor ) – Predicted semantic segmentation map with shape (num classes, H, W). •  gt_masks  ( Tensor ) – Ground truth masks for each image with the same shape of the input image. •  gt_labels  ( Tensor ) – Class indices corresponding to each box. \nReturns \nSemantic segmentation targets with shape  (num classes, H, W). \nReturn type  Tensor "}
{"page": 383, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_383.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nloss(segm_pred, gt_masks, gt_labels)\nCompute loss of the head.\n\nParameters\n\n* segm_pred (list[Tensor]) — Predicted semantic segmentation map with shape (N,\nnum_classes, H, W).\n\n¢ gt_masks (list [Tensor])— Ground truth masks for each image with the same shape of\nthe input image.\n\n¢ gt_labels (list [Tensor ]) — Class indices corresponding to each box.\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nsimple_test (feats, img_metas, rescale=False)\nTest function without test-time augmentation.\n\nclass mmdet.models.dense_heads.YOLOFHead (num_classes, in_channels, num_cls_convs=2,\nnum_reg_convs=4, norm_cfg={'requires_grad': True, 'type':\n‘BN'}, **kwargs)\nYOLOFHead Paper link: https://arxiv.org/abs/2103.09460.\n\nParameters\n* num_classes (int) — The number of object classes (w/o background)\n¢ in_channels (List [int ]) — The number of input channels per scale.\n¢ cls_num_convs (int) — The number of convolutions of cls branch. Default 2.\n* reg_num_convs (int) — The number of convolutions of reg branch. Default 4.\n* norm_cfg (dict) — Dictionary to construct and config norm layer.\n\nforward_single (feature)\nForward feature of a single scale level.\n\nParameters x (Tensor) — Features of a single scale level.\n\nReturns cls_score (Tensor): Cls scores for a single scale level the channels number is\nnum_base_priors * num_classes. bbox_pred (Tensor): Box energies / deltas for a single scale\nlevel, the channels number is num_base_priors * 4.\n\nReturn type tuple\n\nget_targets(c/ls_scores_list, bbox_preds_list, anchor_list, valid_flag_list, gt_bboxes_list, img_metas,\ngt_bboxes_ignore_list=None, gt_labels_list=None, label_channels=1, unmap_outputs=True)\nCompute regression and classification targets for anchors in multiple images.\n\nParameters\n\n¢ cls_scores_list (list [Tensor]) — each image. each is a 4D-tensor, the shape is (h *\nw, num_anchors * num_classes).\n\n¢ bbox_preds_list (list[Tensor]) — each is a 4D-tensor, the shape is (h * w,\nnum_anchors * 4).\n\n¢ anchor_list (list [Tensor]) — Anchors of each image. Each element of is a tensor of\nshape (h * w * num_anchors, 4).\n\n¢ valid_flag_list (list [Tensor]) — Valid flags of each image. Each element of is a\ntensor of shape (h * w * num_anchors, )\n\n376 Chapter 39. mmdet.models\n", "vlm_text": "loss ( segm_pred ,  gt_masks ,  gt_labels ) Compute loss of the head. \nParameters \n•  segm_pred  ( list[Tensor] ) – Predicted semantic segmentation map with shape (N, num classes, H, W). •  gt_masks  ( list[Tensor] ) – Ground truth masks for each image with the same shape of the input image. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box. \nReturns  A dictionary of loss components. Return type  dict[str, Tensor] \nsimple test ( feats ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test function without test-time augmentation. \nclass  mmdet.models.dense heads. YOLOFHead ( num classes ,  in channels ,  num cls con vs  $\\scriptstyle{\\prime=2}$  , num reg con vs  $\\scriptstyle{\\prime}=4$  ,  norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} ,  \\*\\*kwargs ) YOLOFHead Paper link:  https://arxiv.org/abs/2103.09460 . \nParameters \n•  num classes  ( int ) – The number of object classes (w/o background) •  in channels  ( List[int] ) – The number of input channels per scale. •  cls num con vs  ( int ) – The number of convolutions of cls branch. Default 2. •  reg num con vs  ( int ) – The number of convolutions of reg branch. Default 4. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. \nforward single ( feature ) Forward feature of a single scale level. Parameters  x  ( Tensor ) – Features of a single scale level. Returns  cls_score (Tensor): Cls scores for a single scale level the channels number is num base priors \\* num classes. bbox_pred (Tensor): Box energies / deltas for a single scale level, the channels number is num base priors  $^{*}\\,4$  . Return type  tuple \nget targets ( cls scores list ,  b box p reds list ,  anchor list ,  valid flag list ,  gt b boxes list ,  img_metas , gt b boxes ignore lis  $\\mathbf{\\dot{\\rho}}=$  None ,  gt labels list  $\\leftrightharpoons$  None ,  label channel  $\\scriptstyle{\\mathfrak{s}}=I$  ,  un map outputs  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ) Compute regression and classification targets for anchors in multiple images. \nParameters \n•  cls scores list  ( list[Tensor] ) – each image. each is a 4D-tensor, the shape is (h \\* w, num anchors \\* num classes). •  b box p reds list  ( list[Tensor] ) – each is a 4D-tensor, the shape is   $(\\textbf{h}^{*}\\ \\textbf{w},$  , num anchors  $^{*}\\,4$  ). •  anchor list  ( list[Tensor] ) – Anchors of each image. Each element of is a tensor of shape   $(\\mathbf{h}^{\\ast}\\mathbf{\\Sigma}^{\\ast}\\mathbf{w}^{\\ast}$   num anchors, 4). •  valid flag list  ( list[Tensor] ) – Valid flags of each image. Each element of is a tensor of shape   $(\\mathbf{h}^{\\mathbf{\\alpha}*}\\mathbf{w}^{\\mathbf{\\alpha}*}$   num anchors, ) "}
{"page": 384, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_384.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ngt_bboxes_list (list [Tensor ]) — Ground truth bboxes of each image.\nimg_metas (list [dict]) — Meta info of each image.\n\ngt_bboxes_ignore_list (list [Tensor]) — Ground truth bboxes to be ignored.\ngt_labels_list (list [Tensor]) — Ground truth labels of each box.\nlabel_channels (int) — Channel of label.\n\nunmap_outputs (bool) — Whether to map outputs back to the original set of anchors.\n\nReturns\n\nUsually returns a tuple containing learning targets.\n\nbatch_labels (Tensor): Label of all images. Each element of is a tensor of shape (batch, h\n* w * num_anchors)\n\nbatch_label_weights (Tensor): Label weights of all images of is a tensor of shape (batch, h\n* w * num_anchors)\n\nnum_total_pos (int): Number of positive samples in all images.\n\nnum_total_neg (int): Number of negative samples in all images.\n\nadditional_returns: This function enables user-defined returns from\n\nself._get_targets_single. These returns are currently refined to properties at each\nfeature map (i.e. having HxW dimension). The results will be concatenated after the end\n\nReturn type tuple\n\ninit_weights()\nInitialize the weights.\n\nloss (cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute losses of the head.\n\nParameters\n\ncls_scores (list[Tensor]) — Box scores for each scale level Has shape (batch,\nnum_anchors * num_classes, h, w)\n\nbbox_preds (list [Tensor]) — Box energies / deltas for each scale level with shape\n(batch, num_anchors * 4, h, w)\n\ngt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\ngt_labels (list [Tensor ]) — class indices corresponding to each box\n\nimg_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\ngt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss. Default: None\n\nReturns A dictionary of loss components.\n\nReturn type dict[str, Tensor]\n\n39.4. dense_heads\n\n377\n", "vlm_text": "•  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  img_metas  ( list[dict] ) – Meta info of each image. •  gt b boxes ignore list  ( list[Tensor] ) – Ground truth bboxes to be ignored. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. •  label channels  ( int ) – Channel of label. •  un map outputs  ( bool ) – Whether to map outputs back to the original set of anchors. \nReturns \n• batch labels (Tensor): Label of all images. Each element of is a tensor of shape (batch, h \\* w \\* num anchors) • batch label weights (Tensor): Label weights of all images of is a tensor of shape (batch, h \\* w \\* num anchors) • num total pos (int): Number of positive samples in all images. • num total ne g (int): Number of negative samples in all images. \nadditional returns: This function enables user-defined returns from self.get targets single . These returns are currently refined to properties at each feature map (i.e. having HxW dimension). The results will be concatenated after the end \nReturn type  tuple \nin it weights()Initialize the weights. \nloss ( cls_scores ,  bbox_preds ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None ) Compute losses of the head. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level Has shape (batch, num anchors \\* num classes, h, w) •  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (batch, num anchors \\* 4, h, w) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. Default: None \nReturns  A dictionary of loss components. \nReturn type  dict[str, Tensor] "}
{"page": 385, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_385.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.dense_heads. YOLOV3Head (num_classes, in_channels, out_channels=(1024, 512, 256),\nanchor_generator={'base_sizes': [[(116, 90), (156, 198),\n(373, 326)], [(30, 61), (62, 45), (59, 119)], [(10, 13), (16, 30),\n(33, 23)]], ‘strides': [32, 16, 8], ‘type’:\n'YOLOAnchorGenerator'}, bbox_coder={'type':\n'YOLOBBoxCoder'}, featmap_strides=[32, 16, 8],\none_hot_smoother=0.0, conv_cfg=None,\nnorm_cfg={ ‘requires_grad': True, 'type': 'BN'},\nact_cfg={‘'negative_slope': 0.1, 'type': 'LeakyReLU'},\nloss_cls={‘loss_weight': 1.0, 'type': 'CrossEntropyLoss'’,\n‘use_sigmoid': True}, loss_conf={‘loss_weight': 1.0, 'type':\n‘CrossEntropyLoss’, 'use_sigmoid': True},\nloss_xy=(‘loss_weight': 1.0, ‘type’: 'CrossEntropyLoss'’,\n‘use_sigmoid': True}, loss_wh={‘loss_weight': 1.0, 'type':\n'MSELoss'}, train_cfg=None, test_cfg=None,\ninit_cfg=(‘override': {‘name': 'convs_pred'}, ‘std’: 0.01,\n‘type’: 'Normal'})\n\nYOLOV3Head Paper link: https://arxiv.org/abs/1804.02767.\n\nParameters\n* num_classes (int) — The number of object classes (w/o background)\n¢ in_channels (List [int ]) — Number of input channels per scale.\n\n* out_channels (List [int ]) — The number of output channels per scale before the final\n1x1 layer. Default: (1024, 512, 256).\n\n* anchor_generator (dict) — Config dict for anchor generator\n* bbox_coder (dict) — Config of bounding box coder.\n\n¢ featmap_strides (List [int ])—The stride of each scale. Should be in descending order.\nDefault: (32, 16, 8).\n\n* one_hot_smoother (float) — Set a non-zero value to enable label-smooth Default: 0.\n* conv_cfg (dict) — Config dict for convolution layer. Default: None.\n\n* norm_cfg (dict) — Dictionary to construct and config norm layer. Default: dict(type=’BN’,\nrequires_grad=True)\n\n* act_cfg (dict) — Config dict for activation layer. Default: dict(type=’ LeakyReLU’, nega-\ntive_slope=0.1).\n\n* loss_cls (dict) — Config of classification loss.\n\n* loss_conf (dict) — Config of confidence loss.\n\n* loss_xy (dict) — Config of xy coordinate loss.\n\n* loss_wh (dict) — Config of wh coordinate loss.\n\n* train_cfg (dict) — Training config of YOLOV3 head. Default: None.\n\n* test_cfg (dict) — Testing config of YOLOV3 head. Default: None.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\naug_test (feats, img_metas, rescale=False)\nTest function with test time augmentation.\n\nParameters\n\n378 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.dense heads. YOLOV3Head ( num classes ,  in channels ,  out_channels=(1024, 512, 256) , anchor_generator={'base_sizes': [[(116, 90), (156, 198), (373, 326)], [(30, 61), (62, 45), (59, 119)], [(10, 13), (16, 30), (33, 23)]], 'strides': [32, 16, 8], 'type': 'YO LO Anchor Generator'} ,  bbox_coder={'type': 'YO LOB Box Code r'} ,  feat map strides=[32, 16, 8] , one hot smoother=0.0 ,  conv_cfg=None , norm_cfg  $\\mathrm{=}$  {'requires grad': True, 'type': 'BN'} , act_cfg  $=$  {'negative slope': 0.1, 'type': 'LeakyReLU'} , loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  loss_conf={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_xy={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  loss_wh={'loss weight': 1.0, 'type': 'MSELoss'} ,  train_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  test_cfg=None , init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'override': {'name': 'convs_pred'}, 'std': 0.01, 'type': 'Normal'} ) \nYOLOV3Head Paper link:  https://arxiv.org/abs/1804.02767 . \nParameters \n•  num classes  ( int ) – The number of object classes (w/o background) •  in channels  ( List[int] ) – Number of input channels per scale. •  out channels  ( List[int] ) – The number of output channels per scale before the final 1x1 layer. Default: (1024, 512, 256). •  anchor generator  ( dict ) – Config dict for anchor generator •  bbox_coder  ( dict ) – Config of bounding box coder. •  feat map strides  ( List[int] ) – The stride of each scale. Should be in descending order. Default: (32, 16, 8). •  one hot smoother  ( float ) – Set a non-zero value to enable label-smooth Default: 0. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Dictionary to construct and config norm layer. Default: dict(type  $='$  ’BN’, requires grad  $\\scriptstyle\\varepsilon=$  True) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $=^{!}$  ’LeakyReLU’, nega- tive_slope  $\\mathrm{{=}}0.1$  ). •  loss_cls  ( dict ) – Config of classification loss. •  loss_conf  ( dict ) – Config of confidence loss. •  loss_xy  ( dict ) – Config of xy coordinate loss. •  loss_wh  ( dict ) – Config of wh coordinate loss. •  train_cfg  ( dict ) – Training config of YOLOV3 head. Default: None. •  test_cfg  ( dict ) – Testing config of YOLOV3 head. Default: None. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \naug_test ( feats ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test function with test time augmentation. Parameters "}
{"page": 386, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_386.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ feats (list [Tensor ]) — the outer list indicates test-time augmentations and inner Tensor\nshould have a shape NxCxHxW, which contains features for all images in the batch.\n\n¢ img_metas (list[list[dict]]) — the outer list indicates test-time augs (multiscale,\nflip, etc.) and the inner list indicates images in a batch. each dict has image information.\n\n¢ rescale (bool, optional) — Whether to rescale the results. Defaults to False.\nReturns bbox results of each class\nReturn type list[ndarray]\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\n\nA tuple of multi-level predication map, each isa 4D-tensor of shape  (batch_size,\n5+num_classes, height, width).\n\nReturn type tuple[Tensor]\n\nget_bboxes (pred_maps, img_metas, cfg=None, rescale=False, with_nms=True)\nTransform network output for a batch into bbox predictions. It has been accelerated since PR #5991.\n\nParameters\n* pred_maps (list [Tensor]) — Raw predictions for a batch of images.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* cfg (mmcv.Config | None) — Test / postprocessing configuration, if None, test_cfg\nwould be used. Default: None.\n\n¢ rescale (bool) — If True, return boxes in original image space. Default: False.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default: True.\nReturns\n\nEach item in result_list is 2-tuple. The first item is an (n, 5) tensor, where 5 represent (tl_x,\ntl_y, br_x, br_y, score) and the score between 0 and 1. The shape of the second tensor in\nthe tuple is (n,), and each element represents the class label of the corresponding box.\n\nReturn type list[tuple[Tensor, Tensor]]\n\nget_targets(anchor_list, responsible_flag_list, gt_bboxes_list, gt_labels_list)\nCompute target maps for anchors in multiple images.\n\nParameters\n\n¢ anchor_list (list [list [Tensor]]) — Multi level anchors of each image. The outer\nlist indicates images, and the inner list corresponds to feature levels of the image. Each\nelement of the inner list is a tensor of shape (num_total_anchors, 4).\n\n* responsible_flag_list (list [list [Tensor]]) — Multi level responsible flags of\neach image. Each element is a tensor of shape (num_total_anchors, )\n\n* gt_bboxes_list (list [Tensor]) — Ground truth bboxes of each image.\n¢ gt_labels_list (list [Tensor ]) — Ground truth labels of each box.\n\nReturns\n\n39.4. dense_heads 379\n", "vlm_text": "•  feats  ( list[Tensor] ) – the outer list indicates test-time augmentations and inner Tensor should have a shape NxCxHxW, which contains features for all images in the batch. •  img_metas  ( list[list[dict]] ) – the outer list indicates test-time augs (multiscale, flip, etc.) and the inner list indicates images in a batch. each dict has image information. •  rescale  ( bool, optional ) – Whether to rescale the results. Defaults to False. \nReturns  bbox results of each class \nReturn type  list[ndarray] \nforward ( feats ) Forward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nA tuple of multi-level predication map, each is a  4D-tensor of shape (batch_size,  $^{5+}$  num classes, height, width). \nReturn type  tuple[Tensor] \nget_bboxes ( pred_maps ,  img_metas ,  cfg  $=$  None ,  rescale  $=$  False ,  with_nms  $\\mathbf{:=}$  True ) Transform network output for a batch into bbox predictions. It has been accelerated since PR #5991. \nParameters \n•  pred_maps  ( list[Tensor] ) – Raw predictions for a batch of images. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  cfg  ( mmcv.Config | None ) – Test / post processing configuration, if None, test_cfg would be used. Default: None. •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default: True. \nReturns \nEach item in result list is 2-tuple.  The first item is an (n, 5) tensor, where 5 represent (tl_x, tl_y, br_x, br_y, score) and the score between 0 and 1. The shape of the second tensor in the tuple is (n,), and each element represents the class label of the corresponding box. \nReturn type  list[tuple[Tensor, Tensor]] \nget targets ( anchor list ,  responsible flag list ,  gt b boxes list ,  gt labels list ) Compute target maps for anchors in multiple images. \nParameters \n•  anchor list  ( list[list[Tensor]] ) – Multi level anchors of each image. The outer list indicates images, and the inner list corresponds to feature levels of the image. Each element of the inner list is a tensor of shape (num total anchors, 4). •  responsible flag list  ( list[list[Tensor]] ) – Multi level responsible flags of each image. Each element is a tensor of shape (num total anchors, ) •  gt b boxes list  ( list[Tensor] ) – Ground truth bboxes of each image. •  gt labels list  ( list[Tensor] ) – Ground truth labels of each box. \nReturns "}
{"page": 387, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_387.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nUsually returns a tuple containing learning targets.\n* target_map_list (list{Tensor]): Target map of each level.\n* neg_map_list (list{Tensor]): Negative map of each level.\nReturn type tuple\n\ninit_weights()\nInitialize the weights.\n\nloss (pred_maps, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head.\n\nParameters\n\n* pred_maps (list[Tensor]) — Prediction map for each scale level, shape (N,\nnum_anchors * num_attrib, H, W)\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\nReturns A dictionary of loss components.\nReturn type dict[str, Tensor]\n\nloss_single(pred_map, target_map, neg_map)\nCompute loss of a single image from a batch.\n\nParameters\n* pred_map (Tensor) — Raw predictions for a single level.\n* target_map (Tensor) — The Ground-Truth target for a single level.\n* neg_map (Tensor) — The negative masks for a single level.\n\nReturns loss_cls (Tensor): Classification loss. loss_conf (Tensor): Confidence loss. loss_xy\n(Tensor): Regression loss of x, y coordinate. loss_wh (Tensor): Regression loss of w, h\ncoordinate.\n\nReturn type tuple\n\nproperty num_anchors\nReturns: int: Number of anchors on each point of feature map.\n\nproperty num_attrib\nnumber of attributes in pred_map, bboxes (4) + objectness (1) + num_classes\n\nType int\n\nonnx_export (pred_maps, img_metas, with_nms=True)\nTransform network output for a batch into bbox predictions.\n\nParameters\n\n¢ cls_scores (list[Tensor]) — Box scores for each scale level with shape (N,\nnum_points * num_classes, H, W).\n\n380 Chapter 39. mmdet.models\n", "vlm_text": "Usually returns a tuple containing learning targets. \n• target map list (list[Tensor]): Target map of each level. • ne g map list (list[Tensor]): Negative map of each level. \nReturn type  tuple \nin it weights()Initialize the weights. \nloss ( pred_maps ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore=None ) Compute loss of the head. \nParameters \n•  pred_maps  ( list[Tensor] ) – Prediction map for each scale level, shape (N, num anchors \\* num_attrib, H, W) •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss.  A dictionary of loss components. \nloss single ( pred_map ,  target_map ,  neg_map ) Compute loss of a single image from a batch. \nParameters \n•  pred_map  ( Tensor ) – Raw predictions for a single level. •  target_map  ( Tensor ) – The Ground-Truth target for a single level. •  neg_map  ( Tensor ) – The negative masks for a single level. Returns  loss_cls (Tensor): Classification loss. loss_conf (Tensor): Confidence loss. loss_xy (Tensor): Regression loss of x, y coordinate. loss_wh (Tensor): Regression loss of w, h coordinate. \nReturn type  tuple \nproperty num anchors Returns: int: Number of anchors on each point of feature map. \nproperty num_attrib number of attributes in pred_map, bboxes (4)  $^+$   objectness (1)  $^+$   num classes \nType  int \non nx export ( pred_maps ,  img_metas ,  with_nms  $\\mathbf{\\check{\\mathbf{\\alpha}}}$  True ) Transform network output for a batch into bbox predictions. \nParameters \n•  cls_scores  ( list[Tensor] ) – Box scores for each scale level with shape (N, num_points \\* num classes, H, W). "}
{"page": 388, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_388.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ bbox_preds (list [Tensor ]) — Box energies / deltas for each scale level with shape (N,\nnum_points * 4, H, W).\n\n* score_factors (list [Tensor]) — score_factors for each s cale level with shape (N,\nnum_points * 1, H, W). Default: None.\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc. Default: None.\n\n¢ with_nms (bool) — Whether apply nms to the bboxes. Default: True.\n\nReturns When with_nms is True, it is tuple[Tensor, Tensor], first tensor bboxes with shape [N,\n\nnum_det, 5], 5 arrange as (x1, yl, x2, y2, score) and second element is class labels of shape\n[N, num_det]. When with_nms is False, first tensor is bboxes with shape [N, num_det, 4],\nsecond tensor is raw score has shape [N, num_det, num_classes].\n\nReturn type tuple[Tensor, Tensor] | list[tuple]\n\nclass mmdet.models.dense_heads . YOLOXHead (num_classes, in_channels, feat_channels=256,\n\nstacked_convs=2, strides=[8, 16, 32], use_depthwise=False,\ndcn_on_last_conv=False, conv_bias='auto', conv_cfg=None,\nnorm_cfg={'eps': 0.001, 'momentum': 0.03, 'type': 'BN'},\nact_cfg=('type': ‘Swish'}, loss_cls={‘loss_weight': 1.0,\n‘reduction': ‘sum’, 'type': 'CrossEntropyLoss’, 'use_sigmoid':\nTrue}, loss_bbox={'eps': le-16, ‘loss_weight': 5.0, 'mode':\n‘square’, 'reduction': ‘sum’, 'type': 'ToULoss'},\nloss_obj={'loss_weight': 1.0, 'reduction': ‘sum’, 'type':\n‘CrossEntropyLoss’, 'use_sigmoid': True},\nloss_l1={'loss_weight': 1.0, 'reduction': ‘sum’, 'type':\n‘LILoss'}, train_cfg=None, test_cfg=None, init_cfg=('a':\n2.23606797749979, ‘distribution’: ‘uniform’, ‘layer’: 'Conv2d',\n‘mode’: ‘fan_in', ‘nonlinearity’: ‘leaky_relu’, 'type': 'Kaiming'})\n\nYOLOXHead head used in YOLOX.\n\nParameters\n\nnum_classes (int) — Number of categories excluding the background category.\nin_channels (int) — Number of channels in the input feature map.\nfeat_channels (int) — Number of hidden channels in stacking convs. Default: 256\nstacked_convs (int) — Number of stacking convs of the head. Default: 2.\nstrides (tuple) —- Downsample factor of each feature map.\n\nuse_depthwise (bool) — Whether to depthwise separable convolution in blocks. Default:\nFalse\n\ndcn_on_last_conv (boo1) — If true, use den in the last layer of towers. Default: False.\n\nconv_bias (bool | str) -— If specified as auto, it will be decided by the norm_cfg. Bias\nof conv will be set as True if norm_cfg is None, otherwise False. Default: “auto”.\n\nconv_cfg (dict) — Config dict for convolution layer. Default: None.\nnorm_cfg (dict) — Config dict for normalization layer. Default: None.\nact_cfg (dict) — Config dict for activation layer. Default: None.\nloss_cls (dict) — Config of classification loss.\n\nloss_bbox (dict) — Config of localization loss.\n\n39.4. dense_heads 381\n", "vlm_text": "•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level with shape (N, num_points \\* 4, H, W). •  score factors  ( list[Tensor] ) – score factors for each s cale level with shape (N, num_points \\* 1, H, W). Default: None. •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. Default: None. •  with_nms  ( bool ) – Whether apply nms to the bboxes. Default: True. \nReturns  When  with_nms  is True, it is tuple[Tensor, Tensor], first tensor bboxes with shape [N, num_det, 5], 5 arrange as (x1, y1, x2, y2, score) and second element is class labels of shape [N, num_det]. When  with_nms  is False, first tensor is bboxes with shape [N, num_det, 4], second tensor is raw score has shape [N, num_det, num classes]. \nReturn type  tuple[Tensor, Tensor] | list[tuple] \n( num classes ,  in channels ,  feat channel  $\\imath{=}256$  , stacked con vs  $^{=2}$  ,  strides=[8, 16, 32] ,  use depth wise  $\\mathbf{=}$  False , dc n on last con v  $\\mathbf{=}$  False ,  conv_bias  $=$  'auto' ,  conv_cfg  $\\mathbf{\\hat{\\mu}}$  None , norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} , act_cfg  $=$  {'type': 'Swish'} ,  loss_cls={'loss weight': 1.0, 'reduction': 'sum', 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  loss_bbox={'eps': 1e-16, 'loss_weight': 5.0, 'mode': 'square', 'reduction': 'sum', 'type': 'IoULoss'} , loss_obj={'loss weight': 1.0, 'reduction': 'sum', 'type': 'Cross Entropy Loss', 'use s igm oid': True} , loss_l1={'loss weight': 1.0, 'reduction': 'sum', 'type': 'L1Loss'} ,  train_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  test_cfg  $=$  None ,  init_cfg={'a': 2.23606797749979, 'distribution': 'uniform', 'layer': 'Conv2d', 'mode': 'fan_in', 'non linearity': 'leaky_relu', 'type': 'Kaiming'} ) \nParameters \n•  num classes  ( int ) – Number of categories excluding the background category. •  in channels  ( int ) – Number of channels in the input feature map. •  feat channels  ( int ) – Number of hidden channels in stacking convs. Default: 256 •  stacked con vs  ( int ) – Number of stacking convs of the head. Default: 2. •  strides  ( tuple ) – Downsample factor of each feature map. •  use depth wise  ( bool ) – Whether to depthwise separable convolution in blocks. Default: False •  dc n on last con v  ( bool ) – If true, use dcn in the last layer of towers. Default: False. •  conv_bias  ( bool | str ) – If specified as  auto , it will be decided by the norm_cfg. Bias of conv will be set as True if  norm_cfg  is None, otherwise False. Default: “auto”. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  act_cfg  ( dict ) – Config dict for activation layer. Default: None. •  loss_cls  ( dict ) – Config of classification loss. •  loss_bbox  ( dict ) – Config of localization loss. "}
{"page": 389, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_389.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* loss_obj (dict) — Config of objectness loss.\n\nloss_11 (dict) — Config of L1 loss.\n\n* train_cfg (dict) — Training config of anchor head.\n* test_cfg (dict) — Testing config of anchor head.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nForward features from the upstream network.\n\nParameters feats (tuple[Tensor]) — Features from the upstream network, each is a 4D-\ntensor.\n\nReturns\n\nA tuple of multi-level predication map, each isa 4D-tensor of shape  (batch_size,\n5+num_classes, height, width).\n\nReturn type tuple[Tensor]\n\nforward_single (x, cls_convs, reg_convs, conv_cls, conv_reg, conv_obj)\nForward feature of a single scale level.\n\nget_bboxes (cls_scores, bbox_preds, objectnesses, img_metas=None, cfg=None, rescale=False,\nwith_nms=True)\nTransform network outputs of a batch into bbox results. :param cls_scores: Classification scores for all\n\nscale levels, each is a 4D-tensor, has shape (batch_size, num_priors * num_classes, H, W).\n\nParameters\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for all scale levels, each is a 4D-\ntensor, has shape (batch_size, num_priors * 4, H, W).\n\n¢ objectnesses (list [Tensor], Optional) — Score factor for all scale level, each is a\n4D-tensor, has shape (batch_size, 1, H, W).\n\n¢ img_metas (list[dict], Optional) — Image meta info. Default None.\n\n* cfg (mmcv.Config, Optional) — Test / postprocessing configuration, if None, test_cfg\nwould be used. Default None.\n\n* rescale (bool) — If True, return boxes in original image space. Default False.\n¢ with_nms (boo1) — If True, do nms before return boxes. Default True.\nReturns\n\nEach item in result_list is 2-tuple. The first item is an (n, 5) tensor, where the first 4\ncolumns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\nbetween 0 and 1. The second item is a (n,) tensor where each item is the predicted class\nlabel of the corresponding box.\n\nReturn type list[list{Tensor, Tensor]]\ninit_weights()\nInitialize the weights.\n\nloss (cls_scores, bbox_preds, objectnesses, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None)\nCompute loss of the head. :param cls_scores: Box scores for each scale level,\n\neach is a 4D-tensor, the channel number is num_priors * num_classes.\n\n382 Chapter 39. mmdet.models\n", "vlm_text": "•  loss_obj  ( dict ) – Config of objectness loss. •  loss_l1  ( dict ) – Config of L1 loss. •  train_cfg  ( dict ) – Training config of anchor head. •  test_cfg  ( dict ) – Testing config of anchor head. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) Forward features from the upstream network. \nParameters  feats  ( tuple[Tensor] ) – Features from the upstream network, each is a 4D- tensor. \nReturns \nA tuple of multi-level predication map, each is a  4D-tensor of shape (batch_size,  $^{5+}$  num classes, height, width). Return type  tuple[Tensor] \nforward single ( x ,  cls_convs ,  reg_convs ,  conv_cls ,  conv_reg ,  conv_obj ) Forward feature of a single scale level. \nget_bboxes ( cls_scores ,  bbox_preds ,  object ness es ,  img_metas  $=$  None ,  cfg  $=$  None ,  rescale=False , with_nms  $\\backsimeq$  True ) Transform network outputs of a batch into bbox results. :param cls_scores: Classification scores for all scale levels, each is a 4D-tensor, has shape (batch_size, num_priors \\* num classes, H, W). \nParameters \n•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for all scale levels, each is a 4D- tensor, has shape (batch_size, num_priors \\* 4, H, W). •  object ness es  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, 1, H, W). •  img_metas  ( list[dict], Optional ) – Image meta info. Default None. •  cfg  ( mmcv.Config, Optional ) – Test / post processing configuration, if None, test_cfg would be used. Default None. •  rescale  ( bool ) – If True, return boxes in original image space. Default False. •  with_nms  ( bool ) – If True, do nms before return boxes. Default True. \nReturns \nEach item in result list is 2-tuple.  The first item is an (n, 5) tensor, where the first 4 columns are bounding box positions (tl_x, tl_y, br_x, br_y) and the 5-th column is a score between 0 and 1. The second item is a (n,) tensor where each item is the predicted class label of the corresponding box. \nReturn type  list[list[Tensor, Tensor]] \nin it weights()\nInitialize the weights. \nloss ( cls_scores ,  bbox_preds ,  object ness es ,  gt_bboxes ,  gt_labels ,  img_metas ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None ) Compute loss of the head. :param cls_scores: Box scores for each scale level, \neach is a 4D-tensor, the channel number is num_priors \\* num classes. "}
{"page": 390, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_390.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\n¢ bbox_preds (list [Tensor]) — Box energies / deltas for each scale level, each is a 4D-\ntensor, the channel number is num_priors * 4.\n\n¢ objectnesses (list [Tensor], Optional) — Score factor for all scale level, each is a\n4D-tensor, has shape (batch_size, 1, H, W).\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n¢ img_metas (list [dict]) — Meta information of each image, e.g., image size, scaling\nfactor, etc.\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n39.5 roi_heads\n\nclass mmdet.models.roi_heads.BBoxHead(with_avg_pool=False, with_cls=True, with_reg=True,\nroi_feat_size=7, in_channels=256, num_classes=80,\nbbox_coder={'clip_border': True, 'target_means': [0.0, 0.0, 0.0,\n0.0], 'target_stds': [0.1, 0.1, 0.2, 0.2], 'type':\n‘DeltaX YWHBBoxCoder'}, reg_class_agnostic=False,\nreg_decoded_bbox=False, reg_predictor_cfg={'type': 'Linear'},\ncls_predictor_cfg={'type': 'Linear'}, loss_cls={‘loss_weight': 1.0,\n‘type’: 'CrossEntropyLoss’, ‘use_sigmoid': False},\nloss_bbox={'beta': 1.0, 'loss_weight': 1.0, 'type': 'SmoothL1Loss'},\ninit_cfg=None)\n\nSimplest Rol head, with only two fc layers for classification and regression respectively.\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nget_bboxes (rois, cls_score, bbox_pred, img_shape, scale_factor, rescale=False, cfg=None)\nTransform network output for a batch into bbox predictions.\n\nParameters\n\n* rois (Tensor) — Boxes to be transformed. Has shape (num_boxes, 5). last dimension 5\narrange as (batch_index, x1, yl, x2, y2).\n\n* cls_score (Tensor) — Box scores, has shape (num_boxes, num_classes + 1).\n\n¢ bbox_pred (Tensor, optional) - Box energies / deltas. has shape (num_boxes,\nnum_classes * 4).\n\n¢ img_shape (Sequence[int], optional) — Maximum bounds for boxes, specifies (H,\nW, C) or (H, W).\n\n39.5. roi_heads 383\n", "vlm_text": "Parameters \n•  bbox_preds  ( list[Tensor] ) – Box energies / deltas for each scale level, each is a 4D- tensor, the channel number is num_priors   $^{*}\\,4.$  . •  object ness es  ( list[Tensor], Optional ) – Score factor for all scale level, each is a 4D-tensor, has shape (batch_size, 1, H, W). •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  img_metas  ( list[dict] ) – Meta information of each image, e.g., image size, scaling factor, etc. •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. \n39.5 roi_heads \n( with avg poo  $\\leftrightharpoons$  False ,  with_cls=True ,  with_reg=True , roi feat size=7 ,  in channels=256 ,  num classes=80 , bbox_coder={'clip border': True, 'target means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [0.1, 0.1, 0.2, 0.2], 'type': 'Delta XY WH B Box Code r'} ,  reg class agnostic=False , reg decoded b box  $\\mathbf{\\beta}=$  False ,  reg predictor cf g={'type': 'Linear'} , cls predictor cf g={'type': 'Linear'} ,  loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': False} , loss_bbox={'beta': 1.0, 'loss weight': 1.0, 'type': 'Smooth L 1 Loss'} init_cfg  $\\scriptstyle\\in$  None ) \nSimplest RoI head, with only two fc layers for classification and regression respectively. \nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nget_bboxes ( rois ,  cls_score ,  bbox_pred ,  img_shape ,  scale factor ,  rescale  $\\mathbf{=}$  False ,  cfg=None ) Transform network output for a batch into bbox predictions. \nParameters \n•  rois  ( Tensor ) – Boxes to be transformed. Has shape (num_boxes, 5). last dimension 5 arrange as (batch index, x1, y1, x2, y2). •  cls_score  ( Tensor ) – Box scores, has shape (num_boxes, num classes  $+\\;1$  ). •  bbox_pred  ( Tensor, optional ) – Box energies / deltas. has shape (num_boxes, num classes \\* 4). •  img_shape  ( Sequence[int], optional ) – Maximum bounds for boxes, specifies (H, W, C) or (H, W). "}
{"page": 391, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_391.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* scale_factor (ndarray) — Scale factor of the image arrange as (w_scale, h_scale,\nw_scale, h_scale).\n\n¢ rescale (bool) — If True, return boxes in original image space. Default: False.\n* (obj (cfg) — ConfigDict): test_cfg of Bbox Head. Default: None\n\nReturns First tensor is det_bboxes, has the shape (num_boxes, 5) and last dimension 5 represent\n(tLx, tl_y, br_x, br_y, score). Second tensor is the labels with shape (num_boxes, ).\n\nReturn type tuple[Tensor, Tensor]\n\nget_targets(sampling_results, gt_bboxes, gt_labels, renn_train_cfg, concat=True)\nCalculate the ground truth for all samples in a batch according to the sampling_results.\n\nAlmost the same as the implementation in bbox_head, we passed additional parameters pos_inds_list and\nneg_inds_list to _get_target_single function.\n\nParameters\n\n¢ (List [obj (sampling_results) — SamplingResults]): Assign results of all images in a\nbatch after sampling.\n\n* gt_bboxes (list [Tensor]) — Gt_bboxes of all images in a batch, each tensor has shape\n(num_gt, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y].\n\n¢ gt_labels (list [Tensor ]) — Gt_labels of all images in a batch, each tensor has shape\n(num_gt,).\n\n* (obj (rcnn_train_cfg) — ConfigDict): train_cfg of RCNN.\n\n* concat (bool) — Whether to concatenate the results of all the images in a single batch.\nReturns\n\nGround truth for proposals in a single image. Containing the following list of Tensors:\n\n¢ labels (list{Tensor],Tensor): Gt_labels for all proposals in a batch, each tensor in list\nhas shape (num_proposals,) when concat=False, otherwise just a single tensor has shape\n(num_all_proposals,).\n\n¢ label_weights (list[Tensor]): Labels_weights for all proposals in a batch, each tensor in list\nhas shape (num_proposals,) when concat=False, otherwise just a single tensor has shape\n(num_all_proposals,).\n\n* bbox_targets (list[Tensor],Tensor): Regression target for all proposals in a batch, each ten-\nsor in list has shape (num_proposals, 4) when concat=False, otherwise just a single tensor\nhas shape (num_all_proposals, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y].\n\n* bbox_weights (list[tensor],Tensor): Regression weights for all proposals in a batch, each\ntensor in list has shape (num_proposals, 4) when concat=False, otherwise just a single\ntensor has shape (num_all_proposals, 4).\n\nReturn type Tuple[Tensor]\n\nonnx_export (rois, cls_score, bbox_pred, img_shape, cfg=None, **kwargs)\nTransform network output for a batch into bbox predictions.\n\nParameters\n* rois (Tensor) — Boxes to be transformed. Has shape (B, num_boxes, 5)\n\n* cls_score (Tensor) — Box scores. has shape (B, num_boxes, num_classes + 1), 1 repre-\nsent the background.\n\n384 Chapter 39. mmdet.models\n", "vlm_text": "•  scale factor  ( ndarray ) – Scale factor of the image arrange as (w_scale, h_scale, w_scale, h_scale). •  rescale  ( bool ) – If True, return boxes in original image space. Default: False. •  (obj  ( cfg ) –  ConfigDict ):  test_cfg  of Bbox Head. Default: None Returns  First tensor is  det_bboxes , has the shape (num_boxes, 5) and last dimension 5 represent (tl_x, tl_y, br_x, br_y, score). Second tensor is the labels with shape (num_boxes, ). \n\nget targets ( sampling results ,  gt_bboxes ,  gt_labels ,  r cnn train cf g ,  conca  $t{=}$  True ) Calculate the ground truth for all samples in a batch according to the sampling results. \nAlmost the same as the implementation in bbox_head, we passed additional parameters pos in ds list and ne g in ds list to  get target single  function. \nParameters \n•  (List[obj  ( sampling results ) – Sampling Results]): Assign results of all images in a batch after sampling. •  gt_bboxes  ( list[Tensor] ) – Gt_bboxes of all images in a batch, each tensor has shape (num_gt, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  gt_labels  ( list[Tensor] ) – Gt_labels of all images in a batch, each tensor has shape (num_gt,). •  (obj  ( r cnn train cf g ) – ConfigDict):  train_cfg  of RCNN. •  concat  ( bool ) – Whether to concatenate the results of all the images in a single batch. \nReturns \nGround truth for proposals in a single image. Containing the following list of Tensors: \n• labels (list[Tensor],Tensor): Gt_labels for all proposals in a batch, each tensor in list has shape (num proposals,) when  conca  $\\leftleftarrows$  False , otherwise just a single tensor has shape (num all proposals,). • label weights (list[Tensor]): Labels weights for all proposals in a batch, each tensor in list has shape (num proposals,) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals,). • b box targets (list[Tensor],Tensor): Regression target for all proposals in a batch, each ten- sor in list has shape (num proposals, 4) when  conca  $\\mathbf{\\dot{\\rho}}=$  False , otherwise just a single tensor has shape (num all proposals, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. • b box weights (list[tensor],Tensor): Regression weights for all proposals in a batch, each tensor in list has shape (num proposals, 4) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals, 4). \nReturn type  Tuple[Tensor] \non nx export ( rois ,  cls_score ,  bbox_pred ,  img_shape ,  cfg  $\\mathbf{\\hat{\\mu}}$  None ,  \\*\\*kwargs ) Transform network output for a batch into bbox predictions. \nParameters \n•  rois  ( Tensor ) – Boxes to be transformed. Has shape (B, num_boxes, 5) •  cls_score  ( Tensor ) – Box scores. has shape (B, num_boxes, num classes  $+\\;1$  ), 1 repre- sent the background. "}
{"page": 392, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_392.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ bbox_pred (Tensor, optional) — Box energies / deltas for, has shape (B, num_boxes,\nnum_classes * 4) when.\n\n¢ img_shape (torch. Tensor) — Shape of image.\n\n* (obj (cfg) — ConfigDict): test_cfg of Bbox Head. Default: None\nReturns\n\ndets of shape [N, num_det, 5] and class labels of shape [N, num_det].\nReturn type tuple[Tensor, Tensor]\n\nrefine_bboxes (rois, labels, bbox_preds, pos_is_gts, img_metas)\nRefine bboxes during training.\n\nParameters\n\n* rois (Tensor) — Shape (n*bs, 5), where n is image number per GPU, and bs is the sampled\nRols per image. The first column is the image id and the next 4 columns are x1, y1, x2, y2.\n\n* labels (Tensor) — Shape (n*bs, ).\n¢ bbox_preds (Tensor) — Shape (n*bs, 4) or (n*bs, 4*#class).\n* pos_is_gts (list [Tensor]) — Flags indicating if each positive bbox is a gt bbox.\n¢ img_metas (list [dict ]) — Meta info of each image.\nReturns Refined bboxes of each image in a mini-batch.\n\nReturn type list[Tensor]\n\nExample\n\n>>> # xdoctest: +REQUIRES (module: kwarray)\n\n>>> import kwarray\n\n>>> import numpy as np\n\n>>> from mmdet.core.bbox.demodata import random_boxes\n\n>>> self = BBoxHead(reg_class_agnostic=True)\n\n>>> n_roi = 2\n\n>>> n_img = 4\n\n>>> scale = 512\n\n>>> rng = np.random.RandomState(0)\n\n>>> img_metas = [{'img_shape': (scale, scale)}\n\nwae for _ in range(n_img) ]\n\n>>> # Create rois in the expected format\n\n>>> roi_boxes = random_boxes(n_roi, scale=scale, rng=rng)\n>>> img_ids = torch.randint(9, n_img, (n_roi,))\n\n>>> img_ids = img_ids.float()\n\n>>> rois = torch.cat([img_ids[:, None], roi_boxes], dim=1)\n>>> # Create other args\n\n>>> labels = torch.randint(0, 2, (n_roi,)).longQ\n\n>>> bbox_preds = random_boxes(n_roi, scale=scale, rng=rng)\n>>> # For each image, pretend random positive boxes are gts\n>>> is_label_pos = (labels.numpy() > 0).astype(np.int)\n>>> lbl_per_img = kwarray.group_items(is_label_pos,\n\na img_ids.numpy())\n\n>>> pos_per_img = [sum(lbl_per_img.get(gid, []))\n\n(continues on next page)\n\n39.5. roi_heads 385\n", "vlm_text": "•  bbox_pred  ( Tensor, optional ) – Box energies / deltas for, has shape (B, num_boxes, num classes   $^{*}\\,4$  ) when. •  img_shape  ( torch.Tensor ) – Shape of image. •  (obj  ( cfg ) –  ConfigDict ):  test_cfg  of Bbox Head. Default: None \nReturns \ndets of shape [N, num_det, 5]  and class labels of shape [N, num_det]. \nReturn type  tuple[Tensor, Tensor] \nrefine b boxes ( rois ,  labels ,  bbox_preds ,  pos_is_gts ,  img_metas ) Refine bboxes during training. \nParameters \n•  rois  ( Tensor ) – Shape   $(\\mathtt{n}^{*}\\mathtt{b s},5)$  , where n is image number per GPU, and bs is the sampled RoIs per image. The first column is the image id and the next 4 columns are x1, y1, x2, y2. •  labels  ( Tensor ) – Shape   $(\\mathsf{n}^{\\ast}\\mathsf{b s}.$  , ). •  bbox_preds  ( Tensor ) – Shape   $(\\mathbf{n}^{*}\\mathbf{b}\\mathbf{s},4)$  ) or (n\\*bs, 4\\*#class). •  pos_is_gts  ( list[Tensor] ) – Flags indicating if each positive bbox is a gt bbox. •  img_metas  ( list[dict] ) – Meta info of each image. \nReturns  Refined bboxes of each image in a mini-batch. \nReturn type  list[Tensor] \nExample \nThe image shows a snippet of Python code related to object detection. Here's a brief overview of the code:\n\n1. **Imports**: \n   - `kwarray`\n   - `numpy` as `np`\n   - `random_boxes` from `mmdet.core.bbox.demodata`\n\n2. **Configuration**:\n   - `BBoxHead` is initialized with `reg_class_agnostic=True`.\n   - Number of regions of interest (`n_roi`): 2\n   - Number of images (`n_img`): 4\n   - `scale`: 512\n   - Random number generator is set with a seed of 0.\n\n3. **Image metadata**:\n   - Contains 'img_shape' set to `(scale, scale)`.\n\n4. **ROI Creation**:\n   - `roi_boxes` are generated using `random_boxes`.\n   - `img_ids` are random integers between 0 and `n_img`.\n\n5. **Concatenation**:\n   - `rois` is created by concatenating `img_ids` with `roi_boxes`.\n\n6. **Labels and Predictions**:\n   - Random labels are generated.\n   - `bbox_preds` are also generated using `random_boxes`.\n\n7. **Positive Labels**:\n   - Determines positive boxes and groups items accordingly.\n   - Summarizes the number of positive labels per image in `pos_per_img`.\n\nThe code seems to be part of a test or initialization process for object detection, likely preparing data for training or evaluation purposes."}
{"page": 393, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_393.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n(continued from previous page)\n\nwee for gid in range(n_img)]\n\n>>> pos_is_gts [\n\n>>> torch.randint(0, 2, (npos,)).byte().sort(\n\n>>> descending=True) [0]\n\n>>> for npos in pos_per_img\n\n>>> ]\n\n>>> bboxes_list = self.refine_bboxes(rois, labels, bbox_preds,\n>>> pos_is_gts, img_metas)\n\n>>> print (bboxes_list)\n\nregress_by_class(vois, label, bbox_pred, img_meta)\nRegress the bbox for the predicted class. Used in Cascade R-CNN.\n\nParameters\n\n* rois (Tensor) — Rois from rpn_head or last stage bbox_head, has shape (num_proposals,\n4) or (num_proposals, 5).\n\n¢ label (Tensor) — Only used when self:reg_class_agnostic is False, has shape\n(num_proposals, ).\n\n¢ bbox_pred (Tensor) - Regression prediction of current stage bbox_head. When\n\nself.reg_class_agnostic is False, it has shape (n, num_classes * 4), otherwise it has shape\n(n, 4).\n\n¢ img_meta (dict) — Image meta info.\nReturns Regressed bboxes, the same shape as input rois.\nReturn type Tensor\n\nclass mmdet.models.roi_heads.BaseRoIExtractor (roi_layer, out_channels, featmap_strides,\ninit_cfg=None)\nBase class for Rol extractor.\n\nParameters\n* roi_layer (dict) — Specify Rol layer type and arguments.\n* out_channels (int) — Output channels of Rol layers.\n* featmap_strides (int) — Strides of input feature maps.\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\nbuild_roi_layers (/ayer_cfg, featmap_strides)\nBuild Rol operator to extract feature from each level feature map.\n\nParameters\n\n¢ layer_cfg (dict) — Dictionary to construct and config Rol layer operation. Options are\nmodules under mmcv/ops such as RoIAlign.\n\n¢ featmap_strides (List [int]) — The stride of input feature map w.r.t to the original\nimage size, which would be used to scale Rol coordinate (original image coordinate system)\nto feature coordinate system.\n\nReturns\nThe Rol extractor modules for each level feature map.\n\nReturn type nn.ModuleList\n\n386 Chapter 39. mmdet.models\n\n", "vlm_text": "This image shows a snippet of Python code, likely part of a machine learning or computer vision program. Here's a breakdown:\n\n- It's using the PyTorch library, as indicated by `torch.randint`.\n- The loop iterates over a range given by `n_img`.\n- `pos_is_gts` is a list comprehension where random integers (0 or 1) are generated, sorted in descending order, and selected for the number of positive samples per image (`npos`).\n- `bboxes_list` is assigned by calling a method `self.refine_bboxes` with arguments `rois`, `labels`, `bbox_preds`, `pos_is_gts`, and `img_metas`.\n- Finally, it prints `bboxes_list`.\n\nThe code appears to be part of a function or script related to refining bounding boxes in an object detection task.\nregress by class ( rois ,  label ,  bbox_pred ,  img_meta ) Regress the bbox for the predicted class. Used in Cascade R-CNN. \nParameters \n•  rois  ( Tensor ) – Rois from  rpn_head  or last stage  bbox_head , has shape (num proposals, 4) or (num proposals, 5). •  label  ( Tensor ) – Only used when  self.reg class agnostic  is False, has shape (num proposals, ). •  bbox_pred  ( Tensor ) – Regression prediction of current stage  bbox_head . When self.reg class agnostic  is False, it has shape (n, num classes  $^{*}\\,4$  ), otherwise it has shape (n, 4). •  img_meta  ( dict ) – Image meta info. Returns  Regressed bboxes, the same shape as input rois. \nclass  mmdet.models.roi_heads. Base RoI Extractor ( roi_layer ,  out channels ,  feat map strides init_cfg=None ) \nParameters \n•  roi_layer  ( dict ) – Specify RoI layer type and arguments. •  out channels  ( int ) – Output channels of RoI layers. •  feat map strides  ( int ) – Strides of input feature maps. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nbuild roi layers ( layer_cfg ,  feat map strides ) Build RoI operator to extract feature from each level feature map. \n•  layer_cfg  ( dict ) – Dictionary to construct and config RoI layer operation. Options are modules under  mmcv/ops  such as  RoIAlign . •  feat map strides  ( List[int] ) – The stride of input feature map w.r.t to the original image size, which would be used to scale RoI coordinate (original image coordinate system) to feature coordinate system. \nReturns \nThe RoI extractor modules for each level feature  map. Return type  nn.ModuleList "}
{"page": 394, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_394.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nabstract forward (feats, rois, roi_scale_factor=None)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nproperty num_inputs\nNumber of input feature maps.\n\nType int\n\nroi_rescale(rois, scale_factor)\nScale Rol coordinates by scale factor.\n\nParameters\n\n* rois (torch. Tensor) — Rol (Region of Interest), shape (n, 5)\n\n* scale_factor (float) — Scale factor that Rol will be multiplied by.\nReturns Scaled Rol.\nReturn type torch.Tensor\n\nclass mmdet.models.roi_heads.BaseRoIHead (bbox_roi_extractor=None, bbox_head=None,\nmask_roi_extractor=None, mask_head=None,\nshared_head=None, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nBase class for RolHeads.\n\nasync async_simple_test (x, proposal_list, img_metas, proposals=None, rescale=False, **kwargs)\nAsynchronized test function.\n\naug_test (x, proposal_list, img_metas, rescale=False, **kwargs)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nabstract forward_train(x, img_meta, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore=None,\ngt_masks=None, **kwargs)\nForward function during training.\n\nabstract init_assigner_sampler()\nInitialize assigner and sampler.\n\nabstract init_bbox_head()\nInitialize bbox_head\n\nabstract init_mask_head()\nInitialize mask_head\n\nsimple_test (x, proposal_list, img_meta, proposals=None, rescale=False, **kwargs)\nTest without augmentation.\n\nproperty with_bbox\nwhether the Rol head contains a bbox_head\n\nType bool\n\n39.5. roi_heads 387\n", "vlm_text": "abstract forward ( feats ,  rois ,  roi scale factor  $\\leftrightharpoons$  None ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nproperty num_inputs Number of input feature maps. Type  int \nroi re scale ( rois ,  scale factor ) Scale RoI coordinates by scale factor. \nParameters •  rois  ( torch.Tensor ) – RoI (Region of Interest), shape (n, 5) •  scale factor  ( float ) – Scale factor that RoI will be multiplied by. Returns  Scaled RoI. Return type  torch.Tensor \nclass  mmdet.models.roi_heads. Base RoI Head ( b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head  $\\leftrightharpoons$  None , mask roi extractor  $=$  None ,  mask_head=None , shared head  $\\leftrightharpoons$  None ,  train_cfg  $=$  None ,  test_cfg  $\\leftrightharpoons$  None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) \nBase class for RoIHeads. \nasync a sync simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\leftrightharpoons$  None ,  rescale  $=$  False ,  \\*\\*kwargs ) A synchronized test function. \naug_test ( x ,  proposal list ,  img_metas ,  rescale  $=$  False ,  \\*\\*kwargs ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nabstract forward train ( x ,  img_meta ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\dot{\\rho}}$  None , gt_masks  $\\mathbf{\\beta}=$  None ,  \\*\\*kwargs ) Forward function during training. \nabstract in it as signer sampler () Initialize assigner and sampler. \nabstract in it b box head () Initialize  bbox_head \nabstract in it mask head () Initialize  mask_head \nsimple test (  $\\dot{\\boldsymbol{x}}.$  ,  proposal list ,  img_meta ,  proposal  $\\mathfrak{s}{=}$  None ,  rescale=False ,  \\*\\*kwargs ) Test without augmentation. \nproperty with_bbox whether the RoI head contains a  bbox_head \nType  bool "}
{"page": 395, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_395.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nproperty with_mask\nwhether the Rol head contains a mask_head\n\nType bool\n\nproperty with_shared_head\nwhether the Rol head contains a shared_head\n\nType bool\nclass mmdet.models.roi_heads.CascadeRoIHead (num_stages, stage_loss_weights,\n\nbbox_roi_extractor=None, bbox_head=None,\nmask_roi_extractor=None, mask_head=None,\nshared_head=None, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\n\nCascade roi head including one bbox head and one mask head.\n\nhttps://arxiv.org/abs/1712.00726\n\naug_test (features, proposal_list, img_metas, rescale=False)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nforward_dummy (x, proposals)\nDummy forward function.\n\nforward_train(x, img_metas, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore=None,\n\ngt_masks=None)\n\nParameters\n\nx (list [Tensor]) — list of multi-level img features.\n\nimg_metas (list[dict]) — list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\nproposals (list [Tensors]) — list of region proposals.\n\ngt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\ngt_labels (list [Tensor ]) — class indices corresponding to each box\n\ngt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\ngt_masks (None | Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\nReturns a dictionary of loss components\n\nReturn type dict[str, Tensor]\n\ninit_assigner_sampler()\nInitialize assigner and sampler for each stage.\n\ninit_bbox_head (bbox_roi_extractor, bbox_head)\nInitialize box head and box roi extractor.\n\nParameters\n\n388\n\nChapter 39. mmdet.models\n", "vlm_text": "property with_mask whether the RoI head contains a  mask_head \nType  bool \nproperty with shared head whether the RoI head contains a  shared head \nType  bool \nclass  mmdet.models.roi_heads. Cascade RoI Head ( num_stages ,  stage loss weights , b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head=None , mask roi extractor  $\\leftrightharpoons$  None ,  mask_head  $\\mathbf{\\{}=}$  None , shared head  $\\leftrightharpoons$  None ,  train_cfg  $=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nCascade roi head including one bbox head and one mask head. \nhttps://arxiv.org/abs/1712.00726 \naug_test ( features ,  proposal list ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test with augmentations. \nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nforward dummy ( x ,  proposals ) Dummy forward function. \nforward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $=$  None , gt_mask  $\\mathfrak{s}{=}$  None ) \nParameters \n•  x  ( list[Tensor] ) – list of multi-level img features. \n•  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . \n•  proposals  ( list[Tensors] ) – list of region proposals. \n•  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. \n•  gt_labels  ( list[Tensor] ) – class indices corresponding to each box \n gt b boxes ignore  ( None   $I$   list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. \n gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. \nReturns  a dictionary of loss components \nReturn type  dict[str, Tensor] \nin it as signer sampler () Initialize assigner and sampler for each stage. \nin it b box head ( b box roi extractor ,  bbox_head ) Initialize box head and box roi extractor. \nParameters "}
{"page": 396, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_396.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ bbox_roi_extractor (dict) — Config of box roi extractor.\n¢ bbox_head (dict) — Config of box in box head.\n\ninit_mask_head (mask_roi_extractor, mask_head)\nInitialize mask head and mask roi extractor.\n\nParameters\n¢ mask_roi_extractor (dict) — Config of mask roi extractor.\n¢ mask_head (dict) — Config of mask in mask head.\n\nsimple_test (x, proposal_list, img_metas, rescale=False)\nTest without augmentation.\n\nParameters\n\n¢ x(tuple[Tensor]) — Features from upstream network. Each has shape (batch_size, c, h,\nw).\n\n* proposal_list (list(Tensor)) — Proposals from rpn head. Each has shape\n(num_proposals, 5), last dimension 5 represent (x1, yl, x2, y2, score).\n\n¢ img_metas (list [dict ]) — Meta information of images.\n* rescale (bool) — Whether to rescale the results to the original image. Default: True.\n\nReturns When no mask branch, it is bbox results of each image and classes with type\nlist[list{np.ndarray]]. The outer list corresponds to each image. The inner list corresponds\nto each class. When the model has mask branch, it contains bbox results and mask results.\nThe outer list corresponds to each image, and first element of tuple is bbox results, second\nelement is mask results.\n\nReturn type list[list{np.ndarray]] or list[tuple]\n\nclass mmdet.models.roi_heads.CoarseMaskHead (num_convs=0, num_fcs=2, fc_out_channels=1024,\ndownsample_factor=2, init_cfg={‘override': [{'name':\n'fcs'}, {‘type': ‘Constant’, 'val': 0.001, ‘name’: ‘fc_logits'} ],\n‘type’: 'Xavier'}, *arg, **kwarg)\nCoarse mask head used in PointRend.\n\nCompared with standard FCNMaskHead, CoarseMaskHead will downsample the input feature map instead of\nupsample it.\n\nParameters\n* num_convs (int) — Number of conv layers in the head. Default: 0.\n* num_fcs (int) — Number of fc layers in the head. Default: 2.\n¢ fc_out_channels (int) — Number of output channels of fc layer. Default: 1024.\n¢ downsample_factor (int) — The factor that feature map is downsampled by. Default: 2.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\n\n39.5. roi_heads 389\n", "vlm_text": "•  b box roi extractor  ( dict ) – Config of box roi extractor. •  bbox_head  ( dict ) – Config of box in box head. \nin it mask head ( mask roi extractor ,  mask_head ) Initialize mask head and mask roi extractor. \nParameters \n•  mask roi extractor  ( dict ) – Config of mask roi extractor. •  mask_head  ( dict ) – Config of mask in mask head. \nsimple test (  $\\cdot x,$  ,  proposal list ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test without augmentation. \nParameters \n•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). •  proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). •  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. \nReturns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. \nclass  mmdet.models.roi_heads. Coarse Mask Head ( num_convs  $\\mathrel{\\mathop:}=\\!\\!O$  ,  num_fc  $\\backsimeq\\!2$  ,  fc out channels  $\\mathbf{\\tilde{=}}$  1024 , down sample facto  $_{r=2}$  ,  init_cfg  $=$  {'override': [{'name': 'fcs'}, {'type': 'Constant', 'val': 0.001, 'name': 'fc_logits'}], 'type': 'Xavier'} ,  \\*arg ,  \\*\\*kwarg ) \nCoarse mask head used in PointRend. \nCompared with standard  FC N Mask Head ,  Coarse Mask Head  will downsample the input feature map instead of upsample it. \nParameters \n•  num_convs  ( int ) – Number of conv layers in the head. Default: 0. •  num_fcs  ( int ) – Number of fc layers in the head. Default: 2. •  fc out channels  ( int ) – Number of output channels of fc layer. Default: 1024. •  down sample factor  ( int ) – The factor that feature map is down sampled by. Default: 2. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while "}
{"page": 397, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_397.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nthe latter silently ignores them.\n\ninit_weights()\nInitialize the weights.\n\nclass mmdet.models.roi_heads .ConvFCBBoxHead (num_shared_convs=0, num_shared_fcs=0,\n\nnum_cls_convs=0, num_cls_fcs=0, num_reg_convs=0,\nnum_reg_fcs=0, conv_out_channels=256,\nfc_out_channels=1024, conv_cfg=None, norm_cfg=None,\ninit_cfg=None, *args, **kwargs)\n\nMore general bbox head, with shared conv and fc layers and two optional separated branches.\n\n/-> cls convs -> cls fcs -> cls\n\nshared convs -> shared fcs\n\n\\-> reg convs -> reg fcs -> reg\n\nforward(x)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.roi_heads .DIIHead(num_classes=80, num_ffn_fcs=2, num_heads=8, num_cls_fcs=1,\n\nnum_reg_fcs=3, feedforward_channels=2048, in_channels=256,\ndropout=0.0, ffn_act_cfg={'inplace': True, 'type': 'ReLU'},\ndynamic_conv_cfg={'act_cfg': {‘inplace': True, 'type': 'ReLU'},\n‘feat_channels': 64, 'in_channels': 256, 'input_feat_shape': 7,\n‘norm_cfg': {'type': 'LN'}, 'out_channels': 256, 'type':\n‘DynamicConv'}, loss_iou={'loss_weight': 2.0, ‘type’: 'GloULoss'},\ninit_cfg=None, **kwargs)\n\nDynamic Instance Interactive Head for Sparse R-CNN: End-to-End Object Detection with Learnable Proposals\n\nParameters\n\nnum_classes (int) — Number of class in dataset. Defaults to 80.\nnum_ffn_fcs (int) — The number of fully-connected layers in FFNs. Defaults to 2.\nnum_heads (int) — The hidden dimension of FFNs. Defaults to 8.\n\nnum_cls_fcs (int) — The number of fully-connected layers in classification subnet. De-\nfaults to 1.\n\nnum_reg_fcs (int) — The number of fully-connected layers in regression subnet. Defaults\nto 3.\n\nfeedforward_channels (int) — The hidden dimension of FFNs. Defaults to 2048\nin_channels (int) — Hidden_channels of MultiheadAttention. Defaults to 256.\ndropout (float) — Probability of drop the channel. Defaults to 0.0\n\n£f£n_act_cfg (dict) — The activation config for FFNs.\n\ndynamic_conv_cfg (dict) — The convolution config for DynamicConv.\n\nloss_iou (dict) — The config for iou or giou loss.\n\n390\n\nChapter 39. mmdet.models\n\n", "vlm_text": "the latter silently ignores them. \nin it weights()Initialize the weights. \nclass  mmdet.models.roi_heads. Con v FCB Box Head ( num shared con vs  $\\mathrm{\\Sigma=}0$  ,  num shared fc  $\\scriptstyle{s=0}$  , num cls con vs  $\\mathord{:=}0$  ,  num_cls_fc  $s{=}0$  ,  num reg con vs  $\\imath{=}0$  , num_reg_fc  $\\scriptstyle{:s=0}$  ,  con v out channel  $s{=}256$  , fc out channels  $\\mathbf{\\hat{=}}$  1024 ,  conv_cfg  $=$  None ,  norm_cfg  $=$  None , init_cfg  $\\mathbf{\\beta}=$  None ,  \\*args ,  \\*\\*kwargs ) \nMore general bbox head, with shared conv and fc layers and two optional separated branches. \nThe table appears to represent a flow or processing pipeline for a neural network architecture, possibly a region-based object detection framework. Here's a breakdown of the flow illustrated in the table:\n\n1. **Shared Convs -> Shared FCs**: The data or features initially undergo shared convolutional layers (convs), followed by shared fully connected layers (FCs). This suggests a shared processing path for certain features.\n\n2. **Branching Paths**:\n   - **Cls Path**: One branch of the process involves further convolutional layers for classification (cls convs), followed by fully connected layers for classification (cls FCs), and finally producing a classification output (cls).\n   - **Reg Path**: Another branch involves convolutional layers for regression (reg convs), followed by fully connected layers for regression (reg FCs), and concluding with a regression output (reg).\n\nThis structure is indicative of multi-task learning, where both classification and regression tasks share some initial processing steps before diverging into specialized paths for their respective outputs.\nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.roi_heads. DIIHead ( num_classe  $\\wp{=}8O$  ,  num ff n fcs  $\\scriptstyle{\\prime=2}$  ,  num_heads  $\\scriptstyle{:=8}$  ,  num_cls_fc  $s{=}I$  , num_reg_fc  $\\cdot_{s=3}$  ,  feed forward channels  $:=$  2048 ,  in channels=256 , dropou  $\\mathopen{}\\mathclose\\bgroup\\left.\\aftergroup\\egroup\\right.$  ,  ff n act cf g  $=$  {'inplace': True, 'type': 'ReLU'} , dynamic con v cf g  $=$  {'act_cfg': {'inplace': True, 'type': 'ReLU'}, 'feat channels': 64, 'in channels': 256, 'input feat shape': 7, 'norm_cfg': {'type': 'LN'}, 'out channels': 256, 'type': 'Dynamic Con v'} ,  loss_iou={'loss weight': 2.0, 'type': 'GIoULoss'} , init_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  \\*\\*kwargs ) \nDynamic Instance Interactive Head for  Sparse R-CNN: End-to-End Object Detection with Learnable Proposals \nParameters \n•  num classes  ( int ) – Number of class in dataset. Defaults to 80. •  num ff n fcs  ( int ) – The number of fully-connected layers in FFNs. Defaults to 2. •  num_heads  ( int ) – The hidden dimension of FFNs. Defaults to 8. •  num cls fcs  ( int ) – The number of fully-connected layers in classification subnet. De- faults to 1. •  num reg fcs  ( int ) – The number of fully-connected layers in regression subnet. Defaults to 3. •  feed forward channels  ( int ) – The hidden dimension of FFNs. Defaults to 2048 •  in channels  ( int ) – Hidden channels of Multi head Attention. Defaults to 256. •  dropout  ( float ) – Probability of drop the channel. Defaults to 0.0 •  ff n act cf g  ( dict ) – The activation config for FFNs. •  dynamic con v cf g  ( dict ) – The convolution config for Dynamic Con v. •  loss_iou  ( dict ) – The config for iou or giou loss. "}
{"page": 398, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_398.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward (roi_feat, proposal_feat)\nForward function of Dynamic Instance Interactive Head.\n\nParameters\n\n* roi_feat (Tensor) — Roi-pooling features with shape (batch_size*num_proposals, fea-\nture_dimensions, pooling_h , pooling _w).\n\n* proposal_feat — Intermediate feature get from diihead in last stage, has shape\n(batch_size, num_proposals, feature_dimensions)\n\nget_targets(sampling_results, gt_bboxes, gt_labels, renn_train_cfg, concat=True)\nCalculate the ground truth for all samples in a batch according to the sampling_results.\n\nAlmost the same as the implementation in bbox_head, we passed additional parameters pos_inds_list and\nneg_inds_list to _get_target_single function.\n\nParameters\n\n¢ (List [obj (sampling_results) — SamplingResults]): Assign results of all images in a\nbatch after sampling.\n\n* gt_bboxes (list [Tensor]) — Gt_bboxes of all images in a batch, each tensor has shape\n(num_gt, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y].\n\n¢ gt_labels (list [Tensor ]) — Gt_labels of all images in a batch, each tensor has shape\n(num_gt,).\n\n* (obj (rcnn_train_cfg) — ConfigDict): train_cfg of RCNN.\n\n* concat (bool) — Whether to concatenate the results of all the images in a single batch.\nReturns\n\nGround truth for proposals in a single image. Containing the following list of Tensors:\n\n¢ labels (list{Tensor],Tensor): Gt_labels for all proposals in a batch, each tensor in list\nhas shape (num_proposals,) when concat=False, otherwise just a single tensor has shape\n(num_all_proposals,).\n\n¢ label_weights (list[Tensor]): Labels_weights for all proposals in a batch, each tensor in list\nhas shape (num_proposals,) when concat=False, otherwise just a single tensor has shape\n(num_all_proposals,).\n\n* bbox_targets (list[Tensor],Tensor): Regression target for all proposals in a batch, each ten-\nsor in list has shape (num_proposals, 4) when concat=False, otherwise just a single tensor\nhas shape (num_all_proposals, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y].\n\n* bbox_weights (list[tensor],Tensor): Regression weights for all proposals in a batch, each\ntensor in list has shape (num_proposals, 4) when concat=False, otherwise just a single\ntensor has shape (num_all_proposals, 4).\n\nReturn type Tuple[Tensor]\n\ninit_weights()\nUse xavier initialization for all weight parameter and set classification head bias as a specific value when\nuse focal loss.\n\nloss (cls_score, bbox_pred, labels, label_weights, bbox_targets, bbox_weights, imgs_whwh=None,\nreduction_override=None, **kwargs)\n“Loss function of DIIHead, get loss of all images.\n\nParameters\n\n39.5. roi_heads 391\n", "vlm_text": "forward ( roi_feat ,  proposal feat ) Forward function of Dynamic Instance Interactive Head. \nParameters \n•  roi_feat  ( Tensor ) – Roi-pooling features with shape (batch_size\\*num proposals, fea- ture dimensions, pooling_h , pooling_w). •  proposal feat  – Intermediate feature get from diihead in last stage, has shape (batch_size, num proposals, feature dimensions) \nget targets ( sampling results ,  gt_bboxes ,  gt_labels ,  r cnn train cf g ,  conca  $\\leftleftarrows$  True ) Calculate the ground truth for all samples in a batch according to the sampling results. \nAlmost the same as the implementation in bbox_head, we passed additional parameters pos in ds list and ne g in ds list to  get target single  function. \nParameters \n•  (List[obj  ( sampling results ) – Sampling Results]): Assign results of all images in a batch after sampling. •  gt_bboxes  ( list[Tensor] ) – Gt_bboxes of all images in a batch, each tensor has shape (num_gt, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  gt_labels  ( list[Tensor] ) – Gt_labels of all images in a batch, each tensor has shape (num_gt,). •  (obj  ( r cnn train cf g ) –  ConfigDict ):  train_cfg  of RCNN. •  concat  ( bool ) – Whether to concatenate the results of all the images in a single batch. \nReturns \nGround truth for proposals in a single image. Containing the following list of Tensors: • labels (list[Tensor],Tensor): Gt_labels for all proposals in a batch, each tensor in list has shape (num proposals,) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals,). • label weights (list[Tensor]): Labels weights for all proposals in a batch, each tensor in list has shape (num proposals,) when  conca  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals,). • b box targets (list[Tensor],Tensor): Regression target for all proposals in a batch, each ten- sor in list has shape (num proposals, 4) when  conca  $\\fallingdotseq$  False , otherwise just a single tensor has shape (num all proposals, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. • b box weights (list[tensor],Tensor): Regression weights for all proposals in a batch, each tensor in list has shape (num proposals, 4) when  concat  $\\leftrightharpoons$  False , otherwise just a single tensor has shape (num all proposals, 4). \nReturn type  Tuple[Tensor] \nin it weights()\nUse xavier initialization for all weight parameter and set classification head bias as a specific value when use focal loss. \nloss ( cls_score ,  bbox_pred ,  labels ,  label weights ,  b box targets ,  b box weights ,  imgs_whwh=None , reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) “Loss function of DIIHead, get loss of all images. \nParameters "}
{"page": 399, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_399.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ncls_score (Tensor) — Classification prediction results of all class, has shape (batch_size\n* num_proposals_single_image, num_classes)\n\nbbox_pred (Tensor) - Regression prediction results, has shape (batch_size *\nnum_proposals_single_image, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y].\n\nlabels (Tensor) -— Label of each proposals, has shape (batch_size *\nnum_proposals_single_image\n\nlabel_weights (Tensor) — Classification loss weight of each proposals, has shape\n(batch_size * num_proposals_single_image\n\nbbox_targets (Tensor) — Regression targets of each proposals, has shape (batch_size *\nnum_proposals_single_image, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y].\n\nbbox_weights (Tensor) - Regression loss weight of each proposals’s coordinate, has\nshape (batch_size * num_proposals_single_image, 4),\n\nimgs_whwh (Tensor) — imgs_whwh (Tensor): Tensor with shape (batch_size,\nnum_proposals, 4), the last dimension means [img_width,img_height, img_width,\nimg_height].\n\nreduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Options are “none”, “mean” and “sum”. Defaults to\nNone,\n\nReturns -— dict[str, Tensor]: Dictionary of loss components\n\nclass mmdet.models.roi_heads .DoubleConvFCBBoxHead (num_convs=0, num_fcs=0,\n\nconv_out_channels=1024, fc_out_channels=1024,\n\nconv_cfg=None, norm_cfg={ ‘type’: 'BN'},\ninit_cfg={ ‘override’: [{'type': ‘Normal’, 'name':\n‘fc_cls', 'std': 0.01}, {'type': ‘Normal’, ‘name’:\n'fc_reg', 'std': 0.001}, {'type': 'Xavier', ‘name’:\n‘fc_branch', ‘distribution’: 'uniform'}], ‘type’:\n‘Normal'}, **kwargs)\n\nBbox head used in Double-Head R-CNN\n\n/-> cls\n/-> shared convs ->\n\\-> reg\nroi features\n/-> cls\n\\-> shared fc ->\n\\-> reg\nforward (x_cls, x_reg)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\n\nthe latter silently ignores them.\n\nclass mmdet.models.roi_heads .DoubleHeadRoIHead (reg_roi_scale_factor, **kwargs)\nRol head for Double Head RCNN.\n\nhttps://arxiv.org/abs/1904.06493\n\n392\n\nChapter 39. mmdet.models\n\n", "vlm_text": "•  cls_score  ( Tensor ) – Classification prediction results of all class, has shape (batch_size \\* num proposals single image, num classes) •  bbox_pred  ( Tensor ) – Regression prediction results, has shape (batch_size \\* num proposals single image, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  labels ( Tensor ) – Label of each proposals, has shape (batch_size \\* num proposals single image •  label weights  ( Tensor ) – Classification loss weight of each proposals, has shape (batch_size \\* num proposals single image •  b box targets  ( Tensor ) – Regression targets of each proposals, has shape (batch_size \\* num proposals single image, 4), the last dimension 4 represents [tl_x, tl_y, br_x, br_y]. •  b box weights  ( Tensor ) – Regression loss weight of each proposals’s coordinate, has shape (batch_size \\* num proposals single image, 4), •  imgs_whwh  ( Tensor ) – imgs_whwh (Tensor): Tensor with shape (batch_size, num proposals, 4), the last dimension means [img_width,img_height, img_width, img_height]. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. Defaults to None, •  Returns  – dict[str, Tensor]: Dictionary of loss components \nclass  mmdet.models.roi_heads. Double Con v FCB Box Head ( num_convs  $\\scriptstyle{\\varepsilon=0}$  ,  num_fc  $s{=}0$  , \nBbox head used in Double-Head R-CNN con v out channel  $\\mathbf{:=}$  1024 ,  fc out channels  $\\mathbf{\\beta}=\\mathbf{\\beta}$  1024 , conv_cfg  $=$  None ,  norm_cfg  $\\scriptstyle{\\tilde{}}=$  {'type': 'BN'} , init_cfg={'override': [{'type': 'Normal', 'name':\n\n 'fc_cls', 'std': 0.01}, {'type': 'Normal', 'name':\n\n 'fc_reg', 'std': 0.001}, {'type': 'Xavier', 'name':\n\n 'fc_branch', 'distribution': 'uniform'}], 'type':\n\n 'Normal'} ,  \\*\\*kwargs ) \n\nThe table visually represents a flow of processing steps for \"roi features\" (Region of Interest features) in a computational model. It outlines two main paths:\n\n1. **Shared Convolutions Path:**\n   - Starts with roi features\n   - Goes through shared convolutions\n   - Splits into two outputs: \"cls\" (classification) and \"reg\" (regression)\n\n2. **Shared Fully Connected (fc) Path:**\n   - Starts with roi features\n   - Goes through shared fully connected layers\n   - Splits into two outputs: \"cls\" (classification) and \"reg\" (regression)\n\nThis table seems to depict a dual-path processing architecture, likely part of an object detection or image segmentation model, where both convolutional and fully connected operations are shared and lead to classification and regression tasks.\nforward  $(x\\_c l s,x\\_r e g)$  Defines the computation performed at every call. \nShould be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.roi_heads. Double Head RoI Head ( reg roi scale factor ,  \\*\\*kwargs ) RoI head for Double Head RCNN. \nhttps://arxiv.org/abs/1904.06493 "}
{"page": 400, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_400.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.roi_heads .DynamicRoIHead (**kwargs)\nRol head for Dynamic R-CNN.\n\nforward_train(x, img_metas, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore=None,\ngt_masks=None)\nForward function for training.\n\nParameters\n¢ x (list [Tensor]) — list of multi-level img features.\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* proposals (list [Tensors]) — list of region proposals.\n\n* gt_bboxes (list [Tensor ]) — each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n* gt_masks (None | Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\nupdate_hyperparameters ()\n\nUpdate hyperparameters like IoU thresholds for assigner and beta for SmoothL1 loss based on the training\nStatistics.\n\nReturns the updated iou_thr and beta.\nReturn type tuple[float]\n\nclass mmdet.models.roi_heads.FCNMaskHead (num_convs=4, roi_feat_size=14, in_channels=256,\nconv_kernel_size=3, conv_out_channels=256,\nnum_classes=80, class_agnostic=False,\nupsample_cfg={'scale_factor': 2, 'type': ‘'deconv'},\nconv_cfg=None, norm_cfg=None, predictor_cfg={'type':\n‘Conv'}, loss_mask={‘loss_weight': 1.0, 'type':\n‘CrossEntropyLoss’, 'use_mask': True}, init_cfg=None)\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nget_seg_masks (mask_pred, det_bboxes, det_labels, rcnn_test_cfg, ori_shape, scale_factor, rescale)\nGet segmentation masks from mask_pred and bboxes.\n\n39.5. roi_heads 393\n", "vlm_text": "class  mmdet.models.roi_heads. Dynamic RoI Head ( \\*\\*kwargs ) RoI head for  Dynamic R-CNN . \nforward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None , gt_mask  $\\mathfrak{s}\\mathbf{=}\\mathfrak{s}$  None ) Forward function for training. \nParameters \n•  x  ( list[Tensor] ) – list of multi-level img features. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposals  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task.  a dictionary of loss components  dict[str, Tensor] \nupdate hyper parameters () \nUpdate hyper parameters like IoU thresholds for assigner and beta for SmoothL1 loss based on the training statistics. \nReturns  the updated  iou_thr  and  beta . Return type  tuple[float] \nclass  mmdet.models.roi_heads. FC N Mask Head ( num_convs  $\\scriptstyle{\\prime}=4$  ,  roi feat size  $\\scriptstyle{\\varepsilon=I4}$  ,  in channels  $=\\!256$  , \ncon v kernel size  $\\scriptstyle{\\stackrel{\\prime}{=}}3$  ,  con v out ch anne  $I_{S=256}$  , num_classe  $s{=}8O$  ,  class agnostic  $\\mathbf{\\dot{\\rho}}=$  False , up sample cf g  $\\mathbf{\\beta}=$  {'scale factor': 2, 'type': 'deconv'} , conv_cfg  $=$  None ,  norm_cfg  $=$  None ,  predictor cf g={'type': 'Conv'} ,  loss_mask={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use_mask': True} ,  init_cfg=None ) \nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nget seg masks ( mask_pred ,  det_bboxes ,  det_labels ,  r cnn test cf g ,  ori_shape ,  scale factor ,  rescale ) Get segmentation masks from mask_pred and bboxes. "}
{"page": 401, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_401.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\n* mask_pred (Tensor or ndarray) — shape (n, #class, h, w). For single-scale testing,\nmask_pred is the direct output of model, whose type is Tensor, while for multi-scale testing,\nit will be converted to numpy array outside of this method.\n\n* det_bboxes (Tensor) — shape (n, 4/5)\n\n¢ det_labels (Tensor) — shape (n, )\n\n* renn_test_cfg (dict) — rcnn testing config\n\n* ori_shape (Tup1e) — original image height and width, shape (2,)\n\n¢ scale_factor (ndarray | Tensor) —If rescale is True, box coordinates are di-\nvided by this scale factor to fit ori_shape.\n\n* rescale (bool) — If True, the resulting masks will be rescaled to ori_shape.\nReturns\n\nencoded masks. The c-th item in the outer list corresponds to the c-th class. Given the c-\nth outer list, the i-th item in that inner list is the mask for the i-th box with class label\nc.\n\nReturn type list[list]\n\nExample\n\n>>> import mmcv\n\n>>> from mmdet.models.roi_heads.mask_heads.fcn_mask_head import * # NOQA\n>>> N= 7 #N = number of extracted ROIs\n\n>>> C, H, W= 11, 32, 32\n\n>>> # Create example instance of FCN Mask Head.\n\n>>> self = FCNMaskHead(num_classes=C, num_convs=0)\n\n>>> inputs = torch.rand(N, self.in_channels, H, W)\n\n>>> mask_pred = self.forward(inputs)\n\n>>> # Each input is associated with some bounding box\n>>> det_bboxes = torch.Tensor([[1, 1, 42, 42 ]] * N\n\n>>> det_labels = torch.randint(9, C, size=(N,))\n\n>>> renn_test_cfg = mmcv.Config({'mask_thr_binary': 0, })\n>>> ori_shape = (H * 4, W* 4)\n\n>>> scale_factor = torch.FloatTensor((1, 1))\n\n>>> rescale = False\n\n>>> # Encoded masks are a list for each category.\n\n>>> encoded_masks = self.get_seg_masks(\n\n>>> mask_pred, det_bboxes, det_labels, rcnn_test_cfg, ori_shape,\n>>> scale_factor, rescale\n\n>>> )\n\n>>> assert len(encoded_masks) == C\n\n>>> assert sum(list(map(len, encoded_masks))) == N\n\ninit_weights()\nInitialize the weights.\n\nloss (mask_pred, mask_targets, labels)\n\n394\n\nChapter 39. mmdet.models\n", "vlm_text": "Parameters \n•  mask_pred  ( Tensor or ndarray ) – shape (n, #class, h, w). For single-scale testing, mask_pred is the direct output of model, whose type is Tensor, while for multi-scale testing, it will be converted to numpy array outside of this method. •  det_bboxes  ( Tensor ) – shape (n, 4/5) •  det_labels  ( Tensor ) – shape (n, ) •  r cnn test cf g  ( dict ) – rcnn testing config •  ori_shape  ( Tuple ) – original image height and width, shape (2,) •  scale factor  ( ndarray | Tensor ) – If  rescale is True , box coordinates are di- vided by this scale factor to fit  ori_shape . •  rescale  ( bool ) – If True, the resulting masks will be rescaled to  ori_shape . \nReturns \nencoded masks. The c-th item in the outer list  corresponds to the c-th class. Given the c- th outer list, the i-th item in that inner list is the mask for the i-th box with class label c. \nReturn type  list[list] \nExample \n $>>$   import  mmcv  $>>$   from  mmdet.models.roi_heads.mask_heads.fc n mask head  import  \\* # NOQA >>>  $\\texttt{N=7}$  #   $N\\;=\\;$   number of extracted ROIs  $\\ggg$  , H,  $\\texttt{W}=\\texttt{11}$  ,  32 ,  32  $>>$   # Create example instance of FCN Mask Head.  $>>$   self  $=$   FC N Mask Head(num classes  $\\mathsf{\\Gamma}=\\mathsf{C}$  , num_convs  $\\scriptstyle\\left.=0\\right)$  )  $>>$   inputs  $=$   torch . rand(N,  self . in channels, H, W)  $>>$   mask_pred  $=$   self . forward(inputs)  $>>$   # Each input is associated with some bounding box  $>>$   det_bboxes  $=$   torch . Tensor([[ 1 ,  1 ,  42 ,  42  ]]  \\*  N) >>>  det_labels  $=$   torch . randint( 0 , C, size  $=$  (N,)) >>>  r cnn test cf g  $=$   mmcv . Config({ ' mask thr binary ' :  0 , }) >>>  ori_shape  $=$   (H  \\*  4 , W  \\*  4 ) >>>  scale factor  $=$   torch . Float Tensor(( 1 ,  1 )) >>>  rescale  $=$   False >>>  # Encoded masks are a list for each category. >>>  encoded masks  $=$   self . get seg masks( >>> mask_pred, det_bboxes, det_labels, r cnn test cf g, ori_shape, >>> scale factor, rescale >>>  ) >>>  assert  len (encoded masks)  $\\mathbf{\\mu}=\\textsf{C}$  >>>  assert  sum ( list ( map ( len , encoded masks)))  ==  N \nin it weights()\nInitialize the weights. \nloss ( mask_pred ,  mask targets ,  labels ) "}
{"page": 402, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_402.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> from mmdet.models.roi_heads.mask_heads.fcn_mask_head import * # NOQA\n>>> N= 7 #N = number of extracted ROIs\n\n>>> C, H, W= 11, 32, 32\n\n>>> # Create example instance of FCN Mask Head.\n\n>>> # There are lots of variations depending on the configuration\n>>> self = FCNMaskHead(num_classes=C, num_convs=1)\n\n>>> inputs = torch.rand(N, self.in_channels, H, W)\n\n>>> mask_pred = self.forward(inputs)\n\n>>> sf = self.scale_factor\n\n>>> labels = torch.randint(0, C, size=(N,))\n\n>>> # With the default properties the mask targets should indicate\n>>> # a (potentially soft) single-class label\n\n>>> mask_targets = torch.rand(N, H * sf, W * sf)\n\n>>> loss = self.loss(mask_pred, mask_targets, labels)\n\n>>> print('loss = {!r}'.format(loss))\n\nonnx_export (mask_pred, det_bboxes, det_labels, rcnn_test_cfg, ori_shape, **kwargs)\nGet segmentation masks from mask_pred and bboxes.\n\nParameters\n\n* mask_pred (Tensor) — shape (n, #class, h, w).\n\n* det_bboxes (Tensor) — shape (n, 4/5)\n\n¢ det_labels (Tensor) — shape (n, )\n\n* renn_test_cfg (dict) — rcnn testing config\n\n* ori_shape (Tup1e) — original image height and width, shape (2,)\nReturns a mask of shape (N, img_h, img_w).\nReturn type Tensor\n\nclass mmdet.models.roi_heads.FeatureRelayHead (in_channels=1024, out_conv_channels=256,\nroi_feat_size=7, scale_factor=2, init_cfg=(‘layer':\n‘Linear’, 'type': 'Kaiming'})\nFeature Relay Head used in SCNet.\n\nParameters\n¢ in_channels (int, optional) — number of input channels. Default: 256.\n\n* conv_out_channels (int, optional) —number of output channels before classification\nlayer. Default: 256.\n\n* roi_feat_size (int, optional) — roi feat size at box head. Default: 7.\n\n* scale_factor (int, optiona1) — scale factor to match roi feat size at mask head. De-\nfault: 2.\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward(x)\nForward function.\n\n39.5. roi_heads\n", "vlm_text": "Example \n $>>$   from  mmdet.models.roi_heads.mask_heads.fc n mask head  import  \\* # NOQA >>>  $\\texttt{N=7}$  #   $N\\;=\\;$   number of extracted ROIs >>>  C, H,  $\\texttt{W}=\\texttt{11}$  ,  32 ,  32  $>>$   # Create example instance of FCN Mask Head. >>>  # There are lots of variations depending on the configuration  $>>$   self  $=$   FC N Mask Head(num classes  $\\mathsf{\\Lambda}_{=}\\mathsf{C}$  , num_convs  $^{=1}$  )  $>>$   inputs  $=$   torch . rand(N,  self . in channels, H, W) >>>  mask_pred  $=$   self . forward(inputs) >>>  sf  $=$   self . scale factor  $>>$   labels  $=$   torch . randint( 0 , C, size  $=$  (N,)) >>>  # With the default properties the mask targets should indicate >>>  # a (potentially soft) single-class label  $>>$  mask targets $=$  torch.rand(N, H \\* sf, W \\* sf) $>>$   loss  $=$   self . loss(mask_pred, mask targets, labels) >>>  print (  $\\,^{\\dagger}\\,\\bot o s s\\ =\\ \\{\\,I\\,x\\}\\,^{\\dagger}$  . format(loss)) \non nx export ( mask_pred ,  det_bboxes ,  det_labels ,  r cnn test cf g ,  ori_shape ,  \\*\\*kwargs ) Get segmentation masks from mask_pred and bboxes. \nParameters \n•  mask_pred  ( Tensor ) – shape (n, #class, h, w). •  det_bboxes  ( Tensor ) – shape (n, 4/5) •  det_labels  ( Tensor ) – shape (n, ) •  r cnn test cf g  ( dict ) – rcnn testing config •  ori_shape  ( Tuple ) – original image height and width, shape (2,) Returns  a mask of shape (N, img_h, img_w). Return type  Tensor \nclass  mmdet.models.roi_heads. Feature Relay Head ( in channels  $\\mathbf{\\hat{\\rho}}$  1024 ,  out con v channel  $s{=}256.$  , roi feat size  $\\mathbf{\\beta}=\\mathbf{\\dot{\\beta}}$  7 ,  scale factor  ${\\it\\Delta\\phi}=\\!2{\\it\\Delta\\Psi}$  ,  init_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'layer': 'Linear', 'type': 'Kaiming'} ) \nFeature Relay Head used in  SCNet . \nParameters \n•  in channels  ( int, optional ) – number of input channels. Default: 256. •  con v out channels  ( int, optional ) – number of output channels before classification layer. Default: 256. •  roi feat size  ( int, optional ) – roi feat size at box head. Default: 7. •  scale factor  ( int, optional ) – scale factor to match roi feat size at mask head. De- fault: 2. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward  $(x)$  Forward function. "}
{"page": 403, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_403.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.roi_heads.FusedSemanticHead(num_ins, fusion_level, num_convs=4,\nin_channels=256, conv_out_channels=256,\nnum_classes=183, conv_cfg=None, norm_cfg=None,\nignore_label=None, loss_weight=None,\nloss_seg={‘ignore_index': 255, ‘loss_weight': 0.2,\n‘type’: 'CrossEntropyLoss'}, init_cfg={‘override':\n{‘name’': 'conv_logits'}, 'type': 'Kaiming'})\n\nMulti-level fused semantic segmentation head.\n\nin_1 -> 1x1 conv ---\nin_2 -> 1x1 conv --\n\n|\nin_3 -> 1x1 conv - |\n\n| /-> 1x1 conv (mask prediction)\nin_4 -> 1x1 conv ----- > 3x3 convs (*4)\n\n| \\-> 1x1 conv (feature)\nin_5 -> 1x1 conv ---\n\nforward (feats)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.roi_heads.GenericRoIExtractor (aggregation='sum', pre_cfg=None, post_cfg=None,\n*“*kwargs)\nExtract Rol features from all level feature maps levels.\n\nThis is the implementation of A novel Region of Interest Extraction Layer for Instance Segmentation.\nParameters\n\n* aggregation (str) - The method to aggregate multiple feature maps. Options are ‘sum’,\n\n>\n\n‘concat’. Default: ‘sum’.\n* pre_cfg (dict | None) — Specify pre-processing modules. Default: None.\n* post_cfg (dict | None) — Specify post-processing modules. Default: None.\n* kwargs (keyword arguments) — Arguments that are the same as BaseRoIExtractor.\n\nforward (feats, rois, roi_scale_factor=None)\nForward function.\n\nclass mmdet.models.roi_heads.GlobalContextHead (num_convs=4, in_channels=256,\nconv_out_channels=256, num_classes=80,\nloss_weight=1.0, conv_cfg=None, norm_cfg=None,\nconv_to_res=False, init_cfg={ ‘override’: {‘name': ‘fc'},\n‘std': 0.01, ‘type’: 'Normal'})\nGlobal context head used in SCNet.\n\nParameters\n\n* num_convs (int, optional) -— number of convolutional layer in GlbCtxHead. Default: 4.\n\n396 Chapter 39. mmdet.models\n", "vlm_text": "The image contains a code snippet that defines a class named `FusedSemanticHead` from the `mmdet.models.roi_heads` module. It includes several parameters and configurations, such as:\n\n- `num_ins`\n- `fusion_level`\n- `num_convs=4`\n- `in_channels=256`\n- `conv_out_channels=256`\n- `num_classes=183`\n- `conv_cfg=None`\n- `norm_cfg=None`\n- `ignore_label=None`\n- `loss_weight=None`\n- `loss_seg` which includes details: \n  - `'ignore_index': 255`\n  - `'loss_weight': 0.2`\n  - `'type': 'CrossEntropyLoss'`\n- `init_cfg` which includes:\n  - `{'override': {'name': 'conv_logits'}, 'type': 'Kaiming'}`\n\nThis appears to be part of a configuration for a deep learning model, likely related to semantic segmentation.\nMulti-level fused semantic segmentation head. \nThe image is a schematic diagram of a convolutional neural network (CNN) architecture, showing how different input layers (in_1 to in_5) are processed through convolutional layers. Here's a breakdown of the diagram:\n\n1. **Inputs (in_1 to in_5)**: Five different inputs labeled in_1, in_2, in_3, in_4, and in_5.\n\n2. **1x1 Convolutional Layers**: Each input is passed through a 1x1 convolutional layer. This layer is typically used for dimensionality reduction or feature transformation.\n\n3. **Aggregation**: The outputs of the 1x1 convolutional layers from in_1 to in_5 are aggregated (appears to be concatenated or added together).\n\n4. **3x3 Convolutional Layer**: The aggregated output undergoes further processing through a series of four 3x3 convolutional layers, which are often used to capture spatial relationships.\n\n5. **Final Layers**:\n   - **Mask Prediction**: The output of the 3x3 convolutions is passed through another 1x1 convolutional layer specifically designated for mask prediction.\n   - **Feature Layer**: A different path from the 1x1 convolution layer outputs a \"feature\" layer, also through a 1x1 convolutional layer.\n\nThis diagram represents a common pattern in CNNs used for tasks like object detection, segmentation, or feature extraction, where different inputs are convolved and combined in a structured manner to produce outputs like masks or features.\nforward ( feats ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.roi_heads. Generic RoI Extractor ( aggregation  $=$  sum' ,  pre_cfg  $=$  None ,  post_cfg=None , \\*\\*kwargs ) \nExtract RoI features from all level feature maps levels. This is the implementation of  A novel Region of Interest Extraction Layer for Instance Segmentation . \n\nParameters \n•  aggregation  ( str ) – The method to aggregate multiple feature maps. Options are ‘sum’, ‘concat’. Default: ‘sum’. \n•  pre_cfg  ( dict | None ) – Specify pre-processing modules. Default: None. •  post_cfg  ( dict | None ) – Specify post-processing modules. Default: None. •  kwargs  ( keyword arguments ) – Arguments that are the same as  Base RoI Extractor . \n\n\nforward ( feats ,  rois ,  roi scale factor  $\\leftrightharpoons$  None ) Forward function. \nclass  mmdet.models.roi_heads. Global Context Head ( num_convs  $\\scriptstyle{\\prime}=4$  ,  in_channel  $\\varsigma{=}256$  , con v out channel  $\\mathfrak{s}{=}256$  ,  num classes  $\\scriptstyle{:=80}$  , loss_weigh  $t{=}I.0$  ,  conv_cfg  $=$  None ,  norm_cfg  $=$  None , con v to res  $=$  False ,  init_cfg  $=$  {'override': {'name': 'fc'}, 'std': 0.01, 'type': 'Normal'} ) \nGlobal context head used in  SCNet . \nParameters \n•  num_convs  ( int, optional ) – number of convolutional layer in GlbCtxHead. Default: 4. "}
{"page": 404, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_404.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nin_channels (int, optional) -— number of input channels. Default: 256.\n\nconv_out_channels (int, optional) —number of output channels before classification\nlayer. Default: 256.\n\nnum_classes (int, optional) — number of classes. Default: 80.\nloss_weight (float, optional) — global context loss weight. Default: 1.\nconv_cfg (dict, optional) — config to init conv layer. Default: None.\nnorm_cfg (dict, optional) — config to init norm layer. Default: None.\n\nconv_to_res (bool, optional) — if True, 2 convs will be grouped into 1 SimplifiedBa-\nsicBlock using a skip connection. Default: False.\n\ninit_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (feats)\nForward function.\n\nloss (pred, labels)\nLoss function.\n\nclass mmdet.models.roi_heads.GridHead(grid_points=9, num_convs=8, roi_feat_size=14, in_channels=256,\n\nconv_kernel_size=3, point_feat_channels=64,\ndeconv_kernel_size=4, class_agnostic=False,\nloss_grid={‘loss_weight': 15, 'type': 'CrossEntropyLoss',\n‘use_sigmoid': True}, conv_cfg=None, norm_cfg={‘num_groups':\n36, ‘type’: 'GN'}, init_cfg=[[{'type': 'Kaiming’, ‘layer’: ['Conv2d',\n'‘Linear']}, {‘type': 'Normal’, ‘layer’: 'ConvTranspose2d’, ‘std’:\n0.001, ‘override’: {'type': ‘Normal’, ‘name’: 'deconv2’, 'std': 0.001,\n'bias': - 4.59511985013459}}])\n\ncalc_sub_regions()\nCompute point specific representation regions.\n\nSee Grid R-CNN Plus (https://arxiv.org/abs/1906.05688) for details.\n\nforward(x)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.roi_heads.GridRoIHead (grid_roi_extractor, grid_head, **kwargs)\nGrid roi head for Grid R-CNN.\n\nhttps://arxiv.org/abs/1811.12030\n\nforward_dummy (x, proposals)\nDummy forward function.\n\nsimple_test (x, proposal_list, img_metas, proposals=None, rescale=False)\nTest without augmentation.\n\nclass mmdet.models.roi_heads.HTCMaskHead (with_conv_res=True, “args, **kwargs)\n\n39.5. roi_heads\n\n397\n", "vlm_text": "•  in channels  ( int, optional ) – number of input channels. Default: 256. •  con v out channels  ( int, optional ) – number of output channels before classification layer. Default: 256. •  num classes  ( int, optional ) – number of classes. Default: 80. •  loss weight  ( float, optional ) – global context loss weight. Default: 1. •  conv_cfg  ( dict, optional ) – config to init conv layer. Default: None. •  norm_cfg  ( dict, optional ) – config to init norm layer. Default: None. •  con v to res  ( bool, optional ) – if True, 2 convs will be grouped into 1  Simplified Ba- sicBlock  using a skip connection. Default: False. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( feats ) Forward function. loss ( pred ,  labels ) Loss function. \nclass  mmdet.models.roi_heads. GridHead ( grid points  $_{;=9}$  ,  num_convs=8 ,  roi feat size=14 ,  in channels=256 , con v kernel size=3 ,  point feat channels=64 , dec on v kernel size=4 ,  class agnostic=False , loss_grid={'loss weight': 15, 'type': 'Cross Entropy Loss', 'use s igm oid': True} ,  conv_cfg=None ,  norm_cfg={'num_groups': 36, 'type': 'GN'} ,  init_cfg=[{'type': 'Kaiming', 'layer': ['Conv2d', 'Linear']}, {'type': 'Normal', 'layer': 'Con v Transpose 2 d', 'std': 0.001, 'override': {'type': 'Normal', 'name': 'deconv2', 'std': 0.001, 'bias': - 4.59511985013459}}])\ncalc sub regions () Compute point specific representation regions. See Grid R-CNN Plus ( https://arxiv.org/abs/1906.05688 ) for details. \nforward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.roi_heads. Grid RoI Head ( grid roi extractor ,  grid_head ,  \\*\\*kwargs ) Grid roi head for Grid R-CNN. \nhttps://arxiv.org/abs/1811.12030 \nforward dummy ( x ,  proposals ) Dummy forward function. \nsimple test ( x ,  proposal list ,  img_metas ,  proposals  $\\mathbf{\\check{\\Sigma}}$  None ,  rescale  $\\mathbf{=}$  False ) Test without augmentation. \nclass  mmdet.models.roi_heads. HTC Mask Head ( with con v res  $\\mathbf{=}$  True ,  \\*args ,  \\*\\*kwargs ) "}
{"page": 405, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_405.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward (x, res_feat=None, return_logits=True, return_feat=True)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.roi_heads.HybridTaskCascadeRoIHead(num_stages, stage_loss_weights,\nsemantic_roi_extractor=None,\nsemantic_head=None,\nsemantic_fusion=(‘bbox', 'mask’'),\ninterleaved=True, mask_info_flow=True,\n**kwargs)\nHybrid task cascade roi head including one bbox head and one mask head.\n\nhttps://arxiv.org/abs/1901.07518\n\naug_test (img_feats, proposal_list, img_metas, rescale=False)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nforward_dummy (x, proposals)\nDummy forward function.\n\nforward_train(x, img_metas, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore=None,\ngt_masks=None, gt_semantic_seg=None)\n\nParameters\n¢ x (list [Tensor]) — list of multi-level img features.\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* proposal_list (list [Tensors]) — list of region proposals.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None, list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n* gt_masks (None, Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\n* gt_semantic_seg (None, list [Tensor]) — semantic segmentation masks used if the\narchitecture supports semantic segmentation task.\n\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\nsimple_test (x, proposal_list, img_metas, rescale=False)\nTest without augmentation.\n\n398 Chapter 39. mmdet.models\n", "vlm_text": "forward ( x ,  res_feat  $=$  None ,  return log its  $\\mathbf{=}$  True ,  return feat  $=$  True ) Defines the computation performed at every call. \nShould be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.roi_heads. Hybrid Task Cascade RoI Head ( num_stages ,  stage loss weights , semantic roi extractor  $\\bf{=}$  None , semantic head  $\\leftrightharpoons$  None , semantic fusion  $=$  ('bbox', 'mask') , interleaved  $\\leftrightharpoons$  True ,  mask info flow  $\\scriptstyle\\sum=$  True , \\*\\*kwargs ) \nHybrid task cascade roi head including one bbox head and one mask head. \nhttps://arxiv.org/abs/1901.07518 \naug_test ( img_feats ,  proposal list ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Test with augmentations. \nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nforward dummy ( x ,  proposals ) Dummy forward function. \nforward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , gt_mask  $\\leftrightharpoons$  None ,  gt semantic seg  $=$  None ) \nParameters \n x  ( list[Tensor] ) – list of multi-level img features. \n•  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposal list  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None, list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None, Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. •  gt semantic seg  ( None, list[Tensor] ) – semantic segmentation masks used if the architecture supports semantic segmentation task. Returns  a dictionary of loss components  dict[str, Tensor] \nsimple test  $\\langle x.$  ,  proposal list ,  img_metas ,  rescale  $=$  False ) Test without augmentation. "}
{"page": 406, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_406.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\n¢ x(tuple[Tensor]) — Features from upstream network. Each has shape (batch_size, c, h,\nw).\n\n* proposal_list (list(Tensor)) — Proposals from rpn head. Each has shape\n(num_proposals, 5), last dimension 5 represent (x1, yl, x2, y2, score).\n\n¢ img_metas (list [dict ]) — Meta information of images.\n* rescale (bool) — Whether to rescale the results to the original image. Default: True.\n\nReturns When no mask branch, it is bbox results of each image and classes with type\nlist[list{np.ndarray]]. The outer list corresponds to each image. The inner list corresponds\nto each class. When the model has mask branch, it contains bbox results and mask results.\nThe outer list corresponds to each image, and first element of tuple is bbox results, second\nelement is mask results.\n\nReturn type list[list{np.ndarray]] or list[tuple]\n\nproperty with_semantic\nwhether the head has semantic head\n\nType bool\n\nclass mmdet.models.roi_heads.MaskIoUHead (num_convs=4, num_fcs=2, roi_feat_size=14,\nin_channels=256, conv_out_channels=256,\nfc_out_channels=1024, num_classes=80,\nloss_iou={'loss_weight': 0.5, ‘type’: 'MSELoss'},\ninit_cfg=[{'type': 'Kaiming’, ‘override’: {‘name': 'convs'}},\n{'type’: 'Caffe2Xavier’, ‘override’: {‘name': 'fcs'}}, {‘type':\n‘Normal’, 'std': 0.01, ‘override’: {'‘name': 'fc_mask_iou'}}])\n\nMask IoU Head.\n\nThis head predicts the loU of predicted masks and corresponding gt masks.\n\nforward (mask_feat, mask_pred)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nget_mask_scores (mask_iou_pred, det_bboxes, det_labels)\nGet the mask scores.\n\nmask_score = bbox_score * mask_iou\n\nget_targets(sampling_results, gt_masks, mask_pred, mask_targets, rcnn_train_cfg)\nCompute target of mask IoU.\n\nMask IoU target is the IoU of the predicted mask (inside a bbox) and the gt mask of corresponding gt mask\n(the whole instance). The intersection area is computed inside the bbox, and the gt mask area is computed\nwith two steps, firstly we compute the gt area inside the bbox, then divide it by the area ratio of gt area\ninside the bbox and the gt area of the whole instance.\n\nParameters\n\n¢ sampling_results (list{SamplingResult]) — sampling results.\n\n39.5. roi_heads 399\n", "vlm_text": "Parameters \n•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). \n proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). \n•  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. \n\nReturns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. \nReturn type  list[list[np.ndarray]] or list[tuple] \nproperty with semantic whether the head has semantic head \nType  bool \nclass  mmdet.models.roi_heads. Mask I oU Head ( num_convs  $\\scriptstyle{\\prime}=4$  ,  num_fcs=2 ,  roi feat size=14 , \nin_channel  $\\backsimeq$  256 ,  con v out channels=256 , fc out channels  $\\mathbf{\\hat{\\rho}}$  1024 ,  num classes=80 , loss_iou  $\\mathbf{\\beta}=$  {'loss weight': 0.5, 'type': 'MSELoss'} , init_cfg=[{'type': 'Kaiming', 'override': {'name': 'convs'}}, {'type': 'Caff e 2 Xavier', 'override': {'name': 'fcs'}}, {'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc mask i ou'}}] ) \nMask IoU Head. \nThis head predicts the IoU of predicted masks and corresponding gt masks. \nforward ( mask_feat ,  mask_pred ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nget mask scores ( mask i ou p red ,  det_bboxes ,  det_labels ) Get the mask scores. \nmask_score  $=$   bbox_score \\* mask_iou \nget targets ( sampling results ,  gt_masks ,  mask_pred ,  mask targets ,  r cnn train cf g ) Compute target of mask IoU. \nMask IoU target is the IoU of the predicted mask (inside a bbox) and the gt mask of corresponding gt mask (the whole instance). The intersection area is computed inside the bbox, and the gt mask area is computed with two steps, firstly we compute the gt area inside the bbox, then divide it by the area ratio of gt area inside the bbox and the gt area of the whole instance. \nParameters \n sampling results  (list[ Sampling Result ]) – sampling results. "}
{"page": 407, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_407.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ gt_masks (BitmapMask | PolygonMask) — Gt masks (the whole instance) of each im-\nage, with the same shape of the input image.\n\n¢ mask_pred (Tensor) — Predicted masks of each positive proposal, shape (num_pos, h, w).\n\n* mask_targets (Tensor) — Gt mask of each positive proposal, binary map of the shape\n(num_pos, h, w).\n\n* renn_train_cfg (dict) — Training config for R-CNN part.\nReturns mask iou target (length == num positive).\nReturn type Tensor\n\nclass mmdet.models.roi_heads.MaskPointHead (num_classes, num_fcs=3, in_channels=256,\nfc_channels=256, class_agnostic=False,\ncoarse_pred_each_layer=True, conv_cfg=({'type':\n‘Conv1d'}, norm_cfg=None, act_cfg={'type': 'ReLU'},\nloss_point={‘loss_weight': 1.0, 'type': 'CrossEntropyLoss’,\n‘use_mask': True}, init_cfg={‘override': {name':\n‘fc_logits'}, ‘std’: 0.001, 'type': 'Normal'})\n\nA mask point head use in PointRend.\n\nMaskPointHead use shared multi-layer perceptron (equivalent to nn.Conv 1d) to predict the logit of input points.\nThe fine-grained feature and coarse feature will be concatenate together for predication.\n\nParameters\n* num_fcs (int) — Number of fc layers in the head. Default: 3.\n¢ in_channels (int) — Number of input channels. Default: 256.\n¢ £c_channels (int) — Number of fc channels. Default: 256.\n* num_classes (int) — Number of classes for logits. Default: 80.\n\n* class_agnostic (bool) — Whether use class agnostic classification. If so, the output chan-\nnels of logits will be 1. Default: False.\n\n* coarse_pred_each_layer (bool) — Whether concatenate coarse feature with the output\nof each fc layer. Default: True.\n\n* conv_cfg (dict | None) — Dictionary to construct and config conv layer. Default:\ndict(type=’Conv1d’))\n\n* norm_cfg (dict | None) — Dictionary to construct and config norm layer. Default: None.\n\n* loss_point (dict) — Dictionary to construct and config loss layer of point head. Default:\ndict(type=’ CrossEntropyLoss’, use_mask=True, loss_weight=1.0).\n\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (fine_grained_feats, coarse_feats)\nClassify each point base on fine grained and coarse feats.\n\nParameters\n\n¢ fine_grained_feats (Tensor) — Fine grained feature sampled from FPN, shape\n(num_rois, in_channels, num_points).\n\n* coarse_feats (Tensor) — Coarse feature sampled from CoarseMaskHead, shape\n(num_rois, num_classes, num_points).\n\nReturns\n\nPoint classification results, shape (num_rois, num_class, num_points).\n\n400 Chapter 39. mmdet.models\n", "vlm_text": "•  gt_masks  ( BitmapMask | Polygon Mask ) – Gt masks (the whole instance) of each im- age, with the same shape of the input image. •  mask_pred  ( Tensor ) – Predicted masks of each positive proposal, shape (num_pos, h, w). •  mask targets  ( Tensor ) – Gt mask of each positive proposal, binary map of the shape (num_pos, h, w). •  r cnn train cf g  ( dict ) – Training config for R-CNN part. \nReturns  mask iou target (length  $==$   num positive). \nReturn type  Tensor \nclass  mmdet.models.roi_heads. Mask Point Head ( num classes ,  num_fc  $\\mathfrak{s}{=}3$  ,  in channels=256 , fc channels=256 ,  class agnostic=False , coarse p red each layer  $\\mathbf{=}$  True ,  conv_cfg  $=$  {'type': 'Conv1d'} ,  norm_cfg  $=$  None ,  act_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'ReLU'} , loss_point={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use_mask': True} ,  init_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'override': {'name': 'fc_logits'}, 'std': 0.001, 'type': 'Normal'} ) \nA mask point head use in PointRend. \nMask Point Head  use shared multi-layer perceptron (equivalent to nn.Conv1d) to predict the logit of input points. The fine-grained feature and coarse feature will be concatenate together for predication. \nParameters \n•  num_fcs  ( int ) – Number of fc layers in the head. Default: 3. •  in channels  ( int ) – Number of input channels. Default: 256. •  fc channels  ( int ) – Number of fc channels. Default: 256. •  num classes  ( int ) – Number of classes for logits. Default: 80. •  class agnostic  ( bool ) – Whether use class agnostic classification. If so, the output chan- nels of logits will be 1. Default: False. •  coarse p red each layer  ( bool ) – Whether concatenate coarse feature with the output of each fc layer. Default: True. •  conv_cfg  ( dict | None ) – Dictionary to construct and config conv layer. Default: dict(type  $:=$  ’Conv1d’)) •  norm_cfg  ( dict | None ) – Dictionary to construct and config norm layer. Default: None. •  loss_point  ( dict ) – Dictionary to construct and config loss layer of point head. Default: dict(type  $\\mathrel{\\mathop:}=^{:}$  ’Cross Entropy Loss’, use_mask  $\\risingdotseq$  True, loss weight  $\\mathrm{\\Sigma=}1.0$  ). •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( fine grained feats ,  coarse feats ) Classify each point base on fine grained and coarse feats. \nParameters \n•  fine grained feats  ( Tensor ) – Fine grained feature sampled from FPN, shape (num_rois, in channels, num_points). •  coarse feats  ( Tensor ) – Coarse feature sampled from Coarse Mask Head, shape (num_rois, num classes, num_points). \nReturns \nPoint classification results,  shape (num_rois, num_class, num_points). "}
{"page": 408, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_408.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type Tensor\n\nget_roi_rel_points_test (mask_pred, pred_label, cfg)\nGet num_points most uncertain points during test.\n\nParameters\n\n¢« mask_pred (Tensor) -— A tensor of shape (num_rois, num_classes, mask_height,\nmask_width) for class-specific or class-agnostic prediction.\n\n¢ pred_label (list) — The predication class for each instance.\n* cfg (dict) — Testing config of point head.\n\nReturns\n\nA tensor of shape (num_rois, num_points) that contains indices from [0, mask_height x\nmask_width) of the most uncertain points.\n\npoint_coords (Tensor): A tensor of shape (num_rois, num_points, 2) that contains [0, 1]\nx [0, 1] normalized coordinates of the most uncertain points from the [mask_height,\nmask_width] grid .\n\nReturn type point_indices (Tensor)\n\nget_roi_rel_points_train(mask_pred, labels, cfg)\nGet num_points most uncertain points with random points during train.\n\nSample points in [0, 1] x [0, 1] coordinate space based on their uncertainty. The uncertainties are calculated\nfor each point using ‘_get_uncertainty()’ function that takes point’s logit prediction as input.\n\nParameters\n\n¢« mask_pred (Tensor) -— A tensor of shape (num_rois, num_classes, mask_height,\nmask_width) for class-specific or class-agnostic prediction.\n\n¢ labels (list) — The ground truth class for each instance.\n* cfg (dict) — Training config of point head.\n\nReturns\n\nA tensor of shape (num_rois, num_points, 2) that contains the coordinates sampled\npoints.\n\nReturn type point_coords (Tensor)\n\nget_targets (rois, rel_roi_points, sampling_results, gt_masks, cfg)\nGet training targets of MaskPointHead for all images.\n\nParameters\n* rois (Tensor) — Region of Interest, shape (num_rois, 5).\n¢ rel_roi_points — Points coordinates relative to Rol, shape (num_rois, num_points, 2).\n\n¢ sampling_results (SamplingResult) — Sampling result after sampling and assign-\nment.\n\n* gt_masks (Tensor) — Ground truth segmentation masks of corresponding boxes, shape\n(num_rois, height, width).\n\n* cfg (dict) — Training cfg.\nReturns Point target, shape (num_rois, num_points).\n\nReturn type Tensor\n\n39.5. roi_heads 401\n", "vlm_text": "Return type  Tensor \nget roi rel points test ( mask_pred ,  pred_label ,  cfg ) Get  num_points  most uncertain points during test. \nParameters \n•  mask_pred  ( Tensor ) – A tensor of shape (num_rois, num classes, mask height, mask_width) for class-specific or class-agnostic prediction. •  pred_label  ( list ) – The predication class for each instance. •  cfg  ( dict ) – Testing config of point head. \nReturns \nA tensor of shape (num_rois, num_points)  that contains indices from [0, mask height x mask_width) of the most uncertain points. point coord s (Tensor): A tensor of shape (num_rois, num_points, 2)  that contains [0, 1] x [0, 1] normalized coordinates of the most uncertain points from the [mask height, mask_width] grid . \n\nget roi rel points train ( mask_pred ,  labels ,  cfg ) Get  num_points  most uncertain points with random points during train. \nSample points in  $[0,1]\\,\\mathtt{x}\\,[0,1]$   coordinate space based on their uncertainty. The uncertainties are calculated for each point using ‘get uncertainty()’ function that takes point’s logit prediction as input. \nParameters \n•  mask_pred  ( Tensor ) – A tensor of shape (num_rois, num classes, mask height, mask_width) for class-specific or class-agnostic prediction. •  labels  ( list ) – The ground truth class for each instance. •  cfg  ( dict ) – Training config of point head. \nReturns \nA tensor of shape (num_rois, num_points, 2)  that contains the coordinates sampled points. Return type  point coord s (Tensor) \nget targets ( rois ,  rel roi points ,  sampling results ,  gt_masks ,  cfg ) Get training targets of Mask Point Head for all images. \nParameters \n•  rois  ( Tensor ) – Region of Interest, shape (num_rois, 5). •  rel roi points  – Points coordinates relative to RoI, shape (num_rois, num_points, 2). •  sampling results  ( Sampling Result ) – Sampling result after sampling and assign- ment. •  gt_masks  ( Tensor ) – Ground truth segmentation masks of corresponding boxes, shape (num_rois, height, width). •  cfg  ( dict ) – Training cfg. Returns  Point target, shape (num_rois, num_points). \nReturn type  Tensor "}
{"page": 409, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_409.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nloss (point_pred, point_targets, labels)\nCalculate loss for MaskPointHead.\n\nParameters\n\n* point_pred (Tensor) — Point predication result, shape (num_rois, num_classes,\nnum_points).\n\n* point_targets (Tensor) — Point targets, shape (num_roi, num_points).\n\n¢ labels (Tensor) — Class label of corresponding boxes, shape (num_rois, )\nReturns a dictionary of point loss components\nReturn type dict[str, Tensor]\n\nclass mmdet.models.roi_heads.MaskScoringRoIHead (mask_iou_head, **kwargs)\nMask Scoring RolHead for Mask Scoring RCNN.\n\nhttps://arxiv.org/abs/1903.00241\n\nsimple_test_mask (x, img_metas, det_bboxes, det_labels, rescale=False)\nObtain mask prediction without augmentation.\n\nclass mmdet.models.roi_heads.PISARoIHead (bbox_roi_extractor=None, bbox_head=None,\nmask_roi_extractor=None, mask_head=None,\nshared_head=None, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nThe Rol head for Prime Sample Attention in Object Detection.\n\nforward_train(x, img_metas, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore=None,\ngt_masks=None)\nForward function for training.\n\nParameters\n¢ x (list [Tensor]) — List of multi-level img features.\n\n¢ img_metas (list[dict]) - List of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* proposals (list [Tensors]) — List of region proposals.\n\n* gt_bboxes (list [Tensor ]) — Each item are the truth boxes for each image in [tl_x, tl_y,\nbr_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — Class indices corresponding to each box\n\n* gt_bboxes_ignore (list[Tensor], optional) — Specify which bounding boxes can\nbe ignored when computing the loss.\n\n* gt_masks (None | Tensor) — True segmentation masks for each box used if the archi-\ntecture supports a segmentation task.\n\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\nclass mmdet.models.roi_heads.PointRendRoIHead (point_head, *args, **kwargs)\nPointRend.\n\naug_test_mask (feats, img_metas, det_bboxes, det_labels)\nTest for mask head with test time augmentation.\n\n402 Chapter 39. mmdet.models\n", "vlm_text": "loss ( point_pred ,  point targets ,  labels ) Calculate loss for Mask Point Head. \nParameters \n•  point_pred  ( Tensor ) – Point predication result, shape (num_rois, num classes, num_points). •  point targets  ( Tensor ) – Point targets, shape (num_roi, num_points). •  labels  ( Tensor ) – Class label of corresponding boxes, shape (num_rois, ) \nReturns  a dictionary of point loss components \nReturn type  dict[str, Tensor] \nclass  mmdet.models.roi_heads. Mask Scoring RoI Head ( mask i ou head ,  \\*\\*kwargs ) Mask Scoring RoIHead for Mask Scoring RCNN. \nhttps://arxiv.org/abs/1903.00241 \nsimple test mask ( x ,  img_metas ,  det_bboxes ,  det_labels ,  rescale=False ) Obtain mask prediction without augmentation. \nclass  mmdet.models.roi_heads. PISA RoI Head ( b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head  $\\leftrightharpoons$  None , mask roi extractor=None ,  mask_head=None , shared head  $\\leftrightharpoons$  None ,  train_cfg=None ,  test_cfg=None , pretrained=None ,  init_cfg=None ) \nThe RoI head for  Prime Sample Attention in Object Detection . \nforward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , gt_mask  $\\mathbf{\\hat{\\rho}}=$  None ) Forward function for training. \nParameters \n•  x  ( list[Tensor] ) – List of multi-level img features. \n•  img_metas ( list[dict] ) – List of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposals  ( list[Tensors] ) – List of region proposals. •  gt_bboxes  ( list[Tensor] ) – Each item are the truth boxes for each image in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – Class indices corresponding to each box •  gt b boxes ignore  ( list[Tensor], optional ) – Specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – True segmentation masks for each box used if the archi- tecture supports a segmentation task. \n\nReturn type  dict[str, Tensor] \nclass  mmdet.models.roi_heads. Point Rend RoI Head ( point_head ,  \\*args ,  \\*\\*kwargs ) PointRend . \naug test mask ( feats ,  img_metas ,  det_bboxes ,  det_labels ) Test for mask head with test time augmentation. "}
{"page": 410, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_410.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ninit_point_head (point_head)\nInitialize point_head\n\nmask_onnx_export (x, img_metas, det_bboxes, det_labels, **kwargs)\nExport mask branch to onnx which supports batch inference.\n\nParameters\n¢ x (tuple[Tensor ]) — Feature maps of all scale level.\n¢ img_metas (list [dict ]) — Image meta info.\n\n¢ det_bboxes (Tensor) — Bboxes and corresponding scores. has shape [N, num_bboxes,\n5].\n\n¢ det_labels (Tensor) — class labels of shape [N, num_bboxes].\nReturns\n\nThe segmentation results of shape [N, num_bboxes, image_height, image_width].\nReturn type Tensor\n\nsimple_test_mask (x, img_metas, det_bboxes, det_labels, rescale=False)\nObtain mask prediction without augmentation.\n\nclass mmdet.models.roi_heads.ResLayer (depth, stage=3, stride=2, dilation=1, style='pytorch',\nnorm_cfg={ 'requires_grad': True, 'type': 'BN'}, norm_eval=True,\nwith_cp=False, dcn=None, pretrained=None, init_cfg=None)\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\ntrain (mode=True)\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular modules for details of their\nbehaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.\n\nParameters mode (bool) — whether to set training mode (True) or evaluation mode (False).\nDefault: True.\n\nReturns self\n\nReturn type Module\n\n39.5. roi_heads 403\n", "vlm_text": "in it point head ( point_head ) Initialize  point_head \nmask on nx export ( x ,  img_metas ,  det_bboxes ,  det_labels ,  \\*\\*kwargs ) Export mask branch to onnx which supports batch inference. \nParameters \n•  x  ( tuple[Tensor] ) – Feature maps of all scale level. •  img_metas  ( list[dict] ) – Image meta info. •  det_bboxes  ( Tensor ) – Bboxes and corresponding scores. has shape [N, num_bboxes, 5]. •  det_labels  ( Tensor ) – class labels of shape [N, num_bboxes]. \nReturns \nThe segmentation results of shape [N, num_bboxes,  image height, image width]. \nReturn type  Tensor \nsimple test mask (  $\\cdot_{x}$  ,  img_metas ,  det_bboxes ,  det_labels ,  rescale  $\\mathbf{=}$  False ) Obtain mask prediction without augmentation. \nclass  mmdet.models.roi_heads. ResLayer ( depth ,  stage  $\\scriptstyle{:=3}$  ,  stride  $_{:=2}$  ,  dilation  ${=}I$  ,  style  $=$  'pytorch' , norm_cfg  $=$  {'requires grad': True, 'type': 'BN'} ,  norm_eval=True , with_cp $\\leftrightharpoons$ False, dcn $=$ None, pretrained $\\leftrightharpoons$ None, init_cfg $\\mathbf{\\hat{\\mu}}$ None)\nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \ntrain  $(m o d e{=}T r u e)$  \nSets the module in training mode. \nThis has any effect only on certain modules. See documentation s of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g.  Dropout ,  BatchNorm , etc. \nParameters  mode  ( bool ) – whether to set training mode ( True ) or evaluation mode ( False ). Default:  True . \nReturns  self \nReturn type  Module "}
{"page": 411, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_411.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.roi_heads .SABLHead(num_classes, cls_in_channels=256, reg_in_channels=256,\nroi_feat_size=7, reg_feat_up_ratio=2, reg_pre_kernel=3,\nreg_post_kernel=3, reg_pre_num=2, reg_post_num=1,\ncls_out_channels=1024, reg_offset_out_channels=256,\nreg_cls_out_channels=256, num_cls_fcs=1, num_reg_fcs=0,\nreg_class_agnostic=True, norm_cfg=None,\nbbox_coder={'num_buckets': 14, 'scale_factor': 1.7, ‘type’:\n‘BucketingBBoxCoder'’}, loss_cls={'loss_weight': 1.0, ‘type’:\n'CrossEntropyLoss’, 'use_sigmoid': False},\nloss_bbox_cls={'loss_weight': 1.0, 'type': 'CrossEntropyLoss',\n‘use_sigmoid': True}, loss_bbox_reg=({'‘beta': 0.1, ‘loss_weight':\n1.0, ‘type’: '‘SmoothL1Loss'}, init_cfg=None)\n\nSide-Aware Boundary Localization (SABL) for Rol-Head.\n\nSide-Aware features are extracted by conv layers with an attention mechanism. Boundary Localization with\nBucketing and Bucketing Guided Rescoring are implemented in BucketingBBoxCoder.\n\nPlease refer to https://arxiv.org/abs/1912.04260 for more details.\nParameters\n* cls_in_channels (int) — Input channels of cls Rol feature. Defaults to 256.\n* reg_in_channels (int) — Input channels of reg Rol feature. Defaults to 256.\n* roi_feat_size (int) — Size of Rol features. Defaults to 7.\n* reg_feat_up_ratio (int) — Upsample ratio of reg features. Defaults to 2.\n\n* reg_pre_kernel (int) — Kernel of 2D conv layers before attention pooling. Defaults to 3.\n\nreg_post_kernel (int) — Kernel of 1D conv layers after attention pooling. Defaults to 3.\n\nreg_pre_num (int) — Number of pre convs. Defaults to 2.\n\nreg_post_num (int) — Number of post convs. Defaults to 1.\n* num_classes (int) — Number of classes in dataset. Defaults to 80.\n¢ cls_out_channels (int) — Hidden channels in cls fcs. Defaults to 1024.\n\n* reg_offset_out_channels (int) — Hidden and output channel of reg offset branch. De-\nfaults to 256.\n\n* reg_cls_out_channels (int) — Hidden and output channel of reg cls branch. Defaults to\n256.\n\n* num_cls_fcs (int) — Number of fcs for cls branch. Defaults to 1.\n\n* num_reg_fcs (int) — Number of fcs for reg branch.. Defaults to 0.\n\n* reg_class_agnostic (bool) — Class agnostic regression or not. Defaults to True.\n* norm_cfg (dict) — Config of norm layers. Defaults to None.\n\n* bbox_coder (dict) — Config of bbox coder. Defaults ‘BucketingBBoxCoder’.\n\n* loss_cls (dict) — Config of classification loss.\n\n* loss_bbox_cls (dict) — Config of classification loss for bbox branch.\n\n* loss_bbox_reg (dict) — Config of regression loss for bbox branch.\n\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\n404 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.roi_heads. SABLHead ( num classes ,  cls in channels=256 ,  reg in channel  $s{=}256$  , roi feat size=7 ,  reg feat up ratio=2 ,  reg pre kernel=3 , reg post kernel  $!\\!=\\!\\!3$  ,  reg pre num=2 ,  reg post num  $\\scriptstyle{=I}$  , cls out channels  $:=$  1024 ,  reg offset out channels=256 , reg cls out channels=256 ,  num cls fcs=1 ,  num reg fcs=0 , reg class agnostic=True ,  norm_cfg  $=$  None , bbox_coder={'num buckets': 14, 'scale factor': 1.7, 'type': 'Bucketing B Box Code r'} ,  loss_cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': False} , loss b box cls={'loss weight': 1.0, 'type': 'Cross Entropy Loss', 'use s igm oid': True}, loss b box reg $=$ {'beta': 0.1, 'loss weight':1.0, 'type': 'Smooth L 1 Loss'} ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) \nSide-Aware Boundary Localization (SABL) for RoI-Head. \nSide-Aware features are extracted by conv layers with an attention mechanism. Boundary Localization with Bucketing and Bucketing Guided Rescoring are implemented in Bucketing B Box Code r. \nPlease refer to  https://arxiv.org/abs/1912.04260  for more details. \nParameters \n•  cls in channels  ( int ) – Input channels of cls RoI feature. Defaults to 256. •  reg in channels  ( int ) – Input channels of reg RoI feature. Defaults to 256. •  roi feat size  ( int ) – Size of RoI features. Defaults to 7. •  reg feat up ratio  ( int ) – Upsample ratio of reg features. Defaults to 2. •  reg pre kernel  ( int ) – Kernel of 2D conv layers before attention pooling. Defaults to 3. •  reg post kernel  ( int ) – Kernel of 1D conv layers after attention pooling. Defaults to 3. •  reg pre num  ( int ) – Number of pre convs. Defaults to 2. •  reg post num  ( int ) – Number of post convs. Defaults to 1. •  num classes  ( int ) – Number of classes in dataset. Defaults to 80. •  cls out channels  ( int ) – Hidden channels in cls fcs. Defaults to 1024. •  reg offset out channels  ( int ) – Hidden and output channel of reg offset branch. De- faults to 256. •  reg cls out channels  ( int ) – Hidden and output channel of reg cls branch. Defaults to 256. •  num cls fcs  ( int ) – Number of fcs for cls branch. Defaults to 1. •  num reg fcs  ( int ) – Number of fcs for reg branch.. Defaults to 0. •  reg class agnostic  ( bool ) – Class agnostic regression or not. Defaults to True. •  norm_cfg  ( dict ) – Config of norm layers. Defaults to None. •  bbox_coder  ( dict ) – Config of bbox coder. Defaults ‘Bucketing B Box Code r’. •  loss_cls  ( dict ) – Config of classification loss. •  loss b box cls  ( dict ) – Config of classification loss for bbox branch. •  loss b box reg  ( dict ) – Config of regression loss for bbox branch. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None "}
{"page": 412, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_412.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nattention_pool (reg_x)\nExtract direction-specific features fx and fy with attention methanism.\n\nbbox_pred_split (bbox_pred, num_proposals_per_img)\nSplit batch bbox prediction back to each image.\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nrefine_bboxes (rois, labels, bbox_preds, pos_is_gts, img_metas)\nRefine bboxes during training.\n\nParameters\n\n* rois (Tensor) — Shape (n*bs, 5), where n is image number per GPU, and bs is the sampled\nRols per image.\n\n* labels (Tensor) — Shape (n*bs, ).\n\n¢ bbox_preds (list[Tensor]) -— Shape [(n*bs, num_buckets*2), (n*bs,\nnum_buckets*2)].\n\n* pos_is_gts (list [Tensor]) — Flags indicating if each positive bbox is a gt bbox.\n¢ img_metas (list [dict ]) — Meta info of each image.\n\nReturns Refined bboxes of each image in a mini-batch.\n\nReturn type list[Tensor]\n\nreg_pred(x, offset_fcs, cls_fcs)\nPredict bucketing estimation (cls_pred) and fine regression (offset pred) with side-aware features.\n\nregress_by_class(vois, label, bbox_pred, img_meta)\nRegress the bbox for the predicted class. Used in Cascade R-CNN.\n\nParameters\n* rois (Tensor) — shape (n, 4) or (n, 5)\n¢ label (Tensor) — shape (n, )\n¢ bbox_pred (list [Tensor ]) — shape [(n, num_buckets *2), (n, num_buckets *2)]\n¢ img_meta (dict) — Image meta info.\nReturns Regressed bboxes, the same shape as input rois.\nReturn type Tensor\n\nside_aware_feature_extractor(reg_x)\nRefine and extract side-aware features without split them.\n\nside_aware_split (feat)\nSplit side-aware features aligned with orders of bucketing targets.\n\n39.5. roi_heads 405\n", "vlm_text": "attention pool ( reg_x ) Extract direction-specific features fx and fy with attention methanism. \nb box p red split ( bbox_pred ,  num proposals per img ) Split batch bbox prediction back to each image. \nforward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nrefine b boxes ( rois ,  labels ,  bbox_preds ,  pos_is_gts ,  img_metas ) Refine bboxes during training. \nParameters •  rois  ( Tensor ) – Shape (n\\*bs, 5), where n is image number per GPU, and bs is the sampled RoIs per image. •  labels  ( Tensor ) – Shape (n\\*bs, ). •  bbox_preds ( list[Tensor] ) – Shape [(n\\*bs, num buckets\\*2), (n\\*bs, num buckets\\*2)]. •  pos_is_gts  ( list[Tensor] ) – Flags indicating if each positive bbox is a gt bbox. •  img_metas  ( list[dict] ) – Meta info of each image. Returns  Refined bboxes of each image in a mini-batch. Return type  list[Tensor] \nreg_pred ( x ,  offset_fcs ,  cls_fcs ) Predict bucketing estimation (cls_pred) and fine regression (offset pred) with side-aware features. \nregress by class ( rois ,  label ,  bbox_pred ,  img_meta ) Regress the bbox for the predicted class. Used in Cascade R-CNN. Parameters •  rois  ( Tensor ) – shape (n, 4) or (n, 5) •  label  ( Tensor ) – shape (n, ) •  bbox_pred  ( list[Tensor] ) – shape [(n, num buckets  $^{*}2$  ), (n, num buckets    $^{*}2$  )] •  img_meta  ( dict ) – Image meta info. Returns  Regressed bboxes, the same shape as input rois. Return type  Tensor \nside aware feature extractor ( reg_x ) Refine and extract side-aware features without split them. \nside aware split ( feat ) Split side-aware features aligned with orders of bucketing targets. "}
{"page": 413, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_413.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.roi_heads.SCNetBBoxHead (num_shared_convs=0, num_shared_fcs=0,\nnum_cls_convs=0, num_cls_fcs=0, num_reg_convs=0,\nnum_reg_fcs=0, conv_out_channels=256,\nfc_out_channels=1024, conv_cfg=None, norm_cfg=None,\ninit_cfg=None, *args, **kwargs)\nBBox head for SCNet.\n\nThis inherits ConvFCBBoxHead with modified forward() function, allow us to get intermediate shared feature.\n\nforward (x, return_shared_feat=False)\nForward function.\n\nParameters\n\n¢ x (Tensor) — input features\n\n* return_shared_feat (boo1) — If True, return cls-reg-shared feature.\nReturns\n\ncontain cls_score and bbox_pred, if return_shared_feat is True, append x_shared\nto the returned tuple.\n\nReturn type out (tuple[Tensor])\n\nclass mmdet.models.roi_heads.SCNetMaskHead (conv_to_res=True, **kwargs)\n\nMask head for SCNet.\nParameters conv_to_res (bool, optional) - if True, change the conv layers to\nSimplifiedBasicBlock.\n\nclass mmdet.models.roi_heads.SCNetRoIHead (num_stages, stage_loss_weights,\nsemantic_roi_extractor=None, semantic_head=None,\n\nfeat_relay_head=None, glbctx_head=None, **kwargs)\nRolHead for SCNet.\n\nParameters\n* num_stages (int) — number of cascade stages.\n* stage_loss_weights (list) — loss weight of cascade stages.\n* semantic_roi_extractor (dict) — config to init semantic roi extractor.\n* semantic_head (dict) — config to init semantic head.\n¢ feat_relay_head (dict) — config to init feature_relay_head.\n* glbctx_head (dict) — config to init global context head.\n\naug_test (img_feats, proposal_list, img_metas, rescale=False)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nforward_train(x, img_metas, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore=None,\ngt_masks=None, gt_semantic_seg=None)\n\nParameters\n¢ x (list [Tensor]) — list of multi-level img features.\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n\n406 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.roi_heads. S CNet B Box Head ( num shared con vs  $\\mathrel{\\mathop:}=\\!\\!O$  ,  num shared fc  $\\cdot_{s=0}$  , num cls con vs  $\\scriptstyle{\\prime=0}$  ,  num_cls_fc  $s{=}0.$  ,  num reg con vs  $\\imath\\!=\\!\\!O$  , num_reg_fc  $\\cdot_{S=0}$  ,  con v out channel  $\\mathfrak{s}{=}256$  , fc out channels  $=$  1024 ,  conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $=$  None , init_cfg  $\\mathbf{\\hat{\\mu}}$  None ,  \\*args ,  \\*\\*kwargs ) \nBBox head for  SCNet . \nThis inherits  Con v FCB Box Head  with modified forward() function, allow us to get intermediate shared feature. \nforward (  $\\dot{x}_{i}$  ,  return shared feat  $\\fallingdotseq$  False ) Forward function. \nParameters \n•  x  ( Tensor ) – input features •  return shared feat  ( bool ) – If True, return cls-reg-shared feature. \nReturns \ncontain  cls_score  and  bbox_pred ,  if  return shared feat  is True, append  x_shared to the returned tuple. Return type  out (tuple[Tensor]) \nclass  mmdet.models.roi_heads. S CNet Mask Head ( con v to res  $=$  True ,  \\*\\*kwargs ) Mask head for  SCNet . \nParameters  con v to res ( bool, optional ) – if True, change the conv layers to Simplified Basic Block . \nclass  mmdet.models.roi_heads. S CNet RoI Head ( num_stages ,  stage loss weights , semantic roi extractor=None ,  semantic head  $\\leftrightharpoons$  None , feat relay head  $\\leftrightharpoons$  None ,  gl bc tx head=None ,  \\*\\*kwargs ) \nRoIHead for  SCNet . \nParameters \n•  num_stages  ( int ) – number of cascade stages. •  stage loss weights  ( list ) – loss weight of cascade stages. •  semantic roi extractor  ( dict ) – config to init semantic roi extractor. •  semantic head  ( dict ) – config to init semantic head. •  feat relay head  ( dict ) – config to init feature relay head. •  gl bc tx head  ( dict ) – config to init global context head. \naug_test ( img_feats ,  proposal list ,  img_metas ,  rescale  $\\mathbf{\\beta}=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nforward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\varepsilon}=$  None , gt_mask  $\\mathfrak{s}{=}$  None ,  gt semantic seg  $=$  None ) \nParameters \n•  x  ( list[Tensor] ) – list of multi-level img features. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, "}
{"page": 414, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_414.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* proposal_list (list [Tensors]) — list of region proposals.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None, list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n* gt_masks (None, Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\n* gt_semantic_seg (None, list [Tensor]) — semantic segmentation masks used if the\narchitecture supports semantic segmentation task.\n\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\ninit_mask_head (mask_roi_extractor, mask_head)\nInitialize mask_head\n\nsimple_test (x, proposal_list, img_metas, rescale=False)\nTest without augmentation.\n\nParameters\n\n¢ x(tuple[Tensor]) — Features from upstream network. Each has shape (batch_size, c, h,\nw).\n\n* proposal_list (list(Tensor)) — Proposals from rpn head. Each has shape\n(num_proposals, 5), last dimension 5 represent (x1, yl, x2, y2, score).\n\n¢ img_metas (list [dict ]) — Meta information of images.\n* rescale (bool) — Whether to rescale the results to the original image. Default: True.\n\nReturns When no mask branch, it is bbox results of each image and classes with type\nlist[list{np.ndarray]]. The outer list corresponds to each image. The inner list corresponds\nto each class. When the model has mask branch, it contains bbox results and mask results.\nThe outer list corresponds to each image, and first element of tuple is bbox results, second\nelement is mask results.\n\nReturn type list[list{np.ndarray]] or list[tuple]\n\nproperty with_feat_relay\nwhether the head has feature relay head\n\nType bool\n\nproperty with_glbctx\nwhether the head has global context head\n\nType bool\n\nproperty with_semantic\nwhether the head has semantic head\n\nType bool\n\nclass mmdet.models.roi_heads.SCNetSemanticHead(conv_to_res=True, **kwargs)\nMask head for SCNet.\n\n39.5. roi_heads 407\n", "vlm_text": "‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . \n•  proposal list  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None, list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None, Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. •  gt semantic seg  ( None, list[Tensor] ) – semantic segmentation masks used if the architecture supports semantic segmentation task. \nReturns  a dictionary of loss components \nReturn type  dict[str, Tensor] \nin it mask head ( mask roi extractor ,  mask_head ) Initialize  mask_head \nsimple test ( x ,  proposal list ,  img_metas ,  rescale=False ) Test without augmentation. \nParameters \n•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). •  proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). •  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. \nReturns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. \nReturn type  list[list[np.ndarray]] or list[tuple] \nproperty with feat relay whether the head has feature relay head \nproperty with gl bc tx whether the head has global context head \nType  bool \nproperty with semantic whether the head has semantic head \nType  bool \nclass  mmdet.models.roi_heads. S CNet Semantic Head ( con v to res  $\\mathbf{=}$  True ,  \\*\\*kwargs ) Mask head for  SCNet . "}
{"page": 415, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_415.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters conv_to_res (bool, optional) - if True, change\nSimplifiedBasicBlock.\n\nthe conv layers to\nclass mmdet.models.roi_heads .Shared2FCBBoxHead (fc_out_channels=1024, *args, **kwargs)\nclass mmdet.models.roi_heads.Shared4Conv1FCBBoxHead (fc_out_channels=1024, *args, **kwargs)\n\nclass mmdet.models.roi_heads.SingleRoIExtractor (roi_layer, out_channels, featmap_strides,\n\nfinest_scale=56, init_cfg=None)\nExtract Rol features from a single level feature map.\n\nIf there are multiple input feature levels, each Rol is mapped to a level according to its scale. The mapping rule\nis proposed in FPN.\n\nParameters\n\n* roi_layer (dict) — Specify Rol layer type and arguments.\n\n* out_channels (int) — Output channels of Rol layers.\n\n¢ featmap_strides (List [int ]) — Strides of input feature maps.\n\n¢ finest_scale (int) — Scale threshold of mapping to level 0. Default: 56.\n\n¢ init_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\nforward (feats, rois, roi_scale_factor=None)\nForward function.\n\nmap_roi_levels(rois, num_levels)\nMap rois to corresponding feature levels by scales.\n\n¢ scale < finest_scale * 2: level 0\n¢ finest_scale * 2 <= scale < finest_scale * 4: level 1\n\n¢ finest_scale * 4 <= scale < finest_scale * 8: level 2\n\n* scale >= finest_scale * 8: level 3\n\nParameters\n\n* rois (Tensor) — Input Rols, shape (k, 5).\n\n¢ num_levels (int) — Total level number.\nReturns Level index (0-based) of each Rol, shape (k, )\n\nReturn type Tensor\n\nclass mmdet.models.roi_heads.SparseRoIHead (num_stages=6, stage_loss_weights=(1, 1, 1, 1, 1, 1),\n\nproposal_feature_channel=256,\nbbox_roi_extractor={ ‘featmap_strides': [4, 8, 16, 32],\n‘out_channels': 256, 'roi_layer': {'output_size': 7,\n‘sampling_ratio': 2, 'type': 'RoIAlign'}, ‘type’:\n‘SingleRolExtractor'}, mask_roi_extractor=None,\nbbox_head={‘dropout': 0.0, ‘feedforward_channels': 2048,\nffn_act_cfg': {‘inplace': True, ‘type’: 'ReLU'},\n‘hidden_channels': 256, 'num_classes': 80, '‘num_cls_fcs': 1,\n‘num_fcs': 2, ‘num_heads': 8, ‘num_reg_fes': 3,\n‘roi_feat_size': 7, 'type': 'DIIHead'}, mask_head=None,\ntrain_cfg=None, test_cfg=None, pretrained=None,\ninit_cfg=None)\n\nThe RolHead for Sparse R-CNN: End-to-End Object Detection with Learnable Proposals and Instances as\n\n408 Chapter 39. mmdet.models\n", "vlm_text": "Parameters  con v to res ( bool, optional ) – if True, change the conv layers to Simplified Basic Block . \nclass  mmdet.models.roi_heads. Shared 2 FCB Box Head ( fc out channels  $=$  1024 ,  \\*args ,  \\*\\*kwargs ) \nclass  mmdet.models.roi_heads. Shared 4 Con v 1 FCB Box Head ( fc out channel  $\\mathbf{:=}_{1}$  1024 ,  \\*args ,  \\*\\*kwargs ) \nclass  mmdet.models.roi_heads. Single RoI Extractor ( roi_layer ,  out channels ,  feat map strides , finest scale=56 ,  init_cfg  $\\mathbf{\\beta}=$  None ) \nExtract RoI features from a single level feature map. \nIf there are multiple input feature levels, each RoI is mapped to a level according to its scale. The mapping rule is proposed in  FPN . \nParameters \n•  roi_layer  ( dict ) – Specify RoI layer type and arguments. •  out channels  ( int ) – Output channels of RoI layers. •  feat map strides  ( List[int] ) – Strides of input feature maps. •  finest scale  ( int ) – Scale threshold of mapping to level 0. Default: 56. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward ( feats ,  rois ,  roi scale factor  $\\leftrightharpoons$  None ) Forward function. \nmap roi levels ( rois ,  num_levels ) Map rois to corresponding feature levels by scales. • scale  $<$   finest scale  $^{*}\\,2$  : level 0 • finest scale  $^*\\;2<=$  scale  $<$   finest scale \\* 4: level 1 • finest scale  $^*4<=$  scale  $<$   finest scale  $^\\ast\\,8$  : level 2 • scale  $>=$   finest scale  $^{*}\\mathrm{~}8$  : level 3 \nParameters \n•  rois  ( Tensor ) – Input RoIs, shape (k, 5). •  num_levels  ( int ) – Total level number. Returns  Level index (0-based) of each RoI, shape (k, ) Return type  Tensor \nclass  mmdet.models.roi_heads. Sparse RoI Head ( num_stages  $\\scriptstyle{\\prime=6}$  ,  stage_loss_weights=(1, 1, 1, 1, 1, 1) , \nproposal feature ch anne  $l{=}256$  , b box roi extractor={'feat map strides': [4, 8, 16, 32], 'out channels': 256, 'roi_layer': {'output size': 7, 'sampling ratio': 2, 'type': 'RoIAlign'}, 'type': 'Single RoI Extractor'} ,  mask roi extractor=None , bbox_head={'dropout': 0.0, 'feed forward channels': 2048, 'ff n act cf g': {'inplace': True, 'type': 'ReLU'}, 'hidden channels': 256, 'num classes': 80, 'num cls fcs': 1, 'num_fcs': 2, 'num_heads': 8, 'num_reg_fcs': 3, 'roi feat size': 7, 'type': 'DIIHead'} ,  mask_head  $\\leftrightharpoons$  None , train_cfg  $=$  None ,  test_cfg  $=$  None ,  pretrained  $\\fallingdotseq$  None , init_cfg=None ) \nThe RoIHead for  Sparse R-CNN: End-to-End Object Detection with Learnable Proposals  and  Instances as Queries "}
{"page": 416, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_416.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nQueries\n\nParameters\n\nnum_stages (int) — Number of stage whole iterative process. Defaults to 6.\n\nstage_loss_weights (Tuple[float]) — The loss weight of each stage. By default all\nstages have the same weight 1.\n\nbbox_roi_extractor (dict) — Config of box roi extractor.\nmask_roi_extractor (dict) — Config of mask roi extractor.\nbbox_head (dict) — Config of box head.\n\nmask_head (dict) — Config of mask head.\n\ntrain_cfg (dict, optional) — Configuration information in train stage. Defaults to\nNone.\n\ntest_cfg (dict, optional) — Configuration information in test stage. Defaults to None.\npretrained (str, optional) — model pretrained path. Default: None\n\ninit_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\naug_test (features, proposal_list, img_metas, rescale=False)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nforward_dummy (x, proposal_boxes, proposal_features, img_metas)\nDummy forward function when do the flops computing.\n\nforward_train(x, proposal_boxes, proposal_features, img_metas, gt_bboxes, gt_labels,\n\ngt_bboxes_ignore=None, imgs_whwh=None, gt_masks=None)\n\nForward function in training stage.\n\nParameters\n\n¢ x (list [Tensor]) — list of multi-level img features.\n\n* proposals (Tensor) — Decoded proposal bboxes, has shape (batch_size, num_proposals,\n4)\n\n* proposal_features (Tensor) — Expanded proposal features, has shape (batch_size,\n\nnum_proposals, proposal_feature_channel)\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n¢ imgs_whwh (Tensor) — Tensor with shape (batch_size, 4), the dimension means\n[img_width,img_height, img_width, img_height)].\n\n* gt_masks (None | Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\n39.5. roi_heads\n\n409\n", "vlm_text": "\nParameters \n•  num_stages  ( int ) – Number of stage whole iterative process. Defaults to 6. •  stage loss weights  ( Tuple[float] ) – The loss weight of each stage. By default all stages have the same weight 1. •  b box roi extractor  ( dict ) – Config of box roi extractor. •  mask roi extractor  ( dict ) – Config of mask roi extractor. •  bbox_head  ( dict ) – Config of box head. •  mask_head  ( dict ) – Config of mask head. •  train_cfg  ( dict, optional ) – Configuration information in train stage. Defaults to None. •  test_cfg  ( dict, optional ) – Configuration information in test stage. Defaults to None. •  pretrained  ( str, optional ) – model pretrained path. Default: None •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \naug_test ( features ,  proposal list ,  img_metas ,  rescale  $\\mathbf{=}$  False ) Test with augmentations. \nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nforward dummy ( x ,  proposal boxes ,  proposal features ,  img_metas ) Dummy forward function when do the flops computing. \nforward train ( x ,  proposal boxes ,  proposal features ,  img_metas ,  gt_bboxes ,  gt_labels , gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None ,  imgs_whwh  $=$  None ,  gt_mask  $\\leftrightharpoons$  None ) Forward function in training stage. \nParameters \n•  x  ( list[Tensor] ) – list of multi-level img features. •  proposals  ( Tensor ) – Decoded proposal bboxes, has shape (batch_size, num proposals, 4) •  proposal features  ( Tensor ) – Expanded proposal features, has shape (batch_size, num proposals, proposal feature channel) •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  imgs_whwh  ( Tensor ) – Tensor with shape (batch_size, 4), the dimension means [img_width,img_height, img_width, img_height]. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. "}
{"page": 417, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_417.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturns a dictionary of loss components of all stage.\nReturn type dict[str, Tensor]\n\nsimple_test (x, proposal_boxes, proposal_features, img_metas, imgs_whwh, rescale=False)\nTest without augmentation.\n\nParameters\n¢ x (list [Tensor]) — list of multi-level img features.\n\n* proposal_boxes (Tensor) — Decoded proposal bboxes, has shape (batch_size,\nnum_proposals, 4)\n\n* proposal_features (Tensor) — Expanded proposal features, has shape (batch_size,\nnum_proposals, proposal_feature_channel)\n\n¢ img_metas (dict) — meta information of images.\n\n¢ imgs_whwh (Tensor) — Tensor with shape (batch_size, 4), the dimension means\n[img_width,img_height, img_width, img_height)].\n\n* rescale (bool) — If True, return boxes in original image space. Defaults to False.\n\nReturns When no mask branch, it is bbox results of each image and classes with type\nlist[list{np.ndarray]]. The outer list corresponds to each image. The inner list corresponds\nto each class. When the model has a mask branch, it is a list{tuple] that contains bbox results\nand mask results. The outer list corresponds to each image, and first element of tuple is bbox\nresults, second element is mask results.\n\nReturn type list[list{np.ndarray]] or list[tuple]\n\nclass mmdet.models.roi_heads.StandardRoIHead (bbox_roi_extractor=None, bbox_head=None,\nmask_roi_extractor=None, mask_head=None,\nshared_head=None, train_cfg=None, test_cfg=None,\npretrained=None, init_cfg=None)\nSimplest base roi head including one bbox head and one mask head.\n\nasync async_simple_test (x, proposal_list, img_metas, proposals=None, rescale=False)\nAsync test without augmentation.\n\naug_test (x, proposal_list, img_metas, rescale=False)\nTest with augmentations.\n\nIf rescale is False, then returned bboxes and masks will fit the scale of imgs[0].\n\nbbox_onnx_export (x, img_metas, proposals, rcnn_test_cfg, **kwargs)\nExport bbox branch to onnx which supports batch inference.\n\nParameters\n¢ x (tuple[Tensor ]) — Feature maps of all scale level.\n¢ img_metas (list [dict ]) — Image meta info.\n\n* proposals (Tensor) — Region proposals with batch dimension, has shape [N,\nnum_bboxes, 5].\n\n* (obj (rcnn_test_cfg) — ConfigDict): test_cfg of R-CNN.\nReturns\nbboxes of shape [N, num_bboxes, 5] and class labels of shape [N, num_bboxes].\n\nReturn type tuple[Tensor, Tensor]\n\n410 Chapter 39. mmdet.models\n", "vlm_text": "Returns  a dictionary of loss components of all stage. \nReturn type  dict[str, Tensor] \nsimple test (  $\\dot{\\boldsymbol{x}}.$  ,  proposal boxes ,  proposal features ,  img_metas ,  imgs_whwh ,  rescale=False ) Test without augmentation. \nParameters \n•  x  ( list[Tensor] ) – list of multi-level img features. •  proposal boxes  ( Tensor ) – Decoded proposal bboxes, has shape (batch_size, num proposals, 4) •  proposal features  ( Tensor ) – Expanded proposal features, has shape (batch_size, num proposals, proposal feature channel) •  img_metas  ( dict ) – meta information of images. •  imgs_whwh  ( Tensor ) – Tensor with shape (batch_size, 4), the dimension means [img_width,img_height, img_width, img_height]. •  rescale  ( bool ) – If True, return boxes in original image space. Defaults to False. \nReturns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has a mask branch, it is a list[tuple] that contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. \nReturn type  list[list[np.ndarray]] or list[tuple] \nclass  mmdet.models.roi_heads. Standard RoI Head ( b box roi extractor  $\\leftrightharpoons$  None ,  bbox_head  $\\leftrightharpoons$  None , mask roi extractor  $=$  None ,  mask_head  $\\leftrightharpoons$  None , shared head  $\\leftrightharpoons$  None ,  train_cfg  $\\mathbf{\\beta}=$  None ,  test_cfg=None , pretrained  $\\leftrightharpoons$  None ,  init_cfg  $=$  None ) \nSimplest base roi head including one bbox head and one mask head. \nasync a sync simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\leftrightharpoons$  None ,  rescale=False ) Async test without augmentation. \naug_test (  $\\dot{\\boldsymbol{x}}.$  ,  proposal list ,  img_metas ,  rescale  $=$  False ) Test with augmentations. If rescale is False, then returned bboxes and masks will fit the scale of imgs[0]. \nb box on nx export (  $\\cdot_{x}$  ,  img_metas ,  proposals ,  r cnn test cf g ,  \\*\\*kwargs ) Export bbox branch to onnx which supports batch inference. \nParameters \n•  x  ( tuple[Tensor] ) – Feature maps of all scale level. •  img_metas  ( list[dict] ) – Image meta info. •  proposals  ( Tensor ) – Region proposals with batch dimension, has shape [N, num_bboxes, 5]. •  (obj  ( r cnn test cf g ) –  ConfigDict ):  test_cfg  of R-CNN. \nReturns \nbboxes of shape [N, num_bboxes, 5]  and class labels of shape [N, num_bboxes]. Return type  tuple[Tensor, Tensor] "}
{"page": 418, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_418.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward_dummy (x, proposals)\nDummy forward function.\n\nforward_train(x, img_metas, proposal_list, gt_bboxes, gt_labels, gt_bboxes_ignore=None,\ngt_masks=None, **kwargs)\n\nParameters\n¢ x (list [Tensor]) — list of multi-level img features.\n\n¢ img_metas (list[dict]) -— list of image info dict where each dict has:\n‘img_shape’, ‘scale_factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’,\n‘pad_shape’, and ‘img_norm_cfg’. For details on the values of these keys see\nmmdet/datasets/pipelines/formatting.py: Collect.\n\n* proposals (list [Tensors]) — list of region proposals.\n\n* gt_bboxes (list [Tensor ]) — Ground truth bboxes for each image with shape (num_gts,\n4) in [tl_x, tl_y, br_x, br_y] format.\n\n¢ gt_labels (list [Tensor ]) — class indices corresponding to each box\n\n* gt_bboxes_ignore (None | list[Tensor]) — specify which bounding boxes can be\nignored when computing the loss.\n\n* gt_masks (None | Tensor) — true segmentation masks for each box used if the architec-\nture supports a segmentation task.\n\nReturns a dictionary of loss components\nReturn type dict[str, Tensor]\n\ninit_assigner_sampler()\nInitialize assigner and sampler.\n\ninit_bbox_head (bbox_roi_extractor, bbox_head)\nInitialize bbox_head\n\ninit_mask_head (mask_roi_extractor, mask_head)\nInitialize mask_head\n\nmask_onnx_export (x, img_metas, det_bboxes, det_labels, **kwargs)\nExport mask branch to onnx which supports batch inference.\n\nParameters\n¢ x (tuple[Tensor ]) — Feature maps of all scale level.\n¢ img_metas (list [dict ]) — Image meta info.\n\n¢ det_bboxes (Tensor) — Bboxes and corresponding scores. has shape [N, num_bboxes,\n5].\n\n¢ det_labels (Tensor) — class labels of shape [N, num_bboxes].\nReturns\n\nThe segmentation results of shape [N, num_bboxes, image_height, image_width].\nReturn type Tensor\n\nonnx_export (x, proposals, img_metas, rescale=False)\nTest without augmentation.\n\n39.5. roi_heads 411\n", "vlm_text": "forward dummy ( x ,  proposals ) Dummy forward function. \nforward train ( x ,  img_metas ,  proposal list ,  gt_bboxes ,  gt_labels ,  gt b boxes ignore  $\\mathbf{\\Pi}^{*}=$  None , gt_mask  $\\mathfrak{s}{=}$  None ,  \\*\\*kwargs ) \nParameters \n•  x  ( list[Tensor] ) – list of multi-level img features. •  img_metas ( list[dict] ) – list of image info dict where each dict has: ‘img_shape’, ‘scale factor’, ‘flip’, and may also contain ‘filename’, ‘ori_shape’, ‘pad_shape’, and ‘img norm cf g’. For details on the values of these keys see mmdet/datasets/pipelines/formatting.py:Collect . •  proposals  ( list[Tensors] ) – list of region proposals. •  gt_bboxes  ( list[Tensor] ) – Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format. •  gt_labels  ( list[Tensor] ) – class indices corresponding to each box •  gt b boxes ignore  ( None | list[Tensor] ) – specify which bounding boxes can be ignored when computing the loss. •  gt_masks  ( None | Tensor ) – true segmentation masks for each box used if the architec- ture supports a segmentation task. Returns  a dictionary of loss components Return type  dict[str, Tensor] \nin it as signer sampler () Initialize assigner and sampler. \nin it b box head ( b box roi extractor ,  bbox_head ) Initialize  bbox_head \nin it mask head ( mask roi extractor ,  mask_head ) Initialize  mask_head \nmask on nx export ( x ,  img_metas ,  det_bboxes ,  det_labels ,  \\*\\*kwargs ) Export mask branch to onnx which supports batch inference. \nParameters \n•  x  ( tuple[Tensor] ) – Feature maps of all scale level. •  img_metas  ( list[dict] ) – Image meta info. •  det_bboxes  ( Tensor ) – Bboxes and corresponding scores. has shape [N, num_bboxes, 5]. •  det_labels  ( Tensor ) – class labels of shape [N, num_bboxes]. \nReturns \non nx export (  $\\cdot x,$  ,  proposals ,  img_metas ,  rescale  $\\mathbf{\\dot{\\rho}}=$  False ) Test without augmentation. "}
{"page": 419, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_419.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nsimple_test (x, proposal_list, img_metas, proposals=None, rescale=False)\nTest without augmentation.\n\nParameters\n\n¢ x(tuple[Tensor]) — Features from upstream network. Each has shape (batch_size, c, h,\nw).\n\n* proposal_list (list(Tensor)) — Proposals from rpn head. Each has shape\n(num_proposals, 5), last dimension 5 represent (x1, yl, x2, y2, score).\n\n¢ img_metas (list [dict ]) — Meta information of images.\n* rescale (bool) — Whether to rescale the results to the original image. Default: True.\n\nReturns When no mask branch, it is bbox results of each image and classes with type\nlist[list{np.ndarray]]. The outer list corresponds to each image. The inner list corresponds\nto each class. When the model has mask branch, it contains bbox results and mask results.\nThe outer list corresponds to each image, and first element of tuple is bbox results, second\nelement is mask results.\n\nReturn type list[list{np.ndarray]] or list[tuple]\n\nclass mmdet.models.roi_heads.TridentRoIHead (num_branch, test_branch_idx, **kwargs)\nTrident roi head.\n\nParameters\n¢ num_branch (int) — Number of branches in TridentNet.\n\n* test_branch_idx (int) — In inference, all 3 branches will be used if test_branch_idx==-1,\notherwise only branch with index test_branch_idx will be used.\n\naug_test_bboxes (feats, img_metas, proposal_list, rcnn_test_cfg)\nTest det bboxes with test time augmentation.\n\nmerge_trident_bboxes (trident_det_bboxes, trident_det_labels)\nMerge bbox predictions of each branch.\n\nsimple_test (x, proposal_list, img_metas, proposals=None, rescale=False)\nTest without augmentation as follows:\n\n1. Compute prediction bbox and label per branch.\n\n2. Merge predictions of each branch according to scores of bboxes, i.e., bboxes with higher score are kept\nto give top-k prediction.\n\n39.6 losses\n\nclass mmdet.models.losses.Accuracy (topk=(1), thresh=None)\n\nforward (pred, target)\nForward function to calculate accuracy.\n\nParameters\n¢ pred (torch. Tensor) — Prediction of models.\n* target (torch. Tensor) — Target for each prediction.\n\nReturns The accuracies under different topk criterions.\n\n412 Chapter 39. mmdet.models\n", "vlm_text": "simple test ( x ,  proposal list ,  img_metas ,  proposals  $\\mathbf{\\check{\\Sigma}}$  None ,  rescale  $\\mathbf{=}$  False ) Test without augmentation. \nParameters \n•  x  ( tuple[Tensor] ) – Features from upstream network. Each has shape (batch_size, c, h, w). •  proposal list  ( list(Tensor) ) – Proposals from rpn head. Each has shape (num proposals, 5), last dimension 5 represent (x1, y1, x2, y2, score). •  img_metas  ( list[dict] ) – Meta information of images. •  rescale  ( bool ) – Whether to rescale the results to the original image. Default: True. \nReturns  When no mask branch, it is bbox results of each image and classes with type list[list[np.ndarray]] . The outer list corresponds to each image. The inner list corresponds to each class. When the model has mask branch, it contains bbox results and mask results. The outer list corresponds to each image, and first element of tuple is bbox results, second element is mask results. \n\nclass  mmdet.models.roi_heads. Trident RoI Head ( num_branch ,  test branch i dx ,  \\*\\*kwargs ) Trident roi head. \nParameters \n•  num_branch  ( int ) – Number of branches in TridentNet. •  test branch i dx  ( int ) – In inference, all 3 branches will be used if  test branch i dx  $==$  -1 , otherwise only branch with index  test branch i dx  will be used. \naug test b boxes ( feats ,  img_metas ,  proposal list ,  r cnn test cf g ) Test det bboxes with test time augmentation. \nmerge trident b boxes ( trident det b boxes ,  trident det labels ) Merge bbox predictions of each branch. \nsimple test ( x ,  proposal list ,  img_metas ,  proposals  $\\mathbf{\\check{\\Sigma}}$  None ,  rescale=False ) Test without augmentation as follows: \n1. Compute prediction bbox and label per branch. 2. Merge predictions of each branch according to scores of bboxes, i.e., bboxes with higher score are kept to give top-k prediction. \n39.6 losses \nclass  mmdet.models.losses. Accuracy ( topk=(1) ,  thresh  $=$  None ) \nforward ( pred ,  target ) Forward function to calculate accuracy. \nParameters \n•  pred  ( torch.Tensor ) – Prediction of models. •  target  ( torch.Tensor ) – Target for each prediction. \nReturns  The accuracies under different topk criterions. "}
{"page": 420, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_420.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nReturn type tuple[float]\n\nclass mmdet.models.losses.AssociativeEmbeddingLoss (pull_weight=0.25, push_weight=0.25)\nAssociative Embedding Loss.\n\nMore details can be found in Associative Embedding and CornerNet . Code is modified from kp_utils.py # noqa:\nE501\n\nParameters\n* pull_weight (float) — Loss weight for corners from same object.\n* push_weight (float) — Loss weight for corners from different object.\n\nforward (pred, target, match)\nForward function.\n\nclass mmdet.models.losses.BalancedL1Loss (alpha=0.5, gamma=1.5, beta=1.0, reduction='mean',\nloss_weight=1.0)\nBalanced L1 Loss.\n\narXiv: https://arxiv.org/pdf/1904.02701.pdf (CVPR 2019)\nParameters\n¢ alpha (float) — The denominator alpha in the balanced L1 loss. Defaults to 0.5.\n* gamma (float) — The gamma in the balanced L1 loss. Defaults to 1.5.\n\n* beta(float, optional) —The loss is a piecewise function of prediction and target. beta\nserves as a threshold for the difference between the prediction and target. Defaults to 1.0.\n\n* reduction (str, optional) — The method that reduces the loss to a scalar. Options are\n\n99 66.\n\n“none”, “mean” and “sum”.\n* loss_weight (float, optional) -— The weight of the loss. Defaults to 1.0\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None, **kwargs)\nForward function of loss.\n\nParameters\n* pred (torch. Tensor) — The prediction with shape (N, 4).\n* target (torch. Tensor) — The learning target of the prediction with shape (N, 4).\n¢ weight (torch.Tensor, optional) —Sample-wise loss weight with shape (N, ).\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\n\nre\n\noriginal reduction method of the loss. Options are “none”, “mean” and “sum”.\nReturns The calculated loss\nReturn type torch.Tensor\n\nclass mmdet.models.losses.BoundedIoULoss (beta=0.2, eps=0.001, reduction='mean', loss_weight=1.0)\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None, **kwargs)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n39.6. losses 413\n", "vlm_text": "Return type  tuple[float] \nclass  mmdet.models.losses. Associative Embedding Loss ( pull_weigh  $t{=}0.25$  ,  push weight=0.25 ) Associative Embedding Loss. \nMore details can be found in  Associative Embedding  and  CornerNet  . Code is modified from  kp_utils.py  # noqa: E501 \nParameters \n•  pull weight  ( float ) – Loss weight for corners from same object. •  push weight  ( float ) – Loss weight for corners from different object. \nforward ( pred ,  target ,  match ) Forward function. \nclass  mmdet.models.losses. Balanced L 1 Loss ( alpha  $\\mathbf{\\varepsilon}_{:=0.5}$  ,  gamma  $\\mathrm{=}I.5$  ,  beta  ${=}l.0$  ,  reduction  $.=$  'mean' , loss_weigh $t{=}I.0$ )\nBalanced L1 Loss. \narXiv:  https://arxiv.org/pdf/1904.02701.pdf  (CVPR 2019) \nParameters \n•  alpha  ( float ) – The denominator  alpha  in the balanced L1 loss. Defaults to 0.5. •  gamma  ( float ) – The  gamma  in the balanced L1 loss. Defaults to 1.5. •  beta  ( float, optional ) – The loss is a piecewise function of prediction and target.  beta serves as a threshold for the difference between the prediction and target. Defaults to 1.0. •  reduction  ( str, optional ) – The method that reduces the loss to a scalar. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of the loss. Defaults to 1.0 \nforward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) Forward function of loss. \nParameters \n•  pred  ( torch.Tensor ) – The prediction with shape (N, 4). •  target  ( torch.Tensor ) – The learning target of the prediction with shape (N, 4). •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight with shape (N, ). •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”.  The calculated loss \nReturn type  torch.Tensor \nclass  mmdet.models.losses. Bounded I oU Loss ( beta  $\\a=\\!0.2$  ,  eps=0.001 ,  reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0.$  ) \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. Should be overridden by all subclasses. "}
{"page": 421, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_421.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.losses.CIoULoss (eps=/e-06, reduction='mean', loss_weight=1.0)\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None, **kwargs)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.losses.CrossEntropyLoss (use_sigmoid=False, use_mask=False, reduction='mean',\nclass_weight=None, ignore_index=None, loss_weight=1.0)\n\nforward (cls_score, label, weight=None, avg_factor=None, reduction_override=None, ignore_index=None,\n**kwargs)\nForward function.\n\nParameters\n* cls_score (torch. Tensor) — The prediction.\n¢ label (torch. Tensor) — The learning label of the prediction.\n\n* weight (torch.Tensor, optional) —Sample-wise loss weight.\n\navg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\nreduction_override (str, optional) -The method used to reduce the loss. Options\n\nare “none”, “mean” and “sum”.\n\n¢ ignore_index(int | None)-The label index to be ignored. If not None, it will override\nthe default value. Default: None.\n\nReturns The calculated loss.\nReturn type torch.Tensor\n\nclass mmdet.models.losses.DIoULoss (eps=/e-06, reduction='mean', loss_weight=1.0)\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None, **kwargs)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\n414 Chapter 39. mmdet.models\n", "vlm_text": "Note:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.losses. CIoULoss ( eps=1e-06 ,  reduction  $\\mathbf{\\beta}=$  'mean' ,  loss_weigh  $\\scriptstyle{t=l.O.}$  ) \nforward ( pred ,  target ,  weigh  $\\leftleftarrows$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. \nShould be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.losses. Cross Entropy Loss ( use s igm oid  $\\leftrightharpoons$  False ,  use_mask  $\\mathbf{\\beta}=$  False ,  reduction  $.=$  'mean' , class weight  $\\leftrightharpoons$  None ,  ignore index  $=$  None ,  loss_weigh  $t{=}I.0.$  ) \nforward ( cls_score ,  label ,  weight  $\\leftrightharpoons$  None ,  avg_factor=None ,  reduction override  $=$  None ,  ignore index  $\\leftrightharpoons$  None , \\*\\*kwargs ) Forward function. \nParameters \n cls_score torch.Tensor •  label  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. •  ignore index  ( int | None ) – The label index to be ignored. If not None, it will override the default value. Default: None. Returns  The calculated loss. Return type  torch.Tensor \nclass  mmdet.models.losses. DIoULoss (  $.e p s{=}I e{\\mathrm{-}}$  06 ,  reduction  $=$  'mean' ,  loss weight  $\\scriptstyle{:=I.O}$  ) \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. \nShould be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. "}
{"page": 422, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_422.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.losses.DiceLoss (use_sigmoid=True, activate=True, reduction='mean', loss_weight=1.0,\neps=0.001)\n\nforward (pred, target, weight=None, reduction_override=None, avg_factor=None)\nForward function.\n\nParameters\n¢ pred (torch. Tensor) — The prediction, has a shape (n, *).\n* target (torch. Tensor) — The label of the prediction, shape (n, *), same shape of pred.\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction, has a\nshape (n,). Defaults to None.\n\navg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\nreduction_override (str, optional) — The reduction method used to override the\n\nre\n\noriginal reduction method of the loss. Options are “none”, “mean” and “sum”.\nReturns The calculated loss\nReturn type torch.Tensor\n\nclass mmdet.models.losses.DistributionFocalLoss (reduction='mean', loss_weight=1.0)\nDistribution Focal Loss (DFL) is a variant of Generalized Focal Loss: Learning Qualified and Distributed Bound-\ning Boxes for Dense Object Detection.\n\nParameters\n* reduction (str) — Options are ‘none’, ‘mean’ and ‘sum’.\n* loss_weight (float) — Loss weight of current loss.\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n\n¢ pred (torch. Tensor) — Predicted general distribution of bounding boxes (before soft-\nmax) with shape (N, n+1), n is the max value of the integral set /0, ..., nj} in paper.\n\ntarget (torch. Tensor) — Target distance label for bounding boxes with shape (N,).\n\nweight (torch.Tensor, optional) -— The weight of loss for each prediction. Defaults\nto None.\n\navg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\nreduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nclass mmdet.models.losses.FocalLoss (use_sigmoid=True, gamma=2.0, alpha=0.25, reduction='mean',\nloss_weight=1.0)\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n\n¢ pred (torch. Tensor) — The prediction.\n\n39.6. losses 415\n", "vlm_text": "class  mmdet.models.losses. DiceLoss ( use s igm oid  $\\leftrightharpoons$  True ,  activate  $=$  True ,  reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0$  ,  $e p s{=}0.00I)$  ) \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  avg_factor=None ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – The prediction, has a shape   $(\\mathbf{n},\\mathbf{\\Omega}^{*})$  . •  target  ( torch.Tensor ) – The label of the prediction, shape (n,  \\* ), same shape of pred. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction, has a shape (n,). Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. \nReturns  The calculated loss \nReturn type  torch.Tensor \nclass  mmdet.models.losses. Distribution Focal Loss ( reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ,  loss weight  $\\scriptstyle{t=I.O}$  ) Distribution Focal Loss (DFL) is a variant of  Generalized Focal Loss: Learning Qualified and Distributed Bound- ing Boxes for Dense Object Detection . \nParameters \n•  reduction  ( str ) – Options are  ‘none’ ,  ‘mean’  and  ‘sum’ . •  loss weight  ( float ) – Loss weight of current loss. \nforward ( pred ,  target ,  weigh  $\\leftleftarrows$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – Predicted general distribution of bounding boxes (before soft- max) with shape (N,  $_{\\mathrm{n+1}}$  ), n is the max value of the integral set  $\\langle O,\\,...\\,,\\,n\\rangle$   in paper. •  target  ( torch.Tensor ) – Target distance label for bounding boxes with shape   $\\mathbf{(N,})$  . •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nclass  mmdet.models.losses. FocalLoss ( use s igm oid  $\\leftrightharpoons$  True ,  gamma  $\\it{=2.0}$  ,  alpha=0.25 ,  reduction  $.=$  'mean' , loss weight  $\\scriptstyle{:=}I.O$  ) \nforward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – The prediction. "}
{"page": 423, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_423.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ntarget (torch. Tensor) — The learning label of the prediction.\n\nweight (torch.Tensor, optional) -— The weight of loss for each prediction. Defaults\nto None.\n\navg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\nreduction_override (str, optional) — The reduction method used to override the\n\nre\n\noriginal reduction method of the loss. Options are “none”, “mean” and “sum”.\nReturns The calculated loss\nReturn type torch.Tensor\n\nclass mmdet.models.losses.GHMC(bins=10, momentum=0, use_sigmoid=True, loss_weight=1.0,\nreduction='mean')\nGHM Classification Loss.\n\nDetails of the theorem can be viewed in the paper Gradient Harmonized Single-stage Detector.\nParameters\n* bins (int) — Number of the unit regions for distribution calculation.\n* momentum (float) — The parameter for moving average.\n* use_sigmoid (bool) — Can only be true for BCE based loss now.\n* loss_weight (float) — The weight of the total GHM-C loss.\n\n* reduction (str) — Options are “none”, “mean” and “sum”. Defaults to “mean”\n\nforward (pred, target, label_weight, reduction_override=None, **kwargs)\nCalculate the GHM-C loss.\n\nParameters\n\n¢ pred(float tensor of size [batch_num, class_num])-The direct prediction of\nclassification fe layer.\n\n¢ target (float tensor of size [batch_num, class_num]) - Binary class target\nfor each sample.\n\n¢ label_weight (float tensor of size [batch_num, class_num]) — the value is\n1 if the sample is valid and 0 if ignored.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nReturns The gradient harmonized loss.\n\nclass mmdet.models.losses.GHMR(mu=0.02, bins=10, momentum=0, loss_weight=1.0, reduction='mean')\nGHM Regression Loss.\n\nDetails of the theorem can be viewed in the paper Gradient Harmonized Single-stage Detector.\nParameters\n* mu (float) — The parameter for the Authentic Smooth L1 loss.\n* bins (int) — Number of the unit regions for distribution calculation.\n* momentum (float) — The parameter for moving average.\n* loss_weight (float) — The weight of the total GHM-R loss.\n\n9966,\n\n* reduction (str) — Options are “none”, “mean” and “sum”. Defaults to “mean”\n\n416 Chapter 39. mmdet.models\n", "vlm_text": "•  target  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. \nReturns  The calculated loss \nReturn type  torch.Tensor \nclass  mmdet.models.losses. GHMC (  $.b i n s{=}I O$  ,  momentum  $\\mathbf{\\chi}_{=0}$  ,  use s igm oid  $\\leftrightharpoons$  True ,  loss weight=1.0 , reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ) \nGHM Classification Loss. \nDetails of the theorem can be viewed in the paper  Gradient Harmonized Single-stage Detector . \nParameters \n•  bins  ( int ) – Number of the unit regions for distribution calculation. •  momentum  ( float ) – The parameter for moving average. •  use s igm oid  ( bool ) – Can only be true for BCE based loss now. •  loss weight  ( float ) – The weight of the total GHM-C loss. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. Defaults to “mean” \nforward ( pred ,  target ,  label weight ,  reduction override  $=$  None ,  \\*\\*kwargs ) Calculate the GHM-C loss. \nParameters \n•  pred  ( float tensor of size [batch_num, class_num] ) – The direct prediction of classification fc layer. •  target  ( float tensor of size [batch_num, class_num] ) – Binary class target for each sample. •  label weight  ( float tensor of size [batch_num, class_num] ) – the value is 1 if the sample is valid and 0 if ignored. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. Returns  The gradient harmonized loss. \nclass  mmdet.models.losses. GHMR (  $\\cdot_{m u=0.02}$  ,  bins  $\\mathrm{=}I O$  ,  momentum  $\\mathbf{\\chi}_{=0}$  ,  loss weight=1.0 ,  reduction  $.=$  mean' ) GHM Regression Loss. \nDetails of the theorem can be viewed in the paper  Gradient Harmonized Single-stage Detector . \nParameters \n•  mu  ( float ) – The parameter for the Authentic Smooth L1 loss. •  bins  ( int ) – Number of the unit regions for distribution calculation. •  momentum  ( float ) – The parameter for moving average. •  loss weight  ( float ) – The weight of the total GHM-R loss. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. Defaults to “mean” "}
{"page": 424, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_424.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward (pred, target, label_weight, avg_factor=None, reduction_override=None)\nCalculate the GHM-R loss.\n\nParameters\n\n¢ pred(float tensor of size [batch_num, 4 (* class_num)])-—The prediction\nof box regression layer. Channel number can be 4 or 4 * class_num depending on whether\nit is class-agnostic.\n\n¢ target (float tensor of size [batch_num, 4 (* class_num)]) — The target\nregression values with the same size of pred.\n\n¢ label_weight (float tensor of size [batch_num, 4 (* class_num)])-—The\nweight of each sample, 0 if ignored.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nReturns The gradient harmonized loss.\n\nclass mmdet.models.losses.GIoULoss (eps=/e-06, reduction='mean', loss_weight=1.0)\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None, **kwargs)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.losses.GaussianFocalLoss (alpha=2.0, gamma=4.0, reduction='mean',\nloss_weight=1.0)\nGaussianFocalLoss is a variant of focal loss.\n\nMore details can be found in the paper Code is modified from kp_utils.py # noqa: E501 Please notice that the\ntarget in GaussianFocalLoss is a gaussian heatmap, not 0/1 binary target.\n\nParameters\n* alpha (float) — Power of prediction.\n* gamma (float) — Power of target for negative samples.\n* reduction (str) — Options are “none”, “mean” and “sum”.\n* loss_weight (float) — Loss weight of current loss.\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n¢ pred (torch. Tensor) — The prediction.\n* target (torch. Tensor) — The learning target of the prediction in gaussian distribution.\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction. Defaults\nto None.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n39.6. losses 417\n", "vlm_text": "forward ( pred ,  target ,  label weight ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Calculate the GHM-R loss. \nParameters \n•  pred  ( float tensor of size [batch_num, 4 (\\* class_num)] ) – The prediction of box regression layer. Channel number can be 4 or  $^{4\\ast}$   class_num depending on whether it is class-agnostic. •  target  ( float tensor of size [batch_num, 4 (\\* class_num)] ) – The target regression values with the same size of pred. •  label weight  ( float tensor of size [batch_num, 4 (\\* class_num)] ) – The weight of each sample, 0 if ignored. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nReturns  The gradient harmonized loss. \nclass  mmdet.models.losses. GIoULoss ( eps=1e-06 ,  reduction  $=$  'mean' ,  loss weight=1.0 ) \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.losses. Gaussian Focal Loss ( alpha  $=\\!2.0$  ,  gamma  $\\mathopen{:=}4.0$  ,  reduction  $=$  'mean' , loss_weigh $t{=}I.0$ )\nGaussian Focal Loss is a variant of focal loss. \nMore details can be found in the  paper  Code is modified from  kp_utils.py  # noqa: E501 Please notice that the target in Gaussian Focal Loss is a gaussian heatmap, not 0/1 binary target. \nParameters \n•  alpha  ( float ) – Power of prediction. •  gamma  ( float ) – Power of target for negative samples. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. •  loss weight  ( float ) – Loss weight of current loss. \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction in gaussian distribution. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. "}
{"page": 425, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_425.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nclass mmdet.models.losses.IoULoss (linear=False, eps=1e-06, reduction='mean', loss_weight=1.0,\nmode='log')\nIoULoss.\n\nComputing the IoU loss between a set of predicted bboxes and target bboxes.\nParameters\n¢ linear (boo1) — If True, use linear scale of loss else determined by mode. Default: False.\n* eps (float) — Eps to avoid log(0).\n* reduction (str) — Options are “none”, “mean” and “sum”.\n* loss_weight (float) — Weight of loss.\n\n* mode (str) — Loss scaling mode, including “linear”, “square”, and “log”. Default: ‘log’\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None, **kwargs)\nForward function.\n\nParameters\n¢ pred (torch. Tensor) — The prediction.\n* target (torch. Tensor) — The learning target of the prediction.\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction. Defaults\nto None.\n\navg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\nreduction_override (str, optional) — The reduction method used to override the\n\nees\n\noriginal reduction method of the loss. Defaults to None. Options are “none”, “mean” and\n\n“ ”\n\nsum.\n\nclass mmdet.models.losses.KnowledgeDistillationKLDivLoss (reduction='mean', loss_weight=1.0,\nT=10)\nLoss function for knowledge distilling using KL divergence.\n\nParameters\n* reduction (str) — Options are ‘none’, ‘mean’ and ‘sum’.\n* loss_weight (float) — Loss weight of current loss.\n¢ T (int) — Temperature for distillation.\n\nforward (pred, soft_label, weight=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n* pred (Tensor) — Predicted logits with shape (N, n + 1).\n¢ soft_label (Tensor) — Target logits with shape (N, N + 1).\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction. Defaults\nto None.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n418 Chapter 39. mmdet.models\n", "vlm_text": "•  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nclass  mmdet.models.losses. IoULoss ( linear=False ,  eps=1e-06 ,  reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ,  loss_weigh  $t{=}I.0$  , mode  $=$  'log' ) \nComputing the IoU loss between a set of predicted bboxes and target bboxes. \nParameters \n•  linear  ( bool ) – If True, use linear scale of loss else determined by mode. Default: False. •  eps  ( float ) – Eps to avoid log(0). •  reduction  ( str ) – Options are “none”, “mean” and “sum”. •  loss weight  ( float ) – Weight of loss. •  mode  ( str ) – Loss scaling mode, including “linear”, “square”, and “log”. Default: ‘log’ \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ,  \\*\\*kwargs ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. Options are “none”, “mean” and “sum”. \nclass  mmdet.models.losses. Knowledge Distillation KL Div Loss ( reduction  $=$  mean' ,  loss_weigh  $t{=}I.0$  ,  $\\scriptstyle{T=I O}$  ) Loss function for knowledge distilling using KL divergence. \nParameters \n•  reduction  ( str ) – Options are  ‘none’ ,  ‘mean’  and  ‘sum’ . •  loss weight  ( float ) – Loss weight of current loss. •  T  ( int ) – Temperature for distillation. \nforward ( pred ,  soft_label ,  weigh  $\\leftleftarrows$  None ,  avg_factor=None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. \nParameters \n•  pred  ( Tensor ) – Predicted logits with shape   $(\\mathsf{N},\\mathsf{n}+1)$  . •  soft_label  ( Tensor ) – Target logits with shape   $(\\mathbf{N},\\mathbf{N}+1)$  . •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. "}
{"page": 426, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_426.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nclass mmdet.models.losses.L1Loss(reduction='mean', loss_weight=1.0)\nL1 loss.\n\nParameters\n\n* reduction (str, optional) — The method to reduce the loss. Options are “none”,\n“mean” and “sum”.\n\n* loss_weight (float, optional) — The weight of loss.\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n¢ pred (torch. Tensor) — The prediction.\n* target (torch. Tensor) — The learning target of the prediction.\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction. Defaults\nto None.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nclass mmdet.models.losses.MSELoss (reduction='mean', loss_weight=1.0)\nMSELoss.\n\nParameters\n\n* reduction (str, optional) — The method that reduces the loss to a scalar. Options are\n\n“none”, “mean” and “sum”.\n* loss_weight (float, optional) -— The weight of the loss. Defaults to 1.0\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None)\nForward function of loss.\n\nParameters\n¢ pred (torch. Tensor) — The prediction.\n* target (torch. Tensor) — The learning target of the prediction.\n\n¢ weight (torch.Tensor, optional) — Weight of the loss for each prediction. Defaults\nto None.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nReturns The calculated loss\n\nReturn type torch.Tensor\n\n39.6. losses 419\n", "vlm_text": "•  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nclass  mmdet.models.losses. L1Loss ( reduction  $\\scriptstyle{\\mathcal{S}}$  'mean' ,  loss_weigh  $t{=}l.0$  ) L1 loss. \nParameters \n•  reduction  ( str, optional ) – The method to reduce the loss. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of loss. \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nclass  mmdet.models.losses. MSELoss ( reduction  $\\mathbf{\\beta}=$  'mean' ,  loss_weigh  $t{=}I.0$  ) MSELoss. \nParameters \n•  reduction  ( str, optional ) – The method that reduces the loss to a scalar. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of the loss. Defaults to 1.0 \nforward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ) Forward function of loss. \nParameters \n•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – Weight of the loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nReturns  The calculated loss \nReturn type  torch.Tensor "}
{"page": 427, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_427.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.losses.QualityFocalLoss (use_sigmoid=True, beta=2.0, reduction='mean',\nloss_weight=1.0)\nQuality Focal Loss (QFL) is a variant of Generalized Focal Loss: Learning Qualified and Distributed Bounding\nBoxes for Dense Object Detection.\n\nParameters\n* use_sigmoid (bool) — Whether sigmoid operation is conducted in QFL. Defaults to True.\n* beta (float) — The beta parameter for calculating the modulating factor. Defaults to 2.0.\n* reduction (str) — Options are “none”, “mean” and “sum”.\n\n* loss_weight (float) — Loss weight of current loss.\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n\n¢ pred (torch. Tensor) — Predicted joint representation of classification and quality (IoU)\nestimation with shape (N, C), C is the number of classes.\n\n¢ target (tuple([torch.Tensor])) — Target category label with shape (N,) and target\nquality label with shape (N,).\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction. Defaults\nto None.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nclass mmdet.models.losses.SeesawLoss (use_sigmoid=False, p=0.8, q=2.0, num_classes=1203, eps=0.01,\nreduction='mean', loss_weight=1.0, return_dict=True)\nSeesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021) arXiv: https://arxiv.org/abs/2008.10032\n\nParameters\n\n* use_sigmoid (bool, optional) - Whether the prediction uses sigmoid of softmax. Only\nFalse is supported.\n\n* p(float, optional) — The p in the mitigation factor. Defaults to 0.8.\n* q(float, optional) -— The q in the compenstation factor. Defaults to 2.0.\n\n* num_classes (int, optional) — The number of classes. Default to 1203 for LVIS v1\ndataset.\n\n* eps (float, optional) — The minimal value of divisor to smooth the computation of\ncompensation factor\n\n* reduction (str, optional) — The method that reduces the loss to a scalar. Options are\n\n99 66.\n\n“none”, “mean” and “sum”.\n* loss_weight (float, optional) -— The weight of the loss. Defaults to 1.0\n* return_dict (bool, optional) — Whether return the losses as a dict. Default to True.\n\nforward (cls_score, labels, label_weights=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n\n420 Chapter 39. mmdet.models\n", "vlm_text": "class  mmdet.models.losses. Quality Focal Loss ( use s igm oid  $\\leftrightharpoons$  True ,  beta  $=\\!2.0$  ,  reduction  $=$  'mean' , loss_weigh $t{=}I.0$ )Quality Focal Loss (QFL) is a variant of  Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection . \nParameters \n•  use s igm oid  ( bool ) – Whether sigmoid operation is conducted in QFL. Defaults to True. •  beta  ( float ) – The beta parameter for calculating the modulating factor. Defaults to 2.0. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. •  loss weight  ( float ) – Loss weight of current loss. \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\beta}=$  None ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – Predicted joint representation of classification and quality (IoU) estimation with shape (N, C), C is the number of classes. •  target  ( tuple([torch.Tensor]) ) – Target category label with shape (N,) and target quality label with shape (N,). •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nclass  mmdet.models.losses. SeesawLoss ( use s igm oid  $\\leftrightharpoons$  False ,  $p{=}0.8$  ,  $q{=}2.0$  ,  num_classe  $s{=}1203$  ,  eps  $\\it{=0.01}$  , reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0$  ,  return dic t  $\\mathbf{\\dot{\\rho}}=$  True ) Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021) arXiv:  https://arxiv.org/abs/2008.10032 \nParameters \n•  use s igm oid  ( bool, optional ) – Whether the prediction uses sigmoid of softmax. Only False is supported. •  p  ( float, optional ) – The  p  in the mitigation factor. Defaults to 0.8. •  q  ( float, optional ) – The  q  in the com pen station factor. Defaults to 2.0. •  num classes  ( int, optional ) – The number of classes. Default to 1203 for LVIS v1 dataset. •  eps  ( float, optional ) – The minimal value of divisor to smooth the computation of compensation factor •  reduction  ( str, optional ) – The method that reduces the loss to a scalar. Options are “none”, “mean” and “sum”. •  loss weight  ( float, optional ) – The weight of the loss. Defaults to 1.0 •  return dic t  ( bool, optional ) – Whether return the losses as a dict. Default to True. \nforward ( cls_score ,  labels ,  label weights  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override=None ) Forward function. \nParameters "}
{"page": 428, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_428.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* cls_score (torch. Tensor) — The prediction with shape (N, C + 2).\n¢ labels (torch. Tensor) — The learning label of the prediction.\n¢ label_weights (torch.Tensor, optional) -—Sample-wise loss weight.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ reduction(str, optional) -The method used to reduce the loss. Options are “none”,\n“mean” and “sum”.\n\nReturns if return_dict == False: The calculated loss | if return_dict == True: The dict of calcu-\nlated losses for objectness and classes, respectively.\n\nReturn type torch.Tensor | Dict [str, torch.Tensor]\n\nget_accuracy (cls_score, labels)\nGet custom accuracy w.r.t. cls_score and labels.\n\nParameters\n* cls_score (torch. Tensor) — The prediction with shape (N, C + 2).\n¢ labels (torch. Tensor) — The learning label of the prediction.\nReturns\nThe accuracy for objectness and classes, respectively.\nReturn type Dict [str, torch. Tensor]\n\nget_activation(cls_score)\nGet custom activation of cls_score.\n\nParameters cls_score (torch. Tensor) — The prediction with shape (N, C + 2).\nReturns\n\nThe custom activation of cls_score with shape (N, C + 1).\nReturn type torch.Tensor\n\nget_cls_channels (num_classes)\nGet custom classification channels.\n\nParameters num_classes (int) — The number of classes.\nReturns The custom classification channels.\nReturn type int\n\nclass mmdet.models.losses.SmoothL1Loss (beta=1.0, reduction='mean', loss_weight=1.0)\nSmooth L1 loss.\n\nParameters\n* beta (float, optional) — The threshold in the piecewise function. Defaults to 1.0.\n\n* reduction (str, optional) — The method to reduce the loss. Options are “none”,\n“mean” and “sum”. Defaults to “mean”.\n\n* loss_weight (float, optional) — The weight of loss.\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None, **kwargs)\nForward function.\n\nParameters\n\n39.6. losses 421\n", "vlm_text": "•  cls_score  ( torch.Tensor ) – The prediction with shape   $(\\mathbf{N},\\mathbf{C}+2)$  . •  labels  ( torch.Tensor ) – The learning label of the prediction. •  label weights  ( torch.Tensor, optional ) – Sample-wise loss weight. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. \nlated losses for objectness and classes, respectively. Return type  torch.Tensor | Dict [str, torch.Tensor] \n\nget accuracy ( cls_score ,  labels ) Get custom accuracy w.r.t. cls_score and labels. \nParameters •  cls_score  ( torch.Tensor ) – The prediction with shape   $(\\mathbf{N},\\mathbf{C}+2)$  . •  labels  ( torch.Tensor ) – The learning label of the prediction. Returns The accuracy for objectness and classes,  respectively. Return type  Dict [str, torch.Tensor] \nget activation ( cls_score ) Get custom activation of cls_score. Parameters  cls_score  ( torch.Tensor ) – The prediction with shape   $(\\mathbf{N},\\mathbf{C}+2)$  . Returns The custom activation of cls_score with shape    $(\\mathbf{N},\\mathbf{C}+1)$  . Return type  torch.Tensor \nget cls channels ( num classes ) Get custom classification channels. Parameters  num classes  ( int ) – The number of classes. Returns  The custom classification channels. Return type  int \nclass  mmdet.models.losses. Smooth L 1 Loss ( beta  ${=}l.0$  ,  reduction  $\\mathbf{\\beta}=$  'mean' ,  loss_weigh  $t{=}I.0$  ) Smooth L1 loss. \nParameters \n•  beta  ( float, optional ) – The threshold in the piecewise function. Defaults to 1.0. •  reduction  ( str, optional ) – The method to reduce the loss. Options are “none”, “mean” and “sum”. Defaults to “mean”. •  loss weight  ( float, optional ) – The weight of loss. \nforward ( pred ,  target ,  weight  $\\fallingdotseq$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $\\mathbf{\\dot{\\rho}}$  None ,  \\*\\*kwargs ) Forward function. \nParameters "}
{"page": 429, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_429.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n¢ pred (torch. Tensor) — The prediction.\n* target (torch. Tensor) — The learning target of the prediction.\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction. Defaults\nto None.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Defaults to None.\n\nclass mmdet.models.losses.VarifocalLoss (use_sigmoid=True, alpha=0.75, gamma=2.0,\niou_weighted=True, reduction='mean', loss_weight=1.0)\n\nforward (pred, target, weight=None, avg_factor=None, reduction_override=None)\nForward function.\n\nParameters\n¢ pred (torch. Tensor) — The prediction.\n* target (torch. Tensor) — The learning target of the prediction.\n\n¢ weight (torch.Tensor, optional) — The weight of loss for each prediction. Defaults\nto None.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ reduction_override (str, optional) — The reduction method used to override the\noriginal reduction method of the loss. Options are “none”, “mean” and “sum”.\nReturns The calculated loss\nReturn type torch.Tensor\n\nmmndet .models.losses.binary_cross_entropy (pred, label, weight=None, reduction='mean',\navg_factor=None, class_weight=None, ignore_index=- 100)\nCalculate the binary CrossEntropy loss.\n\nParameters\n* pred (torch. Tensor) — The prediction with shape (N, 1).\n¢ label (torch. Tensor) — The learning label of the prediction.\n* weight (torch.Tensor, optional) -—Sample-wise loss weight.\n\n* reduction (str, optional) — The method used to reduce the loss. Options are “none”,\n“mean” and “sum”.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ class_weight (list[float], optional) — The weight for each class.\n\n* ignore_index (int / None) — The label index to be ignored. If None, it will be set to\ndefault value. Default: -100.\n\nReturns The calculated loss.\n\nReturn type torch.Tensor\n\n422 Chapter 39. mmdet.models\n", "vlm_text": "•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Defaults to None. \nclass  mmdet.models.losses. Var i focal Loss ( use s igm oid  $\\leftrightharpoons$  True ,  alpha  $\\it{\\Lambda}_{i=0.75}$  ,  gamma  $\\it{=}2.0$  , i ou weighted  $\\leftrightharpoons$  True ,  reduction  $.=$  'mean' ,  loss_weigh  $t{=}I.0$  ) \nforward ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  avg_factor  $\\leftrightharpoons$  None ,  reduction override  $=$  None ) Forward function. \nParameters \n•  pred  ( torch.Tensor ) – The prediction. •  target  ( torch.Tensor ) – The learning target of the prediction. •  weight  ( torch.Tensor, optional ) – The weight of loss for each prediction. Defaults to None. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  reduction override  ( str, optional ) – The reduction method used to override the original reduction method of the loss. Options are “none”, “mean” and “sum”. \nReturns  The calculated loss \nReturn type  torch.Tensor \nmmdet.models.losses. binary cross entropy ( pred ,  label ,  weigh  $\\leftrightharpoons$  None ,  reduction  $.=$  'mean' , avg_factor  $\\leftrightharpoons$  None ,  class weigh  $t{=}t$  None ,  ignore index  $:=:$  - 100 ) Calculate the binary Cross Entropy loss. \nParameters \n•  pred  ( torch.Tensor ) – The prediction with shape (N, 1). •  label  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  reduction  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  class weight  ( list[float], optional ) – The weight for each class. •  ignore index  ( int | None ) – The label index to be ignored. If None, it will be set to default value. Default: -100. \nReturns  The calculated loss. \nReturn type  torch.Tensor "}
{"page": 430, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_430.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmndet .models.losses.cross_entropy (pred, label, weight=None, reduction='mean', avg_factor=None,\nclass_weight=None, ignore_index=- 100)\nCalculate the CrossEntropy loss.\n\nParameters\n* pred (torch. Tensor) — The prediction with shape (N, C), C is the number of classes.\n¢ label (torch. Tensor) — The learning label of the prediction.\n* weight (torch.Tensor, optional) -—Sample-wise loss weight.\n¢ reduction(str, optional) — The method used to reduce the loss.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ class_weight (list[float], optional) — The weight for each class.\n\n* ignore_index (int / None) — The label index to be ignored. If None, it will be set to\ndefault value. Default: -100.\n\nReturns The calculated loss\nReturn type torch.Tensor\n\nmmndet .models.losses.mask_cross_entropy (pred, target, label, reduction='mean', avg_factor=None,\nclass_weight=None, ignore_index=None)\nCalculate the CrossEntropy loss for masks.\n\nParameters\n\n* pred (torch. Tensor) — The prediction with shape (N, C, *), C is the number of classes.\nThe trailing * indicates arbitrary shape.\n\n* target (torch. Tensor) — The learning label of the prediction.\n\n¢ label (torch. Tensor) — label indicates the class label of the mask corresponding object.\nThis will be used to select the mask in the of the class which the object belongs to when the\nmask prediction if not class-agnostic.\n\n* reduction (str, optional) — The method used to reduce the loss. Options are “none”,\n“mean” and “sum”.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\n¢ class_weight (list[float], optional) — The weight for each class.\n¢ ignore_index (None) — Placeholder, to be consistent with other loss. Default: None.\nReturns The calculated loss\n\nReturn type torch.Tensor\n\n39.6. losses 423\n", "vlm_text": "mmdet.models.losses. cross entropy ( pred ,  label ,  weight  $=$  None ,  reduction  $=$  'mean' ,  avg_factor  $\\leftrightharpoons$  None , class weigh  $\\leftrightharpoons$  None ,  ignore index  $\\mathbf{=}\\mathbf{\\cdot}$  - 100 ) \nCalculate the Cross Entropy loss. \nParameters \n•  pred  ( torch.Tensor ) – The prediction with shape (N, C), C is the number of classes. •  label  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  reduction  ( str, optional ) – The method used to reduce the loss. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  class weight  ( list[float], optional ) – The weight for each class. •  ignore index  ( int | None ) – The label index to be ignored. If None, it will be set to default value. Default: -100. \nReturns  The calculated loss \nReturn type  torch.Tensor \nmmdet.models.losses. mask cross entropy ( pred ,  target ,  label ,  reduction  $=$  'mean' ,  avg_factor=None , class weight  $=$  None ,  ignore index  $\\mathbf{\\beta}=$  None ) \nCalculate the Cross Entropy loss for masks. \nParameters \n•  pred  ( torch.Tensor ) – The prediction with shape (N, C,  \\* ), C is the number of classes. The trailing \\* indicates arbitrary shape. •  target  ( torch.Tensor ) – The learning label of the prediction. •  label  ( torch.Tensor ) –  label  indicates the class label of the mask corresponding object. This will be used to select the mask in the of the class which the object belongs to when the mask prediction if not class-agnostic. •  reduction  ( str, optional ) – The method used to reduce the loss. Options are “none”, “mean” and “sum”. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. •  class weight  ( list[float], optional ) – The weight for each class. •  ignore index  ( None ) – Placeholder, to be consistent with other loss. Default: None. \nReturns  The calculated loss \nReturn type  torch.Tensor "}
{"page": 431, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_431.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nExample\n\n>>> N, C = 3, ll\n\n>>> H, W= 2, 2\n\n>>> pred = torch.randn(N, C, H, W) * 1000\n\n>>> target = torch.rand(N, H, W)\n\n>>> label = torch.randint(0, C, size=(N,))\n\n>>> reduction = 'mean'\n\n>>> avg_factor = None\n\n>>> class_weights = None\n\n>>> loss = mask_cross_entropy(pred, target, label, reduction,\n>>> avg_factor, class_weights)\n>>> assert loss.shape == (1,)\n\nmmdet .models.losses.mse_loss (pred, target)\nWarpper of mse loss.\n\nmmdet.models.losses.reduce_loss (loss, reduction)\nReduce loss as specified.\n\nParameters\n\n¢ loss (Tensor) — Elementwise loss tensor.\n\n* reduction (str) — Options are “none”, “mean” and “sum”.\nReturns Reduced loss tensor.\n\nReturn type Tensor\n\nmmdet .models.losses.sigmoid_focal_loss (pred, target, weight=None, gamma=2.0, alpha=0.25,\nreduction='mean', avg_factor=None)\nA warpper of cuda version Focal Loss.\n\nParameters\n* pred (torch. Tensor) — The prediction with shape (N, C), C is the number of classes.\n* target (torch. Tensor) — The learning label of the prediction.\n* weight (torch.Tensor, optional) -—Sample-wise loss weight.\n\n* gamma (float, optional) — The gamma for calculating the modulating factor. Defaults\nto 2.0.\n\n¢ alpha (float, optional) -—A balanced form for Focal Loss. Defaults to 0.25.\n\n¢ reduction (str, optional) —The method used to reduce the loss into a scalar. Defaults\n\n99 66.\n\nto ‘mean’. Options are “none”, “mean” and “sum”.\n\n* avg_factor (int, optional) — Average factor that is used to average the loss. Defaults\nto None.\n\nmmndet .models.losses.weighted_loss (loss_func)\nCreate a weighted version of a given loss function.\n\nTo use this decorator, the loss function must have the signature like loss_func(pred, target, **kwargs). The\nfunction only needs to compute element-wise loss without any reduction. This decorator will add weight and\nreduction arguments to the function. The decorated function will have the signature like loss_func(pred, target,\nweight=None, reduction=’mean’, avg_factor=None, **kwargs).\n\nExample\n\n424 Chapter 39. mmdet.models\n", "vlm_text": "Example \n $>>\\texttt{N}$  ,   $\\textsf{C}=\\ 3$  ,  11  $>>\\texttt{H}$  ,   $\\texttt{\\^{W}}=\\texttt{\\^{2}}$  ,  2  $>>$   pred  $=$   torch . randn(N, C, H, W)  \\*  1000  $>>$   target  $=$   torch . rand(N, H, W)  $>>$   label  $=$   torch . randint( 0 , C, size  $\\scriptstyle{\\varepsilon}$  (N,))  $>>$   reduction  $=$   ' mean '  $>>$   avg_factor  $=$   None  $>>$   class weights  $=$   None  $>>$   loss  $=$   mask cross entropy(pred, target, label, reduction,  $>>$  avg_factor, class weights)  $>>$   assert  loss . shape  ==  ( 1 ,) \nmmdet.models.losses. mse_loss ( pred ,  target ) Warpper of mse loss. \nmmdet.models.losses. reduce loss ( loss ,  reduction ) Reduce loss as specified. \nParameters \n•  loss  ( Tensor ) – Element wise loss tensor. •  reduction  ( str ) – Options are “none”, “mean” and “sum”. \nReturns  Reduced loss tensor. \nReturn type  Tensor \nmmdet.models.losses. s igm oid focal loss ( pred ,  target ,  weight  $\\leftrightharpoons$  None ,  gamma=2.0 ,  alpha=0.25 , reduction  $.=$  'mean' ,  avg_factor  $\\leftrightharpoons$  None ) \nA warpper of cuda version  Focal Loss . \nParameters \n•  pred  ( torch.Tensor ) – The prediction with shape (N, C), C is the number of classes. •  target  ( torch.Tensor ) – The learning label of the prediction. •  weight  ( torch.Tensor, optional ) – Sample-wise loss weight. •  gamma  ( float, optional ) – The gamma for calculating the modulating factor. Defaults to 2.0. •  alpha  ( float, optional ) – A balanced form for Focal Loss. Defaults to 0.25. •  reduction  ( str, optional ) – The method used to reduce the loss into a scalar. Defaults to ‘mean’. Options are “none”, “mean” and “sum”. •  avg_factor  ( int, optional ) – Average factor that is used to average the loss. Defaults to None. \nmmdet.models.losses. weighted loss ( loss_func ) Create a weighted version of a given loss function. \nTo use this decorator, the loss function must have the signature like  loss_func(pred, target, \\*\\*kwargs) . The function only needs to compute element-wise loss without any reduction. This decorator will add weight and reduction arguments to the function. The decorated function will have the signature like  loss_func(pred, target, weight  $\\leftrightharpoons$  None, reduction  $\\scriptstyle{\\mathcal{S}}$  ’mean’, avg_factor  $\\leftrightharpoons$  None, \\*\\*kwargs) . \nExample "}
{"page": 432, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_432.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n>>> import\n\ntorch\n\n>>> @weighted_loss\n>>> def 11_loss(pred, target):\n>>> return (pred - target).abs()\n\n>>> pred =\n>>> target\n>>> weight\n\ntorch.Tensor([0, 2, 3])\n= torch.Tensor([1, 1, 1])\n= torch.Tensor([1, 9, 1])\n\ntensor(1.)\n\n>>> li_loss(pred, target)\ntensor(1. 3333)\n>>> li_loss(pred, target, weight)\n\n>>> li_loss(pred, target, reduction='none')\ntensor([1.,\n>>> li_loss(pred, target, weight, avg_factor=2)\ntensor(1.5000)\n\n1., 2.])\n\n39.7 utils\n\nclass mmdet.models.utils.AdaptiveAvgPoo12d(output_size: Union[int, None, Tuple[Optional[int], ...]])\nHandle empty batch dimension to AdaptiveAvgPool2d.\n\nforward(x)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.utils.CSPLayer (in_channels, out_channels, expand_ratio=0.5, num_blocks=1,\n\nadd_identity=True, use_depthwise=False, conv_cfg=None,\nnorm_cfg={‘eps': 0.001, ‘momentum’: 0.03, ‘type': 'BN'},\nact_cfg=('type': 'Swish'}, init_cfg=None)\n\nCross Stage Partial Layer.\n\nParameters\n\nin_channels (int) — The input channels of the CSP layer.\nout_channels (int) — The output channels of the CSP layer.\n\nexpand_ratio (float) — Ratio to adjust the number of channels of the hidden layer. De-\nfault: 0.5\n\nnum_blocks (int) — Number of blocks. Default: 1\nadd_identity (bool) — Whether to add identity in blocks. Default: True\n\nuse_depthwise (bool) — Whether to depthwise separable convolution in blocks. Default:\nFalse\n\n39.7. utils\n\n425\n\n", "vlm_text": "The image shows Python code using PyTorch to demonstrate the application of a custom L1 loss function with weights and different computation modes. Here’s a breakdown of the code:\n\n1. **Importing and Defining L1 Loss:**\n   - The `torch` library is imported.\n   - A custom decorator `@weighted_loss` (assumed to be defined elsewhere) is applied to the function `l1_loss`, indicating that it may modify or enhance the behavior of the `l1_loss` function.\n   - The `l1_loss` function is defined, which calculates the absolute difference between `pred` (predictions) and `target`.\n\n2. **Tensor Definitions:**\n   - A prediction tensor `pred` is created with values `[0, 2, 3]`.\n   - A target tensor `target` is created with values `[1, 1, 1]`.\n   - A weight tensor `weight` is created with values `[1, 0, 1]`, indicating the importance of different elements in the loss calculation.\n\n3. **Examples of L1 Loss Calculation:**\n   - `l1_loss(pred, target)` calculates the average L1 loss between `pred` and `target`. The result is `tensor(1.3333)`.\n   - `l1_loss(pred, target, weight)` calculates the weighted L1 loss, considering the `weight`. The result is `tensor(1.)`.\n   - `l1_loss(pred, target, reduction='none')` computes the element-wise L1 loss without reduction, resulting in `tensor([1., 1., 2.])`.\n   - `l1_loss(pred, target, weight, avg_factor=2)` computes the weighted L1 loss with a specified averaging factor (`avg_factor`), resulting in `tensor(1.5000)`.\n\nThe table effectively illustrates how L1 loss can be customized with weights and different reduction modes using PyTorch, allowing users to apply the specifics of their problem contexts in loss calculations.\n39.7 utils \nclass  mmdet.models.utils. Adaptive Avg Pool 2 d ( output size: Union[int, None, Tuple[Optional[int], ...]] ) Handle empty batch dimension to Adaptive Avg Pool 2 d. \nforward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.utils. CSPLayer ( in channels ,  out channels ,  expand ratio  $\\backsimeq\\!O.5$  ,  num_blocks  $\\scriptstyle{\\mathfrak{s}}=I$  , add identity  $\\scriptstyle\\gamma=$  True ,  use depth wise  $\\mathbf{=}$  False ,  conv_cfg  $\\leftrightharpoons$  None , norm_cfg  $\\scriptstyle{\\tilde{\\,}}=$  {'eps': 0.001, 'momentum': 0.03, 'type': 'BN'} , act_cfg  $=$  {'type': 'Swish'} ,  init_cfg  $\\mathbf{\\hat{\\mu}}$  None ) \nCross Stage Partial Layer. \nParameters \n•  in channels  ( int ) – The input channels of the CSP layer. •  out channels  ( int ) – The output channels of the CSP layer. •  expand ratio  ( float ) – Ratio to adjust the number of channels of the hidden layer. De- fault: 0.5 •  num_blocks  ( int ) – Number of blocks. Default: 1 •  add identity  ( bool ) – Whether to add identity in blocks. Default: True •  use depth wise  ( bool ) – Whether to depthwise separable convolution in blocks. Default: False "}
{"page": 433, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_433.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* conv_cfg (dict, optional) — Config dict for convolution layer. Default: None, which\nmeans using conv2d.\n\n* norm_cfg (dict) — Config dict for normalization layer. Default: dict(type=’ BN’)\n* act_cfg (dict) — Config dict for activation layer. Default: dict(type=’Swish’)\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.utils.ConvUpsample(in_channels, inner_channels, num_layers=1,\nnum_upsample=None, conv_cfg=None, norm_cfg=None,\ninit_cfg=None, **kwargs)\nConvUpsample performs 2x upsampling after Conv.\n\nThere are several ConvModule layers. In the first few layers, upsampling will be applied after each layer of\nconvolution. The number of upsampling must be no more than the number of ConvModule layers.\n\nParameters\n¢ in_channels (int) — Number of channels in the input feature map.\n* inner_channels (int) — Number of channels produced by the convolution.\n* num_layers (int) — Number of convolution layers.\n\n* num_upsample (int / optional) — Number of upsampling layer. Must be no more than\nnum_layers. Upsampling will be applied after the first num_upsamp1e layers of convolution.\nDefault: num_layers.\n\n* conv_cfg (dict) — Config dict for convolution layer. Default: None, which means using\nconv2d.\n\n* norm_cfg (dict) — Config dict for normalization layer. Default: None.\n¢ init_cfg (dict) — Config dict for initialization. Default: None.\n¢ kwargs (key word augments) - Other augments used in ConvModule.\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.utils.DetrTransformerDecoder ( “args, post_norm_cfg=({'type': 'LN'},\nreturn_intermediate=False, **kwargs)\nImplements the decoder in DETR transformer.\n\nParameters\n\n* return_intermediate (bool) — Whether to return intermediate outputs.\n\n426 Chapter 39. mmdet.models\n", "vlm_text": "•  conv_cfg  ( dict, optional ) – Config dict for convolution layer. Default: None, which means using conv2d. \n•  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict  $\\scriptstyle(\\mathrm{type}={\\mathrm{BN}}^{\\prime})$  ) •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $=^{,}$  ’Swish’) \n\nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.utils. Con v Up sample ( in channels ,  inner channels ,  num_layers  $\\mathrm{\\Sigma}_{:=I}$  , num up sample  $\\mathbf{\\dot{\\rho}}$  None ,  conv_cfg  $=$  None ,  norm_cfg=None , init_cfg  $=$  None ,  \\*\\*kwargs ) \nCon v Up sample performs   $2\\mathrm{x}$   upsampling after Conv. \nThere are several  ConvModule  layers. In the first few layers, upsampling will be applied after each layer of convolution. The number of upsampling must be no more than the number of ConvModule layers. \nParameters \n•  in channels  ( int ) – Number of channels in the input feature map. •  inner channels  ( int ) – Number of channels produced by the convolution. •  num_layers  ( int ) – Number of convolution layers. \n\n\n•  num up sample  ( int | optional ) – Number of upsampling layer. Must be no more than num_layers. Upsampling will be applied after the first  num up sample  layers of convolution. Default:  num_layers . \n•  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None, which means using conv2d. \n•  norm_cfg  ( dict ) – Config dict for normalization layer. Default: None. •  init_cfg  ( dict ) – Config dict for initialization. Default: None. •  kwargs  ( key word augments ) – Other augments used in ConvModule. \n\n\nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.utils. De tr Transformer Decoder ( \\*args ,  post norm cf g  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'LN'} , return intermediate  $=$  False ,  \\*\\*kwargs ) \nImplements the decoder in DETR transformer. \nParameters \n•  return intermediate  ( bool ) – Whether to return intermediate outputs. "}
{"page": 434, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_434.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* post_norm_cfg (dict) — Config of last normalization layer. Default LN.\n\nforward (query, “args, **kwargs)\nForward function for TransformerDecoder.\n\nParameters query (Tensor) — Input query with shape (num_query, bs, embed_dims).\nReturns\n\nResults with shape [1, num_query, bs, embed_dims] when return_intermediate is False,\notherwise it has shape [num_layers, num_query, bs, embed_dims].\n\nReturn type Tensor\n\nclass mmdet.models.utils.DetrTransformerDecoderLayer (attn_cfgs, feedforward_channels,\nSfn_dropout=0.0, operation_order=None,\nact_cfg=(‘inplace': True, 'type': 'ReLU'},\nnorm_cfg={ ‘type’: 'LN'}, ffn_num_fcs=2,\n**kwargs)\nImplements decoder layer in DETR transformer.\n\nParameters\n\n* attn_cfgs (list{mmcv.ConfigDict] | list{dict] | dict )) — Configs for self_attention or\ncross_attention, the order should be consistent with it in operation_order. If it is a dict,\nit would be expand to the number of attention in operation_order.\n\n¢ feedforward_channels (int) — The hidden dimension for FFNs.\n¢ £fn_dropout (float) — Probability of an element to be zeroed in ffn. Default 0.0.\n\n* operation_order (tuple[str]) — The execution order of operation in transformer. Such\nas (‘self_attn’, ‘norm’, ‘ffn’, ‘norm’). DefaultNone\n\n* act_cfg (dict) — The activation config for FFNs. Default: LN\n* norm_cfg (dict) — Config dict for normalization layer. Default: LN.\n¢ £f£n_num_fcs (int) — The number of fully-connected layers in FFNs. Default2.\n\nclass mmdet.models.utils.DynamicConv(in_channels=256, feat_channels=64, out_channels=None,\ninput_feat_shape=7, with_proj=True, act_cfg={‘inplace’: True,\n‘type’: 'ReLU'}, norm_cfg={'type': 'LN'}, init_cfg=None)\nImplements Dynamic Convolution.\n\nThis module generate parameters for each sample and use bmm to implement 1*1 convolution. Code is modified\nfrom the official github repo .\n\nParameters\n¢ in_channels (int) — The input feature channel. Defaults to 256.\n¢ feat_channels (int) — The inner feature channel. Defaults to 64.\n\n* out_channels (int, optional) -The output feature channel. When not specified, it will\nbe set to in_channels by default\n\n¢ input_feat_shape (int) — The shape of input feature. Defaults to 7.\n\n* with_proj (bool) — Project two-dimentional feature to one-dimentional feature. Default to\nTrue.\n\n* act_cfg (dict) — The activation config for DynamicConv.\n\n* norm_cfg (dict) — Config dict for normalization layer. Default layer normalization.\n\n39.7. utils 427\n", "vlm_text": "•  post norm cf g  ( dict ) – Config of last normalization layer. Default  LN . \nforward ( query ,  \\*args ,  \\*\\*kwargs ) Forward function for  Transformer Decoder . \n query Tensor \nReturns \nResults with shape [1, num_query, bs, embed_dims] when  return intermediate is  False , otherwise it has shape [num_layers, num_query, bs, embed_dims]. \nReturn type  Tensor \nclass  mmdet.models.utils. De tr Transformer Decoder Layer ( attn_cfgs ,  feed forward channels , ffn_dropou  $\\because0.0$  ,  operation order=None , act_cfg  $\\mathbf{\\hat{\\rho}}$  {'inplace': True, 'type': 'ReLU'} , norm_cfg  $=$  {'type': 'LN'} ,  ffn_num_fc  $\\cdot_{s=2}$  , \\*\\*kwargs ) \nImplements decoder layer in DETR transformer. \nParameters \n•  attn_cfgs  (list[ mmcv.ConfigDict ] | list[dict] | dict )) – Configs for self attention or cross attention, the order should be consistent with it in  operation order . If it is a dict, it would be expand to the number of attention in  operation order . •  feed forward channels  ( int ) – The hidden dimension for FFNs. •  ff n dropout  ( float ) – Probability of an element to be zeroed in ffn. Default 0.0. •  operation order  ( tuple[str] ) – The execution order of operation in transformer. Such as (‘self_attn’, ‘norm’, ‘ffn’, ‘norm’). Default None •  act_cfg  ( dict ) – The activation config for FFNs. Default:  LN •  norm_cfg  ( dict ) – Config dict for normalization layer. Default:  LN . •  ff n num fcs  ( int ) – The number of fully-connected layers in FFNs. Default2. \nclass  mmdet.models.utils. Dynamic Con v ( in_channel  $\\imath{=}256$  ,  feat channel  $\\scriptstyle s=64$  ,  out channel  $\\scriptstyle\\sum=$  None , input feat shape  $\\mathrm{\\Sigma=}7$  ,  with_proj  $\\leftleftarrows$  True ,  act_cfg  $=$  {'inplace': True, 'type': 'ReLU'} ,  norm_cfg  $=$  {'type': 'LN'} ,  init_cfg  $=$  None ) \nImplements Dynamic Convolution. \nThis module generate parameters for each sample and use bmm to implement   $1^{*}1$   convolution. Code is modified from the  official github repo  . \nParameters \n•  in channels  ( int ) – The input feature channel. Defaults to 256. •  feat channels  ( int ) – The inner feature channel. Defaults to 64. •  out channels  ( int, optional ) – The output feature channel. When not specified, it will be set to  in channels  by default •  input feat shape  ( int ) – The shape of input feature. Defaults to 7. •  with_proj  ( bool ) – Project two-di mention al feature to one-di mention al feature. Default to True. •  act_cfg  ( dict ) – The activation config for Dynamic Con v. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default layer normalization. "}
{"page": 435, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_435.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* (obj (init_cfg) —mmcv.ConfigDict): The Config for initialization. Default: None.\n\nforward (param_feature, input_feature)\nForward function for DynamicConv.\n\nParameters\n\n* param_feature (Tensor) — The feature can be used to generate the parameter, has shape\n(num_all_proposals, in_channels).\n\n¢ input_feature (Tensor) — Feature that interact with parameters, has shape\n(num_all_proposals, in_channels, H, W).\n\nReturns The output feature has shape (num_all_proposals, out_channels).\nReturn type Tensor\n\nclass mmdet.models.utils.InvertedResidual (in_channels, out_channels, mid_channels, kernel_size=3,\nstride=1, se_cfg=None, with_expand_conv=True,\nconv_cfg=None, norm_cfg=('type': 'BN'}, act_cfg={ ‘type’:\n'ReLU'}, with_cp=False, init_cfg=None)\nInverted Residual Block.\n\nParameters\n¢ in_channels (int) — The input channels of this Module.\n* out_channels (int) — The output channels of this Module.\n* mid_channels (int) — The input channels of the depthwise convolution.\n¢ kernel_size (int) — The kernel size of the depthwise convolution. Default: 3.\n* stride (int) — The stride of the depthwise convolution. Default: 1.\n* se_cfg (dict) — Config dict for se layer. Default: None, which means no se layer.\n\n* with_expand_conv (bool) — Use expand conv or not. If set False, mid_channels must be\nthe same with in_channels. Default: True.\n\n* conv_cfg (dict) — Config dict for convolution layer. Default: None, which means using\nconv2d.\n\n* norm_cfg (dict) — Config dict for normalization layer. Default: dict(type=’ BN’).\n* act_cfg (dict) — Config dict for activation layer. Default: dict(type=’ReLU’).\n\n* with_cp (bool) - Use checkpoint or not. Using checkpoint will save some memory while\nslowing down the training speed. Default: False.\n\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\nReturns The output tensor.\nReturn type Tensor\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\n428 Chapter 39. mmdet.models\n", "vlm_text": "•  (obj  ( init_cfg ) –  mmcv.ConfigDict ): The Config for initialization. Default: None. \nforward ( param feature ,  input feature ) Forward function for  Dynamic Con v . \nParameters \n•  param feature  ( Tensor ) – The feature can be used to generate the parameter, has shape (num all proposals, in channels). •  input feature  ( Tensor ) – Feature that interact with parameters, has shape (num all proposals, in channels, H, W). \nReturns  The output feature has shape (num all proposals, out channels). \nReturn type  Tensor \nclass  mmdet.models.utils. Inverted Residual ( in channels ,  out channels ,  mid channels ,  kernel_siz  $\\scriptstyle{\\stackrel{\\prime}{=}}3$  , stride  $\\scriptstyle{:=I}$  ,  se_cfg  $\\mathbf{\\beta}=$  None ,  with expand con v  $\\mathbf{\\tilde{\\Sigma}}$  True , conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $=$  {'type': 'BN'} ,  act_cfg  $\\mathbf{\\beta}=$  {'type': 'ReLU'} ,  with_cp=False ,  init_cfg  $\\mathbf{\\Pi}^{\\prime}\\mathbf{=}$  None ) \nInverted Residual Block. \nParameters \n•  in channels  ( int ) – The input channels of this Module. •  out channels  ( int ) – The output channels of this Module. •  mid channels  ( int ) – The input channels of the depthwise convolution. •  kernel size  ( int ) – The kernel size of the depthwise convolution. Default: 3. •  stride  ( int ) – The stride of the depthwise convolution. Default: 1. •  se_cfg  ( dict ) – Config dict for se layer. Default: None, which means no se layer. •  with expand con v  ( bool ) – Use expand conv or not. If set False, mid channels must be the same with in channels. Default: True. •  conv_cfg  ( dict ) – Config dict for convolution layer. Default: None, which means using conv2d. •  norm_cfg  ( dict ) – Config dict for normalization layer. Default: dict  $\\scriptstyle(\\mathrm{type}={\\mathrm{BN}}^{\\prime})$  ). •  act_cfg  ( dict ) – Config dict for activation layer. Default: dict(type  $\\fallingdotseq$  ReLU’). •  with_cp  ( bool ) – Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nReturns  The output tensor. \nReturn type  Tensor \n\nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. "}
{"page": 436, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_436.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nclass mmdet.models.utils.LearnedPositionalEncoding(num_feats, row_num_embed=50,\ncol_num_embed=S0, init_cfg=('‘layer':\n‘Embedding’, 'type': 'Uniform'})\nPosition embedding with learnable embedding weights.\n\nParameters\n\n* num_feats (int) — The feature dimension for each position along x-axis or y-axis. The final\nreturned dimension for each position is 2 times of this value.\n\n* row_num_embed (int, optional) — The dictionary size of row embeddings. Default 50.\n* col_num_embed (int, optional) -— The dictionary size of col embeddings. Default 50.\n¢ init_cfg (dict or list[dict], optional) — Initialization config dict.\n\nforward (mask)\nForward function for LearnedPositionalEncoding.\n\nParameters mask (Tensor) — ByteTensor mask. Non-zero values representing ignored posi-\ntions, while zero values means valid positions for this image. Shape [bs, h, w].\n\nReturns\nReturned position embedding with shape [bs, num_feats*2, h, w].\nReturn type pos (Tensor)\n\nclass mmdet.models.utils.NormedConv2d( “args, tempearture=20, power=1.0, eps=1e-06,\nnorm_over_kernel=False, **kwargs)\nNormalized Conv2d Layer.\n\nParameters\n* tempeature (float, optional) -—Tempeature term. Default to 20.\n* power (int, optional) — Power term. Default to 1.0.\n\n* eps(float, optional)—The minimal value of divisor to keep numerical stability. Default\nto le-6.\n\n* norm_over_kernel (bool, optional) — Normalize over kernel. Default to False.\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.utils.NormedLinear ( “args, tempearture=20, power=1.0, eps=1e-06, **kwargs)\nNormalized Linear Layer.\n\nParameters\n* tempeature (float, optional) -—Tempeature term. Default to 20.\n* power (int, optional) — Power term. Default to 1.0.\n\n* eps(float, optional)—The minimal value of divisor to keep numerical stability. Default\nto le-6.\n\n39.7. utils 429\n", "vlm_text": "class  mmdet.models.utils. Learned Positional Encoding ( num_feats ,  row num embed  $\\iota{=}50$  , col num embed  $l{=}50$  ,  init_cfg  $=_{i}$  {'layer': 'Embedding', 'type': 'Uniform'} ) \nPosition embedding with learnable embedding weights. \nParameters \n•  num_feats  ( int ) – The feature dimension for each position along x-axis or y-axis. The final returned dimension for each position is 2 times of this value. •  row num embed  ( int, optional ) – The dictionary size of row embeddings. Default 50. •  col num embed  ( int, optional ) – The dictionary size of col embeddings. Default 50. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. \nforward ( mask ) Forward function for  Learned Positional Encoding \nParameters  mask  ( Tensor ) – ByteTensor mask. Non-zero values representing ignored posi- tions, while zero values means valid positions for this image. Shape [bs, h, w]. \nReturns \nReturned position embedding with shape  [bs, num_feats\\*2, h, w]. Return type  pos (Tensor) \nclass  mmdet.models.utils. Norm edC on v 2 d ( \\*args ,  tempe artur e=20 ,  power  $=$  1.0 ,  eps=1e-06 , norm over kernel  $\\leftrightharpoons$  False ,  \\*\\*kwargs ) \nParameters \n•  tempeature  ( float, optional ) – Tempeature term. Default to 20. •  power  ( int, optional ) – Power term. Default to 1.0. •  eps  ( float, optional ) – The minimal value of divisor to keep numerical stability. Default to 1e-6. •  norm over kernel  ( bool, optional ) – Normalize over kernel. Default to False. \nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.utils. Norm ed Linear ( \\*args ,  tempe artur e  $\\mathrm{=}20$  ,  power=1.0 ,  eps=1e-06 ,  \\*\\*kwargs ) Normalized Linear Layer. \nParameters \n•  tempeature  ( float, optional ) – Tempeature term. Default to 20. •  power  ( int, optional ) – Power term. Default to 1.0. •  eps  ( float, optional ) – The minimal value of divisor to keep numerical stability. Default to 1e-6. "}
{"page": 437, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_437.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward(x)\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.utils.PatchEmbed (in_channels=3, embed_dims=768, conv_type='Conv2d',\nkernel_size=16, stride=16, padding='corner', dilation=1, bias=True,\nnorm_cfg=None, input_size=None, init_cfg=None)\nImage to Patch Embedding.\n\nWe use a conv layer to implement PatchEmbed.\nParameters\n¢ in_channels (int) — The num of input channels. Default: 3\n* embed_dims (int) — The dimensions of embedding. Default: 768\n\n* conv_type (str) — The config dict for embedding conv layer type selection. Default:\n“Conv2d.\n\n¢ kernel_size (int) — The kernel_size of embedding conv. Default: 16.\n\n* stride (int) — The slide stride of embedding conv. Default: None (Would be set as ker-\nnel_size).\n\n* padding (int | tuple | string) - The padding length of embedding conv. When it is\na string, it means the mode of adaptive padding, support “same” and “corner” now. Default:\n“corner”.\n\n¢ dilation (int) — The dilation rate of embedding conv. Default: 1.\n* bias (bool) — Bias of embed conv. Default: True.\n* norm_cfg (dict, optional) — Config dict for normalization layer. Default: None.\n\n¢ input_size (int | tuple | None) — The size of input, which will be used to calculate\nthe out size. Only work when dynamic_size is False. Default: None.\n\n¢ init_cfg (mmcv.ConfigDict, optional) — The Config for initialization. Default: None.\n\nforward(x)\n\nParameters x (Tensor) — Has shape (B, C, H, W). In most case, C is 3.\nReturns\n\nContains merged results and its spatial shape.\n\n* x (Tensor): Has shape (B, out_h * out_w, embed_dims)\n\n* out_size (tuple[int]): Spatial shape of x, arrange as (out_h, out_w).\nReturn type tuple\n\nclass mmdet.models.utils.ResLayer (block, inplanes, planes, num_blocks, stride=1, avg_down=False,\nconv_cfg=None, norm_cfg=('type': 'BN'}, downsample_first=True,\n**kwargs)\nResLayer to build ResNet style backbone.\n\n430 Chapter 39. mmdet.models\n", "vlm_text": "forward  $(x)$  Defines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.utils. PatchEmbed ( in_channel  $\\mathfrak{s}{=}3$  ,  embed_dim  $\\imath{=}768$  ,  conv_t  $\\scriptstyle{y e=C o n\\nu2d^{\\prime}}$  , kernel_siz  $\\scriptstyle{\\mathit{\\Sigma}}=I6$  ,  stride  $\\mathrm{\\Sigma=}l6$  ,  padding  $\\mathbf{\\dot{\\Sigma}}$  'corner' ,  dilation  $\\mathrm{\\Sigma}_{:=I}$  ,  bias  $\\mathbf{=}$  True , norm_cfg  $\\leftrightharpoons$  None ,  input_size  $=$  None ,  init_cfg  $\\leftrightharpoons$  None ) \nImage to Patch Embedding. \nWe use a conv layer to implement PatchEmbed. \nParameters \n•  in channels  ( int ) – The num of input channels. Default: 3 •  embed_dims  ( int ) – The dimensions of embedding. Default: 768 •  conv_type  ( str ) – The config dict for embedding conv layer type selection. Default: “Conv2d. •  kernel size  ( int ) – The kernel size of embedding conv. Default: 16. •  stride  ( int ) – The slide stride of embedding conv. Default: None (Would be set as  ker- nel_size ). •  padding  ( int | tuple | string ) – The padding length of embedding conv. When it is a string, it means the mode of adaptive padding, support “same” and “corner” now. Default: “corner”. •  dilation  ( int ) – The dilation rate of embedding conv. Default: 1. •  bias  ( bool ) – Bias of embed conv. Default: True. •  norm_cfg  ( dict, optional ) – Config dict for normalization layer. Default: None. •  input_size  ( int | tuple | None ) – The size of input, which will be used to calculate the out size. Only work when  dynamic size  is False. Default: None. •  init_cfg  ( mmcv.ConfigDict , optional) – The Config for initialization. Default: None. \nforward ( x ) \nParameters  x  ( Tensor ) – Has shape (B, C, H, W). In most case, C is 3. \nReturns \nContains merged results and its spatial shape. • x (Tensor): Has shape (B, out_h \\* out_w, embed_dims) •  out_size (tuple[int]): Spatial shape of x, arrange as  (out_h, out_w). \nclass  mmdet.models.utils. ResLayer ( block ,  inplanes ,  planes ,  num_blocks ,  stride  $\\scriptstyle{\\varepsilon=I}$  ,  avg_down  $=$  False , conv_cfg  $=$  None ,  norm_cfg  $=$  {'type': 'BN'} ,  down sample first=True , \\*\\*kwargs ) \nResLayer to build ResNet style backbone. "}
{"page": 438, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_438.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n\nblock (nn. Module) — block used to build ResLayer.\ninplanes (int) — inplanes of block.\n\nplanes (int) — planes of block.\n\nnum_blocks (int) — number of blocks.\n\nstride (int) — stride of the first block. Default: 1\n\navg_down (bool) — Use AvgPool instead of stride conv when downsampling in the bottle-\nneck. Default: False\n\nconv_cfg (dict) — dictionary to construct and config conv layer. Default: None\nnorm_cfg (dict) — dictionary to construct and config norm layer. Default: dict(type=’ BN’)\n\ndownsample_first (bool) — Downsample at the first block or last block. False for Hour-\nglass, True for ResNet. Default: True\n\nclass mmdet.models.utils.SELayer (channels, ratio=16, conv_cfg=None, act_cfg=({'type': 'ReLU’}, {'type':\n\n‘Sigmoid'}), init_cfg=None)\n\nSqueeze-and-Excitation Module.\n\nParameters\n\nforward(x)\n\nchannels (int) — The input (and output) channels of the SE layer.\n\nratio (int) — Squeeze ratio in SELayer, the intermediate channel will be int (channels/\nratio). Default: 16.\n\nconv_cfg (None or dict) — Config dict for convolution layer. Default: None, which\nmeans using conv2d.\n\nact_cfg (dict or Sequence[dict]) — Config dict for activation layer. If act_cfg is a\ndict, two activation layers will be configurated by this dict. If act_cfg is a sequence of dicts,\nthe first activation layer will be configurated by the first dict and the second activation layer\nwill be configurated by the second dict. Default: (dict(type=’ReLU’), dict(type=’Sigmoid’))\n\ninit_cfg(dict or list[dict], optional) —Initialization config dict. Default: None\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote: Although the recipe for forward pass needs to be defined within this function, one should call the\nModule instance afterwards instead of this since the former takes care of running the registered hooks while\nthe latter silently ignores them.\n\nclass mmdet.models.utils.SimplifiedBasicBlock (inplanes, planes, stride=1, dilation=1,\n\ndownsample=None, style='pytorch', with_cp=False,\n\nconv_cfg=None, norm_cfg=('type': 'BN'}, dcn=None,\n\nplugins=None, init_fg=None)\n\nSimplified version of original basic residual block. This is used in SCNet.\n\n¢ Norm layer is now optional\n\n¢ Last ReLU in forward function is removed\n\n39.7. utils\n\n431\n", "vlm_text": "Parameters \n•  block  ( nn.Module ) – block used to build ResLayer. •  inplanes  ( int ) – inplanes of block. •  planes  ( int ) – planes of block. •  num_blocks  ( int ) – number of blocks. •  stride  ( int ) – stride of the first block. Default: 1 •  avg_down  ( bool ) – Use AvgPool instead of stride conv when down sampling in the bottle- neck. Default: False •  conv_cfg  ( dict ) – dictionary to construct and config conv layer. Default: None •  norm_cfg  ( dict ) – dictionary to construct and config norm layer. Default: dict(type  $=^{!}$  ’BN’) •  down sample first  ( bool ) – Downsample at the first block or last block. False for Hour- glass, True for ResNet. Default: True \nclass  mmdet.models.utils. SELayer ( channels ,  ratio  $\\scriptstyle\\prime=$  16 ,  conv_cfg  $=$  None ,  act_cfg  $=$  ({'type': 'ReLU'}, {'type': 'Sigmoid'}) ,  init_cfg  $=$  None ) \nSqueeze-and-Excitation Module. \nParameters \n•  channels  ( int ) – The input (and output) channels of the SE layer. •  ratio  ( int ) – Squeeze ratio in SELayer, the intermediate channel will be  int(channels/ ratio) . Default: 16. •  conv_cfg  ( None or dict ) – Config dict for convolution layer. Default: None, which means using conv2d. •  act_cfg  ( dict or Sequence[dict] ) – Config dict for activation layer. If act_cfg is a dict, two activation layers will be config u rated by this dict. If act_cfg is a sequence of dicts, the first activation layer will be config u rated by the first dict and the second activation layer will be config u rated by the second dict. Default: (dict(type  $='$  ’ReLU’), dict(type  $\\fallingdotseq$  Sigmoid’)) •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward  $(x)$  \nDefines the computation performed at every call. Should be overridden by all subclasses. \nNote:  Although the recipe for forward pass needs to be defined within this function, one should call the Module  instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. \nclass  mmdet.models.utils. Simplified Basic Block ( inplanes ,  planes ,  stride  $\\scriptstyle{:=I}$  ,  dilation  $\\scriptstyle{=I}$  , downsample  $\\mathbf{\\Pi}^{*}=$  None ,  style  $=$  'pytorch' ,  with_cp  $\\leftrightharpoons$  False , conv_cfg  $\\mathbf{\\hat{\\Sigma}}$  None ,  norm_cfg  $\\mathbf{\\dot{\\Sigma}}$  {'type': 'BN'} ,  dcn  $=$  None , plugins  $\\mathbf{\\hat{\\rho}}$  None ,  init_fg  $=$  None ) \nSimplified version of original basic residual block. This is used in  SCNet . \n• Norm layer is now optional • Last ReLU in forward function is removed "}
{"page": 439, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_439.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward(x)\nForward function.\n\nproperty normi\nnormalization layer after the first convolution layer\n\nType nn.Module\n\nproperty norm2\nnormalization layer after the second convolution layer\n\nType nn.Module\n\nclass mmdet.models.utils.SinePositionalEncoding (num_feats, temperature=10000, normalize=False,\nscale=6.283 185307179586, eps=1e-06, offset=0.0,\ninit_cfg=None)\nPosition encoding with sine and cosine functions.\n\nSee End-to-End Object Detection with Transformers for details.\nParameters\n\n* num_feats (int) — The feature dimension for each position along x-axis or y-axis. Note the\nfinal returned dimension for each position is 2 times of this value.\n\n* temperature (int, optional) — The temperature used for scaling the position embed-\nding. Defaults to 10000.\n\n* normalize (bool, optional) — Whether to normalize the position embedding. Defaults\nto False.\n\n* scale (float, optional) —A scale factor that scales the position embedding. The scale\nwill be used only when normalize is True. Defaults to 2*pi.\n\n* eps (float, optional) -— A value added to the denominator for numerical stability. De-\nfaults to le-6.\n\n* offset (float) — offset add to embed when do the normalization. Defaults to 0.\n¢ init_cfg(dict or list[dict], optional)-—Initialization config dict. Default: None\n\nforward (mask)\nForward function for SinePositionalEncoding.\n\nParameters mask (Tensor) — ByteTensor mask. Non-zero values representing ignored posi-\ntions, while zero values means valid positions for this image. Shape [bs, h, w].\n\nReturns\nReturned position embedding with shape [bs, num_feats*2, h, w].\nReturn type pos (Tensor)\n\nclass mmdet.models.utils.Transformer (encoder=None, decoder=None, init_cfg=None)\nImplements the DETR transformer.\n\nFollowing the official DETR implementation, this module copy-paste from torch.nn.Transformer with modifica-\ntions:\n\n* positional encodings are passed in MultiheadAttention\n* extra LN at the end of encoder is removed\n* decoder returns a stack of activations from all decoding layers\n\nSee paper: End-to-End Object Detection with Transformers for details.\n\n432 Chapter 39. mmdet.models\n", "vlm_text": "forward  $(x)$  Forward function. \nproperty norm1 normalization layer after the first convolution layer \nType  nn.Module \n\nnormalization layer after the second convolution layer \nType  nn.Module \nclass  mmdet.models.utils. Sine Positional Encoding ( num_feats ,  temperature  $\\mathbf{\\beta}=$  10000 ,  normalize  $=$  False , scale=6.283185307179586 ,  eps=1e-06 ,  offset=0.0 , init_cfg  $\\mathbf{\\beta}=$  None ) \nPosition encoding with sine and cosine functions. \nSee  End-to-End Object Detection with Transformers  for details. \nParameters \n•  num_feats  ( int ) – The feature dimension for each position along x-axis or y-axis. Note the final returned dimension for each position is 2 times of this value. •  temperature  ( int, optional ) – The temperature used for scaling the position embed- ding. Defaults to 10000. •  normalize  ( bool, optional ) – Whether to normalize the position embedding. Defaults to False. •  scale  ( float, optional ) – A scale factor that scales the position embedding. The scale will be used only when  normalize  is True. Defaults to   $2^{*}\\mathrm{\\textmu}$  . •  eps  ( float, optional ) – A value added to the denominator for numerical stability. De- faults to 1e-6. •  offset  ( float ) – offset add to embed when do the normalization. Defaults to 0. •  init_cfg  ( dict or list[dict], optional ) – Initialization config dict. Default: None \nforward ( mask ) \n\nParameters  mask  ( Tensor ) – ByteTensor mask. Non-zero values representing ignored posi- tions, while zero values means valid positions for this image. Shape [bs, h, w]. Returns Returned position embedding with shape  [bs, num_feats\\*2, h, w]. Return type  pos (Tensor) \nclass  mmdet.models.utils. Transformer ( encoder  $=$  None ,  decoder  $\\leftrightharpoons$  None ,  init_cfg  $\\mathbf{\\beta}=\\mathbf{\\beta}$  None ) Implements the DETR transformer. \nFollowing the official DETR implementation, this module copy-paste from torch.nn.Transformer with modifica- tions: \n• positional encodings are passed in Multi head Attention • extra LN at the end of encoder is removed • decoder returns a stack of activation s from all decoding layers \nSee  paper: End-to-End Object Detection with Transformers  for details. "}
{"page": 440, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_440.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nParameters\n* encoder (mmcv.ConfigDict | Dict) — Config of TransformerEncoder. Defaults to None.\n* decoder ((mmcv.ConfigDict | Dict)) — Config of TransformerDecoder. Defaults to None\n* (obj (init_cfg) — mmcv.ConfigDict): The Config for initialization. Defaults to None.\n\nforward (x, mask, query_embed, pos_embed)\nForward function for Transformer.\n\nParameters\n¢ x (Tensor) — Input query with shape [bs, c, h, w] where c = embed_dims.\n\n¢ mask (Tensor) — The key_padding_mask used for encoder and decoder, with shape [bs, h,\nwl.\n\n* query_embed (Tensor) — The query embedding for decoder, with shape [num_query, c].\n\n* pos_embed (Tensor) — The positional encoding for encoder and decoder, with the same\nshape as x.\n\nReturns\nresults of decoder containing the following tensor.\n\n* out_dec: Output from decoder. If return_intermediate_dec is True output has shape [num_dec_layers, bs.\nnum_query, embed_dims], else has shape [1, bs, num_query, embed_dims].\n\n* memory: Output results from encoder, with shape [bs, embed_dims, h, w].\nReturn type tuple[Tensor]\n\ninit_weights()\nInitialize the weights.\n\nmndet .models.utils.adaptive_avg_pool2d (input, output_size)\nHandle empty batch dimension to adaptive_avg_pool2d.\n\nParameters\n¢ input (tensor) — 4D tensor.\n* output_size (int, tuple[int,int]) — the target output size.\n\nmmndet .models.utils.build_linear_layer (cfg, “args, **kwargs)\nBuild linear layer. :param cfg: The linear layer config, which should contain:\n\n* type (str): Layer type.\n\n* layer args: Args needed to instantiate an linear layer.\n\nParameters\n\n* args (argument list) -— Arguments passed to the __init__ method of the corresponding\nlinear layer.\n\n¢ kwargs (keyword arguments) — Keyword arguments passed to the __init__ method of the\ncorresponding linear layer.\n\nReturns Created linear layer.\nReturn type nn.Module\n\nmmdet .models.utils.build_transformer (cfg, default_args=None)\nBuilder for Transformer.\n\n39.7. utils 433\n", "vlm_text": "Parameters \n•  encoder  ( mmcv.ConfigDict  | Dict) – Config of Transformer Encoder. Defaults to None. •  decoder  (( mmcv.ConfigDict  | Dict)) – Config of Transformer Decoder. Defaults to None •  (obj  ( init_cfg ) –  mmcv.ConfigDict ): The Config for initialization. Defaults to None. \nforward ( x ,  mask ,  query embed ,  pos_embed ) Forward function for  Transformer . \nParameters \n•  x  ( Tensor ) – Input query with shape [bs, c, h, w] where  $\\mathbf{c}=$   embed_dims. •  mask  ( Tensor ) – The key padding mask used for encoder and decoder, with shape [bs, h, w].•  query embed  ( Tensor ) – The query embedding for decoder, with shape [num_query, c]. •  pos_embed  ( Tensor ) – The positional encoding for encoder and decoder, with the same shape as  $x$  . \nReturns \nresults of decoder containing the following tensor. •  out_dec: Output from decoder. If return intermediate dec is True output has shape [num dec layers, bs, num_query, embed_dims], else has shape [1, bs, num_query, embed_dims]. • memory: Output results from encoder, with shape [bs, embed_dims, h, w]. \nReturn type  tuple[Tensor] \nin it weights()Initialize the weights. \nmmdet.models.utils.adaptive avg pool 2 d(input, output size)Handle empty batch dimension to adaptive avg pool 2 d. \nParameters \n•  input  ( tensor ) – 4D tensor. •  output size  ( int, tuple[int,int] ) – the target output size. \nmmdet.models.utils. build linear layer ( cfg ,  \\*args ,  \\*\\*kwargs ) Build linear layer. :param cfg: The linear layer config, which should contain: • type (str): Layer type. • layer args: Args needed to instantiate an linear layer. \nParameters \n•  args  ( argument list ) – Arguments passed to the  __init__  method of the corresponding linear layer. •  kwargs  ( keyword arguments ) – Keyword arguments passed to the  __init__  method of the corresponding linear layer. \nReturns  Created linear layer. \nReturn type  nn.Module \nmmdet.models.utils. build transformer ( cfg ,  default arg s  $\\mathbf{\\hat{\\rho}}$  None ) Builder for Transformer. "}
{"page": 441, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_441.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmndet .models.utils.gaussian_radius (det_size, min_overlap)\nGenerate 2D gaussian radius.\n\nThis function is modified from the official github repo.\nGiven min_overlap, radius could computed by a quadratic equation according to Vieta’s formulas.\nThere are 3 cases for computing gaussian radius, details are following:\n\n¢ Explanation of figure: 1t and br indicates the left-top and bottom-right corner of ground truth box. x\nindicates the generated corner at the limited position when radius=r.\n\n* Casel: one corner is inside the gt box and the other is outside.\n\n< width >\nt-+---------- + -\n|\n+--X---------- +--+\n| |\n| | height\n| overlap |\n| |\n| | v\n4+--4--------- br--+ -\n| |\n4+---------- +--X\n\nTo ensure IoU of generated box and gt box is larger than min_overlap:\n\n(w —r) *(h—r) 2 1—iou\n\n>i aid + h)r 4 vxh>0\n\nweh+(w+hr—re~ rs (w+ hr T+iou 9 *\"=\n\n1—iou —b- Vb? —4eaxc\n\na=1, b=-(w+h), ¢ — *« w * hr < —\n1+ iou 2*a\n\n* Case2: both two corners are inside the gt box.\n\n< width >\nt-+---------- + -\n|\n+--X------- +\n| |\n|overlap | height\n| |\n4+------- X--+\n| v\nt---------- +-br -\n\nTo ensure IoU of generated box and gt box is larger than min_overlap:\n\n(w—2*r)*«(h—2*r)\n\nweh\n\n> iou 4r? — 2(w + h)r + (1—iou) #w*h>0\n\n—b- Vb? —4*axc\n\n4, b=-AXwth), c=(1-i yxhr <\na=4, (w ), c= (1-iou) *w*hr < yea\n\n* Case3: both two comers are outside the gt box.\n\n434 Chapter 39. mmdet.models\n", "vlm_text": "mmdet.models.utils. gaussian radius ( det_size ,  min overlap ) Generate 2D gaussian radius. \nThis function is modified from the  official github repo . Given  min overlap , radius could computed by a quadratic equation according to Vieta’s formulas. There are 3 cases for computing gaussian radius, details are following: \n\n\n• Explanation of figure:  lt  and  br  indicates the left-top and bottom-right corner of ground truth box.  x indicates the generated corner at the limited position when  radius  $\\tt\\lrcorner\\mathrm{\\bfr}$  . \n• Case1: one corner is inside the gt box and the other is outside. \nThe image contains an ASCII diagram representing a rectangle, labeled with dimensions and symbols indicating various aspects of the shape. Here's a breakdown of the elements:\n\n1. **Width**: Indicated by a horizontal line at the top of the rectangle, with the label `width` and arrows pointing left (`<|`) and right (`|>`).\n\n2. **Height**: Denoted by a vertical line next to the rectangle, with a label `height` and arrows pointing up (`^`) and down (`v`).\n\n3. **Points and Symbols**:\n   - `lt` and `br`: These labels likely stand for the top-left and bottom-right corners of the rectangle.\n   - `x`: Occurs twice, suggesting it marks specific reference points along the top and bottom of the rectangle.\n   - `overlap`: Positioned in the center of the rectangle, possibly indicating shared or intersecting space.\n\n4. **Different Line Styles**: The diagram uses vertical (`|`), horizontal (`-`), and corner (`+`) lines to shape the rectangle and indicate structure.\n\nOverall, the image is a visual representation of a rectangle, highlighting its width, height, and particular points or features within it.\nTo ensure IoU of generated box and gt box is larger than  min overlap : \n\n$$\n\\frac{(w-r)*(h-r)}{w*h+(w+h)r-r^{2}}\\geq i o u\\quad\\Rightarrow\\quad r^{2}-(w+h)r+\\frac{1-i o u}{1+i o u}*w*h\\geq0\n$$\n \n\n$$\na=1,\\quad b=-(w+h),\\quad c=\\frac{1-i o u}{1+i o u}*w*h r\\leq\\frac{-\\;b-\\sqrt{b^{2}-4*a*c}}{2*a}\n$$\n \n• Case2: both two corners are inside the gt box. \nThe image is an ASCII diagram illustrating two overlapping rectangles. The key elements are:\n\n- The outer rectangle is defined by \"lt\" (left top) and \"br\" (bottom right) corners.\n- There is a smaller rectangle inside indicated by \"+\" and \"x\" symbols.\n- The text \"overlap\" is within the overlapping section of the inner rectangle.\n- Dimensions labeled as \"width\" and \"height\" with arrows indicate the size or direction.\nTo ensure IoU of generated box and gt box is larger than  min overlap : \n\n$$\n\\begin{array}{r}{\\displaystyle\\frac{(w-2*r)*(h-2*r)}{w*h}\\geq i o u\\quad\\Rightarrow\\quad4r^{2}-2(w+h)r+(1-i o u)*w*h\\geq0}\\\\ {\\displaystyle a=4,\\quad b=-2(w+h),\\quad c=(1-i o u)*w*h r\\leq\\frac{-\\displaystyle b-\\sqrt{b^{2}-4*a*c}}{2*a}}\\end{array}\n$$\n \n• Case3: both two corners are outside the gt box. "}
{"page": 442, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_442.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n< width >\n\nX--+------------- ---+\n\n|\n\n+-1t------------- + -\n\n|\n\n|\n\n| overlap height\n|\n\n| v\n\n| +------------ br--+ -\n\n|\n\n4+---------------- +--X\nTo ensure IoU of generated box and gt box is larger than min_overlap:\n\nwh s\n(w+2*r)*(h+2*r)~\n\niou Axiouxr? +2% iou* (w+ h)r + (iou-1)xweh <0\n\na=4x*iou, b=2*ioux(wth), c=(iow-l1)*w*h\n\n—b+ Vb? -—4ax*c\n\n2*a\n\nr<\n\nParameters\n* det_size (list [int]) — Shape of object.\n\n* min_overlap (float) -— Min IoU with ground truth for boxes generated by keypoints inside\nthe gaussian kernel.\n\nReturns Radius of gaussian kernel.\nReturn type radius (int)\n\nmmdet .models.utils.gen_gaussian_target (heatmap, center, radius, k=1)\nGenerate 2D gaussian heatmap.\n\nParameters\n\n¢ heatmap (Tensor) — Input heatmap, the gaussian kernel will cover on it and maintain the\nmax value.\n\n* center (list [int ]) — Coord of gaussian kernel’s center.\n* radius (int) — Radius of gaussian kernel.\n¢ k (int) — Coefficient of gaussian kernel. Default: 1.\nReturns Updated heatmap covered by gaussian kernel.\nReturn type out_heatmap (Tensor)\n\nmmndet .models.utils.interpolate_as (source, target, mode='bilinear', align_corners=False)\nInterpolate the source to the shape of the target.\n\nThe source must be a Tensor, but the target can be a Tensor or a np.ndarray with the shape (..., target_h, target_w).\nParameters\n* source (Tensor) — A 3D/4D Tensor with the shape (N, H, W) or (N, C, H, W).\n\n* target (Tensor | np.ndarray) — The interpolation target with the shape (..., target_h,\ntarget_w).\n\n39.7. utils 435\n\n", "vlm_text": "The image is a diagram illustrating two overlapping rectangles with labels and dimensions. The outer rectangle has a width and height marked, while there's an overlapping inner rectangle labeled \"overlap\" with points \"lt\" and \"br\". The diagram includes directional arrows and measurements, with \"x\" indicating endpoints on the horizontal axis. The labels and symbols are likely used for indicating geometric properties or layout constraints.\nTo ensure IoU of generated box and gt box is larger than  min overlap : \n\n$$\n\\begin{array}{r l r}{\\displaystyle\\frac{w*h}{w+2*r)*(h+2*r)}\\geq i o u}&{{}\\Rightarrow}&{4*i o u*r^{2}+2*i o u*(w+h)r+(i o u-1)*w*h\\leq0}\\\\ {\\displaystyle}&{{}}&{a=4*i o u,\\quad b=2*i o u*(w+h),\\quad c=(i o u-1)*w*h}\\\\ {\\displaystyle}&{{}}&{r\\leq\\displaystyle\\frac{-b+\\sqrt{b^{2}-4*a*c}}{2*a}}\\end{array}\n$$\n \nParameters \n•  det_size  ( list[int] ) – Shape of object. •  min overlap  ( float ) – Min IoU with ground truth for boxes generated by keypoints inside the gaussian kernel. \nReturns  Radius of gaussian kernel. \nReturn type  radius (int) \nmmdet.models.utils. gen gaussian target ( heatmap ,  center ,  radius ,  $k{=}I$  ) Generate 2D gaussian heatmap. \nParameters \n•  heatmap  ( Tensor ) – Input heatmap, the gaussian kernel will cover on it and maintain the max value. •  center  ( list[int] ) – Coord of gaussian kernel’s center. •  radius  ( int ) – Radius of gaussian kernel. •  k  ( int ) – Coefficient of gaussian kernel. Default: 1. \nReturns  Updated heatmap covered by gaussian kernel. \nReturn type  out heat map (Tensor) \nmmdet.models.utils. interpolate as ( source ,  target ,  mode  $\\mathbf{=}$  'bilinear' ,  align corners  $\\mathbf{\\hat{=}}$  False ) Interpolate the  source  to the shape of the  target . \nThe  source  must be a Tensor, but the  target  can be a Tensor or a np.ndarray with the shape (..., target_h, target_w). \nParameters \n•  source  ( Tensor ) – A 3D/4D Tensor with the shape (N, H, W) or (N, C, H, W). •  target  ( Tensor | np.ndarray ) – The interpolation target with the shape (..., target_h, target_w). "}
{"page": 443, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_443.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n* mode (str) — Algorithm used for interpolation. The options are the same as those in\n\nFinterpolate(). Default: 'bilinear'.\n* align_corners (bool) — The same as the argument in F.interpolate().\nReturns The interpolated source Tensor.\nReturn type Tensor\n\nmmdet .models.utils.make_divisible (value, divisor, min_value=None, min_ratio=0.9)\nMake divisible function.\n\nThis function rounds the channel number to the nearest value that can be divisible by the divisor. It is taken from\nthe original tf repo. It ensures that all layers have a channel number that is divisible by divisor. It can be seen\nhere: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py # noqa\n\nParameters\n* value (int) — The original channel number.\n\n¢ divisor (int) — The divisor to fully divide the channel number.\n\n* min_value (int) — The minimum value of the output channel. Default: None, means that\n\nthe minimum value equal to the divisor.\n\n* min_ratio (float) — The minimum ratio of the rounded channel number to the original\n\nchannel number. Default: 0.9.\nReturns The modified output channel number.\nReturn type int\n\nmmndet .models.utils.nchw_to_nlc(x)\nFlatten [N, C, H, W] shape tensor to [N, L, C] shape tensor.\n\nParameters x (Tensor) — The input tensor of shape [N, C, H, W] before conversion.\nReturns The output tensor of shape [N, L, C] after conversion.\nReturn type Tensor\n\nmmndet .models.utils.nlc_to_nchw(x, hw_shape)\nConvert [N, L, C] shape tensor to [N, C, H, W] shape tensor.\n\nParameters\n\n* x (Tensor) — The input tensor of shape [N, L, C] before conversion.\n\n¢ hw_shape (Sequence [int ]) — The height and width of output feature map.\nReturns The output tensor of shape [N, C, H, W] after conversion.\n\nReturn type Tensor\n\n436 Chapter 39.\n\nmmdet.models\n", "vlm_text": "•  mode  ( str ) – Algorithm used for interpolation. The options are the same as those in F.interpolate(). Default: 'bilinear'.\n•  align corners  ( bool ) – The same as the argument in F.interpolate(). \nReturns  The interpolated source Tensor. \nReturn type  Tensor \nmmdet.models.utils. make divisible ( value ,  divisor ,  min_value  $=$  None ,  min_ratio=0.9 ) Make divisible function. \nThis function rounds the channel number to the nearest value that can be divisible by the divisor. It is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by divisor. It can be seen here:  https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py  # noqa \nParameters \n•  value  ( int ) – The original channel number. •  divisor  ( int ) – The divisor to fully divide the channel number. •  min_value  ( int ) – The minimum value of the output channel. Default: None, means that the minimum value equal to the divisor. •  min_ratio  ( float ) – The minimum ratio of the rounded channel number to the original channel number. Default: 0.9. \n\nReturn type  int \nmmdet.models.utils. nch w to nl c ( x ) Flatten [N, C, H, W] shape tensor to [N, L, C] shape tensor. \nParameters  x  ( Tensor ) – The input tensor of shape [N, C, H, W] before conversion. Returns  The output tensor of shape [N, L, C] after conversion. Return type  Tensor \nmmdet.models.utils. nl c to nch w ( x ,  hw_shape ) Convert [N, L, C] shape tensor to [N, C, H, W] shape tensor. \nParameters \n•  x  ( Tensor ) – The input tensor of shape [N, L, C] before conversion. •  hw_shape  ( Sequence[int] ) – The height and width of output feature map. \nReturns  The output tensor of shape [N, C, H, W] after conversion. \nReturn type  Tensor "}
{"page": 444, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_444.jpg", "ocr_text": "CHAPTER\nFORTY\n\nMMDET.UTILS\n\n437\n", "vlm_text": "MMDET.UTILS "}
{"page": 445, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_445.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n438 Chapter 40. mmdet.utils\n", "vlm_text": "MMDetection, Release 2.18.0\n\n438 Chapter 40. mmdet.utils\n"}
{"page": 446, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_446.jpg", "ocr_text": "CHAPTER\nFORTYONE\n\nINDICES AND TABLES\n\n* genindex\n\n* search\n\n439\n", "vlm_text": "INDICES AND TABLES \n• genindex\n\n • search "}
{"page": 447, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_447.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n440 Chapter 41. Indices and tables\n", "vlm_text": "MMDetection, Release 2.18.0\n\n440 Chapter 41. Indices and tables\n"}
{"page": 448, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_448.jpg", "ocr_text": "mmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\nmmd\n\net.\net.\net.\net.\net.\net.\net.\net.\net.\net.\net.\n.datasets.samplers, 256\n\net\n\net.\n-models.den:\nmodels.\n\net\n\net.\net.\net.\net.\net.\n\napis,\ncore.\n\ncore\n\ncore.\n-export, 208\n\ncore\n\ncore.\n-post_\ncore.\ndatasets, 227\n\ncore\n\n181\nanchor, 183\n. bbox, 191\n\nevaluation, 219\n\nmask, 211\n\nprocessing, 221\n\nutils, 224\n\ndatasets.a\ndatasets.pipelines, 239\n\nmodels.bac.\n\nmodels.los\nmodels.nec\n\nmodels.\nmodels.\n\npi_wrappers, 257\n\nkbones, 273\nse_heads, 301\n\ndetectors, 259\n\nses, 412\nks, 291\n\nroi_heads, 383\nutils, 425\n\nPYTHON MODULE INDEX\n\n441\n", "vlm_text": "m mmdet.apis ,  181 mmdet.core.anchor ,  183 mmdet.core.bbox ,  191 mmdet.core.evaluation ,  219 mmdet.core.export ,  208 mmdet.core.mask ,  211 mmdet.core.post processing ,  221 mmdet.core.utils, 224mmdet.datasets ,  227 mmdet.datasets.api wrappers ,  257 mmdet.datasets.pipelines ,  239 mmdet.datasets.samplers ,  256 mmdet.models.backbones ,  273 mmdet.models.dense heads ,  301 mmdet.models.detectors ,  259 mmdet.models.losses ,  412 mmdet.models.necks ,  291 mmdet.models.roi_heads ,  383 mmdet.models.utils ,  425 "}
{"page": 449, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_449.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\n442 Python Module Index\n", "vlm_text": "MMDetection, Release 2.18.0\n\n442 Python Module Index\n"}
{"page": 450, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_450.jpg", "ocr_text": "A\n\nAccuracy (class in mmdet.models.losses), 412\nadaptive_avg_pool2d() (in\nmmdet.models.utils), 433\nAdaptiveAvgPoo12d (class in mmdet.models.utils), 425\nadd_dummy_nms_for_onnx() (in module\nmmdet.core.export), 208\nadd_gt_Q (mmdet.core.bbox.AssignResult method), 192\nadjust_width_group()\n(mmdet.models. backbones.RegNet\n282\nAlbu (class in mmdet.datasets.pipelines), 239\nalbu_builder() (mmdet.datasets.pipelines.Albu\nmethod), 240\nall_reduce_dict(Q (in module mmdet.core.utils), 224\nallreduce_grads() (in module mmdet.core.utils), 224\n\nmodule\n\nmethod),\n\nINDEX\n\n199\nAssignResult (class in mmdet.core.bbox), 191\nAssociativeEmbeddingLoss (class in\nmmdet.models.losses), 413\nasync_inference_detector() (in module\n\nmmdet.apis), 181\n\nasync_simple_test()\n(mmdet.models.detectors. TwoStageDetector\nmethod), 269\n\nasync_simple_test()\n(mmdet.models.roi_heads.BaseRolHead\nmethod), 387\n\nasync_simple_test()\n(mmdet.models.roi_heads.StandardRolHead\nmethod), 410\n\nATSS (class in mmdet.models.detectors), 259\n\nanchor_center () (mmdet.models.dense_heads.GFLHead ATSSHead (class in mmdet.models.dense_heads), 301\n\nmethod), 340\nanchor_inside_flags()\nmmdet.core.anchor), 191\n\n(in module\n\nattention_pool () (mmdet.models.roi_heads.SABLHead\nmethod), 404\naug_test() (mmdet.models.dense_heads.AnchorFreeHead\n\nanchor_offset() (mmdet.models.dense_heads.StageCascadeRPNHOYANO®), 303\n\nmethod), 364\n\nAnchorFreeHead (class in mmdet.models.dense_heads),\n302\n\nAnchorGenerator (class in mmdet.core.anchor), 183\n\nAnchorHead (class in mmdet.models.dense_heads), 305\n\nareas (mmdet.core.mask.BaselnstanceMasks property),\n211\n\nareas (mmdet.core.mask.BitmapMasks property), 214\n\nareas (mmdet.core.mask.PolygonMasks property), 216\n\nassign() (mmdet.core.bbox.BaseAssigner method), 193\n\nassign() (mmdet.core.bbox. CenterRegionAssigner\nmethod), 194\n\nassign() (mmdet.core.bbox.MaxloUAssigner method),\n198\n\nassign() (mmdet.core.bbox.RegionAssigner method),\n200\n\nassign_one_hot_gt_indices()\n(mmdet.core.bbox. CenterRegionAssigner\nmethod), 195\n\nassign_wrt_overlaps()\n\n(mmdet.core.bbox.MaxloUAssigner method),\n\naug_test() (mmdet.models.dense_heads.AnchorHead\nmethod), 305\n\naug_test() (mmdet.models.dense_heads.YOLOV3Head\nmethod), 378\n\naug_test() (mmdet.models.detectors.Base Detector\nmethod), 259\n\naug_test() (mmdet.models.detectors. CenterNet\nmethod), 261\naug_test() (mmdet.models.detectors.CornerNet\n\nmethod), 262\n\naug_test() (mmdet.models.detectors.RPN method), 265\n\naug_test() (mmdet.models.detectors.SingleStageDetector\nmethod), 267\n\naug_test() (mmdet.models.detectors.TridentFasterRCNN\nmethod), 269\n\naug_test() (mmdet.models.detectors.TwoStageDetector\nmethod), 269\n\naug_test() (mmdet.models.detectors. YOLACT method),\n271\n\naug_test() (mmdet.models.roi_heads.BaseRoIHead\nmethod), 387\n\n443\n", "vlm_text": "\nAccuracy  ( class in mmdet.models.losses ),  412 adaptive avg pool 2 d() ( in module mmdet.models.utils ),  433 Adaptive Avg Pool 2 d  ( class in mmdet.models.utils ),  425 add dummy nm s for on nx() ( in module mmdet.core.export ),  208 add_gt_()  ( mmdet.core.bbox.Assign Result method ),  192 adjust width group() ( mmdet.models.backbones.RegNet method ), 282 Albu  ( class in mmdet.datasets.pipelines ),  239 alb u builder() ( mmdet.datasets.pipelines.Albu method ),  240 all reduce dic t()  ( in module mmdet.core.utils ),  224 all reduce grads()  ( in module mmdet.core.utils ),  224 anchor center()  ( mmdet.models.dense heads.GFLHead method ),  340 anchor inside flags() ( in module mmdet.core.anchor ),  191 anchor offset()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  364 Anchor Free Head  ( class in mmdet.models.dense heads ), 302 Anchor Generator  ( class in mmdet.core.anchor ),  183 AnchorHead  ( class in mmdet.models.dense heads ),  305 areas  ( mmdet.core.mask.Base Instance Masks property ), 211 areas  ( mmdet.core.mask.Bitmap Masks property ),  214 areas  ( mmdet.core.mask.Polygon Masks property ),  216 assign()  ( mmdet.core.bbox.Base As signer method ),  193 assign() ( mmdet.core.bbox.Center Region As signer method ),  194 assign()  ( mmdet.core.bbox.MaxI oU As signer method ), 198 assign()  ( mmdet.core.bbox.Region As signer method ), 200 assign one hot gt indices() ( mmdet.core.bbox.Center Region As signer method ),  195 assign w rt overlaps() ( mmdet.core.bbox.MaxI oU As signer method ), \n199 Assign Result  ( class in mmdet.core.bbox ),  191 Associative Embedding Loss ( class in mmdet.models.losses ),  413 a sync inference detector() ( in module mmdet.apis ),  181 a sync simple test() ( mmdet.models.detectors.Two Stage Detector method ),  269 a sync simple test() ( mmdet.models.roi_heads.Base RoI Head method ),  387 a sync simple test() ( mmdet.models.roi_heads.Standard RoI Head method ),  410 ATSS  ( class in mmdet.models.detectors ),  259 ATSSHead  ( class in mmdet.models.dense heads ),  301 attention pool()  ( mmdet.models.roi_heads.SABLHead method ),  404 aug_test()  ( mmdet.models.dense heads.Anchor Free Head method ),  303 aug_test() ( mmdet.models.dense heads.AnchorHead method ),  305 aug_test()  ( mmdet.models.dense heads.YOLOV3Head method ),  378 aug_test() ( mmdet.models.detectors.Base Detector method ),  259 aug_test() ( mmdet.models.detectors.CenterNet method ),  261 aug_test() ( mmdet.models.detectors.CornerNet method ),  262 aug_test()  ( mmdet.models.detectors.RPN method ),  265 aug_test()  ( mmdet.models.detectors.Single Stage Detector method ),  267 aug_test()  ( mmdet.models.detectors.Trident Faster R CNN method ),  269 aug_test()  ( mmdet.models.detectors.Two Stage Detector method ),  269 aug_test()  ( mmdet.models.detectors.YOLACT method ), 271 aug_test() ( mmdet.models.roi_heads.Base RoI Head method ),  387 "}
{"page": 451, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_451.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\naug_test() (mmdet.models.roi_heads.CascadeRolHead\nmethod), 388\n\naug_test() (mmdet.models.roi_heads.HybridTaskCascadeWiakbeydcross_entropy ()\n\nmethod), 398\n\naug_test() (mmdet.models.roi_heads.SCNetRoIHead\nmethod), 406\n\naug_test() (mmdet.models.roi_heads.SparseRolHead\nmethod), 409\n\naug_test() (mmdet.models.roi_heads.StandardRoIHead\nmethod), 410\n\naug_test_bboxes () (mmdet.models.roi_heads.TridentRoIHead\n\nmethod), 412\n\naug_test_mask () (mmdet.models.roi_heads.PointRendRokitédd_linear_layer()\n\nmethod), 402\n\naug_test_rpn() (mmdet.models.dense_heads.CascadeRPNtiebd_model_from_cfg()\n\nmethod), 310\nAutoAssign (class in mmdet.models.detectors), 259\nAutoAssignHead (class in mmdet.models.dense_heads),\n308\nAutoAugment (class in mmdet.datasets.pipelines), 240\naverage_precision() (in module\nmmdet.core.evaluation), 219\n\nB\n\nBalancedL1Loss (class in mmdet.models.losses), 413\nBaseAssigner (class in mmdet.core.bbox), 193\nBaseBBoxCoder (class in mmdet.core.bbox), 193\nBaseDetector (class in mmdet.models.detectors), 259\nBaseInstanceMasks (class in mmdet.core.mask), 211\nBaseRoIExtractor (class in mmdet.models.roi_heads),\n386\nBaseRolHead (class in mmdet.models.roi_heads), 387\nBaseSampler (class in mmdet.core.bbox), 193\nbbox2distance() (in module mmdet.core.bbox), 204\nbbox2result() (in module mmdet.core.bbox), 204\nbbox2roi () (in module mmdet.core.bbox), 204\nbbox_cxcywh_to_xyxy (in\nmmdet.core.bbox), 204\nbbox_flipQ (in module mmdet.core.bbox), 204\nbbox_flipQ) = (mmdet.datasets.pipelines.RandomFlip\nmethod), 252\nbbox_mapping() (in module mmdet.core.bbox), 205\nbbox_mapping_back() (in module mmdet.core.bbox),\n205\n\nmodule\n\nBboxOverlaps2D (class in mmdet.core.bbox), 194\nBFP (class in mmdet.models.necks), 291\n(in module\nmmdet.models.losses), 422\nBitmapMasks (class in mmdet.core.mask), 213\nBoundedIoULoss (class in mmdet.models.losses), 413\nBrightnessTransform (class in\nmmdet.datasets.pipelines), 241\nbuild_assigner() (in module mmdet.core.bbox), 207\n\nbuild_bbox_coder() (in module mmdet.core.bbox),\n\n207\nbuild_dataloader() (in module mmdet.datasets), 237\n(in module\nmmdet.models.utils), 433\n(in module\n\nmmdet.core.export), 209\n\nbuild_roi_layers() (mmdet.models.roi_heads.BaseRolExtractor\nmethod), 386\n\nbuild_sampler() (in module mmdet.core.bbox), 207\nbuild_transformer () (in module mmdet.models.utils),\n\n433\n\nC\n\ncalc_region() (in module mmdet.core.anchor), 191\n\ncalc_sub_regions() (mmdet.models.roi_heads.GridHead\nmethod), 397\n\ncalculate_pos_recall()\n(mmdet.models.dense_heads.F SAFHead\nmethod), 334\n\nCascadeRCNN (class in mmdet.models.detectors), 261\n\nCascadeRolIHead (class in mmdet.models.roi_heads),\n388\n\nCascadeRPNHead (class in mmdet.models.dense_heads),\n310\n\ncenter_of_mass() (in module mmdet.core.utils), 224\n\ncenterness_target()\n(mmdet.models.dense_heads.F COSHead\nmethod), 333\n\nCenterNet (class in mmdet.models.detectors), 261\n\nCenterNetHead (class in mmdet.models.dense_heads),\n311\n\nCenterRegionAssigner (class in mmdet.core.bbox),\n194\n\ncenters_to_bboxes()\n\nbbox_onnx_export () (mmdet.models.roi_heads.StandardRolHead (mmdet.models.dense_heads.RepPointsHead\n\nmethod), 410\nbbox_overlaps() (in module mmdet.core.bbox), 205\n\nbbox_pred_split (© (mmdet.models.roi_heads.SABLHead\n\nmethod), 405\nbbox_rescale() (in module mmdet.core.bbox), 207\nbbox_xyxy_to_cxcywh() (in module\nmmdet.core.bbox), 207\nbboxes (mmdet.core.bbox.SamplingResult property), 201\nBBoxHead (class in mmdet.models.roi_heads), 383\n\nmethod), 354\nCentripetalHead (class in\nmmdet.models.dense_heads), 313\nChannelMapper (class in mmdet.models.necks), 292\nCIoULoss (class in mmdet.models.losses), 414\nCityscapesDataset (class in mmdet.datasets), 227\nClassBalancedDataset (class in mmdet.datasets), 228\nCoarseMaskHead (class in mmdet.models.roi_heads),\n\n389\n\n444\n\nIndex\n", "vlm_text": "aug_test() (mmdet.models.roi_heads.Cascade RoI Headmethod ),  388 aug_test()  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head method ),  398 aug_test() ( mmdet.models.roi_heads.S CNet RoI Head method ),  406 aug_test() ( mmdet.models.roi_heads.Sparse RoI Head method ),  409 aug_test()  ( mmdet.models.roi_heads.Standard RoI Head method ),  410 aug test b boxes()  ( mmdet.models.roi_heads.Trident RoI Head method ),  412 aug test mask()  ( mmdet.models.roi_heads.Point Rend RoI Head method ),  402 aug test rp n()  ( mmdet.models.dense heads.Cascade RP N Head method ),  310 AutoAssign  ( class in mmdet.models.detectors ),  259 Auto Assign Head  ( class in mmdet.models.dense heads ), 308 Auto Augment  ( class in mmdet.datasets.pipelines ),  240 average precision() ( in module mmdet.core.evaluation ),  219 \nB box Overlaps 2 D  ( class in mmdet.core.bbox ),  194 BFP  ( class in mmdet.models.necks ),  291 binary cross entropy() ( in module mmdet.models.losses ),  422 Bitmap Masks (class in mmdet.core.mask), 213Bounded I oU Loss  ( class in mmdet.models.losses ),  413 Brightness Transform ( class in mmdet.datasets.pipelines ),  241 build as signer()  ( in module mmdet.core.bbox ),  207 build b box code r()  ( in module mmdet.core.bbox ), 207 build data loader()  ( in module mmdet.datasets ),  237 build linear layer() ( in module mmdet.models.utils ),  433 build model from cf g() ( in module mmdet.core.export ),  209 build roi layers()  ( mmdet.models.roi_heads.Base RoI Extractor method ),  386 build sampler()  ( in module mmdet.core.bbox ),  207 build transformer()  ( in module mmdet.models.utils ), 433 \n\ncalc region()  ( in module mmdet.core.anchor ),  191 calc sub regions()  ( mmdet.models.roi_heads.GridHead method ),  397 calculate pos recall() ( mmdet.models.dense heads.FSAFHead method ),  334 Cascade R CNN  ( class in mmdet.models.detectors ),  261 Cascade RoI Head  ( class in mmdet.models.roi_heads ), 388 Cascade RP N Head  ( class in mmdet.models.dense heads ), 310 center of mass()  ( in module mmdet.core.utils ),  224 center ness target() ( mmdet.models.dense heads.FCOSHead method ),  333 CenterNet  ( class in mmdet.models.detectors ),  261 Center Net Head  ( class in mmdet.models.dense heads ), 311 Center Region As signer  ( class in mmdet.core.bbox ), 194 centers to b boxes() ( mmdet.models.dense heads.Rep Points Head method ),  354 Centripetal Head ( class in mmdet.models.dense heads ),  313 Channel Mapper  ( class in mmdet.models.necks ),  292 CIoULoss  ( class in mmdet.models.losses ),  414 Cityscape s Data set  ( class in mmdet.datasets ),  227 Class Balanced Data set  ( class in mmdet.datasets ),  228 Coarse Mask Head  ( class in mmdet.models.roi_heads ), 389 \n\nBalanced L 1 Loss  ( class in mmdet.models.losses ),  413 Base As signer  ( class in mmdet.core.bbox ),  193 Base B Box Code r (class in mmdet.core.bbox), 193Base Detector  ( class in mmdet.models.detectors ),  259 Base Instance Masks  ( class in mmdet.core.mask ),  211 Base RoI Extractor  ( class in mmdet.models.roi_heads ), 386 Base RoI Head  ( class in mmdet.models.roi_heads ),  387 Base Sampler (class in mmdet.core.bbox), 193b box 2 distance()  ( in module mmdet.core.bbox ),  204 b box 2 result()  ( in module mmdet.core.bbox ),  204 bbox2roi()  ( in module mmdet.core.bbox ),  204 b box cx cy wh to xy xy() ( in module mmdet.core.bbox ),  204 bbox_flip()  ( in module mmdet.core.bbox ),  204 bbox_flip() ( mmdet.datasets.pipelines.RandomFlip method ),  252 b box mapping()  ( in module mmdet.core.bbox ),  205 b box mapping back()  ( in module mmdet.core.bbox ), 205 b box on nx export()  ( mmdet.models.roi_heads.Standard RoI Head method ),  410 b box overlaps()  ( in module mmdet.core.bbox ),  205 b box p red split()  ( mmdet.models.roi_heads.SABLHead method ),  405 b box re scale()  ( in module mmdet.core.bbox ),  207 b box xy xy to cx cy wh() ( in module mmdet.core.bbox ),  207 bboxes  ( mmdet.core.bbox.Sampling Result property ),  201 BBoxHead  ( class in mmdet.models.roi_heads ),  383 "}
{"page": 452, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_452.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nCOCO (class in mmdet.datasets.api_wrappers), 257\nCocoDataset (class in mmdet.datasets), 228\nCocoPanopticDataset (class in mmdet.datasets), 230\nCollect (class in mmdet.datasets.pipelines), 241\ncollect_loss_level_single()\n(mmdet.models.dense_heads.F SAF Head\nmethod), 335\nColorTransform (class in mmdet.datasets.pipelines),\n242\nCombinedSampler (class in mmdet.core.bbox), 195\nCompose (class in mmdet.datasets.pipelines), 242\nConcatDataset (class in mmdet.datasets), 231\nContrastTransform (class in\nmmdet.datasets.pipelines), 242\nConvFCBBoxHead (class in mmdet.models.roi_heads),\n390\nConvUpsamp1e (class in mmdet.models.utils), 426\nCornerHead (class in mmdet.models.dense_heads), 317\nCornerNet (class in mmdet.models.detectors), 262\ncrop() (mmdet.core.mask.BaseInstanceMasks method),\n211\ncrop() (mmdet.core.mask.BitmapMasks method), 214\ncrop() (mmdet.core.mask.PolygonMasks method), 216\ncropQ) (mmdet.models.dense_heads. YOLACTProtonet\nmethod), 373\n\ndecode_heatmap () (mmdet.models.dense_heads.CenterNetHead\n\nmethod), 311\n\ndecode_heatmap () (mmdet.models.dense_heads.CornerHead\n\nmethod), 317\n\nDecoupledSOLOHead (class in\nmmdet.models.dense_heads), 327\n\nDecoupledSOLOLightHead (class in\nmmdet.models.dense_heads), 329\n\nDeepFashionDataset (class in mmdet.datasets), 234\n\nDefaultFormatBundle (class in\nmmdet.datasets.pipelines), 243\n\nDeformableDETR (class in mmdet.models.detectors), 263\n\nDeformableDETRHead (class in\nmmdet.models.dense_heads), 329\n\nDeltaXYWHBBoxCoder (class in mmdet.core.bbox), 195\n\nDetectoRS_ResNet (class in mmdet.models.backbones),\n276\n\nDetectoRS_ResNeXt (class in\nmmdet.models.backbones), 275\n\nDETR (class in mmdet.models.detectors), 263\n\nDETRHead (class in mmdet.models.dense_heads), 322\n\nDetrTransformerDecoder (class in\nmmdet.models.utils), 426\nDetrTransformerDecoderLayer (class in\n\nmmdet.models.utils), 427\n\ncrop_and_resize() (mmdet.core.mask.BaseInstanceMask3iceLoss (class in mmdet.models.losses), 414\n\nmethod), 211\n\ncrop_and_resize()\n\nmethod), 214\n\ncrop_and_resize() (mmdet.core.mask.PolygonMasks\nmethod), 216\n\ncross_entropy() (in module mmdet.models.losses),\n422\n\nCrossEntropyLoss (class in mmdet.models.losses), 414\n\nCSPDarknet (class in mmdet.models.backbones), 273\n\nCSPLayer (class in mmdet.models.utils), 425\n\nCTResNetNeck (class in mmdet.models.necks), 291\n\n(mmdet.core.mask. BitmapMasks\n\nDIIHead (class in mmdet.models.roi_heads), 390\nDilatedEncoder (class in mmdet.models.necks), 292\nDIoULoss (class in mmdet.models.losses), 414\ndistance2bbox() (in module mmdet.core.bbox), 207\nDistancePointBBoxCoder (class in mmdet.core.bbox),\n196\nDistEvalHook (class in mmdet.core.evaluation), 219\nDistOptimizerHook (class in mmdet.core.utils), 224\nDistributedGroupSampler (class in mmdet.datasets),\n234\n\nDistributedGroupSampler (class in\n\ncuda() (mmdet.models.detectors. KnowledgeDistillationSingleStageDetaattat.datasets.samplers), 256\n\nmethod), 264\nCustomDataset (class in mmdet.datasets), 232\nCutOut (class in mmdet.datasets.pipelines), 242\n\nD\n\nDarknet (class in mmdet.models.backbones), 274\n\ndecode() (mmdet.core.bbox.BaseBBoxCoder method),\n193\n\ndecode() (mmdet.core.bbox.DeltaX YWHBBoxCoder\nmethod), 196\n\ndecode() (mmdet.core.bbox.DistancePointBBoxCoder\nmethod), 196\n\ndecode() (mmdet.core.bbox.PseudoBBoxCoder\nmethod), 199\n\ndecode() (mmdet.core.bbox.TBLRBBoxCoder method),\n203\n\nDistributedSampler (class in mmdet.datasets), 234\nDistributedSampler (class in\nmmdet.datasets.samplers), 256\n\nDistributionFocalLoss (class in\nmmdet.models.losses), 415\n\nDoubleConvFCBBoxHead (class in\nmmdet.models.roi_heads), 392\n\nDoubleHeadRoIHead (class in\n\nmmdet.models.roi_heads), 392\ndynamic_clip_for_onnx() (in\nmmdet.core.export), 209\nDynamicConv (class in mmdet.models.utils), 427\nDynamicRoIHead (class in mmdet.models.roi_heads),\n393\n\nmodule\n\nIndex\n\n445\n", "vlm_text": "COCO  ( class in mmdet.datasets.api wrappers ),  257 Coco Data set  ( class in mmdet.datasets ),  228 Coco Pan optic Data set  ( class in mmdet.datasets ),  230 Collect  ( class in mmdet.datasets.pipelines ),  241 collect loss level single() ( mmdet.models.dense heads.FSAFHead method ),  335 Color Transform  ( class in mmdet.datasets.pipelines ), 242 Combined Sampler (class in mmdet.core.bbox), 195Compose  ( class in mmdet.datasets.pipelines ),  242 Con cat Data set  ( class in mmdet.datasets ),  231 Contrast Transform ( class in mmdet.datasets.pipelines ),  242 Con v FCB Box Head  ( class in mmdet.models.roi_heads ), 390 Con v Up sample  ( class in mmdet.models.utils ),  426 CornerHead  ( class in mmdet.models.dense heads ),  317 CornerNet  ( class in mmdet.models.detectors ),  262 crop()  ( mmdet.core.mask.Base Instance Masks method ), 211 crop()  ( mmdet.core.mask.Bitmap Masks method ),  214 crop()  ( mmdet.core.mask.Polygon Masks method ),  216 crop() ( mmdet.models.dense heads.YO L ACT Proton et method ),  373 crop and resize()  ( mmdet.core.mask.Base Instance Masks method ),  211 crop and resize() ( mmdet.core.mask.Bitmap Masks method ),  214 crop and resize()  ( mmdet.core.mask.Polygon Masks method ),  216 cross entropy()  ( in module mmdet.models.losses ), 422 Cross Entropy Loss  ( class in mmdet.models.losses ),  414 CSPDarknet  ( class in mmdet.models.backbones ),  273 CSPLayer  ( class in mmdet.models.utils ),  425 C TRes Net Neck  ( class in mmdet.models.necks ),  291 cuda()  ( mmdet.models.detectors.Knowledge Distillation Single Stage Detector method ),  264 Custom Data set  ( class in mmdet.datasets ),  232 CutOut  ( class in mmdet.datasets.pipelines ),  242 \nD \nDarknet  ( class in mmdet.models.backbones ),  274 decode()  ( mmdet.core.bbox.Base B Box Code r method ), 193 decode() ( mmdet.core.bbox.Delta XY WH B Box Code r method ),  196 decode() ( mmdet.core.bbox.Distance Point B Box Code r method ),  196 decode() ( mmdet.core.bbox.Pseudo B Box Code r method ),  199 decode()  ( mmdet.core.bbox.T BL RB Box Code r method ), 203 \ndecode heat map()  ( mmdet.models.dense heads.Center Net Head method ),  311 decode heat map()  ( mmdet.models.dense heads.CornerHead method ),  317 Decoupled SOLO Head ( class in mmdet.models.dense heads ),  327 Decoupled SOLO Light Head ( class in mmdet.models.dense heads ),  329 Deep Fashion Data set  ( class in mmdet.datasets ),  234 Default Format Bundle ( class in mmdet.datasets.pipelines ),  243 De formable DE TR  ( class in mmdet.models.detectors ),  263 De formable DET RHead ( class in mmdet.models.dense heads ),  329 Delta XY WH B Box Code r (class in mmdet.core.bbox), 195DetectoRS Res Net  ( class in mmdet.models.backbones ), 276 DetectoRS Res NeXt ( class in mmdet.models.backbones ),  275 DETR  ( class in mmdet.models.detectors ),  263 DETRHead  ( class in mmdet.models.dense heads ),  322 De tr Transformer Decoder ( class in mmdet.models.utils ),  426 De tr Transformer Decoder Layer ( class in mmdet.models.utils ),  427 DiceLoss  ( class in mmdet.models.losses ),  414 DIIHead  ( class in mmdet.models.roi_heads ),  390 Dilated Encoder  ( class in mmdet.models.necks ),  292 DIoULoss  ( class in mmdet.models.losses ),  414 distance 2 b box()  ( in module mmdet.core.bbox ),  207 Distance Point B Box Code r  ( class in mmdet.core.bbox ), 196 Di stEv al Hook  ( class in mmdet.core.evaluation ),  219 Dist Optimizer Hook  ( class in mmdet.core.utils ),  224 Distributed Group Sampler  ( class in mmdet.datasets ), 234 Distributed Group Sampler ( class in mmdet.datasets.samplers ),  256 Distributed Sampler  ( class in mmdet.datasets ),  234 Distributed Sampler ( class in mmdet.datasets.samplers ),  256 Distribution Focal Loss ( class in mmdet.models.losses ),  415 Double Con v FCB Box Head ( class in mmdet.models.roi_heads ),  392 Double Head RoI Head ( class in mmdet.models.roi_heads ),  392 dynamic clip for on nx() ( in module mmdet.core.export ),  209 Dynamic Con v  ( class in mmdet.models.utils ),  427 Dynamic RoI Head  ( class in mmdet.models.roi_heads ), 393 "}
{"page": 453, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_453.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nE\n\nEmbeddingRPNHead (class in\nmmdet.models.dense_heads), 331\n\nencode() (mmdet.core.bbox.BaseBBoxCoder method),\n193\n\nencode() (mmdet.core.bbox.DeltaX YWHBBoxCoder\nmethod), 196\n\nencode() (mmdet.core.bbox.DistancePointBBoxCoder\nmethod), 197\n\nencode() (mmdet.core.bbox.PseudoBBoxCoder\nmethod), 199\n\nencode() (mmdet.core.bbox.TBLRBBoxCoder method),\n204\n\nencode_mask_results() (in module\nmmdet.core.mask), 217\nEqualizeTransform (class in\n\nmmdet.datasets.pipelines), 243\n\neval_map() (in module mmdet.core.evaluation), 219\n\neval_recalls() (in module mmdet.core.evaluation),\n220\n\nEvalHook (class in mmdet.core.evaluation), 219\n\nevaluate() (mmdet.datasets. Cityscapes Dataset\nmethod), 227\n\nevaluate() (mmdet.datasets.CocoDataset method), 228\n\nevaluate() (mmdet.datasets. CocoPanopticDataset\nmethod), 231\n\nevaluate() (mmdet.datasets.ConcatDataset method),\n232\n\nevaluate() (mmdet.datasets.CustomDataset method),\n233\n\nevaluate() (mmdet.datasets.LVISVO5Dataset method),\n235\n\nevaluate() (mmdet.datasets. VOCDataset method), 236\n\nevaluate_pan_json()\n(mmdet.datasets.CocoPanopticDataset\nmethod), 231\n\nExpand (class in mmdet.datasets.pipelines), 243\n\nexpand () (mmdet.core.mask.BaseInstanceMasks\nmethod), 211\n\nexpand() (mmdet.core.mask.BitmapMasks method), 214\n\nexpand() (mmdet.core.mask.PolygonMasks_ method),\n216\n\nextract_feat() (mmdet.models.detectors.BaseDetector\nmethod), 259\n\nextract_feat()\nmethod), 266\n\n(mmdet.models.detectors.RPN\n\nF\n\nfast_nms() (in module mmdet.core.post_processing),\n221\n\nFasterRCNN (class in mmdet.models.detectors), 264\n\nFastRCNN (class in mmdet.models.detectors), 263\n\nFCNMaskHead (class in mmdet.models.roi_heads), 393\n\nFCOS (class in mmdet.models.detectors), 263\n\nFCOSHead (class in mmdet.models.dense_heads), 331\n\nFeatureAdaption (class in\nmmdet.models.dense_heads), 336\n\nFeatureRelayHead (class in mmdet.models.roi_heads),\n395\n\nfilter_scores_and_topk()\nmmdet.core.utils), 225\n\nflip©O (mmdet.core.mask.BaseInstanceMasks method),\n211\n\nf£lip© (mmdet.core.mask.BitmapMasks method), 214\n\nf£lip© (mmdet.core.mask.PolygonMasks method), 216\n\nflip_tensor() (in module mmdet.core.utils), 225\n\nFocalLoss (class in mmdet.models.losses), 415\n\nformat_results() (mmdet.datasets.CityscapesDataset\nmethod), 227\n\nformat_results()\nmethod), 229\n\nformat_results()\nmethod), 233\n\nforward() (mmdet.models.backbones.CSP Darknet\n\nmethod), 274\n\nforward() (mmdet.models.backbones.Darknet method),\n\n275\n\nforward() (mmdet.models.backbones.DetectoRS_ResNet\n\nmethod), 276\n\nforward() (mmdet.models. backbones. HourglassNet\n\nmethod), 279\n\nforward() (mmdet.models.backbones.HRNet method),\n\n278\n\nforward() (mmdet.models.backbones.MobileNetV2\n\nmethod), 279\n\n(in module\n\n(mmdet.datasets.CocoDataset\n\n(mmdet.datasets. CustomDataset\n\nforward () (mmdet.models.backbones.PyramidVisionTransformer\n\nmethod), 281\nforward() (mmdet.models.backbones.RegNet method),\n\n283\n\nforward() (mmdet.models.backbones.ResNet method),\n287\n\nforward() (mmdet.models.backbones.SSDVGG\n\nmethod), 289\n\nextract_feat () (mmdet.models.detectors.SingleStageDetefoamard() (mmdet.models.backbones.SwinTransformer\n\nmethod), 267\n\nmethod), 290\n\nextract_feat () (mmdet.models.detectors.TwoStageDetecthorward() (mmdet.models.dense_heads.AnchorFreeHead\n\nmethod), 270\n\nextract_feats() (mmdet.models.detectors.BaseDetector forward()\n\nmethod), 259\n\nmethod), 303\n(mmdet.models.dense_heads.AnchorHead\nmethod), 306\nforward() (mmdet.models.dense_heads.ATSSHead\nmethod), 301\n\n446\n\nIndex\n", "vlm_text": "E \nEmbedding RP N Head ( class in mmdet.models.dense heads ),  331 encode()  ( mmdet.core.bbox.Base B Box Code r method ), 193 encode() ( mmdet.core.bbox.Delta XY WH B Box Code r method ),  196 encode() ( mmdet.core.bbox.Distance Point B Box Code r method ),  197 encode() ( mmdet.core.bbox.Pseudo B Box Code r method ),  199 encode()  ( mmdet.core.bbox.T BL RB Box Code r method ), 204 encode mask results() ( in module mmdet.core.mask), 217Equalize Transform ( class in mmdet.datasets.pipelines ),  243 eval_map()  ( in module mmdet.core.evaluation ),  219 e val recalls()  ( in module mmdet.core.evaluation ), 220 EvalHook  ( class in mmdet.core.evaluation ),  219 evaluate() ( mmdet.datasets.Cityscape s Data set method ),  227 evaluate()  ( mmdet.datasets.Coco Data set method ),  228 evaluate() ( mmdet.datasets.Coco Pan optic Data set method ),  231 evaluate()  ( mmdet.datasets.Con cat Data set method ), 232 evaluate()  ( mmdet.datasets.Custom Data set method ), 233 evaluate()  ( mmdet.datasets.LV IS V 05 Data set method ), 235 evaluate()  ( mmdet.datasets.VOCDataset method ),  236 evaluate pan json() ( mmdet.datasets.Coco Pan optic Data set method ),  231 Expand  ( class in mmdet.datasets.pipelines ),  243 expand() ( mmdet.core.mask.Base Instance Masks method ),  211 expand()  ( mmdet.core.mask.Bitmap Masks method ),  214 expand() ( mmdet.core.mask.Polygon Masks method ), 216 extract feat()  ( mmdet.models.detectors.Base Detector method ),  259 extract feat() ( mmdet.models.detectors.RPN method ),  266 extract feat()  ( mmdet.models.detectors.Single Stage Detector method ),  267 extract feat()  ( mmdet.models.detectors.Two Stage Detector method ),  270 extract feats()  ( mmdet.models.detectors.Base Detector method ),  259 \nF \nfast_nms()  ( in module mmdet.core.post processing ), 221 FasterRCNN  ( class in mmdet.models.detectors ),  264 FastRCNN  ( class in mmdet.models.detectors ),  263 FC N Mask Head  ( class in mmdet.models.roi_heads ),  393 FCOS  ( class in mmdet.models.detectors ),  263 FCOSHead  ( class in mmdet.models.dense heads ),  331 Feature Adaption ( class in mmdet.models.dense heads ),  336 Feature Relay Head  ( class in mmdet.models.roi_heads ), 395 filter scores and top k() ( in module mmdet.core.utils ),  225 flip()  ( mmdet.core.mask.Base Instance Masks method ), 211 flip()  ( mmdet.core.mask.Bitmap Masks method ),  214 flip()  ( mmdet.core.mask.Polygon Masks method ),  216 flip tensor()  ( in module mmdet.core.utils ),  225 FocalLoss  ( class in mmdet.models.losses ),  415 format results()  ( mmdet.datasets.Cityscape s Data set method ),  227 format results() ( mmdet.datasets.Coco Data set method ),  229 format results() ( mmdet.datasets.Custom Data set method ),  233 forward() ( mmdet.models.backbones.CSPDarknet method ),  274 forward()  ( mmdet.models.backbones.Darknet method ), 275 forward()  ( mmdet.models.backbones.DetectoRS Res Net method ),  276 forward() ( mmdet.models.backbones.Hourglass Net method ),  279 forward()  ( mmdet.models.backbones.HRNet method ), 278 forward() ( mmdet.models.backbones.Mobile Ne tV 2 method ),  279 forward()  ( mmdet.models.backbones.Pyramid Vision Transformer method ),  281 forward()  ( mmdet.models.backbones.RegNet method ), 283 forward()  ( mmdet.models.backbones.ResNet method ), 287 forward() ( mmdet.models.backbones.SSDVGG method ),  289 forward()  ( mmdet.models.backbones.S win Transformer method ),  290 forward()  ( mmdet.models.dense heads.Anchor Free Head method ),  303 forward() ( mmdet.models.dense heads.AnchorHead method ),  306 forward() ( mmdet.models.dense heads.ATSSHead method ),  301 "}
{"page": 454, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_454.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\n© (mmdet.models.dense_heads.CenterNetHead\n\nmethod), 311 forward\noO (mmdet.models.dense_heads.CornerHead forward\nmethod), 318\n\n© (mmdet.models.dense_heads.DecoupledSOLOHéaiward\nmethod), 328\n\nmethod), 414\n\n© (mmdet.models.losses.DiceLoss method), 415\n© (mmdet.models.losses.DIoULoss method),\n414\n\n© (mmdet.models.losses.DistributionFocalLoss\nmethod), 415\n\n© (mmdet.models.dense_heads.DecoupledSOLOLigguiand() (mmdet.models.losses.FocalLoss method),\n\nmethod), 329\n\n© (mmdet.models.dense_heads.DeformableDETRHeudard\nmethod), 329\n\noO (mmdet.models.dense_heads.DETRHead forward\nmethod), 323 forward\noO (mmdet.models.dense_heads.FCOSHead forward\nmethod), 333\n\n© (mmdet.models.dense_heads.FeatureAdaption forward\nmethod), 336 forward\noO (mmdet.models.dense_heads.GF LHead\n\nmethod), 340 forward\n© (mmdet.models.dense_heads.GuidedAnchorHeadlorward\nmethod), 343\n\n© (mmdet.models.dense_heads.RepPointsHead forward\nmethod), 354\n\n© (mmdet.models.dense_heads.RetinaSepBNHeadforward\nmethod), 357\n\n© (mmdet.models.dense_heads.SABLRetinaHead forward\nmethod), 359\n\noO (mmdet.models.dense_heads.SOLOHead_ forward\nmethod), 361\n\noO (mmdet.models.dense_heads.SSDHead_ forward\nmethod), 363 forward\n© (mmdet.models.dense_heads.StageCascadeRPNHead\nmethod), 364 forward\noO (mmdet.models.dense_heads.VF NetHead\nmethod), 367\n\n© (mmdet.models.dense_heads. YOLACTProtonet\nmethod), 373 forward\n© (mmdet.models.dense_heads. YOLACTSegmHeadorward\nmethod), 375 forward\n\nforward\n\n© (mmdet.models.dense_heads. YOLOV3Head\nmethod), 379 forward\n© (mmdet.models.dense_heads.YOLOXHead_ forward\n\nmethod), 382\n\noO (mmdet.models.detectors.BaseDetector forward\nmethod), 259 forward\n© (mmdet.models.losses.Accuracy method),412 forward\n\n© (mmdet.models.losses.AssociativeEmbeddingLoforward\nmethod), 413 forward\noO (mmdet.models.losses.BalancedLI Loss\nmethod), 413\n\noO (mmdet.models.losses.BoundedIoULoss\nmethod), 413\n\n© (mmdet.models.losses.CloULoss method),\n\nforward\n\nforward\n\n414\noO\n\nforward\n(mmdet.models.losses.CrossEntropyLoss\n\n415\n\noO (mmdet.models.losses.GaussianFocalLoss\nmethod), 417\n\n© (mmdet.models.losses.GHMC method), 416\n© (mmdet.models.losses.GHMR method), 416\n© (mmdet.models.losses.GIloULoss method),\n417\n\n© (mmdet.models.losses.loULoss method), 418\nO (mmdet.models.losses. KnowledgeDistillationKLDivLoss\nmethod), 418\n\n© (mmdet.models.losses.L1 Loss method), 419\n© (mmdet.models.losses.MSELoss method),\n419\n\noO (mmdet.models.losses. Quality FocalLoss\nmethod), 420\n\n© (mmdet.models.losses.SeesawLoss method),\n420\n\noO (mmdet.models.losses.SmoothL1Loss\nmethod), 421\n\noO (mmdet.models. losses. VarifocalLoss\nmethod), 422\n\n© (mmdet.models.necks.BFP method), 291\n\noO (mmdet.models.necks. ChannelMapper\nmethod), 292\n\noO (mmdet.models.necks. CTResNetNeck\nmethod), 291\n\noO (mmdet.models.necks.DilatedEncoder\nmethod), 293\n\n© (mmdet.models.necks.F PG method), 294\n\n© (mmdet.models.necks.F PN method), 295\n\noO (mmdet.models.necks.F PN_CARAFE\nmethod), 296\n\n© (mmdet.models.necks. HRFPN method), 296\noO (mmdet.models.necks.NASFCOS_FPN\nmethod), 297\n\n© (mmdet.models.necks.NASFPN method), 297\n© (mmdet.models.necks.PAFPN method), 298\n© (mmdet.models.necks.RFP method), 298\n\n© (mmdet.models.necks.SSDNeck method), 299\n© (mmdet.models.necks. YOLOV3Neck method),\n300\n\noO (mmdet.models.necks. YOLOXPAF PN\nmethod), 300\n\n©. (mmdet.models.roi_heads.BaseRolExtractor\nmethod), 386\n\noO (mmdet.models.roi_heads.BBoxHead\nmethod), 383\n\nIndex\n\n447\n", "vlm_text": "forward()  ( mmdet.models.dense heads.Center Net Head method ),  414 method ),  311 forward()  ( mmdet.models.losses.DiceLoss method ),  415 forward() ( mmdet.models.dense heads.CornerHead forward() ( mmdet.models.losses.DIoULoss method ), method ),  318 414 forward()  ( mmdet.models.dense heads.Decoupled SOLO Head forward()  ( mmdet.models.losses.Distribution Focal Loss method ),  328 method ),  415 forward()  ( mmdet.models.dense heads.Decoupled SOLO Light Head ( mmdet.models.losses.FocalLoss method ), method ),  329 415 forward()  ( mmdet.models.dense heads.De formable DET RHead forward() ( mmdet.models.losses.Gaussian Focal Loss method ),  329 method ),  417 forward() ( mmdet.models.dense heads.DETRHead forward()  ( mmdet.models.losses.GHMC method ),  416 method ),  323 forward()  ( mmdet.models.losses.GHMR method ),  416 forward() ( mmdet.models.dense heads.FCOSHead forward() ( mmdet.models.losses.GIoULoss method ), method ),  333 417 forward()  ( mmdet.models.dense heads.Feature Adaption forward()  ( mmdet.models.losses.IoULoss method ),  418 method ),  336 forward()  ( mmdet.models.losses.Knowledge Distillation KL Div Loss forward() ( mmdet.models.dense heads.GFLHead method ),  418 method ),  340 forward()  ( mmdet.models.losses.L1Loss method ),  419 forward()  ( mmdet.models.dense heads.Guided Anchor Head forward() ( mmdet.models.losses.MSELoss method ), method ),  343 419 forward()  ( mmdet.models.dense heads.Rep Points Head forward() ( mmdet.models.losses.Quality Focal Loss method ),  354 method ),  420 forward()  ( mmdet.models.dense heads.RetinaS epB N Head forward()  ( mmdet.models.losses.SeesawLoss method ), method ),  357 420 forward()  ( mmdet.models.dense heads.S ABL Retina Head forward() ( mmdet.models.losses.Smooth L 1 Loss method ),  359 method ),  421 forward() ( mmdet.models.dense heads.SOLOHead forward() ( mmdet.models.losses.Var i focal Loss method ),  361 method ),  422 forward() ( mmdet.models.dense heads.SSDHead forward()  ( mmdet.models.necks.BFP method ),  291 method ),  363 forward() ( mmdet.models.necks.Channel Mapper forward()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  292 method ),  364 forward() ( mmdet.models.necks.C TRes Net Neck forward() ( mmdet.models.dense heads.VFNetHead method ),  291 method ),  367 forward() ( mmdet.models.necks.Dilated Encoder forward()  ( mmdet.models.dense heads.YO L ACT Proton et method ),  293 method ),  373 forward()  ( mmdet.models.necks.FPG method ),  294 forward()  ( mmdet.models.dense heads.YO LAC TSe gm Head forward()  ( mmdet.models.necks.FPN method ),  295 method ),  375 forward() ( mmdet.models.necks.FPN_CARAFE forward() ( mmdet.models.dense heads.YOLOV3Head method ),  296 method ),  379 forward()  ( mmdet.models.necks.HRFPN method ),  296 forward() ( mmdet.models.dense heads.YOLOXHead forward() ( mmdet.models.necks.NAS FCO S FP N method ),  382 method ),  297 forward() ( mmdet.models.detectors.Base Detector forward()  ( mmdet.models.necks.NASFPN method ),  297 method ),  259 forward()  ( mmdet.models.necks.PAFPN method ),  298 forward()  ( mmdet.models.losses.Accuracy method ),  412 forward()  ( mmdet.models.necks.RFP method ),  298 forward()  ( mmdet.models.losses.Associative Embedding Loss forward()  ( mmdet.models.necks.SSDNeck method ),  299 method ),  413 forward()  ( mmdet.models.necks.YOLOV3Neck method ), forward() ( mmdet.models.losses.Balanced L 1 Loss 300 method ),  413 forward() ( mmdet.models.necks.YOLOXPAFPN forward() ( mmdet.models.losses.Bounded I oU Loss method ),  300 method ),  413 forward()  ( mmdet.models.roi_heads.Base RoI Extractor forward() ( mmdet.models.losses.CIoULoss method ), method ),  386 414 forward() ( mmdet.models.roi_heads.BBoxHead forward() ( mmdet.models.losses.Cross Entropy Loss method ),  383 "}
{"page": 455, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_455.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\n© (mmdet.models.roi_heads.CoarseMaskHead forward\nmethod), 389\n\n© (mmdet.models.roi_heads.ConvF CBBoxHead forward\nmethod), 390\n\n© (mmdet.models.roi_heads.DI[Head method),\n391\n\n© (mmdet.models.roi_heads.DoubleConvF CBBoxHendard\nmethod), 392\n\noO (mmdet.models.roi_heads.FCNMaskHead_ forward\nmethod), 393\n\n© (mmdet.models.roi_heads.FeatureRelayHead forward\nmethod), 395\n\nforward\n\n© (mmdet.models.roi_heads.FusedSemanticHead forward_t\n\nmethod), 396\n\n© (mmdet.models.roi_heads.GenericRolExtractor forward\nmethod), 396\n\n© (mmdet.models.roi_heads.GlobalContextHead\nmethod), 397\n\nforward\n\noO (mmdet.models.roi_heads.GridHead forward\nmethod), 397\n\noO (mmdet.models.roi_heads.HTCMaskHead_ forward\nmethod), 397\n\noO (mmdet.models.roi_heads.MaskloUHead forward\nmethod), 399\n\n© (mmdet.models.roi_heads.MaskPointHead forward\nmethod), 400\n\n© (mmdet.models.roi_heads.ResLayer method), forward\n403\n\noO (mmdet.models.roi_heads.SABLHead_ forward\nmethod), 405\n\n© (mmdet.models.roi_heads.SCNetBBoxHead forward\nmethod), 406\n\n© (mmdet.models.roi_heads.SingleRolExtractor forward\nmethod), 408\n\noO (mmdet.models.utils.AdaptiveAvgPool2d forward\nmethod), 425\n\noO (mmdet.models.utils.ConvUpsample forward\nmethod), 426\n\n© (mmdet.models.utils.CSPLayer method), 426 forward\n© (mmdet.models.utils. DetrTransformerDecoder\n\nmethod), 427 forward\n© (mmdet.models.utils.DynamicConv method),\n\n428 forward\noO (mmdet.models.utils. mvertedResidual\n\nmethod), 428 forward\n© (mmdet.models.utils.LearnedPositionalEncoding\nmethod), 429 forward\noO (mmdet.models.utils.NormedConv2d\n\nmethod), 429 forward\n© (mmdet.models.utils.NormedLinear method),\n\n429\n© (mmdet.models.utils.PatchEmbed method),\n\nforward\n\n430\n© (mmdet.models.utils.SELayer method), 431\n\nforward\n\noO (mmdet.models. utils. SimplifiedBasicBlock\n\nmethod), 431\n\n© (mmdet.models.utils.SinePositionalEncoding\n\nmethod), 432\n\n© (mmdet.models.utils. Transformer method),\n\n433\n\n| dummy () (mmdet.models.dense_heads.EmbeddingRPNHead\nmethod), 331\n\n| dummy () (mmdet.models.detectors. DETR\nmethod), 263\n| dummy () (mmdet.models.detectors.RPN\n\nmethod), 266\n\ndummy () (mmdet.models.detectors.SingleStageDetector\nmethod), 267\n\n| dummy () (mmdet.models.detectors.SparseRCNN\nmethod), 268\n\n| dummy () (mmdet.models.detectors. TwoStageDetector\nmethod), 270\n\n|_d\nmethod), 271\n_dummy C)\nmethod), 271\n__ dummy () (mmdet.models.roi_heads.CascadeRolIHead\nmethod), 388\n\n|_ dummy () (mmdet.models.roi_heads.GridRolHead\nmethod), 397\n\n|_d\nmethod), 398\n| dummy () (mmdet.models.roi_heads.SparseRolHead\nmethod), 409\n\n| dummy () (mmdet.models.roi_heads.StandardRoIHead\nmethod), 410\n\n__onnx () (mmdet.models.dense_heads. DETRHead\nmethod), 324\n\n|_single() (mmdet.models.dense_heads.AnchorFreeHead\nmethod), 304\n\n| single) (mmdet.models.dense_heads.AnchorHead\nmethod), 306\n\n|_single() (mmdet.models.dense_heads.ATSSHead\nmethod), 301\n\n| single() (mmdet.models.dense_heads.AutoAssignHead\nmethod), 308\n\n| single() (mmdet.models.dense_heads.CenterNetHead\nmethod), 312\n\n| single() (mmdet.models.dense_heads.CentripetalHead\nmethod), 314\n\n| single() (mmdet.models.dense_heads.CornerHead\nmethod), 318\n\n| single) (mmdet.models.dense_heads. DETRHead\nmethod), 324\n\n__single() (mmdet.models.dense_heads.F COSHead\nmethod), 333\n\n|_single() (mmdet.models.dense_heads.FoveaHead\nmethod), 337\n\n(mmdet.models.detectors. YOLACT\n\n448\n\nIndex\n\ntummy () (mmdet.models.detectors. TwoStagePanopticSegmentor\n\nlummy () (mmdet.models.roi_heads.HybridTaskCascadeRolHeaa\n", "vlm_text": "The table appears to list various function calls from the MMDetection framework, focusing on methods related to different model components. Each row includes a `forward()` function call with specific model components and methods, often related to region of interest (ROI) heads, utilities, and dense heads in object detection models. The methods are often followed by a page or section number (e.g., 389, 391) indicating their location or reference point within a document or codebase."}
{"page": 456, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_456.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\nforward\n\n| single() (mmdet.models.dense_heads.FSAFHead method), 270\n\nmethod), 335 forward_train() (mmdet.models.detectors. TwoStagePanopticSegmentor\n| single() (mmdet.models.dense_heads.GARetinaHead — method), 271\n\nmethod), 339 forward_train(Q)  (mmdet.models.detectors. YOLACT\n| single() (mmdet.models.dense_heads.GARPNHead method), 271\n\nmethod), 339 forward_train() (mmdet.models.roi_heads.BaseRoIHead\n| single () (mmdet.models.dense_heads.GF LHead method), 387\n\nmethod), 341 forward_train() (mmdet.models.roi_heads.CascadeRolHead\n| single() (mmdet.models.dense_heads.GuidedAnchorHeadnethod), 388\n\nmethod), 343 forward_train() (mmdet.models.roi_heads.DynamicRolHead\n| single() (mmdet.models.dense_heads.RepPointsHead method), 393\n\nmethod), 354 forward_train() (mmdet.models.roi_heads.HybridTaskCascadeRolHeaa\n| single() (mmdet.models.dense_heads.RetinaHead method), 398\n\nmethod), 356 forward_train() (mmdet.models.roi_heads.PISARoIHead\n| single() (mmdet.models.dense_heads.RPNHead method), 402\n\nmethod), 353 forward_train() (mmdet.models.roi_heads.SCNetRoIHead\n| single() (mmdet.models.dense_heads.StageCascadeRPNhhtlod), 406\n\nmethod), 365 forward_train() (mmdet.models.roi_heads.SparseRolHead\n\n| single() (mmdet.models.dense_heads.VFNetHead method), 409\n\nmethod), 367 forward_train() (mmdet.models.roi_heads.StandardRolHead\n| single() (mmdet.models.dense_heads. YOLACTHead method), 411\n\nmethod), 371 FOVEA (class in mmdet.models.detectors), 263\n\n| single() (mmdet.models.dense_heads.YOLOF HeaweaHead (class in mmdet.models.dense_heads), 337\nmethod), 376 FPG (class in mmdet.models.necks), 293\n\n| single() (mmdet.models.dense_heads. YOLOXHE#®U (class in mmdet.models.necks), 294\n\nmethod), 382 FPN_CARAFE (class in mmdet.models.necks), 295\n\n| single_onnx() FreeAnchorRetinaHead (class in\n(mmdet.models.dense_heads. DETRHead mmdet.models.dense_heads), 337\n\nmethod), 324 FSAF (class in mmdet.models.detectors), 263\n\n| test () (mmdet.models.detectors.BaseDetector FSAFHead (class in mmdet.models.dense_heads), 334\n\nmethod), 259 FusedSemanticHead (class in\n|.test()  (mmdet.models.detectors.FastRCNN mmdet.models.roi_heads), 395\n\nmethod), 263\n\nforward_train() (mmdet.models. dense_heads.CascadeR KHead\nmethod), 310 ga_loc_targets() (mmdet.models.dense_heads.GuidedAnchorHead\nforward_train() (mmdet.models.dense_heads.DETRHead method), 343\nmethod), 325 ga_shape_targets() (mmdet.models.dense_heads.GuidedAnchorHead\nforward_train() (mmdet.models.dense_heads.EmbeddingRPNHeaqyethod), 344\nmethod), 331 GARetinaHead (class in mmdet.models.dense_heads),\nforward_train() (mmdet.models.dense_heads.LDHead 339\nmethod), 346 GARPNHead (class in mmdet.models.dense_heads), 339\nforward_train() (mmdet.models.detectors.BaseDetector gaussian_radius() (in module mmdet.models.utils),\nmethod), 259 433\nforward_train() (mmdet.models. detectors.KnowledgeDis@ ad sighed uReRstecauss in mmdet.models.losses),\nmethod), 264 417\nforward_train() (mmdet.models.detectors.RPN —gen_base_anchors() (mmdet.core.anchor.AnchorGenerator\nmethod), 266 method), 184\nforward_train() (mmdet.models.detectors.SingleStageDegefohase_anchors() (mmdet.core.anchor. YOLOAnchorGenerator\nmethod), 267 method), 190\nforward_train( (mmdet.models.detectors.SparseRCNN gen_gaussian_target() (in module\nmethod), 268 mmdet.models.utils), 435\nforward_train() (mmdet.models.detectors. TridentFasterR§e4Ngrid_from_reg oO\nmethod), 269 (mmdet.models.dense_heads.RepPointsHead\nforward_train() (mmdet.models.detectors. TwoStageDetector method), 355\nIndex 449\n", "vlm_text": "forward single()  ( mmdet.models.dense heads.FSAFHead method ),  270 method ),  335 forward train()  ( mmdet.models.detectors.Two Stage Pan optic Segment or forward single()  ( mmdet.models.dense heads.GA Retina Head method ),  271 method ),  339 forward train() ( mmdet.models.detectors.YOLACT forward single()  ( mmdet.models.dense heads.GARPNHead method ),  271 method ),  339 forward train()  ( mmdet.models.roi_heads.Base RoI Head forward single()  ( mmdet.models.dense heads.GFLHead method ),  387 method ),  341 forward train()  ( mmdet.models.roi_heads.Cascade RoI Head forward single()  ( mmdet.models.dense heads.Guided Anchor Head method ),  388 method ),  343 forward train()  ( mmdet.models.roi_heads.Dynamic RoI Head forward single()  ( mmdet.models.dense heads.Rep Points Head method ),  393 method ),  354 forward train()  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head forward single()  ( mmdet.models.dense heads.RetinaHead method ),  398 method ),  356 forward train()  ( mmdet.models.roi_heads.PISA RoI Head forward single()  ( mmdet.models.dense heads.RPNHead method ),  402 method ),  353 forward train()  ( mmdet.models.roi_heads.S CNet RoI Head forward single()  ( mmdet.models.dense heads.Stage Cascade RP N Head ),  406 method ),  365 forward train()  ( mmdet.models.roi_heads.Sparse RoI Head forward single()  ( mmdet.models.dense heads.VFNetHead method ),  409 method ),  367 forward train()  ( mmdet.models.roi_heads.Standard RoI Head forward single()  ( mmdet.models.dense heads.YOLACTHead method ),  411 method ),  371 FOVEA  ( class in mmdet.models.detectors ),  263 forward single()  ( mmdet.models.dense heads.YOLOFHead FoveaHead  ( class in mmdet.models.dense heads ),  337 method ),  376 FPG  ( class in mmdet.models.necks ),  293 forward single()  ( mmdet.models.dense heads.YOLOXHead  ( class in mmdet.models.necks ),  294 method ),  382 FPN_CARAFE  ( class in mmdet.models.necks ),  295 forward single on nx() Free Anchor Retina Head ( class in ( mmdet.models.dense heads.DETRHead mmdet.models.dense heads ),  337 method ),  324 FSAF  ( class in mmdet.models.detectors ),  263 forward test()  ( mmdet.models.detectors.Base Detector FSAFHead  ( class in mmdet.models.dense heads ),  334 method ),  259 Fused Semantic Head ( class in forward test() ( mmdet.models.detectors.FastRCNN mmdet.models.roi_heads ),  395 method ),  263 forward train()  ( mmdet.models.dense heads.Cascade RP N Head G method ),  310 ga loc targets()  ( mmdet.models.dense heads.Guided Anchor Head forward train()  ( mmdet.models.dense heads.DETRHead method ),  343 method ),  325 ga shape targets()  ( mmdet.models.dense heads.Guided Anchor Head forward train()  ( mmdet.models.dense heads.Embedding RP N Head method ),  344 method ),  331 GA Retina Head  ( class in mmdet.models.dense heads ), forward train()  ( mmdet.models.dense heads.LDHead 339 method ),  346 GARPNHead  ( class in mmdet.models.dense heads ),  339 forward train()  ( mmdet.models.detectors.Base Detector  ( in module mmdet.models.utils ), gaussian radius() method ),  259 433 forward train()  ( mmdet.models.detectors.Knowledge Distillation Single Stage Detector class in mmdet.models.losses ), method ),  264 417 forward train() ( mmdet.models.detectors.RPN  ( mmdet.core.anchor.Anchor Generator gen base anchors() method ),  266 method ),  184 forward train()  ( mmdet.models.detectors.Single Stage Detector  ( mmdet.core.anchor.YO LO Anchor Generator gen base anchors() method ),  267 method ),  190 forward train()  ( mmdet.models.detectors.SparseRCNN ( in module gen gaussian target() method ),  268 mmdet.models.utils ),  435 forward train()  ( mmdet.models.detectors.Trident Faster R CNN gen grid from reg() method ),  269 ( mmdet.models.dense heads.Rep Points Head forward train()  ( mmdet.models.detectors.Two Stage Detector method ),  355 "}
{"page": 457, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_457.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\ngen_single_level_base_anchors()\n(mmdet.core.anchor.AnchorGenerator method),\n184\n\ngen_single_level_base_anchors()\n(mmdet.core.anchor.LegacyAnchorGenerator\nmethod), 187\n\ngen_single_level_base_anchors()\n(mmdet.core.anchor. YOLOAnchorGenerator\nmethod), 190\n\ngenerate_coordinate() (in module mmdet.core.utils),\n225\n\ngenerate_inputs_and_wrap_model ()\nmmdet.core.export), 209\n\ngenerate_regnet() (mmdet.models.backbones.RegNet\nmethod), 283\n\nGenericRoIExtractor (class in\nmmdet.models.roi_heads), 396\n\nget_accuracy() (mmdet.models.losses.SeesawLoss\nmethod), 421\n\nget_activation() (mmdet.models.losses.SeesawLoss\nmethod), 421\n\n(in module\n\nmethod), 348\nget_bboxes() (mmdet.models.dense_heads.SABLRetinaHead\nmethod), 359\nget_bboxes () (mmdet.models.dense_heads.StageCascadeRP NHead\nmethod), 365\nget_bboxes() (mmdet.models.dense_heads.YOLACTHead\nmethod), 371\nget_bboxes () (mmdet.models.dense_heads. YOLOV3Head\nmethod), 379\nget_bboxes () (mmdet.models.dense_heads. YOLOXHead\nmethod), 382\nget_bboxes() (mmdet.models.roi_heads.BBoxHead\nmethod), 383\nget_cat_ids() (mmdet.datasets.CocoDataset method),\n229\nget_cat_ids()\nmethod), 232\nget_cat_ids()\nmethod), 233\nget_cat_ids()\nmethod), 236\n\n(mmdet.datasets. ConcatDataset\n(mmdet.datasets. CustomDataset\n\n(mmdet.datasets.RepeatDataset\n\nget_anchors() (mmdet.models.dense_heads.AnchorHead get_cat_ids() (mmdet.datasets.XMLDataset method),\n\nmethod), 306\n\n237\n\nget_anchors() (mmdet.models.dense_heads.GuidedAnchogéfead1asses () (in module mmdet.core.evaluation), 220\n\nmethod), 344\n\nget_anchors() (mmdet.models.dense_heads.SABLRetinaHead\n\nmethod), 359\n\nget_anchors() (mmdet.models.dense_heads. VF NetHead\n\nmethod), 368\nget_ann_info()\nmethod), 229\nget_ann_info() (mmdet.datasets.CocoPanopticDataset\nmethod), 231\nget_ann_info()\nmethod), 233\nget_ann_info()\nmethod), 237\n\n(mmdet.datasets. CocoDataset\n\n(mmdet.datasets. CustomDataset\n\n(mmdet.datasets.XMLDataset\n\nget_atss_targets() (mmdet.models.dense_heads. VF NetHead\n\nmethod), 368\n\nget_bboxes () (mmdet.models.dense_heads.CascadeRPNHead\n\nmethod), 310\n\nget_classes() (mmdet.datasets.CustomDataset class\nmethod), 233\nget_cls_channels() (mmdet.models.losses.SeesawLoss\nmethod), 421\nget_extra_property()\n(mmdet.core.bbox.AssignResult\n192\nget_fcos_targets() (mmdet.models.dense_heads.VFNetHead\nmethod), 368\nget_gt_priorities()\n(mmdet.core.bbox. CenterRegionAssigner\nmethod), 195\nget_indexes()\nmethod), 246\nget_indexes()\nmethod), 247\nget_k_for_topk() (in module mmdet.core.export), 210\n\nmethod),\n\n(mmdet.datasets.pipelines.MixUp\n\n(mmdet.datasets.pipelines.Mosaic\n\nget_bboxes() (mmdet.models.dense_heads.CenterNetHeaget_loading_pipeline() (in module mmdet.datasets),\n\nmethod), 312\n\n238\n\nget_bboxes() (mmdet.models.dense_heads. CentripetalHe@ét_mask_scores () (mmdet.models.roi_heads.MaskloUHead\n\nmethod), 314\n\nmethod), 399\n\nget_bboxes() (mmdet.models.dense_heads.CornerHead get_neg_loss_single()\n\nmethod), 319\n\n(mmdet.models.dense_heads.AutoAssignHead\n\nget_bboxes() (mmdet.models.dense_heads.DeformableDETRHead method), 308\n\nmethod), 330\nget_bboxes() (mmdet.models.dense_heads. DETRHead\nmethod), 325\n\nget_bboxes () (mmdet.models.dense_heads.GuidedAnchorHead\n\nmethod), 344\nget_bboxes() (mmdet.models.dense_heads.PAAHead\n\nget_points() (mmdet.models.dense_heads.AnchorFreeHead\nmethod), 304\n\nget_points() (mmdet.models.dense_heads.RepPointsHead\n\nmethod), 355\n\nget_pos_loss() (mmdet.models.dense_heads.PAAHead\nmethod), 348\n\n450\n\nIndex\n", "vlm_text": "gen single level base anchors() method ),  348 ( mmdet.core.anchor.Anchor Generator method ), get_bboxes()  ( mmdet.models.dense heads.S ABL Retina Head 184 method ),  359 gen single level base anchors() get_bboxes()  ( mmdet.models.dense heads.Stage Cascade RP N Head ( mmdet.core.anchor.Legacy Anchor Generator method ),  365 method ),  187 get_bboxes()  ( mmdet.models.dense heads.YOLACTHead gen single level base anchors() method ),  371 ( mmdet.core.anchor.YO LO Anchor Generator get_bboxes()  ( mmdet.models.dense heads.YOLOV3Head method ),  190 method ),  379 generate coordinate()  ( in module mmdet.core.utils ), get_bboxes()  ( mmdet.models.dense heads.YOLOXHead 225 method ),  382 generate inputs and wrap model() ( in module get_bboxes() ( mmdet.models.roi_heads.BBoxHead mmdet.core.export ),  209 method ),  383 generate reg net()  ( mmdet.models.backbones.RegNet get cat ids()  ( mmdet.datasets.Coco Data set method ), method ),  283 229 Generic RoI Extractor ( class in get cat ids() ( mmdet.datasets.Con cat Data set mmdet.models.roi_heads ),  396 method ),  232 get accuracy() ( mmdet.models.losses.SeesawLoss get cat ids() ( mmdet.datasets.Custom Data set method ),  421 method ),  233 get activation() ( mmdet.models.losses.SeesawLoss get cat ids() ( mmdet.datasets.Repeat Data set method ),  421 method ),  236 get anchors()  ( mmdet.models.dense heads.AnchorHead get cat ids()  ( mmdet.datasets.XMLDataset method ), method ),  306 237 get anchors()  ( mmdet.models.dense heads.Guided Anchor Head get classes()  ( in module mmdet.core.evaluation ),  220 method ),  344 get classes()  ( mmdet.datasets.Custom Data set class get anchors()  ( mmdet.models.dense heads.S ABL Retina Head method ),  233 method ),  359 get cls channels()  ( mmdet.models.losses.SeesawLoss get anchors()  ( mmdet.models.dense heads.VFNetHead method ),  421 method ),  368 get extra property() get ann info() ( mmdet.datasets.Coco Data set ( mmdet.core.bbox.Assign Result method ), method ),  229 192 get ann info()  ( mmdet.datasets.Coco Pan optic Data set get fco s targets()  ( mmdet.models.dense heads.VFNetHead method ),  231 method ),  368 get ann info() ( mmdet.datasets.Custom Data set get gt priorities() method ),  233 ( mmdet.core.bbox.Center Region As signer get ann info() ( mmdet.datasets.XMLDataset method ),  195 method ),  237 get indexes() ( mmdet.datasets.pipelines.MixUp get at s s targets()  ( mmdet.models.dense heads.VFNetHead method ),  246 method ),  368 get indexes() ( mmdet.datasets.pipelines.Mosaic get_bboxes()  ( mmdet.models.dense heads.Cascade RP N Head method ),  247 method ),  310 get k for top k()  ( in module mmdet.core.export ),  210 get_bboxes()  ( mmdet.models.dense heads.Center Net Head get loading pipeline()  ( in module mmdet.datasets ), method ),  312 238 get_bboxes()  ( mmdet.models.dense heads.Centripetal Head get mask scores()  ( mmdet.models.roi_heads.Mask I oU Head method ),  314 method ),  399 get_bboxes()  ( mmdet.models.dense heads.CornerHead get ne g loss single() method ),  319 ( mmdet.models.dense heads.Auto Assign Head get_bboxes()  ( mmdet.models.dense heads.De formable DET RHead method ),  308 method ),  330 get_points()  ( mmdet.models.dense heads.Anchor Free Head get_bboxes()  ( mmdet.models.dense heads.DETRHead method ),  304 method ),  325 get_points()  ( mmdet.models.dense heads.Rep Points Head get_bboxes()  ( mmdet.models.dense heads.Guided Anchor Head method ),  355 method ),  344 get pos loss()  ( mmdet.models.dense heads.PAAHead get_bboxes() ( mmdet.models.dense heads.PAAHead method ),  348 "}
{"page": 458, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_458.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nget_pos_loss_single() get_targets() (mmdet.models.dense_heads. YOLACTProtonet\n(mmdet.models.dense_heads.AutoAssignHead method), 373\nmethod), 309 get_targets() (mmdet.models.dense_heads. YOLACTSegmHead\nget_results() (mmdet.models.dense_heads.DecoupledSOLOHead method), 375\n\nmethod), 328 get_targets() (mmdet.models.dense_heads. YOLOFHead\n\nget_results() (mmdet.models.dense_heads.SOLOHead\nmethod), 361\n\nget_roi_rel_points_test()\n(mmdet.models.roi_heads.MaskPointHead\nmethod), 401\n\nget_roi_rel_points_train()\n(mmdet.models.roi_heads.MaskPointHead\nmethod), 401\n\nget_root_logger() (in module mmdet.apis), 181\n\nget_sampled_approxs()\n\n(mmdet.models.dense_heads.GuidedAnchorHead\n\nmethod), 345\n\nmethod), 376\n\nget_targets() (mmdet.models.dense_heads. YOLOV3Head\n\nmethod), 379\n\nget_targets()  (mmdet.models.roi_heads.BBoxHead\nmethod), 384\n\nget_targets() (mmdet.models.roi_heads.DIHead\nmethod), 391\n\nget_targets() (mmdet.models.roi_heads.MaskloUHead\nmethod), 399\n\nget_targets() (mmdet.models.roi_heads.MaskPointHead\nmethod), 401\n\nGFL (class in mmdet.models.detectors), 264\n\nget_seg_masks () (mmdet.models.dense_heads. YOLACTP GtbHerad (class in mmdet.models.dense_heads), 340\n\nmethod), 373\n\nGHMC (class in mmdet.models.losses), 416\n\nget_seg_masks () (mmdet.models.roi_heads.F CNMaskHe&#MR (class in mmdet.models.losses), 416\n\nmethod), 393\n\nget_stages_from_blocks()\n(mmdet.models. backbones.RegNet\n283\n\nmethod),\n\nget_target () (mmdet.models.dense_heads.SABLRetinaHead\n\nGIoULoss (class in mmdet.models.losses), 417\nGlobalContextHead (class\nmmdet.models.roi_heads), 396\ngmm_separation_scheme()\n(mmdet.models.dense_heads.PAAHead\n\nin\n\nmethod), 359 method), 349\n\nget_targets() (mmdet.models.dense_heads.AnchorFreeHguild_anchors() (mmdet.core.anchor.AnchorGenerator\nmethod), 304 method), 184\n\nget_targets() (mmdet.models.dense_heads.AnchorHead grid_priors() (mmdet.core.anchor.AnchorGenerator\nmethod), 306 method), 184\n\nget_targets() (mmdet.models.dense_heads.ATSSHead grid_priors() (mmdet.core.anchor.MlvIPointGenerator\nmethod), 301 method), 188\n\nget_targets() (mmdet.models.dense_heads.AutoAssignH€ad dHead (class in mmdet.models.roi_heads), 397\nmethod), 309 GridRCNN (class in mmdet.models.detectors), 264\n\nget_targets() (mmdet.models.dense_heads.CenterNetHe&didRolHead (class in mmdet.models.roi_heads), 397\nmethod), 312 GroupSampler (class in mmdet.datasets), 234\n\nget_targets() (mmdet.models.dense_heads.CornerHead GroupSamp1ler (class in mmdet.datasets.samplers), 256\nmethod), 319 gt_inds (mmdet.core.bbox.AssignResult attribute), 191\n\nget_targets() (mmdet.models.dense_heads.DETRHead GuidedAnchorHead (class in\nmethod), 325 mmdet.models.dense_heads), 342\n\nget_targets() (mmdet.models.dense_heads.F COSHead\nmethod), 333\n\nget_targets() (mmdet.models.dense_heads.FoveaHead\nmethod), 337\n\nget_targets() (mmdet.models.dense_heads.GFLHead\nmethod), 341\n\nHourglassNet (class in mmdet.models.backbones), 278\nHRFPN (class in mmdet.models.necks), 296\nHRNet (class in mmdet.models.backbones), 276\nHTCMaskHead (class in mmdet.models.roi_heads), 397\nget_targetsQ (mumdet.models.dense_heads.PAAHead yypridTaskCascade (class in mmdet.models.detectors),\nmethod), 349 264\nget_targetsQ (mmdet.models.dense_heads.RepPointsHeggpri dTaskCascadeRoIHead\nmethod), 355 mmdet.models.roi_heads), 398\nget_targets() (mmdet.models.dense_heads.StageCascadeRPNHead\nmethod), 365 |\nget_targets() (mmdet.models.dense_heads. VF NetHead\nmethod), 369\n\n(class in\n\nimages_to_levels() (in module mmdet.core.anchor),\n191\n\nIndex 451\n", "vlm_text": "get pos loss single() get targets()  ( mmdet.models.dense heads.YO L ACT Proton et ( mmdet.models.dense heads.Auto Assign Head method ),  373 method ),  309 get targets()  ( mmdet.models.dense heads.YO LAC TSe gm Head get results()  ( mmdet.models.dense heads.Decoupled SOLO Head method ),  375 method ),  328 get targets()  ( mmdet.models.dense heads.YOLOFHead get results()  ( mmdet.models.dense heads.SOLOHead method ),  376 method ),  361 get targets()  ( mmdet.models.dense heads.YOLOV3Head get roi rel points test() method ),  379 ( mmdet.models.roi_heads.Mask Point Head get targets() ( mmdet.models.roi_heads.BBoxHead method ),  401 method ),  384 get roi rel points train() get targets() ( mmdet.models.roi_heads.DIIHead ( mmdet.models.roi_heads.Mask Point Head method ),  391 method ),  401 get targets()  ( mmdet.models.roi_heads.Mask I oU Head get root logger()  ( in module mmdet.apis ),  181 method ),  399 get sampled approx s() get targets()  ( mmdet.models.roi_heads.Mask Point Head ( mmdet.models.dense heads.Guided Anchor Head method ),  401 method ),  345 GFL  ( class in mmdet.models.detectors ),  264 get seg masks()  ( mmdet.models.dense heads.YO L ACT Proton et  ( class in mmdet.models.dense heads ),  340 method ),  373 GHMC  ( class in mmdet.models.losses ),  416 get seg masks()  ( mmdet.models.roi_heads.FC N Mask Head GHMR  ( class in mmdet.models.losses ),  416 method ),  393 GIoULoss  ( class in mmdet.models.losses ),  417 get stages from blocks() Global Context Head ( class in ( mmdet.models.backbones.RegNet method ), mmdet.models.roi_heads ),  396 283 g mm separation scheme() get_target()  ( mmdet.models.dense heads.S ABL Retina Head ( mmdet.models.dense heads.PAAHead method ),  359 method ),  349 get targets()  ( mmdet.models.dense heads.Anchor Free Head grid anchors()  ( mmdet.core.anchor.Anchor Generator method ),  304 method ),  184 get targets()  ( mmdet.models.dense heads.AnchorHead grid priors() ( mmdet.core.anchor.Anchor Generator method ),  306 method ),  184 get targets()  ( mmdet.models.dense heads.ATSSHead grid priors()  ( mmdet.core.anchor.Ml vl Point Generator method ),  301 method ),  188 get targets()  ( mmdet.models.dense heads.Auto Assign Head GridHead  ( class in mmdet.models.roi_heads ),  397 method ),  309 GridRCNN  ( class in mmdet.models.detectors ),  264 get targets()  ( mmdet.models.dense heads.Center Net Head Grid RoI Head  ( class in mmdet.models.roi_heads ),  397 method ),  312 Group Sampler  ( class in mmdet.datasets ),  234 get targets()  ( mmdet.models.dense heads.CornerHead Group Sampler  ( class in mmdet.datasets.samplers ),  256 method ),  319 gt_inds  ( mmdet.core.bbox.Assign Result attribute ),  191 get targets()  ( mmdet.models.dense heads.DETRHead Guided Anchor Head ( class in method ),  325 mmdet.models.dense heads ),  342 get targets()  ( mmdet.models.dense heads.FCOSHead method ),  333 H get targets()  ( mmdet.models.dense heads.FoveaHead  ( class in mmdet.models.backbones ),  278 Hourglass Net method ),  337 HRFPN  ( class in mmdet.models.necks ),  296 get targets()  ( mmdet.models.dense heads.GFLHead  ( class in mmdet.models.backbones ),  276 HRNet method ),  341 HTC Mask Head  ( class in mmdet.models.roi_heads ),  397 get targets()  ( mmdet.models.dense heads.PAAHead  ( class in mmdet.models.detectors ), Hybrid Task Cascade method ),  349 264 get targets()  ( mmdet.models.dense heads.Rep Points Head ( class in Hybrid Task Cascade RoI Head method ),  355 mmdet.models.roi_heads ),  398 get targets()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  365 I get targets()  ( mmdet.models.dense heads.VFNetHead images to levels()  ( in module mmdet.core.anchor ), method ),  369 191 "}
{"page": 459, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_459.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nImageToTensor (class in mmdet.datasets.pipelines), 243 init_weights() (mmdet.models.dense_heads.DETRHead\n\ninference_detector() (in module mmdet.apis), 181 method), 326\nInfiniteBatchSampler (class in init_weights() (mmdet.models.dense_heads.EmbeddingRPNHead\nmmdet.datasets.samplers), 256 method), 331\nInfiniteGroupBatchSampler (class in init_weights() (mmdet.models.dense_heads.RetinaSepBNHead\nmmdet.datasets.samplers), 256 method), 357\ninfo (mmdet.core.bbox.AssignResult property), 192 init_weights() (mmdet.models.dense_heads. YOLOF Head\ninfo (mmdet.core.bbox.SamplingResult property), 201 method), 377\ninit_assigner_sampler() init_weights() (mmdet.models.dense_heads. YOLOV3Head\n(mmdet.models.roi_heads.BaseRolHead method), 380\nmethod), 387 init_weights() (mmdet.models.dense_heads. YOLOXHead\ninit_assigner_sampler() method), 382\n(mmdet.models.roi_heads.CascadeRolHead init_weights() (mmdet.models.necks.CTResNetNeck\nmethod), 388 method), 291\ninit_assigner_sampler() init_weights() (mmdet.models.necks.FPN_CARAFE\n(mmdet.models.roi_heads.StandardRolHead method), 296\nmethod), 411 init_weights() (mmdet.models.necks.NASFCOS_FPN\ninit_bbox_head() (mmdet.models.roi_heads.BaseRolIHead method), 297\nmethod), 387 init_weights() (mmdet.models.necks.RFP method),\ninit_bbox_head() (mmdet.models.roi_heads.CascadeRolHead 298\nmethod), 388 init_weights() (mmdet.models.roi_heads.CoarseMaskHead\ninit_bbox_head() (mmdet.models.roi_heads.StandardRoIHead — method), 390\nmethod), 411 init_weights() = (mmdet.models.roi_heads.DIIHead\ninit_detector() (in module mmdet.apis), 181 method), 391\ninit_mask_head() (mmdet.models.roi_heads.BaseRoIHeaihit_weights () (mmdet.models.roi_heads.F CNMaskHead\nmethod), 387 method), 394\ninit_mask_head() (mmdet.models.roi_heads.CascadeRolifxitl_weights () (mmdet.models.utils. Transformer\nmethod), 389 method), 433\ninit_mask_head() (mmdet.models.roi_heads.SCNetRolHdmbtaBoost (class in mmdet.datasets.pipelines), 243\nmethod), 407 InstanceBalancedPosSampler (class in\ninit_mask_head() (mmdet.models.roi_heads.StandardRoIHead mmdet.core.bbox), 197\nmethod), 411 interpolate_as() (in module mmdet.models.utils), 435\ninit_point_head() (mmdet.models.roi_heads. PointRendRaWtakkdResidual (class in mmdet.models.utils), 428\nmethod), 402 IoUBalancedNegSampler (class in mmdet.core.bbox),\ninit_weights() (mmdet.models.backbones.DetectoRS_ResNet 197\nmethod), 276 IoULoss (class in mmdet.models.losses), 418\n\ninit_weights() (mmdet.models.backbones.HourglassNet\nmethod), 279\n\ninit_weights() (mmdet.models. backbones. Pyramid Visiongie#VedgeDi stillationKLDivLoss (class in\n\n-_ method), 281 mmdet.models.losses), 418\ninit_weights()  (mmdet.models.backbones.SSDVGG  KnowledgeDistillationSingleStageDetector\nmethod), 289 (class in mmdet.models.detectors), 264\n\ninit_weights() (mmdet.models.backbones.SwinTransformer\nmethod), 290\ninit_weights() (mmdet.models.dense_heads.AutoAssignH4¢q, ss (class in mmdet.models.losses), 419\n\n. method), 310 labels (mmdet.core.bbox.AssignResult attribute), 191\ninit_weights() (mmdet.models.dense_heads. CenterNetHegg ead (class in mmdet.models.dense_heads), 345\n\n| method), 313 . LearnedPositionalEncoding (class in\ninit_weights() (mmdet.models.dense_heads.CentripetalHead mmdet.models.utils), 428\n\n| method), 315 LegacyAnchorGenerator (class in mmdet.core.anchor),\ninit_weights() (mmdet.models.dense_heads.CornerHead 186\n\nmethod), 320 load_annotations() — (mmdet.datasets.CocoDataset\ninit_weights() (mmdet.models.dense_heads.DeformableDE' TRHeag ethod), 229\n\nmethod), 330\n\n452 Index\n", "vlm_text": "Image To Tensor  ( class in mmdet.datasets.pipelines ),  243 in it weights()  ( mmdet.models.dense heads.DETRHead inference detector()  ( in module mmdet.apis ),  181 method ),  326 Infinite Batch Sampler ( class in in it weights()  ( mmdet.models.dense heads.Embedding RP N Head mmdet.datasets.samplers ),  256 method ),  331 Infinite Group Batch Sampler ( class in in it weights()  ( mmdet.models.dense heads.RetinaS epB N Head mmdet.datasets.samplers ),  256 method ),  357 info  ( mmdet.core.bbox.Assign Result property ),  192 in it weights()  ( mmdet.models.dense heads.YOLOFHead info  ( mmdet.core.bbox.Sampling Result property ),  201 method ),  377 in it as signer sampler() in it weights()  ( mmdet.models.dense heads.YOLOV3Head ( mmdet.models.roi_heads.Base RoI Head method ),  380 method ),  387 in it weights()  ( mmdet.models.dense heads.YOLOXHead in it as signer sampler() method ),  382 ( mmdet.models.roi_heads.Cascade RoI Head in it weights() ( mmdet.models.necks.C TRes Net Neck method ),  388 method ),  291 in it as signer sampler() in it weights()  ( mmdet.models.necks.FPN_CARAFE ( mmdet.models.roi_heads.Standard RoI Head method ),  296 method ),  411 in it weights()  ( mmdet.models.necks.NAS FCO S FP N in it b box head()  ( mmdet.models.roi_heads.Base RoI Head method ),  297 method ),  387 in it weights()  ( mmdet.models.necks.RFP method ), in it b box head() (mmdet.models.roi_heads.Cascade RoI Head298method ),  388 in it weights()  ( mmdet.models.roi_heads.Coarse Mask Head in it b box head()  ( mmdet.models.roi_heads.Standard RoI Head method ),  390 method ),  411 in it weights() ( mmdet.models.roi_heads.DIIHead in it detector()  ( in module mmdet.apis ),  181 method ),  391 in it mask head()  ( mmdet.models.roi_heads.Base RoI Head in it weights()  ( mmdet.models.roi_heads.FC N Mask Head method ),  387 method ),  394 in it mask head() (mmdet.models.roi_heads.Cascade RoI Headin it weights()(mmdet.models.utils.Transformermethod ),  389 method ),  433 in it mask head()  ( mmdet.models.roi_heads.S CNet RoI Head InstaBoost  ( class in mmdet.datasets.pipelines ),  243 method ),  407 Instance Balanced Pos Sampler ( class in in it mask head() (mmdet.models.roi_heads.Standard RoI Headmmdet.core.bbox), 197method ),  411 interpolate as()  ( in module mmdet.models.utils ),  435 in it point head()  ( mmdet.models.roi_heads.Point Rend RoI Head Inverted Residual  ( class in mmdet.models.utils ),  428 method ),  402 I oU Balanced Ne g Sampler  ( class in mmdet.core.bbox ), in it weights()  ( mmdet.models.backbones.DetectoRS Res Net 197 method ),  276 IoULoss  ( class in mmdet.models.losses ),  418 in it weights()  ( mmdet.models.backbones.Hourglass Net method ),  279 K in it weights()  ( mmdet.models.backbones.Pyramid Vision Transformer ( class in Knowledge Distillation KL Div Loss method ),  281 mmdet.models.losses ),  418 in it weights() ( mmdet.models.backbones.SSDVGG Knowledge Distillation Single Stage Detector method ),  289 ( class in mmdet.models.detectors ),  264 in it weights()  ( mmdet.models.backbones.S win Transformer method ),  290 L in it weights()  ( mmdet.models.dense heads.Auto Assign Head L1Loss  ( class in mmdet.models.losses ),  419 method ),  310 labels  ( mmdet.core.bbox.Assign Result attribute ),  191 in it weights()  ( mmdet.models.dense heads.Center Net Head LDHead  ( class in mmdet.models.dense heads ),  345 method ),  313 Learned Positional Encoding ( class in in it weights()  ( mmdet.models.dense heads.Centripetal Head mmdet.models.utils ),  428 method ),  315 Legacy Anchor Generator  ( class in mmdet.core.anchor ), in it weights()  ( mmdet.models.dense heads.CornerHead 186 method ),  320 load annotations() ( mmdet.datasets.Coco Data set in it weights()  ( mmdet.models.dense heads.De formable DET RHead method ),  229 method ),  330 "}
{"page": 460, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_460.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmethod), 231\n\noad_annotations() (mmdet.datasets.CustomDataset\nmethod), 233\n\noad_annotations() (mmdet.datasets.LVISVO5Dataset\nmethod), 235\n\noad_annotations() (mmdet.datasets.LVISV1 Dataset\nmethod), 235\n\noad_annotations () (mmdet.datasets. WIDER FaceDatasei\nmethod), 237\n\nt\n\noad_annotations() (mmdet.datasets.XMLDataset\nmethod), 237\noad_proposals() (mmdet.datasets. CustomDataset\n\nmethod), 234\nLoadAnnotations (class in mmdet.datasets.pipelines),\n\n244\nLoadImageFromFile (class in\nmmdet.datasets.pipelines), 244\nLoadImageFromWebcam (class in\n\nmmdet.datasets.pipelines), 245\nLoadMultiChannelImageFromFiles\n\nmmdet.datasets.pipelines), 245\nLoadProposals (class in mmdet.datasets.pipelines), 245\n\n(class in\n\noss() — (mmdet.models.dense_heads.AnchorFreeHead\nmethod), 304\n\nossQ() (mmdet.models.dense_heads.AnchorHead\nmethod), 307\n\nossQ() (mmdet.models.dense_heads.ATSSHead\nmethod), 301\n\nossQ() (mmdet.models.dense_heads.AutoAssignHead\nmethod), 310\n\noss() (mmdet.models.dense_heads.CascadeRPNHead\nmethod), 310\n\nossQ() (mmdet.models.dense_heads.CenterNetHead\nmethod), 313\n\nossQ() (mmdet.models.dense_heads. CentripetalHead\nmethod), 315\n\nossQ() (mmdet.models.dense_heads.CornerHead\n\nmethod), 320\n\noss() (mmdet.models.dense_heads.DecoupledSOLOHead.\nmethod), 328\n\noss() (mmdet.models.dense_heads.DeformableDETRHead\nmethod), 330\n\nossQ() (mmdet.models.dense_heads. DETRHead\nmethod), 326\n\nossQ() (mmdet.models.dense_heads.F COSHead\nmethod), 333\n\nossQ() (mmdet.models.dense_heads.FoveaHead\n\nmethod), 337\noss() (mmdet.models.dense_heads.FreeAnchorRetinaHead\nmethod), 338\n\nossQ() (mmdet.models.dense_heads.F SAF Head\nmethod), 335\nossQ() (mmdet.models.dense_heads.GARPNHead\n\noad_annotations() (mmdet.datasets.CocoPanopticDataset\n\nmethod), 339\n\noss() (mmdet.models.dense_heads.GFLHead method),\n341\n\noss() (mmdet.models.dense_heads.GuidedAnchorHead\nmethod), 345\n\noss() (mmdet.models.dense_heads.LDHead method),\n\n346\n\noss() (mmdet.models.dense_heads.PAAHead method),\n350\n\noss() = (mmdet.models.dense_heads.PISARetinaHead\nmethod), 351\n\nossQ() (mmdet.models.dense_heads.PISASSDHead\nmethod), 352\n\nossQ() (mmdet.models.dense_heads.RepPointsHead\n\nmethod), 355\noss() (mmdet.models.dense_heads.RPNHead method),\n353\n\noss()  (mmdet.models.dense_heads.SABLRetinaHead\nmethod), 360\nossQ() (mmdet.models.dense_heads.SOLOHead\n\nmethod), 362\n\noss() (mmdet.models.dense_heads.SSDHead method),\n363\n\noss() (mmdet.models.dense_heads.StageCascadeRPNHead\nmethod), 365\n\nossQ() (mmdet.models.dense_heads. VF NetHead\nmethod), 369\n\nossQ() (mmdet.models.dense_heads. YOLACTHead\nmethod), 372\n\noss() (mmdet.models.dense_heads. YOLACTProtonet\n\nmethod), 374\noss() (mmdet.models.dense_heads. YOLACTSegmHead\nmethod), 375\n\nossQ() (mmdet.models.dense_heads. YOLOF Head\nmethod), 377\n\nossQ() (mmdet.models.dense_heads. YOLOV3Head\nmethod), 380\n\nossQ() (mmdet.models.dense_heads. YOLOXHead\n\nmethod), 382\noss() (mmdet.models.roi_heads.DIIHead method), 391\nossQ() (mmdet.models.roi_heads.F CNMaskHead\nmethod), 394\n\noss() = (mmdet.models.roi_heads.GlobalContextHead\nmethod), 397\nossQ() (mmdet.models.roi_heads.MaskPointHead\n\nmethod), 401\n\noss_single() (mmdet.models.dense_heads.AnchorHead\nmethod), 307\n\noss_single() (mmdet.models.dense_heads.ATSSHead\nmethod), 302\n\noss_single() (mmdet.models.dense_heads.CentripetalHead\nmethod), 316\n\noss_single() (mmdet.models.dense_heads.CornerHead\nmethod), 321\n\nIndex\n\n453\n", "vlm_text": "load annotations()  ( mmdet.datasets.Coco Pan optic Data set method ),  231 load annotations()  ( mmdet.datasets.Custom Data set method ),  233 load annotations()  ( mmdet.datasets.LV IS V 05 Data set method ),  235 load annotations()  ( mmdet.datasets.LV IS V 1 Data set method ),  235 load annotations()  ( mmdet.datasets.WIDER Face Data set method ),  237 load annotations() ( mmdet.datasets.XMLDataset method ),  237 load proposals() ( mmdet.datasets.Custom Data set method ),  234 Load Annotations  ( class in mmdet.datasets.pipelines ), 244 Load Image From File ( class in mmdet.datasets.pipelines ),  244 Load Image From Webcam ( class in mmdet.datasets.pipelines ),  245 Load MultiChannel Image From Files ( class in mmdet.datasets.pipelines ),  245 Load Proposals  ( class in mmdet.datasets.pipelines ),  245 loss() ( mmdet.models.dense heads.Anchor Free Head method ),  304 loss() ( mmdet.models.dense heads.AnchorHead method ),  307 loss() ( mmdet.models.dense heads.ATSSHead method ),  301 loss() ( mmdet.models.dense heads.Auto Assign Head method ),  310 loss()  ( mmdet.models.dense heads.Cascade RP N Head method ),  310 loss() ( mmdet.models.dense heads.Center Net Head method ),  313 loss() ( mmdet.models.dense heads.Centripetal Head method ),  315 loss() ( mmdet.models.dense heads.CornerHead method ),  320 loss()  ( mmdet.models.dense heads.Decoupled SOLO Head method ),  328 loss()  ( mmdet.models.dense heads.De formable DET RHead method ),  330 loss() ( mmdet.models.dense heads.DETRHead method ),  326 loss() ( mmdet.models.dense heads.FCOSHead method ),  333 loss() ( mmdet.models.dense heads.FoveaHead method ),  337 loss()  ( mmdet.models.dense heads.Free Anchor Retina Head method ),  338 loss() ( mmdet.models.dense heads.FSAFHead method ),  335 loss() ( mmdet.models.dense heads.GARPNHead \nmethod ),  339 loss()  ( mmdet.models.dense heads.GFLHead method ), 341 loss()  ( mmdet.models.dense heads.Guided Anchor Head method ),  345 loss()  ( mmdet.models.dense heads.LDHead method ), 346 loss()  ( mmdet.models.dense heads.PAAHead method ), 350 loss() ( mmdet.models.dense heads.PISA Retina Head method ),  351 loss() ( mmdet.models.dense heads.PISA SSD Head method ),  352 loss() ( mmdet.models.dense heads.Rep Points Head method ),  355 loss()  ( mmdet.models.dense heads.RPNHead method ), 353 loss() ( mmdet.models.dense heads.S ABL Retina Head method ),  360 loss() ( mmdet.models.dense heads.SOLOHead method ),  362 loss()  ( mmdet.models.dense heads.SSDHead method ), 363 loss()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  365 loss() ( mmdet.models.dense heads.VFNetHead method ),  369 loss() ( mmdet.models.dense heads.YOLACTHead method ),  372 loss() ( mmdet.models.dense heads.YO L ACT Proton et method ),  374 loss()  ( mmdet.models.dense heads.YO LAC TSe gm Head method ),  375 loss() ( mmdet.models.dense heads.YOLOFHead method ),  377 loss() ( mmdet.models.dense heads.YOLOV3Head method ),  380 loss() ( mmdet.models.dense heads.YOLOXHead method ),  382 loss()  ( mmdet.models.roi_heads.DIIHead method ),  391 loss() ( mmdet.models.roi_heads.FC N Mask Head method ),  394 loss() ( mmdet.models.roi_heads.Global Context Head method ),  397 loss() ( mmdet.models.roi_heads.Mask Point Head method ),  401 loss single()  ( mmdet.models.dense heads.AnchorHead method ),  307 loss single()  ( mmdet.models.dense heads.ATSSHead method ),  302 loss single()  ( mmdet.models.dense heads.Centripetal Head method ),  316 loss single()  ( mmdet.models.dense heads.CornerHead method ),  321 "}
{"page": 461, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_461.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\noss_sing\nmethod), 326\n\noss_single() (mmdet.models.dense_heads.GFLHead\nmethod), 341\n\noss_single(Q) (mmdet.models.dense_heads.LDHead\nmethod), 347\n\ne(Q (mmdet.models.dense_heads.DETRHead MaskIoUHead (class in mmdet.models.roi_heads), 399\n\nMaskPointHead (class in mmdet.models.roi_heads), 400\n\nMaskRCNN (class in mmdet.models.detectors), 265\n\nMaskScoringRCNN (class in mmdet.models.detectors),\n265\n\nMaskScoringRoIHead (class in\n\noss_single() (mmdet.models.dense_heads.SSDHead mmdet.models.roi_heads), 402\nmethod), 363 max_overlaps (mmdet.core.bbox.AssignResult —_at-\noss_single() (mmdet.models.dense_heads.StageCascadeRPNHeadribute), 191\nmethod), 366 MaxIoUAssigner (class in mmdet.core.bbox), 197\noss_single() (mmdet.models.dense_heads. YOLOV3Heaiherge_aug_bboxes() (in module\nmethod), 380 mmdet.core.post_processing), 222\noss_single_OHEM() (mmdet.models.dense_heads. YOLAGEHgedaug_masks () (in module\nmethod), 372 mmdet.core.post_processing), 223\nLVISDataset (in module mmdet.datasets), 234 merge_aug_proposals() (in module\nLVISVO5Dataset (class in mmdet.datasets), 234 mmdet.core.post_processing), 223\nLVISV1iDataset (class in mmdet.datasets), 235 merge_aug_results()\n(mmdet.models.detectors.CenterNet method),\nM 262\nmake_conv_res_block() merge_aug_results()\n(mmdet.models. backbones.Darknet static (mmdet.models.detectors.CornerNet method),\nmethod), 275 263\nmake_divisible() (in module mmdet.models.utils), 436 merge_aug_scores () (in module\n\nmake_layer() (mmdet.models.backbones.MobileNetV2\nmethod), 279\n\nmake_res_layer() (mmdet.models.backbones.DetectoRS_ResNet\nmethod), 276\nmake_res_\n\nmethod), 276\n\nmake_res_layer() (mmdet.models.backbones.Res2Net\nmethod), 284\n\nmake_res_layer() (mmdet.models.backbones.ResNeSt\nmethod), 285\n\nmake_res_layer()\nmethod), 287\n\nmake_res_layer() (mmdet.models.backbones.ResNeXt\nmethod), 285\n\nmake_stage_plugins()\n(mmdet.models.backbones.ResNet\n287\n\n(mmdet.models.backbones.ResNet\n\nmethod),\n\nmmdet.core.post_processing), 223\nmerge_trident_bboxes()\n(mmdet.models.roi_heads.TridentRoIHead\nmethod), 412\n\nayer() (mmdet.models.backbones.DetectoRS _YeseXRandomCr ‘op (class in mmdet.datasets. pipelines),\n\n245\nMixUp (class in mmdet.datasets.pipelines), 245\nMlvlPointGenerator (class in mmdet.core.anchor),\n\n188\nmmdet.apis\nmodule, 18\nmmdet.core.anchor\nmodule, 183\nmmdet .core.bbox\nmodule, 19\nmmdet.core.evaluation\nmodule, 219\n\nmap_roi_levels() (mmdet. models.roi_heads.SingleRoIEMPOee3r- core.export\n\nmethod), 408\nmapper () (mmdet.datasets.pipelines.Albu static method),\n240\n\nmask2ndarray() (in module mmdet.core.utils), 225\n\nmask_cross_entropy() (in module\nmmdet.models.losses), 423\n\nmask_matrix_nms() (in module\n\nmmdet.core.post_processing), 222\n\nmodule, 208\nmmdet.core.mask\nmodule, 21\nmmdet .core.post_processing\nmodule, 22\nmmdet.core.utils\nmodule, 224\nmmdet.datasets\n\nmask_onnx_export () (mmdet.models. roi_heads.PointRendRolHed#.le, 227\n\nmethod), 403\n\nmmdet.datasets.api_wrappers\n\nmask_onnx_export () (mmdet.models. roi_heads.StandardRolHeyaule, 257\n\nmethod), 411\nmask_target() (in module mmdet.core.mask), 218\n\nmmdet.datasets.pipelines\nmodule, 239\n\n454\n\nIndex\n", "vlm_text": "loss single()  ( mmdet.models.dense heads.DETRHead Mask I oU Head  ( class in mmdet.models.roi_heads ),  399 method ),  326 Mask Point Head  ( class in mmdet.models.roi_heads ),  400 loss single()  ( mmdet.models.dense heads.GFLHead MaskRCNN  ( class in mmdet.models.detectors ),  265 method ),  341 Mask Scoring R CNN  ( class in mmdet.models.detectors ), loss single() ( mmdet.models.dense heads.LDHead 265 method ),  347 Mask Scoring RoI Head ( class in loss single()  ( mmdet.models.dense heads.SSDHead mmdet.models.roi_heads ),  402 method ),  363 max overlaps ( mmdet.core.bbox.Assign Result at- loss single()  ( mmdet.models.dense heads.Stage Cascade RP N Head tribute ),  191 method ),  366 MaxI oU As signer  ( class in mmdet.core.bbox ),  197 loss single()  ( mmdet.models.dense heads.YOLOV3Head merge aug b boxes() ( in module method ),  380 mmdet.core.post processing ),  222 loss single OH EM()  ( mmdet.models.dense heads.YOLACTHead merge aug masks() ( in module method ),  372 mmdet.core.post processing ),  223 LV IS Data set  ( in module mmdet.datasets ),  234 merge aug proposals() ( in module LV IS V 05 Data set  ( class in mmdet.datasets ),  234 mmdet.core.post processing ),  223 LV IS V 1 Data set  ( class in mmdet.datasets ),  235 merge aug results() ( mmdet.models.detectors.CenterNet method ), M 262 merge aug results() make con v res block() ( mmdet.models.backbones.Darknet static ( mmdet.models.detectors.CornerNet method ), method ),  275 263  ( in module mmdet.models.utils ),  436 merge aug scores() ( in module make divisible() make_layer()  ( mmdet.models.backbones.Mobile Ne tV 2 mmdet.core.post processing ),  223 method ),  279 merge trident b boxes() make res layer()  ( mmdet.models.backbones.DetectoRS Res Net ( mmdet.models.roi_heads.Trident RoI Head method ),  276 method ),  412  ( mmdet.models.backbones.DetectoRS Res NeXt MinI oU Random Crop  ( class in mmdet.datasets.pipelines ), make res layer() method ),  276 245  ( mmdet.models.backbones.Res2Net MixUp  ( class in mmdet.datasets.pipelines ),  245 make res layer() method ),  284 Ml vl Point Generator  ( class in mmdet.core.anchor ), make res layer()  ( mmdet.models.backbones.ResNeSt 188 method ),  285 mmdet.apis ( mmdet.models.backbones.ResNet module ,  181 make res layer() method ),  287 mmdet.core.anchor  ( mmdet.models.backbones.ResNeXt module ,  183 make res layer() method ),  285 mmdet.core.bbox module ,  191 make stage plugins() ( mmdet.models.backbones.ResNet method ), mmdet.core.evaluation 287 module ,  219  ( mmdet.models.roi_heads.Single RoI Extractor mmdet.core.export map roi levels() method ),  408 module ,  208  ( mmdet.datasets.pipelines.Albu static method ), mmdet.core.mask mapper() 240module, 211 ( in module mmdet.core.utils ),  225 mmdet.core.post processing mask 2 nd array()( in module module ,  221 mask cross entropy() mmdet.models.losses ),  423 mmdet.core.utils ( in module module ,  224 mask matrix nm s() mmdet.core.post processing ),  222 mmdet.datasets  ( mmdet.models.roi_heads.Point Rend RoI Head module ,  227 mask on nx export()method ),  403 mmdet.datasets.api wrappers  ( mmdet.models.roi_heads.Standard RoI Head module ,  257 mask on nx export()method ),  411 mmdet.datasets.pipelines  ( in module mmdet.core.mask ),  218 module ,  239 mask target() "}
{"page": 462, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_462.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmdet.datasets.samplers\n\nmodule, 256\n\nmmdet .models.backbones\n\nmodule, 273\n\nmmdet .models.dense_heads\n\nmodule, 301\n\nmmndet .models.detectors\n\nmodule, 259\n\nmmdet .models.losses\n\nmodule, 412\n\nmmdet .models.necks\n\nmodule, 291\n\nmmdet .models.roi_heads\n\nmodule, 383\n\nmmndet .models.utils\n\nmodule, 425\n\nMobileNetV2 (class in mmdet.models.backbones), 279\n\nmodule\nmmdet.apis, 181\nmmdet.core. anchor, 183\nmmdet.core.bbox, 191\nmmdet.core.evaluation, 219\nmmdet.core. export, 208\nmmdet.core.mask, 211\nmmdet.core.post_processing, 221\nmmdet.core.utils, 224\nmmdet.datasets, 227\nmmdet.datasets.api_wrappers, 257\nmmdet.datasets.pipelines, 239\nmmdet.datasets.samplers, 256\nmmdet.models.backbones, 273\nmmdet.models.dense_heads, 301\nmmdet.models.detectors, 259\nmmdet.models.losses, 412\nmmdet.models.necks, 291\nmmdet.models.roi_heads, 383\nmmdet.models.utils, 425\n\nMosaic (class in mmdet.datasets.pipelines), 246\n\nmse_loss() (in module mmdet.models.losses), 424\n\nMSELoss (class in mmdet.models.losses), 419\n\nmulti_applyQ (in module mmdet.core.utils), 226\n\nmulti_gpu_test (© (in module mmdet.apis), 181\n\nmulticlass_nms() (in\n\nmmdet.core.post_processing), 223\nMultiImageMixDataset (class in mmdet.datasets), 235\nMultiScaleFlipAug (class in\nmmdet.datasets.pipelines), 247\n\nmodule\n\nN\n\nNASFCOS (class in mmdet.models.detectors), 265\nNASFCOS_FPN (class in mmdet.models.necks), 296\nNASFCOSHead (class in mmdet.models.dense_heads), 347\nNASFPN (class in mmdet.models.necks), 297\nnchw_to_nlc() (in module mmdet.models.utils), 436\n\nnegative_bag_loss()\n(mmdet.models.dense_heads.FreeAnchorRetinaHead\nmethod), 338\n\nnlc_to_nchw() (in module mmdet.models.utils), 436\n\nnorm1 (mmdet.models.backbones.HRNet property), 278\n\nnorm1 (mmdet.models.backbones.ResNet property), 288\n\nnorml (mmdet.models.utils.SimplifiedBasicBlock prop-\nerty), 432\n\nnorm2 (mmdet.models.backbones.HRNet property), 278\n\nnorm2 (mmdet.models.utils.SimplifiedBasicBlock prop-\nerty), 432\n\nNormalize (class in mmdet.datasets.pipelines), 248\n\nNormedConv2d (class in mmdet.models.utils), 429\n\nNormedLinear (class in mmdet.models.utils), 429\n\nnum_anchors (mmdet.models.dense_heads.SSDHead\nproperty), 364\n\nnum_anchors (mmdet.models.dense_heads.VFNetHead\nproperty), 370\n\nnum_anchors (mmdet.models.dense_heads. YOLOV3Head\nproperty), 380\n\nnum_attrib (mmdet.models.dense_heads. YOLOV3Head\nproperty), 380\n\nnum_base_anchors (mmdet.core.anchor.AnchorGenerator\nproperty), 185\n\nnum_base_priors (mmdet.core.anchor.AnchorGenerator\nproperty), 185\n\nnum_base_priors (mmdet.core.anchor.MlvlPointGenerator\nproperty), 188\n\nnum_gts (mmdet.core.bbox.AssignResult attribute), 191\n\nnum_inputs (mmdet.models.roi_heads.BaseRolExtractor\nproperty), 387\n\nnum_levels (mmdet.core.anchor.AnchorGenerator\nproperty), 185\n\nnum_levels = (mmdet.core.anchor.MlvIPointGenerator\nproperty), 188\n\nnum_levels (mmdet.core.anchor. YOLOAnchorGenerator\nproperty), 190\n\nnum_preds (mmdet.core.bbox.AssignResult property),\n192\n\nO\n\noffset_to_pts() (mmdet.models.dense_heads.RepPointsHead\nmethod), 356\n\nOHEMSampler (class in mmdet.core.bbox), 199\n\nonnx_export () (mmdet.models.dense_heads.CornerHead\n\nmethod), 322\n\nonnx_export () (mmdet.models.dense_heads. DETRHead\n\nmethod), 327\n\nonnx_export() (mmdet.models.dense_heads.RPNHead\n\nmethod), 353\n\nonnx_export () (mmdet.models.dense_heads. YOLOV3Head\n\nmethod), 380\n\nonnx_export()\nmethod), 263\n\n(mmdet.models.detectors. DETR\n\nIndex\n\n455\n", "vlm_text": "mmdet.datasets.samplers module ,  256 mmdet.models.backbones module ,  273 mmdet.models.dense heads module ,  301 mmdet.models.detectors module ,  259 mmdet.models.losses module ,  412 mmdet.models.necks module ,  291 mmdet.models.roi_heads module ,  383 mmdet.models.utils module ,  425 Mobile Ne tV 2  ( class in mmdet.models.backbones ),  279 module mmdet.apis ,  181 mmdet.core.anchor ,  183 mmdet.core.bbox ,  191 mmdet.core.evaluation ,  219 mmdet.core.export ,  208 mmdet.core.mask ,  211 mmdet.core.post processing ,  221 mmdet.core.utils, 224mmdet.datasets ,  227 mmdet.datasets.api wrappers ,  257 mmdet.datasets.pipelines ,  239 mmdet.datasets.samplers ,  256 mmdet.models.backbones ,  273 mmdet.models.dense heads ,  301 mmdet.models.detectors ,  259 mmdet.models.losses ,  412 mmdet.models.necks ,  291 mmdet.models.roi_heads ,  383 mmdet.models.utils ,  425 Mosaic  ( class in mmdet.datasets.pipelines ),  246 mse_loss()  ( in module mmdet.models.losses ),  424 MSELoss  ( class in mmdet.models.losses ),  419 multi apply()  ( in module mmdet.core.utils ),  226 multi gpu test()  ( in module mmdet.apis ),  181 multi class nm s() ( in module mmdet.core.post processing ),  223 Multi Image Mix Data set  ( class in mmdet.datasets ),  235 Multi Scale Flip Aug ( class in mmdet.datasets.pipelines ),  247 \nN \nNASFCOS  ( class in mmdet.models.detectors ),  265 NAS FCO S FP N  ( class in mmdet.models.necks ),  296 NAS FCO S Head  ( class in mmdet.models.dense heads ),  347 NASFPN  ( class in mmdet.models.necks ),  297 nch w to nl c()  ( in module mmdet.models.utils ),  436 \nnegative bag loss() ( mmdet.models.dense heads.Free Anchor Retina Head method ),  338 nl c to nch w()  ( in module mmdet.models.utils ),  436 norm1  ( mmdet.models.backbones.HRNet property ),  278 norm1  ( mmdet.models.backbones.ResNet property ),  288 norm1  ( mmdet.models.utils.Simplified Basic Block prop- erty ),  432 norm2  ( mmdet.models.backbones.HRNet property ),  278 norm2  ( mmdet.models.utils.Simplified Basic Block prop- erty ),  432 Normalize  ( class in mmdet.datasets.pipelines ),  248 Norm edC on v 2 d  ( class in mmdet.models.utils ),  429 Norm ed Linear  ( class in mmdet.models.utils ),  429 num anchors ( mmdet.models.dense heads.SSDHead property ),  364 num anchors ( mmdet.models.dense heads.VFNetHead property ),  370 num anchors  ( mmdet.models.dense heads.YOLOV3Head property ),  380 num_attrib  ( mmdet.models.dense heads.YOLOV3Head property ),  380 num base anchors  ( mmdet.core.anchor.Anchor Generator property ),  185 num base priors  ( mmdet.core.anchor.Anchor Generator property ),  185 num base priors  ( mmdet.core.anchor.Ml vl Point Generator property ),  188 num_gts  ( mmdet.core.bbox.Assign Result attribute ),  191 num_inputs  ( mmdet.models.roi_heads.Base RoI Extractor property ),  387 num_levels ( mmdet.core.anchor.Anchor Generator property ),  185 num_levels ( mmdet.core.anchor.Ml vl Point Generator property ),  188 num_levels  ( mmdet.core.anchor.YO LO Anchor Generator property ),  190 num_preds ( mmdet.core.bbox.Assign Result property ), 192 \n\noffset to pts()  ( mmdet.models.dense heads.Rep Points Head method ),  356 OH EM Sampler (class in mmdet.core.bbox), 199on nx export()  ( mmdet.models.dense heads.CornerHead method ),  322 on nx export()  ( mmdet.models.dense heads.DETRHead method ),  327 on nx export()  ( mmdet.models.dense heads.RPNHead method ),  353 on nx export()  ( mmdet.models.dense heads.YOLOV3Head method ),  380 on nx export() ( mmdet.models.detectors.DETR method ),  263 "}
{"page": 463, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_463.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nonnx_export () (mmdet.models.detectors.SingleStageDetecpwint_recall_summary ()\n\nmethod), 268\nonnx_export()\nmethod), 272\nonnx_export() (mmdet.models.roi_heads.BBoxHead\nmethod), 384\n\n(mmdet.models.detectors. YOLOV3\n\nonnx_export () (mmdet.models.roi_heads.F CNMaskHead PyramidVisionTrans former\n\nmethod), 395\n\nonnx_export () (mmdet.models.roi_heads.StandardRolHed&yramidVisionTransformerV2\n\nmethod), 411\n\nP\n\nPAA (class in mmdet.models.detectors), 265\n\npaa_reassign() (mmdet.models.dense_heads.PAAHead\nmethod), 350\n\nPAAHead (class in mmdet.models.dense_heads), 347\n\nPad (class in mmdet.datasets.pipelines), 248\n\npad() (mmdet.core.mask.BaseInstanceMasks_ method),\n211\n\npad() (mmdet.core.mask.BitmapMasks method), 214\n\npad() (mmdet.core.mask.PolygonMasks method), 216\n\nPAFPN (class in mmdet.models.necks), 297\n\nPanopticFPN (class in mmdet.models.detectors), 265\n\nPatchEmbed (class in mmdet.models.utils), 430\n\nPhotoMetricDistortion (class in\nmmdet.datasets.pipelines), 248\n\nPISARetinaHead (class in mmdet.models.dense_heads),\n351\n\nPISARoTHead (class in mmdet.models.roi_heads), 402\n\nPISASSDHead (class in mmdet.models.dense_heads), 352\n\nplot_iou_recall() (in module\nmmdet.core.evaluation), 220\nplot_num_recall() (in module\n\nmmdet.core.evaluation), 221\nPointRend (class in mmdet.models.detectors), 265\nPointRendRolHead (class in mmdet.models.roi_heads),\n402\n\n(in module\n\nmmdet.core.evaluation), 221\n\nprocess_polygons() (mmdet.datasets.pipelines.LoadAnnotations\n\nmethod), 244\nPseudoBBoxCoder (class in mmdet.core.bbox), 199\nPseudoSampler (class in mmdet.core.bbox), 199\n(class in\nmmdet.models.backbones), 280\n(class in\nmmdet.models.backbones), 281\n\nQ\n\nQualityFocalLoss (class in mmdet.models.losses), 419\n\nquantize_float() (mmdet.models.backbones.RegNet\nstatic method), 283\n\nQueryInst (class in mmdet.models.detectors), 265\n\nR\n\nrandom() (mmdet.core.bbox.AssignResult class method),\n\n192\n\nrandom() (mmdet.core.bbox.SamplingResult class\nmethod), 201\n\nrandom() (mmdet.core.mask. BitmapMasks class\nmethod), 214\n\nrandom() (mmdet.core.mask.PolygonMasks class\n\nmethod), 217\n\nrandom_choice() (mmdet.core.bbox.RandomSampler\nmethod), 200\n\nrandom_choice() (mmdet.core.bbox.ScoreHLRSampler\nstatic method), 202\n\nrandom_sample() (mmdet.datasets.pipelines.Resize\nstatic method), 253\n\nrandom_sample_ratio()\n(mmdet.datasets.pipelines.Resize\nmethod), 253\n\nrandom_select() (mmdet.datasets.pipelines.Resize\nstatic method), 253\n\nstatic\n\npoints2bbox() (mmdet.models.dense_heads.RepPointsHegdndomAf fine (class in mmdet.datasets. pipelines), 249\n\nmethod), 356\nPolygonMasks (class in mmdet.core.mask), 215\npositive_bag_loss()\n\nRandomCenterCropPad (class in\nmmdet.datasets.pipelines), 249\nRandomCrop (class in mmdet.datasets.pipelines), 251\n\n(mmdet.models.dense_heads. FreeAnchorRetinaHeggndomF Lip (class in mmdet.datasets.pipelines), 251\n\nmethod), 339\npre_pipeline()\nmethod), 234\nprepare_test_img() (mmdet.datasets.CustomDataset\nmethod), 234\nprepare_train_img()\n\n(mmdet.datasets. CustomDataset\n\n(mmdet.datasets. CustomDataset method),\n234\n\npreprocess_example_input() (in module\nmmdet.core.export), 210\n\nprint_map_summary () (in module\n\nmmdet.core.evaluation), 221\n\nRandomSampler (class in mmdet.core.bbox), 199\nRandomShi ft (class in mmdet.datasets.pipelines), 252\nreduce_loss() (in module mmdet.models.losses), 424\nreduce_mean() (in module mmdet.core.utils), 226\n\nrefine_bboxes() (mmdet.models.dense_heads.StageCascadeRPNHead\n\nmethod), 366\n\nrefine_bboxes() (mmdet.models.roi_heads.BBoxHead\nmethod), 385\n\nrefine_bboxes() (mmdet.models.roi_heads.SABLHead\nmethod), 405\n\nreg_pred() (mmdet.models.roi_heads.SABLHead\nmethod), 405\n\n456\n\nIndex\n", "vlm_text": "on nx export()  ( mmdet.models.detectors.Single Stage Detector print recall summary() ( in module method ),  268 mmdet.core.evaluation ),  221 on nx export() ( mmdet.models.detectors.YOLOV3 process polygons()  ( mmdet.datasets.pipelines.Load Annotations method ),  272 method ),  244 on nx export() ( mmdet.models.roi_heads.BBoxHead Pseudo B Box Code r  ( class in mmdet.core.bbox ),  199 method ),  384 Pseudo Sampler  ( class in mmdet.core.bbox ),  199 on nx export()  ( mmdet.models.roi_heads.FC N Mask Head Pyramid Vision Transformer ( class in method ),  395 mmdet.models.backbones ),  280 on nx export()  ( mmdet.models.roi_heads.Standard RoI Head Pyramid Vision Transformer V 2 ( class in method ),  411 mmdet.models.backbones ),  281 \nQ \n\nPAA  ( class in mmdet.models.detectors ),  265 paa reassign()  ( mmdet.models.dense heads.PAAHead method ),  350 PAAHead  ( class in mmdet.models.dense heads ),  347 Pad  ( class in mmdet.datasets.pipelines ),  248 pad()  ( mmdet.core.mask.Base Instance Masks method ), 211 pad()  ( mmdet.core.mask.Bitmap Masks method ),  214 pad()  ( mmdet.core.mask.Polygon Masks method ),  216 PAFPN  ( class in mmdet.models.necks ),  297 Pan optic FP N  ( class in mmdet.models.detectors ),  265 PatchEmbed  ( class in mmdet.models.utils ),  430 PhotoMetric Distortion ( class in mmdet.datasets.pipelines ),  248 PISA Retina Head  ( class in mmdet.models.dense heads ), 351 PISA RoI Head  ( class in mmdet.models.roi_heads ),  402 PISA SSD Head  ( class in mmdet.models.dense heads ),  352 plot i ou recall()(inmodulemmdet.core.evaluation ),  220 plot num recall() ( in module mmdet.core.evaluation ),  221 PointRend  ( class in mmdet.models.detectors ),  265 Point Rend RoI Head  ( class in mmdet.models.roi_heads ), 402 points 2 b box()  ( mmdet.models.dense heads.Rep Points Head method ),  356 Polygon Masks  ( class in mmdet.core.mask ),  215 positive bag loss() ( mmdet.models.dense heads.Free Anchor Retina Head method ),  339 pre pipeline() ( mmdet.datasets.Custom Data set method ),  234 prepare test img()  ( mmdet.datasets.Custom Data set method ),  234 prepare train img() ( mmdet.datasets.Custom Data set method ), 234 pre process example input() ( in module mmdet.core.export ),  210 print map summary() ( in module mmdet.core.evaluation ),  221 \nQuality Focal Loss  ( class in mmdet.models.losses ),  419 quant ize float() ( mmdet.models.backbones.RegNet static method ),  283 QueryInst  ( class in mmdet.models.detectors ),  265 \n\nrandom()  ( mmdet.core.bbox.Assign Result class method ), 192 random() ( mmdet.core.bbox.Sampling Result class method ),  201 random() ( mmdet.core.mask.Bitmap Masks class method ),  214 random() ( mmdet.core.mask.Polygon Masks class method ),  217 random choice() ( mmdet.core.bbox.Random Sampler method ),  200 random choice()  ( mmdet.core.bbox.Score HL R Sampler static method ),  202 random sample() ( mmdet.datasets.pipelines.Resize static method ),  253 random sample ratio()( mmdet.datasets.pipelines.Resize static method ),  253 random select() ( mmdet.datasets.pipelines.Resize static method ),  253 Random Affine  ( class in mmdet.datasets.pipelines ),  249 Random Center Crop Pad ( class in mmdet.datasets.pipelines ),  249 RandomCrop  ( class in mmdet.datasets.pipelines ),  251 RandomFlip  ( class in mmdet.datasets.pipelines ),  251 Random Sampler (class in mmdet.core.bbox), 199Random Shift  ( class in mmdet.datasets.pipelines ),  252 reduce loss()  ( in module mmdet.models.losses ),  424 reduce mean()  ( in module mmdet.core.utils ),  226 refine b boxes()  ( mmdet.models.dense heads.Stage Cascade RP N Head method ),  366 refine b boxes()  ( mmdet.models.roi_heads.BBoxHead method ),  385 refine b boxes()  ( mmdet.models.roi_heads.SABLHead method ),  405 reg_pred() ( mmdet.models.roi_heads.SABLHead method ),  405 "}
{"page": 464, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_464.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nregion_targets() (mmdet.models.dense_heads.StageCascadeRPN Hatliod), 276\n\nmethod), 366\nRegionAssigner (class in mmdet.core.bbox), 200\nRegNet (class in mmdet.models.backbones), 281\n\nroi2bbox() (in module mmdet.core.bbox), 208\nroi_rescale() (mmdet.models.roi_heads.BaseRolExtractor\nmethod), 387\n\nregress_by_class() (mmdet.models.roi_heads.BBoxHeakotate (class in mmdet.datasets.pipelines), 254\n\nmethod), 386\n\nregress_by_class() (mmdet.models.roi_heads.SABLHead\n\nmethod), 405\n\nRepeatDataset (class in mmdet.datasets), 236\n\nreplace_ImageToTensor() (in\nmmdet.datasets), 238\n\nRepPointsDetector (class in mmdet.models.detectors),\n267\n\nRepPointsHead (class in mmdet.models.dense_heads),\n353\n\nRes2Net (class in mmdet.models.backbones), 283\n\nrescale() (mmdet.core.mask.BaseInstanceMasks\nmethod), 212\n\nrescale() (mmdet.core.mask.BitmapMasks_ method),\n214\n\nrescale() (mmdet.core.mask.PolygonMasks method),\n217\n\nResize (class in mmdet.datasets.pipelines), 252\n\nresize() (mmdet.core.mask.BaseInstanceMasks\nmethod), 212\n\nresize() (mmdet.core.mask.BitmapMasks method), 214\n\nresize() (mmdet.core.mask.PolygonMasks_ method),\n217\n\nmodule\n\nresize_feats() (mmdet.models.dense_heads.SOLOHead\n\nmethod), 362\nResLayer (class in mmdet.models.roi_heads), 403\nResLayer (class in mmdet.models.utils), 430\nResNeSt (class in mmdet.models.backbones), 284\nResNet (class in mmdet.models.backbones), 285\nResNetV1d (class in mmdet.models.backbones), 288\nResNeXt (class in mmdet.models.backbones), 285\nresponsible_flags()\n(mmdet.core.anchor. YOLOAnchorGenerator\nmethod), 190\nresults2json()\nmethod), 230\nresults2json() (mmdet.datasets.CocoPanopticDataset\nmethod), 231\nresults2txt() (mmdet.datasets. Cityscapes Dataset\nmethod), 228\nRetinaHead (class in mmdet.models.dense_heads), 356\nRetinaNet (class in mmdet.models.detectors), 267\nRetinaSepBNHead (class in\nmmdet.models.dense_heads), 357\nreweight_loss_single()\n(mmdet.models.dense_heads.F SAF Head\nmethod), 336\nRFP (class in mmdet.models.necks), 298\n\n(mmdet.datasets. CocoDataset\n\nrotate() (mmdet.core.mask.BaseInstanceMasks\n\nmethod), 212\n\nrotate() (mmdet.core.mask.BitmapMasks method), 214\n\nrotate() (mmdet.core.mask.PolygonMasks method),\n217\n\nRPN (class in mmdet.models.detectors), 265\n\nRPNHead (class in mmdet.models.dense_heads), 352\n\niS)\n\nSABLHead (class in mmdet.models.roi_heads), 403\n\nSABLRetinaHead (class in mmdet.models.dense_heads),\n357\n\nsample() (mmdet.core.bbox.BaseSampler method), 193\n\nsample() (mmdet.core.bbox.PseudoSampler method),\n199\n\nsample() (mmdet.core.bbox.ScoreHLRSampler method),\n203\n\nsample_via_interval()\n(mmdet.core.bbox.loUBalancedNegSampler\nmethod), 197\n\nSamplingResult (class in mmdet.core.bbox), 201\n\nsanitize_coordinates()\n(mmdet.models.dense_heads. YOLACTProtonet\nmethod), 374\n\nSCNet (class in mmdet.models.detectors), 267\n\nSCNetBBoxHead (class in mmdet.models.roi_heads), 405\n\nSCNetMaskHead (class in mmdet.models.roi_heads), 406\n\nSCNetRoIHead (class in mmdet.models.roi_heads), 406\n\nSCNetSemanticHead (class in\nmmdet.models.roi_heads), 407\n\nscore_voting() (mmdet.models.dense_heads.PAAHead\nmethod), 351\n\nScoreHLRSampler (class in mmdet.core.bbox), 202\n\nSeesawLoss (class in mmdet.models.losses), 420\n\nSegRescale (class in mmdet.datasets.pipelines), 254\n\nSELayer (class in mmdet.models.utils), 431\n\nselect_single_mlv1l() (in module mmdet.core.utils),\n226\n\nset_epoch() (mmdet.datasets.samplers.InfiniteBatchSampler\nmethod), 256\n\nset_epoch() (mmdet.datasets.samplers.InfiniteGroupBatchSampler\nmethod), 257\n\nset_extra_property()\n\n(mmdet.core.bbox.AssignResult method),\n193\nset_random_seed() (in module mmdet.apis), 182\nShared2FCBBoxHead (class in\n\nmmdet.models.roi_heads), 408\n\nrfp_forward() (mmdet.models.backbones.DetectoRS_ResNet\n\nIndex\n\n457\n", "vlm_text": "region targets()  ( method ),  366 Region As signer  ( class in mmdet.core.bbox ),  200 RegNet  ( class in mmdet.models.backbones ),  281 regress by class()  ( mmdet.models.roi_heads.BBoxHead method ),  386 regress by class()  ( mmdet.models.roi_heads.SABLHead method ),  405 Repeat Data set  ( class in mmdet.datasets ),  236 replace Image To Tensor() ( in module mmdet.datasets ),  238 Rep Points Detector  ( class in mmdet.models.detectors ), 267 Rep Points Head  ( class in mmdet.models.dense heads ), 353 Res2Net  ( class in mmdet.models.backbones ),  283 rescale() ( mmdet.core.mask.Base Instance Masks method ),  212 rescale() ( mmdet.core.mask.Bitmap Masks method ), 214 rescale()  ( mmdet.core.mask.Polygon Masks method ), 217 Resize  ( class in mmdet.datasets.pipelines ),  252 resize() ( mmdet.core.mask.Base Instance Masks method ),  212 resize()  ( mmdet.core.mask.Bitmap Masks method ),  214 resize() ( mmdet.core.mask.Polygon Masks method ), 217 resize feats()  ( mmdet.models.dense heads.SOLOHead method ),  362 ResLayer  ( class in mmdet.models.roi_heads ),  403 ResLayer  ( class in mmdet.models.utils ),  430 ResNeSt  ( class in mmdet.models.backbones ),  284 ResNet  ( class in mmdet.models.backbones ),  285 ResNetV1d  ( class in mmdet.models.backbones ),  288 ResNeXt  ( class in mmdet.models.backbones ),  285 responsible flags() ( mmdet.core.anchor.YO LO Anchor Generator method ),  190 results 2 json() ( mmdet.datasets.Coco Data set method ),  230 results 2 json()  ( mmdet.datasets.Coco Pan optic Data set method ),  231 results 2 txt() ( mmdet.datasets.Cityscape s Data set method ),  228 RetinaHead  ( class in mmdet.models.dense heads ),  356 RetinaNet  ( class in mmdet.models.detectors ),  267 RetinaS epB N Head ( class in mmdet.models.dense heads ),  357 re weight loss single() ( mmdet.models.dense heads.FSAFHead method ),  336 RFP  ( class in mmdet.models.necks ),  298 rfp forward()  ( mmdet.models.backbones.DetectoRS Res Net roi2bbox()  ( in module mmdet.core.bbox ),  208 roi re scale()  ( mmdet.models.roi_heads.Base RoI Extractor method ),  387 Rotate  ( class in mmdet.datasets.pipelines ),  254 rotate() ( mmdet.core.mask.Base Instance Masks method ),  212 rotate()  ( mmdet.core.mask.Bitmap Masks method ),  214 rotate() ( mmdet.core.mask.Polygon Masks method ), 217 RPN  ( class in mmdet.models.detectors ),  265 RPNHead  ( class in mmdet.models.dense heads ),  352 \n\nSABLHead  ( class in mmdet.models.roi_heads ),  403 S ABL Retina Head  ( class in mmdet.models.dense heads ), 357 sample()  ( mmdet.core.bbox.Base Sampler method ),  193 sample()  ( mmdet.core.bbox.Pseudo Sampler method ), 199 sample()  ( mmdet.core.bbox.Score HL R Sampler method ), 203 sample via interval() ( mmdet.core.bbox.I oU Balanced Ne g Sampler method ),  197 Sampling Result  ( class in mmdet.core.bbox ),  201 sanitize coordinates() ( mmdet.models.dense heads.YO L ACT Proton et method ),  374 SCNet  ( class in mmdet.models.detectors ),  267 S CNet B Box Head  ( class in mmdet.models.roi_heads ),  405 S CNet Mask Head  ( class in mmdet.models.roi_heads ),  406 S CNet RoI Head  ( class in mmdet.models.roi_heads ),  406 S CNet Semantic Head ( class in mmdet.models.roi_heads ),  407 score voting()  ( mmdet.models.dense heads.PAAHead method ),  351 Score HL R Sampler  ( class in mmdet.core.bbox ),  202 SeesawLoss  ( class in mmdet.models.losses ),  420 SegRescale  ( class in mmdet.datasets.pipelines ),  254 SELayer  ( class in mmdet.models.utils ),  431 select single ml vl()  ( in module mmdet.core.utils ), 226 set_epoch()  ( mmdet.datasets.samplers.Infinite Batch Sampler method ),  256 set_epoch() (mmdet.datasets.samplers.Infinite Group Batch Samplermethod ),  257 set extra property()( mmdet.core.bbox.Assign Result method ), 193 set random seed() (in module mmdet.apis), 182Shared 2 FCB Box Head ( class in mmdet.models.roi_heads ),  408 "}
{"page": 465, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_465.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nShared4Conv1FCBBoxHead (class in method), 411\n\nmmdet.models.roi_heads), 408 simple_test() (mmdet.models.roi_heads.TridentRoIHead\nShear (class in mmdet.datasets.pipelines), 254 method), 412\nshear () (mmdet.core.mask.BaseInstanceMasks simple_test_bboxes()\n\nmethod), 212 (mmdet.models.dense_heads. DETRHead\nshear () (mmdet.core.mask.BitmapMasks method), 214 method), 327\nshear () (mmdet.core.mask.PolygonMasks method), 217 simple_test_mask() (mmdet.models.detectors. TwoStagePanopticSegmen\nshow_result() (mmdet.models.detectors.BaseDetector method), 271\n\nmethod), 260 simple_test_mask() (mmdet.models.roi_heads.MaskScoringRolHead\nshow_result () (mmdet.models.detectors.CascadeRCNN method), 402\n\nmethod), 261 simple_test_mask() (mmdet.models.roi_heads.PointRendRolHead\nshow_result() (mmdet.models.detectors.RPN method), method), 403\n\n266 simple_test_rpn() (mmdet.models.dense_heads.CascadeRPNHead\nshow_result_pyplot() (in module mmdet.apis), 182 method), 310\nside_aware_feature_extractor() simple_test_rpn() (mmdet.models.dense_heads.EmbeddingRPNHead\n\n(mmdet.models.roi_heads.SABLHead method), method), 331\n\n405 SimplifiedBasicBlock (class in mmdet.models.utils),\nside_aware_splitQ (mmdet.models.roi_heads.SABLHead 431\n\nmethod), 405 SinePositionalEncoding (class in\nsigmoid_focal_loss() (in module mmdet.models.utils), 432\n\nmmdet.models. losses), 424 single_level_grid_anchors()\nsimple_test() (mmdet.models.dense_heads.EmbeddingRPNHead (mmdet.core.anchor.AnchorGenerator method),\n\nmethod), 331 185\nsimple_test() (mmdet.models.dense_heads. YOLACTProteteigle_level_grid_priors()\n\nmethod), 374 (mmdet.core.anchor.AnchorGenerator method),\nsimple_test() (mmdet.models.dense_heads. YOLACTSegmHead 185\n\nmethod), 376 single_level_grid_priors()\nsimple_test() (mmdet.models.detectors.RPN method), (mmdet.core.anchor.MlvlPointGenerator\n\n266 method), 188\nsimple_test() (mmdet.models.detectors.SingleStageDetectdmgle_level_responsible_flags()\n\nmethod), 268 (mmdet.core.anchor. YOLOAnchorGenerator\nsimple_test() (mmdet.models.detectors.SparseRCNN method), 190\n\nmethod), 269 single_level_valid_flags(Q)\nsimple_test() (mmdet.models.detectors.TridentFasterRCNN (mmdet.core.anchor.AnchorGenerator method),\n\nmethod), 269 186\nsimple_test() (mmdet.models.detectors. TwoStageDetectosingle_level_valid_flags()\n\nmethod), 270 (mmdet.core.anchor.MlvlPointGenerator\nsimple_test () (mmdet.models.detectors. TwoStagePanopticSegmentanethod), 189\n\nmethod), 271 SingleRoIExtractor (class in\nsimple_test() (mmdet.models.detectors. YOLACT mmdet.models.roi_heads), 408\n\nmethod), 272 SingleStageDetector (class in\nsimple_test() (mmdet.models.roi_heads.BaseRolHead mmdet.models.detectors), 267\n\nmethod), 387 slice_as() (mmdet.models.necks.F PN_CARAFE\nsimple_test() (mmdet.models.roi_heads.CascadeRoIHead method), 296\n\nmethod), 389 SmoothL1Loss (class in mmdet.models.losses), 421\nsimple_test() (mmdet.models.roi_heads.GridRoIHead SOLO (class in mmdet.models.detectors), 267\n\nmethod), 397 SOLOHead (class in mmdet.models.dense_heads), 360\nsimple_test() (mmdet.models.roi_heads.HybridTaskCascoplaReéHpndor s () (mmdet.core.anchor.AnchorGenerator\n\nmethod), 398 method), 186\nsimple_test() (mmdet.models.roi_heads.SCNetRoIHead sparse_priors() (mmdet.core.anchor.MlvlPointGenerator\n\nmethod), 407 method), 189\nsimple_test() (mmdet.models.roi_heads.SparseRolHead SparseRCNN (class in mmdet.models.detectors), 268\n\nmethod), 410 SparseRolHead (class in mmdet.models.roi_heads), 408\nsimple_test() (mmdet.models.roi_heads.StandardRolHeaxp1it_combined_polys() (in module\n\n458 Index\n", "vlm_text": "Shared 4 Con v 1 FCB Box Head ( class in method ),  411 mmdet.models.roi_heads ),  408 simple test()  ( mmdet.models.roi_heads.Trident RoI Head Shear  ( class in mmdet.datasets.pipelines ),  254 method ),  412 shear() ( mmdet.core.mask.Base Instance Masks simple test b boxes() method ),  212 ( mmdet.models.dense heads.DETRHead shear()  ( mmdet.core.mask.Bitmap Masks method ),  214 method ),  327 shear()  ( mmdet.core.mask.Polygon Masks method ),  217 simple test mask()  ( mmdet.models.detectors.Two Stage Pan opticS eg men show result()  ( mmdet.models.detectors.Base Detector method ),  271 method ),  260 simple test mask()  ( mmdet.models.roi_heads.Mask Scoring RoI Head show result()  ( mmdet.models.detectors.Cascade R CNN method ),  402 method ),  261 simple test mask()  ( mmdet.models.roi_heads.Point Rend RoI Head show result()  ( mmdet.models.detectors.RPN method ), method ),  403 266 simple test rp n()  ( mmdet.models.dense heads.Cascade RP N Head show result py plot()  ( in module mmdet.apis ),  182 method ),  310 side aware feature extractor() simple test rp n()  ( mmdet.models.dense heads.Embedding RP N Head ( mmdet.models.roi_heads.SABLHead method ), method ),  331 405 Simplified Basic Block  ( class in mmdet.models.utils ), side aware split()  ( mmdet.models.roi_heads.SABLHead 431 method ),  405 Sine Positional Encoding ( class in s igm oid focal loss() ( in module mmdet.models.utils ),  432 mmdet.models.losses ),  424 single level grid anchors() simple test()  ( mmdet.models.dense heads.Embedding RP N Head ( mmdet.core.anchor.Anchor Generator method ), method ),  331 185 simple test()  ( mmdet.models.dense heads.YO L ACT Proton et single level grid priors() method ),  374 ( mmdet.core.anchor.Anchor Generator method ), simple test()  ( mmdet.models.dense heads.YO LAC TSe gm Head 185 method ),  376 single level grid priors() simple test()  ( mmdet.models.detectors.RPN method ), ( mmdet.core.anchor.Ml vl Point Generator 266 method ),  188 simple test()  ( mmdet.models.detectors.Single Stage Detector single level responsible flags() method ),  268 ( mmdet.core.anchor.YO LO Anchor Generator simple test()  ( mmdet.models.detectors.SparseRCNN method ),  190 method ),  269 single level valid flags() simple test()  ( mmdet.models.detectors.Trident Faster R CNN ( mmdet.core.anchor.Anchor Generator method ), method ),  269 186 simple test()  ( mmdet.models.detectors.Two Stage Detector single level valid flags() method ),  270 ( mmdet.core.anchor.Ml vl Point Generator simple test()  ( mmdet.models.detectors.Two Stage Pan optic Segment or method ),  189 method ),  271 Single RoI Extractor ( class in simple test() ( mmdet.models.detectors.YOLACT mmdet.models.roi_heads ),  408 method ),  272 Single Stage Detector ( class in simple test()  ( mmdet.models.roi_heads.Base RoI Head mmdet.models.detectors ),  267 method ),  387 slice_as() ( mmdet.models.necks.FPN_CARAFE simple test()  ( mmdet.models.roi_heads.Cascade RoI Head method ),  296 method ),  389 Smooth L 1 Loss  ( class in mmdet.models.losses ),  421 simple test()  ( mmdet.models.roi_heads.Grid RoI Head SOLO  ( class in mmdet.models.detectors ),  267 method ),  397 SOLOHead  ( class in mmdet.models.dense heads ),  360 simple test()  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head  ( mmdet.core.anchor.Anchor Generator method ),  398 method ),  186 simple test()  ( mmdet.models.roi_heads.S CNet RoI Head sparse priors()  ( mmdet.core.anchor.Ml vl Point Generator method ),  407 method ),  189 simple test()  ( mmdet.models.roi_heads.Sparse RoI Head SparseRCNN  ( class in mmdet.models.detectors ),  268 method ),  410 Sparse RoI Head  ( class in mmdet.models.roi_heads ),  408 simple test()  ( mmdet.models.roi_heads.Standard RoI Head split combined poly s() ( in module "}
{"page": 466, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_466.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nmmdet.core.mask), 218\nSSDHead (class in mmdet.models.dense_heads), 362\nSSDNeck (class in mmdet.models.necks), 298\nSSDVGG (class in mmdet.models.backbones), 288\nStageCascadeRPNHead (class in\nmmdet.models.dense_heads), 364\nStandardRoIHead (class in mmdet.models.roi_heads),\n410\n\nstar_dcn_offset © (mmdet.models.dense_heads.VFNetHead\n\nmethod), 370\nSwinTransformer (class in mmdet.models.backbones),\n289\n\nT\n\nTBLRBBoxCoder (class in mmdet.core.bbox), 203\n\ntensor_add() (mmdet.models.necks.F PN_CARAFE\nmethod), 296\n\ntoQ (mmdet.core.bbox.SamplingResult method), 202\n\nto_bitmap () (mmdet.core.mask.PolygonMasks\nmethod), 217\n\nto_ndarray()  (mmdet.core.mask.BaseInstanceMasks\nmethod), 212\n\nto_ndarray()\nmethod), 215\n\nto_ndarray()\nmethod), 217\n\nto_tensor() (in module mmdet.datasets.pipelines), 255\n\nto_tensor() (mmdet.core.mask.BaseInstanceMasks\nmethod), 213\n\nto_tensor() (mmdet.core.mask.BitmapMasks method),\n215\n\nto_tensor()\nmethod), 217\n\nToDataContainer (class in mmdet.datasets.pipelines),\n255\n\nToTensor (class in mmdet.datasets.pipelines), 255\n\ntrainQ (mmdet.models.backbones.CSP Darknet\nmethod), 274\n\ntrain() (mmdet.models.backbones.Darknet method),\n275\n\ntrain() (mmdet.models.backbones.HRNet method), 278\n\ntrainQ (mmdet.models.backbones.MobileNetV2\nmethod), 280\n\ntrain() (mmdet.models.backbones.ResNet method), 288\n\ntrainQ (mmdet.models.backbones.SwinTransformer\nmethod), 290\n\n(mmdet.core.mask. BitmapMasks\n\n(mmdet.core.mask.PolygonMasks\n\n(mmdet.core.mask.PolygonMasks\n\nmethod), 370\n\nTransformer (class in mmdet.models.utils), 432\n\nTranslate (class in mmdet.datasets.pipelines), 255\n\ntranslate() (mmdet.core.mask.BaseInstanceMasks\nmethod), 213\n\ntranslate() (mmdet.core.mask.BitmapMasks method),\n215\n\ntranslate()\n\nmethod), 217\n\nTranspose (class in mmdet.datasets.pipelines), 255\n\nTridentFasterRCNN (class in mmdet.models.detectors),\n269\n\nTridentResNet (class in mmdet.models.backbones), 290\n\nTridentRoIHead (class in mmdet.models.roi_heads),\n412\n\nTwoStageDetector (class in mmdet.models.detectors),\n269\n\nTwoStagePanopticSegmentor\nmmdet.models.detectors), 270\n\n(mmdet.core.mask.PolygonMasks\n\n(class in\n\nU\n\nunmap () (in module mmdet.core.utils), 226\n\nupdate_dynamic_scale()\n(mmdet.datasets.MultilmageMixDataset\nmethod), 236\n\nupdate_hyperparameters()\n(mmdet.models.roi_heads.DynamicRolIHead\nmethod), 393\n\nupdate_skip_type_keys()\n(mmdet.datasets.MultilmageMixDataset\nmethod), 236\n\nVv\n\nval_step() (mmdet.models.detectors.Base Detector\nmethod), 261\n\nvalid_flags() (mmdet.core.anchor.AnchorGenerator\nmethod), 186\n\nvalid_flags() (mmdet.core.anchor.MlvIPointGenerator\nmethod), 189\n\nVarifocalLoss (class in mmdet.models.losses), 422\n\nVFNet (class in mmdet.models.detectors), 271\n\nVFNetHead (class in mmdet.models.dense_heads), 366\n\nvoCDataset (class in mmdet.datasets), 236\n\nWw\n\nweighted_loss() (in module mmdet.models.losses),\n\ntrainQ (mmdet.models.detectors.KnowledgeDistillationSingleStageDesgctor\n\nmethod), 265\n\ntrain() (mmdet.models.roi_heads.ResLayer method),\n403\n\ntrain_step() (mmdet.models.detectors.BaseDetector\nmethod), 260\n\ntransform_bbox_targets()\n(mmdet.models.dense_heads.VFNetHead\n\nWIDERFaceDataset (class in mmdet.datasets), 237\n\nwith_bbox (mmdet.models.detectors.BaseDetector prop-\nerty), 261\n\nwith_bbox (mmdet.models.roi_heads.BaseRolHead\nproperty), 387\n\nwith_feat_relay (mmdet.models.roi_heads.SCNetRolHead\n\nproperty), 407\n\nIndex\n\n459\n", "vlm_text": "\nSSDHead  ( class in mmdet.models.dense heads ),  362 SSDNeck  ( class in mmdet.models.necks ),  298 SSDVGG  ( class in mmdet.models.backbones ),  288 Stage Cascade RP N Head ( class in mmdet.models.dense heads ),  364 Standard RoI Head  ( class in mmdet.models.roi_heads ), 410 star dc n offset()  ( mmdet.models.dense heads.VFNetHead method ),  370 S win Transformer  ( class in mmdet.models.backbones ), 289 \n\nT BL RB Box Code r  ( class in mmdet.core.bbox ),  203 tensor_add() ( mmdet.models.necks.FPN_CARAFE method ),  296 to()  ( mmdet.core.bbox.Sampling Result method ),  202 to_bitmap() ( mmdet.core.mask.Polygon Masks method ),  217 to_ndarray() ( mmdet.core.mask.Base Instance Masks method ),  212 to_ndarray() ( mmdet.core.mask.Bitmap Masks method ),  215 to_ndarray() ( mmdet.core.mask.Polygon Masks method ),  217 to_tensor()  ( in module mmdet.datasets.pipelines ),  255 to_tensor() ( mmdet.core.mask.Base Instance Masks method ),  213 to_tensor()  ( mmdet.core.mask.Bitmap Masks method ), 215 to_tensor() ( mmdet.core.mask.Polygon Masks method ),  217 To Data Container  ( class in mmdet.datasets.pipelines ), 255 ToTensor  ( class in mmdet.datasets.pipelines ),  255 train() ( mmdet.models.backbones.CSPDarknet method ),  274 train() ( mmdet.models.backbones.Darknet method ), 275 train()  ( mmdet.models.backbones.HRNet method ),  278 train() ( mmdet.models.backbones.Mobile Ne tV 2 method ),  280 train()  ( mmdet.models.backbones.ResNet method ),  288 train() ( mmdet.models.backbones.S win Transformer method ),  290 train()  ( mmdet.models.detectors.Knowledge Distillation Single Stage Detector method ),  265 train()  ( mmdet.models.roi_heads.ResLayer method ), 403 train_step() ( mmdet.models.detectors.Base Detector method ),  260 transform b box targets() ( mmdet.models.dense heads.VFNetHead \nmethod ),  370 Transformer  ( class in mmdet.models.utils ),  432 Translate  ( class in mmdet.datasets.pipelines ),  255 translate() ( mmdet.core.mask.Base Instance Masks method ),  213 translate()  ( mmdet.core.mask.Bitmap Masks method ), 215 translate() ( mmdet.core.mask.Polygon Masks method ),  217 Transpose  ( class in mmdet.datasets.pipelines ),  255 Trident Faster R CNN  ( class in mmdet.models.detectors ), 269 Trident Res Net  ( class in mmdet.models.backbones ),  290 Trident RoI Head  ( class in mmdet.models.roi_heads ), 412 Two Stage Detector  ( class in mmdet.models.detectors ), 269 Two Stage Pan optic Segment or ( class in mmdet.models.detectors ),  270 \n\nunmap()  ( in module mmdet.core.utils ),  226 update dynamic scale() ( mmdet.datasets.Multi Image Mix Data set method ),  236 update hyper parameters() ( mmdet.models.roi_heads.Dynamic RoI Head method ),  393 update skip type keys()( mmdet.datasets.Multi Image Mix Data set method ),  236 \n\nval_step() ( mmdet.models.detectors.Base Detector method ),  261 valid flags() ( mmdet.core.anchor.Anchor Generator method ),  186 valid flags()  ( mmdet.core.anchor.Ml vl Point Generator method ),  189 Var i focal Loss  ( class in mmdet.models.losses ),  422 VFNet  ( class in mmdet.models.detectors ),  271 VFNetHead  ( class in mmdet.models.dense heads ),  366 VOCDataset  ( class in mmdet.datasets ),  236 \nweighted loss()  ( in module mmdet.models.losses ), WIDER Face Data set  ( class in mmdet.datasets ),  237 with_bbox  ( mmdet.models.detectors.Base Detector prop- erty ),  261 with_bbox ( mmdet.models.roi_heads.Base RoI Head property ),  387 with feat relay  ( mmdet.models.roi_heads.S CNet RoI Head property ),  407 "}
{"page": 467, "image_path": "doc_images/mmdetection-readthedocs-io-en-v2.18.0_467.jpg", "ocr_text": "MMDetection, Release 2.18.0\n\nwith_glbctx (mmdet.models.roi_heads.SCNetRolHead\nproperty), 407\n\nwith_mask (mmdet.models.detectors.BaseDetector prop-\nerty), 261\n\nwith_mask (mmdet.models.roi_heads.BaseRolHead\nproperty), 387\n\nwith_neck (mmdet.models.detectors.BaseDetector prop-\nerty), 261\n\nwith_roi_head (mmdet.models.detectors.TwoStageDetector\nproperty), 270\n\nwith_rpn  (mmdet.models.detectors.TwoStage Detector\nproperty), 270\n\nwith_semantic (mmdet.models.detectors.HybridTaskCascade\nproperty), 264\n\nwith_semantic (mmdet.models.roi_heads.HybridTaskCascadeRolHead\nproperty), 399\n\nwith_semantic (mmdet.models.roi_heads.SCNetRoIHead\nproperty), 407\n\nwith_shared_head (mmdet.models.detectors.BaseDetector\nproperty), 261\n\nwith_shared_head (mmdet.models.roi_heads.BaseRolHead\nproperty), 388\n\nX\n\nXMLDataset (class in mmdet.datasets), 237\nxyxy2xywh() (mmdet.datasets.CocoDataset method),\n230\n\nY\n\nYOLACT (class in mmdet.models.detectors), 27\nYOLACTHead (class in mmdet.models.dense_heads), 370\nYOLACTProtonet (class in mmdet.models.dense_heads),\n372\n\nYOLACTSegmHead (class in mmdet.models.dense_heads),\n375\n\nyOLOAnchorGenerator (class in mmdet.core.anchor),\n189\n\nYOLOF (class in mmdet.models.detectors), 272\nYOLOFHead (class in mmdet.models.dense_heads), 376\nYOLOV3 (class in mmdet.models.detectors), 272\nYOLOV3Head (class in mmdet.models.dense_heads), 377\nYOLOV3Neck (class in mmdet.models.necks), 299\n\nYOLOX (class in mmdet.models.detectors), 272\nYOLOXHead (class in mmdet.models.dense_heads), 381\nYOLOXPAFPN (class in mmdet.models.necks), 300\n\n460\n\nIndex\n", "vlm_text": "with gl bc tx  ( mmdet.models.roi_heads.S CNet RoI Head property ),  407 with_mask  ( mmdet.models.detectors.Base Detector prop- erty ),  261 with_mask ( mmdet.models.roi_heads.Base RoI Head property ),  387 with_neck  ( mmdet.models.detectors.Base Detector prop- erty ),  261 with roi head  ( mmdet.models.detectors.Two Stage Detector property ),  270 with_rpn ( mmdet.models.detectors.Two Stage Detector property ),  270 with semantic  ( mmdet.models.detectors.Hybrid Task Cascade property ),  264 with semantic  ( mmdet.models.roi_heads.Hybrid Task Cascade RoI Head property ),  399 with semantic  ( mmdet.models.roi_heads.S CNet RoI Head property ),  407 with shared head  ( mmdet.models.detectors.Base Detector property ),  261 with shared head  ( mmdet.models.roi_heads.Base RoI Head property ),  388 \nX \nXMLDataset  ( class in mmdet.datasets ),  237 xyxy2xywh() ( mmdet.datasets.Coco Data set method ), 230 \n\nYOLACT  ( class in mmdet.models.detectors ),  271 YOLACTHead  ( class in mmdet.models.dense heads ),  370 YO L ACT Proton et  ( class in mmdet.models.dense heads ), 372 YO LAC TSe gm Head  ( class in mmdet.models.dense heads ), 375 YO LO Anchor Generator  ( class in mmdet.core.anchor ), 189 YOLOF  ( class in mmdet.models.detectors ),  272 YOLOFHead  ( class in mmdet.models.dense heads ),  376 YOLOV3  ( class in mmdet.models.detectors ),  272 YOLOV3Head  ( class in mmdet.models.dense heads ),  377 YOLOV3Neck  ( class in mmdet.models.necks ),  299 YOLOX  ( class in mmdet.models.detectors ),  272 YOLOXHead  ( class in mmdet.models.dense heads ),  381 YOLOXPAFPN  ( class in mmdet.models.necks ),  300 "}
